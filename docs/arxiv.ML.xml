<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</title>
<link>https://arxiv.org/abs/2512.14706</link>
<guid>https://arxiv.org/abs/2512.14706</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Architecture Search, Large Language Models, Image Captioning, BLEU-4, AutoML<br><br>Summary:  
1. The paper introduces NN-Caption, a neural architecture search (NAS) pipeline guided by large language models (LLMs) to automatically generate runnable image-captioning models.  
2. NN-Caption composes convolutional neural network (CNN) encoders from LEMUR’s classification backbones with sequence decoders such as LSTM, GRU, or Transformer, adhering to a strict Net API for compatibility.  
3. The approach utilizes DeepSeek-R1-0528-Qwen3-8B as the primary LLM generator, and the authors provide prompt templates and examples of generated architectures.  
4. Evaluation is performed on the MS COCO dataset, using the BLEU-4 metric to measure caption quality.  
5. Results demonstrate that the LLM generated dozens of image-captioning models, with over half successfully trained and producing meaningful captions.  
6. An analysis compares using 5 versus 10 input model snippets in prompts, finding a slight decrease in success rate when more components are provided.  
7. Training dynamics, including caption accuracy over epochs and peak BLEU-4 scores, are reported to characterize model performance.  
8. The study highlights the LLM’s additional role in suggesting hyperparameters and training practices alongside architecture design.  
9. Challenges encountered include code hallucinations and API compliance issues, which were mitigated through prompt engineering and iterative code corrections.  
10. The work integrates prompt-based code generation with automatic evaluation and contributes numerous novel captioning models to the public LEMUR dataset, facilitating reproducible benchmarking and advancing AutoML research. <div>
arXiv:2512.14706v1 Announce Type: new 
Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Source Knowledge Selection in Multi-Domain Adaptation</title>
<link>https://arxiv.org/abs/2512.14710</link>
<guid>https://arxiv.org/abs/2512.14710</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised multi-domain adaptation, transfer learning, source knowledge selection, density-driven selection, pseudo-label enhancement<br><br>Summary: Unsupervised multi-domain adaptation is a critical area of transfer learning that utilizes knowledge from multiple labeled source domains to improve learning in an unlabeled target domain. The challenge addressed in this paper is the redundancy and irrelevance of information present in numerous source domains, which can negatively impact transfer performance, especially in scenarios with many source domains. To tackle this, the authors propose a novel method called Autonomous Source Knowledge Selection (AutoS), which autonomously selects the most relevant source training samples and models to enhance target task prediction. AutoS employs a density-driven selection strategy to efficiently pick source samples during training and to determine the contribution of various source models toward target predictions. Additionally, the method integrates a pseudo-label enhancement module that leverages a pre-trained multimodal model to reduce label noise in the target domain and boost self-supervision. Experimental results on real-world datasets demonstrate the superiority and effectiveness of AutoS in improving transfer learning performance by selecting more transferable knowledge and filtering out less relevant source information, paving the way for more scalable and efficient multi-domain adaptation. <div>
arXiv:2512.14710v1 Announce Type: new 
Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</title>
<link>https://arxiv.org/abs/2512.14712</link>
<guid>https://arxiv.org/abs/2512.14712</guid>
<content:encoded><![CDATA[
<div> Keywords: Sepsis, Deep Fusion, Context-Aware Stacking, Mixture-of-Experts, Antibiotic Selection<br><br>Summary:<br><br>Sepsis is a major cause of ICU admissions worldwide, but traditional prediction models struggle to effectively combine diverse data types such as vitals, text, and imaging. This study compares two architectural approaches: End-to-End Deep Fusion and Context-Aware Stacking. Initially, the authors developed SepsisFusionFormer, a Quad-Modal Hierarchical Gated Attention Network designed to handle complex cross-modal interactions. However, it underperformed due to "attention starvation" and overfitting in a limited antibiotic cohort, achieving an AUC of only 0.66 on MIMIC-IV data. Learning from this, they proposed SepsisLateFusion, a simplified Context-Aware Mixture-of-Experts model that considers each modality as an independent expert: "Historian" (Static data), "Monitor" (Temporal data), and "Reader" (NLP data). These are dynamically weighted by a CatBoost meta-learner. This approach significantly improved performance, reaching a state-of-the-art AUC of 0.915 for predicting sepsis 4 hours before clinical onset. Additionally, adjusting the decision threshold reduced missed cases by 48%, allowing a genuine preventive intervention window. For multi-class antibiotic selection—a novel prescriptive task—the Quad-Modal Ensemble also excelled with a 0.72 AUC. The resulting models are packaged in SepsisSuite, a freely available Python clinical decision support framework. <div>
arXiv:2512.14712v1 Announce Type: new 
Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour</title>
<link>https://arxiv.org/abs/2512.14713</link>
<guid>https://arxiv.org/abs/2512.14713</guid>
<content:encoded><![CDATA[
<div> travel decisions, experience formation, heterogeneity, Latent Class Reinforcement Learning, Variational Bayes  

<br><br>Summary:  
This paper addresses the role of experience formation in travel decisions, emphasizing that individuals learn and adapt their preferences over time. It highlights the significant heterogeneity among travelers, both in their baseline preferences and in how these preferences develop. To model these aspects, the authors propose a Latent Class Reinforcement Learning (LCRL) framework that captures preference evolution and individual differences simultaneously. The model is applied to data from a driving simulator experiment, providing a practical setting for parameter estimation. Variational Bayes is employed as the estimation technique, allowing for efficient inference within the complex model. Through estimation, three distinct traveler classes are identified: (1) individuals with context-dependent preferences who show exploitative behavior tailored to specific contexts, (2) individuals who persistently exploit a fixed strategy regardless of the context, and (3) individuals who adopt an exploratory approach while still maintaining context-sensitive preferences. These findings advance understanding of how different travelers form and adjust preferences and provide a valuable tool for analyzing dynamic decision-making in travel behavior. The study’s methodological contributions and insights have implications for personalized travel interventions and policy design. <div>
arXiv:2512.14713v1 Announce Type: new 
Abstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model that allows analysts to capture both of these phenomena. We apply the model to a driving simulator dataset and estimate the parameters through Variational Bayes. We identify three distinct classes of individuals that differ markedly in how they adapt their preferences: the first displays context-dependent preferences with context-specific exploitative tendencies; the second follows a persistent exploitative strategy regardless of context; and the third engages in an exploratory strategy combined with context-specific preferences.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms</title>
<link>https://arxiv.org/abs/2512.14714</link>
<guid>https://arxiv.org/abs/2512.14714</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater acoustic classification, Gabor convolution, ResNeXt, squeeze-and-excitation, environmental noise<br><br>Summary:<br><br>This paper addresses the challenge of remotely detecting and classifying underwater acoustic targets, crucial for environmental monitoring and defense. Underwater noise complexity, including ship-radiated and environmental noise, complicates accurate signal processing. To overcome these challenges, the authors propose GSE ResNeXt, a deep learning model that integrates learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor convolution layers act as adaptive 2D band-pass filters, expanding feature channel representation, while the channel attention modules improve training stability and convergence. The model is tested on three classification tasks with increasing complexity, including the impact of temporal and spatial differences between training and testing data, highlighting that vessel-sensor distance significantly influences performance. Results demonstrate that GSE ResNeXt consistently outperforms popular baseline models such as Xception, ResNet, and MobileNetV2 in classification accuracy. Additionally, incorporating Gabor convolutions in early layers leads to a 28% reduction in training time, boosting convergence speed. The findings emphasize the importance of integrating signal processing techniques in deep learning to enhance robustness and generalization in data-limited underwater acoustic classification scenarios. Future work should aim to further mitigate environmental factors affecting input signals for improved reliability. <div>
arXiv:2512.14714v1 Announce Type: new 
Abstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues such as limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated on three classification tasks of increasing complexity. In particular, the impact of temporal differences between the training and testing data is explored, revealing that the distance between the vessel and sensor significantly affects performance. Results show that, GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, the addition of Gabor convolutions in the initial layers of the model represents a 28% reduction in training time. These results emphasise the importance of signal processing strategies in improving the reliability and generalisation of models under different environmental conditions, especially in data-limited underwater acoustic classification scenarios. Future developments should focus on mitigating the impact of environmental factors on input signals.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection</title>
<link>https://arxiv.org/abs/2512.14715</link>
<guid>https://arxiv.org/abs/2512.14715</guid>
<content:encoded><![CDATA[
<div> Keywords: bit flips, large language model, image captioning, semantic drift, differentiable fault analysis  

<br><br>Summary:  
This work explores the impact of hardware bit flips—low-level, bitwise perturbations—in the weights of large language models (LLMs) used for image captioning. Unlike previous fault injection studies that focused primarily on accuracy degradation or system crashes, this research investigates how single flipped bits can subtly influence the semantic meaning of generated captions without disrupting grammar or fluency. The authors hypothesize that these semantic changes are not random but can be predicted using the model’s gradients. They propose BLADE (Bit-level Fault Analysis via Differentiable Estimation), a novel framework that leverages gradient-based sensitivity estimation combined with a semantic-fluency objective at the caption level to identify bits that critically affect meaning. This approach allows a targeted understanding of how semantic information is encoded and distributed at an extremely low level within generative vision-language models. The findings demonstrate that imperceptible bit-level alterations can steer the narrative produced by such models, highlighting vulnerabilities and interpretability aspects. This insight facilitates robustness testing and adversarial defense while contributing to explainable AI by showing how structured low-level faults reshape high-level semantic outputs in generative systems. <div>
arXiv:2512.14715v1 Announce Type: new 
Abstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox</title>
<link>https://arxiv.org/abs/2512.14717</link>
<guid>https://arxiv.org/abs/2512.14717</guid>
<content:encoded><![CDATA[
<div> Keywords: GPT-OSS, financial NLP, model efficiency, large language models, benchmark evaluation  

<br><br>Summary:  
This paper presents a comprehensive evaluation of the GPT-OSS large language model family in financial natural language processing (NLP) tasks, benchmarking against other contemporary LLMs. The study focuses on two variants of GPT-OSS, a 120 billion parameter model and a smaller 20 billion parameter version, revealing that the smaller GPT-OSS-20B achieves performance nearly equal to its larger counterpart (65.1% vs 66.5% accuracy). It also demonstrates superior computational efficiency, measured by a Token Efficiency Score of 198.4 and a processing speed of 159.80 tokens per second. The evaluation covers ten diverse financial NLP tasks, including sentiment analysis, question answering, and entity recognition, utilizing real-world datasets such as Financial PhraseBank, FiQA-SA, and FLARE FINERORD. The authors introduce new efficiency metrics that effectively balance model accuracy with resource consumption, critical for practical deployment decisions. Additionally, the GPT-OSS models consistently outperform larger models like Qwen3-235B, challenging the assumption that bigger models always yield better task performance. The findings highlight how architectural advancements and training methodologies enable smaller GPT-OSS models to maintain competitive accuracy while significantly reducing computational demands, supporting more sustainable and cost-effective use of LLMs in financial service applications. <div>
arXiv:2512.14717v1 Announce Type: new 
Abstract: The rapid adoption of large language models in financial services necessitates rigorous evaluation frameworks to assess their performance, efficiency, and practical applicability. This paper conducts a comprehensive evaluation of the GPT-OSS model family alongside contemporary LLMs across ten diverse financial NLP tasks. Through extensive experimentation on 120B and 20B parameter variants of GPT-OSS, we reveal a counterintuitive finding: the smaller GPT-OSS-20B model achieves comparable accuracy (65.1% vs 66.5%) while demonstrating superior computational efficiency with 198.4 Token Efficiency Score and 159.80 tokens per second processing speed [1]. Our evaluation encompasses sentiment analysis, question answering, and entity recognition tasks using real-world financial datasets including Financial PhraseBank, FiQA-SA, and FLARE FINERORD. We introduce novel efficiency metrics that capture the trade-off between model performance and resource utilization, providing critical insights for deployment decisions in production environments. The benchmark reveals that GPT-OSS models consistently outperform larger competitors including Qwen3-235B, challenging the prevailing assumption that model scale directly correlates with task performance [2]. Our findings demonstrate that architectural innovations and training strategies in GPT-OSS enable smaller models to achieve competitive performance with significantly reduced computational overhead, offering a pathway toward sustainable and cost-effective deployment of LLMs in financial applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.14718</link>
<guid>https://arxiv.org/abs/2512.14718</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate time series, spectral entropy, dependency modeling, signed graph, temporal positions<br><br>Summary: This paper introduces SEED, a novel framework designed to improve multivariate time series forecasting by effectively modeling complex spatial-temporal dependencies among variables. The authors identify three main challenges in existing attention- or graph-based methods: disruption of strong temporal self-dependencies by irrelevant variables, the masking and reversal of negative correlations due to softmax normalization, and poor perception of temporal positions by variables. To overcome these, SEED incorporates a Dependency Evaluator that uses spectral entropy to dynamically assess spatial and temporal dependencies per variable, allowing an adaptive balance between channel independence and dependence. It further employs a Spectral Entropy-based Fuser to isolate temporal regularities influenced externally rather than intrinsically. The framework also features a Signed Graph Constructor to maintain negative correlations by enabling signed edge weights, circumventing the softmax limitations. Lastly, SEED introduces a Context Spatial Extractor that uses local contextual windows to help variables capture temporal positions, leading to richer spatial feature representations. Extensive experiments on 12 diverse real-world datasets demonstrate that SEED achieves state-of-the-art forecasting performance, confirming its effectiveness and broad applicability across domains. <div>
arXiv:2512.14718v1 Announce Type: new 
Abstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Attribution Priors for Explainable and Robust Model Training</title>
<link>https://arxiv.org/abs/2512.14719</link>
<guid>https://arxiv.org/abs/2512.14719</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Language Models, Attribution Priors, Interpretability, Class-Aware Attribution Prior, Robustness<br><br>Summary:<br><br>Small language models (SLMs) are increasingly employed in low-latency tasks such as classification, where interpretability and robustness are critical. Existing explanation-guided learning methods introduce attribution-based supervision but struggle due to the difficulty in deriving reliable and general attribution priors. The paper analyzes popular attribution techniques and reveals that while they effectively highlight tokens relevant to each class, they tend to focus on shared keywords across semantically similar classes. This overlap hampers the model’s ability to distinguish between such classes because the attribution priors lack discriminative power. To address this, the authors propose the Class-Aware Attribution Prior (CAP), a novel method designed to extract attribution priors that emphasize fine-grained class distinctions and generate more discriminative features. Extending this further, CAP Hybrid integrates CAP-derived priors with those from existing methods to provide a more balanced and comprehensive supervisory signal. By training models to align their self-attribution with these enriched priors, the approach fosters the learning of diverse and decision-relevant features. Extensive experiments across full-data, few-shot, and adversarial testing scenarios demonstrate consistent improvements in both interpretability and robustness of the language models. <div>
arXiv:2512.14719v1 Announce Type: new 
Abstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example</title>
<link>https://arxiv.org/abs/2512.14721</link>
<guid>https://arxiv.org/abs/2512.14721</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, Synthea, rule-based data generation, glioblastoma, privacy-preserving research  

<br><br>Summary:  
1. The paper addresses the generation of synthetic medical data as a tool for enabling privacy-compliant secondary use of medical records.  
2. It highlights the rule-based Synthea data generator, which creates realistic patient data by simulating the lifetime of synthetic patients using probabilistic rules based on factors like age.  
3. Although Synthea rules contain only statistical information and typically have no specific data protection issues, designing them requires expert knowledge and realistic datasets.  
4. The authors propose an automated method to generate Synthea rules by extracting statistical information from existing tabular medical data specifically related to cancer reports.  
5. As a case study, they developed a Synthea module for glioblastoma using real-world data, then generated synthetic datasets that successfully mirrored known disease courses and largely preserved the statistical properties of the original data.  
6. The study underscores the potential of synthetic patient data for privacy-preserving medical research, enabling hypothesis formulation and prototype development while cautioning that interpretation must consider inherent limitations of synthetic data. <div>
arXiv:2512.14721v1 Announce Type: new 
Abstract: The generation of synthetic data is a promising technology to make medical data available for secondary use in a privacy-compliant manner. A popular method for creating realistic patient data is the rule-based Synthea data generator. Synthea generates data based on rules describing the lifetime of a synthetic patient. These rules typically express the probability of a condition occurring, such as a disease, depending on factors like age. Since they only contain statistical information, rules usually have no specific data protection requirements. However, creating meaningful rules can be a very complex process that requires expert knowledge and realistic sample data. In this paper, we introduce and evaluate an approach to automatically generate Synthea rules based on statistics from tabular data, which we extracted from cancer reports. As an example use case, we created a Synthea module for glioblastoma from a real-world dataset and used it to generate a synthetic dataset. Compared to the original dataset, the synthetic data reproduced known disease courses and mostly retained the statistical properties. Overall, synthetic patient data holds great potential for privacy-preserving research. The data can be used to formulate hypotheses and to develop prototypes, but medical interpretation should consider the specific limitations as with any currently available approach.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers</title>
<link>https://arxiv.org/abs/2512.14722</link>
<guid>https://arxiv.org/abs/2512.14722</guid>
<content:encoded><![CDATA[
<div> transformers, Groebner bases, Hierarchical Attention Transformers, multivariate polynomials, curriculum learning<br><br>Summary:<br>1. The paper builds on the work presented by Kera et al. at NeurIPS 2024, who introduced the use of transformers to compute Groebner bases, which are essential tools in computer algebra for solving polynomial systems and other applications.<br>2. The authors propose an improved method that leverages Hierarchical Attention Transformers (HATs), a transformer architecture with a tree-structured inductive bias that better captures the hierarchical relationships inherent in the data.<br>3. Compared to traditional flat attention models, the HAT approach provides substantial computational savings by exploiting these hierarchical structures, making it more efficient for Groebner basis computation.<br>4. The approach generalizes to arbitrary depths, allowing it to handle more complex and larger polynomial systems than previously possible.<br>5. By integrating curriculum learning, the method successfully solves larger problem instances than those tackled by Kera et al. (2024), demonstrating enhanced scalability and effectiveness in computing Groebner bases for multivariate polynomial equations. <div>
arXiv:2512.14722v1 Announce Type: new 
Abstract: At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion</title>
<link>https://arxiv.org/abs/2512.14725</link>
<guid>https://arxiv.org/abs/2512.14725</guid>
<content:encoded><![CDATA[
<div> Keywords: urban wind flow, generative diffusion, graph neural network, CFD simulation, uncertainty-aware predictions<br><br>Summary:<br><br>1. The article addresses the challenge of modeling and simulating urban wind flow, which is crucial for air quality and sustainable urban planning. <br>2. Traditional low-order models fail to capture complex urban geometries effectively, while high-fidelity CFD simulations are computationally expensive and impractical for multiple scenarios.<br>3. The authors propose a generative diffusion framework that synthesizes steady-state urban wind velocity fields on unstructured meshes based solely on geometry information.<br>4. The framework integrates a hierarchical graph neural network with score-based diffusion modeling, enabling the generation of accurate, diverse velocity fields without the need for temporal rollouts or dense measurement data.<br>5. Trained on datasets spanning multiple mesh slices and wind angles, the model generalizes well to unseen geometries and successfully recovers important flow features like wakes and recirculation zones.<br>6. The model provides uncertainty-aware predictions and robustness to variations in mesh design and inference methods, as demonstrated by ablation studies.<br>7. This approach sets the foundation for building environment models that can assist urban planners in rapidly evaluating design decisions, especially under conditions of urban densification and climate uncertainty. <div>
arXiv:2512.14725v1 Announce Type: new 
Abstract: Urban wind flow modeling and simulation play an important role in air quality assessment and sustainable city planning. A key challenge for modeling and simulation is handling the complex geometries of the urban landscape. Low order models are limited in capturing the effects of geometry, while high-fidelity Computational Fluid Dynamics (CFD) simulations are prohibitively expensive, especially across multiple geometries or wind conditions. Here, we propose a generative diffusion framework for synthesizing steady-state urban wind fields over unstructured meshes that requires only geometry information. The framework combines a hierarchical graph neural network with score-based diffusion modeling to generate accurate and diverse velocity fields without requiring temporal rollouts or dense measurements. Trained across multiple mesh slices and wind angles, the model generalizes to unseen geometries, recovers key flow structures such as wakes and recirculation zones, and offers uncertainty-aware predictions. Ablation studies confirm robustness to mesh variation and performance under different inference regimes. This work develops is the first step towards foundation models for the built environment that can help urban planners rapidly evaluate design decisions under densification and climate uncertainty.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.14726</link>
<guid>https://arxiv.org/abs/2512.14726</guid>
<content:encoded><![CDATA[
<div> Keywords: Offline reinforcement learning, Quantum Decision Transformer, Quantum-Inspired Attention, Quantum Feedforward Networks, Long-horizon credit assignment<br><br>Summary:<br><br>1. The paper introduces the Quantum Decision Transformer (QDT), a novel architecture designed to improve offline reinforcement learning, especially addressing challenges with long-horizon credit assignment and complex state-action dependencies.  
2. QDT incorporates two main quantum-inspired components: Quantum-Inspired Attention that uses entanglement operations to capture non-local feature correlations, and Quantum Feedforward Networks which employ multi-path processing and learnable interference for adaptive computation.  
3. Experimental results on continuous control tasks show that QDT achieves over 2,000% performance improvement compared to standard Decision Transformers, along with superior generalization across different data qualities.  
4. Ablation studies highlight a significant synergy between the quantum-inspired components, where individually neither component achieves competitive results, but together they produce dramatic performance gains exceeding their individual effects.  
5. The architecture’s advantages include enhanced credit assignment through capturing non-local correlations, implicit ensemble-like behavior via parallel processing paths, and adaptive resource allocation enabled by learnable interference, demonstrating a holistic quantum-inspired design principle that could influence broader neural architecture design beyond reinforcement learning. <div>
arXiv:2512.14726v1 Announce Type: new 
Abstract: Offline reinforcement learning enables policy learning from pre-collected datasets without environment interaction, but existing Decision Transformer (DT) architectures struggle with long-horizon credit assignment and complex state-action dependencies. We introduce the Quantum Decision Transformer (QDT), a novel architecture incorporating quantum-inspired computational mechanisms to address these challenges. Our approach integrates two core components: Quantum-Inspired Attention with entanglement operations that capture non-local feature correlations, and Quantum Feedforward Networks with multi-path processing and learnable interference for adaptive computation. Through comprehensive experiments on continuous control tasks, we demonstrate over 2,000\% performance improvement compared to standard DTs, with superior generalization across varying data qualities. Critically, our ablation studies reveal strong synergistic effects between quantum-inspired components: neither alone achieves competitive performance, yet their combination produces dramatic improvements far exceeding individual contributions. This synergy demonstrates that effective quantum-inspired architecture design requires holistic co-design of interdependent mechanisms rather than modular component adoption. Our analysis identifies three key computational advantages: enhanced credit assignment through non-local correlations, implicit ensemble behavior via parallel processing, and adaptive resource allocation through learnable interference. These findings establish quantum-inspired design principles as a promising direction for advancing transformer architectures in sequential decision-making, with implications extending beyond reinforcement learning to neural architecture design more broadly.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications</title>
<link>https://arxiv.org/abs/2512.14727</link>
<guid>https://arxiv.org/abs/2512.14727</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Conformal Prediction, Uncertainty Estimates, Calibration Set Size, Medical Image Classification  

<br><br>Summary:  
1. Machine learning (ML) is increasingly used in healthcare, but dependable uncertainty estimates are necessary for safe clinical decision-making, which traditional ML models typically lack.  
2. Conformal prediction (CP) is a technique that provides statistically guaranteed uncertainty estimates by converting ML model predictions, combined with a calibration sample, into prediction sets with a user-specified coverage probability.  
3. CP theory claims to hold regardless of calibration set size, implying that meaningful uncertainty quantification can still be achieved with small calibration datasets.  
4. The authors challenge this assumption by demonstrating that while statistical guarantees remain valid for arbitrarily sized calibration sets, the practical effectiveness of these guarantees is strongly dependent on the calibration set size.  
5. This finding is particularly significant for medical applications where data scarcity often restricts the size of calibration sets, potentially limiting the usefulness of CP-based uncertainty estimates.  
6. Their critique is supported by empirical results from a medical image classification task, illustrating the practical limitations of CP with small calibration sets in a real-world healthcare context. <div>
arXiv:2512.14727v1 Announce Type: new 
Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems</title>
<link>https://arxiv.org/abs/2512.14728</link>
<guid>https://arxiv.org/abs/2512.14728</guid>
<content:encoded><![CDATA[
<div> Keywords: trajectory inference, urban rail transit, Automatic Fare Collection, data-driven, KL divergence

<br><br>Summary:  
This paper presents a fully data-driven method for inferring individual travel trajectories within urban rail transit systems by leveraging data from Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems. The approach focuses on identifying key trajectory elements, including the selected train, access and egress times, and transfer times. It begins by establishing sets of alternative trains based on spatio-temporal constraints, followed by adaptive trajectory inference and complete travel trajectory construction. A novel data-driven parameter estimation algorithm, KLEM, which combines KL divergence with the Expectation-Maximization (EM) algorithm, is introduced to eliminate dependency on external or survey datasets for parameter fitting. This advancement enhances the robustness and applicability of the model across different contexts. Unlike previous studies that mostly rely on synthetic data, this paper validates its results using real individual travel trajectory data, addressing a significant gap in verification practices. The experimental findings demonstrate that the proposed approach achieves over 90% accuracy in inferring urban rail transit travel trajectories during peak hours, indicating its potential effectiveness and precision for operational organization and transit management purposes. <div>
arXiv:2512.14728v1 Announce Type: new 
Abstract: Refined trajectory inference of urban rail transit is of great significance to the operation organization. In this paper, we develop a fully data-driven approach to inferring individual travel trajectories in urban rail transit systems. It utilizes data from the Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems to infer key trajectory elements, such as selected train, access/egress time, and transfer time. The approach includes establishing train alternative sets based on spatio-temporal constraints, data-driven adaptive trajectory inference, and trave l trajectory construction. To realize data-driven adaptive trajectory inference, a data-driven parameter estimation method based on KL divergence combined with EM algorithm (KLEM) was proposed. This method eliminates the reliance on external or survey data for parameter fitting, enhancing the robustness and applicability of the model. Furthermore, to overcome the limitations of using synthetic data to validate the result, this paper employs real individual travel trajectory data for verification. The results show that the approach developed in this paper can achieve high-precision passenger trajectory inference, with an accuracy rate of over 90% in urban rail transit travel trajectory inference during peak hours.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Geometry for policy-constrained interpretation</title>
<link>https://arxiv.org/abs/2512.14731</link>
<guid>https://arxiv.org/abs/2512.14731</guid>
<content:encoded><![CDATA[
<div> Keywords: geometric framework, policy constraints, semantic interpretation, hallucination prevention, financial regulation<br><br>Summary:<br><br>This paper introduces a novel geometric framework designed to enforce policy constraints on semantic interpretation processes, thereby preventing hallucinated commitments that could lead to errors in critical, high-stakes applications. Semantic meanings are geometrically represented as directions on a unit sphere, while evidence is captured as sets of witness vectors. The framework defines admissible interpretations as spherical convex regions, enabling a clear geometric delineation between possible semantic outcomes. Policy constraints are incorporated as explicit priors on the manifold, distinctly separated from the evidence geometry to maintain clarity and modularity. Interpretation is formulated as a constrained optimization problem over these admissible regions, where refusal to interpret naturally arises from topological conditions under contradictory evidence or policy exclusion. The framework is rigorously connected to established theoretical foundations such as information theory, Bayesian inference, and sheaf-theoretic semantics, achieving provably optimal complexity bounds from an information-theoretic perspective. Empirically, the approach is validated on large-scale, regulated financial datasets, demonstrating zero instances of hallucinated approvals across various policy regimes, marking a significant advance as the first scalable method to guarantee policy-compliant semantic interpretation without erroneous commitments in this domain. <div>
arXiv:2512.14731v1 Announce Type: new 
Abstract: We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT</title>
<link>https://arxiv.org/abs/2512.14732</link>
<guid>https://arxiv.org/abs/2512.14732</guid>
<content:encoded><![CDATA[
<div> Keywords: incidental findings, CT scans, large language models, vision-language models, automated detection<br><br>Summary:<br><br>1. The paper addresses the challenge of detecting and reporting incidental findings in abdominal CT scans, which are commonly benign but clinically important when identified correctly.<br><br>2. Traditional manual inspection by radiologists is time-consuming and inconsistently reliable, necessitating automated methods for efficiency and precision.<br><br>3. The authors propose a novel plan-and-execute framework combining large language models (LLMs) with foundational vision-language models (VLMs) to automate detection, classification, and reporting of incidental findings.<br><br>4. The framework includes a planner, powered by an LLM that generates Python scripts based on medical guidelines and predefined base functions, and an executor that runs these scripts using VLMs, segmentation models, and image processing tools.<br><br>5. Experimental evaluation on an abdominal CT benchmark targeting three organs demonstrates that this integrated approach achieves better accuracy and efficiency than existing VLM-only methods, enabling fully automatic end-to-end incidental findings management. <div>
arXiv:2512.14732v1 Announce Type: new 
Abstract: Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.
  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness</title>
<link>https://arxiv.org/abs/2512.14734</link>
<guid>https://arxiv.org/abs/2512.14734</guid>
<content:encoded><![CDATA[
<div> Keywords: recommender systems, intra-day personalization, long-form video streaming, user engagement, model-agnostic approach  

<br><br>Summary:  
1. Traditional recommender systems for long-form video streaming often rely on batch-trained models with user features updated once daily, creating stale recommendations throughout the day.  
2. The paper proposes a lightweight, model-agnostic method to inject recent watch history at inference time, enabling intra-day personalization without the need for model retraining.  
3. This method selectively overrides outdated user features using recent user actions, allowing the recommendation system to adapt quickly to changing user preferences.  
4. Implementation of this approach reduced the personalization feedback cycle from a daily update to intra-day updates, resulting in a statistically significant increase of 0.47% in key user engagement metrics.  
5. This work is the first to provide published evidence that intra-day personalization can produce meaningful improvements in user engagement within long-form video streaming, offering an efficient alternative to costly full real-time retraining architectures. <div>
arXiv:2512.14734v1 Announce Type: new 
Abstract: Many recommender systems in long-form video streaming reply on batch-trained models and batch-updated features, where user features are updated daily and served statically throughout the day. While efficient, this approach fails to incorporate a user's most recent actions, often resulting in stale recommendations. In this work, we present a lightweight, model-agnostic approach for intra-day personalization that selectively injects recent watch history at inference time without requiring model retraining. Our approach selectively overrides stale user features at inference time using the recent watch history, allowing the system to adapt instantly to evolving preferences. By reducing the personalization feedback loop from daily to intra-day, we observed a statistically significant 0.47% increase in key user engagement metrics which ranked among the most substantial engagement gains observed in recent experimentation cycles. To our knowledge, this is the first published evidence that intra-day personalization can drive meaningful impact in long-form video streaming service, providing a compelling alternative to full real-time architectures where model retraining is required.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NoveltyRank: Estimating Conceptual Novelty of AI Papers</title>
<link>https://arxiv.org/abs/2512.14738</link>
<guid>https://arxiv.org/abs/2512.14738</guid>
<content:encoded><![CDATA[
<div> Keywords: novelty estimation, AI research, semantic similarity, binary classification, pairwise comparison<br><br>Summary: This paper addresses the challenge of evaluating the conceptual novelty of AI research papers amidst a rapid increase in academic publications. It proposes a model to estimate and rank the novelty of papers by analyzing their titles, abstracts, and semantic similarity to existing literature. The goal is to provide a scalable, data-driven way to identify genuinely innovative work and assist conference reviewers with consistent novelty assessments. Two task formulations are explored: (1) binary classification, which predicts absolute novelty based on patterns learned from prior novel works, and (2) pairwise novelty comparison, which ranks papers relative to others in terms of novelty. The authors fine-tune two language models, Qwen3-4B-Instruct-2507 and SciBERT, under both formulations and benchmark their performance against GPT-5.1. This comparison highlights how different modeling choices impact novelty estimation effectiveness. The implementation of their approach is publicly accessible via GitHub, encouraging further research and practical application in managing the increasing volume of AI research publications. <div>
arXiv:2512.14738v1 Announce Type: new 
Abstract: With the growing ease of academic publishing, the volume of research papers, especially in AI-related fields, has surged dramatically. This flood of publications makes it difficult for truly novel and impactful work to stand out, and manual novelty assessment is often unstable and time-consuming. Our project aims to develop a model that estimates and ranks the conceptual novelty of AI papers, enabling a data-driven and scalable assessment of research originality. Such a system can help researchers efficiently identify submissions that introduce genuinely innovative ideas rather than minor variants, and provide conference reviewers with a quantitative and consistent signal of novelty. Our approach evaluates novelty primarily through a paper's title, abstract, and semantic similarity to prior literature. Given the motivation of novelty estimation, we explore two task formulations with different modeling objectives, each offering a different perspective: (1) binary classification, which predicts the paper's absolute novelty from learned patterns of prior novel works, and (2) pairwise novelty comparison, which learns to distinguish papers by relative novelty over others. We fine-tune Qwen3-4B-Instruct-2507 and SciBERT on both tasks, benchmarking against GPT-5.1 to analyze how task formulation and modeling choices affect performance. The implementation is publicly available at https://github.com/ZhengxuYan/NoveltyRank.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Discrete Diffusion for Constraint Satisfaction Problems</title>
<link>https://arxiv.org/abs/2512.14765</link>
<guid>https://arxiv.org/abs/2512.14765</guid>
<content:encoded><![CDATA[
<div> Keywords: discrete diffusion, constraint satisfaction problems, CSPs, Sudoku, unsupervised learning<br><br>Summary:  
1. The paper introduces a novel method called discrete diffusion guidance tailored for solving constraint satisfaction problems (CSPs).  
2. The approach leverages discrete diffusion processes to effectively navigate the solution space of CSPs, guiding the search towards feasible solutions.  
3. A key application demonstrated is the ability of this method to solve Sudoku puzzles, a classical NP-complete CSP, without any supervised training or labeled data.  
4. By utilizing unsupervised learning mechanisms, the method circumvents the need for handcrafted heuristics or extensive domain knowledge traditionally required in solving CSPs like Sudoku.  
5. Experimental results illustrate that discrete diffusion guidance can efficiently and accurately find valid Sudoku solutions, showcasing its potential as a general strategy for CSPs beyond Sudoku puzzles. <div>
arXiv:2512.14765v1 Announce Type: new 
Abstract: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Weather Forecasts from a Decision Maker's Perspective</title>
<link>https://arxiv.org/abs/2512.14779</link>
<guid>https://arxiv.org/abs/2512.14779</guid>
<content:encoded><![CDATA[
<div> Keywords: decision calibration, weather forecast evaluation, machine learning, numerical weather prediction, decision-making

<br><br>Summary: This paper critiques standard weather forecast evaluations for focusing primarily on the forecaster's perspective and statistical comparisons between forecasts and observations. It argues that since forecasts are ultimately employed to make decisions, evaluating forecasts from the decision-maker's perspective offers greater practical relevance. To this end, the study introduces and applies decision calibration—a framework that assesses forecast performance based on improvement in decision-making outcomes rather than traditional forecast accuracy metrics. The authors conduct experiments comparing machine learning-based models and classical numerical weather prediction models across various weather-dependent decision tasks. Their findings reveal a disconnect between forecast-level performance and utility in downstream decision-making; some differences only surface when viewed through decision calibration. They also observe that model rankings fluctuate depending on the specific decision task considered. The study concludes that conventional forecast evaluations are often insufficient for choosing the best forecasting model for particular decision applications, highlighting the need for decision-level evaluation approaches to better align forecasting with operational needs. <div>
arXiv:2512.14779v1 Announce Type: new 
Abstract: Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unreliable Uncertainty Estimates with Monte Carlo Dropout</title>
<link>https://arxiv.org/abs/2512.14851</link>
<guid>https://arxiv.org/abs/2512.14851</guid>
<content:encoded><![CDATA[
<div> Keywords: uncertainty estimation, Monte Carlo dropout, Bayesian inference, Gaussian Processes, Bayesian Neural Networks<br><br>Summary:<br><br>Reliable uncertainty estimation is essential for machine learning models, especially in safety-critical applications. Exact Bayesian inference provides a principled method for uncertainty quantification but is often computationally prohibitive for deep neural networks. Monte Carlo dropout (MCD) is proposed as an efficient approximation of Bayesian inference by applying neuron dropout during inference to generate multiple sub-model predictions. This research empirically evaluates MCD's effectiveness in capturing true uncertainty compared to Gaussian Processes (GP) and Bayesian Neural Networks (BNN). The findings reveal that MCD struggles to accurately represent uncertainty and fails notably in reflecting increased uncertainty in extrapolation and interpolation regions, where Bayesian models show expected rise in uncertainty. Consequently, the study suggests that uncertainty estimates produced by MCD are less reliable for capturing both epistemic (model) and aleatoric (data) uncertainty than those obtained from traditional Bayesian approaches. <div>
arXiv:2512.14851v1 Announce Type: new 
Abstract: Reliable uncertainty estimation is crucial for machine learning models, especially in safety-critical domains. While exact Bayesian inference offers a principled approach, it is often computationally infeasible for deep neural networks. Monte Carlo dropout (MCD) was proposed as an efficient approximation to Bayesian inference in deep learning by applying neuron dropout at inference time \citep{gal2016dropout}. Hence, the method generates multiple sub-models yielding a distribution of predictions to estimate uncertainty. We empirically investigate its ability to capture true uncertainty and compare to Gaussian Processes (GP) and Bayesian Neural Networks (BNN). We find that MCD struggles to accurately reflect the underlying true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions as observed in Bayesian models. The findings suggest that uncertainty estimates from MCD, as implemented and evaluated in these experiments, is not as reliable as those from traditional Bayesian approaches for capturing epistemic and aleatoric uncertainty.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal</title>
<link>https://arxiv.org/abs/2512.14873</link>
<guid>https://arxiv.org/abs/2512.14873</guid>
<content:encoded><![CDATA[
<div> Keywords: Fourier Analysis Network, sine activation, dying-ReLU problem, Dual-Activation Layer, neural network convergence<br><br>Summary:<br><br>1. Fourier Analysis Network (FAN) improves neural network performance by replacing parts of ReLU activations with sine and cosine functions, yet the exact reason behind the gains remained unclear.<br>2. The study finds that only the sine activation positively impacts performance, while cosine activation is often detrimental.<br>3. Improvement stems not from the sine function’s periodic nature but from its local behavior near zero, where the non-zero derivative mitigates the vanishing-gradient problem.<br>4. FAN primarily addresses the dying-ReLU problem, where neurons stuck with negative inputs produce zero gradients and cease learning.<br>5. Although modern ReLU variants like Leaky ReLU, GELU, and Swish reduce zero-gradient regions, they still suffer from diminished gradients that slow optimization.<br>6. FAN introduces a more stable gradient pathway improving training dynamics beyond the spectral interpretation.<br>7. Based on this analysis, the Dual-Activation Layer (DAL) is developed as a more efficient convergence accelerator.<br>8. Experiments on noisy sinusoidal signal classification, MNIST digit recognition, and ECG-based biometric recognition show DAL models converge faster and achieve equal or better validation accuracy than conventional activations. <div>
arXiv:2512.14873v1 Announce Type: new 
Abstract: Fourier Analysis Network (FAN) was recently proposed as a simple way to improve neural network performance by replacing part of ReLU activations with sine and cosine functions. Although several studies have reported small but consistent gains across tasks, the underlying mechanism behind these improvements has remained unclear. In this work, we show that only the sine activation contributes positively to performance, whereas the cosine activation tends to be detrimental. Our analysis reveals that the improvement is not a consequence of the sine function's periodic nature; instead, it stems from the function's local behavior near x = 0, where its non-zero derivative mitigates the vanishing-gradient problem. We further show that FAN primarily alleviates the dying-ReLU problem, in which a neuron consistently receives negative inputs, produces zero gradients, and stops learning. Although modern ReLU-like activations, such as Leaky ReLU, GELU, and Swish, reduce ReLU's zero-gradient region, they still contain input domains where gradients remain significantly diminished, contributing to slower optimization and hindering rapid convergence. FAN addresses this limitation by introducing a more stable gradient pathway. This analysis shifts the understanding of FAN's benefits from a spectral interpretation to a concrete analysis of training dynamics, leading to the development of the Dual-Activation Layer (DAL), a more efficient convergence accelerator. We evaluate DAL on three tasks: classification of noisy sinusoidal signals versus pure noise, MNIST digit classification, and ECG-based biometric recognition. In all cases, DAL models converge faster and achieve equal or higher validation accuracy compared to models with conventional activations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse</title>
<link>https://arxiv.org/abs/2512.14879</link>
<guid>https://arxiv.org/abs/2512.14879</guid>
<content:encoded><![CDATA[
<div> Keywords: self-referential learning, entropy collapse, Bregman projection, entropy reservoir, model stabilization<br><br>Summary:<br><br>1. The paper addresses the problem of model collapse in self-referential learning, where models trained on their own generated data deteriorate into low-entropy states such as repetitive text, mode dropping, or policy over-exploitation.<br><br>2. Existing fixes like mixing real data, entropy bonuses, knowledge distillation, or retrieval-augmented generation work but lack a unifying theoretical explanation.<br><br>3. The authors introduce the Entropy-Reservoir Bregman Projection (ERBP) framework, which models closed-loop learning as a stochastic Bregman projection sequence in distribution space and explains entropy decay as a shrinking empirical support caused by finite-sample noise.<br><br>4. The concept of an Entropy Reservoir—mixing a high-entropy distribution at each projection step—inserts entropy flux to stabilize training dynamics, preventing collapse.<br><br>5. Their theory derives necessary and sufficient conditions for collapse and stabilized entropy floors, along with closed-form rates dependent on sample size and mathematical properties of the Bregman generator.<br><br>6. Experimental validation covers large language model self-training, Soft Actor-Critic reinforcement learning, and GAN optimization, confirming that many heuristics correspond to specific choices of entropy reservoirs.<br><br>7. Overall, ERBP unifies disparate stabilization techniques into a single quantitative design principle: actively monitor and manage your entropy flux to prevent model collapse in self-referential training systems. <div>
arXiv:2512.14879v1 Announce Type: new 
Abstract: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task Matrices: Linear Maps for Cross-Model Finetuning Transfer</title>
<link>https://arxiv.org/abs/2512.14880</link>
<guid>https://arxiv.org/abs/2512.14880</guid>
<content:encoded><![CDATA[
<div> Keywords: task matrix, linear transformation, vision models, language models, finetuning<br><br>Summary:<br><br>1. This paper introduces the concept of a task matrix, defined as a linear transformation that maps embeddings from a base pretrained model to their finetuned counterparts. <br>2. The authors demonstrate that for both vision and text models, applying a task matrix enables the base model to achieve performance that exceeds traditional linear probes and sometimes nears that of fully finetuned models. <br>3. Their experiments span ten different datasets, confirming the broad applicability of the task matrix across multiple domains and architectures. <br>4. The work validates the presence of implicit cross-layer linear encodings connecting pretrained and finetuned model states, extending previous interpretability findings from in-context prompting scenarios to general adaptation regimes. <br>5. Additionally, they propose an efficient, data-driven method to approximate these task matrices, which is shown to be generalizable and practical. The authors have released their implementation publicly to facilitate further research and adoption. <div>
arXiv:2512.14880v1 Announce Type: new 
Abstract: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams</title>
<link>https://arxiv.org/abs/2512.14892</link>
<guid>https://arxiv.org/abs/2512.14892</guid>
<content:encoded><![CDATA[
<div> Keywords: Online learning, Weighted average, Multivariate regression, Concept drift, Convergence analysis<br><br>Summary:<br><br>1. The paper introduces OLR-WA (OnLine Regression with Weighted Average), a novel and versatile multivariate online linear regression model designed to update incrementally using new data, reducing storage demands and computational overhead.<br><br>2. OLR-WA is specifically crafted to handle scenarios involving drift, where data patterns evolve over time, including both time-based (temporal) and confidence-based drift situations.<br><br>3. A key innovation of OLR-WA is its conservative update mechanism that prioritizes older data points with higher confidence, enabling it to effectively manage challenging confidence-based scenarios, which other models struggle with.<br><br>4. Through convergence analysis and comparison with existing online regression models, OLR-WA demonstrates rapid convergence and high performance, consistently achieving strong r² values from the first iteration onward, even when initialized with as little as 1% to 10% of the total data.<br><br>5. Overall, OLR-WA matches or exceeds the performance of state-of-the-art online regression approaches and performs comparably to batch regression, affirming its efficacy and versatility as a robust solution for online linear regression tasks across diverse contexts. <div>
arXiv:2512.14892v1 Announce Type: new 
Abstract: Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce "OLR-WA; OnLine Regression with Weighted Average", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections</title>
<link>https://arxiv.org/abs/2512.14895</link>
<guid>https://arxiv.org/abs/2512.14895</guid>
<content:encoded><![CDATA[
<div> Keywords: imitation learning, covariate shift, on-policy expert corrections, multi-turn language models, software engineering tasks<br><br>Summary:  
This paper addresses the limitations of imitation learning for training multi-turn language model (LM) agents, specifically highlighting the problem of covariate shift where the student policy diverges from the expert behavior and encounters unfamiliar states. To overcome this, the authors propose a novel data generation method inspired by the DAgger algorithm called on-policy expert corrections (OECs). OECs involve generating partially on-policy data by starting rollouts with a student model and switching to an expert model mid-trajectory to provide corrections. The approach is evaluated in the domain of software engineering (SWE) tasks, where LM agents interact with a development environment to fix bugs over multiple turns. Experiments compare OEC data with other on-policy and imitation learning approaches using a unified training method that combines rejection sampling based on environment rewards and supervised fine-tuning. Results show that models trained with OEC trajectories achieve a significant relative improvement of 14% and 13% over traditional imitation learning benchmarks in 7 billion and 32 billion parameter models respectively on the SWE-bench verified dataset. The findings emphasize the importance of integrating expert demonstrations with on-policy data to effectively train multi-turn LM agents in complex interactive environments. <div>
arXiv:2512.14895v1 Announce Type: new 
Abstract: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs</title>
<link>https://arxiv.org/abs/2512.14908</link>
<guid>https://arxiv.org/abs/2512.14908</guid>
<content:encoded><![CDATA[
<div> Keywords: ATLAS, graph neural networks, heterophilic graphs, community detection, scalability<br><br>Summary:  
1. The paper introduces ATLAS (Adaptive Topology-based Learning at Scale), a novel graph learning algorithm designed to overcome key challenges in graph neural networks (GNNs), particularly in heterophilic graph scenarios and scalability issues.  
2. Traditional GNNs experience reduced accuracy on heterophilic graphs, where node neighbors tend to have different labels, and their iterative feature aggregation mechanisms often struggle to scale to large graphs.  
3. ATLAS addresses these challenges by extracting topological information through multi-level community detection, concatenating these community assignments to node feature vectors, and then processing them using multilayer perceptrons (MLPs).  
4. This approach provides rich topological context without relying on iterative neighborhood aggregation, enabling scalability to large graphs without the need for sampling, as MLPs are typically more scalable than GNNs.  
5. Experimentally, ATLAS matches or exceeds baseline methods, achieving improvements up to 20 percentage points over Graph Convolutional Networks (GCN) for heterophilic graphs and 11 percentage points over MLP for homophilic graphs.  
6. By leveraging multi-resolution community features, ATLAS not only improves accuracy but also offers a pathway to more explainable graph learning models that perform effectively across different graph structures. <div>
arXiv:2512.14908v1 Announce Type: new 
Abstract: We present ATLAS (Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs), a novel graph learning algorithm that addresses two important challenges in graph neural networks (GNNs). First, the accuracy of GNNs degrades when the graph is heterophilic. Second, iterative feature aggregation limits the scalability of GNNs to large graphs. We address these challenges by extracting topological information about graph communities at multiple levels of refinement, concatenating community assignments to the feature vector, and applying multilayer perceptrons (MLPs) to the resulting representation. This provides topological context about nodes and their neighborhoods without invoking aggregation. Because MLPs are typically more scalable than GNNs, our approach applies to large graphs without the need for sampling. Across a wide set of graphs, ATLAS achieves comparable accuracy to baseline methods, with gains as high as 20 percentage points over GCN for heterophilic graphs with negative structural bias and 11 percentage points over MLP for homophilic graphs. Furthermore, we show how multi-resolution community features systematically modulate performance in both homophilic and heterophilic settings, opening a principled path toward explainable graph learning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective</title>
<link>https://arxiv.org/abs/2512.14932</link>
<guid>https://arxiv.org/abs/2512.14932</guid>
<content:encoded><![CDATA[
<div> regularization parameter, low-rank MMSE filters, Kronecker-product representation, rank selection, simulations<br><br>Summary:<br><br>1. The article introduces a novel method to efficiently determine the regularization parameter in low-rank MMSE (Minimum Mean Square Error) filters by utilizing a Kronecker-product representation.<br><br>2. The authors reveal a surprising connection between the regularization parameter and the problem of rank selection, emphasizing that correct choice of this parameter is vital for achieving optimal performance in low-rank filtering scenarios.<br><br>3. The proposed approach is designed to enhance the effectiveness of low-rank MMSE filters by carefully tuning the regularization parameter according to the underlying rank structure.<br><br>4. Extensive simulations are conducted to validate the method, demonstrating its advantages and significant performance improvements over traditional, commonly used techniques for regularization parameter selection.<br><br>5. Overall, the study contributes a practical and theoretically insightful solution that addresses one of the key challenges in low-rank MMSE filtering, potentially impacting a range of signal processing applications. <div>
arXiv:2512.14932v1 Announce Type: new 
Abstract: In this work, we propose a method to efficiently find the regularization parameter for low-rank MMSE filters based on a Kronecker-product representation. We show that the regularization parameter is surprisingly linked to the problem of rank selection and, thus, properly choosing it, is crucial for low-rank settings. The proposed method is validated through simulations, showing significant gains over commonly used methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise</title>
<link>https://arxiv.org/abs/2512.14967</link>
<guid>https://arxiv.org/abs/2512.14967</guid>
<content:encoded><![CDATA[
<div> Keywords: McKean-Vlasov equations, elicitability, deep learning, mean-field games, systemic risk<br><br>Summary:<br><br>1. The paper introduces a new numerical method to solve McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs) with common noise by integrating Picard iterations, the concept of elicitability, and deep learning techniques. <br>2. A significant advancement is the use of elicitability to formulate a path-wise loss function that enables efficient neural network training for approximating the backward process and the conditional expectations associated with common noise, avoiding costly nested Monte Carlo simulations. <br>3. The mean-field interaction term is modeled with a recurrent neural network optimized by minimizing an elicitable score, while the backward process is approximated through a feedforward network representing the decoupling field. <br>4. The methodology is validated on a systemic risk inter-bank borrowing and lending model where analytic solutions exist, showing high accuracy in solution recovery. The framework is further extended to handle quantile-mediated interactions, demonstrating flexibility beyond conditional means or moments. <br>5. Finally, the approach is applied to a non-stationary Aiyagari–Bewley–Huggett economic growth model featuring endogenous interest rates, highlighting the method’s applicability to complex mean-field games lacking closed-form solutions. <div>
arXiv:2512.14967v1 Announce Type: new 
Abstract: We present a novel numerical method for solving McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs) with common noise, combining Picard iterations, elicitability and deep learning. The key innovation involves elicitability to derive a path-wise loss function, enabling efficient training of neural networks to approximate both the backward process and the conditional expectations arising from common noise - without requiring computationally expensive nested Monte Carlo simulations. The mean-field interaction term is parameterized via a recurrent neural network trained to minimize an elicitable score, while the backward process is approximated through a feedforward network representing the decoupling field. We validate the algorithm on a systemic risk inter-bank borrowing and lending model, where analytical solutions exist, demonstrating accurate recovery of the true solution. We further extend the model to quantile-mediated interactions, showcasing the flexibility of the elicitability framework beyond conditional means or moments. Finally, we apply the method to a non-stationary Aiyagari--Bewley--Huggett economic growth model with endogenous interest rates, illustrating its applicability to complex mean-field games without closed-form solutions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Softly Constrained Denoisers for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.14980</link>
<guid>https://arxiv.org/abs/2512.14980</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, constraints, denoiser adjustment, scientific data, inductive bias  

<br><br>Summary:  
This paper addresses the challenge diffusion models face in generating samples that adhere to specific constraints, which is particularly important in scientific applications. 1) Existing methods incorporate constraints by adding regularization terms to the loss function or using guidance strategies during sampling; however, these approaches can lead to biased generative models that deviate from the true data distribution. 2) This bias is especially problematic when constraints are misspecified, a frequent issue in real-world scientific data scenarios. 3) Instead of modifying the loss or the sampling process, the authors propose embedding a guidance-inspired adjustment directly within the denoiser component of diffusion models. 4) This modification acts as a soft inductive bias that encourages the generation of constraint-compliant samples without rigidly enforcing the constraints. 5) Experiments demonstrate that the softly constrained denoisers better exploit prior constraint knowledge to improve adherence compared to standard denoisers while retaining sufficient flexibility to accommodate discrepancies caused by misspecified constraints in observed data. This approach balances constraint compliance with model adaptability, enhancing the practical applicability of diffusion models in scientific domains. <div>
arXiv:2512.14980v1 Announce Type: new 
Abstract: Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Repetition Improves Non-Reasoning LLMs</title>
<link>https://arxiv.org/abs/2512.14982</link>
<guid>https://arxiv.org/abs/2512.14982</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt repetition, performance improvement, reasoning, language models, inference efficiency  

<br><br>Summary:  
1. The article investigates the effect of repeating the input prompt on the performance of popular language models such as Gemini, GPT, Claude, and Deepseek.  
2. It demonstrates that repeating the prompt enhances these models’ performance even when reasoning processes are not employed.  
3. Importantly, this performance boost occurs without an increase in the number of generated tokens.  
4. Moreover, repeating the input does not lead to higher latency, meaning the inference speed remains unaffected.  
5. These findings suggest a simple and efficient method to improve model output quality during inference without additional computational cost or delay. <div>
arXiv:2512.14982v1 Announce Type: new 
Abstract: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes</title>
<link>https://arxiv.org/abs/2512.14991</link>
<guid>https://arxiv.org/abs/2512.14991</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, controlled diffusion processes, adaptive partitioning, regret bounds, high-dimensional finance<br><br>Summary:<br><br>1. The article addresses reinforcement learning in settings with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards, commonly encountered in finance, economics, and operations research.<br><br>2. A novel model-based algorithm is proposed that adaptively partitions the joint state-action space to manage complexity in continuous and high-dimensional domains.<br><br>3. This algorithm keeps estimators for drift, volatility, and rewards in each partition and refines partitions when the bias surpasses statistical confidence, effectively balancing exploration and approximation.<br><br>4. Theoretical contributions include regret bounds dependent on the horizon, state dimension, reward growth, and a newly introduced zooming dimension concept adapted for unbounded diffusion processes.<br><br>5. The results extend known bounds from bounded settings to a wider range of diffusion problems, and numerical experiments demonstrate the algorithm’s efficacy, including its application to challenging tasks like multi-asset mean-variance portfolio selection. <div>
arXiv:2512.14991v1 Announce Type: new 
Abstract: We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding</title>
<link>https://arxiv.org/abs/2512.15000</link>
<guid>https://arxiv.org/abs/2512.15000</guid>
<content:encoded><![CDATA[
<div> Process Reward Models, Large Language Models, Code Generation, Meta-learning, Chain-of-Function  

<br><br>Summary:  
This paper addresses the limitations of Process Reward Models (PRMs) in coding tasks, where step decompositions are less meaningful and labels generated via Monte-Carlo methods tend to be noisy. The authors introduce DreamPRM-Code, a specialized PRM for coding that conceptualizes functions as reasoning steps by employing a Chain-of-Function prompting strategy. This approach encourages modular code generation, enabling the PRM framework to be applied similarly to mathematical reasoning problems. To tackle noisy intermediate labels, DreamPRM-Code incorporates a meta-learning-based correction mechanism. This mechanism uses clean final unit-test results as reliable labels to perform bi-level optimization, which refines and improves the quality of noisy intermediate labels. The proposed model is applied in test-time scaling scenarios to effectively enhance Large Language Model coding performance. Experiments on LiveCodeBench demonstrate that DreamPRM-Code achieves a state-of-the-art pass@1 rate of 80.9%, outperforming the OpenAI o4-mini baseline. This shows the viability and effectiveness of treating coding functions as reasoning steps combined with a meta-learning strategy for label noise correction. The work provides a significant advancement in improving coding ability of LLMs via PRMs with modular, step-wise training and test-time scaling. <div>
arXiv:2512.15000v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets</title>
<link>https://arxiv.org/abs/2512.15008</link>
<guid>https://arxiv.org/abs/2512.15008</guid>
<content:encoded><![CDATA[
<div> Keywords: Stock Pattern Assistant, price structure, deterministic framework, event alignment, explainability  

<br><br>Summary:  
1. The article introduces the Stock Pattern Assistant (SPA), a deterministic framework designed to analyze price movements in financial markets by extracting monotonic price runs and linking them with relevant public events.  
2. SPA uses daily OHLCV (Open, High, Low, Close, Volume) data alongside a normalized event stream, ensuring the methodology is easy to audit, reproduce, and transparent.  
3. Unlike traditional technical indicators or predictive models, SPA focuses on interpretability and factual explanation rather than forecasting or generating trading signals, filling the gap in tools that lack transparency and auditability.  
4. The framework applies a symmetric correlation window to attach public events to price movements, providing clear, historically grounded narratives that help explain price structure over time.  
5. The study evaluates SPA on four equities (AAPL, NVDA, SCHW, and PGR) representing diverse volatility profiles and sectors, demonstrating consistent structural decomposition and contextual storytelling.  
6. Ablation experiments highlight that deterministic segmentation, event alignment, and constrained explanation mechanisms are crucial for SPA’s interpretability.  
7. SPA is intended to complement analyst workflows, risk assessments, and explainable AI efforts rather than serve as a prediction or trading system, promoting transparency in understanding historical price behavior. <div>
arXiv:2512.15008v1 Announce Type: new 
Abstract: Understanding how prices evolve over time often requires peeling back the layers of market noise to identify clear, structural behavior. Many of the tools commonly used for this purpose technical indicators, chart heuristics, or even sophisticated predictive models leave important questions unanswered. Technical indicators depend on platform-specific rules, and predictive systems typically offer little in terms of explanation. In settings that demand transparency or auditability, this poses a significant challenge. We introduce the Stock Pattern Assistant (SPA), a deterministic framework designed to extract monotonic price runs, attach relevant public events through a symmetric correlation window, and generate explanations that are factual, historical, and guardrailed. SPA relies only on daily OHLCV data and a normalized event stream, making the pipeline straight-forward to audit and easy to reproduce. To illustrate SPA's behavior in practice, we evaluate it across four equities-AAPL, NVDA, SCHW, and PGR-chosen to span a range of volatility regimes and sector characteristics. Although the evaluation period is modest, the results demonstrate how SPA consistently produces stable structural decompositions and contextual narratives. Ablation experiments further show how deterministic segmentation, event alignment, and constrained explanation each contribute to interpretability. SPA is not a forecasting system, nor is it intended to produce trading signals. Its value lies in offering a transparent, reproducible view of historical price structure that can complement analyst workflows, risk reviews, and broader explainable-AI pipelines.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic diversity across language models mitigates knowledge collapse</title>
<link>https://arxiv.org/abs/2512.15011</link>
<guid>https://arxiv.org/abs/2512.15011</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, knowledge collapse, epistemic diversity, AI ecosystem, model performance<br><br>Summary: The article investigates the phenomenon of knowledge collapse in artificial intelligence (AI), where repeated training on AI-generated outputs results in a loss of information diversity, leading to performance degradation. Building on prior studies of single-model collapse, the authors explore how diversity across multiple AI models—referred to as an AI ecosystem—affects this decline. They segment training data among various language models and evaluate these ecosystems through ten iterations of self-training on their collective outputs. Their findings reveal that increasing epistemic diversity mitigates knowledge collapse but only up to an optimal point. Ecosystems with too few diverse models fail to capture the full complexity of the true data distribution, causing rapid performance decay over time. Conversely, distributing data across too many models diminishes each model's capacity to approximate the true distribution effectively, resulting in poor initial performance. The study highlights the importance of balancing diversity within AI ecosystems to sustain model performance and knowledge richness. Finally, the authors emphasize the risk of AI monoculture and advocate for monitoring diversity among AI systems, recommending policies that encourage the development of more domain- and community-specific AI models to maintain a healthy and robust AI ecosystem. <div>
arXiv:2512.15011v1 Announce Type: new 
Abstract: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Representation-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.15036</link>
<guid>https://arxiv.org/abs/2512.15036</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, spectral representations, transition operator, policy optimization, DeepMind Control Suite<br><br>Summary:  
This paper addresses challenges in reinforcement learning (RL) encountered in environments with large state and action spaces by introducing spectral representations derived from the spectral decomposition of the transition operator. These representations offer a theoretically grounded abstraction of system dynamics, which can improve policy optimization. The authors develop methods to construct spectral representations tailored to transition operators exhibiting latent variable or energy-based structures, highlighting distinct learning techniques for extracting these representations from data. Each method corresponds to an effective RL algorithm within the proposed spectral framework. Importantly, the framework is extended with provable guarantees to handle partially observable Markov decision processes (POMDPs), broadening its applicability. The effectiveness of these spectral RL algorithms is empirically validated on over 20 challenging tasks from the DeepMind Control Suite, where they demonstrate performance that is comparable to or better than existing model-free and model-based state-of-the-art baselines. This work not only offers a novel theoretical perspective to better understand and address instability and exploration issues in RL but also presents practical algorithms that leverage spectral properties for improved learning and generalization. <div>
arXiv:2512.15036v1 Announce Type: new 
Abstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks</title>
<link>https://arxiv.org/abs/2512.15067</link>
<guid>https://arxiv.org/abs/2512.15067</guid>
<content:encoded><![CDATA[
<div> Keywords: EMFusion, electromagnetic field forecasting, multivariate diffusion, uncertainty quantification, residual U-Net<br><br>Summary:  
1. The paper addresses the growing need for accurate estimation and forecasting of electromagnetic field (EMF) levels due to the expansion of wireless infrastructure.  
2. Current studies mainly employ univariate forecasting on aggregate EMF data, but capturing frequency-selective and inter-operator variations requires a multivariate approach.  
3. EMFusion, the proposed framework, utilizes a conditional multivariate diffusion-based probabilistic forecasting model that incorporates contextual factors like time of day, season, and holidays.  
4. The architecture is built on a residual U-Net enhanced with a cross-attention mechanism, allowing dynamic integration of external conditions to guide predictions.  
5. An imputation-based sampling strategy is used to treat the forecasting task as structural inpainting, maintaining temporal coherence even with irregular data.  
6. Unlike standard single-point forecasts, EMFusion produces calibrated probabilistic intervals from the learned conditional distribution, enabling explicit uncertainty quantification.  
7. Experiments on frequency-selective EMF datasets demonstrate that EMFusion, especially when incorporating working hours as contextual information, outperforms baseline models.  
8. The model improves continuous ranked probability score (CRPS) by 23.85%, normalized root mean square error by 13.93%, and reduces CRPS prediction error by 22.47%, showing superior accuracy and reliability for network planning and health assessment purposes. <div>
arXiv:2512.15067v1 Announce Type: new 
Abstract: The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</title>
<link>https://arxiv.org/abs/2512.15068</link>
<guid>https://arxiv.org/abs/2512.15068</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Hallucination Detection, Conformal Prediction, Embedding-based Methods, Large Language Models<br><br>Summary: This paper investigates the challenge of detecting hallucinations in Retrieval-Augmented Generation (RAG) systems, which often produce factually incorrect outputs despite being grounded in retrieved evidence. Current detection techniques commonly rely on semantic similarity measures and natural language inference (NLI), but these methods have not been rigorously evaluated for their fundamental limitations. The authors introduce conformal prediction to hallucination detection, offering finite-sample coverage guarantees that enable precise assessment of detection performance. With calibration sets of around 600 examples, they demonstrate 94% coverage and 0% false positive rate (FPR) on synthetic hallucinations using the Natural Questions dataset. However, when tested on three real-world hallucination benchmarks involving multiple prominent large language models (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based detection methods, including top-tier models like OpenAI text-embedding-3-large and cross-encoder architectures, show very high false positive rates—100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. In contrast, GPT-4 used as a reasoning judge achieves a much lower FPR of 7% (95% CI: [3.4%, 13.7%]) on the same datasets. The authors identify a "semantic illusion," whereby hallucinated texts remain semantically close to source documents yet contain factual errors undetectable by embeddings. This phenomenon, consistent across embedding designs, LLM generators, and task types, suggests that embedding-based detection methods are insufficient for reliable deployment of RAG systems in production. <div>
arXiv:2512.15068v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks</title>
<link>https://arxiv.org/abs/2512.15082</link>
<guid>https://arxiv.org/abs/2512.15082</guid>
<content:encoded><![CDATA[
<div> Feature Engineering, Multi-Label Learning, Large Language Models, Code Generation, Feedback Mechanism<br><br>Summary:<br><br>This paper addresses the limitations of existing feature engineering methods based on large language models (LLMs) in the context of multi-label learning, where complex label dependencies are common and task characteristics demand specialized approaches. The authors propose FEAML (Feature Engineering Automation for Multi-Label Learning), a novel automated feature engineering framework specifically designed for multi-label classification tasks. FEAML leverages the code generation capabilities of LLMs to produce high-quality features by incorporating metadata and label co-occurrence matrices, enabling the model to better understand relationships between data features and task objectives. The effectiveness of the generated features is evaluated using model accuracy, while Pearson correlation coefficients are employed to detect feature redundancy. A key innovation of FEAML is its feedback mechanism, which uses evaluation results to iteratively optimize the LLM’s code generation process, fostering continuous self-improvement. This integration of LLMs with a feedback-driven loop facilitates an efficient and interpretable feature engineering pipeline tailored for multi-label problems. Empirical evaluations on diverse multi-label datasets demonstrate that FEAML outperforms existing feature engineering methods, validating its effectiveness and efficiency for complex multi-label learning scenarios. <div>
arXiv:2512.15082v1 Announce Type: new 
Abstract: Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Modular Physics for Elastic Simulation</title>
<link>https://arxiv.org/abs/2512.15083</link>
<guid>https://arxiv.org/abs/2512.15083</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Modular Physics, elastic simulation, physical interpretability, modular neural networks, generalization

<br><br>Summary:  
This paper introduces Neural Modular Physics (NMP), a novel approach for elastic simulation that integrates the learning capability of neural networks with the reliability and interpretability of classical physics simulators. Unlike traditional end-to-end neural models that treat dynamics as a monolithic problem, NMP decomposes elastic dynamics into distinct neural modules corresponding to physically meaningful intermediate quantities. This modular design allows for direct supervision at intermediate stages and the enforcement of physical constraints, leading to improved physical consistency. The authors design a specialized architecture and training method that effectively transform the numerical computation process into a modular neural simulation framework. Experimentally, NMP shows superior generalization abilities when tested on unseen initial conditions and different spatial resolutions, addressing limitations seen in other neural simulators. It also achieves long-term stable simulations and better preservation of core physical properties. Moreover, NMP demonstrates greater flexibility and practicality in scenarios where the underlying physical dynamics are unknown, an area where conventional simulators often struggle. Overall, NMP represents a significant step forward in combining neural approximation with physical fidelity for improved elastic dynamics simulation. <div>
arXiv:2512.15083v1 Announce Type: new 
Abstract: Learning-based methods have made significant progress in physics simulation, typically approximating dynamics with a monolithic end-to-end optimized neural network. Although these models offer an effective way to simulation, they may lose essential features compared to traditional numerical simulators, such as physical interpretability and reliability. Drawing inspiration from classical simulators that operate in a modular fashion, this paper presents Neural Modular Physics (NMP) for elastic simulation, which combines the approximation capacity of neural networks with the physical reliability of traditional simulators. Beyond the previous monolithic learning paradigm, NMP enables direct supervision of intermediate quantities and physical constraints by decomposing elastic dynamics into physically meaningful neural modules connected through intermediate physical quantities. With a specialized architecture and training strategy, our method transforms the numerical computation flow into a modular neural simulator, achieving improved physical consistency and generalizability. Experimentally, NMP demonstrates superior generalization to unseen initial conditions and resolutions, stable long-horizon simulation, better preservation of physical properties compared to other neural simulators, and greater feasibility in scenarios with unknown underlying dynamics than traditional simulators.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network</title>
<link>https://arxiv.org/abs/2512.15086</link>
<guid>https://arxiv.org/abs/2512.15086</guid>
<content:encoded><![CDATA[
<div> Keywords: Operator learning, DeepONet, Partition-of-unity, PDEs, Physics-informed regularization<br><br>Summary:<br><br>1. The paper addresses the challenge of improving operator learning methods, which are used for efficiently predicting solutions of parameterized partial differential equations (PDEs) by learning mappings from inputs like initial conditions or forcing functions to full spatiotemporal fields. 2. Existing state-of-the-art architectures such as DeepONet and Fourier Neural Operator (FNO) demonstrate strong empirical results but often demand large training datasets, lack explicit incorporation of physical structure, and may experience instability issues in trunk-network features, notably mode imbalance or collapse, which degrade operator approximation accuracy. 3. Inspired by the stability and locality properties of classical partition-of-unity (PoU) methods, the authors propose PoU-based regularization techniques to enhance operator learning frameworks. 4. They develop an improved version of the PoU-PI-DeepONet framework, named Physics-informed Partition Penalty Deep Operator Network (PIP² Net), which introduces a simplified, more principled partition penalty. This penalty encourages better coordination among trunk outputs, increasing expressiveness without losing DeepONet’s flexibility. 5. The performance of PIP² Net is demonstrated on three nonlinear PDEs—the viscous Burgers equation, the Allen–Cahn equation, and a diffusion–reaction system—where it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in both prediction accuracy and robustness, highlighting its practical benefits for stable, efficient operator learning in complex PDE scenarios. <div>
arXiv:2512.15086v1 Announce Type: new 
Abstract: Operator learning has become a powerful tool for accelerating the solution of parameterized partial differential equations (PDEs), enabling rapid prediction of full spatiotemporal fields for new initial conditions or forcing functions. Existing architectures such as DeepONet and the Fourier Neural Operator (FNO) show strong empirical performance but often require large training datasets, lack explicit physical structure, and may suffer from instability in their trunk-network features, where mode imbalance or collapse can hinder accurate operator approximation. Motivated by the stability and locality of classical partition-of-unity (PoU) methods, we investigate PoU-based regularization techniques for operator learning and develop a revised formulation of the existing POU--PI--DeepONet framework. The resulting \emph{P}hysics-\emph{i}nformed \emph{P}artition \emph{P}enalty Deep Operator Network (PIP$^{2}$ Net) introduces a simplified and more principled partition penalty that improved the coordinated trunk outputs that leads to more expressiveness without sacrificing the flexibility of DeepONet. We evaluate PIP$^{2}$ Net on three nonlinear PDEs: the viscous Burgers equation, the Allen--Cahn equation, and a diffusion--reaction system. The results show that it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs</title>
<link>https://arxiv.org/abs/2512.15088</link>
<guid>https://arxiv.org/abs/2512.15088</guid>
<content:encoded><![CDATA[
<div> Keywords: Stochastic Differential Equations, fractional Brownian motion, Signature Multi-head Attention, parameter estimation, deep learning  

<br><br>Summary:  
This paper addresses the challenge of parameter inference in stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm), which exhibit rough dynamics and long-range dependence. Classical estimation methods struggle due to the non-Markovian and non-semimartingale nature of these processes. The authors propose SigMA (Signature Multi-head Attention), a novel neural network architecture that integrates path signatures with multi-head self-attention. SigMA also uses convolutional preprocessing and a multilayer perceptron for effective feature extraction. The model is trained on synthetic data generated from fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, focusing on estimating the Hurst parameter and handling joint multi-parameter inference. SigMA shows strong generalization to unseen data. Extensive experiments on synthetic and two real-world datasets—equity-index realized volatility and Li-ion battery degradation—demonstrate SigMA's superiority over CNNs, LSTMs, vanilla Transformers, and Deep Signature models in terms of accuracy, robustness, and compactness. The results highlight that combining signature transforms with attention-based architectures forms an effective, scalable framework for parameter estimation in complex stochastic systems with persistent or rough temporal dependencies. <div>
arXiv:2512.15088v1 Announce Type: new 
Abstract: Stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm) are increasingly used to model systems with rough dynamics and long-range dependence, such as those arising in quantitative finance and reliability engineering. However, these processes are non-Markovian and lack a semimartingale structure, rendering many classical parameter estimation techniques inapplicable or computationally intractable beyond very specific cases. This work investigates two central questions: (i) whether integrating path signatures into deep learning architectures can improve the trade-off between estimation accuracy and model complexity, and (ii) what constitutes an effective architecture for leveraging signatures as feature maps. We introduce SigMA (Signature Multi-head Attention), a neural architecture that integrates path signatures with multi-head self-attention, supported by a convolutional preprocessing layer and a multilayer perceptron for effective feature encoding. SigMA learns model parameters from synthetically generated paths of fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, with a particular focus on estimating the Hurst parameter and on joint multi-parameter inference, and it generalizes robustly to unseen trajectories. Extensive experiments on synthetic data and two real-world datasets (i.e., equity-index realized volatility and Li-ion battery degradation) show that SigMA consistently outperforms CNN, LSTM, vanilla Transformer, and Deep Signature baselines in accuracy, robustness, and model compactness. These results demonstrate that combining signature transforms with attention-based architectures provides an effective and scalable framework for parameter inference in stochastic systems with rough or persistent temporal structure.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption</title>
<link>https://arxiv.org/abs/2512.15112</link>
<guid>https://arxiv.org/abs/2512.15112</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised node representation learning, graph convolution, homophily, adaptive convolution usage, clustering proxies  

<br><br>Summary:  
Unsupervised node representation learning aims to generate meaningful node embeddings without the use of node labels, typically utilizing graph convolution to aggregate information from neighboring nodes and encode features and topology. However, relying heavily on graph convolution can be detrimental, particularly in non-homophilic graphs, as it may cause embeddings of nodes with different features or topology to become overly similar. While adaptive adjustment of graph convolution has been studied in supervised settings, it remains underexplored in unsupervised learning. The authors propose FUEL, a method that adaptively learns the optimal degree of graph convolution by enhancing intra-class similarity and inter-class separability in the embedding space. Since true class labels are unavailable, FUEL identifies node clusters based on features and uses these clusters as proxies for classes. Extensive experiments comparing FUEL against 15 baseline methods across 14 benchmark datasets show that FUEL consistently improves performance in downstream tasks. Furthermore, FUEL achieves state-of-the-art results on graphs exhibiting various levels of homophily, demonstrating its robustness and wide applicability in unsupervised node representation learning scenarios. <div>
arXiv:2512.15112v1 Announce Type: new 
Abstract: Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models</title>
<link>https://arxiv.org/abs/2512.15115</link>
<guid>https://arxiv.org/abs/2512.15115</guid>
<content:encoded><![CDATA[
<div> Keywords: sequence modeling, interaction operator, factorized framework, state space models, gradient propagation  

<br><br>Summary:  
This paper introduces a unified theoretical framework for understanding sequence modeling architectures, including recurrent neural networks, Transformers, and state space models (SSMs). The framework represents sequence maps through an input-dependent interaction operator \( W_{ij}(X) \), highlighting two key construction patterns: (i) the Unified Factorized Framework (explicit attention-style mixing) where \( W_{ij}(X) \) changes via scalar coefficients on shared value maps, and (ii) Structured Dynamics (implicit state-space recurrences) where \( W_{ij} \) is generated by latent dynamical systems. Three main theoretical contributions are derived: first, the Interaction Rank Gap theorem which shows that models in the factorized class (like single-head attention) have a limited low-dimensional operator span and thus cannot express certain structured dynamical maps; second, the Equivalence (Head-Count) Theorem demonstrating that representing a linear SSM with lag operators spanning a \( k \)-dimensional subspace over sequences of length \( n \) requires exactly \( H = k \) attention heads; third, the Gradient Highway Result proving that attention mechanisms support gradient pathways that do not diminish with distance, unlike stable linear dynamics where gradient signals attenuate over time. Collectively, these results illuminate a fundamental trade-off between algebraic expressivity and trainability (gradient propagation), providing a rigorous foundation for the design and understanding of modern sequence models. <div>
arXiv:2512.15115v1 Announce Type: new 
Abstract: Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation</title>
<link>https://arxiv.org/abs/2512.15116</link>
<guid>https://arxiv.org/abs/2512.15116</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate time series imputation, diffusion model, frequency bias, Fourier Bias Projection, temporal modeling  

<br><br>Summary:  
Multivariate time series imputation is crucial in fields like healthcare, traffic forecasting, and biological modeling, where data often suffers from missing values due to sensor failures and irregular sampling. Existing methods, particularly Transformer- and diffusion-based models, struggle with explicit inductive biases and frequency awareness, which limits their ability to generalize under structured missing data and distribution shifts. To address this, the authors propose FADTI, a novel diffusion-based imputation framework that incorporates frequency-informed feature modulation using a learnable Fourier Bias Projection (FBP) module. The FBP module supports multiple spectral bases, allowing adaptive encoding of both stationary and non-stationary time series patterns, thus introducing frequency-domain inductive bias into the imputation process. FADTI also integrates temporal modeling mechanisms, leveraging self-attention and gated convolution for enhanced performance. Experiments are conducted across multiple benchmark datasets, including a newly introduced biological time series dataset, demonstrating that FADTI consistently outperforms current state-of-the-art approaches. Its advantage is especially notable under high missing data rates, showcasing robustness in challenging scenarios. The authors have made their code publicly available, enabling reproducibility and further research on frequency-aware diffusion models for time series imputation. <div>
arXiv:2512.15116v1 Announce Type: new 
Abstract: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Reward Shaping from Multi-Objective Human Heuristics</title>
<link>https://arxiv.org/abs/2512.15120</link>
<guid>https://arxiv.org/abs/2512.15120</guid>
<content:encoded><![CDATA[
<div> Multi-Objective, Reward Shaping, Exploration, Bi-level Optimization, Reinforcement Learning<br><br>Summary:<br><br>1. The paper addresses the challenge of designing effective reward functions in reinforcement learning, particularly in environments with multiple objectives. 2. It introduces MORSE (Multi-Objective Reward Shaping with Exploration), a novel framework that automatically integrates several human-designed heuristic rewards into a single unified reward function. 3. MORSE approaches reward shaping as a bi-level optimization problem where the inner loop focuses on training a policy to maximize the current shaped reward, and the outer loop updates the reward function to enhance overall task performance. 4. To promote exploration and avoid getting stuck in suboptimal local minima, MORSE incorporates stochasticity by injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. 5. Experimental evaluations conducted in MuJoCo and Isaac Sim environments demonstrate that MORSE successfully balances competing objectives across diverse robotic tasks, delivering task performance comparable to that achieved by manually tuned reward functions. <div>
arXiv:2512.15120v1 Announce Type: new 
Abstract: Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training</title>
<link>https://arxiv.org/abs/2512.15123</link>
<guid>https://arxiv.org/abs/2512.15123</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, adversarial training, privacy-preserving, proxy dataset, edge devices  

<br><br>Summary:  
1. The paper addresses the challenge of enhancing adversarial robustness for deep learning models deployed on edge devices in Federated Learning (FL) environments.  
2. Traditional adversarial training methods are difficult to implement in FL due to strict client data privacy and limited computational resources on edge devices.  
3. The authors propose TrajSyn, a novel framework that synthesizes a proxy dataset on the server side using trajectories of client model updates, enabling adversarial training without direct access to raw client data.  
4. TrajSyn is privacy-preserving, as it does not expose any sensitive client data and requires no additional computational burden on the clients.  
5. Experimental results demonstrate that TrajSyn consistently improves adversarial robustness on standard image classification benchmarks while maintaining the privacy and efficiency constraints critical in FL settings. <div>
arXiv:2512.15123v1 Announce Type: new 
Abstract: Deep learning models deployed on edge devices are increasingly used in safety-critical applications. However, their vulnerability to adversarial perturbations poses significant risks, especially in Federated Learning (FL) settings where identical models are distributed across thousands of clients. While adversarial training is a strong defense, it is difficult to apply in FL due to strict client-data privacy constraints and the limited compute available on edge devices. In this work, we introduce TrajSyn, a privacy-preserving framework that enables effective server-side adversarial training by synthesizing a proxy dataset from the trajectories of client model updates, without accessing raw client data. We show that TrajSyn consistently improves adversarial robustness on image classification benchmarks with no extra compute burden on the client device.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?</title>
<link>https://arxiv.org/abs/2512.15134</link>
<guid>https://arxiv.org/abs/2512.15134</guid>
<content:encoded><![CDATA[
<div> Keywords: interpretability, disentanglement, sparse autoencoders, concept representations, compositional evaluation

<br><br>Summary:  
This study addresses the challenge of recovering representations of causally relevant concepts from neural network activations, with a focus on interpretability. It critiques common concept representation evaluations as relying on isolation and independence assumptions that often do not hold in practical settings involving correlated concepts. To address this, the authors propose a multi-concept evaluation framework where correlations between textual concepts like sentiment, domain, and tense are controlled and progressively increased. They evaluate how different featurization methods, including sparse autoencoders (SAEs) and sparse probes, perform under varying correlation strengths with regard to learning disentangled concept representations. Results reveal a one-to-many relationship: each feature tends to correspond to a single concept, but individual concepts are distributed across multiple features. Steering experiments show that SAE features, even when trained on uniformly distributed concepts, generally influence multiple concepts upon manipulation, indicating limited selectivity and independence, though the affected features tend to occupy disjoint subspaces. The findings highlight that commonly used correlational metrics do not reliably indicate true independence or concept selectivity when features are steered. Overall, the work stresses the necessity of compositional and multi-concept evaluations to better understand and improve interpretability methods for neural network concept representations. <div>
arXiv:2512.15134v1 Announce Type: new 
Abstract: A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany</title>
<link>https://arxiv.org/abs/2512.15140</link>
<guid>https://arxiv.org/abs/2512.15140</guid>
<content:encoded><![CDATA[
<div> Keywords: generalization, crop yield prediction, machine learning, interpretability, temporal validation  

<br><br>Summary:  
This study investigates the generalization ability and interpretability of various machine learning models for predicting crop yield and anomalies across Germany's NUTS-3 regions. Leveraging a robust, long-term dataset, the research compares ensemble tree-based models such as XGBoost and Random Forest with deep learning techniques including LSTM and TCN. While all models demonstrate strong performance on spatially split, conventional test sets, their accuracy significantly declines when validated on temporally independent years, highlighting persistent challenges in temporal generalization. Interestingly, models showing high accuracy on conventional tests but poor temporal validation still generate plausible SHAP feature importance explanations, revealing a critical flaw in post hoc interpretability methods—explanations can seem reliable even when model predictions fail to generalize. This emphasizes the importance of validation-aware interpretation in agricultural and environmental machine learning applications. The study argues against accepting feature importance at face value unless the underlying models prove capable of generalizing across both unseen temporal and spatial conditions. Ultimately, it calls for incorporating domain knowledge into validation protocols, exploring hybrid modeling approaches, and applying stricter evaluation of explainability methods to improve trustworthiness in data-driven agricultural decision-making. The findings confront a broader challenge in environmental data science concerning the robustness of generalization assessment needed to confidently rely on model explanations. <div>
arXiv:2512.15140v1 Announce Type: new 
Abstract: This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).
  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.
  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Gradient-Based Inference Attack for Federated Learning</title>
<link>https://arxiv.org/abs/2512.15143</link>
<guid>https://arxiv.org/abs/2512.15143</guid>
<content:encoded><![CDATA[

arXiv:2512.15143v1 Announce Type: new 
Abstract: Federated Learning is a machine learning setting that reduces direct data exposure, improving the privacy guarantees of machine learning models. Yet, the exchange of model updates between the participants and the aggregator can still leak sensitive information. In this work, we present a new gradient-based membership inference attack for federated learning scenarios that exploits the temporal evolution of last-layer gradients across multiple federated rounds. Our method uses the shadow technique to learn round-wise gradient patterns of the training records, requiring no access to the private dataset, and is designed to consider both semi-honest and malicious adversaries (aggregators or data owners). Beyond membership inference, we also provide a natural extension of the proposed attack to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses. The proposed attacks are model-agnostic, and therefore applicable to any gradient-based model and can be applied to both classification and regression settings. We evaluate the attack on CIFAR-100 and Purchase100 datasets for membership inference and on Breast Cancer Wisconsin for attribute inference. Our findings reveal strong attack performance and comparable computational and memory overhead in membership inference when compared to another attack from the literature. The obtained results emphasize that multi-round federated learning can increase the vulnerability to inference attacks, that aggregators pose a more substantial threat than data owners, and that attack performance is strongly influenced by the nature of the training dataset, with richer, high-dimensional data leading to stronger leakage than simpler tabular data.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding NTK Variance in Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2512.15169</link>
<guid>https://arxiv.org/abs/2512.15169</guid>
<content:encoded><![CDATA[

arXiv:2512.15169v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs) often converge slowly and struggle to recover high-frequency details due to spectral bias. While prior work links this behavior to the Neural Tangent Kernel (NTK), how specific architectural choices affect NTK conditioning remains unclear. We show that many INR mechanisms can be understood through their impact on a small set of pairwise similarity factors and scaling terms that jointly determine NTK eigenvalue variance. For standard coordinate MLPs, limited input-feature interactions induce large eigenvalue dispersion and poor conditioning. We derive closed-form variance decompositions for common INR components and show that positional encoding reshapes input similarity, spherical normalization reduces variance via layerwise scaling, and Hadamard modulation introduces additional similarity factors strictly below one, yielding multiplicative variance reduction. This unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning. Experiments across multiple tasks confirm the predicted variance reductions and demonstrate faster, more stable convergence with improved reconstruction quality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEER: Draft with Diffusion, Verify with Autoregressive Models</title>
<link>https://arxiv.org/abs/2512.15176</link>
<guid>https://arxiv.org/abs/2512.15176</guid>
<content:encoded><![CDATA[

arXiv:2512.15176v1 Announce Type: new 
Abstract: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT</title>
<link>https://arxiv.org/abs/2512.15206</link>
<guid>https://arxiv.org/abs/2512.15206</guid>
<content:encoded><![CDATA[

arXiv:2512.15206v1 Announce Type: new 
Abstract: In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures</title>
<link>https://arxiv.org/abs/2512.15228</link>
<guid>https://arxiv.org/abs/2512.15228</guid>
<content:encoded><![CDATA[

arXiv:2512.15228v1 Announce Type: new 
Abstract: The adsorption energy serves as a crucial descriptor for the large-scale screening of catalysts. Nevertheless, the limited distribution of training data for the extensively utilised machine learning interatomic potential (MLIP), predominantly sourced from near-equilibrium structures, results in unreliable adsorption structures and consequent adsorption energy predictions. In this context, we present DBCata, a deep generative model that integrates a periodic Brownian-bridge framework with an equivariant graph neural network to establish a low-dimensional transition manifold between unrelaxed and DFT-relaxed structures, without requiring explicit energy or force information. Upon training, DBCata effectively generates high-fidelity adsorption geometries, achieving an interatomic distance mean absolute error (DMAE) of 0.035 \text{\AA} on the Catalysis-Hub dataset, which is nearly three times superior to that of the current state-of-the-art machine learning potential models. Moreover, the corresponding DFT accuracy can be improved within 0.1 eV in 94\% of instances by identifying and refining anomalous predictions through a hybrid chemical-heuristic and self-supervised outlier detection approach. We demonstrate that the remarkable performance of DBCata facilitates accelerated high-throughput computational screening for efficient alloy catalysts in the oxygen reduction reaction, highlighting the potential of DBCata as a powerful tool for catalyst design and optimisation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization</title>
<link>https://arxiv.org/abs/2512.15229</link>
<guid>https://arxiv.org/abs/2512.15229</guid>
<content:encoded><![CDATA[

arXiv:2512.15229v1 Announce Type: new 
Abstract: We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis</title>
<link>https://arxiv.org/abs/2512.15250</link>
<guid>https://arxiv.org/abs/2512.15250</guid>
<content:encoded><![CDATA[

arXiv:2512.15250v1 Announce Type: new 
Abstract: Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory</title>
<link>https://arxiv.org/abs/2512.15267</link>
<guid>https://arxiv.org/abs/2512.15267</guid>
<content:encoded><![CDATA[

arXiv:2512.15267v1 Announce Type: new 
Abstract: Sparse neural systems are gaining traction for efficient continual learning due to their modularity and low interference. Architectures such as Sparse Distributed Memory Multi-Layer Perceptrons (SDMLP) construct task-specific subnetworks via Top-K activation and have shown resilience against catastrophic forgetting. However, their rigid modularity limits cross-task knowledge reuse and leads to performance degradation under high sparsity. We propose Selective Subnetwork Distillation (SSD), a structurally guided continual learning framework that treats distillation not as a regularizer but as a topology-aligned information conduit. SSD identifies neurons with high activation frequency and selectively distills knowledge within previous Top-K subnetworks and output logits, without requiring replay or task labels. This enables structural realignment while preserving sparse modularity. Experiments on Split CIFAR-10, CIFAR-100, and MNIST demonstrate that SSD improves accuracy, retention, and representation coverage, offering a structurally grounded solution for sparse continual learning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topological Metric for Unsupervised Embedding Quality Evaluation</title>
<link>https://arxiv.org/abs/2512.15285</link>
<guid>https://arxiv.org/abs/2512.15285</guid>
<content:encoded><![CDATA[

arXiv:2512.15285v1 Announce Type: new 
Abstract: Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions</title>
<link>https://arxiv.org/abs/2512.15286</link>
<guid>https://arxiv.org/abs/2512.15286</guid>
<content:encoded><![CDATA[

arXiv:2512.15286v1 Announce Type: new 
Abstract: The increasing number of cyber threats and rapidly evolving tactics, as well as the high volume of data in recent years, have caused classical machine learning, rules, and signature-based defence strategies to fail, rendering them unable to keep up. An alternative, Quantum Machine Learning (QML), has recently emerged, making use of computations based on quantum mechanics. It offers better encoding and processing of high-dimensional structures for certain problems. This survey provides a comprehensive overview of QML techniques relevant to the domain of security, such as Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), and Quantum Generative Adversarial Networks (QGANs), and discusses the contributions of this paper in relation to existing research in the field and how it improves over them. It also maps these methods across supervised, unsupervised, and generative learning paradigms, and to core cybersecurity tasks, including intrusion and anomaly detection, malware and botnet classification, and encrypted-traffic analytics. It also discusses their application in the domain of cloud computing security, where QML can enhance secure and scalable operations. Many limitations of QML in the domain of cybersecurity have also been discussed, along with the directions for addressing them.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference</title>
<link>https://arxiv.org/abs/2512.15335</link>
<guid>https://arxiv.org/abs/2512.15335</guid>
<content:encoded><![CDATA[

arXiv:2512.15335v1 Announce Type: new 
Abstract: Deep neural networks are widely deployed with quantization techniques to reduce memory and computational costs by lowering the numerical precision of their parameters. While quantization alters model parameters and their outputs, existing privacy analyses primarily focus on full-precision models, leaving a gap in understanding how bit-width reduction can affect privacy leakage. We present the first systematic study of the privacy-utility relationship in post-training quantization (PTQ), a versatile family of methods that can be applied to pretrained models without further training. Using membership inference attacks as our evaluation framework, we analyze three popular PTQ algorithms-AdaRound, BRECQ, and OBC-across multiple precision levels (4-bit, 2-bit, and 1.58-bit) on CIFAR-10, CIFAR-100, and TinyImageNet datasets. Our findings consistently show that low-precision PTQs can reduce privacy leakage. In particular, lower-precision models demonstrate up to an order of magnitude reduction in membership inference vulnerability compared to their full-precision counterparts, albeit at the cost of decreased utility. Additional ablation studies on the 1.58-bit quantization level show that quantizing only the last layer at higher precision enables fine-grained control over the privacy-utility trade-off. These results offer actionable insights for practitioners to balance efficiency, utility, and privacy protection in real-world deployments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery</title>
<link>https://arxiv.org/abs/2512.15344</link>
<guid>https://arxiv.org/abs/2512.15344</guid>
<content:encoded><![CDATA[

arXiv:2512.15344v1 Announce Type: new 
Abstract: Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\% accuracy (+5.4\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Regime-Aware Fusion Framework for Time Series Classification</title>
<link>https://arxiv.org/abs/2512.15378</link>
<guid>https://arxiv.org/abs/2512.15378</guid>
<content:encoded><![CDATA[

arXiv:2512.15378v1 Announce Type: new 
Abstract: Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection</title>
<link>https://arxiv.org/abs/2512.15385</link>
<guid>https://arxiv.org/abs/2512.15385</guid>
<content:encoded><![CDATA[

arXiv:2512.15385v1 Announce Type: new 
Abstract: The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.
  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.15405</link>
<guid>https://arxiv.org/abs/2512.15405</guid>
<content:encoded><![CDATA[

arXiv:2512.15405v1 Announce Type: new 
Abstract: At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows</title>
<link>https://arxiv.org/abs/2512.15420</link>
<guid>https://arxiv.org/abs/2512.15420</guid>
<content:encoded><![CDATA[

arXiv:2512.15420v1 Announce Type: new 
Abstract: Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistics of Min-max Normalized Eigenvalues in Random Matrices</title>
<link>https://arxiv.org/abs/2512.15427</link>
<guid>https://arxiv.org/abs/2512.15427</guid>
<content:encoded><![CDATA[

arXiv:2512.15427v1 Announce Type: new 
Abstract: Random matrix theory has played an important role in various areas of pure mathematics, mathematical physics, and machine learning. From a practical perspective of data science, input data are usually normalized prior to processing. Thus, this study investigates the statistical properties of min-max normalized eigenvalues in random matrices. Previously, the effective distribution for such normalized eigenvalues has been proposed. In this study, we apply it to evaluate a scaling law of the cumulative distribution. Furthermore, we derive the residual error that arises during matrix factorization of random matrices. We conducted numerical experiments to verify these theoretical predictions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments</title>
<link>https://arxiv.org/abs/2512.15430</link>
<guid>https://arxiv.org/abs/2512.15430</guid>
<content:encoded><![CDATA[

arXiv:2512.15430v1 Announce Type: new 
Abstract: Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Double Horizon Model-Based Policy Optimization</title>
<link>https://arxiv.org/abs/2512.15439</link>
<guid>https://arxiv.org/abs/2512.15439</guid>
<content:encoded><![CDATA[

arXiv:2512.15439v1 Announce Type: new 
Abstract: Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long "distribution rollout" (DR) and a short "training rollout" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting</title>
<link>https://arxiv.org/abs/2512.15442</link>
<guid>https://arxiv.org/abs/2512.15442</guid>
<content:encoded><![CDATA[

arXiv:2512.15442v1 Announce Type: new 
Abstract: Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2512.15460</link>
<guid>https://arxiv.org/abs/2512.15460</guid>
<content:encoded><![CDATA[

arXiv:2512.15460v1 Announce Type: new 
Abstract: Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance</title>
<link>https://arxiv.org/abs/2512.15469</link>
<guid>https://arxiv.org/abs/2512.15469</guid>
<content:encoded><![CDATA[

arXiv:2512.15469v1 Announce Type: new 
Abstract: As machine learning models are increasingly deployed in high-stakes settings, e.g. as decision support systems in various societal sectors or in critical infrastructure, designers and auditors are facing the need to ensure that models satisfy a wider variety of requirements (e.g. compliance with regulations, fairness, computational constraints) beyond performance. Although most of them are the subject of ongoing studies, typical approaches face critical challenges: post-processing methods tend to compromise performance, which is often counteracted by fine-tuning or, worse, training from scratch, an often time-consuming or even unavailable strategy. This raises the following question: "Can we efficiently edit models to satisfy requirements, without sacrificing their utility?" In this work, we approach this with a unifying framework, in a data-driven manner, i.e. we learn to edit neural networks (NNs), where the editor is an NN itself - a graph metanetwork - and editing amounts to a single inference step. In particular, the metanetwork is trained on NN populations to minimise an objective consisting of two terms: the requirement to be enforced and the preservation of the NN's utility. We experiment with diverse tasks (the data minimisation principle, bias mitigation and weight pruning) improving the trade-offs between performance, requirement satisfaction and time efficiency compared to popular post-processing or re-training alternatives.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs</title>
<link>https://arxiv.org/abs/2512.15483</link>
<guid>https://arxiv.org/abs/2512.15483</guid>
<content:encoded><![CDATA[

arXiv:2512.15483v1 Announce Type: new 
Abstract: Self-driving laboratories (SDLs) are combining recent technological advances in robotics, automation, and machine learning based data analysis and decision-making to perform autonomous experimentation toward human-directed goals without requiring any direct human intervention. SDLs are successfully used in materials science, chemistry, and beyond, to optimise processes, materials, and devices in a systematic and data-efficient way. At present, the most widely used algorithm to identify the most informative next experiment is Bayesian optimisation. While relatively simple to apply to a wide range of optimisation problems, standard Bayesian optimisation relies on a fixed experimental workflow with a clear set of optimisation parameters and one or more measurable objective functions. This excludes the possibility of making on-the-fly decisions about changes in the planned sequence of operations and including intermediate measurements in the decision-making process. Therefore, many real-world experiments need to be adapted and simplified to be converted to the common setting in self-driving labs. In this paper, we introduce an extension to Bayesian optimisation that allows flexible sampling of multi-stage workflows and makes optimal decisions based on intermediate observables, which we call proxy measurements. We systematically compare the advantage of taking into account proxy measurements over conventional Bayesian optimisation, in which only the final measurement is observed. We find that over a wide range of scenarios, proxy measurements yield a substantial improvement, both in the time to find good solutions and in the overall optimality of found solutions. This not only paves the way to use more complex and thus more realistic experimental workflows in autonomous labs but also to smoothly combine simulations and experiments in the next generation of SDLs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier</title>
<link>https://arxiv.org/abs/2512.15492</link>
<guid>https://arxiv.org/abs/2512.15492</guid>
<content:encoded><![CDATA[

arXiv:2512.15492v1 Announce Type: new 
Abstract: We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Geometric Inductive Bias for Object Centric Dynamics</title>
<link>https://arxiv.org/abs/2512.15493</link>
<guid>https://arxiv.org/abs/2512.15493</guid>
<content:encoded><![CDATA[

arXiv:2512.15493v1 Announce Type: new 
Abstract: Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking Temporal Dynamics of Vector Sets with Gaussian Process</title>
<link>https://arxiv.org/abs/2512.15538</link>
<guid>https://arxiv.org/abs/2512.15538</guid>
<content:encoded><![CDATA[

arXiv:2512.15538v1 Announce Type: new 
Abstract: Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation</title>
<link>https://arxiv.org/abs/2512.15574</link>
<guid>https://arxiv.org/abs/2512.15574</guid>
<content:encoded><![CDATA[

arXiv:2512.15574v1 Announce Type: new 
Abstract: Feature and instance co-selection, which aims to reduce both feature dimensionality and sample size by identifying the most informative features and instances, has attracted considerable attention in recent years. However, when dealing with unlabeled incomplete multi-view data, where some samples are missing in certain views, existing methods typically first impute the missing data and then concatenate all views into a single dataset for subsequent co-selection. Such a strategy treats co-selection and missing data imputation as two independent processes, overlooking potential interactions between them. The inter-sample relationships gleaned from co-selection can aid imputation, which in turn enhances co-selection performance. Additionally, simply merging multi-view data fails to capture the complementary information among views, ultimately limiting co-selection effectiveness. To address these issues, we propose a novel co-selection method, termed Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE). JUICE first reconstructs incomplete multi-view data using available observations, bringing missing data recovery and feature and instance co-selection together in a unified framework. Then, JUICE leverages cross-view neighborhood information to learn inter-sample relationships and further refine the imputation of missing values during reconstruction. This enables the selection of more representative features and instances. Extensive experiments demonstrate that JUICE outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Corrective Diffusion Language Models</title>
<link>https://arxiv.org/abs/2512.15596</link>
<guid>https://arxiv.org/abs/2512.15596</guid>
<content:encoded><![CDATA[

arXiv:2512.15596v1 Announce Type: new 
Abstract: Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Smoothing is N-simplicial Attention?</title>
<link>https://arxiv.org/abs/2512.15600</link>
<guid>https://arxiv.org/abs/2512.15600</guid>
<content:encoded><![CDATA[

arXiv:2512.15600v1 Announce Type: new 
Abstract: Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction</title>
<link>https://arxiv.org/abs/2512.15605</link>
<guid>https://arxiv.org/abs/2512.15605</guid>
<content:encoded><![CDATA[

arXiv:2512.15605v1 Announce Type: new 
Abstract: Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary</title>
<link>https://arxiv.org/abs/2512.15614</link>
<guid>https://arxiv.org/abs/2512.15614</guid>
<content:encoded><![CDATA[

arXiv:2512.15614v1 Announce Type: new 
Abstract: Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoFlow: Solution Flow Models for One-Step Generative Modeling</title>
<link>https://arxiv.org/abs/2512.15657</link>
<guid>https://arxiv.org/abs/2512.15657</guid>
<content:encoded><![CDATA[

arXiv:2512.15657v1 Announce Type: new 
Abstract: The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multivariate Statistical Framework for Detection, Classification and Pre-localization of Anomalies in Water Distribution Networks</title>
<link>https://arxiv.org/abs/2512.15685</link>
<guid>https://arxiv.org/abs/2512.15685</guid>
<content:encoded><![CDATA[

arXiv:2512.15685v1 Announce Type: new 
Abstract: This paper presents a unified framework, for the detection, classification, and preliminary localization of anomalies in water distribution networks using multivariate statistical analysis. The approach, termed SICAMS (Statistical Identification and Classification of Anomalies in Mahalanobis Space), processes heterogeneous pressure and flow sensor data through a whitening transformation to eliminate spatial correlations among measurements. Based on the transformed data, the Hotelling's $T^2$ statistic is constructed, enabling the formulation of anomaly detection as a statistical hypothesis test of network conformity to normal operating conditions. It is shown that Hotelling's $T^2$ statistic can serve as an integral indicator of the overall "health" of the system, exhibiting correlation with total leakage volume, and thereby enabling approximate estimation of water losses via a regression model. A heuristic algorithm is developed to analyze the $T^2$ time series and classify detected anomalies into abrupt leaks, incipient leaks, and sensor malfunctions. Furthermore, a coarse leak localization method is proposed, which ranks sensors according to their statistical contribution and employs Laplacian interpolation to approximate the affected region within the network. Application of the proposed framework to the BattLeDIM L-Town benchmark dataset demonstrates high sensitivity and reliability in leak detection, maintaining robust performance even under multiple leaks. These capabilities make the method applicable to real-world operational environments without the need for a calibrated hydraulic model.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.15687</link>
<guid>https://arxiv.org/abs/2512.15687</guid>
<content:encoded><![CDATA[

arXiv:2512.15687v1 Announce Type: new 
Abstract: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Semantic Communication</title>
<link>https://arxiv.org/abs/2512.15691</link>
<guid>https://arxiv.org/abs/2512.15691</guid>
<content:encoded><![CDATA[

arXiv:2512.15691v1 Announce Type: new 
Abstract: Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FrontierCS: Evolving Challenges for Evolving Intelligence</title>
<link>https://arxiv.org/abs/2512.15699</link>
<guid>https://arxiv.org/abs/2512.15699</guid>
<content:encoded><![CDATA[

arXiv:2512.15699v1 Announce Type: new 
Abstract: We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data</title>
<link>https://arxiv.org/abs/2512.15706</link>
<guid>https://arxiv.org/abs/2512.15706</guid>
<content:encoded><![CDATA[

arXiv:2512.15706v1 Announce Type: new 
Abstract: In a mathematical model of interacting biological organisms, where external interventions may alter behavior over time, traditional models that assume fixed parameters usually do not capture the evolving dynamics. In oncology, this is further exacerbated by the fact that experimental data are often sparse and sometimes are composed of a few time points of tumor volume. In this paper, we propose to learn time-varying interactions between cells, such as those of bladder cancer tumors and immune cells, and their response to a combination of anticancer treatments in a limited data scenario. We employ the physics-informed neural network (PINN) approach to predict possible subpopulation trajectories at time points where no observed data are available. We demonstrate that our approach is consistent with the biological explanation of subpopulation trajectories. Our method provides a framework for learning evolving interactions among biological organisms when external interventions are applied to their environment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Graph-Embedded Hazard Model (GEHM): Stochastic Network Survival Dynamics on Economic Graphs</title>
<link>https://arxiv.org/abs/2512.14705</link>
<guid>https://arxiv.org/abs/2512.14705</guid>
<content:encoded><![CDATA[

arXiv:2512.14705v1 Announce Type: cross 
Abstract: This paper develops a nonlinear evolution framework for modelling survival dynamics on weighted economic networks by coupling a graph-based $p$-Laplacian diffusion operator with a stochastic structural drift. The resulting finite-dimensional PDE--SDE system captures how node-level survival reacts to nonlinear diffusion pressures while an aggregate complexity factor evolves according to an It\^o{} process. Using accretive operator theory, nonlinear semigroup methods, and stochastic analysis, we establish existence and uniqueness of mild solutions, derive topology-dependent energy dissipation inequalities, and characterise the stability threshold separating dissipative, critical, amplifying, and explosive regimes. Numerical experiments on Barab\'asi--Albert networks confirm that hub dominance magnifies nonlinear gradients and compresses stability margins, producing heavy-tailed survival distributions and occasional explosive behaviour.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGEMAS: A Self-Growing Ephemeral Multi-Agent System for Unsupervised Online Anomaly Detection via Entropic Homeostasis</title>
<link>https://arxiv.org/abs/2512.14708</link>
<guid>https://arxiv.org/abs/2512.14708</guid>
<content:encoded><![CDATA[

arXiv:2512.14708v1 Announce Type: cross 
Abstract: Current deep learning approaches for physiological signal monitoring suffer from static topologies and constant energy consumption. We introduce SGEMAS (Self-Growing Ephemeral Multi-Agent System), a bio-inspired architecture that treats intelligence as a dynamic thermodynamic process. By coupling a structural plasticity mechanism (agent birth death) to a variational free energy objective, the system naturally evolves to minimize prediction error with extreme sparsity. An ablation study on the MIT-BIH Arrhythmia Database reveals that adding a multi-scale instability index to the agent dynamics significantly improves performance. In a challenging inter-patient, zero-shot setting, the final SGEMAS v3.3 model achieves a mean AUC of 0.570 +- 0.070, outperforming both its simpler variants and a standard autoencoder baseline. This result validates that a physics-based, energy-constrained model can achieve robust unsupervised anomaly detection, offering a promising direction for efficient biomedical AI.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning</title>
<link>https://arxiv.org/abs/2512.14709</link>
<guid>https://arxiv.org/abs/2512.14709</guid>
<content:encoded><![CDATA[

arXiv:2512.14709v1 Announce Type: cross 
Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where to Explore: A Reach and Cost-Aware Approach for Unbiased Data Collection in Recommender Systems</title>
<link>https://arxiv.org/abs/2512.14733</link>
<guid>https://arxiv.org/abs/2512.14733</guid>
<content:encoded><![CDATA[

arXiv:2512.14733v1 Announce Type: cross 
Abstract: Exploration is essential to improve long-term recommendation quality, but it often degrades short-term business performance, especially in remote-first TV environments where users engage passively, expect instant relevance, and offer few chances for correction. This paper introduces an approach for delivering content-level exploration safely and efficiently by optimizing its placement based on reach and opportunity cost. Deployed on a large-scale streaming platform with over 100 million monthly active users, our approach identifies scroll-depth regions with lower engagement and strategically introduces a dedicated container, the "Something Completely Different" row containing randomized content. Rather than enforcing exploration uniformly across the user interface (UI), we condition its appearance on empirically low-cost, high-reach positions to ensure minimal tradeoff against platform-level watch time goals. Extensive A/B testing shows that this strategy preserves business metrics while collecting unbiased interaction data. Our method complements existing intra-row diversification and bandit-based exploration techniques by introducing a deployable, behaviorally informed mechanism for surfacing exploratory content at scale. Moreover, we demonstrate that the collected unbiased data, integrated into downstream candidate generation, significantly improves user engagement, validating its value for recommender systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)</title>
<link>https://arxiv.org/abs/2512.14742</link>
<guid>https://arxiv.org/abs/2512.14742</guid>
<content:encoded><![CDATA[

arXiv:2512.14742v1 Announce Type: cross 
Abstract: Open Radio Access Networks (O-RAN) enhance modularity and telemetry granularity but also widen the cybersecurity attack surface across disaggregated control, user and management planes. We propose a hierarchical defense framework with three coordinated layers-anomaly detection, intrusion confirmation, and multiattack classification-each aligned with O-RAN's telemetry stack. Our approach integrates hybrid quantum computing and machine learning, leveraging amplitude- and entanglement-based feature encodings with deep and ensemble classifiers. We conduct extensive benchmarking across synthetic and real-world telemetry, evaluating encoding depth, architectural variants, and diagnostic fidelity. The framework consistently achieves near-perfect accuracy, high recall, and strong class separability. Multi-faceted evaluation across decision boundaries, probabilistic margins, and latent space geometry confirms its interpretability, robustness, and readiness for slice-aware diagnostics and scalable deployment in near-RT and non-RT RIC domains.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compute the edge p-Laplacian centrality for air traffic network</title>
<link>https://arxiv.org/abs/2512.14749</link>
<guid>https://arxiv.org/abs/2512.14749</guid>
<content:encoded><![CDATA[

arXiv:2512.14749v1 Announce Type: cross 
Abstract: The problem that we would like to solve in this paper is to compute the edge p-Laplacian centrality for the air traffic network. In this problem, instead of computing the edge p-Laplacian centrality directly which is the very hard problem, we convert the air traffic network to the line graph. Finally, we will compute the node p-Laplacian centrality of the line graph which is equivalent to the edge p-Laplacian of the air traffic network. In this paper, the novel un-normalized graph (p-) Laplacian based ranking method will be developed based on the un-normalized graph p-Laplacian operator definitions such as the curvature operator of graph (i.e. the un-normalized graph 1-Laplacian operator) and will be used to compute the node p-Laplacian centrality of the line graph. The results from the experiments show that the un-normalized graph p-Laplacian ranking methods can be implemented successfully.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPE: Capability Achievement via Policy Execution</title>
<link>https://arxiv.org/abs/2512.14761</link>
<guid>https://arxiv.org/abs/2512.14761</guid>
<content:encoded><![CDATA[

arXiv:2512.14761v1 Announce Type: cross 
Abstract: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.
  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.
  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.
  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge</title>
<link>https://arxiv.org/abs/2512.14766</link>
<guid>https://arxiv.org/abs/2512.14766</guid>
<content:encoded><![CDATA[

arXiv:2512.14766v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</title>
<link>https://arxiv.org/abs/2512.14796</link>
<guid>https://arxiv.org/abs/2512.14796</guid>
<content:encoded><![CDATA[

arXiv:2512.14796v1 Announce Type: cross 
Abstract: Whole-slide images (WSIs) contain tissue information distributed across multiple magnification levels, yet most self-supervised methods treat these scales as independent views. This separation prevents models from learning representations that remain stable when resolution changes, a key requirement for practical neuropathology workflows. This study introduces Magnification-Aware Distillation (MAD), a self-supervised strategy that links low-magnification context with spatially aligned high-magnification detail, enabling the model to learn how coarse tissue structure relates to fine cellular patterns. The resulting foundation model, MAD-NP, is trained entirely through this cross-scale correspondence without annotations. A linear classifier trained only on 10x embeddings maintains 96.7% of its performance when applied to unseen 40x tiles, demonstrating strong resolution-invariant representation learning. Segmentation outputs remain consistent across magnifications, preserving anatomical boundaries and minimizing noise. These results highlight the feasibility of scalable, magnification-robust WSI analysis using a unified embedding space
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis</title>
<link>https://arxiv.org/abs/2512.14801</link>
<guid>https://arxiv.org/abs/2512.14801</guid>
<content:encoded><![CDATA[

arXiv:2512.14801v1 Announce Type: cross 
Abstract: OpenAI has recently argued that hallucinations in large language models result primarily from misaligned evaluation incentives that reward confident guessing rather than epistemic humility. On this view, hallucination is a contingent behavioral artifact, remediable through improved benchmarks and reward structures. In this paper, we challenge that interpretation. Drawing on previous work on structural hallucination and empirical experiments using a Licensing Oracle, we argue that hallucination is not an optimization failure but an architectural inevitability of the transformer model.
  Transformers do not represent the world; they model statistical associations among tokens. Their embedding spaces form a pseudo-ontology derived from linguistic co-occurrence rather than world-referential structure. At ontological boundary conditions - regions where training data is sparse or incoherent - the model necessarily interpolates fictional continuations in order to preserve coherence. No incentive mechanism can modify this structural dependence on pattern completion.
  Our empirical results demonstrate that hallucination can only be eliminated through external truth-validation and abstention modules, not through changes to incentives, prompting, or fine-tuning. The Licensing Oracle achieves perfect abstention precision across domains precisely because it supplies grounding that the transformer lacks.
  We conclude that hallucination is a structural property of generative architectures and that reliable AI requires hybrid systems that distinguish linguistic fluency from epistemic responsibility.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction</title>
<link>https://arxiv.org/abs/2512.14865</link>
<guid>https://arxiv.org/abs/2512.14865</guid>
<content:encoded><![CDATA[

arXiv:2512.14865v1 Announce Type: cross 
Abstract: End-to-end (E2E) spoken dialogue systems are increasingly replacing cascaded pipelines for voice-based human-AI interaction, processing raw audio directly without intermediate transcription. Existing benchmarks primarily evaluate these models on synthetic speech and single-turn tasks, leaving realistic multi-turn conversational ability underexplored. We introduce Audio MultiChallenge, an open-source benchmark to evaluate E2E spoken dialogue systems under natural multi-turn interaction patterns. Building on the text-based MultiChallenge framework, which evaluates Inference Memory, Instruction Retention, and Self Coherence, we introduce a new axis Voice Editing that tests robustness to mid-utterance speech repairs and backtracking. We further augment each axis to the audio modality, such as introducing Audio-Cue challenges for Inference Memory that require recalling ambient sounds and paralinguistic signals beyond semantic content. We curate 452 conversations from 47 speakers with 1,712 instance-specific rubrics through a hybrid audio-native agentic and human-in-the-loop pipeline that exposes model failures at scale while preserving natural disfluencies found in unscripted human speech. Our evaluation of proprietary and open-source models reveals that even frontier models struggle on our benchmark, with Gemini 3 Pro Preview (Thinking), our highest-performing model achieving a 54.65% pass rate. Error analysis shows that models fail most often on our new axes and that Self Coherence degrades with longer audio context. These failures reflect difficulty of tracking edits, audio cues, and long-range context in natural spoken dialogue. Audio MultiChallenge provides a reproducible testbed to quantify them and drive improvements in audio-native multi-turn interaction capability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models</title>
<link>https://arxiv.org/abs/2512.14926</link>
<guid>https://arxiv.org/abs/2512.14926</guid>
<content:encoded><![CDATA[

arXiv:2512.14926v1 Announce Type: cross 
Abstract: Focusing on low-resource languages is an essential step toward democratizing generative AI. In this work, we contribute to reducing the multimodal NLP resource gap for Romanian. We translate the widely known Flickr30k dataset into Romanian and further extend it for visual question answering by leveraging open-source LLMs. We demonstrate the usefulness of our datasets by fine-tuning open-source VLMs on Romanian visual question answering. We select VLMs from three widely used model families: LLaMA 3.2, LLaVA 1.6, and Qwen2. For fine-tuning, we employ the parameter-efficient LoRA method. Our models show improved Romanian capabilities in visual QA, as well as on tasks they were not trained on, such as Romanian image description generation. The seven-billion-parameter Qwen2-VL-RoVQA obtains top scores on both tasks, with improvements of +6.05% and +2.61% in BERTScore F1 over its original version. Finally, the models show substantial reductions in grammatical errors compared to their original forms, indicating improvements not only in language understanding but also in Romanian fluency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep learning water-unsuppressed MRSI at ultra-high field for simultaneous quantitative metabolic, susceptibility and myelin water imaging</title>
<link>https://arxiv.org/abs/2512.14929</link>
<guid>https://arxiv.org/abs/2512.14929</guid>
<content:encoded><![CDATA[

arXiv:2512.14929v1 Announce Type: cross 
Abstract: Purpose: Magnetic Resonance Spectroscopic Imaging (MRSI) maps endogenous brain metabolism while suppressing the overwhelming water signal. Water-unsuppressed MRSI (wu-MRSI) allows simultaneous imaging of water and metabolites, but large water sidebands cause challenges for metabolic fitting. We developed an end-to-end deep-learning pipeline to overcome these challenges at ultra-high field. Methods:Fast high-resolution wu-MRSI was acquired at 7T with non-cartesian ECCENTRIC sampling and ultra-short echo time. A water and lipid removal network (WALINET+) was developed to remove lipids, water signal, and sidebands. MRSI reconstruction was performed by DeepER and a physics-informed network for metabolite fitting. Water signal was used for absolute metabolite quantification, quantitative susceptibility mapping (QSM), and myelin water fraction imaging (MWF). Results: WALINET+ provided the lowest NRMSE (< 2%) in simulations and in vivo the smallest bias (< 20%) and limits-of-agreement (+-63%) between wu-MRSI and ws-MRSI scans. Several metabolites such as creatine and glutamate showed higher SNR in wu-MRSI. QSM and MWF obtained from wu-MRSI and GRE showed good agreement with 0 ppm/5.5% bias and +-0.05 ppm/ +- 12.75% limits-of-agreement. Conclusion: High-quality metabolic, QSM, and MWF mapping of the human brain can be obtained simultaneously by ECCENTRIC wu-MRSI at 7T with 2 mm isotropic resolution in 12 min. WALINET+ robustly removes water sidebands while preserving metabolite signal, eliminating the need for water suppression and separate water acquisitions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection</title>
<link>https://arxiv.org/abs/2512.14935</link>
<guid>https://arxiv.org/abs/2512.14935</guid>
<content:encoded><![CDATA[

arXiv:2512.14935v1 Announce Type: cross 
Abstract: Cloud Security Operations Center (SOC) enable cloud governance, risk and compliance by providing insights visibility and control. Cloud SOC triages high-volume, heterogeneous telemetry from elastic, short-lived resources while staying within tight budgets. In this research, we implement an AI-Augmented Security Operations Center (AISOC) on AWS that combines cloud-native instrumentation with ML-based detection. The architecture uses three Amazon EC2 instances: Attacker, Defender, and Monitoring. We simulate a reverse-shell intrusion with Metasploit, and Filebeat forwards Defender logs to an Elasticsearch and Kibana stack for analysis. We train two classifiers, a malware detector built on a public dataset and a log-anomaly detector trained on synthetically augmented logs that include adversarial variants. We calibrate and fuse the scores to produce multi-modal threat intelligence and triage activity into NORMAL, SUSPICIOUS, and HIGH\_CONFIDENCE\_ATTACK. On held-out tests the fusion achieves strong macro-F1 (up to 1.00) under controlled conditions, though performance will vary in noisier and more diverse environments. These results indicate that simple, calibrated fusion can enhance cloud SOC capabilities in constrained, cost-sensitive setups.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boundary condition enforcement with PINNs: a comparative study and verification on 3D geometries</title>
<link>https://arxiv.org/abs/2512.14941</link>
<guid>https://arxiv.org/abs/2512.14941</guid>
<content:encoded><![CDATA[

arXiv:2512.14941v1 Announce Type: cross 
Abstract: Since their advent nearly a decade ago, physics-informed neural networks (PINNs) have been studied extensively as a novel technique for solving forward and inverse problems in physics and engineering. The neural network discretization of the solution field is naturally adaptive and avoids meshing the computational domain, which can both improve the accuracy of the numerical solution and streamline implementation. However, there have been limited studies of PINNs on complex three-dimensional geometries, as the lack of mesh and the reliance on the strong form of the partial differential equation (PDE) make boundary condition (BC) enforcement challenging. Techniques to enforce BCs with PINNs have proliferated in the literature, but a comprehensive side-by-side comparison of these techniques and a study of their efficacy on geometrically complex three-dimensional test problems are lacking. In this work, we i) systematically compare BC enforcement techniques for PINNs, ii) propose a general solution framework for arbitrary three-dimensional geometries, and iii) verify the methodology on three-dimensional, linear and nonlinear test problems with combinations of Dirichlet, Neumann, and Robin boundaries. Our approach is agnostic to the underlying PDE, the geometry of the computational domain, and the nature of the BCs, while requiring minimal hyperparameter tuning. This work represents a step in the direction of establishing PINNs as a mature numerical method, capable of competing head-to-head with incumbents such as the finite element method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2512.14946</link>
<guid>https://arxiv.org/abs/2512.14946</guid>
<content:encoded><![CDATA[

arXiv:2512.14946v1 Announce Type: cross 
Abstract: Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.
  We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation</title>
<link>https://arxiv.org/abs/2512.14954</link>
<guid>https://arxiv.org/abs/2512.14954</guid>
<content:encoded><![CDATA[

arXiv:2512.14954v1 Announce Type: cross 
Abstract: Computing next-token likelihood ratios between two language models (LMs) is a standard task in training paradigms such as knowledge distillation. Since this requires both models to share the same probability space, it becomes challenging when the teacher and student LMs use different tokenizers, for instance, when edge-device deployment necessitates a smaller vocabulary size to lower memory overhead. In this work, we address this vocabulary misalignment problem by uncovering an implicit recursive structure in the commonly deployed Byte-Pair Encoding (BPE) algorithm and utilizing it to create a probabilistic framework for cross-tokenizer likelihood scoring. Our method enables sequence likelihood evaluation for vocabularies different from the teacher model native tokenizer, addressing two specific scenarios: when the student vocabulary is a subset of the teacher vocabulary, and the general case where it is arbitrary. In the subset regime, our framework computes exact likelihoods and provides next-token probabilities for sequential sampling with only O(1) model evaluations per token. When used for distillation, this yields up to a 12% reduction in memory footprint for the Qwen2.5-1.5B model while also improving baseline performance up to 4% on the evaluated tasks. For the general case, we introduce a rigorous lossless procedure that leverages BPE recursive structure, complemented by a fast approximation that keeps large-vocabulary settings practical. Applied to distillation for mathematical reasoning, our approach improves GSM8K accuracy by more than 2% over the current state of the art.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrusion Detection in Internet of Vehicles Using Machine Learning</title>
<link>https://arxiv.org/abs/2512.14958</link>
<guid>https://arxiv.org/abs/2512.14958</guid>
<content:encoded><![CDATA[

arXiv:2512.14958v1 Announce Type: cross 
Abstract: The Internet of Vehicles (IoV) has evolved modern transportation through enhanced connectivity and intelligent systems. However, this increased connectivity introduces critical vulnerabilities, making vehicles susceptible to cyber-attacks such Denial-ofService (DoS) and message spoofing. This project aims to develop a machine learning-based intrusion detection system to classify malicious Controller Area network (CAN) bus traffic using the CiCIoV2024 benchmark dataset. We analyzed various attack patterns including DoS and spoofing attacks targeting critical vehicle parameters such as Spoofing-GAS - gas pedal position, Spoofing-RPM, Spoofing-Speed, and Spoofing-Steering\_Wheel. Our initial findings confirm a multi-class classification problem with a clear structural difference between attack types and benign data, providing a strong foundation for machine learning models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent</title>
<link>https://arxiv.org/abs/2512.14990</link>
<guid>https://arxiv.org/abs/2512.14990</guid>
<content:encoded><![CDATA[

arXiv:2512.14990v1 Announce Type: cross 
Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Nudged Elastic Band Method using Neural Network Bayesian Algorithm Execution</title>
<link>https://arxiv.org/abs/2512.14993</link>
<guid>https://arxiv.org/abs/2512.14993</guid>
<content:encoded><![CDATA[

arXiv:2512.14993v1 Announce Type: cross 
Abstract: The discovery of a minimum energy pathway (MEP) between metastable states is crucial for scientific tasks including catalyst and biomolecular design. However, the standard nudged elastic band (NEB) algorithm requires hundreds to tens of thousands of compute-intensive simulations, making applications to complex systems prohibitively expensive. We introduce Neural Network Bayesian Algorithm Execution (NN-BAX), a framework that jointly learns the energy landscape and the MEP. NN-BAX sequentially fine-tunes a foundation model by actively selecting samples targeted at improving the MEP. Tested on Lennard-Jones and Embedded Atom Method systems, our approach achieves a one to two order of magnitude reduction in energy and force evaluations with negligible loss in MEP accuracy and demonstrates scalability to >100-dimensional systems. This work is therefore a promising step towards removing the computational barrier for MEP discovery in scientifically relevant systems, suggesting that weeks-long calculations may be achieved in hours or days with minimal loss in accuracy.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports</title>
<link>https://arxiv.org/abs/2512.15003</link>
<guid>https://arxiv.org/abs/2512.15003</guid>
<content:encoded><![CDATA[

arXiv:2512.15003v1 Announce Type: cross 
Abstract: Monitoring issue tracker submissions is a crucial software maintenance activity. A key goal is the prioritization of high risk, security-related bugs. If such bugs can be recognized early, the risk of propagation to dependent products and endangerment of stakeholder benefits can be mitigated. To assist triage engineers with this task, several automatic detection techniques, from Machine Learning (ML) models to prompting Large Language Models (LLMs), have been proposed. Although promising to some extent, prior techniques often memorize lexical cues as decision shortcuts, yielding low detection rate specifically for more complex submissions. As such, these classifiers do not yet reach the practical expectations of a real-time detector of security-related issues. To address these limitations, we propose SEBERTIS, a framework to train Deep Neural Networks (DNNs) as classifiers independent of lexical cues, so that they can confidently detect fully unseen security-related issues. SEBERTIS capitalizes on fine-tuning bidirectional transformer architectures as Masked Language Models (MLMs) on a series of semantically equivalent vocabulary to prediction labels (which we call Semantic Surrogates) when they have been replaced with a mask. Our SEBERTIS-trained classifier achieves a 0.9880 F1-score in detecting security-related issues of a curated corpus of 10,000 GitHub issue reports, substantially outperforming state-of-the-art issue classifiers, with 14.44%-96.98%, 15.40%-93.07%, and 14.90%-94.72% higher detection precision, recall, and F1-score over ML-based baselines. Our classifier also substantially surpasses LLM baselines, with an improvement of 23.20%-63.71%, 36.68%-85.63%, and 39.49%-74.53% for precision, recall, and F1-score.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</title>
<link>https://arxiv.org/abs/2512.15053</link>
<guid>https://arxiv.org/abs/2512.15053</guid>
<content:encoded><![CDATA[

arXiv:2512.15053v1 Announce Type: cross 
Abstract: The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based "prompt engineering," fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for "Observable Software Engineering" in the era of probabilistic computing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Weighted Genetic Algorithm-Optimized SVR for Robust Long-Term Forecasting of Global Stock Indices for investment decisions</title>
<link>https://arxiv.org/abs/2512.15113</link>
<guid>https://arxiv.org/abs/2512.15113</guid>
<content:encoded><![CDATA[

arXiv:2512.15113v1 Announce Type: cross 
Abstract: Long-term price forecasting remains a formidable challenge due to the inherent uncertainty over the long term, despite some success in short-term predictions. Nonetheless, accurate long-term forecasts are essential for high-net-worth individuals, institutional investors, and traders. The proposed improved genetic algorithm-optimized support vector regression (IGA-SVR) model is specifically designed for long-term price prediction of global indices. The performance of the IGA-SVR model is rigorously evaluated and compared against the state-of-the-art baseline models, the Long Short-Term Memory (LSTM), and the forward-validating genetic algorithm optimized support vector regression (OGA-SVR). Extensive testing was conducted on the five global indices, namely Nifty, Dow Jones Industrial Average (DJI), DAX Performance Index (DAX), Nikkei 225 (N225), and Shanghai Stock Exchange Composite Index (SSE) from 2021 to 2024 of daily price prediction up to a year. Overall, the proposed IGA-SVR model achieved a reduction in MAPE by 19.87% compared to LSTM and 50.03% compared to OGA-SVR, demonstrating its superior performance in long-term daily price forecasting of global indices. Further, the execution time for LSTM was approximately 20 times higher than that of IGA-SVR, highlighting the high accuracy and computational efficiency of the proposed model. The genetic algorithm selects the optimal hyperparameters of SVR by minimizing the arithmetic mean of the Mean Absolute Percentage Error (MAPE) calculated over the full training dataset and the most recent five years of training data. This purposefully designed training methodology adjusts for recent trends while retaining long-term trend information, thereby offering enhanced generalization compared to the LSTM and rolling-forward validation approach employed by OGA-SVR, which forgets long-term trends and suffers from recency bias.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BEAT2AASIST model with layer fusion for ESDD 2026 Challenge</title>
<link>https://arxiv.org/abs/2512.15180</link>
<guid>https://arxiv.org/abs/2512.15180</guid>
<content:encoded><![CDATA[

arXiv:2512.15180v1 Announce Type: cross 
Abstract: Recent advances in audio generation have increased the risk of realistic environmental sound manipulation, motivating the ESDD 2026 Challenge as the first large-scale benchmark for Environmental Sound Deepfake Detection (ESDD). We propose BEAT2AASIST which extends BEATs-AASIST by splitting BEATs-derived representations along frequency or channel dimension and processing them with dual AASIST branches. To enrich feature representations, we incorporate top-k transformer layer fusion using concatenation, CNN-gated, and SE-gated strategies. In addition, vocoder-based data augmentation is applied to improve robustness against unseen spoofing methods. Experimental results on the official test sets demonstrate that the proposed approach achieves competitive performance across the challenge tracks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-consistent clustering for evolving data</title>
<link>https://arxiv.org/abs/2512.15210</link>
<guid>https://arxiv.org/abs/2512.15210</guid>
<content:encoded><![CDATA[

arXiv:2512.15210v1 Announce Type: cross 
Abstract: Data analysis often involves an iterative process, where solutions must be continuously refined in response to new data. Typically, as new data becomes available, an existing solution must be updated to incorporate the latest information. In addition to seeking a high-quality solution for the task at hand, it is also crucial to ensure consistency by minimizing drastic changes from previous solutions. Applying this approach across many iterations, ensures that the solution evolves gradually and smoothly.
  In this paper, we study the above problem in the context of clustering, specifically focusing on the $k$-center problem. More precisely, we study the following problem: Given a set of points $X$, parameters $k$ and $b$, and a prior clustering solution $H$ for $X$, our goal is to compute a new solution $C$ for $X$, consisting of $k$ centers, which minimizes the clustering cost while introducing at most $b$ changes from $H$. We refer to this problem as label-consistent $k$-center, and we propose two constant-factor approximation algorithms for it. We complement our theoretical findings with an experimental evaluation demonstrating the effectiveness of our methods on real-world datasets.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ColliderML: The First Release of an OpenDataDetector High-Luminosity Physics Benchmark Dataset</title>
<link>https://arxiv.org/abs/2512.15230</link>
<guid>https://arxiv.org/abs/2512.15230</guid>
<content:encoded><![CDATA[

arXiv:2512.15230v1 Announce Type: cross 
Abstract: We introduce ColliderML - a large, open, experiment-agnostic dataset of fully simulated and digitised proton-proton collisions in High-Luminosity Large Hadron Collider conditions ($\sqrt{s}=14$ TeV, mean pile-up $\mu = 200$). ColliderML provides one million events across ten Standard Model and Beyond Standard Model processes, plus extensive single-particle samples, all produced with modern next-to-leading order matrix element calculation and showering, realistic per-event pile-up overlay, a validated OpenDataDetector geometry, and standard reconstructions. The release fills a major gap for machine learning (ML) research on detector-level data, provided on the ML-friendly Hugging Face platform. We present physics coverage and the generation, simulation, digitisation and reconstruction pipeline, describe format and access, and initial collider physics benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.15254</link>
<guid>https://arxiv.org/abs/2512.15254</guid>
<content:encoded><![CDATA[

arXiv:2512.15254v1 Announce Type: cross 
Abstract: Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model inference for ranking from pairwise comparisons</title>
<link>https://arxiv.org/abs/2512.15269</link>
<guid>https://arxiv.org/abs/2512.15269</guid>
<content:encoded><![CDATA[

arXiv:2512.15269v1 Announce Type: cross 
Abstract: We consider the problem of ranking objects from noisy pairwise comparisons, for example, ranking tennis players from the outcomes of matches. We follow a standard approach to this problem and assume that each object has an unobserved strength and that the outcome of each comparison depends probabilistically on the strengths of the comparands. However, we do not assume to know a priori how skills affect outcomes. Instead, we present an efficient algorithm for simultaneously inferring both the unobserved strengths and the function that maps strengths to probabilities. Despite this problem being under-constrained, we present experimental evidence that the conclusions of our Bayesian approach are robust to different model specifications. We include several case studies to exemplify the method on real-world data sets.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs</title>
<link>https://arxiv.org/abs/2512.15306</link>
<guid>https://arxiv.org/abs/2512.15306</guid>
<content:encoded><![CDATA[

arXiv:2512.15306v1 Announce Type: cross 
Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Varying Audio Effect Modeling by End-to-End Adversarial Training</title>
<link>https://arxiv.org/abs/2512.15313</link>
<guid>https://arxiv.org/abs/2512.15313</guid>
<content:encoded><![CDATA[

arXiv:2512.15313v1 Announce Type: cross 
Abstract: Deep learning has become a standard approach for the modeling of audio effects, yet strictly black-box modeling remains problematic for time-varying systems. Unlike time-invariant effects, training models on devices with internal modulation typically requires the recording or extraction of control signals to ensure the time-alignment required by standard loss functions. This paper introduces a Generative Adversarial Network (GAN) framework to model such effects using only input-output audio recordings, removing the need for modulation signal extraction. We propose a convolutional-recurrent architecture trained via a two-stage strategy: an initial adversarial phase allows the model to learn the distribution of the modulation behavior without strict phase constraints, followed by a supervised fine-tuning phase where a State Prediction Network (SPN) estimates the initial internal states required to synchronize the model with the target. Additionally, a new objective metric based on chirp-train signals is developed to quantify modulation accuracy. Experiments modeling a vintage hardware phaser demonstrate the method's ability to capture time-varying dynamics in a fully black-box context.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models</title>
<link>https://arxiv.org/abs/2512.15347</link>
<guid>https://arxiv.org/abs/2512.15347</guid>
<content:encoded><![CDATA[

arXiv:2512.15347v1 Announce Type: cross 
Abstract: Group Relative Policy Optimization (GRPO) is a powerful technique for aligning generative models, but its effectiveness is bottlenecked by the conflict between large group sizes and prohibitive computational costs. In this work, we investigate the trade-off through empirical studies, yielding two key observations. First, we discover the reward clustering phenomenon in which many trajectories collapse toward the group-mean reward, offering limited optimization value. Second, we design a heuristic strategy named Optimal Variance Filtering (OVF), and verify that a high-variance subset of trajectories, selected by OVF can outperform the larger, unfiltered group. However, this static, post-sampling OVF approach still necessitates critical computational overhead, as it performs unnecessary sampling for trajectories that are ultimately discarded. To resolve this, we propose Pro-GRPO (Proactive GRPO), a novel dynamic framework that integrates latent feature-based trajectory pruning into the sampling process. Through the early termination of reward-clustered trajectories, Pro-GRPO reduces computational overhead. Leveraging its efficiency, Pro-GRPO employs an "Expand-and-Prune" strategy. This strategy first expands the size of initial sampling group to maximize trajectory diversity, then it applies multi-step OVF to the latents, avoiding prohibitive computational costs. Extensive experiments on both diffusion-based and flow-based models demonstrate the generality and effectiveness of our Pro-GRPO framework.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.15372</link>
<guid>https://arxiv.org/abs/2512.15372</guid>
<content:encoded><![CDATA[

arXiv:2512.15372v1 Announce Type: cross 
Abstract: Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Remotely Detectable Robot Policy Watermarking</title>
<link>https://arxiv.org/abs/2512.15379</link>
<guid>https://arxiv.org/abs/2512.15379</guid>
<content:encoded><![CDATA[

arXiv:2512.15379v1 Announce Type: cross 
Abstract: The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</title>
<link>https://arxiv.org/abs/2512.15396</link>
<guid>https://arxiv.org/abs/2512.15396</guid>
<content:encoded><![CDATA[

arXiv:2512.15396v1 Announce Type: cross 
Abstract: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Partitioned Local Depth for semi-supervised applications</title>
<link>https://arxiv.org/abs/2512.15436</link>
<guid>https://arxiv.org/abs/2512.15436</guid>
<content:encoded><![CDATA[

arXiv:2512.15436v1 Announce Type: cross 
Abstract: We introduce an extension of the partitioned local depth (PaLD) algorithm that is adapted to online applications such as semi-supervised prediction. The new algorithm we present, online PaLD, is well-suited to situations where it is a possible to pre-compute a cohesion network from a reference dataset. After $O(n^3)$ steps to construct a queryable data structure, online PaLD can extend the cohesion network to a new data point in $O(n^2)$ time. Our approach complements previous speed up approaches based on approximation and parallelism. For illustrations, we present applications to online anomaly detection and semi-supervised classification for health-care datasets.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</title>
<link>https://arxiv.org/abs/2512.15503</link>
<guid>https://arxiv.org/abs/2512.15503</guid>
<content:encoded><![CDATA[

arXiv:2512.15503v1 Announce Type: cross 
Abstract: Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models</title>
<link>https://arxiv.org/abs/2512.15521</link>
<guid>https://arxiv.org/abs/2512.15521</guid>
<content:encoded><![CDATA[

arXiv:2512.15521v1 Announce Type: cross 
Abstract: The development of nuclear fusion requires materials that can withstand extreme conditions. The IFMIF-DONES facility, a high-power particle accelerator, is being designed to qualify these materials. A critical testbed for its development is the MuVacAS prototype, which replicates the final segment of the accelerator beamline. Precise regulation of argon gas pressure within its ultra-high vacuum chamber is vital for this task. This work presents a fully data-driven approach for autonomous pressure control. A Deep Learning Surrogate Model, trained on real operational data, emulates the dynamics of the argon injection system. This high-fidelity digital twin then serves as a fast-simulation environment to train a Deep Reinforcement Learning agent. The results demonstrate that the agent successfully learns a control policy that maintains gas pressure within strict operational limits despite dynamic disturbances. This approach marks a significant step toward the intelligent, autonomous control systems required for the demanding next-generation particle accelerator facilities.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Conditioned UNet for Music Source Separation</title>
<link>https://arxiv.org/abs/2512.15532</link>
<guid>https://arxiv.org/abs/2512.15532</guid>
<content:encoded><![CDATA[

arXiv:2512.15532v1 Announce Type: cross 
Abstract: In this paper we propose a conditioned UNet for Music Source Separation (MSS). MSS is generally performed by multi-output neural networks, typically UNets, with each output representing a particular stem from a predefined instrument vocabulary. In contrast, conditioned MSS networks accept an audio query related to a stem of interest alongside the signal from which that stem is to be extracted. Thus, a strict vocabulary is not required and this enables more realistic tasks in MSS. The potential of conditioned approaches for such tasks has been somewhat hidden due to a lack of suitable data, an issue recently addressed with the MoisesDb dataset. A recent method, Banquet, employs this dataset with promising results seen on larger vocabularies. Banquet uses Bandsplit RNN rather than a UNet and the authors state that UNets should not be suitable for conditioned MSS. We counter this argument and propose QSCNet, a novel conditioned UNet for MSS that integrates network conditioning elements in the Sparse Compressed Network for MSS. We find QSCNet to outperform Banquet by over 1dB SNR on a couple of MSS tasks, while using less than half the number of parameters.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Photonics-Enhanced Graph Convolutional Networks</title>
<link>https://arxiv.org/abs/2512.15549</link>
<guid>https://arxiv.org/abs/2512.15549</guid>
<content:encoded><![CDATA[

arXiv:2512.15549v1 Announce Type: cross 
Abstract: Photonics can offer a hardware-native route for machine learning (ML). However, efficient deployment of photonics-enhanced ML requires hybrid workflows that integrate optical processing with conventional CPU/GPU based neural network architectures. Here, we propose such a workflow that combines photonic positional embeddings (PEs) with advanced graph ML models. We introduce a photonics-based method that augments graph convolutional networks (GCNs) with PEs derived from light propagation on synthetic frequency lattices whose couplings match the input graph. We simulate propagation and readout to obtain internode intensity correlation matrices, which are used as PEs in GCNs to provide global structural information. Evaluated on Long Range Graph Benchmark molecular datasets, the method outperforms baseline GCNs with Laplacian based PEs, achieving $6.3\%$ lower mean absolute error for regression and $2.3\%$ higher average precision for classification tasks using a two-layer GCN as a baseline. When implemented in high repetition rate photonic hardware, correlation measurements can enable fast feature generation by bypassing digital simulation of PEs. Our results show that photonic PEs improve GCN performance and support optical acceleration of graph ML.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models in Scientific Discovery</title>
<link>https://arxiv.org/abs/2512.15567</link>
<guid>https://arxiv.org/abs/2512.15567</guid>
<content:encoded><![CDATA[

arXiv:2512.15567v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</title>
<link>https://arxiv.org/abs/2512.15581</link>
<guid>https://arxiv.org/abs/2512.15581</guid>
<content:encoded><![CDATA[

arXiv:2512.15581v1 Announce Type: cross 
Abstract: High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point</title>
<link>https://arxiv.org/abs/2512.15606</link>
<guid>https://arxiv.org/abs/2512.15606</guid>
<content:encoded><![CDATA[

arXiv:2512.15606v1 Announce Type: cross 
Abstract: Near an optimal learning point of a neural network, the learning performance of gradient descent dynamics is dictated by the Hessian matrix of the loss function with respect to the network parameters. We characterize the Hessian eigenspectrum for some classes of teacher-student problems, when the teacher and student networks have matching weights, showing that the smaller eigenvalues of the Hessian determine long-time learning performance. For linear networks, we analytically establish that for large networks the spectrum asymptotically follows a convolution of a scaled chi-square distribution with a scaled Marchenko-Pastur distribution. We numerically analyse the Hessian spectrum for polynomial and other non-linear networks. Furthermore, we show that the rank of the Hessian matrix can be seen as an effective number of parameters for networks using polynomial activation functions. For a generic non-linear activation function, such as the error function, we empirically observe that the Hessian matrix is always full rank.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning continuous SOC-dependent thermal decomposition kinetics for Li-ion cathodes using KA-CRNNs</title>
<link>https://arxiv.org/abs/2512.15628</link>
<guid>https://arxiv.org/abs/2512.15628</guid>
<content:encoded><![CDATA[

arXiv:2512.15628v1 Announce Type: cross 
Abstract: Thermal runaway in lithium-ion batteries is strongly influenced by the state of charge (SOC). Existing predictive models typically infer scalar kinetic parameters at a full SOC or a few discrete SOC levels, preventing them from capturing the continuous SOC dependence that governs exothermic behavior during abuse conditions. To address this, we apply the Kolmogorov-Arnold Chemical Reaction Neural Network (KA-CRNN) framework to learn continuous and realistic SOC-dependent exothermic cathode-electrolyte interactions. We apply a physics-encoded KA-CRNN to learn SOC-dependent kinetic parameters for cathode-electrolyte decomposition directly from differential scanning calorimetry (DSC) data. A mechanistically informed reaction pathway is embedded into the network architecture, enabling the activation energies, pre-exponential factors, enthalpies, and related parameters to be represented as continuous and fully interpretable functions of the SOC. The framework is demonstrated for NCA, NM, and NMA cathodes, yielding models that reproduce DSC heat-release features across all SOCs and provide interpretable insight into SOC-dependent oxygen-release and phase-transformation mechanisms. This approach establishes a foundation for extending kinetic parameter dependencies to additional environmental and electrochemical variables, supporting more accurate and interpretable thermal-runaway prediction and monitoring.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness</title>
<link>https://arxiv.org/abs/2512.15634</link>
<guid>https://arxiv.org/abs/2512.15634</guid>
<content:encoded><![CDATA[

arXiv:2512.15634v1 Announce Type: cross 
Abstract: Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&amp;A tasks and generalisation. In this work, we perform a comprehensive evaluation across multiple reasoning and recall datasets, conducting a rank sweep to quantify the trade-off between SFT and PEFT. We also compare the accuracy of PEFT and SFT models across in-domain and out-of-domain adaptation, highlighting distinct generalisation behaviour and task-specific forgetting. We demonstrate that LoRA achieves competitive and in some cases superior performance compared to SFT, particularly on reasoning tasks at specific rank values. Additionally, we analyze the internal representations via spectral features and layer-wise attention structures, offering insights into representational drift and structural changes in attention patterns.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning</title>
<link>https://arxiv.org/abs/2512.15658</link>
<guid>https://arxiv.org/abs/2512.15658</guid>
<content:encoded><![CDATA[

arXiv:2512.15658v1 Announce Type: cross 
Abstract: Continual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks. In PPSEBM, progressive parameter selection allocates distinct, task-specific parameters for each new task, while the EBM generates representative pseudo-samples from prior tasks. These generated samples actively inform and guide the parameter selection process, enhancing the model's ability to retain past knowledge while adapting to new tasks. Experimental results on diverse NLP benchmarks demonstrate that PPSEBM outperforms state-of-the-art continual learning methods, offering a promising and robust solution to mitigate catastrophic forgetting.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prospects for quantum advantage in machine learning from the representability of functions</title>
<link>https://arxiv.org/abs/2512.15661</link>
<guid>https://arxiv.org/abs/2512.15661</guid>
<content:encoded><![CDATA[

arXiv:2512.15661v1 Announce Type: cross 
Abstract: Demonstrating quantum advantage in machine learning tasks requires navigating a complex landscape of proposed models and algorithms. To bring clarity to this search, we introduce a framework that connects the structure of parametrized quantum circuits to the mathematical nature of the functions they can actually learn. Within this framework, we show how fundamental properties, like circuit depth and non-Clifford gate count, directly determine whether a model's output leads to efficient classical simulation or surrogation. We argue that this analysis uncovers common pathways to dequantization that underlie many existing simulation methods. More importantly, it reveals critical distinctions between models that are fully simulatable, those whose function space is classically tractable, and those that remain robustly quantum. This perspective provides a conceptual map of this landscape, clarifying how different models relate to classical simulability and pointing to where opportunities for quantum advantage may lie.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</title>
<link>https://arxiv.org/abs/2512.15674</link>
<guid>https://arxiv.org/abs/2512.15674</guid>
<content:encoded><![CDATA[

arXiv:2512.15674v1 Announce Type: cross 
Abstract: Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stylized Synthetic Augmentation further improves Corruption Robustness</title>
<link>https://arxiv.org/abs/2512.15675</link>
<guid>https://arxiv.org/abs/2512.15675</guid>
<content:encoded><![CDATA[

arXiv:2512.15675v1 Announce Type: cross 
Abstract: This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common FID metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Dimensional Partial Least Squares: Spectral Analysis and Fundamental Limitations</title>
<link>https://arxiv.org/abs/2512.15684</link>
<guid>https://arxiv.org/abs/2512.15684</guid>
<content:encoded><![CDATA[

arXiv:2512.15684v1 Announce Type: cross 
Abstract: Partial Least Squares (PLS) is a widely used method for data integration, designed to extract latent components shared across paired high-dimensional datasets. Despite decades of practical success, a precise theoretical understanding of its behavior in high-dimensional regimes remains limited. In this paper, we study a data integration model in which two high-dimensional data matrices share a low-rank common latent structure while also containing individual-specific components. We analyze the singular vectors of the associated cross-covariance matrix using tools from random matrix theory and derive asymptotic characterizations of the alignment between estimated and true latent directions. These results provide a quantitative explanation of the reconstruction performance of the PLS variant based on Singular Value Decomposition (PLS-SVD) and identify regimes where the method exhibits counter-intuitive or limiting behavior. Building on this analysis, we compare PLS-SVD with principal component analysis applied separately to each dataset and show its asymptotic superiority in detecting the common latent subspace. Overall, our results offer a comprehensive theoretical understanding of high-dimensional PLS-SVD, clarifying both its advantages and fundamental limitations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</title>
<link>https://arxiv.org/abs/2512.15692</link>
<guid>https://arxiv.org/abs/2512.15692</guid>
<content:encoded><![CDATA[

arXiv:2512.15692v1 Announce Type: cross 
Abstract: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Rebatching for Efficient Early-Exit Inference with DREX</title>
<link>https://arxiv.org/abs/2512.15705</link>
<guid>https://arxiv.org/abs/2512.15705</guid>
<content:encoded><![CDATA[

arXiv:2512.15705v1 Announce Type: cross 
Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</title>
<link>https://arxiv.org/abs/2512.15712</link>
<guid>https://arxiv.org/abs/2512.15712</guid>
<content:encoded><![CDATA[

arXiv:2512.15712v1 Announce Type: cross 
Abstract: Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension</title>
<link>https://arxiv.org/abs/2302.13849</link>
<guid>https://arxiv.org/abs/2302.13849</guid>
<content:encoded><![CDATA[

arXiv:2302.13849v4 Announce Type: replace 
Abstract: A classical result in online learning characterizes the optimal mistake bound achievable by deterministic learners using the Littlestone dimension (Littlestone '88). We prove an analogous result for randomized learners: we show that the optimal expected mistake bound in learning a class $\mathcal{H}$ equals its randomized Littlestone dimension, which is the largest $d$ for which there exists a tree shattered by $\mathcal{H}$ whose average depth is $2d$. We further study optimal mistake bounds in the agnostic case, as a function of the number of mistakes made by the best function in $\mathcal{H}$, denoted by $k$. We show that the optimal randomized mistake bound for learning a class with Littlestone dimension $d$ is $k + \Theta (\sqrt{k d} + d )$. This also implies an optimal deterministic mistake bound of $2k + \Theta(d) + O(\sqrt{k d})$, thus resolving an open question which was studied by Auer and Long ['99].
  As an application of our theory, we revisit the classical problem of prediction using expert advice: about 30 years ago Cesa-Bianchi, Freund, Haussler, Helmbold, Schapire and Warmuth studied prediction using expert advice, provided that the best among the $n$ experts makes at most $k$ mistakes, and asked what are the optimal mistake bounds. Cesa-Bianchi, Freund, Helmbold, and Warmuth ['93, '96] provided a nearly optimal bound for deterministic learners, and left the randomized case as an open problem. We resolve this question by providing an optimal learning rule in the randomized case, and showing that its expected mistake bound equals half of the deterministic bound of Cesa-Bianchi et al. ['93,'96], up to negligible additive terms. In contrast with previous works by Abernethy, Langford, and Warmuth ['06], and by Br\^anzei and Peres ['19], our result applies to all pairs $n,k$.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SketchOGD: Memory-Efficient Continual Learning</title>
<link>https://arxiv.org/abs/2305.16424</link>
<guid>https://arxiv.org/abs/2305.16424</guid>
<content:encoded><![CDATA[

arXiv:2305.16424v3 Announce Type: replace 
Abstract: When machine learning models are trained continually on a sequence of tasks, they are often liable to forget what they learned on previous tasks--a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper develops a memory-efficient solution to catastrophic forgetting using the idea of matrix sketching, in the context of a simple continual learning algorithm known as orthogonal gradient descent (OGD). OGD finds weight updates that aim to preserve performance on prior datapoints, using gradients of the model on those datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over long time horizons. To address this problem, we propose SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fixed, user-determined size. In contrast to existing memory-efficient variants of OGD, SketchOGD runs online without the need for advance knowledge of the total number of tasks, is simple to implement, and is more amenable to analysis. We provide theoretical guarantees on the approximation error of the relevant sketches under a novel metric suited to the downstream task of OGD. Experimentally, we find that SketchOGD tends to outperform current state-of-the-art variants of OGD given a fixed memory budget.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Continual Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2402.08182</link>
<guid>https://arxiv.org/abs/2402.08182</guid>
<content:encoded><![CDATA[

arXiv:2402.08182v2 Announce Type: replace 
Abstract: Continual Test-Time Adaptation (CTTA) task investigates effective domain adaptation under the scenario of continuous domain shifts during testing time. Due to the utilization of solely unlabeled samples, there exists significant uncertainty in model updates, leading CTTA to encounter severe error accumulation issues. In this paper, we introduce VCoTTA, a variational Bayesian approach to measure uncertainties in CTTA. At the source stage, we transform a pretrained deterministic model into a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting uncertainties into the model. During the testing time, we employ a mean-teacher update strategy using variational inference for the student model and exponential moving average for the teacher model. Our novel approach updates the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating error accumulation within the CTTA framework.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning</title>
<link>https://arxiv.org/abs/2403.13522</link>
<guid>https://arxiv.org/abs/2403.13522</guid>
<content:encoded><![CDATA[

arXiv:2403.13522v3 Announce Type: replace 
Abstract: Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning (CIL) without available historical training samples as exemplars. Compared with its exemplar-based CIL counterpart that stores exemplars, EFCIL suffers more from forgetting issues. Recently, a new EFCIL branch named Analytic Continual Learning (ACL) introduces a gradient-free paradigm via Recursive Least-Square, achieving a forgetting-resistant classifier training with a frozen backbone during CIL. However, existing ACL suffers from ineffective representations and insufficient utilization of backbone knowledge. In this paper, we propose a representation-enhanced analytic learning (REAL) to address these problems. To enhance the representation, REAL constructs a dual-stream base pretraining followed by representation enhancing distillation process. The dual-stream base pretraining combines self-supervised contrastive learning for general features and supervised learning for class-specific knowledge, followed by the representation enhancing distillation to merge both streams, enhancing representations for subsequent CIL paradigm. To utilize more knowledge from the backbone, REAL presents a feature fusion buffer to multi-layer backbone features, providing informative features for the subsequent classifier training. Our method can be incorporated into existing ACL techniques and provides more competitive performance. Empirical results demonstrate that, REAL achieves state-of-the-art performance on CIFAR-100, ImageNet-100 and ImageNet-1k benchmarks, outperforming exemplar-free methods and rivaling exemplar-based approaches.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Over-parameterization and Adversarial Robustness in Neural Networks: An Overview and Empirical Analysis</title>
<link>https://arxiv.org/abs/2406.10090</link>
<guid>https://arxiv.org/abs/2406.10090</guid>
<content:encoded><![CDATA[

arXiv:2406.10090v2 Announce Type: replace 
Abstract: Thanks to their extensive capacity, over-parameterized neural networks exhibit superior predictive capabilities and generalization. However, having a large parameter space is considered one of the main suspects of the neural networks' vulnerability to adversarial example -- input samples crafted ad-hoc to induce a desired misclassification. Relevant literature has claimed contradictory remarks in support of and against the robustness of over-parameterized networks. These contradictory findings might be due to the failure of the attack employed to evaluate the networks' robustness. Previous research has demonstrated that depending on the considered model, the algorithm employed to generate adversarial examples may not function properly, leading to overestimating the model's robustness. In this work, we empirically study the robustness of over-parameterized networks against adversarial examples. However, unlike the previous works, we also evaluate the considered attack's reliability to support the results' veracity. Our results show that over-parameterized networks are robust against adversarial attacks as opposed to their under-parameterized counterparts.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imbalances in Neurosymbolic Learning: Characterization and Mitigating Strategies</title>
<link>https://arxiv.org/abs/2407.10000</link>
<guid>https://arxiv.org/abs/2407.10000</guid>
<content:encoded><![CDATA[

arXiv:2407.10000v4 Announce Type: replace 
Abstract: We study one of the most popular problems in **neurosymbolic learning** (NSL), that of learning neural classifiers given only the result of applying a symbolic component $\sigma$ to the gold labels of the elements of a vector $\mathbf x$. The gold labels of the elements in $\mathbf x$ are unknown to the learner. We make multiple contributions, theoretical and practical, to address a problem that has not been studied so far in this context, that of characterizing and mitigating *learning imbalances*, i.e., major differences in the errors that occur when classifying instances of different classes (aka **class-specific risks**). Our theoretical analysis reveals a unique phenomenon: that $\sigma$ can greatly impact learning imbalances. This result sharply contrasts with previous research on supervised and weakly supervised learning, which only studies learning imbalances under data imbalances. On the practical side, we introduce a technique for estimating the marginal of the hidden gold labels using weakly supervised data. Then, we introduce algorithms that mitigate imbalances at training and testing time by treating the marginal of the hidden labels as a constraint. We demonstrate the effectiveness of our techniques using strong baselines from NSL and long-tailed learning, suggesting performance improvements of up to 14%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset</title>
<link>https://arxiv.org/abs/2411.17645</link>
<guid>https://arxiv.org/abs/2411.17645</guid>
<content:encoded><![CDATA[

arXiv:2411.17645v4 Announce Type: replace 
Abstract: The use of machine learning and AI on electronic health records (EHRs) holds substantial potential for clinical insight. However, this approach faces challenges due to data heterogeneity, sparsity, temporal misalignment, and limited labeled outcomes. In this context, we leverage a linked EHR dataset of approximately one million de-identified individuals from Bristol, North Somerset, and South Gloucestershire, UK, to characterize urinary tract infections (UTIs). We implemented a data pre-processing and curation pipeline that transforms the raw EHR data into a structured format suitable for developing predictive models focused on data fairness, accountability and transparency. Given the limited availability and biases of ground truth UTI outcomes, we introduce a UTI risk estimation framework informed by clinical expertise to estimate UTI risk across individual patient timelines. Pairwise XGBoost models are trained using this framework to differentiate UTI risk categories with explainable AI techniques applied to identify key predictors and support interpretability. Our findings reveal differences in clinical and demographic predictors across risk groups. While this study highlights the potential of AI-driven insights to support UTI clinical decision-making, further investigation of patient sub-strata and extensive validation are needed to ensure robustness and applicability in clinical practice.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveGNN: Integrating Graph Neural Networks and Transformers for Decay-Aware Classification of Irregular Clinical Time-Series</title>
<link>https://arxiv.org/abs/2412.10621</link>
<guid>https://arxiv.org/abs/2412.10621</guid>
<content:encoded><![CDATA[

arXiv:2412.10621v2 Announce Type: replace 
Abstract: Clinical time series are often irregularly sampled, with varying sensor frequencies, missing observations, and misaligned timestamps. Prior approaches typically address these irregularities by interpolating data into regular sequences, thereby introducing bias, or by generating inconsistent and uninterpretable relationships across sensor measurements, complicating the accurate learning of both intra-series and inter-series dependencies. We introduce WaveGNN, a model that operates directly on irregular multivariate time series without interpolation or conversion to a regular representation. WaveGNN combines a decay-aware Transformer to capture intra-series dynamics with a sample-specific graph neural network that models both short-term and long-term inter-sensor relationships. Therefore, it generates a single, sparse, and interpretable graph per sample. Across multiple benchmark datasets (P12, P19, MIMIC-III, and PAM), WaveGNN delivers consistently strong performance, whereas other state-of-the-art baselines tend to perform well on some datasets or tasks but poorly on others. While WaveGNN does not necessarily surpass every method in every case, its consistency and robustness across diverse settings set it apart. Moreover, the learned graphs align well with known physiological structures, enhancing interpretability and supporting clinical decision-making.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Temporal Anomaly Causality Discovery in Large Systems: Achieving Computational Efficiency with Binary Anomaly Flag Data</title>
<link>https://arxiv.org/abs/2412.11800</link>
<guid>https://arxiv.org/abs/2412.11800</guid>
<content:encoded><![CDATA[

arXiv:2412.11800v2 Announce Type: replace 
Abstract: Extracting anomaly causality facilitates diagnostics once monitoring systems detect system faults. Identifying anomaly causes in large systems involves investigating a broader set of monitoring variables across multiple subsystems. However, learning graphical causal models (GCMs) comes with a significant computational burden that restrains the applicability of most existing methods in real-time and large-scale deployments. In addition, modern monitoring applications for large systems often generate large amounts of binary alarm flags, and the distinct characteristics of binary anomaly data -- the meaning of state transition and data sparsity -- challenge existing causality learning mechanisms. This study proposes an anomaly causal discovery approach (\textsc{AnomalyCD}), addressing the accuracy and computational challenges of generating GCMs from temporal binary flag datasets. The \textsc{AnomalyCD} presents several strategies, such as anomaly data-aware causality testing, sparse data and prior link compression, and edge pruning adjustment approaches. We validate the performance of of the approach on two datasets: monitoring sensor data of the readout-box system of the Compact Muon Solenoid experiment at CERN, and a public data set for information technology monitoring. The results on temporal GCMs demonstrate a considerable reduction of computation overhead and a moderate enhancement of accuracy on the binary anomaly data sets. Source code: https://github.com/muleina/AnomalyCD .
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Bayesian Optimization via Focalized Sparse Gaussian Processes</title>
<link>https://arxiv.org/abs/2412.20375</link>
<guid>https://arxiv.org/abs/2412.20375</guid>
<content:encoded><![CDATA[

arXiv:2412.20375v2 Announce Type: replace 
Abstract: Bayesian optimization is an effective technique for black-box optimization, but its applicability is typically limited to low-dimensional and small-budget problems due to the cubic complexity of computing the Gaussian process (GP) surrogate. While various approximate GP models have been employed to scale Bayesian optimization to larger sample sizes, most suffer from overly-smooth estimation and focus primarily on problems that allow for large online samples. In this work, we argue that Bayesian optimization algorithms with sparse GPs can more efficiently allocate their representational power to relevant regions of the search space. To achieve this, we propose focalized GP, which leverages a novel variational loss function to achieve stronger local prediction, as well as FocalBO, which hierarchically optimizes the focalized GP acquisition function over progressively smaller search spaces. Experimental results demonstrate that FocalBO can efficiently leverage large amounts of offline and online data to achieve state-of-the-art performance on robot morphology design and to control a 585-dimensional musculoskeletal system.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine learning applications in archaeological practices: a review</title>
<link>https://arxiv.org/abs/2501.03840</link>
<guid>https://arxiv.org/abs/2501.03840</guid>
<content:encoded><![CDATA[

arXiv:2501.03840v5 Announce Type: replace 
Abstract: Artificial intelligence and machine learning applications in archaeology have increased significantly in recent years, and these now span all subfields, geographical regions, and time periods. The prevalence and success of these applications have remained largely unexamined, as recent reviews on the use of machine learning in archaeology have only focused only on specific subfields of archaeology. Our review examined an exhaustive corpus of 135 articles published between 1997 and 2022. We observed a significant increase in the number of publications from 2019 onwards. Automatic structure detection and artefact classification were the most represented tasks in the articles reviewed, followed by taphonomy, and archaeological predictive modelling. From the review, clustering and unsupervised methods were underrepresented compared to supervised models. Artificial neural networks and ensemble learning account for two thirds of the total number of models used. However, if machine learning models are gaining in popularity they remain subject to misunderstanding. We observed, in some cases, poorly defined requirements and caveats of the machine learning methods used. Furthermore, the goals and the needs of machine learning applications for archaeological purposes are in some cases unclear or poorly expressed. To address this, we proposed a workflow guide for archaeologists to develop coherent and consistent methodologies adapted to their research questions, project scale and data. As in many other areas, machine learning is rapidly becoming an important tool in archaeological research and practice, useful for the analyses of large and multivariate data, although not without limitations. This review highlights the importance of well-defined and well-reported structured methodologies and collaborative practices to maximise the potential of applications of machine learning methods in archaeology.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry and Optimization of Shallow Polynomial Networks</title>
<link>https://arxiv.org/abs/2501.06074</link>
<guid>https://arxiv.org/abs/2501.06074</guid>
<content:encoded><![CDATA[

arXiv:2501.06074v2 Announce Type: replace 
Abstract: We study shallow neural networks with monomial activations and output dimension one. The function space for these models can be identified with a set of symmetric tensors with bounded rank. We describe general features of these networks, focusing on the relationship between width and optimization. We then consider teacher-student problems, which can be viewed as problems of low-rank tensor approximation with respect to non-standard inner products that are induced by the data distribution. In this setting, we introduce a teacher-metric data discriminant which encodes the qualitative behavior of the optimization as a function of the training data distribution. Finally, we focus on networks with quadratic activations, presenting an in-depth analysis of the optimization landscape. In particular, we present a variation of the Eckart-Young Theorem characterizing all critical points and their Hessian signatures for teacher-student problems with quadratic networks and Gaussian training data.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Worth Their Weight: Randomized and Regularized Block Kaczmarz Algorithms without Preprocessing</title>
<link>https://arxiv.org/abs/2502.00882</link>
<guid>https://arxiv.org/abs/2502.00882</guid>
<content:encoded><![CDATA[

arXiv:2502.00882v3 Announce Type: replace 
Abstract: Due to the ever growing amounts of data leveraged for machine learning and scientific computing, it is increasingly important to develop algorithms that sample only a small portion of the data at a time. In the case of linear least-squares, the randomized block Kaczmarz method (RBK) is an appealing example of such an algorithm, but its convergence is only understood under sampling distributions that require potentially prohibitively expensive preprocessing steps. To address this limitation, we analyze RBK when the data is sampled uniformly, showing that its iterates converge in a Monte Carlo sense to a $\textit{weighted}$ least-squares solution. Unfortunately, for general problems the bias of the weighted least-squares solution and the variance of the iterates can become arbitrarily large. We show that these quantities can be rigorously controlled by incorporating regularization into the RBK iterations, yielding the regularized algorithm ReBlocK. Numerical experiments including examples arising from natural gradient optimization demonstrate that ReBlocK can outperform both RBK and minibatch stochastic gradient descent for inconsistent problems with rapidly decaying singular values.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Quantum Optimization with Continuous Bandits</title>
<link>https://arxiv.org/abs/2502.04021</link>
<guid>https://arxiv.org/abs/2502.04021</guid>
<content:encoded><![CDATA[

arXiv:2502.04021v2 Announce Type: replace 
Abstract: We introduce a novel approach to variational Quantum algorithms (VQA) via continuous bandits. VQA are a class of hybrid Quantum-classical algorithms where the parameters of Quantum circuits are optimized by classical algorithms. Previous work has used zero and first order gradient based methods, however such algorithms suffer from the barren plateau (BP) problem where gradients and loss differences are exponentially small. We introduce an approach using bandits methods which combine global exploration with local exploitation. We show how VQA can be formulated as a best arm identification problem in a continuous space of arms with Lipschitz smoothness. While regret minimization has been addressed in this setting, existing methods for pure exploration only cover discrete spaces. We give the first results for pure exploration in a continuous setting and derive a fixed-confidence, information-theoretic, instance specific lower bound. Under certain assumptions on the expected payoff, we derive a simple algorithm, which is near-optimal with respect to our lower bound. Finally, we apply our continuous bandit algorithm to two VQA schemes: a PQC and a QAOA quantum circuit, showing that we significantly outperform the previously known state of the art methods (which used gradient based methods).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Efficient Optimization over Generative Priors via Coarse Learnability</title>
<link>https://arxiv.org/abs/2503.06917</link>
<guid>https://arxiv.org/abs/2503.06917</guid>
<content:encoded><![CDATA[

arXiv:2503.06917v3 Announce Type: replace 
Abstract: In zeroth-order optimization, we seek to minimize a function $d(\cdot)$, which may encode combinatorial feasibility, using only function evaluations. We focus on the setting where solutions must also satisfy qualitative constraints or conform to a complex prior distribution. To address this, we introduce a new framework in which such constraints are represented by an initial generative prior $\L(\cdot)$, for example, a Large Language Model (LLM). The objective is to find solutions $s$ that minimize $d(s)$ while having high probability under $\L(s)$, effectively sampling from a target distribution proportional to $\L(s) \cdot e^{-T \cdot d(s)}$ for a temperature parameter $T$.
  While this framework aligns with classical Model-Based Optimization (e.g., the Cross-Entropy method), existing theory is ill-suited for deriving sample complexity bounds in black-box deep generative models. We therefore propose a novel learning assumption, which we term \emph{coarse learnability}, where an agent with access to a polynomial number of samples can learn a model whose point-wise density approximates the target within a polynomial factor. Leveraging this assumption, we design an iterative algorithm that employs a Metropolis-Hastings correction to provably approximate the target distribution using a polynomial number of samples. To the best of our knowledge, this is one of the first works to establish such sample-complexity guarantees for model-based optimization with deep generative priors.
  We provide two lines of evidence supporting the coarse learnability assumption. Theoretically, we show that maximum likelihood estimation naturally induces the required coverage properties, holding for both standard exponential families and for misspecified models. Empirically, we demonstrate that LLMs can adapt their learned distributions to zeroth-order feedback to solve combinatorial optimization problems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch</title>
<link>https://arxiv.org/abs/2503.19779</link>
<guid>https://arxiv.org/abs/2503.19779</guid>
<content:encoded><![CDATA[

arXiv:2503.19779v2 Announce Type: replace 
Abstract: Machine learning (ML) workloads launch hundreds to thousands of short-running GPU kernels per iteration. With GPU compute throughput growing rapidly, CPU-side launch latency of kernels is emerging as a bottleneck. CUDA Graphs promise to address this by replaying a set of kernels with a single dispatch of the graph, removing per-kernel launch costs. However, CUDA Graphs remain surprisingly difficult to deploy correctly and efficiently.
  We present PyGraph - a compiler framework to maximize the coverage and benefits of CUDA Graphs for ML workloads. It introduces three novel optimizations: it applies automatic code transformations to make ML applications amenable to CUDA Graphs; it eliminates the parameter copy overheads for kernels executing in CUDA Graphs, and it selectively deploys CUDA Graphs guided by a cost-benefit analysis. For 25 ML workloads from TorchBench, HuggingFace, and TIMM, PyGraph more than doubles the benefit from deploying CUDA Graph compared to the most popular and widely used ML compiler, PyTorch2. PyGraph is built atop PyTorch2's compilation framework and requires no programmer intervention.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CANet: ChronoAdaptive Network for Enhanced Long-Term Time Series Forecasting under Non-Stationarity</title>
<link>https://arxiv.org/abs/2504.17913</link>
<guid>https://arxiv.org/abs/2504.17913</guid>
<content:encoded><![CDATA[

arXiv:2504.17913v2 Announce Type: replace 
Abstract: Long-term time series forecasting plays a pivotal role in various real-world applications. Despite recent advancements and the success of different architectures, forecasting is often challenging due to non-stationary nature of the real-world data, which frequently exhibit distribution shifts and temporal changes in statistical properties like mean and variance over time. Previous studies suggest that this inherent variability complicates forecasting, limiting the performance of many models by leading to loss of non-stationarity and resulting in over-stationarization (Liu, Wu, Wang and Long, 2022). To address this challenge, we introduce a novel architecture, ChoronoAdaptive Network (CANet), inspired by style-transfer techniques. The core of CANet is the Non-stationary Adaptive Normalization module, seamlessly integrating the Style Blending Gate and Adaptive Instance Normalization (AdaIN) (Huang and Belongie, 2017). The Style Blending Gate preserves and reintegrates non-stationary characteristics, such as mean and standard deviation, by blending internal and external statistics, preventing over-stationarization while maintaining essential temporal dependencies. Coupled with AdaIN, which dynamically adapts the model to statistical changes, this approach enhances predictive accuracy under non-stationary conditions. CANet also employs multi-resolution patching to handle short-term fluctuations and long-term trends, along with Fourier analysis-based adaptive thresholding to reduce noise. A Stacked Kronecker Product Layer further optimizes the model's efficiency while maintaining high performance. Extensive experiments on real-world datasets validate CANet's superiority over state-of-the-art methods, achieving a 42% reduction in MSE and a 22% reduction in MAE. The source code is publicly available at https://github.com/mertsonmezer/CANet.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume</title>
<link>https://arxiv.org/abs/2505.06292</link>
<guid>https://arxiv.org/abs/2505.06292</guid>
<content:encoded><![CDATA[

arXiv:2505.06292v2 Announce Type: replace 
Abstract: Graph Neural Networks have shown strong performance in traffic volume forecasting, particularly on highways and major arterial networks. Applying them to urban settings, however, presents unique challenges: urban networks exhibit greater structural diversity, traffic volumes are highly overdispersed with many zeros, the best way to account for spatial dependencies remains unclear, and sensor coverage is often very sparse. We introduce the Graph Neural Network for Urban Interpolation (GNNUI), a novel urban traffic volume estimation approach. GNNUI employs a masking algorithm to learn interpolation, integrates node features to capture functional roles, and uses a loss function tailored to zero-inflated traffic distributions. In addition to the model, we introduce two new open, large-scale urban traffic volume benchmarks, covering different transportation modes: Strava cycling data from Berlin and New York City taxi data. GNNUI outperforms recent, some graph-based, interpolation methods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence) and remains robust from 90% to 1% sensor coverage. For example, on the Strava dataset, the MAE increases only from 7.1 to 10.5, and on the Taxi dataset, from 23.0 to 40.4. These results demonstrate that GNNUI maintains strong performance despite extreme data scarcity, a common condition in real-world urban settings. We also examine how graph connectivity choices influence model accuracy.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aligned Protein Language Model</title>
<link>https://arxiv.org/abs/2505.16896</link>
<guid>https://arxiv.org/abs/2505.16896</guid>
<content:encoded><![CDATA[

arXiv:2505.16896v2 Announce Type: replace 
Abstract: Protein language models (pLMs) pre-trained on vast protein sequence databases excel at various downstream tasks but often lack the structural knowledge essential for some biological applications. To address this, we introduce a method to enrich pLMs with structural knowledge by leveraging pre-trained protein graph neural networks (pGNNs). First, a latent-level contrastive learning task aligns residue representations from pLMs with those from pGNNs across multiple proteins, injecting inter-protein structural information. Additionally, a physical-level task integrates intra-protein information by training pLMs to predict structure tokens. Together, the proposed dual-task framework effectively incorporates both inter- and intra-protein structural knowledge into pLMs. Given the variability in the quality of protein structures in PDB, we further introduce a residue loss selection module that uses a small model trained on high-quality structures to select reliable yet challenging residue losses for the pLM to learn. Applying our structure alignment method as a simple, lightweight post-training step to the state-of-the-art ESM2 and AMPLIFY yields notable performance gains. These improvements are consistent across a wide range of tasks, including substantial gains in deep mutational scanning (DMS) fitness prediction and a 59% increase in P@L for ESM2 650M contact prediction on CASP16. Furthermore, we demonstrate that these performance gains are robust, scaling with model sizes from 8M to 650M and extending to different downstream tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration</title>
<link>https://arxiv.org/abs/2505.23049</link>
<guid>https://arxiv.org/abs/2505.23049</guid>
<content:encoded><![CDATA[

arXiv:2505.23049v2 Announce Type: replace 
Abstract: Pruning is a widely used technique to compress large language models (LLMs) by removing unimportant weights, but it often suffers from significant performance degradation - especially under semi-structured sparsity constraints. Existing pruning methods primarily focus on estimating the importance of individual weights, which limits their ability to preserve critical capabilities of the model. In this work, we propose a new perspective: rather than merely selecting which weights to prune, we first redistribute parameter importance to make the model inherently more amenable to pruning. By minimizing the information entropy of normalized importance scores, our approach concentrates importance onto a smaller subset of weights, thereby enhancing pruning robustness. We instantiate this idea through DenoiseRotator, which applies learnable orthogonal transformations to the model's weight matrices. Our method can be seamlessly integrated with existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4 semi-structured sparsity, DenoiseRotator consistently improves perplexity and zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4 semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are available at https://github.com/Axel-gu/DenoiseRotator.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bidirectional predictive coding</title>
<link>https://arxiv.org/abs/2505.23415</link>
<guid>https://arxiv.org/abs/2505.23415</guid>
<content:encoded><![CDATA[

arXiv:2505.23415v2 Announce Type: replace 
Abstract: Predictive coding (PC) is an influential computational model of visual learning and inference in the brain. Classical PC was proposed as a top-down generative model, where the brain actively predicts upcoming visual inputs, and inference minimises the prediction errors. Recent studies have also shown that PC can be formulated as a discriminative model, where sensory inputs predict neural activities in a feedforward manner. However, experimental evidence suggests that the brain employs both generative and discriminative inference, while unidirectional PC models show degraded performance in tasks requiring bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC model that incorporates both generative and discriminative inference while maintaining a biologically plausible circuit implementation. We show that bPC matches or outperforms unidirectional models in their specialised generative or discriminative tasks, by developing an energy landscape that simultaneously suits both tasks. We also demonstrate bPC's superior performance in two biologically relevant tasks including multimodal learning and inference with missing information, suggesting that bPC resembles biological visual inference more closely.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HI-SQL: Optimizing Text-to-SQL Systems through Dynamic Hint Integration</title>
<link>https://arxiv.org/abs/2506.18916</link>
<guid>https://arxiv.org/abs/2506.18916</guid>
<content:encoded><![CDATA[

arXiv:2506.18916v2 Announce Type: replace 
Abstract: Text-to-SQL generation bridges the gap between natural language and databases, enabling users to query data without requiring SQL expertise. While large language models (LLMs) have significantly advanced the field, challenges remain in handling complex queries that involve multi-table joins, nested conditions, and intricate operations. Existing methods often rely on multi-step pipelines that incur high computational costs, increase latency, and are prone to error propagation. To address these limitations, we propose HI-SQL, a pipeline that incorporates a novel hint generation mechanism utilizing historical query logs to guide SQL generation. By analyzing prior queries, our method generates contextual hints that focus on handling the complexities of multi-table and nested operations. These hints are seamlessly integrated into the SQL generation process, eliminating the need for costly multi-step approaches and reducing reliance on human-crafted prompts. Experimental evaluations on multiple benchmark datasets demonstrate that our approach significantly improves query accuracy of LLM-generated queries while ensuring efficiency in terms of LLM calls and latency, offering a robust and practical solution for enhancing Text-to-SQL systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination</title>
<link>https://arxiv.org/abs/2507.10532</link>
<guid>https://arxiv.org/abs/2507.10532</guid>
<content:encoded><![CDATA[

arXiv:2507.10532v3 Announce Type: replace 
Abstract: Reasoning in large language models has long been a central research focus, and recent studies employing reinforcement learning (RL) have introduced diverse methods that yield substantial performance gains with minimal or even no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance performance. However, these breakthroughs are predominantly observed for the mathematically strong Qwen2.5 series on benchmarks such as MATH-500, AMC, and AIME, and seldom transfer to models like Llama, which warrants a more in-depth investigation. In this work, our empirical analysis reveals that pre-training on massive web-scale corpora leaves Qwen2.5 susceptible to data contamination in widely used benchmarks. Consequently, conclusions derived from contaminated benchmarks on Qwen2.5 series may be unreliable. To obtain trustworthy evaluation results, we introduce a generator that creates fully clean arithmetic problems of arbitrary length and difficulty, dubbed RandomCalculation. Using this leakage-free dataset, we show that only accurate reward signals yield steady improvements that surpass the base model's performance boundary in mathematical reasoning, whereas random or incorrect rewards do not. Moreover, we conduct more fine-grained analyses to elucidate the factors underlying the different performance observed on the MATH-500 and RandomCalculation benchmarks. Consequently, we recommend that future studies evaluate models on uncontaminated benchmarks and, when feasible, test various model series to ensure trustworthy conclusions about RL and related methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Latency and Bandwidth: A Theoretical Framework and Adaptive Algorithm for Communication-Constrained Training</title>
<link>https://arxiv.org/abs/2507.17346</link>
<guid>https://arxiv.org/abs/2507.17346</guid>
<content:encoded><![CDATA[

arXiv:2507.17346v2 Announce Type: replace 
Abstract: Regional energy caps limit the growth of any single data center used for large-scale model training. This single-center training paradigm works when model size remains manageable, but exponential growth in the model size and computational demand challenges it. A natural alternative is to distribute training across multiple data centers over wide-area networks. This pools distributed resources, but suffers from high latency and low, time-varying bandwidth, sharply reducing throughout. Employing jointly gradient compression and delayed aggregation can alleviate communication problems, but introduces a complex three-way trade-off among compression ratio, staleness (delayed synchronization steps), and convergence rate. Existing work lacks theoretical guidance and can only propose fixed strategies, insensitive to computation and communication conditions. We address this with a new theoretical tool, decomposing the joint optimization problem into a traditional process plus multiple analyzable noise terms. Our analysis yields the first convergence rate for this setting and shows that increasing staleness exponentially amplifies the detrimental effect of compression. Leveraging these insights, we propose DeCo-SGD, which dynamically selects the compression ratio and staleness based on the real-time communication and computation conditions. DeCo-SGD achieves up to $5.07\times$ and $1.37\times$ speed-ups over distributed SGD and static strategy in high-latency and low, varying bandwidth networks, respectively.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory</title>
<link>https://arxiv.org/abs/2507.18183</link>
<guid>https://arxiv.org/abs/2507.18183</guid>
<content:encoded><![CDATA[

arXiv:2507.18183v2 Announce Type: replace 
Abstract: Training deep neural networks on real-world datasets is often hampered by the presence of noisy labels, which can be memorized by over-parameterized models, leading to significant degradation in generalization performance. While existing methods for learning with noisy labels (LNL) have made considerable progress, they fundamentally suffer from static snapshot evaluations and fail to leverage the rich temporal dynamics of learning evolution. In this paper, we propose ChronoSelect (chrono denoting its temporal nature), a novel framework featuring an innovative four-stage memory architecture that compresses prediction history into compact temporal distributions. Our unique sliding update mechanism with controlled decay maintains only four dynamic memory units per sample, progressively emphasizing recent patterns while retaining essential historical knowledge. This enables precise three-way sample partitioning into clean, boundary, and noisy subsets through temporal trajectory analysis and dual-branch consistency. Theoretical guarantees prove the mechanism's convergence and stability under noisy conditions. Extensive experiments demonstrate ChronoSelect's state-of-the-art performance across synthetic and real-world benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution</title>
<link>https://arxiv.org/abs/2507.23600</link>
<guid>https://arxiv.org/abs/2507.23600</guid>
<content:encoded><![CDATA[

arXiv:2507.23600v4 Announce Type: replace 
Abstract: Signal unmixing analysis decomposes data into basic patterns and is widely applied in chemical and biological research. Multivariate curve resolution (MCR), a branch of signal unmixing, separates mixed signals into components (base patterns) and their concentrations (intensity), playing a key role in understanding composition. Classical MCR is typically framed as matrix factorization (MF) and requires a user-specified number of components, usually unknown in real data. Once data or component number increases, the scalability of these MCR approaches face significant challenges. This study reformulates MCR as a data generative process (gMCR), and introduces an Energy-Based solver, EB-gMCR, that automatically discovers the smallest component set and their concentrations for reconstructing the mixed signals faithfully. On synthetic benchmarks with up to 256 components, EB-gMCR attains high reconstruction fidelity and recovers the component count within 5% at 20dB noise and near-exact at 30dB. On two public spectral datasets, it identifies the correct component count and improves component separation over MF-based MCR approaches (NMF variants, ICA, MCR-ALS). EB-gMCR is a general solver for fixed-pattern signal unmixing (components remain invariant across mixtures). Domain priors (non-negativity, nonlinear mixing) enter as plug-in modules, enabling adaptation to new instruments or domains without altering the core selection learning step. The source code is available at https://github.com/b05611038/ebgmcr_solver.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BubbleOKAN: A Physics-Informed Interpretable Neural Operator for High-Frequency Bubble Dynamics</title>
<link>https://arxiv.org/abs/2508.03965</link>
<guid>https://arxiv.org/abs/2508.03965</guid>
<content:encoded><![CDATA[

arXiv:2508.03965v3 Announce Type: replace 
Abstract: In this work, we employ physics-informed neural operators to map pressure profiles from an input function space to the corresponding bubble radius responses. Our approach employs a two-step DeepONet architecture. To address the intrinsic spectral bias of deep learning models, our model incorporates the Rowdy adaptive activation function, enhancing the representation of high-frequency features. Moreover, we introduce the Kolmogorov-Arnold network (KAN) based two-step DeepOKAN model, which enhances interpretability (often lacking in conventional multilayer perceptron architectures) while efficiently capturing high-frequency bubble dynamics without explicit utilization of activation functions in any form. We particularly investigate the use of spline basis functions in combination with radial basis functions (RBF) within our architecture, as they demonstrate superior performance in constructing a universal basis for approximating high-frequency bubble dynamics compared to alternative formulations. Furthermore, we emphasize on the performance bottleneck of RBF while learning the high frequency bubble dynamics and showcase the advantage of using spline basis function for the trunk network in overcoming this inherent spectral bias. The model is systematically evaluated across three representative scenarios: (1) bubble dynamics governed by the Rayleigh-Plesset equation with a single initial radius, (2) bubble dynamics governed by the Keller-Miksis equation with a single initial radius, and (3) Keller-Miksis dynamics with multiple initial radii. We also compare our results with state-of-the-art neural operators, including Fourier Neural Operators, Wavelet Neural Operators, OFormer, and Convolutional Neural Operators. Our findings demonstrate that the two-step DeepOKAN accurately captures both low- and high-frequency behaviors, and offers a promising alternative to conventional numerical solvers.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Time-Integrated DeepONet: Temporal Tangent Space Operator Learning for High-Accuracy Inference</title>
<link>https://arxiv.org/abs/2508.05190</link>
<guid>https://arxiv.org/abs/2508.05190</guid>
<content:encoded><![CDATA[

arXiv:2508.05190v2 Announce Type: replace 
Abstract: Accurately modeling and inferring solutions to time-dependent partial differential equations (PDEs) over extended horizons remains a core challenge in scientific machine learning. Traditional full rollout (FR) methods, which predict entire trajectories in one pass, often fail to capture the causal dependencies and generalize poorly outside the training time horizon. Autoregressive (AR) approaches, evolving the system step by step, suffer from error accumulation, limiting long-term accuracy. These shortcomings limit the long-term accuracy and reliability of both strategies. To address these issues, we introduce the Physics-Informed Time-Integrated Deep Operator Network (PITI-DeepONet), a dual-output architecture trained via fully physics-informed or hybrid physics- and data-driven objectives to ensure stable, accurate long-term evolution well beyond the training horizon. Instead of forecasting future states, the network learns the time-derivative operator from the current state, integrating it using classical time-stepping schemes to advance the solution in time. Additionally, the framework can leverage residual monitoring during inference to estimate prediction quality and detect when the system transitions outside the training domain. Applied to benchmark problems, PITI-DeepONet shows improved accuracy over extended inference time horizons when compared to traditional methods. Mean relative $\mathcal{L}_2$ errors reduced by 84% (vs. FR) and 79% (vs. AR) for the one-dimensional heat equation; by 87% (vs. FR) and 98% (vs. AR) for the one-dimensional Burgers equation; and by 42% (vs. FR) and 89% (vs. AR) for the two-dimensional Allen-Cahn equation. By moving beyond classic FR and AR schemes, PITI-DeepONet paves the way for more reliable, long-term integration of complex, time-dependent PDEs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Verification of Graph Neural Networks with Incremental Constraint Solving</title>
<link>https://arxiv.org/abs/2508.09320</link>
<guid>https://arxiv.org/abs/2508.09320</guid>
<content:encoded><![CDATA[

arXiv:2508.09320v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes applications, such as fraud detection or healthcare, but are susceptible to adversarial attacks. A number of techniques have been proposed to provide adversarial robustness guarantees, but support for commonly used aggregation functions in message-passing GNNs is lacking. In this paper, we develop an exact (sound and complete) verification method for GNNs to compute guarantees against attribute and structural perturbations that involve edge addition or deletion, subject to budget constraints. Our method employs constraint solving with bound tightening, and iteratively solves a sequence of relaxed constraint satisfaction problems while relying on incremental solving capabilities of solvers to improve efficiency. We implement GNNev, a versatile exact verifier for message-passing neural networks, which supports three aggregation functions, sum, max and mean, with the latter two considered here for the first time. Extensive experimental evaluation of GNNev on real-world fraud datasets (Amazon and Yelp) and biochemical datasets (MUTAG and ENZYMES) demonstrates its usability and effectiveness, as well as superior performance for node classification and competitiveness on graph classification compared to existing exact verification tools on sum-aggregated GNNs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Tensor Decompositions for the Theory of Neural Networks</title>
<link>https://arxiv.org/abs/2508.18408</link>
<guid>https://arxiv.org/abs/2508.18408</guid>
<content:encoded><![CDATA[

arXiv:2508.18408v2 Announce Type: replace 
Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge of interest in providing a mathematical basis to deep learning theory. Low-rank tensor decompositions are specially befitting for this task due to their close connection to NNs and their rich theoretical results. Different tensor decompositions have strong uniqueness guarantees, which allow for a direct interpretation of their factors, and polynomial time algorithms have been proposed to compute them. Through the connections between tensors and NNs, such results supported many important advances in the theory of NNs. In this review, we show how low-rank tensor methods--which have been a core tool in the signal processing and machine learning communities--play a fundamental role in theoretically explaining different aspects of the performance of deep NNs, including their expressivity, algorithmic learnability and computational hardness, generalization, and identifiability. Our goal is to give an accessible overview of existing approaches (developed by different communities, ranging from computer science to mathematics) in a coherent and unified way, and to open a broader perspective on the use of low-rank tensor decompositions for the theory of deep NNs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2509.25300</link>
<guid>https://arxiv.org/abs/2509.25300</guid>
<content:encoded><![CDATA[

arXiv:2509.25300v2 Announce Type: replace 
Abstract: While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: 1.Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2.The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3.Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4.In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Foundation Models for Early Disease Detection</title>
<link>https://arxiv.org/abs/2510.01899</link>
<guid>https://arxiv.org/abs/2510.01899</guid>
<content:encoded><![CDATA[

arXiv:2510.01899v2 Announce Type: replace 
Abstract: Healthcare data now span EHRs, medical imaging, genomics, and wearable sensors, but most diagnostic models still process these modalities in isolation. This limits their ability to capture early, cross-modal disease signatures. This paper introduces a multimodal foundation model built on a transformer architecture that integrates heterogeneous clinical data through modality-specific encoders and cross-modal attention. Each modality is mapped into a shared latent space and fused using multi-head attention with residual normalization. We implement the framework using a multimodal dataset that simulates early-stage disease patterns across EHR sequences, imaging patches, genomic profiles, and wearable signals, including missing-modality scenarios and label noise. The model is trained using supervised classification together with self-supervised reconstruction and contrastive alignment to improve robustness. Experimental evaluation demonstrates strong performance in early-detection settings, with stable classification metrics, reliable uncertainty estimates, and interpretable attention patterns. The approach moves toward a flexible, pretrain-and-fine-tune foundation model that supports precision diagnostics, handles incomplete inputs, and improves early disease detection across oncology, cardiology, and neurology applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control-Augmented Autoregressive Diffusion for Data Assimilation</title>
<link>https://arxiv.org/abs/2510.06637</link>
<guid>https://arxiv.org/abs/2510.06637</guid>
<content:encoded><![CDATA[

arXiv:2510.06637v2 Announce Type: replace 
Abstract: Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments a pretrained ARDM with a lightweight controller network, trained offline by previewing future rollouts to output stepwise controls that anticipate upcoming observations under a terminal-cost objective. Our approach is motivated by viewing guided generation as an entropy-regularized stochastic optimal control problem over ARDM trajectories: we learn a reusable policy that injects small control corrections inside each denoising sub-step while remaining anchored to the pretrained dynamics. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), where existing methods can be computationally prohibitive and prone to forecast drift under sparse observations. At inference, DA reduces to a single causal forward rollout with on-the-fly corrections, requiring neither adjoint computations nor gradient-based optimization, and yields an order-of-magnitude speedup over strong diffusion-based DA baselines. Across two canonical PDEs and six observation regimes, our method consistently improves stability, accuracy, and physics-aware fidelity over state-of-the-art baselines. We will release code and checkpoints publicly.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs</title>
<link>https://arxiv.org/abs/2510.11062</link>
<guid>https://arxiv.org/abs/2510.11062</guid>
<content:encoded><![CDATA[

arXiv:2510.11062v4 Announce Type: replace 
Abstract: Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models.
  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Reinforcement Learning After Next-Token Prediction Facilitates Learning</title>
<link>https://arxiv.org/abs/2510.11495</link>
<guid>https://arxiv.org/abs/2510.11495</guid>
<content:encoded><![CDATA[

arXiv:2510.11495v2 Announce Type: replace 
Abstract: Recent advances in reasoning domains with neural networks have primarily been enabled by a training recipe that optimizes Large Language Models, previously trained to predict the next-token in a sequence, with reinforcement learning algorithms. We introduce a framework to study the success of this paradigm, and we theoretically expose the optimization mechanisms by which reinforcement learning improves over next-token prediction in this setting. We study learning from mixture distributions of short and long ``chain-of-thought'' sequences encoding a single task. In particular, when the task consists of predicting the parity of $d$ bits and long sequences are rare, we show how reinforcement learning after next-token prediction enables autoregressive transformers to generalize, whereas mere next-token prediction requires extreme statistical or computational resources to do so. We further explain how reinforcement learning leverages increased test-time computation, manifested in longer responses, to facilitate this learning process. In a simplified setting, we theoretically prove that autoregressive linear models following this training recipe can efficiently learn to predict the parity of $d$ bits as long as the proportion of long demonstrations in the data mix is not exponentially small in the input dimension $d$. Finally, we demonstrate these same phenomena in other settings, including the post-training of Llama-series models on mixture variations of common mathematical reasoning benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QLENS: Towards A Quantum Perspective of Language Transformers</title>
<link>https://arxiv.org/abs/2510.11963</link>
<guid>https://arxiv.org/abs/2510.11963</guid>
<content:encoded><![CDATA[

arXiv:2510.11963v2 Announce Type: replace 
Abstract: In natural language processing, current methods for understanding Transformers are successful at identifying intermediate predictions during a model's inference. However, these approaches function as limited diagnostic checkpoints, lacking a mathematical framework for mechanistically modeling how each layer facilitates transitions between these evolving states. This interpretability gap and past successes of interdisciplinary outlooks inspire us to turn to physics in search of a descriptive mathematical framework for Transformers. We observe that language models are intrinsically probabilistic, an attribute that is echoed in the core postulates of quantum mechanics. This parallel inspires us to translate insights from this discipline to that of natural language processing. Towards this objective, we propose QLENS a novel attempt to develop a physics-based perspective on the Transformer generation process. Under QLENS, a Transformer is studied by converting its latent activations into a state vector in a Hilbert space derived from the model's output units. This state subsequently evolves through hidden layers - reformulated as unitary operators and analogously defined Hamiltonians - during inference. The model's final probability distribution is obtained by applying the Born rule to the end state using a specific measurement operator. To demonstrate QLENS's potential, we conduct a proof-of-concept by probing a toy Transformer to investigate the influence of individual layers in a model's prediction trajectory. We present our work as a foundation for cross-domain insights to be leveraged towards a broader understanding of Transformers.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization</title>
<link>https://arxiv.org/abs/2510.12691</link>
<guid>https://arxiv.org/abs/2510.12691</guid>
<content:encoded><![CDATA[

arXiv:2510.12691v2 Announce Type: replace 
Abstract: Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amortized Active Generation of Pareto Sets</title>
<link>https://arxiv.org/abs/2510.21052</link>
<guid>https://arxiv.org/abs/2510.21052</guid>
<content:encoded><![CDATA[

arXiv:2510.21052v3 Announce Type: replace 
Abstract: We introduce active generation of Pareto sets (A-GPS), a new framework for online discrete black-box multi-objective optimization (MOO). A-GPS learns a generative model of the Pareto set that supports a-posteriori conditioning on user preferences. The method employs a class probability estimator (CPE) to predict non-dominance relations and to condition the generative model toward high-performing regions of the search space. We also show that this non-dominance CPE implicitly estimates the probability of hypervolume improvement (PHVI). To incorporate subjective trade-offs, A-GPS introduces preference direction vectors that encode user-specified preferences in objective space. At each iteration, the model is updated using both Pareto membership and alignment with these preference directions, producing an amortized generative model capable of sampling across the Pareto front without retraining. The result is a simple yet powerful approach that achieves high-quality Pareto set approximations, avoids explicit hypervolume computation, and flexibly captures user preferences. Empirical results on synthetic benchmarks and protein design tasks demonstrate strong sample efficiency and effective preference incorporation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tab-PET: Graph-Based Positional Encodings for Tabular Transformers</title>
<link>https://arxiv.org/abs/2511.13338</link>
<guid>https://arxiv.org/abs/2511.13338</guid>
<content:encoded><![CDATA[

arXiv:2511.13338v2 Announce Type: replace 
Abstract: Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems</title>
<link>https://arxiv.org/abs/2511.17435</link>
<guid>https://arxiv.org/abs/2511.17435</guid>
<content:encoded><![CDATA[

arXiv:2511.17435v2 Announce Type: replace 
Abstract: This paper addresses the cooperative Multi-Vehicle Dynamic Pickup and Delivery Problem with Stochastic Requests (MVDPDPSR) and proposes an end-to-end centralized decision-making framework based on sequence-to-sequence, named Multi-Agent Pointer Transformer (MAPT). MVDPDPSR is an extension of the vehicle routing problem and a spatio-temporal system optimization problem, widely applied in scenarios such as on-demand delivery. Classical operations research methods face bottlenecks in computational complexity and time efficiency when handling large-scale dynamic problems. Although existing reinforcement learning methods have achieved some progress, they still encounter several challenges: 1) Independent decoding across multiple vehicles fails to model joint action distributions; 2) The feature extraction network struggles to capture inter-entity relationships; 3) The joint action space is exponentially large. To address these issues, we designed the MAPT framework, which employs a Transformer Encoder to extract entity representations, combines a Transformer Decoder with a Pointer Network to generate joint action sequences in an AutoRegressive manner, and introduces a Relation-Aware Attention module to capture inter-entity relationships. Additionally, we guide the model's decision-making using informative priors to facilitate effective exploration. Experiments on 8 datasets demonstrate that MAPT significantly outperforms existing baseline methods in terms of performance and exhibits substantial computational time advantages compared to classical operations research methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity</title>
<link>https://arxiv.org/abs/2512.02826</link>
<guid>https://arxiv.org/abs/2512.02826</guid>
<content:encoded><![CDATA[

arXiv:2512.02826v2 Announce Type: replace 
Abstract: Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEA: Spectral Edge Attack</title>
<link>https://arxiv.org/abs/2512.08964</link>
<guid>https://arxiv.org/abs/2512.08964</guid>
<content:encoded><![CDATA[

arXiv:2512.08964v2 Announce Type: replace 
Abstract: Graph based machine learning algorithms occupy an important position in today AI landscape. The ability of graph topology to represent complex data structures is both the key strength of graph algorithms and a source of their vulnerability. In other words, attacking or perturbing a graph can severely degrade the performance of graph-based methods. For the attack methods, the greatest challenge is achieving strong attack effectiveness while remaining undetected. To address this problem, this paper proposes a new attack model that employs spectral adversarial robustness evaluation to quantitatively analyze the vulnerability of each edge in a graph under attack. By precisely targeting the weakest links, the proposed approach achieves the maximum attack impact with minimal perturbation. Experimental results demonstrate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Tensor Principal Component Analysis: Exact Recovery via Deterministic Model</title>
<link>https://arxiv.org/abs/2008.02211</link>
<guid>https://arxiv.org/abs/2008.02211</guid>
<content:encoded><![CDATA[

arXiv:2008.02211v2 Announce Type: replace-cross 
Abstract: Tensor, also known as multi-dimensional array, arises from many applications in signal processing, manufacturing processes, healthcare, among others. As one of the most popular methods in tensor literature, Robust tensor principal component analysis (RTPCA) is a very effective tool to extract the low rank and sparse components in tensors. In this paper, a new method to analyze RTPCA is proposed based on the recently developed tensor-tensor product and tensor singular value decomposition (t-SVD). Specifically, it aims to solve a convex optimization problem whose objective function is a weighted combination of the tensor nuclear norm and the l1-norm. In most of literature of RTPCA, the exact recovery is built on the tensor incoherence conditions and the assumption of a uniform model on the sparse support. Unlike this conventional way, in this paper, without any assumption of randomness, the exact recovery can be achieved in a completely deterministic fashion by characterizing the tensor rank-sparsity incoherence, which is an uncertainty principle between the low-rank tensor spaces and the pattern of sparse tensor.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>tensorflow-riemopt: A Library for Optimization on Riemannian Manifolds</title>
<link>https://arxiv.org/abs/2105.13921</link>
<guid>https://arxiv.org/abs/2105.13921</guid>
<content:encoded><![CDATA[

arXiv:2105.13921v4 Announce Type: replace-cross 
Abstract: This paper presents tensorflow-riemopt, a Python library for geometric machine learning in TensorFlow. The library provides efficient implementations of neural network layers with manifold-constrained parameters, geometric operations on Riemannian manifolds, and stochastic optimization algorithms for non-Euclidean spaces. Designed for integration with TensorFlow Extended, it supports both research prototyping and production deployment of machine learning pipelines. The code and documentation are distributed under the MIT license and available at https://github.com/master/tensorflow-riemopt
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Task Dynamic Pricing in Credit Market with Contextual Information</title>
<link>https://arxiv.org/abs/2410.14839</link>
<guid>https://arxiv.org/abs/2410.14839</guid>
<content:encoded><![CDATA[

arXiv:2410.14839v4 Announce Type: replace-cross 
Abstract: We study the dynamic pricing problem faced by a broker seeking to learn prices for a large number of credit market securities, such as corporate bonds, government bonds, loans, and other credit-related securities. A major challenge in pricing these securities stems from their infrequent trading and the lack of transparency in over-the-counter (OTC) markets, which leads to insufficient data for individual pricing. Nevertheless, many securities share structural similarities that can be exploited. Moreover, brokers often place small "probing" orders to infer competitors' pricing behavior. Leveraging these insights, we propose a multi-task dynamic pricing framework that leverages the shared structure across securities to enhance pricing accuracy.
  In the OTC market, a broker wins a quote by offering a more competitive price than rivals. The broker's goal is to learn winning prices while minimizing expected regret against a clairvoyant benchmark. We model each security using a $d$-dimensional feature vector and assume a linear contextual model for the competitor's pricing of the yield, with parameters unknown a priori. We propose the Two-Stage Multi-Task (TSMT) algorithm: first, an unregularized MLE over pooled data to obtain a coarse parameter estimate; second, a regularized MLE on individual securities to refine the parameters. We show that the TSMT achieves a regret bounded by $\tilde{O} ( \delta_{\max} \sqrt{T M d} + M d ) $, outperforming both fully individual and fully pooled baselines, where $M$ is the number of securities and $\delta_{\max}$ quantifies their heterogeneity.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Variational Annealing for Multimodal Optimization</title>
<link>https://arxiv.org/abs/2501.04667</link>
<guid>https://arxiv.org/abs/2501.04667</guid>
<content:encoded><![CDATA[

arXiv:2501.04667v3 Announce Type: replace-cross 
Abstract: We introduce a new multimodal optimization approach called Natural Variational Annealing (NVA) that combines the strengths of three foundational concepts to simultaneously search for multiple global and local modes of black-box nonconvex objectives. First, it implements a simultaneous search by using variational posteriors, such as, mixtures of Gaussians. Second, it applies annealing to gradually trade off exploration for exploitation. Finally, it learns the variational search distribution using natural-gradient learning where updates resemble well-known and easy-to-implement algorithms. The three concepts come together in NVA giving rise to new algorithms and also allowing us to incorporate "fitness shaping", a core concept from evolutionary algorithms. We assess the quality of search on simulations and compare them to methods using gradient descent and evolution strategies. We also provide an application to a real-world inverse problem in planetary science.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Cycle Structured Pruning via Stability-Driven Subnetwork Search</title>
<link>https://arxiv.org/abs/2501.13439</link>
<guid>https://arxiv.org/abs/2501.13439</guid>
<content:encoded><![CDATA[

arXiv:2501.13439v2 Announce Type: replace-cross 
Abstract: Existing structured pruning methods typically rely on multi-stage training procedures that incur high computational costs. Pruning at initialization aims to reduce this burden but often suffers from degraded performance. To address these limitations, we propose an efficient one-cycle structured pruning framework that integrates pre-training, pruning, and fine-tuning into a single training cycle without sacrificing accuracy. The key idea is to identify an optimal sub-network during the early stages of training, guided by norm-based group saliency criteria and structured sparsity regularization. We introduce a novel pruning indicator that detects a stable pruning epoch by measuring the similarity between pruning sub-networks across consecutive training epochs. In addition, group sparsity regularization accelerates convergence, further reducing overall training time. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet using VGG, ResNet, and MobileNet architectures demonstrate that the proposed method achieves state-of-the-art accuracy while being among the most efficient structured pruning frameworks in terms of training cost. Code is available at https://github.com/ghimiredhikura/OCSPruner.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All Models Are Miscalibrated, But Some Less So: Comparing Calibration with Conditional Mean Operators</title>
<link>https://arxiv.org/abs/2502.11465</link>
<guid>https://arxiv.org/abs/2502.11465</guid>
<content:encoded><![CDATA[

arXiv:2502.11465v2 Announce Type: replace-cross 
Abstract: When working in a high-risk setting, having well calibrated probabilistic predictive models is a crucial requirement. However, estimators for calibration error are not always able to correctly distinguish which model is better calibrated. We propose the \emph{conditional kernel calibration error} (CKCE) which is based on the Hilbert-Schmidt norm of the difference between conditional mean operators. By working directly with the definition of strong calibration as the distance between conditional distributions, which we represent by their embeddings in reproducing kernel Hilbert spaces, the CKCE is less sensitive to the marginal distribution of predictive models. This makes it more effective for relative comparisons than previously proposed calibration metrics. Our experiments, using both synthetic and real data, show that CKCE provides a more consistent ranking of models by their calibration error and is more robust against distribution shift.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Primer C-VAE: An interpretable deep learning primer design method to detect emerging virus variants</title>
<link>https://arxiv.org/abs/2503.01459</link>
<guid>https://arxiv.org/abs/2503.01459</guid>
<content:encoded><![CDATA[

arXiv:2503.01459v2 Announce Type: replace-cross 
Abstract: Motivation: PCR is more economical and quicker than Next Generation Sequencing for detecting target organisms, with primer design being a critical step. In epidemiology with rapidly mutating viruses, designing effective primers is challenging. Traditional methods require substantial manual intervention and struggle to ensure effective primer design across different strains. For organisms with large, similar genomes like Escherichia coli and Shigella flexneri, differentiating between species is also difficult but crucial.
  Results: We developed Primer C-VAE, a model based on a Variational Auto-Encoder framework with Convolutional Neural Networks to identify variants and generate specific primers. Using SARS-CoV-2, our model classified variants (alpha, beta, gamma, delta, omicron) with 98% accuracy and generated variant-specific primers. These primers appeared with >95% frequency in target variants and <5% in others, showing good performance in in-silico PCR tests. For Alpha, Delta, and Omicron, our primer pairs produced fragments <200 bp, suitable for qPCR detection. The model also generated effective primers for organisms with longer gene sequences like E. coli and S. flexneri.
  Conclusion: Primer C-VAE is an interpretable deep learning approach for developing specific primer pairs for target organisms. This flexible, semi-automated and reliable tool works regardless of sequence completeness and length, allowing for qPCR applications and can be applied to organisms with large and highly similar genomes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic Global Optimization of the Acquisition Function in Bayesian Optimization: To Do or Not To Do?</title>
<link>https://arxiv.org/abs/2503.03625</link>
<guid>https://arxiv.org/abs/2503.03625</guid>
<content:encoded><![CDATA[

arXiv:2503.03625v2 Announce Type: replace-cross 
Abstract: Bayesian Optimization (BO) with Gaussian Processes relies on optimizing an acquisition function to determine sampling. We investigate the advantages and disadvantages of using a deterministic global solver (MAiNGO) compared to conventional local and stochastic global solvers (L-BFGS-B and multi-start, respectively) for the optimization of the acquisition function. For CPU efficiency, we set a time limit for MAiNGO, taking the best point as optimal. We perform repeated numerical experiments, initially using the Muller-Brown potential as a benchmark function, utilizing the lower confidence bound acquisition function; we further validate our findings with three alternative benchmark functions. Statistical analysis reveals that when the acquisition function is more exploitative (as opposed to exploratory), BO with MAiNGO converges in fewer iterations than with the local solvers. However, when the dataset lacks diversity, or when the acquisition function is overly exploitative, BO with MAiNGO, compared to the local solvers, is more likely to converge to a local rather than a global ly near-optimal solution of the black-box function. L-BFGS-B and multi-start mitigate this risk in BO by introducing stochasticity in the selection of the next sampling point, which enhances the exploration of uncharted regions in the search space and reduces dependence on acquisition function hyperparameters. Ultimately, suboptimal optimization of poorly chosen acquisition functions may be preferable to their optimal solution. When the acquisition function is more exploratory, BO with MAiNGO, multi-start, and L-BFGS-B achieve comparable probabilities of convergence to a globally near-optimal solution (although BO with MAiNGO may require more iterations to converge under these conditions).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A User-Tunable Machine Learning Framework for Step-Wise Synthesis Planning</title>
<link>https://arxiv.org/abs/2504.02191</link>
<guid>https://arxiv.org/abs/2504.02191</guid>
<content:encoded><![CDATA[

arXiv:2504.02191v3 Announce Type: replace-cross 
Abstract: We introduce MHNpath, a machine learning-driven retrosynthetic tool designed for computer-aided synthesis planning. Leveraging modern Hopfield networks and novel comparative metrics, MHNpath efficiently prioritizes reaction templates, improving the scalability and accuracy of retrosynthetic predictions. The tool incorporates a tunable scoring system that allows users to prioritize pathways based on cost, reaction temperature, and toxicity, thereby facilitating the design of greener and cost-effective reaction routes. We demonstrate its effectiveness through case studies involving complex molecules from ChemByDesign, showcasing its ability to predict novel synthetic and enzymatic pathways. Furthermore, we benchmark MHNpath against existing frameworks using the PaRoutes dataset, achieving a solution rate of 85.4% and replicating 69.2% of experimentally validated "gold-standard" pathways. Our case studies reveal that the tool can generate shorter, cheaper, moderate-temperature routes employing green solvents, as exemplified by compounds such as dronabinol, arformoterol, and lupinine.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dexterous Manipulation through Imitation Learning: A Survey</title>
<link>https://arxiv.org/abs/2504.03515</link>
<guid>https://arxiv.org/abs/2504.03515</guid>
<content:encoded><![CDATA[

arXiv:2504.03515v5 Announce Type: replace-cross 
Abstract: Dexterous manipulation, which refers to the ability of a robotic hand or multi-fingered end-effector to skillfully control, reorient, and manipulate objects through precise, coordinated finger movements and adaptive force modulation, enables complex interactions similar to human hand dexterity. With recent advances in robotics and machine learning, there is a growing demand for these systems to operate in complex and unstructured environments. Traditional model-based approaches struggle to generalize across tasks and object variations due to the high dimensionality and complex contact dynamics of dexterous manipulation. Although model-free methods such as reinforcement learning (RL) show promise, they require extensive training, large-scale interaction data, and carefully designed rewards for stability and effectiveness. Imitation learning (IL) offers an alternative by allowing robots to acquire dexterous manipulation skills directly from expert demonstrations, capturing fine-grained coordination and contact dynamics while bypassing the need for explicit modeling and large-scale trial-and-error. This survey provides an overview of dexterous manipulation methods based on imitation learning, details recent advances, and addresses key challenges in the field. Additionally, it explores potential research directions to enhance IL-driven dexterous manipulation. Our goal is to offer researchers and practitioners a comprehensive introduction to this rapidly evolving domain.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curved representational Bregman divergences and their applications</title>
<link>https://arxiv.org/abs/2504.05654</link>
<guid>https://arxiv.org/abs/2504.05654</guid>
<content:encoded><![CDATA[

arXiv:2504.05654v3 Announce Type: replace-cross 
Abstract: By analogy to the terminology of curved exponential families in statistics, we define curved Bregman divergences as Bregman divergences restricted to nonlinear parameter subspaces and sub-dimensional Bregman divergences when the restrictions are linear. A common example of curved Bregman divergence is the cosine dissimilarity between normalized vectors. We show that the barycenter of a finite weighted set of parameters under a curved Bregman divergence amounts to the right Bregman projection onto the nonlinear subspace of the barycenter with respect to the full Bregman divergence. We demonstrate the significance of curved Bregman divergences with two examples: (1) symmetrized Bregman divergences, (2) pointwise symmetrized Bregman divergences, and (3) the Kullback-Leibler divergence between circular complex normal distributions. We explain how to reparameterize sub-dimensional Bregman divergences on simplicial sub-dimensional domains. We then consider monotonic embeddings to define representational curved Bregman divergences and show that the $\alpha$-divergences are representational curved Bregman divergences with respect to $\alpha$-embeddings of the probability simplex into the positive measure cone. As an application, we report an efficient method to calculate the intersection of a finite set of $\alpha$-divergence spheres.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformalized Decision Risk Assessment</title>
<link>https://arxiv.org/abs/2505.13243</link>
<guid>https://arxiv.org/abs/2505.13243</guid>
<content:encoded><![CDATA[

arXiv:2505.13243v3 Announce Type: replace-cross 
Abstract: In many operational settings, decision-makers must commit to actions before uncertainty resolves, but existing optimization tools rarely quantify how consistently a chosen decision remains optimal across plausible scenarios. This paper introduces CREDO -- Conformalized Risk Estimation for Decision Optimization, a distribution-free framework that quantifies the probability that a prescribed decision remains (near-)optimal across realizations of uncertainty. CREDO reformulates decision risk through the inverse feasible region -- the set of outcomes under which a decision is optimal -- and estimates its probability using inner approximations constructed from conformal prediction balls generated by a conditional generative model. This approach yields finite-sample, distribution-free lower bounds on the probability of decision optimality. The framework is model-agnostic and broadly applicable across a wide range of optimization problems. Extensive numerical experiments demonstrate that CREDO provides accurate, efficient, and reliable evaluations of decision optimality across various optimization settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale-invariant Attention</title>
<link>https://arxiv.org/abs/2505.17083</link>
<guid>https://arxiv.org/abs/2505.17083</guid>
<content:encoded><![CDATA[

arXiv:2505.17083v2 Announce Type: replace-cross 
Abstract: One persistent challenge in LLM research is the development of attention mechanisms that are able to generalise from training on shorter contexts to inference on longer contexts. We propose two conditions that we expect all effective long context attention mechanisms to have: scale-invariant total attention, and scale-invariant attention sparsity. Under a Gaussian assumption, we show that a simple position-dependent transformation of the attention logits is sufficient for these conditions to hold. Experimentally we find that the resulting scale-invariant attention scheme gives considerable benefits in terms of validation loss when zero-shot generalising from training on short contexts to validation on longer contexts, and is effective at long-context retrieval.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden in the Haystack: Smaller Needles are More Difficult for LLMs to Find</title>
<link>https://arxiv.org/abs/2505.18148</link>
<guid>https://arxiv.org/abs/2505.18148</guid>
<content:encoded><![CDATA[

arXiv:2505.18148v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) face significant challenges with needle-in-ahaystack tasks, where relevant information ("the needle") must be drawn from a large pool of irrelevant context ("the haystack"). Previous studies have highlighted positional bias and distractor quantity as critical factors affecting model performance, yet the influence of gold context size, the length of the answer-containing document, has received little attention. We present the first systematic study of gold context size in long-context question answering, spanning three diverse benchmarks (general knowledge, biomedical reasoning, and mathematical reasoning), eleven state-of-the-art LLMs (including recent reasoning models), and more than 150K controlled runs. Our experiments reveal that LLM performance drops sharply when the gold context is shorter, i.e., smaller gold contexts consistently degrade model performance and amplify positional sensitivity, posing a major challenge for agentic systems that must integrate scattered, fine-grained information of varying lengths. This effect persists under rigorous confounder analysis: even after controlling for gold context position, answer token repetition, gold-to-distractor ratio, distractor volume, and domain specificity, gold context size remains a decisive, independent predictor of success. Our work provides clear insights to guide the design of robust, context-aware LLM-driven systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</title>
<link>https://arxiv.org/abs/2505.22657</link>
<guid>https://arxiv.org/abs/2505.22657</guid>
<content:encoded><![CDATA[

arXiv:2505.22657v2 Announce Type: replace-cross 
Abstract: Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07400</link>
<guid>https://arxiv.org/abs/2506.07400</guid>
<content:encoded><![CDATA[

arXiv:2506.07400v3 Announce Type: replace-cross 
Abstract: The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demonstration Sidetracks: Categorizing Systematic Non-Optimality in Human Demonstrations</title>
<link>https://arxiv.org/abs/2506.11262</link>
<guid>https://arxiv.org/abs/2506.11262</guid>
<content:encoded><![CDATA[

arXiv:2506.11262v2 Announce Type: replace-cross 
Abstract: Learning from Demonstration (LfD) is a popular approach for robots to acquire new skills, but most LfD methods suffer from imperfections in human demonstrations. Prior work typically treats these suboptimalities as random noise. In this paper we study non-optimal behaviors in non-expert demonstrations and show that they are systematic, forming what we call demonstration sidetracks. Using a public space study with 40 participants performing a long-horizon robot task, we recreated the setup in simulation and annotated all demonstrations. We identify four types of sidetracks (Exploration, Mistake, Alignment, Pause) and one control pattern (one-dimension control). Sidetracks appear frequently across participants, and their temporal and spatial distribution is tied to task context. We also find that users' control patterns depend on the control interface. These insights point to the need for better models of suboptimal demonstrations to improve LfD algorithms and bridge the gap between lab training and real-world deployment. All demonstrations, infrastructure, and annotations are available at https://github.com/AABL-Lab/Human-Demonstration-Sidetracks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable representation learning of quantum data enabled by probabilistic variational autoencoders</title>
<link>https://arxiv.org/abs/2506.11982</link>
<guid>https://arxiv.org/abs/2506.11982</guid>
<content:encoded><![CDATA[

arXiv:2506.11982v3 Announce Type: replace-cross 
Abstract: Interpretable machine learning is rapidly becoming a crucial tool for scientific discovery. Among existing approaches, variational autoencoders (VAEs) have shown promise in extracting the hidden physical features of some input data, with no supervision nor prior knowledge of the system at study. Yet, the ability of VAEs to create meaningful, interpretable representations relies on their accurate approximation of the underlying probability distribution of their input. When dealing with quantum data, VAEs must hence account for its intrinsic randomness and complex correlations. While VAEs have been previously applied to quantum data, they have often neglected its probabilistic nature, hindering the extraction of meaningful physical descriptors. Here, we demonstrate that two key modifications enable VAEs to learn physically meaningful latent representations: a decoder capable of faithfully reproduce quantum states and a probabilistic loss tailored to this task. Using benchmark quantum spin models, we identify regimes where standard methods fail while the representations learned by our approach remain meaningful and interpretable. Applied to experimental data from Rydberg atom arrays, the model autonomously uncovers the phase structure without access to prior labels, Hamiltonian details, or knowledge of relevant order parameters, highlighting its potential as an unsupervised and interpretable tool for the study of quantum systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI</title>
<link>https://arxiv.org/abs/2507.06092</link>
<guid>https://arxiv.org/abs/2507.06092</guid>
<content:encoded><![CDATA[

arXiv:2507.06092v2 Announce Type: replace-cross 
Abstract: Machine learning-based supervised classifiers are widely used for security tasks, and their improvement has been largely focused on algorithmic advancements. We argue that data challenges that negatively impact the performance of these classifiers have received limited attention. We address the following research question: Can developments in Generative AI (GenAI) address these data challenges and improve classifier performance? We propose augmenting training datasets with synthetic data generated using GenAI techniques to improve classifier generalization. We evaluate this approach across 7 diverse security tasks using 6 state-of-the-art GenAI methods and introduce a novel GenAI scheme called Nimai that enables highly controlled data synthesis. We find that GenAI techniques can significantly improve the performance of security classifiers, achieving improvements of up to 32.6% even in severely data-constrained settings (only ~180 training samples). Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to concept drift post-deployment, requiring minimal labeling in the adjustment process. Despite successes, our study finds that some GenAI schemes struggle to initialize (train and produce data) on certain security tasks. We also identify characteristics of specific tasks, such as noisy labels, overlapping class distributions, and sparse feature vectors, which hinder performance boost using GenAI. We believe that our study will drive the development of future GenAI tools designed for security tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical stability for dense patterns in discrete attractor neural networks</title>
<link>https://arxiv.org/abs/2507.10383</link>
<guid>https://arxiv.org/abs/2507.10383</guid>
<content:encoded><![CDATA[

arXiv:2507.10383v3 Announce Type: replace-cross 
Abstract: Neural networks storing multiple discrete attractors are canonical models of biological memory. Previously, the dynamical stability of such networks could only be guaranteed under highly restrictive conditions. Here, we derive a theory of the local stability of discrete fixed points in a broad class of networks with graded neural activities and in the presence of noise. By directly analyzing the bulk and the outliers of the Jacobian spectrum, we show that all fixed points are stable below a critical load that is distinct from the classical \textit{critical capacity} and depends on the statistics of neural activities in the fixed points as well as the single-neuron activation function. Our analysis highlights the computational benefits of threshold-linear activation and sparse-like patterns.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning without training: The implicit dynamics of in-context learning</title>
<link>https://arxiv.org/abs/2507.16003</link>
<guid>https://arxiv.org/abs/2507.16003</guid>
<content:encoded><![CDATA[

arXiv:2507.16003v2 Announce Type: replace-cross 
Abstract: One of the most striking features of Large Language Models (LLMs) is their ability to learn in-context. Namely at inference time an LLM is able to learn new patterns without any additional weight update when these patterns are presented in the form of examples in the prompt, even if these patterns were not seen during training. The mechanisms through which this can happen are still largely unknown. In this work, we show that the stacking of a self-attention layer with an MLP, allows the transformer block to implicitly modify the weights of the MLP layer according to the context. We argue through theory and experimentation that this simple mechanism may be the reason why LLMs can learn in-context and not only during training. Specifically, we show how a transformer block implicitly transforms a context into a low-rank weight-update of its MLP layer.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Setting the Standard: Recommended Practices for Data Preprocessing in Data-Driven Climate Prediction</title>
<link>https://arxiv.org/abs/2508.07062</link>
<guid>https://arxiv.org/abs/2508.07062</guid>
<content:encoded><![CDATA[

arXiv:2508.07062v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) - and specifically machine learning (ML) - applications for climate prediction across timescales are proliferating quickly. The emergence of these methods prompts a revisit to the impact of data preprocessing, a topic familiar to the climate community, as more traditional statistical models work with relatively small sample sizes. Indeed, the skill and confidence in the forecasts produced by data-driven models are directly influenced by the quality of the datasets and how they are treated during model development, thus yielding the colloquialism, "garbage in, garbage out." As such, this article establishes protocols for the proper preprocessing of input data for AI/ML models designed for climate prediction (i.e., subseasonal to decadal and longer). The three aims are to: (1) educate researchers, developers, and end users on the effects that preprocessing has on climate predictions; (2) provide recommended practices for data preprocessing for such applications; and (3) empower end users to decipher whether the models they are using are properly designed for their objectives. Specific topics covered in this article include the creation of (standardized) anomalies, dealing with non-stationarity and the spatiotemporally correlated nature of climate data, and handling of extreme values and variables with potentially complex distributions. Case studies will illustrate how using different preprocessing techniques can produce different predictions from the same model, which can create confusion and decrease confidence in the overall process. Ultimately, implementing the recommended practices set forth in this article will enhance the robustness and transparency of AI/ML in climate prediction studies.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Autoencoders Make Audio Foundation Models more Explainable</title>
<link>https://arxiv.org/abs/2509.24793</link>
<guid>https://arxiv.org/abs/2509.24793</guid>
<content:encoded><![CDATA[

arXiv:2509.24793v2 Announce Type: replace-cross 
Abstract: Audio pretrained models are widely employed to solve various tasks in speech processing, sound event detection, or music information retrieval. However, the representations learned by these models are unclear, and their analysis mainly restricts to linear probing of the hidden representations. In this work, we explore the use of Sparse Autoencoders (SAEs) to analyze the hidden representations of pretrained models, focusing on a case study in singing technique classification. We first demonstrate that SAEs retain both information about the original representations and class labels, enabling their internal structure to provide insights into self-supervised learning systems. Furthermore, we show that SAEs enhance the disentanglement of vocal attributes, establishing them as an effective tool for identifying the underlying factors encoded in the representations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units</title>
<link>https://arxiv.org/abs/2509.25155</link>
<guid>https://arxiv.org/abs/2509.25155</guid>
<content:encoded><![CDATA[

arXiv:2509.25155v2 Announce Type: replace-cross 
Abstract: The proliferation of large language models has driven demand for long-context inference on resource-constrained edge platforms. However, deploying these models on Neural Processing Units (NPUs) presents significant challenges due to architectural mismatch: the quadratic complexity of standard attention conflicts with NPU memory and compute patterns. This paper presents a comprehensive performance analysis of causal inference operators on a modern NPU, benchmarking quadratic attention against sub-quadratic alternatives including structured state-space models and causal convolutions. Our analysis reveals a spectrum of critical bottlenecks: quadratic attention becomes severely memory-bound with catastrophic cache inefficiency, while sub-quadratic variants span from compute-bound on programmable vector cores to memory-bound by data movement. These findings provide essential insights for co-designing hardware-aware models and optimization strategies to enable efficient long-context inference on edge platforms.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization</title>
<link>https://arxiv.org/abs/2510.02389</link>
<guid>https://arxiv.org/abs/2510.02389</guid>
<content:encoded><![CDATA[

arXiv:2510.02389v2 Announce Type: replace-cross 
Abstract: Large language models show promise for vulnerability discovery, yet prevailing methods inspect code in isolation, struggle with long contexts, and focus on coarse function or file level detections which offers limited actionable guidance to engineers who need precise line-level localization and targeted patches in real-world software development. We present T2L-Agent (Trace-to-Line Agent), a project-level, end-to-end framework that plans its own analysis and progressively narrows scope from modules to exact vulnerable lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer (ATA) that fuses run-time evidence such as crash points, stack traces, and coverage deltas with AST-based code chunking, enabling iterative refinement beyond single pass predictions and translating symptoms into actionable, line-level diagnoses. To benchmark line-level vulnerability discovery, we introduce T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash families and real-world projects. T2L-ARVO is specifically designed to support both coarse-grained detection and fine-grained localization, enabling rigorous evaluation of systems that aim to move beyond file-level predictions. On T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level localization, substantially outperforming baselines. Together, the framework and benchmark push LLM-based vulnerability detection from coarse identification toward deployable, robust, precision diagnostics that reduce noise and accelerate patching in open-source software workflows.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Hippocampus Networks for Efficient Long-Context Modeling</title>
<link>https://arxiv.org/abs/2510.07318</link>
<guid>https://arxiv.org/abs/2510.07318</guid>
<content:encoded><![CDATA[

arXiv:2510.07318v2 Announce Type: replace-cross 
Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and GatedDeltaNet to augment open-weight LLMs. We also propose an efficient self-distillation training method where the base model's all parameters are frozen and only the parameters from AHNs are optimized. For inference, our method sets a default large sliding window size of 32k for attention, and AHNs activate only when the sequence length exceeds the 32k window, addressing the quadratic-complexity issue of attention that emerges at that scale. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geopolitics, Geoeconomics and Risk: A Machine Learning Approach</title>
<link>https://arxiv.org/abs/2510.12416</link>
<guid>https://arxiv.org/abs/2510.12416</guid>
<content:encoded><![CDATA[

arXiv:2510.12416v3 Announce Type: replace-cross 
Abstract: We introduce a novel high-frequency daily panel dataset of both markets and news-based indicators -- including Geopolitical Risk, Economic Policy Uncertainty, Trade Policy Uncertainty, and Political Sentiment -- for 42 countries across both emerging and developed markets. Using this dataset, we study how sentiment dynamics shape sovereign risk, measured by Credit Default Swap (CDS) spreads, and evaluate their forecasting value relative to traditional drivers such as global monetary policy and market volatility. Our horse-race analysis of forecasting models demonstrates that incorporating news-based indicators significantly enhances predictive accuracy and enriches the analysis, with non-linear machine learning methods -- particularly Random Forests -- delivering the largest gains. Our analysis reveals that while global financial variables remain the dominant drivers of sovereign risk, geopolitical risk and economic policy uncertainty also play a meaningful role. Crucially, their effects are amplified through non-linear interactions with global financial conditions. Finally, we document pronounced regional heterogeneity, as certain asset classes and emerging markets exhibit heightened sensitivity to shocks in policy rates, global financial volatility, and geopolitical risk.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations</title>
<link>https://arxiv.org/abs/2510.25544</link>
<guid>https://arxiv.org/abs/2510.25544</guid>
<content:encoded><![CDATA[

arXiv:2510.25544v2 Announce Type: replace-cross 
Abstract: Recently proposed generative models for discrete data, such as Masked Diffusion Models (MDMs), exploit conditional independence approximations to reduce the computational cost of popular Auto-Regressive Models (ARMs), at the price of some bias in the sampling distribution. We study the resulting computation-vs-accuracy trade-off, providing general error bounds (in relative entropy) that depend only on the average number of tokens generated per iteration and are independent of the data dimensionality (i.e. sequence length), thus supporting the empirical success of MDMs. We then investigate the gain obtained by using non-constant schedule sizes (i.e. varying the number of unmasked tokens during the generation process) and identify the optimal schedule as a function of a so-called information profile of the data distribution, thus allowing for a principled optimization of schedule sizes. We define methods directly as sampling algorithms and do not use classical derivations as time-reversed diffusion processes, leading us to simple and transparent proofs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
<link>https://arxiv.org/abs/2511.01140</link>
<guid>https://arxiv.org/abs/2511.01140</guid>
<content:encoded><![CDATA[

arXiv:2511.01140v2 Announce Type: replace-cross 
Abstract: Medical imaging often operates under limited labeled data, especially in rare disease and low resource clinical environments. Existing multimodal and meta learning approaches improve performance in these settings but lack a theoretical explanation of why or when they succeed. This paper presents a unified theoretical framework for few shot multimodal medical imaging that jointly characterizes sample complexity, uncertainty quantification, and interpretability. Using PAC learning, VC theory, and PAC Bayesian analysis, we derive bounds that describe the minimum number of labeled samples required for reliable performance and show how complementary modalities reduce effective capacity through an information gain term. We further introduce a formal metric for explanation stability, proving that explanation variance decreases at an inverse n rate. A sequential Bayesian interpretation of Chain of Thought reasoning is also developed to show stepwise posterior contraction. To illustrate these ideas, we implement a controlled multimodal dataset and evaluate an additive CNN MLP fusion model under few shot regimes, confirming predicted multimodal gains, modality interference at larger sample sizes, and shrinking predictive uncertainty. Together, the framework provides a principled foundation for designing data efficient, uncertainty aware, and interpretable diagnostic models in low resource settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving a Research Problem in Mathematical Statistics with AI Assistance</title>
<link>https://arxiv.org/abs/2511.18828</link>
<guid>https://arxiv.org/abs/2511.18828</guid>
<content:encoded><![CDATA[

arXiv:2511.18828v3 Announce Type: replace-cross 
Abstract: Over the last few months, AI models including large language models have improved greatly. There are now several documented examples where they have helped professional mathematical scientists prove new results, sometimes even helping resolve known open problems. In this short note, we add another example to the list, by documenting how we were able to solve a previously unsolved research problem in robust mathematical statistics with crucial help from GPT-5. Our problem concerns robust density estimation, where the observations are perturbed by Wasserstein-bounded contaminations. In a previous preprint (Chao and Dobriban, 2023, arxiv:2308.01853v2), we have obtained upper and lower bounds on the minimax optimal estimation error; which were, however, not sharp.
  Starting in October 2025, making significant use of GPT-5 Pro, we were able to derive the minimax optimal error rate (reported in version 3 of the above arxiv preprint). GPT-5 provided crucial help along the way, including by suggesting calculations that we did not think of, and techniques that were not familiar to us, such as the dynamic Benamou-Brenier formulation, for key steps in the analysis. Working with GPT-5 took a few weeks of effort, and we estimate that it could have taken several months to get the same results otherwise. At the same time, there are still areas where working with GPT-5 was challenging: it sometimes provided incorrect references, and glossed over details that sometimes took days of work to fill in. We outline our workflow and steps taken to mitigate issues. Overall, our work can serve as additional documentation for a new age of human-AI collaborative work in mathematical science.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 4/$\delta$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee</title>
<link>https://arxiv.org/abs/2512.02080</link>
<guid>https://arxiv.org/abs/2512.02080</guid>
<content:encoded><![CDATA[

arXiv:2512.02080v2 Announce Type: replace-cross 
Abstract: The integration of Formal Verification tools with Large Language Models (LLMs) offers a path to scale software verification beyond manual workflows. However, current methods remain unreliable: without a solid theoretical footing, the refinement process acts as a black box that may oscillate, loop, or diverge. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination in multi-stage verification pipelines. We model the interaction not as a generic loop, but as a sequential absorbing Markov Chain comprising four essential engineering stages: \texttt{CodeGen}, \texttt{Compilation}, \texttt{InvariantSynth}, and \texttt{SMTSolving}. We prove that for any non-zero stage success probability ($\delta > 0$), the system reaches the \texttt{Verified} state almost surely. Furthermore, because of the sequential nature of the pipeline, we derive a precise latency bound of $\mathbb{E}[n] \leq 4/\delta$. We stress-tested this prediction in an extensive empirical campaign comprising over 90,000 trials. The results match the theory with striking consistency: every run reached verification, and the empirical convergence factor clustered tightly around $C_f\approx 1.0$, confirming that the $4/\delta$ bound accurately mirrors system behavior rather than serving as a loose buffer. Based on this data, we identify three distinct operating zones -- marginal, practical, and high-performance -- and propose a dynamic calibration strategy to handle parameter drift in real-world environments. Together, these contributions replace heuristic guesswork with a rigorous architectural foundation, enabling predictable resource planning and performance budgeting for safety-critical software.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stress-Testing Causal Claims via Cardinality Repairs</title>
<link>https://arxiv.org/abs/2512.02491</link>
<guid>https://arxiv.org/abs/2512.02491</guid>
<content:encoded><![CDATA[

arXiv:2512.02491v2 Announce Type: replace-cross 
Abstract: Causal analyses derived from observational data underpin high-stakes decisions in domains such as healthcare, public policy, and economics. Yet such conclusions can be surprisingly fragile: even minor data errors - duplicate records, or entry mistakes - may drastically alter causal relationships. This raises a fundamental question: how robust is a causal claim to small, targeted modifications in the data? Addressing this question is essential for ensuring the reliability, interpretability, and reproducibility of empirical findings. We introduce SubCure, a framework for robustness auditing via cardinality repairs. Given a causal query and a user-specified target range for the estimated effect, SubCure identifies a small set of tuples or subpopulations whose removal shifts the estimate into the desired range. This process not only quantifies the sensitivity of causal conclusions but also pinpoints the specific regions of the data that drive those conclusions. We formalize this problem under both tuple- and pattern-level deletion settings and show both are NP-complete. To scale to large datasets, we develop efficient algorithms that incorporate machine unlearning techniques to incrementally update causal estimates without retraining from scratch. We evaluate SubCure across four real-world datasets covering diverse application domains. In each case, it uncovers compact, high-impact subsets whose removal significantly shifts the causal conclusions, revealing vulnerabilities that traditional methods fail to detect. Our results demonstrate that cardinality repair is a powerful and general-purpose tool for stress-testing causal analyses and guarding against misleading claims rooted in ordinary data imperfections.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning</title>
<link>https://arxiv.org/abs/2512.09368</link>
<guid>https://arxiv.org/abs/2512.09368</guid>
<content:encoded><![CDATA[
<div> Keywords: Traffic Signal Control, Reinforcement Learning, Counterfactual Learning, Safety, Causal Model

<br /><br />Summary:  
Traffic accidents, especially those at intersections, cause millions of injuries and fatalities globally. Traffic Signal Control (TSC) is an effective approach to improve safety at urban intersections. While Reinforcement Learning (RL) has gained popularity for optimizing TSC, existing RL methods typically emphasize driving efficiency over safety and often lack interpretability. This study introduces a novel framework using Counterfactual (CF) learning to enhance safety in RL-based TSC. The framework tackles the critical question: "What if an unsafe event occurs, can alternative actions prevent subsequent unsafe events?" To answer this, the authors propose a new structured causal model that predicts outcomes following different actions and develop a CF module integrated with additional "X" modules to promote safer RL strategies. Using this framework, the researchers design CFLight, a new algorithm that effectively manages safety-critical events and achieves near-zero collision rates at intersections. Extensive experiments on both real-world and synthetic datasets demonstrate that CFLight significantly reduces collisions while improving overall traffic performance compared to traditional RL and recent safe RL models. Furthermore, this approach offers a generalized, interpretable, and safe framework for RL applicable to other domains beyond traffic management. The data and code supporting this study are publicly available on GitHub. <div>
arXiv:2512.09368v2 Announce Type: replace 
Abstract: Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/AdvancedAI-ComplexSystem/SmartCity/tree/main/CFLight.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geospatial Soil Quality Analysis: A Roadmap for Integrated Systems</title>
<link>https://arxiv.org/abs/2512.09817</link>
<guid>https://arxiv.org/abs/2512.09817</guid>
<content:encoded><![CDATA[
<div> soil quality, GIS, remote sensing, machine learning, sustainable agriculture<br /><br />Summary:  
This paper addresses the critical role of soil quality (SQ) in sustainable agriculture, environmental conservation, and land-use planning. It identifies the limitations of traditional SQ assessment methods, which are costly and labor-intensive with limited spatial and temporal coverage. Leveraging advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML), the study proposes a unified and modular pipeline integrating multi-source soil data for efficient and scalable SQ evaluation. This comprehensive roadmap differs from previous reviews by consolidating recent technological advancements across the entire soil quality assessment process rather than focusing narrowly on individual soil parameters or modeling techniques. The pipeline supports transparent, adaptive, and scalable soil quality assessments with practical applications demonstrated. Furthermore, the paper highlights existing challenges and limitations in current methodologies and explores emerging trends to drive the next generation of soil quality systems. These future developments aim to enhance transparency, adaptability, and alignment with sustainable land management goals, thereby enabling more effective environmental stewardship and decision-making in agriculture and land use. <div>
arXiv:2512.09817v2 Announce Type: replace-cross 
Abstract: Soil quality (SQ) plays a crucial role in sustainable agriculture, environmental conservation, and land-use planning. Traditional SQ assessment techniques rely on costly, labor-intensive sampling and laboratory analysis, limiting their spatial and temporal coverage. Advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML) enabled efficient SQ evaluation. This paper presents a comprehensive roadmap distinguishing it from previous reviews by proposing a unified and modular pipeline that integrates multi-source soil data, GIS and remote sensing tools, and machine learning techniques to support transparent and scalable soil quality assessment. It also includes practical applications. Contrary to existing studies that predominantly target isolated soil parameters or specific modeling methodologies, this approach consolidates recent advancements in Geographic Information Systems (GIS), remote sensing technologies, and machine learning algorithms within the entire soil quality assessment pipeline. It also addresses existing challenges and limitations while exploring future developments and emerging trends in the field that can deliver the next generation of soil quality systems making them more transparent, adaptive, and aligned with sustainable land management.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Guided Deep Learning for Heat Pump Stress Detection: A Comprehensive Analysis on When2Heat Dataset</title>
<link>https://arxiv.org/abs/2512.13696</link>
<guid>https://arxiv.org/abs/2512.13696</guid>
<content:encoded><![CDATA[
<div> Keywords: Heat pump, stress detection, physics-guided deep neural network, feature selection, When2Heat dataset  

<br /><br />Summary:  
This paper addresses the challenge of detecting operational stress in heat pump systems, which are vital for energy-efficient buildings but difficult to monitor due to complex thermodynamics and scarce real-world data. The authors propose a novel Physics-Guided Deep Neural Network (PG-DNN) approach for classifying heat pump stress. Their method leverages the extensive When2Heat dataset, consisting of 131,483 samples with 656 features spanning 26 European countries. The model architecture includes five hidden layers and employs dual regularization techniques to enhance performance. Physics-guided feature selection and carefully designed class definitions based on variable thresholding contribute to a realistic and meaningful class distribution. The PG-DNN achieves a test accuracy of 78.1% and a validation accuracy of 78.5%, outperforming baseline models by 5.0% compared to shallow neural networks, 4.0% relative to limited feature sets, and 2.0% beyond single regularization strategies. Extensive ablation studies confirm the importance of each design choice, including physics-based feature selection, thresholding methods for class labeling, and analyzing cross-country energy consumption patterns for improved generalizability. Finally, the system, containing 181,348 parameters, demonstrates practical feasibility by training in 720 seconds on AMD Ryzen 9 7950X with RTX 4080 GPU hardware, making it a production-ready solution for real-world heat pump stress detection. <div>
arXiv:2512.13696v1 Announce Type: new 
Abstract: Heat pump systems are critical components in modern energy-efficient buildings, yet their operational stress detection remains challenging due to complex thermodynamic interactions and limited real-world data. This paper presents a novel Physics-Guided Deep Neural Network (PG-DNN) approach for heat pump stress classification using the When2Heat dataset, containing 131,483 samples with 656 features across 26 European countries. The methodology integrates physics-guided feature selection and class definition with a deep neural network architecture featuring 5 hidden layers and dual regularization strategies. The model achieves 78.1\% test accuracy and 78.5% validation accuracy, demonstrating significant improvements over baseline approaches: +5.0% over shallow networks, +4.0% over limited feature sets, and +2.0% over single regularization strategies. Comprehensive ablation studies validate the effectiveness of physics-guided feature selection, variable thresholding for realistic class distribution, and cross-country energy pattern analysis. The proposed system provides a production-ready solution for heat pump stress detection with 181,348 parameters and 720 seconds training time on AMD Ryzen 9 7950X with RTX 4080 hardware.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling and Transferability of Annealing Strategies in Large Language Model Training</title>
<link>https://arxiv.org/abs/2512.13705</link>
<guid>https://arxiv.org/abs/2512.13705</guid>
<content:encoded><![CDATA[
<div> Learning rate scheduling, annealing strategies, large language models, Warmup-Steady-Decay scheduler, training dynamics<br /><br />Summary:<br /><br />1. The paper addresses the critical role of learning rate scheduling in training large language models and highlights the challenges in understanding optimal annealing strategies across various model configurations. <br />2. The authors investigate the transferability of annealing dynamics and propose an enhanced predictive framework that optimizes annealing strategies specifically under the Warmup-Steady-Decay (WSD) scheduler. <br />3. This improved framework integrates key factors including training steps, the maximum learning rate, and the annealing behavior, enabling more efficient and effective learning rate schedule optimization. <br />4. One major contribution is practical guidance that allows selection of optimal annealing strategies without the need for exhaustive hyperparameter searches, thereby reducing computational overhead. <br />5. Extensive experiments on both Dense and Mixture-of-Experts (MoE) models validate the approach, showing that smaller models can reliably proxy larger ones and that optimal annealing ratios exhibit consistent and transferable patterns across different training setups. <div>
arXiv:2512.13705v1 Announce Type: new 
Abstract: Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training</title>
<link>https://arxiv.org/abs/2512.13706</link>
<guid>https://arxiv.org/abs/2512.13706</guid>
<content:encoded><![CDATA[
<div> Keywords: catastrophic forgetting, finetuning, large language models, mixed training, mathematical reasoning<br /><br />Summary:<br /><br />1. The paper addresses the issue of catastrophic forgetting in large language models when finetuned for specialized tasks like mathematical reasoning. <br />2. Finetuning Flan-T5-Base on the DeepMind Mathematics dataset significantly improves math accuracy from 3.1% to 12.0% but causes a severe drop in natural language inference (NLI) performance on MultiNLI, from 81.0% to 16.5%, within 1,000 training steps.<br />3. To mitigate forgetting, the authors propose mixed training strategies that combine mathematical and NLI examples during training.<br />4. Their experiments show that a balanced 1:1 ratio of math to NLI training examples completely eliminates catastrophic forgetting, maintaining 12.0% math accuracy while improving NLI accuracy to 86.2%, exceeding the original.<br />5. Further analysis of mixing ratios from 1:1 up to 15:1 reveals that even minimal NLI exposure (6.2%) is effective to regularize forgetting, suggesting specialization need not come at the cost of general capabilities. <br />6. These results have important implications for scaling up models, as mixed training not only prevents forgetting but may also provide additional benefits for larger architectures. <div>
arXiv:2512.13706v1 Announce Type: new 
Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Physics-Informed Ansatz for Reconstructing Hidden Interaction Networks from Steady States</title>
<link>https://arxiv.org/abs/2512.13708</link>
<guid>https://arxiv.org/abs/2512.13708</guid>
<content:encoded><![CDATA[
<div> Keywords: complex dynamical systems, interaction reconstruction, steady-state data, variational physics-informed ansatz, nonlinear networks<br /><br />Summary:<br /><br />The article addresses the challenge of reconstructing interaction networks in complex dynamical systems, particularly when the couplings are nonlinear, heterogeneous, and higher-order, and only steady-state data is available. It introduces the Variational Physics-Informed Ansatz (VPIA), a method that infers general interaction operators directly from heterogeneous steady-state observations. VPIA works by embedding the steady-state constraints into a differentiable variational form and minimizing a physics-derived residual without requiring temporal trajectories, derivative calculations, or supervised learning labels. The method uses residual sampling paired with natural-gradient optimization to efficiently handle large-scale networks and multi-body couplings. Tested on various nonlinear systems, VPIA accurately recovers directed, weighted, and complex multi-body interaction structures even under substantial noise conditions. Overall, the approach offers a unified, scalable, and robust framework for physics-constrained inference, making it applicable in scenarios where only snapshot or steady-state data is accessible for understanding complex dynamics. <div>
arXiv:2512.13708v1 Announce Type: new 
Abstract: The interaction structure of a complex dynamical system governs its collective behavior, yet existing reconstruction methods struggle with nonlinear, heterogeneous, and higher-order couplings, especially when only steady states are observable. We propose a Variational Physics-Informed Ansatz (VPIA) that infers general interaction operators directly from heterogeneous steady-state data. VPIA embeds the steady-state constraints of the dynamics into a differentiable variational representation and reconstructs the underlying couplings by minimizing a physics-derived steady-state residual, without requiring temporal trajectories, derivative estimation, or supervision. Residual sampling combined with natural-gradient optimization enables scalable learning of large and higher-order networks. Across diverse nonlinear systems, VPIA accurately recovers directed, weighted, and multi-body structures under substantial noise, providing a unified and robust framework for physics-constrained inference of complex interaction networks in settings where only snapshot observations are available.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables</title>
<link>https://arxiv.org/abs/2512.13710</link>
<guid>https://arxiv.org/abs/2512.13710</guid>
<content:encoded><![CDATA[
<div> Flooding, Synthetic Aperture Radar, Machine Learning, River Nyando, Random Forest<br /><br />Summary:<br /><br />1. Flooding is a significant natural hazard causing severe risks to ecosystems, infrastructure, and human livelihoods worldwide.<br /><br />2. This study focuses on flood susceptibility modeling in the River Nyando watershed, western Kenya, by integrating Sentinel-1 dual-polarization SAR imagery from the May 2024 flood event.<br /><br />3. A binary flood inventory was created from SAR data and used along with six conditioning factors—slope, elevation, aspect, land use/land cover, soil type, and distance from streams—to train four supervised machine learning classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF).<br /><br />4. Model performance was evaluated using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) metrics, revealing that the Random Forest model achieved the highest predictive performance with an accuracy of 0.762 and a Kappa of 0.480.<br /><br />5. The Random Forest-based flood susceptibility map identified the low-lying Kano Plains near Lake Victoria as the most flood-prone area, consistent with historical flood data and recent flooding impacts.<br /><br />6. The study underscores the effectiveness of combining SAR data with ensemble machine learning techniques for reliable flood susceptibility mapping, especially in regions with limited data availability.<br /><br />7. The resulting flood susceptibility maps provide valuable information for disaster risk reduction strategies, land-use planning, and development of early warning systems in flood-affected areas. <div>
arXiv:2512.13710v1 Announce Type: new 
Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delete and Retain: Efficient Unlearning for Document Classification</title>
<link>https://arxiv.org/abs/2512.13711</link>
<guid>https://arxiv.org/abs/2512.13711</guid>
<content:encoded><![CDATA[
<div> Machine unlearning, document classification, Hessian Reassignment, influence function, membership inference<br /><br />Summary:<br /><br />This paper addresses the problem of machine unlearning specifically for document classification models, focusing on class-level unlearning where the influence of an entire target class’s data is removed from a trained model without retraining from scratch. The authors introduce a novel, model-agnostic two-step method called Hessian Reassignment. First, they perform an influence-style update by solving a Hessian-vector equation with conjugate gradients to subtract the collective contribution of all training points from the specified class, relying only on gradient and Hessian-vector products. This approach is computationally efficient compared to full retraining. Second, instead of the typical strategy of randomly reclassifying samples from the deleted class, they enforce a decision-space guarantee by ensuring Top-1 classification consistency, improving the quality of unlearning outcomes. Experiments on standard text classification benchmarks demonstrate that Hessian Reassignment maintains accuracy on the retained classes very close to that of a fully retrained model without the deleted class, while running orders of magnitude faster. Additionally, it effectively reduces membership-inference attack advantage for the removed class, enhancing privacy protections. Overall, this work presents a principled, practical solution enabling efficient, reliable class-level unlearning in document classification tasks. <div>
arXiv:2512.13711v1 Announce Type: new 
Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data</title>
<link>https://arxiv.org/abs/2512.13712</link>
<guid>https://arxiv.org/abs/2512.13712</guid>
<content:encoded><![CDATA[
<div> Keywords: Respiratory syncytial virus, machine learning, wastewater surveillance, hospitalization prediction, environmental factors<br /><br />Summary:<br /><br />This study addresses Respiratory Syncytial Virus (RSV) as a major cause of hospitalization in young children, focusing on how environmental factors affect outbreak severity. A machine learning framework was developed to predict weekly RSV-associated hospitalization rates in the U.S. by integrating data from wastewater surveillance, meteorological conditions, and air quality measures. The dataset combined weekly hospitalization rates with wastewater RSV levels, daily weather records, and air pollutant concentrations. Classification models including CART, Random Forest, and Boosting were trained to categorize hospitalization risk into Low risk, Alert, or Epidemic levels. Among predictors, wastewater RSV levels emerged as the strongest indicator, followed by temperature, ozone, and specific humidity. The analysis revealed significantly higher hospitalization rates in Native American and Alaska Native populations, suggesting disparities needing targeted research and intervention. Moreover, states at high altitudes with lower surface pressure consistently exhibited elevated RSV hospitalization rates. These findings demonstrate the benefit of combining environmental and community surveillance data to improve outbreak forecasting and guide timely public health responses. To facilitate practical use, an interactive R Shiny dashboard was created, enabling users to explore risk levels, visualize key predictors, and generate outbreak forecasts across different states. <div>
arXiv:2512.13712v1 Announce Type: new 
Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints</title>
<link>https://arxiv.org/abs/2512.13717</link>
<guid>https://arxiv.org/abs/2512.13717</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG-based seizure detection, federated learning, few-shot learning, biosignal transformer, privacy-preserving AI<br /><br />Summary:<br /><br />1. The paper addresses the challenges of EEG-based seizure detection in clinical settings, where data are patient-specific, scarce, distributed across institutions, and privacy-sensitive, preventing data centralization.  
2. A novel two-stage federated few-shot learning (FFSL) framework is proposed for personalized seizure detection that respects data privacy and leverages decentralized datasets.  
3. In Stage 1, a pretrained biosignal transformer model (BIOT) is fine-tuned using federated learning across non-IID simulated hospital sites, learning shared representations without pooling EEG data centrally.  
4. In Stage 2, a federated few-shot personalization technique adapts the classifier individually for each patient using only five labeled EEG segments, preserving seizure-specific features while benefiting from multi-site knowledge.  
5. Experimental results on the TUH Event Corpus (six EEG event classes) show federated fine-tuning achieves balanced accuracy of 0.43 versus 0.52 centralized, Cohen’s kappa 0.42 versus 0.49, and weighted F1 0.69 versus 0.74.  
6. The FFSL approach yields client-specific models with improved performance: average balanced accuracy at 0.77, Cohen’s kappa 0.62, and weighted F1 0.73 across four sites with heterogeneous event distributions.  
7. These findings demonstrate FFSL’s efficacy for patient-adaptive seizure detection under realistic data availability and privacy constraints, highlighting its potential in real-world medical AI applications. <div>
arXiv:2512.13717v1 Announce Type: new 
Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce</title>
<link>https://arxiv.org/abs/2512.13726</link>
<guid>https://arxiv.org/abs/2512.13726</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, slate recommendation, time budget, user engagement, Markov Decision Processes<br /><br />Summary:<br /><br />1. The paper addresses the challenge in recommendation systems where finite user time budgets impose resource constraints, requiring a balance between item relevance and the cost of user evaluation. 2. It highlights the scenario of mobile shopping interfaces, where users scroll through lists (slates) of items, and each interaction incurs an evaluation cost that impacts user engagement if highly relevant items consume too much time. 3. The authors propose to use reinforcement learning (RL) algorithms that learn both user preferences and time budget patterns to create more engaging recommendations under constrained time. 4. Their contributions include formulating the time-constrained slate recommendation problem as a Markov Decision Process (MDP) with utilities that account for budget constraints. 5. They develop a simulation framework that leverages Alibaba's Personalized Re-ranking dataset to analyze policy behaviors in re-ranking tasks for e-commerce. 6. Empirical results demonstrate that both on-policy and off-policy RL methods outperform traditional contextual bandit approaches, especially under tight time budgets, improving recommendation performance and user engagement. <div>
arXiv:2512.13726v1 Announce Type: new 
Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing</title>
<link>https://arxiv.org/abs/2512.13727</link>
<guid>https://arxiv.org/abs/2512.13727</guid>
<content:encoded><![CDATA[
<div> Keywords: Ride-hailing, Reinforcement Learning, Spatio-Temporal, Mixture-of-Experts, Congestion Modeling  

<br /><br />Summary:  
This paper addresses the challenge faced by ride-hailing platforms to minimize passenger waiting times while maximizing system efficiency amid uncertain supply-demand patterns. The authors propose adaptive delayed matching, which balances the trade-off between immediate driver assignments and batching requests to reduce delays. To capture the complex spatiotemporal traffic dynamics and long-term outcomes, they formulate the problem as a regime-aware Markov Decision Process (MDP) solved using reinforcement learning. Their approach, Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE), leverages a self-attention based mixture-of-experts encoder that enables automatic expert specialization, thereby enhancing representation capacity without excessive computational cost. The framework incorporates a physics-informed congestion surrogate model, preserving realistic density-speed feedback, which allows for millions of efficient simulations or rollouts. An adaptive reward scheme is designed to prevent undesirable or trivial strategies during training. With only 12 million parameters, RAST-MoE outperforms strong existing methods on real Uber trajectory data from San Francisco, achieving more than 13% improvement in total reward, and reducing average matching and pickup delays by 10% and 15%, respectively. Additionally, it exhibits robustness when exposed to unseen demand regimes and ensures stable training performance. These results demonstrate the promising potential of combining mixture-of-experts architectures with reinforcement learning for scalable, real-world spatiotemporal decision-making problems. <div>
arXiv:2512.13727v1 Announce Type: new 
Abstract: Ride-hailing platforms face the challenge of balancing passenger waiting times with overall system efficiency under highly uncertain supply-demand conditions. Adaptive delayed matching creates a trade-off between matching and pickup delays by deciding whether to assign drivers immediately or batch requests. Since outcomes accumulate over long horizons with stochastic dynamics, reinforcement learning (RL) is a suitable framework. However, existing approaches often oversimplify traffic dynamics or use shallow encoders that miss complex spatiotemporal patterns.
  We introduce the Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE), which formalizes adaptive delayed matching as a regime-aware MDP equipped with a self-attention MoE encoder. Unlike monolithic networks, our experts specialize automatically, improving representation capacity while maintaining computational efficiency. A physics-informed congestion surrogate preserves realistic density-speed feedback, enabling millions of efficient rollouts, while an adaptive reward scheme guards against pathological strategies.
  With only 12M parameters, our framework outperforms strong baselines. On real-world Uber trajectory data (San Francisco), it improves total reward by over 13%, reducing average matching and pickup delays by 10% and 15% respectively. It demonstrates robustness across unseen demand regimes and stable training. These findings highlight the potential of MoE-enhanced RL for large-scale decision-making with complex spatiotemporal dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CurvaDion: Curvature-Adaptive Distributed Orthonormalization</title>
<link>https://arxiv.org/abs/2512.13728</link>
<guid>https://arxiv.org/abs/2512.13728</guid>
<content:encoded><![CDATA[
<div> Keywords: distributed training, gradient synchronization, curvature detection, momentum dynamics, communication efficiency<br /><br />Summary:<br />1. As language models increase to trillions of parameters, distributed training across many GPUs requires efficient gradient synchronization, which remains a critical bottleneck due to communication overhead. <br />2. Existing methods like Dion reduce communication by applying low-rank updates but still synchronize gradients at every training step regardless of the optimization landscape's characteristics. <br />3. The authors observe that synchronization needs vary during training: in flat regions, gradients computed by workers are similar, making frequent synchronization unnecessary, whereas in high-curvature regions, tight coordination is required to avoid divergence. <br />4. To address this, they propose CurvaDion, which detects high-curvature areas via a metric called Relative Maximum Momentum Change (RMMC). RMMC leverages momentum information already computed during optimization as a low-cost, directionally sensitive proxy for curvature, requiring only \(\mathcal{O}(d)\) additional operations per layer. <br />5. The paper establishes theoretical links between RMMC and loss curvature, and their experimental results show that CurvaDion reduces communication by 99% while maintaining convergence comparable to baseline methods across models ranging from 160 million to 1.3 billion parameters. <div>
arXiv:2512.13728v1 Announce Type: new 
Abstract: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution</title>
<link>https://arxiv.org/abs/2512.13729</link>
<guid>https://arxiv.org/abs/2512.13729</guid>
<content:encoded><![CDATA[
<div> Keywords: wind super-resolution, diffusion models, classifier-free guidance, composite classifier-free guidance, WindDM<br /><br />Summary:<br />1. High-resolution and accurate wind data are crucial for various weather-related applications such as forecasting and optimizing turbine placements, but obtaining such data is expensive and challenging.<br />2. Traditional reconstruction methods tend to be either cost-effective or accurate, rarely achieving both simultaneously.<br />3. Deep learning approaches, particularly diffusion models inspired by natural image super-resolution, have been introduced to overcome this trade-off.<br />4. Wind data differs from natural images since it often requires many more input channels (over 10 channels) compared to typical 3-channel RGB images, which poses challenges for conditioning in diffusion models.<br />5. To address this, the authors propose a novel generalization called composite classifier-free guidance (CCFG), which effectively manages multiple conditioning inputs and can be applied to any pre-trained diffusion model using standard classifier-free guidance dropout.<br />6. Experimental results demonstrate that CCFG produces higher-fidelity outputs than standard CFG in wind super-resolution tasks.<br />7. The authors introduce WindDM, a diffusion model leveraging CCFG, tailored for industrial-scale wind dynamics reconstruction.<br />8. WindDM achieves state-of-the-art reconstruction quality among deep learning models while being up to 1000 times cheaper computationally than classical reconstruction methods. <div>
arXiv:2512.13729v1 Announce Type: new 
Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion</title>
<link>https://arxiv.org/abs/2512.13732</link>
<guid>https://arxiv.org/abs/2512.13732</guid>
<content:encoded><![CDATA[
<div> Keywords: PDE inversion, sparse observations, Set Transformer, physical parameters, uncertainty quantification<br /><br />Summary:  
The paper addresses the ill-posed problem of estimating physical parameters governed by partial differential equations (PDEs) from limited and irregular sensor measurements. This issue is prevalent in applications like fluid mechanics, seismic inversion, and structural health monitoring. Traditional deep learning and operator-based models struggle due to assumptions of fixed grids and perform poorly with sparse, irregular sampling, lacking robustness and uncertainty quantification (UQ). The authors propose the Physical Inversion Solver (PIS), a diffusion-based inversion framework that can handle arbitrary sets of observations. PIS uses a Set Transformer encoder allowing it to process variable numbers and geometries of measurements effectively. Additionally, a cosine-annealed sparsity curriculum is introduced to enhance robustness against sparse data. An information-theoretic analysis accompanies the framework, elucidating the fundamental limits of inversion by examining how observation entropy changes with sparsity. The method is tested on three PDE inverse problems: Darcy flow, Helmholtz wavefield inversion, and structural health monitoring via Hooke's Law. Unlike existing models that often fail or collapse under extreme sparsity (down to 0.29% observation rates), PIS maintains stability and significantly reduces inversion error (between 12.28% and 88.73%). It also provides well-calibrated posterior samples capturing uncertainties due to data scarcity and intrinsic physical ambiguities, positioning PIS as a broadly applicable, sparsity-resilient inversion tool. <div>
arXiv:2512.13732v1 Announce Type: new 
Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Compression of Language Models via Differentiable Rank Selection</title>
<link>https://arxiv.org/abs/2512.13733</link>
<guid>https://arxiv.org/abs/2512.13733</guid>
<content:encoded><![CDATA[
<div> Keywords: low-rank decomposition, large-language models, mask learning, compression rate, fine-tuning-free

<br /><br />Summary:  
This paper addresses the challenge of compressing large language models (LLMs) through low-rank decomposition, focusing on optimizing the selection of singular value ranks per layer to balance compression and task accuracy. Existing methods either use heuristics with limited discrete search spaces, leading to sub-optimal results, or gradient-based approaches that require post-compression fine-tuning to be competitive. The authors propose Learning to Low-Rank Compress (LLRC), a novel gradient-based method that learns mask weights selecting singular values directly without requiring fine-tuning. LLRC leverages a small calibration dataset to minimize the divergence between intermediate activations of the compressed and original models while progressively reducing the number of singular values. Their approach consistently outperforms other fine-tuning-free ranking selection methods such as Sensitivity-based Truncation Rank Searching (STRS) on various benchmarks like MMLU, BoolQ, and OpenbookQA, notably achieving a 12% improvement on MMLU with a 20% compression rate on Llama-2-13B. Additionally, LLRC surpasses fine-tuning-free versions of SVD-LLM and LLM-Pruner across different tasks and compression levels, while remaining competitive with fine-tuned variants of LLM-Pruner. This demonstrates LLRC’s effectiveness as a compression strategy that avoids costly post-compression fine-tuning. <div>
arXiv:2512.13733v1 Announce Type: new 
Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation</title>
<link>https://arxiv.org/abs/2512.13734</link>
<guid>https://arxiv.org/abs/2512.13734</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Recommendation, Parameter-Efficient Fine-Tuning, Embedding Compression, Communication Efficiency, Residual Quantized Variational Autoencoders  

<br /><br />Summary:  
This paper addresses the challenge of communication inefficiency in Federated Recommendation (FR) systems due to the large volume of embedding parameters, primarily item embeddings. It highlights that existing FR methods focus on improving model efficiency but largely overlook embedding parameter overhead. To tackle this, the authors propose a Parameter-Efficient Fine-Tuning (PEFT) based embedding framework designed to substantially reduce the embedding parameters transmitted during training. The framework is lightweight and plugin-style, enabling seamless integration with existing FR models without major modifications. The authors incorporate well-known PEFT techniques like LoRA and Hash-based encoding into their approach and introduce a novel strategy using Residual Quantized Variational Autoencoders (RQ-VAE) to further optimize parameter efficiency. Extensive experiments conducted on various FR model backbones and datasets show that their PEFT framework achieves significant communication overhead reduction and simultaneously improves recommendation accuracy. This demonstrates the dual benefit of the approach in enhancing both communication efficiency and model performance in federated settings. The source code for the framework is publicly available, promoting reproducibility and further research in this important area of distributed recommendation systems. <div>
arXiv:2512.13734v1 Announce Type: new 
Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series</title>
<link>https://arxiv.org/abs/2512.13735</link>
<guid>https://arxiv.org/abs/2512.13735</guid>
<content:encoded><![CDATA[
<div> Keywords: Multivariate Time Series, Anomaly Detection, Spatiotemporal Dependencies, Graph Neural Networks, Noise Robustness<br /><br />Summary:<br />Multivariate Time Series Anomaly Detection (MTSAD) targets the identification and localization of complex abnormal patterns in large-scale industrial control systems. Existing methods work well for low-dimensional data but struggle with capturing long-range spatiotemporal dependencies in high-dimensional, noisy time series. To overcome these challenges, the authors propose DARTs, a novel framework combining long- and short-term paths with a window-aware spatiotemporal soft fusion mechanism. The short-term path features a Multi-View Sparse Graph Learner and Diffusion Multi-Relation Graph Unit to adaptively extract hierarchical discriminative short-term spatiotemporal patterns despite noise. The long-term path incorporates a Multi-Scale Spatiotemporal Graph Constructor designed to model salient long-term dynamics across the high-dimensional representation space. The proposed window-aware fusion mechanism filters residual noise and integrates anomalous patterns effectively. Extensive experiments on mainstream benchmarks demonstrate DARTs’ superiority and robustness compared to existing approaches. Ablation studies further validate the importance of each component in the framework. The authors plan to release their code and pre-trained models publicly, facilitating future research and application in industrial anomaly detection. <div>
arXiv:2512.13735v1 Announce Type: new 
Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection</title>
<link>https://arxiv.org/abs/2512.13736</link>
<guid>https://arxiv.org/abs/2512.13736</guid>
<content:encoded><![CDATA[
<div> Keywords: Major Depressive Disorder, EEG, contrastive learning, time-frequency fusion, self-supervised learning<br /><br />Summary:<br /><br />The article addresses the challenges in detecting Major Depressive Disorder (MDD) using EEG signals, particularly the difficulty of obtaining labeled data for supervised learning methods. To overcome this, the authors propose a self-supervised contrastive learning approach, which is less dependent on labeled data. They identify that current contrastive learning methods are not optimized to characterize the time-frequency distribution of EEG signals, limiting their effectiveness for MDD detection. To improve this, they introduce the TF-MCL model, which incorporates a fusion mapping head (FMH) to generate hybrid time-frequency representations, enhancing the synthesis of time and frequency domain information. Additionally, the model optimizes a multi-domain cross-loss function, which aligns the distribution of representations across time-frequency and fusion domains, thereby strengthening the learning of fused features. The model’s performance was evaluated on two publicly available datasets, MODMA and PRED+CT, achieving significant accuracy improvements over existing state-of-the-art methods by 5.87% and 9.96%, respectively. These results demonstrate the effectiveness of the TF-MCL approach in improving EEG-based MDD detection through self-supervised learning and enhanced time-frequency representation fusion. <div>
arXiv:2512.13736v1 Announce Type: new 
Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models</title>
<link>https://arxiv.org/abs/2512.13741</link>
<guid>https://arxiv.org/abs/2512.13741</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, jailbreak detection, Semantic Turbulence, Laminar Flow Hypothesis, safety alignment<br /><br />Summary: This paper addresses the growing problem of adversarial "jailbreaking" attacks on Large Language Models (LLMs), proposing a novel defense approach grounded in the model's internal reasoning dynamics rather than external classifiers or lexical filters. It introduces the Laminar Flow Hypothesis, which suggests that benign inputs create smooth, consistent transitions in the model's high-dimensional latent space, whereas adversarial prompts provoke chaotic, high-variance behaviors termed Semantic Turbulence. This internal conflict arises between the model's safety alignment and instruction-following goals. To quantify this phenomenon, the authors propose a new zero-shot metric called the variance of layer-wise cosine velocity, measuring fluctuations in the latent space during processing. Experimental validation on multiple small-scale LLMs demonstrates strong diagnostic power: the RLHF-aligned Qwen2-1.5B model shows a 75.4% statistically significant increase in Semantic Turbulence under attack, supporting the hypothesis of internal conflict. In contrast, the Gemma-2B model exhibits a 22.0% decrease, indicating a fundamentally different, low-entropy refusal mechanism that operates as a reflex. These results establish Semantic Turbulence as both a lightweight, real-time jailbreak detection tool and a diagnostic method capable of revealing the hidden safety architecture of black-box LLMs. <div>
arXiv:2512.13741v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.13749</link>
<guid>https://arxiv.org/abs/2512.13749</guid>
<content:encoded><![CDATA[
<div> Keywords: financial sentiment analysis, embedding methods, small datasets, data scarcity, model overfitting<br /><br />Summary: This study evaluates different embedding-based methods—Word2Vec, GloVe, and sentence transformers—for financial news sentiment classification, focusing on scenarios with limited labeled data. It demonstrates that despite strong validation performance, these models often underperform trivial baselines on test data due to a significant gap caused by small dataset sizes. The research highlights that pretrained embeddings alone cannot overcome problems introduced by data scarcity, identifying a critical threshold below which embedding quality offers diminishing returns. Additionally, the study shows how small validation sets contribute to model overfitting during selection, misleading performance assessments. To exemplify practical utility, the research applies the models in weekly sentiment aggregation and narrative summarization workflows for market monitoring, illustrating real-world challenges. The findings underscore the importance for practitioners working with limited resources to explore alternative strategies such as few-shot learning, data augmentation, or hybrid approaches that incorporate lexical knowledge. Overall, the paper provides empirical evidence that directly impacts how sentiment classification tasks should be approached under resource constraints in financial contexts. <div>
arXiv:2512.13749v1 Announce Type: new 
Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIDUS: Memory-Infused Depth Up-Scaling</title>
<link>https://arxiv.org/abs/2512.13751</link>
<guid>https://arxiv.org/abs/2512.13751</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Depth Up-Scaling, Memory-Infused Layers, Head-wise Memory, Continual Pre-training<br /><br />Summary:<br /><br />1. The paper addresses the challenge of scaling large language models (LLMs) with methods that increase model capacity without significantly increasing parameters or inference costs.<br /><br />2. Depth Up-Scaling (DUS) is highlighted as a method that duplicates layers followed by Continual Pre-training (CPT), but traditional implementations rely heavily on feed-forward networks (FFNs), limiting efficiency and performance gains.<br /><br />3. The authors propose Memory-Infused Depth Up-Scaling (MIDUS), which replaces the FFNs in duplicated layers with a head-wise memory layer, providing each attention head with an independent memory bank.<br /><br />4. This design allows for head-wise information retrieval and injection into subsequent layers, preserving the functional structure of attention heads while enabling sparse memory access and adding an efficient per-head value factorization module.<br /><br />5. Experimental results demonstrate that MIDUS outperforms strong DUS baselines in continual pre-training settings while maintaining a more efficient parameter footprint, making it a promising and resource-efficient alternative for scaling LLMs through depth up-scaling. <div>
arXiv:2512.13751v1 Announce Type: new 
Abstract: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention</title>
<link>https://arxiv.org/abs/2512.13758</link>
<guid>https://arxiv.org/abs/2512.13758</guid>
<content:encoded><![CDATA[
<div> Traffic volume estimation, spatio-temporal graph neural network, probe vehicle speeds, road network topology, inductive learning<br /><br />Summary:  
This paper introduces the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), a novel inductive deep learning framework designed for network-wide traffic volume estimation. Existing methods tend to focus on forecasting traffic on sensor-equipped roads or imputing missing volumes using nearby sensors, which either ignore unmonitored roads or require volume data at inference time, limiting applicability in sensor-scarce cities. The proposed model leverages widely available probe vehicle speed data and static road attributes alongside road network topology to predict daily traffic volume profiles across all road segments. HDA-STGNN effectively captures complex spatio-temporal dependencies and integrates topological information for enhanced accuracy. Extensive ablation studies demonstrate the model’s ability to estimate traffic volumes without relying on volume data during inference, enabling full coverage even in networks with sparse sensor deployments. This approach provides a scalable, practical solution for urban traffic volume estimation, addressing limitations of traditional forecasting and spatial imputation methods. <div>
arXiv:2512.13758v1 Announce Type: new 
Abstract: Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training</title>
<link>https://arxiv.org/abs/2512.13770</link>
<guid>https://arxiv.org/abs/2512.13770</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-view learning, Graph convolutional network, Semi-supervised learning, Contrastive loss, Pseudo-labeling<br /><br />Summary:<br /><br />The paper introduces MV-SupGCN, a semi-supervised graph convolutional network designed for multi-view learning, aiming to improve feature representation by integrating complementary information across multiple views. First, the model employs a joint loss function combining Cross-Entropy and Supervised Contrastive loss to minimize intra-class variance and maximize inter-class separability, enhancing discriminative feature learning and generalization. Second, it addresses the instability of single graph construction by combining KNN-based and semi-supervised graph construction methods on each view, improving the robustness and reducing generalization errors of the data structure representation. Third, MV-SupGCN utilizes abundant unlabeled data by integrating contrastive learning to enforce consistency among multi-view embeddings, capturing meaningful inter-view relationships. This is complemented with pseudo-labeling, which supplies additional supervision to both the cross-entropy and contrastive loss functions, thereby further enhancing model generalization. Extensive experimental results show that MV-SupGCN consistently outperforms state-of-the-art methods on diverse benchmarks, confirming the effectiveness of its integrated approach. The source code has been made publicly available for further research and application development. <div>
arXiv:2512.13770v1 Announce Type: new 
Abstract: The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constrained Policy Optimization via Sampling-Based Weight-Space Projection</title>
<link>https://arxiv.org/abs/2512.13788</link>
<guid>https://arxiv.org/abs/2512.13788</guid>
<content:encoded><![CDATA[
<div> Keywords: safety-critical learning, constrained policy learning, parameter space projection, safe-by-induction, closed-loop stability  

<br /><br />Summary:  
This paper addresses safety-critical learning by focusing on constrained policy learning where policies must satisfy unknown safety constraints evaluated through rollouts. The authors propose SCPO, a novel method that enforces safety directly in the parameter space using a sampling-based weight-space projection approach, without requiring gradient information of the constraints. SCPO constructs a local safe region by leveraging trajectory rollouts combined with smoothness bounds that connect changes in model parameters with changes in safety metrics. Each gradient update is projected through a convex second-order cone program (SOCP) to generate a safe first-order policy update. The authors prove a safe-by-induction guarantee, ensuring that starting from any safe initial policy, all subsequent policies remain safe under feasible projections. In scenarios with a stabilizing backup policy, their method guarantees closed-loop stability and enables safe adaptation that goes beyond conservative backup policies. Empirical evaluations in regression tasks with harmful supervision and in a constrained control task with a malicious expert demonstrate that SCPO consistently rejects unsafe updates, maintains feasibility during training, and achieves meaningful improvements in the primary objective. This approach provides a principled framework for safe policy optimization in unknown constrained environments. <div>
arXiv:2512.13788v1 Announce Type: new 
Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models</title>
<link>https://arxiv.org/abs/2512.13806</link>
<guid>https://arxiv.org/abs/2512.13806</guid>
<content:encoded><![CDATA[
<div> Deep learning, EEG decoding, latent components, overfitting, few-shot learning<br /><br />Summary:<br /><br />1. This paper addresses the challenge of overfitting in deep learning models applied to EEG signal decoding, highlighting the gap between high benchmark accuracy and poor generalization to real-world applications.<br />2. The authors introduce Disentangled Decoding Decomposition (D3), a weakly supervised training method that predicts the position of EEG data segments within trial sequences to disentangle latent brain activity components, inspired by nonlinear independent component analysis (ICA).<br />3. A novel model architecture with fully independent sub-networks is proposed to ensure strict interpretability of the latent components.<br />4. The paper presents a feature interpretation framework that compares component activation profiles between different EEG datasets and examines their temporal and spatial filter characteristics.<br />5. On motor imagery data, EEG-D3 reliably separates genuine brain activity components from task-related artefacts, reducing hidden overfitting and improving the robustness of downstream classifiers.<br />6. The method also enables effective few-shot learning on sleep stage classification by exploiting the linearly separable latent space.<br />7. This approach requires minimal labeled data, enhancing model generalization in practical scenarios.<br />8. By disentangling brain processes, EEG-D3 offers neuroscientists a tool to explore and potentially discover new brain dynamics, advancing both applied and fundamental research in neural signal analysis. <div>
arXiv:2512.13806v1 Announce Type: new 
Abstract: Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces</title>
<link>https://arxiv.org/abs/2512.13821</link>
<guid>https://arxiv.org/abs/2512.13821</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Code Generation, Backdoor Detection, Semantic Orbit Analysis, Adversarial Robustness Quotient<br /><br />Summary:<br /><br />This paper addresses security concerns related to large language models (LLMs) that generate code, focusing on the risk of backdoor injection and malicious outputs. It introduces the Cross-Trace Verification Protocol (CTVP), an innovative AI control framework designed to verify the trustworthiness of untrusted code-generating models. Instead of executing the generated code, which might be harmful, CTVP analyzes the model's predicted execution traces over multiple semantically equivalent program transformations, forming a semantic orbit. By examining the consistency of these predicted execution traces, the protocol can identify behavioral anomalies suggestive of backdoor manipulations. The paper also presents the Adversarial Robustness Quotient (ARQ), a metric quantifying the computational effort of verification relative to normal code generation, which grows exponentially with the size of the semantic orbit. Theoretical information-theoretic analysis reveals that adversaries cannot exploit (or gamify) the protocol through training because of inherent space complexity constraints. Conclusively, the study demonstrates that semantic orbit analysis provides a scalable and theoretically justified approach to monitoring and controlling AI-driven code generation, enhancing the safety and reliability of LLM-generated programs. <div>
arXiv:2512.13821v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable reinforcement learning from human feedback to improve alignment</title>
<link>https://arxiv.org/abs/2512.13837</link>
<guid>https://arxiv.org/abs/2512.13837</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning from human feedback, language models, post-hoc explanation, unlearning, data selection<br /><br />Summary: This paper explores applying a human improvement strategy—identifying causes of unsatisfactory outcomes and correcting them—to enhance reinforcement learning from human feedback (RLHF) used in aligning language models (LMs). It highlights that despite RLHF tuning, LMs can still generate unsatisfactory responses. To address this, the authors propose a two-part method. First, they develop a post-hoc explanation technique that identifies specific training data causing an unsatisfactory prompt-response output. This is formulated as a constrained combinatorial optimization problem, aiming to find a set of training examples closest to the problematic prompt-response pair in a feature space, with the constraint that the pair can be represented as a convex combination of this set. An efficient iterative data selection algorithm is introduced to solve this problem. Second, the paper presents an unlearning method that removes the influence of identified training data responsible for poor responses, aiming to improve these responses without significantly degrading others that are satisfactory. Experimental results demonstrate that this approach can effectively improve RLHF performance, leading to better alignment and quality of language model outputs. <div>
arXiv:2512.13837v1 Announce Type: new 
Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topologically-Stabilized Graph Neural Networks: Empirical Robustness Across Domains</title>
<link>https://arxiv.org/abs/2512.13852</link>
<guid>https://arxiv.org/abs/2512.13852</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Persistent Homology, Stability Regularization, Robustness, Topological Features<br /><br />Summary:<br /><br />1. This paper addresses the vulnerability of Graph Neural Networks (GNNs) to structural perturbations by proposing a new framework that integrates persistent homology features with stability regularization. <br />2. The approach builds upon the stability theorems of persistent homology and combines Graph Isomorphism Network (GIN) architectures with multi-scale topological features derived from persistence images. <br />3. Stability constraints inspired by Hiraoka-Kusano theory are used to enforce robustness during learning, linking recent advances in topological regularization to GNN design. <br />4. Experimental evaluation on six diverse datasets, including biochemical, social, and collaboration networks, demonstrates that the proposed method significantly improves robustness to edge perturbations while maintaining competitive accuracy. <br />5. Results show minimal performance degradation (0-4% on most datasets) under perturbations, outperforming baseline models and providing a theoretically grounded and empirically validated approach to robust graph learning through topological techniques. <div>
arXiv:2512.13852v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have become the standard for graph representation learning but remain vulnerable to structural perturbations. We propose a novel framework that integrates persistent homology features with stability regularization to enhance robustness. Building on the stability theorems of persistent homology \cite{cohen2007stability}, our method combines GIN architectures with multi-scale topological features extracted from persistence images, enforced by Hiraoka-Kusano-inspired stability constraints. Across six diverse datasets spanning biochemical, social, and collaboration networks , our approach demonstrates exceptional robustness to edge perturbations while maintaining competitive accuracy. Notably, we observe minimal performance degradation (0-4\% on most datasets) under perturbation, significantly outperforming baseline stability. Our work provides both a theoretically-grounded and empirically-validated approach to robust graph learning that aligns with recent advances in topological regularization
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dropout Neural Network Training Viewed from a Percolation Perspective</title>
<link>https://arxiv.org/abs/2512.13853</link>
<guid>https://arxiv.org/abs/2512.13853</guid>
<content:encoded><![CDATA[
<div> Keywords: percolation, dropout, neural networks, stochastic gradient descent, network topology  

<br /><br />Summary:  
This work explores the concept of percolation within the context of training deep neural networks (NNs) using dropout, a technique that randomly removes connections during training to regularize the model. The authors draw an analogy between the random removal of connections in dropout and percolation, a well-studied model in statistical physics. They specifically investigate whether dropout can remove enough connections to sever the path between the input and output layers of the NN, which would prevent the model from making informed predictions. To analyze this, new percolation models are proposed that mimic the dropout process on neural network topologies. Their theoretical findings confirm the existence of a percolative effect in dropout, showing that the network's underlying topology plays a crucial role in maintaining connectivity during training. Furthermore, they demonstrate that in the absence of bias terms, this percolative effect can cause a breakdown in training, where the network fails to learn effectively. The authors also provide heuristic arguments suggesting that this detrimental effect extends to networks with biases as well, highlighting important considerations for the design and training of dropout-enabled neural networks. <div>
arXiv:2512.13853v1 Announce Type: new 
Abstract: In this work, we investigate the existence and effect of percolation in training deep Neural Networks (NNs) with dropout. Dropout methods are regularisation techniques for training NNs, first introduced by G. Hinton et al. (2012). These methods temporarily remove connections in the NN, randomly at each stage of training, and update the remaining subnetwork with Stochastic Gradient Descent (SGD). The process of removing connections from a network at random is similar to percolation, a paradigm model of statistical physics.
  If dropout were to remove enough connections such that there is no path between the input and output of the NN, then the NN could not make predictions informed by the data. We study new percolation models that mimic dropout in NNs and characterise the relationship between network topology and this path problem. The theory shows the existence of a percolative effect in dropout. We also show that this percolative effect can cause a breakdown when training NNs without biases with dropout; and we argue heuristically that this breakdown extends to NNs with biases.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Uncertainty Calibration</title>
<link>https://arxiv.org/abs/2512.13872</link>
<guid>https://arxiv.org/abs/2512.13872</guid>
<content:encoded><![CDATA[
<div> Calibration, binary classifier, L1 error, bounded variation, non-asymptotic<br /><br />Summary:<br /><br />This article addresses the challenge of estimating the $L_1$ calibration error of binary classifiers using finite datasets. First, it establishes an upper bound on this calibration error for classifiers whose calibration functions have bounded variation, providing a theoretical guarantee that can be applied broadly. Second, the authors introduce a novel method to modify any given classifier in a way that enables efficient upper bounding of its calibration error. This modification does not significantly degrade the classifier’s predictive performance and does not rely on restrictive assumptions, making it widely applicable. Importantly, all results presented are non-asymptotic, meaning they hold for finite samples, and distribution-free, so they do not depend on specific data distribution characteristics. Finally, the paper offers practical guidance on how to measure calibration error in real-world settings, emphasizing procedures that can be implemented with modest computational overhead. These contributions collectively offer both theoretical insights and practical tools for better assessing and improving the calibration of binary classifiers in applied machine learning contexts. <div>
arXiv:2512.13872v1 Announce Type: new 
Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization</title>
<link>https://arxiv.org/abs/2512.13880</link>
<guid>https://arxiv.org/abs/2512.13880</guid>
<content:encoded><![CDATA[
<div> Keywords: infant cry classification, federated learning, denoising autoencoder, noise robustness, privacy-preserving

<br /><br />Summary:  
This paper addresses the challenge of infant cry classification, which is crucial for early assessment of infant needs but hindered by privacy concerns, background noise, and domain shifts in recording environments. The authors propose an end-to-end pipeline combining a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder, designed to work efficiently on-device. The system incorporates adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention to improve robustness and reliability. Federated learning (FL) is utilized to preserve privacy by training models across devices without sharing raw data, employing a communication-efficient method using regularized control variate updates and 8-bit adapter deltas under secure aggregation. Experiments conducted on the Baby Chillanto and Donate-a-Cry datasets with added ESC-50 noise overlays show a strong classification performance with a macro F1 score of 0.938, AUC of 0.962, and low Expected Calibration Error (ECE) of 0.032. The approach significantly reduces communication overhead, cutting client upload size per round from about 36–42 MB down to 3.3 MB. Additionally, real-time inference on an NVIDIA Jetson Nano edge device requires only 96 ms per one-second spectrogram frame. These advancements highlight a viable, privacy-conscious, noise-robust, and communication-efficient solution for infant cry classification suitable for federated deployment scenarios. <div>
arXiv:2512.13880v1 Announce Type: new 
Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction</title>
<link>https://arxiv.org/abs/2512.13886</link>
<guid>https://arxiv.org/abs/2512.13886</guid>
<content:encoded><![CDATA[
<div> Keywords: post-training pruning, OPTIMA, quadratic programming, large language models, model compression  

<br /><br />Summary:  
1. Post-training model pruning balances the trade-off between fast, simple heuristics that degrade accuracy and computationally infeasible joint optimization methods that maintain accuracy.  
2. One-shot methods like SparseGPT provide a practical middle ground by applying efficient approximate heuristic weight updates but still leave room for improvement in accuracy and scalability.  
3. OPTIMA is introduced as a novel one-shot post-training pruning method that achieves a better balance of accuracy and scalability by formulating layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) sharing a common Hessian matrix per layer.  
4. The shared-Hessian structure allows batching on hardware accelerators, and a custom accelerator-friendly QP solver solves many small QPs in parallel, enabling large-scale pruning on a single accelerator without requiring fine-tuning.  
5. OPTIMA integrates with existing mask selectors, consistently improving zero-shot model performance by up to 3.97% absolute accuracy across multiple large language model families and sparsity levels. On an NVIDIA H100 accelerator, OPTIMA prunes an 8-billion-parameter transformer model end-to-end in 40 hours using 60GB peak memory, setting a new state-of-the-art in accuracy-efficiency trade-offs for one-shot post-training pruning. <div>
arXiv:2512.13886v1 Announce Type: new 
Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs</title>
<link>https://arxiv.org/abs/2512.13898</link>
<guid>https://arxiv.org/abs/2512.13898</guid>
<content:encoded><![CDATA[
<div> Long-context, inference-time compute, static self-attention, score dilution, gradient updates  

<br /><br />Summary:  
1. Large language models (LLMs) have achieved the ability to process millions of tokens in context, but their effective use of extremely long contexts remains limited.  
2. Current inference-time compute strategies, such as generating additional "thinking tokens" for complex multi-step reasoning tasks, show diminishing returns and fail when applied to long-context scenarios.  
3. The root cause of these failures is identified as "score dilution," an issue intrinsic to the static self-attention mechanism used in most LLM architectures, which hampers the retrieval of relevant signals from long contexts.  
4. The paper proposes a novel approach involving targeted gradient updates on the input context during inference; this method overcomes the limitations of static self-attention and enhances the model’s ability to use long contexts.  
5. Experimental results demonstrate substantial performance improvements, with increases of 12.6 and 14.1 percentage points on subsets of LongBench-v2 and ZeroScrolls benchmarks for the Qwen3-4B model, showing that context-specific training at inference is more effective than merely scaling inference-time compute by generating more tokens.  
6. The practical takeaway is that investing a small amount of inference-time compute in context-specific gradient updates yields better long-context performance than existing strategies focused on scaling token generation. <div>
arXiv:2512.13898v1 Announce Type: new 
Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America</title>
<link>https://arxiv.org/abs/2512.13910</link>
<guid>https://arxiv.org/abs/2512.13910</guid>
<content:encoded><![CDATA[
<div> Keywords: precipitation forecasting, machine learning, deep learning, South America, explainable AI  

<br /><br />Summary:  
1. Forecasting meteorological variables like precipitation is difficult due to their complex nature, yet accurate predictions are crucial for mitigating climate-related impacts on society.  
2. This study evaluates purely data-driven approaches for precipitation forecasting across all seasons of 2019 in South America, comparing classical machine learning (Random Forests, XGBoost) and deep learning methods (1D CNN, LSTM, GRU).  
3. The Brazilian Global Atmospheric Model (BAM) serves as a benchmark representing traditional dynamic modeling.  
4. Explainable artificial intelligence (XAI) techniques were applied to interpret model behavior and improve understanding of results.  
5. Among the models tested, the LSTM deep learning model achieved the best predictive performance, particularly excelling in forecasting heavy precipitation events, despite having higher computational latency.  
6. XGBoost offered a good trade-off with lower latency and only a slight reduction in accuracy, making it a suitable option when computational cost is a concern.  
7. BAM, the traditional dynamic model, performed the worst in forecasting accuracy in this study.  
8. The results support the feasibility and effectiveness of deep learning models in climate and meteorological forecasting, aligning with global trends in meteorological research and operational centers. <div>
arXiv:2512.13910v1 Announce Type: new 
Abstract: Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations</title>
<link>https://arxiv.org/abs/2512.13913</link>
<guid>https://arxiv.org/abs/2512.13913</guid>
<content:encoded><![CDATA[
<div> Keywords: quantum many-body systems, two-particle reduced density matrix, neural ODE, three-particle cumulant, memory effects<br /><br />Summary:<br /><br />1. Out-of-equilibrium quantum many-body systems develop rapid correlation buildup critical to emergent phenomena, but exact wave-function methods scale exponentially with particle number, and mean-field approaches miss essential two-particle correlations.<br /><br />2. The time-dependent two-particle reduced density matrix (TD2RDM) formalism provides a compromise by evolving the 2RDM and approximating the three-particle cumulant to close the BBGKY hierarchy.<br /><br />3. It remains uncertain whether time-local reconstruction functionals that ignore memory effects can effectively describe dynamics across different regimes.<br /><br />4. Using a neural ordinary differential equation (neural ODE) trained on exact 2RDM data, the study demonstrates that the dynamics can be reproduced without explicit three-particle information only when the two- and three-particle cumulants are strongly correlated.<br /><br />5. In regimes where these cumulants are anti-correlated or uncorrelated, the neural ODE fails, indicating the necessity for memory-dependent kernels in three-particle cumulant reconstruction.<br /><br />6. The magnitude of time-averaged three-particle correlation buildup predicts the success of both neural ODE and existing TD2RDM reconstructions; moderate buildup yields accurate predictions while strong buildup causes systematic failure.<br /><br />7. The neural ODE thus serves as a model-agnostic diagnostic tool to identify suitable regimes for cumulant expansion methods and to guide development of non-local closure schemes.<br /><br />8. More generally, this approach offers a pathway for fast, data-driven simulations of correlated quantum matter by learning high-dimensional reduced density matrix dynamics with limited data. <div>
arXiv:2512.13913v1 Announce Type: new 
Abstract: Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics</title>
<link>https://arxiv.org/abs/2512.13919</link>
<guid>https://arxiv.org/abs/2512.13919</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive digital twins, probabilistic graphical models, dynamic Bayesian networks, reinforcement learning, structural health monitoring<br /><br />Summary:<br /><br />This work demonstrates how adaptivity improves the value realization of digital twins in civil engineering by focusing on adaptive state transition models within digital twins. These digital twins are represented via probabilistic graphical models, specifically dynamic Bayesian networks that capture bi-directional interactions between physical and virtual domains. The method treats state transition probabilities as random variables with conjugate priors, facilitating hierarchical online learning of system dynamics through Bayesian updates. The authors extend the mathematical framework to cover a broader class of distributions than prior literature. For policy computation with precision updates, parametric Markov decision processes are solved using reinforcement learning techniques. This adaptive digital twin approach results in enhanced personalization, increased system robustness, and better cost-effectiveness in maintenance operations. The framework is validated through a case study focused on structural health monitoring and maintenance planning for a railway bridge, demonstrating practical benefits in civil engineering asset management. <div>
arXiv:2512.13919v1 Announce Type: new 
Abstract: This work shows how adaptivity can enhance value realization of digital twins in civil engineering. We focus on adapting the state transition models within digital twins represented through probabilistic graphical models. The bi-directional interaction between the physical and virtual domains is modeled using dynamic Bayesian networks. By treating state transition probabilities as random variables endowed with conjugate priors, we enable hierarchical online learning of transition dynamics from a state to another through effortless Bayesian updates. We provide the mathematical framework to account for a larger class of distributions with respect to the current literature. To compute dynamic policies with precision updates we solve parametric Markov decision processes through reinforcement learning. The proposed adaptive digital twin framework enjoys enhanced personalization, increased robustness, and improved cost-effectiveness. We assess our approach on a case study involving structural health monitoring and maintenance planning of a railway bridge.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sliding Window Recurrences for Sequence Models</title>
<link>https://arxiv.org/abs/2512.13921</link>
<guid>https://arxiv.org/abs/2512.13921</guid>
<content:encoded><![CDATA[
<div> Multi-hybrid architectures, hierarchical decomposition, Sliding Window Recurrences, Phalanx layers, GPU memory optimization  

<br /><br />Summary:  
This paper introduces a hierarchical decomposition framework for linear recurrences designed to optimize language model performance by better aligning computations with GPU memory hierarchies. The approach leads to the creation of Sliding Window Recurrences (SWR), which focus specifically on truncating recurrences into hardware-aligned windows. These windows are jagged in shape, which naturally limits the expensive communication between GPU warps, thereby improving computational efficiency. Utilizing SWR, the authors develop Phalanx layers that can replace traditional windowed attention mechanisms or linear recurrence layers seamlessly. In large-scale multi-hybrid language models with 1 billion parameters, integrating Phalanx layers results in significant speed improvements, achieving a 10-40% increase in processing speed across varied context lengths from 4,000 to 32,000 tokens. Importantly, these performance gains do not come at the cost of model quality, as the perplexity remains comparable to that of state-of-the-art optimized Transformer models. Overall, the work presents a scalable and hardware-aware solution for enhancing both the efficiency and quality of long-context language models. <div>
arXiv:2512.13921v1 Announce Type: new 
Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Complete Guide to Spherical Equivariant Graph Transformers</title>
<link>https://arxiv.org/abs/2512.13927</link>
<guid>https://arxiv.org/abs/2512.13927</guid>
<content:encoded><![CDATA[
<div> Keywords: spherical equivariant graph neural networks, SO(3) group, molecular modeling, tensor field networks, equivariant message-passing  

<br /><br />Summary:  
This article presents a comprehensive and intuitive guide to spherical equivariant graph neural networks (EGNNs), which are designed to handle 3D molecular and biomolecular data while respecting rotational symmetries dictated by physics. It begins by establishing foundational concepts including group representations, spherical harmonics, tensor products, and the Clebsch-Gordan decomposition, which are essential for constructing SO(3)-equivariant kernels. These kernels ensure that neural network predictions transform correctly under rotations, maintaining physical relevance. Building on this theoretical groundwork, the authors develop and discuss the Tensor Field Network and SE(3)-Transformer architectures, which extend traditional message-passing graph neural networks and Transformers by encoding features as spherical tensors. The paper explains how these architectures perform equivariant message-passing and attention mechanisms on geometric graphs, enabling effective learning of molecular structures. Additionally, the guide includes detailed mathematical derivations and annotated code snippets to facilitate understanding and implementation. The intended audience includes researchers and practitioners interested in applying spherical EGNNs to chemistry-related domains such as molecular property prediction, protein structure modeling, and generative modeling. This work provides a self-contained resource to bridge theoretical concepts with practical network architectures for equivariant learning on 3D molecular data. <div>
arXiv:2512.13927v1 Announce Type: new 
Abstract: Spherical equivariant graph neural networks (EGNNs) provide a principled framework for learning on three-dimensional molecular and biomolecular systems, where predictions must respect the rotational symmetries inherent in physics. These models extend traditional message-passing GNNs and Transformers by representing node and edge features as spherical tensors that transform under irreducible representations of the rotation group SO(3), ensuring that predictions change in physically meaningful ways under rotations of the input. This guide develops a complete, intuitive foundation for spherical equivariant modeling - from group representations and spherical harmonics, to tensor products, Clebsch-Gordan decomposition, and the construction of SO(3)-equivariant kernels. Building on this foundation, we construct the Tensor Field Network and SE(3)-Transformer architectures and explain how they perform equivariant message-passing and attention on geometric graphs. Through clear mathematical derivations and annotated code excerpts, this guide serves as a self-contained introduction for researchers and learners seeking to understand or implement spherical EGNNs for applications in chemistry, molecular property prediction, protein structure modeling, and generative modeling.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Informing Acquisition Functions via Foundation Models for Molecular Discovery</title>
<link>https://arxiv.org/abs/2512.13935</link>
<guid>https://arxiv.org/abs/2512.13935</guid>
<content:encoded><![CDATA[
<div> Bayesian Optimization, Large Language Models, Molecular Discovery, Monte Carlo Tree Search, Surrogate Modeling<br /><br />Summary:<br /><br />This paper addresses the challenges of Bayesian Optimization (BO) in molecular discovery, particularly in low-data regimes with vast candidate spaces and limited prior knowledge. Traditional BO relies on probabilistic surrogate models to map molecules to properties and optimize acquisition functions, but faces limitations due to high-dimensional features and the computational cost of deep Bayesian surrogates. To overcome these issues, the authors propose a likelihood-free BO approach that eliminates explicit surrogate modeling and directly leverages priors from both general large language models (LLMs) and chemistry-specific foundation models to guide acquisition function estimation. Their method incorporates a tree-structured partitioning of the molecular search space, applying local acquisition functions within each partition, and uses Monte Carlo Tree Search (MCTS) to efficiently navigate candidate selections. Additionally, the method employs coarse-grained LLM-based clustering to focus acquisition function evaluations on clusters with statistically higher property values, improving scalability to large candidate sets. Extensive experiments and ablation studies demonstrate that this approach significantly enhances scalability, robustness, and sample efficiency in LLM-guided Bayesian Optimization for molecular discovery tasks. <div>
arXiv:2512.13935v1 Announce Type: new 
Abstract: Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pattern-Guided Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13945</link>
<guid>https://arxiv.org/abs/2512.13945</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, pattern-guided forecasting, archetypal analysis, uncertainty quantification, multivariate time series<br /><br />Summary:<br /><br />1. The paper introduces Pattern-Guided Diffusion Models (PGDM), a novel approach for forecasting future data in multivariate time series by leveraging recurring patterns inherent in the data.<br /><br />2. PGDM utilizes archetypal analysis to extract patterns and predict the most likely next pattern in the temporal sequence, which guides the diffusion model’s forecasting process.<br /><br />3. This pattern-guided approach helps produce more realistic and plausible future predictions that conform to known temporal structures.<br /><br />4. The authors propose a new uncertainty quantification method based on archetypal analysis, allowing dynamic adjustment of the guidance strength depending on the uncertainty of the pattern estimate.<br /><br />5. The method was evaluated on two applications: predicting visual field measurements and motion capture frames, where PGDM showed significant improvement over baselines.<br /><br />6. Quantitative results demonstrate that pattern guidance enhanced performance metrics such as Mean Absolute Error (MAE) and Continuous Ranked Probability Score (CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10% for the two tasks respectively.<br /><br />7. When compared to baseline methods, PGDM outperformed them by large margins, up to 65.58% / 84.83% and 93.64% / 92.55%, showing the strength of leveraging inherent patterns in time series forecasting. <div>
arXiv:2512.13945v1 Announce Type: new 
Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Single Architecture for Representing Invariance Under Any Space Group</title>
<link>https://arxiv.org/abs/2512.13989</link>
<guid>https://arxiv.org/abs/2512.13989</guid>
<content:encoded><![CDATA[
<div> Keywords: space groups, machine learning, symmetry invariance, Fourier bases, zero-shot learning  

<br /><br />Summary:  
This work addresses the challenge of incorporating crystallographic symmetries, specifically the 230 three-dimensional space groups, into machine learning models for materials science and condensed matter physics. Traditionally, achieving exact invariance to these symmetries requires specialized architectures designed for each group, limiting scalability and knowledge transfer. The authors propose a novel approach that constructs symmetry-adapted Fourier bases by explicitly characterizing the constraints that group operations impose on Fourier coefficients. These constraints are encoded into a neural network layer, enabling automatic adaptation of weights to enforce invariance across any input space group. This weight-sharing mechanism allows the model to leverage structural similarities among different space groups, overcoming issues related to data sparsity when measurements are limited for specific groups. The approach facilitates zero-shot learning, enabling generalization to unseen space groups without retraining. Experiments demonstrate competitive performance on material property prediction tasks, showcasing the practical application and effectiveness of the method. This unified architecture thus significantly advances the scalability and transferability of symmetry-based machine learning models in materials science. <div>
arXiv:2512.13989v1 Announce Type: new 
Abstract: Incorporating known symmetries in data into machine learning models has consistently improved predictive accuracy, robustness, and generalization. However, achieving exact invariance to specific symmetries typically requires designing bespoke architectures for each group of symmetries, limiting scalability and preventing knowledge transfer across related symmetries. In the case of the space groups, symmetries critical to modeling crystalline solids in materials science and condensed matter physics, this challenge is particularly salient as there are 230 such groups in three dimensions. In this work we present a new approach to such crystallographic symmetries by developing a single machine learning architecture that is capable of adapting its weights automatically to enforce invariance to any input space group. Our approach is based on constructing symmetry-adapted Fourier bases through an explicit characterization of constraints that group operations impose on Fourier coefficients. Encoding these constraints into a neural network layer enables weight sharing across different space groups, allowing the model to leverage structural similarities between groups and overcome data sparsity when limited measurements are available for specific groups. We demonstrate the effectiveness of this approach in achieving competitive performance on material property prediction tasks and performing zero-shot learning to generalize to unseen groups.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation</title>
<link>https://arxiv.org/abs/2512.14011</link>
<guid>https://arxiv.org/abs/2512.14011</guid>
<content:encoded><![CDATA[
<div> Keywords: MHC-II, antigenic epitope, immunotherapy, machine learning, peptide presentation<br /><br />Summary:<br /><br />This paper addresses the challenges in studying antigenic epitopes presented by MHC-II proteins, which are more complex and less understood compared to MHC-I. It highlights the limitations of existing MHC-II datasets, which are smaller and less standardized, impeding computational immunotherapy research. To overcome these issues, the authors present a well-curated dataset compiled from the Immune Epitope Database (IEDB) and other public sources, extending and standardizing peptide-MHC-II data while introducing a novel antigen-MHC-II dataset enriched with biological context. Using this dataset, they define three machine learning tasks—peptide binding, peptide presentation, and antigen presentation—that model progressively broader biological processes involved in MHC-II antigen presentation. The study also proposes a multi-scale evaluation framework to benchmark current models and undertakes a detailed analysis of different modeling approaches within a modular framework. Overall, this work provides a significant resource for advancing computational immunotherapy by facilitating ML-guided epitope discovery and improving predictive modeling of immune responses, potentially accelerating future research and applications in immunotherapy. <div>
arXiv:2512.14011v1 Announce Type: new 
Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment</title>
<link>https://arxiv.org/abs/2512.14019</link>
<guid>https://arxiv.org/abs/2512.14019</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal learning, pathology foundation model, cancer progression, contrastive learning, precision oncology<br /><br />Summary:<br />1. The article introduces EXAONE Path 2.5, a pathology foundation model designed to integrate multiple biological data layers, including histologic, genomic, epigenetic, and transcriptomic information, aiming to provide a comprehensive representation of tumor biology beyond image-only models.<br />2. Three main components drive the model: a multimodal SigLIP loss that enables all-pairwise contrastive learning across different types of data modalities; a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial and tissue-fragment structural information in whole slide images (WSI); and domain-specialized internal foundation models for WSI and RNA-seq that produce biologically meaningful embeddings to improve multimodal alignment.<br />3. The model was evaluated against six leading pathology foundation models using two benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark containing 80 diverse tasks.<br />4. Results show that EXAONE Path 2.5 achieves comparable performance to state-of-the-art models on Patho-Bench while demonstrating superior adaptability and parameter/data efficiency within clinical settings.<br />5. The study emphasizes the importance of biologically informed multimodal modeling and showcases the potential of integrated genotype-to-phenotype frameworks to advance precision oncology for better cancer progression understanding and clinical application. <div>
arXiv:2512.14019v1 Announce Type: new 
Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.14023</link>
<guid>https://arxiv.org/abs/2512.14023</guid>
<content:encoded><![CDATA[
<div> Keywords: Multivariate Time Series, Hybrid Geometric Representation, Graph Neural Network, Riemannian Manifold, Forecasting Accuracy  

<br /><br />Summary:  
Multivariate Time Series (MTS) forecasting is crucial for applications like traffic management and predictive maintenance. Traditional methods focus on modeling MTS in either Euclidean or Riemannian space, which restricts their ability to fully capture the complex geometric structures and spatio-temporal dependencies in real-world data. This work introduces the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel model that combines Euclidean and Riemannian geometric representations within a graph neural network framework to enhance expressiveness and comprehensiveness. A key innovation is the Submanifold-Cross-Segment (SCS) embedding, projecting MTS data into both geometric spaces to better capture spatio-temporal variations across domains. To address the heavy computational demands of calculating Riemannian distances, the model includes an Adaptive-Distance-Bank (ADB) layer equipped with a trainable memory mechanism for efficiency. The learned dual-space features are then fused using a Fusion Graph Convolutional Network (FGCN) with a trainable fusion operator to improve prediction accuracy. Experimental results on three benchmark datasets demonstrate that HSMGNN outperforms current state-of-the-art models, achieving up to a 13.8% improvement in forecasting accuracy. This work represents the first use of hybrid geometric frameworks for MTS forecasting, offering a promising direction for future research. <div>
arXiv:2512.14023v1 Announce Type: new 
Abstract: Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis</title>
<link>https://arxiv.org/abs/2512.14078</link>
<guid>https://arxiv.org/abs/2512.14078</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series analysis, multi-task learning, adaptive time-frequency fusion, denoising mechanism, masked pre-training<br /><br />Summary:<br /><br />1. The paper addresses the challenge of building a unified, efficient, and generalizable framework capable of handling multiple time series tasks such as classification, forecasting, and anomaly detection across diverse data types.  
2. Existing methods tend to focus on single tasks or specific time series types, limiting their ability to integrate information across varied datasets and handle noise, complex frequency components, and multi-scale dynamic patterns robustly.  
3. The proposed framework, FusAD, introduces an adaptive time-frequency fusion mechanism that combines Fourier and Wavelet transforms to capture both global-local and multi-scale dynamic features effectively.  
4. FusAD incorporates an adaptive denoising mechanism that automatically detects and filters different kinds of noise, enhancing the extraction of critical sequence variations in complex environments.  
5. The framework includes a general information fusion and decoding structure alongside masked pre-training, facilitating efficient learning and transfer of multi-granularity representations. Experimental results demonstrate that FusAD consistently outperforms state-of-the-art models on standard time series benchmarks while maintaining high efficiency and scalability. The code for FusAD is publicly available for further research and application. <div>
arXiv:2512.14078v1 Announce Type: new 
Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations</title>
<link>https://arxiv.org/abs/2512.14080</link>
<guid>https://arxiv.org/abs/2512.14080</guid>
<content:encoded><![CDATA[
<div> Mixture of Experts, Memory-efficient algorithm, GPU kernels, Token rounding, Sparse MoE training<br /><br />Summary:<br /><br />1. The paper addresses challenges in fine-grained and sparse Mixture of Experts (MoE) models, which, despite improving model quality per FLOP, face significant activation memory overhead and hardware inefficiencies due to IO costs and padding in Grouped GEMM kernels.  
2. The authors propose a novel memory-efficient algorithm that minimizes activation caching during the backward pass of MoEs, reducing activation memory usage by 45%.  
3. They design specialized GPU kernels that effectively overlap memory IO with computation, enhancing hardware utilization across various MoE architectures.  
4. A new "token rounding" method is introduced to reduce wasted computations caused by padding in Grouped GEMM kernels, leading to better efficiency.  
5. Experimental results on Hopper GPUs show that SonicMoE achieves a 1.86x increase in compute throughput compared to ScatterMoE's BF16 kernel on a 7B fine-grained MoE model. On 64 H100 GPUs, SonicMoE attains training throughput of 213 billion tokens/day, comparable to ScatterMoE's 225 billion tokens/day on 96 H100s.  
6. Under high sparsity, the tile-aware token rounding method yields an additional 1.16x speedup over vanilla top-K routing without compromising downstream performance.  
7. All developed GPU kernels are open-sourced to facilitate faster and more memory-efficient MoE model training. <div>
arXiv:2512.14080v1 Announce Type: new 
Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization</title>
<link>https://arxiv.org/abs/2512.14086</link>
<guid>https://arxiv.org/abs/2512.14086</guid>
<content:encoded><![CDATA[
<div> Fourier neural operators, Fréchet derivatives, PDE-constrained optimization, surrogate models, operator learning<br /><br />Summary:<br /><br />This article introduces derivative-informed Fourier neural operators (DIFNOs), which extend conventional FNOs by training on both the operator outputs and their Fréchet derivatives, enabling accurate emulation of operator responses and sensitivities. The authors motivate DIFNOs by emphasizing the necessity of precise surrogate derivatives for effective PDE-constrained optimization. They establish rigorous approximation theorems demonstrating that FNOs can universally approximate both operators and their Fréchet derivatives on compact sets, and can also approximate operators in weighted Sobolev spaces with unbounded input supports. These theoretical guarantees affirm FNOs’ ability to perform accurate derivative-informed operator learning and solve PDE-constrained optimization problems reliably. To enhance practical applicability, the paper develops efficient training strategies incorporating dimension reduction and multi-resolution techniques, significantly reducing memory and computational demands for derivative learning. Finally, numerical experiments on nonlinear diffusion–reaction, Helmholtz, and Navier–Stokes equations showcase DIFNOs’ superior efficiency and accuracy in learning operators and solving infinite-dimensional PDE-constrained inverse problems, particularly requiring fewer training samples compared to traditional approaches, thus highlighting their potential for scalable and effective surrogate modeling in scientific computing. <div>
arXiv:2512.14086v1 Announce Type: new 
Abstract: We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fr\'echet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fr\'echet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fr\'echet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fr\'echet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arithmetic-Intensity-Aware Quantization</title>
<link>https://arxiv.org/abs/2512.14090</link>
<guid>https://arxiv.org/abs/2512.14090</guid>
<content:encoded><![CDATA[
<div> Keywords: Arithmetic-Intensity-Aware Quantization, mixed precision, post-training quantization, neural network inference, DRAM bandwidth<br /><br />Summary:<br />1. The paper addresses the challenge in modern neural networks where inference throughput is constrained by DRAM bandwidth instead of compute capacity, highlighting a memory-bound scenario.<br />2. It proposes Arithmetic-Intensity-Aware Quantization (AIQ), a novel mixed precision quantization framework designed to optimize per-layer bit-widths to maximize the arithmetic intensity (AI) of neural network inference.<br />3. AIQ is a post-training quantization method that employs search algorithms to balance a weighted loss function combining AI maximization and minimal accuracy degradation.<br />4. Experimental results demonstrate that AIQ increases arithmetic intensity by approximately 50% over a full precision (FP32) baseline in ResNet-20 on the CIFAR-10 dataset while maintaining test accuracy within about one percentage point.<br />5. On the memory-bound MobileNetV2 architecture, AIQ achieves a 1.66x throughput improvement compared to FP32, again preserving accuracy within one percentage point.<br />6. An interesting behavior observed is AIQ’s tendency to quantize larger layers more aggressively, which contributes to the optimization of overall inference efficiency.<br />7. The method outperforms global uniform quantization schemes in both arithmetic intensity and throughput, illustrating the benefits of layer-wise adaptive precision allocation under memory constraints. <div>
arXiv:2512.14090v1 Announce Type: new 
Abstract: As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cornserve: Efficiently Serving Any-to-Any Multimodal Models</title>
<link>https://arxiv.org/abs/2512.14098</link>
<guid>https://arxiv.org/abs/2512.14098</guid>
<content:encoded><![CDATA[
<div> Keywords: Cornserve, Any-to-Any models, multimodal, model serving, throughput improvement  

<br /><br />Summary:  
The paper introduces Cornserve, an efficient online serving system designed specifically for Any-to-Any multimodal models that accept and generate various combinations of text, images, video, and audio. These models introduce heterogeneity in request types, computation paths, and computational scaling, presenting challenges for efficient serving. Cornserve enables developers to describe the computation graph of these heterogeneous models, which may include components such as multimodal encoders, Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Using this description, Cornserve's planner automatically optimizes the deployment plan, determining whether and how to disaggregate the model into smaller components based on both model structure and workload characteristics. The system’s distributed runtime efficiently executes the model according to this optimized plan, effectively managing the inherent heterogeneity of Any-to-Any models during online serving. Experimental evaluations demonstrate that Cornserve can serve a diverse range of Any-to-Any models and workloads more efficiently than existing solutions, achieving up to 3.81× improvement in throughput and up to 5.79× reduction in tail latency. This highlights Cornserve’s potential to advance multimodal model serving capabilities in real-world applications. <div>
arXiv:2512.14098v1 Announce Type: new 
Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A First-Order Logic-Based Alternative to Reward Models in RLHF</title>
<link>https://arxiv.org/abs/2512.14100</link>
<guid>https://arxiv.org/abs/2512.14100</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning from Human Feedback, logic-similarity-based reward, S-GRPO, model alignment, preference learning<br /><br />Summary:<br /><br />This paper addresses the challenge of aligning large language models (LLMs) with human values by improving the reward mechanism in Reinforcement Learning from Human Feedback (RLHF). Traditional methods such as Proximal Policy Optimization (PPO) depend heavily on reward models that estimate human preferences heuristically, which can limit stability and alignment quality. To overcome these limitations, the authors introduce a novel logic-similarity-based reward mechanism that leverages formal logical consistency rather than heuristic reward estimation to guide LLMs towards human-aligned behaviors. Recognizing that questions can be interpreted from multiple perspectives and that purely logic-based reinforcement learning might risk model collapse, they develop S-GRPO, a supervised variant of the GRPO framework. S-GRPO integrates an additional supervised component and jointly optimizes three objectives during training: generation quality, KL-divergence regularization, and a label-based objective. Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in terms of both model performance and robustness. Moreover, S-GRPO extends existing preference learning frameworks including GRPO and DPO, providing a more flexible and task-adaptive approach to alignment training. The authors have made their implementation publicly available for further research and development. <div>
arXiv:2512.14100v1 Announce Type: new 
Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario</title>
<link>https://arxiv.org/abs/2512.14150</link>
<guid>https://arxiv.org/abs/2512.14150</guid>
<content:encoded><![CDATA[
<div> Radio path loss prediction, multi-transmitter scenarios, environmental modeling, distribution shift, deep learning<br /><br />Summary:<br /><br />Radio path loss prediction (RPP) is essential for optimizing modern wireless networks such as 5G and IoT. Existing deep learning methods for RPP face three main challenges: they passively model the environment by ignoring key features including transmitters, focus predominantly on single-transmitter scenarios, and perform poorly when the test environment’s distribution differs from that of training, especially regarding building density and transmitter arrangements. To overcome these limitations, the authors propose PathFinder, a novel architecture that actively models both buildings and transmitters through disentangled feature encoding. PathFinder incorporates Mask-Guided Low-rank Attention mechanisms that enable independent focus on the receiver and building regions, improving environmental awareness. Additionally, a Transmitter-Oriented Mixup training strategy is introduced to enhance model robustness against diverse transmitter setups. To rigorously evaluate performance, the authors create a new benchmark called single-to-multi-transmitter RPP (S2MT-RPP), which tests the model’s ability to generalize from single-transmitter training to multi-transmitter scenarios during testing. Extensive experiments demonstrate that PathFinder considerably outperforms state-of-the-art methods, particularly in realistic multi-transmitter and distribution shift conditions. The authors have made their code and project resources publicly available online to support further research and practical applications in RPP. <div>
arXiv:2512.14150v1 Announce Type: new 
Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Improving Deep Active Learning with Formal Verification</title>
<link>https://arxiv.org/abs/2512.14170</link>
<guid>https://arxiv.org/abs/2512.14170</guid>
<content:encoded><![CDATA[
arXiv:2512.14170v1 Announce Type: new 
Abstract: Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing the Adversarial Perturbation with a Momentum-based Adaptive Matrix</title>
<link>https://arxiv.org/abs/2512.14188</link>
<guid>https://arxiv.org/abs/2512.14188</guid>
<content:encoded><![CDATA[
arXiv:2512.14188v1 Announce Type: new 
Abstract: Generating adversarial examples (AEs) can be formulated as an optimization problem. Among various optimization-based attacks, the gradient-based PGD and the momentum-based MI-FGSM have garnered considerable interest. However, all these attacks use the sign function to scale their perturbations, which raises several theoretical concerns from the point of view of optimization. In this paper, we first reveal that PGD is actually a specific reformulation of the projected gradient method using only the current gradient to determine its step-size. Further, we show that when we utilize a conventional adaptive matrix with the accumulated gradients to scale the perturbation, PGD becomes AdaGrad. Motivated by this analysis, we present a novel momentum-based attack AdaMI, in which the perturbation is optimized with an interesting momentum-based adaptive matrix. AdaMI is proved to attain optimal convergence for convex problems, indicating that it addresses the non-convergence issue of MI-FGSM, thereby ensuring stability of the optimization process. The experiments demonstrate that the proposed momentum-based adaptive matrix can serve as a general and effective technique to boost adversarial transferability over the state-of-the-art methods across different networks while maintaining better stability and imperceptibility.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random-Bridges as Stochastic Transports for Generative Models</title>
<link>https://arxiv.org/abs/2512.14190</link>
<guid>https://arxiv.org/abs/2512.14190</guid>
<content:encoded><![CDATA[
arXiv:2512.14190v1 Announce Type: new 
Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Improving Hyperbolic Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.14202</link>
<guid>https://arxiv.org/abs/2512.14202</guid>
<content:encoded><![CDATA[
arXiv:2512.14202v1 Announce Type: new 
Abstract: The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincar\'e Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating problem difficulty without ground truth using Large Language Model comparisons</title>
<link>https://arxiv.org/abs/2512.14220</link>
<guid>https://arxiv.org/abs/2512.14220</guid>
<content:encoded><![CDATA[
arXiv:2512.14220v1 Announce Type: new 
Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding the Gain from Data Filtering in Multimodal Contrastive Learning</title>
<link>https://arxiv.org/abs/2512.14230</link>
<guid>https://arxiv.org/abs/2512.14230</guid>
<content:encoded><![CDATA[
arXiv:2512.14230v1 Announce Type: new 
Abstract: The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $\eta\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\frac{1}{\eta \sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\frac{1}{\sqrt{\eta n}}$ in the large $\eta$ regime, and by $\frac{1}{\sqrt{n}}$ in the small $\eta$ regime.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically consistent model learning for reaction-diffusion systems</title>
<link>https://arxiv.org/abs/2512.14240</link>
<guid>https://arxiv.org/abs/2512.14240</guid>
<content:encoded><![CDATA[
arXiv:2512.14240v1 Announce Type: new 
Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning</title>
<link>https://arxiv.org/abs/2512.14241</link>
<guid>https://arxiv.org/abs/2512.14241</guid>
<content:encoded><![CDATA[
arXiv:2512.14241v1 Announce Type: new 
Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.14253</link>
<guid>https://arxiv.org/abs/2512.14253</guid>
<content:encoded><![CDATA[
arXiv:2512.14253v1 Announce Type: new 
Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization</title>
<link>https://arxiv.org/abs/2512.14263</link>
<guid>https://arxiv.org/abs/2512.14263</guid>
<content:encoded><![CDATA[
arXiv:2512.14263v1 Announce Type: new 
Abstract: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits</title>
<link>https://arxiv.org/abs/2512.14338</link>
<guid>https://arxiv.org/abs/2512.14338</guid>
<content:encoded><![CDATA[
arXiv:2512.14338v1 Announce Type: new 
Abstract: Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis</title>
<link>https://arxiv.org/abs/2512.14361</link>
<guid>https://arxiv.org/abs/2512.14361</guid>
<content:encoded><![CDATA[
arXiv:2512.14361v1 Announce Type: new 
Abstract: Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries</title>
<link>https://arxiv.org/abs/2512.14388</link>
<guid>https://arxiv.org/abs/2512.14388</guid>
<content:encoded><![CDATA[
arXiv:2512.14388v1 Announce Type: new 
Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RePo: Language Models with Context Re-Positioning</title>
<link>https://arxiv.org/abs/2512.14391</link>
<guid>https://arxiv.org/abs/2512.14391</guid>
<content:encoded><![CDATA[
arXiv:2512.14391v1 Announce Type: new 
Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_\phi$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuperWing: a comprehensive transonic wing dataset for data-driven aerodynamic design</title>
<link>https://arxiv.org/abs/2512.14397</link>
<guid>https://arxiv.org/abs/2512.14397</guid>
<content:encoded><![CDATA[
arXiv:2512.14397v1 Announce Type: new 
Abstract: Machine-learning surrogate models have shown promise in accelerating aerodynamic design, yet progress toward generalizable predictors for three-dimensional wings has been limited by the scarcity and restricted diversity of existing datasets. Here, we present SuperWing, a comprehensive open dataset of transonic swept-wing aerodynamics comprising 4,239 parameterized wing geometries and 28,856 Reynolds-averaged Navier-Stokes flow field solutions. The wing shapes in the dataset are generated using a simplified yet expressive geometry parameterization that incorporates spanwise variations in airfoil shape, twist, and dihedral, allowing for an enhanced diversity without relying on perturbations of a baseline wing. All shapes are simulated under a broad range of Mach numbers and angles of attack covering the typical flight envelope. To demonstrate the dataset's utility, we benchmark two state-of-the-art Transformers that accurately predict surface flow and achieve a 2.5 drag-count error on held-out samples. Models pretrained on SuperWing further exhibit strong zero-shot generalization to complex benchmark wings such as DLR-F6 and NASA CRM, underscoring the dataset's diversity and potential for practical usage.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion</title>
<link>https://arxiv.org/abs/2512.14400</link>
<guid>https://arxiv.org/abs/2512.14400</guid>
<content:encoded><![CDATA[
arXiv:2512.14400v1 Announce Type: new 
Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space</title>
<link>https://arxiv.org/abs/2512.14418</link>
<guid>https://arxiv.org/abs/2512.14418</guid>
<content:encoded><![CDATA[
arXiv:2512.14418v1 Announce Type: new 
Abstract: Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF</title>
<link>https://arxiv.org/abs/2512.14444</link>
<guid>https://arxiv.org/abs/2512.14444</guid>
<content:encoded><![CDATA[
arXiv:2512.14444v1 Announce Type: new 
Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts</title>
<link>https://arxiv.org/abs/2512.14461</link>
<guid>https://arxiv.org/abs/2512.14461</guid>
<content:encoded><![CDATA[
arXiv:2512.14461v1 Announce Type: new 
Abstract: Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics</title>
<link>https://arxiv.org/abs/2512.14471</link>
<guid>https://arxiv.org/abs/2512.14471</guid>
<content:encoded><![CDATA[
arXiv:2512.14471v1 Announce Type: new 
Abstract: Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Slow Transfer Predictions: Generative Methods Compared</title>
<link>https://arxiv.org/abs/2512.14522</link>
<guid>https://arxiv.org/abs/2512.14522</guid>
<content:encoded><![CDATA[
arXiv:2512.14522v1 Announce Type: new 
Abstract: Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Electrogram Generation with Variational Autoencoders for ECGI</title>
<link>https://arxiv.org/abs/2512.14537</link>
<guid>https://arxiv.org/abs/2512.14537</guid>
<content:encoded><![CDATA[
arXiv:2512.14537v1 Announce Type: new 
Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions</title>
<link>https://arxiv.org/abs/2512.14559</link>
<guid>https://arxiv.org/abs/2512.14559</guid>
<content:encoded><![CDATA[
arXiv:2512.14559v1 Announce Type: new 
Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection</title>
<link>https://arxiv.org/abs/2512.14563</link>
<guid>https://arxiv.org/abs/2512.14563</guid>
<content:encoded><![CDATA[
arXiv:2512.14563v1 Announce Type: new 
Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs</title>
<link>https://arxiv.org/abs/2512.14596</link>
<guid>https://arxiv.org/abs/2512.14596</guid>
<content:encoded><![CDATA[
arXiv:2512.14596v1 Announce Type: new 
Abstract: The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Persistence Velocity for Network Anomaly Detection: Theory and Applications to Cryptocurrency Markets</title>
<link>https://arxiv.org/abs/2512.14615</link>
<guid>https://arxiv.org/abs/2512.14615</guid>
<content:encoded><![CDATA[
arXiv:2512.14615v1 Announce Type: new 
Abstract: We introduce the Overlap-Weighted Hierarchical Normalized Persistence Velocity (OW-HNPV), a novel topological data analysis method for detecting anomalies in time-varying networks. Unlike existing methods that measure cumulative topological presence, we introduce the first velocity-based perspective on persistence diagrams, measuring the rate at which features appear and disappear, automatically downweighting noise through overlap-based weighting. We also prove that OW-HNPV is mathematically stable. It behaves in a controlled, predictable way, even when comparing persistence diagrams from networks with different feature types. Applied to Ethereum transaction networks (May 2017-May 2018), OW-HNPV demonstrates superior performance for cryptocurrency anomaly detection, achieving up to 10.4% AUC gain over baseline models for 7-day price movement predictions. Compared with established methods, including Vector of Averaged Bettis (VAB), persistence landscapes, and persistence images, velocity-based summaries excel at medium- to long-range forecasting (4-7 days), with OW-HNPV providing the most consistent and stable performance across prediction horizons. Our results show that modeling topological velocity is crucial for detecting structural anomalies in dynamic networks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes</title>
<link>https://arxiv.org/abs/2512.14617</link>
<guid>https://arxiv.org/abs/2512.14617</guid>
<content:encoded><![CDATA[
arXiv:2512.14617v1 Announce Type: new 
Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning</title>
<link>https://arxiv.org/abs/2512.14619</link>
<guid>https://arxiv.org/abs/2512.14619</guid>
<content:encoded><![CDATA[
arXiv:2512.14619v1 Announce Type: new 
Abstract: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation</title>
<link>https://arxiv.org/abs/2512.14658</link>
<guid>https://arxiv.org/abs/2512.14658</guid>
<content:encoded><![CDATA[
arXiv:2512.14658v1 Announce Type: new 
Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$\Delta$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks</title>
<link>https://arxiv.org/abs/2512.14675</link>
<guid>https://arxiv.org/abs/2512.14675</guid>
<content:encoded><![CDATA[
arXiv:2512.14675v1 Announce Type: new 
Abstract: Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Early Warning Index for Patient Deteriorations in Hospitals</title>
<link>https://arxiv.org/abs/2512.14683</link>
<guid>https://arxiv.org/abs/2512.14683</guid>
<content:encoded><![CDATA[
arXiv:2512.14683v1 Announce Type: new 
Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean</title>
<link>https://arxiv.org/abs/2512.14686</link>
<guid>https://arxiv.org/abs/2512.14686</guid>
<content:encoded><![CDATA[
arXiv:2512.14686v1 Announce Type: new 
Abstract: Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $\alpha$ of the noise. Nonetheless, existing complexity results often cover only the case $\alpha \in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $\alpha$ approaches $1$. This paper tackles the general case of noise with tail index $\alpha\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $\alpha \in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPR-1: Interactive Physical Reasoner</title>
<link>https://arxiv.org/abs/2511.15407</link>
<guid>https://arxiv.org/abs/2511.15407</guid>
<content:encoded><![CDATA[
arXiv:2511.15407v2 Announce Type: cross 
Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Pollen Recognition in Optical and Holographic Microscopy Images</title>
<link>https://arxiv.org/abs/2512.08589</link>
<guid>https://arxiv.org/abs/2512.08589</guid>
<content:encoded><![CDATA[
arXiv:2512.08589v1 Announce Type: cross 
Abstract: This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sim2Real Reinforcement Learning for Soccer skills</title>
<link>https://arxiv.org/abs/2512.12437</link>
<guid>https://arxiv.org/abs/2512.12437</guid>
<content:encoded><![CDATA[
arXiv:2512.12437v1 Announce Type: cross 
Abstract: This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular connectivity in neural networks emerges from Poisson noise-motivated regularisation, and promotes robustness and compositional generalisation</title>
<link>https://arxiv.org/abs/2512.13707</link>
<guid>https://arxiv.org/abs/2512.13707</guid>
<content:encoded><![CDATA[
arXiv:2512.13707v1 Announce Type: cross 
Abstract: Circuits in the brain commonly exhibit modular architectures that factorise complex tasks, resulting in the ability to compositionally generalise and reduce catastrophic forgetting. In contrast, artificial neural networks (ANNs) appear to mix all processing, because modular solutions are difficult to find as they are vanishing subspaces in the space of possible solutions. Here, we draw inspiration from fault-tolerant computation and the Poisson-like firing of real neurons to show that activity-dependent neural noise, combined with nonlinear neural responses, drives the emergence of solutions that reflect an accurate understanding of modular tasks, corresponding to acquisition of a correct world model. We find that noise-driven modularisation can be recapitulated by a deterministic regulariser that multiplicatively combines weights and activations, revealing rich phenomenology not captured in linear networks or by standard regularisation methods. Though the emergence of modular structure requires sufficiently many training samples (exponential in the number of modular task dimensions), we show that pre-modularised ANNs exhibit superior noise-robustness and the ability to generalise and extrapolate well beyond training data, compared to ANNs without such inductive biases. Together, our work demonstrates a regulariser and architectures that could encourage modularity emergence to yield functional benefits.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smart Surveillance: Identifying IoT Device Behaviours using ML-Powered Traffic Analysis</title>
<link>https://arxiv.org/abs/2512.13709</link>
<guid>https://arxiv.org/abs/2512.13709</guid>
<content:encoded><![CDATA[
arXiv:2512.13709v1 Announce Type: cross 
Abstract: The proliferation of Internet of Things (IoT) devices has grown exponentially in recent years, introducing significant security challenges. Accurate identification of the types of IoT devices and their associated actions through network traffic analysis is essential to mitigate potential threats. By monitoring and analysing packet flows between IoT devices and connected networks, anomalous or malicious behaviours can be detected. Existing research focuses primarily on device identification within local networks using methods such as protocol fingerprinting and wireless frequency scanning. However, these approaches are limited in their ability to monitor or classify IoT devices externally. To address this gap, we investigate the use of machine learning (ML) techniques, specifically Random Forest (RF), Multilayer Perceptron (MLP), and K-Nearest Neighbours (KNN), in conjunction with targeted network traffic monitoring to classify IoT device types and their actions. We constructed a testbed comprising an NPAT-enabled router and a diverse set of IoT devices, including smart cameras, controller hubs, home appliances, power controllers, and streaming devices. Experimental results demonstrate that IoT device and action recognition is feasible using our proposed ML-driven approach, with the RF classifier achieving the highest accuracy of 91%, while the MLP recorded the lowest accuracy at 56%. Notably, all device categories were successfully classified except for certain actions associated with security cameras, underscoring both the potential and the limitations of the proposed method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms</title>
<link>https://arxiv.org/abs/2512.13713</link>
<guid>https://arxiv.org/abs/2512.13713</guid>
<content:encoded><![CDATA[
arXiv:2512.13713v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN</title>
<link>https://arxiv.org/abs/2512.13715</link>
<guid>https://arxiv.org/abs/2512.13715</guid>
<content:encoded><![CDATA[
arXiv:2512.13715v1 Announce Type: cross 
Abstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Predictions of Process-Induced Deformation in Carbon/Epoxy Composites Using a Deep Operator Network</title>
<link>https://arxiv.org/abs/2512.13746</link>
<guid>https://arxiv.org/abs/2512.13746</guid>
<content:encoded><![CDATA[
arXiv:2512.13746v1 Announce Type: cross 
Abstract: Fiber reinforcement and polymer matrix respond differently to manufacturing conditions due to mismatch in coefficient of thermal expansion and matrix shrinkage during curing of thermosets. These heterogeneities generate residual stresses over multiple length scales, whose partial release leads to process-induced deformation (PID), requiring accurate prediction and mitigation via optimized non-isothermal cure cycles. This study considers a unidirectional AS4 carbon fiber/amine bi-functional epoxy prepreg and models PID using a two-mechanism framework that accounts for thermal expansion/shrinkage and cure shrinkage. The model is validated against manufacturing trials to identify initial and boundary conditions, then used to generate PID responses for a diverse set of non-isothermal cure cycles (time-temperature profiles). Building on this physics-based foundation, we develop a data-driven surrogate based on Deep Operator Networks (DeepONets). A DeepONet is trained on a dataset combining high-fidelity simulations with targeted experimental measurements of PID. We extend this to a Feature-wise Linear Modulation (FiLM) DeepONet, where branch-network features are modulated by external parameters, including the initial degree of cure, enabling prediction of time histories of degree of cure, viscosity, and deformation. Because experimental data are available only at limited time instances (for example, final deformation), we use transfer learning: simulation-trained trunk and branch networks are fixed and only the final layer is updated using measured final deformation. Finally, we augment the framework with Ensemble Kalman Inversion (EKI) to quantify uncertainty under experimental conditions and to support optimization of cure schedules for reduced PID in composites.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-aware UNet and super-resolution deep residual networks for spatial downscaling</title>
<link>https://arxiv.org/abs/2512.13753</link>
<guid>https://arxiv.org/abs/2512.13753</guid>
<content:encoded><![CDATA[
arXiv:2512.13753v1 Announce Type: cross 
Abstract: Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Plausibility of Pressure Distributions Synthesized from Depth through Generative Modeling</title>
<link>https://arxiv.org/abs/2512.13757</link>
<guid>https://arxiv.org/abs/2512.13757</guid>
<content:encoded><![CDATA[
arXiv:2512.13757v1 Announce Type: cross 
Abstract: Monitoring contact pressure in hospital beds is essential for preventing pressure ulcers and enabling real-time patient assessment. Current methods can predict pressure maps but often lack physical plausibility, limiting clinical reliability. This work proposes a framework that enhances plausibility via Informed Latent Space (ILS) and Weight Optimization Loss (WOL) with generative modeling to produce high-fidelity, physically consistent pressure estimates. This study also applies diffusion based conditional Brownian Bridge Diffusion Model (BBDM) and proposes training strategy for its latent counterpart Latent Brownian Bridge Diffusion Model (LBBDM) tailored for pressure synthesis in lying postures. Experiment results shows proposed method improves physical plausibility and performance over baselines: BBDM with ILS delivers highly detailed maps at higher computational cost and large inference time, whereas LBBDM provides faster inference with competitive performance. Overall, the approach supports non-invasive, vision-based, real-time patient monitoring in clinical environments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematics and Coding are Universal AI Benchmarks</title>
<link>https://arxiv.org/abs/2512.13764</link>
<guid>https://arxiv.org/abs/2512.13764</guid>
<content:encoded><![CDATA[
arXiv:2512.13764v1 Announce Type: cross 
Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Deep Learning Surrogate for the Forward Problem in Electrocardiology: A Scalable Alternative to Physics-Based Models</title>
<link>https://arxiv.org/abs/2512.13765</link>
<guid>https://arxiv.org/abs/2512.13765</guid>
<content:encoded><![CDATA[
arXiv:2512.13765v1 Announce Type: cross 
Abstract: The forward problem in electrocardiology, computing body surface potentials from cardiac electrical activity, is traditionally solved using physics-based models such as the bidomain or monodomain equations. While accurate, these approaches are computationally expensive, limiting their use in real-time and large-scale clinical applications. We propose a proof-of-concept deep learning (DL) framework as an efficient surrogate for forward solvers. The model adopts a time-dependent, attention-based sequence-to-sequence architecture to predict electrocardiogram (ECG) signals from cardiac voltage propagation maps. A hybrid loss combining Huber loss with a spectral entropy term was introduced to preserve both temporal and frequency-domain fidelity. Using 2D tissue simulations incorporating healthy, fibrotic, and gap junction-remodelled conditions, the model achieved high accuracy (mean $R^2 = 0.99 \pm 0.01$). Ablation studies confirmed the contributions of convolutional encoders, time-aware attention, and spectral entropy loss. These findings highlight DL as a scalable, cost-effective alternative to physics-based solvers, with potential for clinical and digital twin applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unreasonable effectiveness of unsupervised learning in identifying Majorana topology</title>
<link>https://arxiv.org/abs/2512.13825</link>
<guid>https://arxiv.org/abs/2512.13825</guid>
<content:encoded><![CDATA[
arXiv:2512.13825v1 Announce Type: cross 
Abstract: In unsupervised learning, the training data for deep learning does not come with any labels, thus forcing the algorithm to discover hidden patterns in the data for discerning useful information. This, in principle, could be a powerful tool in identifying topological order since topology does not always manifest in obvious physical ways (e.g., topological superconductivity) for its decisive confirmation. The problem, however, is that unsupervised learning is a difficult challenge, necessitating huge computing resources, which may not always work. In the current work, we combine unsupervised and supervised learning using an autoencoder to establish that unlabeled data in the Majorana splitting in realistic short disordered nanowires may enable not only a distinction between `topological' and `trivial', but also where their crossover happens in the relevant parameter space. This may be a useful tool in identifying topology in Majorana nanowires.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BiCoRec: Bias-Mitigated Context-Aware Sequential Recommendation Model</title>
<link>https://arxiv.org/abs/2512.13848</link>
<guid>https://arxiv.org/abs/2512.13848</guid>
<content:encoded><![CDATA[
arXiv:2512.13848v1 Announce Type: cross 
Abstract: Sequential recommendation models aim to learn from users evolving preferences. However, current state-of-the-art models suffer from an inherent popularity bias. This study developed a novel framework, BiCoRec, that adaptively accommodates users changing preferences for popular and niche items. Our approach leverages a co-attention mechanism to obtain a popularity-weighted user sequence representation, facilitating more accurate predictions. We then present a new training scheme that learns from future preferences using a consistency loss function. BiCoRec aimed to improve the recommendation performance of users who preferred niche items. For these users, BiCoRec achieves a 26.00% average improvement in NDCG@10 over state-of-the-art baselines. When ranking the relevant item against the entire collection, BiCoRec achieves NDCG@10 scores of 0.0102, 0.0047, 0.0021, and 0.0005 for the Movies, Fashion, Games and Music datasets.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery</title>
<link>https://arxiv.org/abs/2512.13857</link>
<guid>https://arxiv.org/abs/2512.13857</guid>
<content:encoded><![CDATA[
arXiv:2512.13857v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safe Online Control-Informed Learning</title>
<link>https://arxiv.org/abs/2512.13868</link>
<guid>https://arxiv.org/abs/2512.13868</guid>
<content:encoded><![CDATA[
arXiv:2512.13868v1 Announce Type: cross 
Abstract: This paper proposes a Safe Online Control-Informed Learning framework for safety-critical autonomous systems. The framework unifies optimal control, parameter estimation, and safety constraints into an online learning process. It employs an extended Kalman filter to incrementally update system parameters in real time, enabling robust and data-efficient adaptation under uncertainty. A softplus barrier function enforces constraint satisfaction during learning and control while eliminating the dependence on high-quality initial guesses. Theoretical analysis establishes convergence and safety guarantees, and the framework's effectiveness is demonstrated on cart-pole and robot-arm systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous and Proportional Finger Motion Decoding Using Spatial Features from High-Density Surface Electromyography</title>
<link>https://arxiv.org/abs/2512.13870</link>
<guid>https://arxiv.org/abs/2512.13870</guid>
<content:encoded><![CDATA[
arXiv:2512.13870v1 Announce Type: cross 
Abstract: Restoring natural and intuitive hand function requires simultaneous and proportional control (SPC) of multiple degrees of freedom (DoFs). This study systematically evaluated the multichannel linear descriptors-based block field method (MLD-BFM) for continuous decoding of five finger-joint DoFs by leveraging the rich spatial information of high-density surface electromyography (HD sEMG). Twenty-one healthy participants performed dynamic sinusoidal finger movements while HD sEMG signals were recorded from the \textit{extensor digitorum communis} (EDC) and \textit{flexor digitorum superficialis} (FDS) muscles. MLD-BFM extracted region-specific spatial features, including effective field strength ($\Sigma$), field-strength variation rate ($\Phi$), and spatial complexity ($\Omega$). Model performance was optimized (block size: $2 \times 2$; window: 0.15 s) and compared with conventional time-domain features and dimensionality reduction approaches when applied to multi-output regression models. MLD-BFM consistently achieved the highest $\mathrm{R}^2_{\mathrm{vw}}$ values across all models. The multilayer perceptron (MLP) combined with MLD-BFM yielded the best performance ($\mathrm{R}^2_{\mathrm{vw}} = 86.68\% \pm 0.33$). Time-domain features also showed strong predictive capability and were statistically comparable to MLD-BFM in some models, whereas dimensionality reduction techniques exhibited lower accuracy. Decoding accuracy was higher for the middle and ring fingers than for the thumb. Overall, MLD-BFM improved continuous finger movement decoding accuracy, underscoring the importance of taking advantage of the spatial richness of HD sEMG. These findings suggest that spatially structured features enhance SPC and provide practical guidance for designing robust, real-time, and responsive myoelectric interfaces.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group-Theoretic Reinforcement Learning of Dynamical Decoupling Sequences</title>
<link>https://arxiv.org/abs/2512.13890</link>
<guid>https://arxiv.org/abs/2512.13890</guid>
<content:encoded><![CDATA[
arXiv:2512.13890v1 Announce Type: cross 
Abstract: Dynamical decoupling seeks to mitigate phase decoherence in qubits by applying a carefully designed sequence of effectively instantaneous electromagnetic pulses. Although analytic solutions exist for pulse timings that are optimal under specific noise regimes, identifying the optimal timings for a realistic noise spectrum remains challenging. We propose a reinforcement learning (RL)-based method for designing pulse sequences on qubits. Our novel action set enables the RL agent to efficiently navigate this inherently non-convex optimization landscape. The action set, derived from Thompson's group $F$, is applicable to a broad class of sequential decision problems whose states can be represented as bounded sequences. We demonstrate that our RL agent can learn pulse sequences that minimize dephasing without requiring explicit knowledge of the underlying noise spectrum. This work opens the possibility for real-time learning of optimal dynamical decoupling sequences on qubits which are dephasing-limited. The model-free nature of our algorithm suggests that the agent may ultimately learn optimal pulse sequences even in the presence of unmodeled physical effects, such as pulse errors or non-Gaussian noise.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing</title>
<link>https://arxiv.org/abs/2512.13892</link>
<guid>https://arxiv.org/abs/2512.13892</guid>
<content:encoded><![CDATA[
arXiv:2512.13892v1 Announce Type: cross 
Abstract: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KLO-Net: A Dynamic K-NN Attention U-Net with CSP Encoder for Efficient Prostate Gland Segmentation from MRI</title>
<link>https://arxiv.org/abs/2512.13902</link>
<guid>https://arxiv.org/abs/2512.13902</guid>
<content:encoded><![CDATA[
arXiv:2512.13902v1 Announce Type: cross 
Abstract: Real-time deployment of prostate MRI segmentation on clinical workstations is often bottlenecked by computational load and memory footprint. Deep learning-based prostate gland segmentation approaches remain challenging due to anatomical variability. To bridge this efficiency gap while still maintaining reliable segmentation accuracy, we propose KLO-Net, a dynamic K-Nearest Neighbor attention U-Net with Cross Stage Partial, i.e., CSP, encoder for efficient prostate gland segmentation from MRI scan. Unlike the regular K-NN attention mechanism, the proposed dynamic K-NN attention mechanism allows the model to adaptively determine the number of attention connections for each spatial location within a slice. In addition, CSP blocks address the computational load to reduce memory consumption. To evaluate the model's performance, comprehensive experiments and ablation studies are conducted on two public datasets, i.e., PROMISE12 and PROSTATEx, to validate the proposed architecture. The detailed comparative analysis demonstrates the model's advantage in computational efficiency and segmentation quality.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent matter consisting of active particles</title>
<link>https://arxiv.org/abs/2512.13912</link>
<guid>https://arxiv.org/abs/2512.13912</guid>
<content:encoded><![CDATA[
arXiv:2512.13912v1 Announce Type: cross 
Abstract: In this book chapter, we review how systems of simple motile agents can be used as a pathway to intelligent systems. It is a well known result from nature that large groups of entities following simple rules, such as swarms of animals, can give rise to much more complex collective behavior in a display of emergence. This begs the question whether we can emulate this behavior in synthetic matter and drive it to a point where the collective behavior reaches the complexity level of intelligent systems. Here, we will use a formalized notion of "intelligent matter" and compare it to recent results in the field of active matter. First, we will explore the approach of emergent computing in which specialized active matter systems are designed to directly solve a given task through emergent behavior. This we will then contrast with the approach of physical reservoir computing powered by the dynamics of active particle systems. In this context, we will also describe a novel reservoir computing scheme for active particles driven ultrasonically or via light refraction.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery</title>
<link>https://arxiv.org/abs/2512.13930</link>
<guid>https://arxiv.org/abs/2512.13930</guid>
<content:encoded><![CDATA[
arXiv:2512.13930v1 Announce Type: cross 
Abstract: Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including a single agent baseline and three multi-agent approaches: peer review, triage-ranking, and triage-forms. Across two chemical applications, CO adsorption on Cu-surface transition metal (M) adatoms and on M-N-C catalysts, reasoning-driven exploration reduces required atomistic simulations by up to 90% relative to trial-and-error selection. Reasoning trajectories reveal chemically grounded decisions that cannot be explained by stochastic sampling or semantic bias. Altogether, multi-agent collaboration accelerates materials discovery and marks a new paradigm for autonomous scientific exploration.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Olmo 3</title>
<link>https://arxiv.org/abs/2512.13961</link>
<guid>https://arxiv.org/abs/2512.13961</guid>
<content:encoded><![CDATA[
arXiv:2512.13961v1 Announce Type: cross 
Abstract: We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics</title>
<link>https://arxiv.org/abs/2512.13997</link>
<guid>https://arxiv.org/abs/2512.13997</guid>
<content:encoded><![CDATA[
arXiv:2512.13997v1 Announce Type: cross 
Abstract: Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Hardness of Conditional Independence Testing In Practice</title>
<link>https://arxiv.org/abs/2512.14000</link>
<guid>https://arxiv.org/abs/2512.14000</guid>
<content:encoded><![CDATA[
arXiv:2512.14000v1 Announce Type: cross 
Abstract: Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on "hiding" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Machine Learning for Two-Phase Moving-Interface and Stefan Problems</title>
<link>https://arxiv.org/abs/2512.14010</link>
<guid>https://arxiv.org/abs/2512.14010</guid>
<content:encoded><![CDATA[
arXiv:2512.14010v1 Announce Type: cross 
Abstract: The Stefan problem is a classical free-boundary problem that models phase-change processes and poses computational challenges due to its moving interface and nonlinear temperature-phase coupling. In this work, we develop a physics-informed neural network framework for solving two-phase Stefan problems. The proposed method explicitly tracks the interface motion and enforces the discontinuity in the temperature gradient across the interface while maintaining global consistency of the temperature field. Our approach employs two neural networks: one representing the moving interface and the other for the temperature field. The interface network allows rapid categorization of thermal diffusivity in the spatial domain, which is a crucial step for selecting training points for the temperature network. The temperature network's input is augmented with a modified zero-level set function to accurately capture the jump in its normal derivative across the interface. Numerical experiments on two-phase dynamical Stefan problems demonstrate the superior accuracy and effectiveness of our proposed method compared with the ones obtained by other neural network methodology in literature. The results indicate that the proposed framework offers a robust and flexible alternative to traditional numerical methods for solving phase-change problems governed by moving boundaries. In addition, the proposed method can capture an unstable interface evolution associated with the Mullins-Sekerka instability.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning</title>
<link>https://arxiv.org/abs/2512.14040</link>
<guid>https://arxiv.org/abs/2512.14040</guid>
<content:encoded><![CDATA[
arXiv:2512.14040v1 Announce Type: cross 
Abstract: With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis</title>
<link>https://arxiv.org/abs/2512.14045</link>
<guid>https://arxiv.org/abs/2512.14045</guid>
<content:encoded><![CDATA[
arXiv:2512.14045v1 Announce Type: cross 
Abstract: A function inlining optimization is a widely used transformation in modern compilers, which replaces a call site with the callee's body in need. While this transformation improves performance, it significantly impacts static features such as machine instructions and control flow graphs, which are crucial to binary analysis. Yet, despite its broad impact, the security impact of function inlining remains underexplored to date. In this paper, we present the first comprehensive study of function inlining through the lens of machine learning-based binary analysis. To this end, we dissect the inlining decision pipeline within the LLVM's cost model and explore the combinations of the compiler options that aggressively promote the function inlining ratio beyond standard optimization levels, which we term extreme inlining. We focus on five ML-assisted binary analysis tasks for security, using 20 unique models to systematically evaluate their robustness under extreme inlining scenarios. Our extensive experiments reveal several significant findings: i) function inlining, though a benign transformation in intent, can (in)directly affect ML model behaviors, being potentially exploited by evading discriminative or generative ML models; ii) ML models relying on static features can be highly sensitive to inlining; iii) subtle compiler settings can be leveraged to deliberately craft evasive binary variants; and iv) inlining ratios vary substantially across applications and build configurations, undermining assumptions of consistency in training and evaluation of ML models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed</title>
<link>https://arxiv.org/abs/2512.14067</link>
<guid>https://arxiv.org/abs/2512.14067</guid>
<content:encoded><![CDATA[
arXiv:2512.14067v1 Announce Type: cross 
Abstract: Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Frameworks for Real-World Audio-Visual Speech Recognition</title>
<link>https://arxiv.org/abs/2512.14083</link>
<guid>https://arxiv.org/abs/2512.14083</guid>
<content:encoded><![CDATA[
arXiv:2512.14083v1 Announce Type: cross 
Abstract: The practical deployment of Audio-Visual Speech Recognition (AVSR) systems is fundamentally challenged by significant performance degradation in real-world environments, characterized by unpredictable acoustic noise and visual interference. This dissertation posits that a systematic, hierarchical approach is essential to overcome these challenges, achieving the robust scalability at the representation, architecture, and system levels. At the representation level, we investigate methods for building a unified model that learns audio-visual features inherently robust to diverse real-world corruptions, thereby enabling generalization to new environments without specialized modules. To address architectural scalability, we explore how to efficiently expand model capacity while ensuring the adaptive and reliable use of multimodal inputs, developing a framework that intelligently allocates computational resources based on the input characteristics. Finally, at the system level, we present methods to expand the system's functionality through modular integration with large-scale foundation models, leveraging their powerful cognitive and generative capabilities to maximize final recognition accuracy. By systematically providing solutions at each of these three levels, this dissertation aims to build a next-generation, robust, and scalable AVSR system with high reliability in real-world applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting</title>
<link>https://arxiv.org/abs/2512.14115</link>
<guid>https://arxiv.org/abs/2512.14115</guid>
<content:encoded><![CDATA[
arXiv:2512.14115v1 Announce Type: cross 
Abstract: Acoustic Word Embeddings (AWEs) improve the efficiency of speech retrieval tasks such as Spoken Term Detection (STD) and Keyword Spotting (KWS). However, existing approaches suffer from limitations, including unimodal supervision, disjoint optimization of audio-audio and audio-text alignment, and the need for task-specific models. To address these shortcomings, we propose a joint multimodal contrastive learning framework that unifies both acoustic and cross-modal supervision in a shared embedding space. Our approach simultaneously optimizes: (i) audio-text contrastive learning, inspired by the CLAP loss, to align audio and text representations and (ii) audio-audio contrastive learning, via Deep Word Discrimination (DWD) loss, to enhance intra-class compactness and inter-class separation. The proposed method outperforms existing AWE baselines on word discrimination task while flexibly supporting both STD and KWS. To our knowledge, this is the first comprehensive approach of its kind.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms</title>
<link>https://arxiv.org/abs/2512.14221</link>
<guid>https://arxiv.org/abs/2512.14221</guid>
<content:encoded><![CDATA[
arXiv:2512.14221v1 Announce Type: cross 
Abstract: Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets</title>
<link>https://arxiv.org/abs/2512.14237</link>
<guid>https://arxiv.org/abs/2512.14237</guid>
<content:encoded><![CDATA[
arXiv:2512.14237v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G\"odel's Poetry</title>
<link>https://arxiv.org/abs/2512.14252</link>
<guid>https://arxiv.org/abs/2512.14252</guid>
<content:encoded><![CDATA[
arXiv:2512.14252v1 Announce Type: cross 
Abstract: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning</title>
<link>https://arxiv.org/abs/2512.14274</link>
<guid>https://arxiv.org/abs/2512.14274</guid>
<content:encoded><![CDATA[
arXiv:2512.14274v1 Announce Type: cross 
Abstract: Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Accuracy of Amortized Model Comparison with Self-Consistency</title>
<link>https://arxiv.org/abs/2512.14308</link>
<guid>https://arxiv.org/abs/2512.14308</guid>
<content:encoded><![CDATA[
arXiv:2512.14308v1 Announce Type: cross 
Abstract: Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning at the Edge: An Agnostic IIoT Architecture</title>
<link>https://arxiv.org/abs/2512.14311</link>
<guid>https://arxiv.org/abs/2512.14311</guid>
<content:encoded><![CDATA[
arXiv:2512.14311v1 Announce Type: cross 
Abstract: The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity</title>
<link>https://arxiv.org/abs/2512.14320</link>
<guid>https://arxiv.org/abs/2512.14320</guid>
<content:encoded><![CDATA[
arXiv:2512.14320v1 Announce Type: cross 
Abstract: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Attention Guided Defense Against Malicious Edits</title>
<link>https://arxiv.org/abs/2512.14333</link>
<guid>https://arxiv.org/abs/2512.14333</guid>
<content:encoded><![CDATA[
arXiv:2512.14333v1 Announce Type: cross 
Abstract: Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Transferable Defense Against Malicious Image Edits</title>
<link>https://arxiv.org/abs/2512.14341</link>
<guid>https://arxiv.org/abs/2512.14341</guid>
<content:encoded><![CDATA[
arXiv:2512.14341v1 Announce Type: cross 
Abstract: Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification</title>
<link>https://arxiv.org/abs/2512.14404</link>
<guid>https://arxiv.org/abs/2512.14404</guid>
<content:encoded><![CDATA[
arXiv:2512.14404v1 Announce Type: cross 
Abstract: In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the $\ell_0$ sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pattern Recognition of Aluminium Arbitrage in Global Trade Data</title>
<link>https://arxiv.org/abs/2512.14410</link>
<guid>https://arxiv.org/abs/2512.14410</guid>
<content:encoded><![CDATA[
arXiv:2512.14410v1 Announce Type: cross 
Abstract: As the global economy transitions toward decarbonization, the aluminium sector has become a focal point for strategic resource management. While policies such as the Carbon Border Adjustment Mechanism (CBAM) aim to reduce emissions, they have inadvertently widened the price arbitrage between primary metal, scrap, and semi-finished goods, creating new incentives for market optimization. This study presents a unified, unsupervised machine learning framework to detect and classify emerging trade anomalies within UN Comtrade data (2020 to 2024). Moving beyond traditional rule-based monitoring, we apply a four-layer analytical pipeline utilizing Forensic Statistics, Isolation Forests, Network Science, and Deep Autoencoders. Contrary to the hypothesis that Sustainability Arbitrage would be the primary driver, empirical results reveal a contradictory and more severe phenomenon of Hardware Masking. Illicit actors exploit bi-directional tariff incentives by misclassifying scrap as high-count heterogeneous goods to justify extreme unit-price outliers of >$160/kg, a 1,900% markup indicative of Trade-Based Money Laundering (TBML) rather than commercial arbitrage. Topologically, risk is not concentrated in major exporters but in high-centrality Shadow Hubs that function as pivotal nodes for illicit rerouting. These actors execute a strategy of Void-Shoring, systematically suppressing destination data to Unspecified Code to fracture mirror statistics and sever forensic trails. Validated by SHAP (Shapley Additive Explanations), the results confirm that price deviation is the dominant predictor of anomalies, necessitating a paradigm shift in customs enforcement from physical volume checks to dynamic, algorithmic valuation auditing.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset</title>
<link>https://arxiv.org/abs/2512.14422</link>
<guid>https://arxiv.org/abs/2512.14422</guid>
<content:encoded><![CDATA[
arXiv:2512.14422v1 Announce Type: cross 
Abstract: The cybersecurity of Industrial Control Systems that manage critical infrastructure such as Water Distribution Systems has become increasingly important as digital connectivity expands. BATADAL benchmark data is a good source of testing intrusion detection techniques, but it presents several important problems, such as imbalance in the number of classes, multivariate time dependence, and stealthy attacks. We consider a hybrid ensemble learning model that will enhance the detection ability of cyber-attacks in WDS by using the complementary capabilities of machine learning and deep learning models. Three base learners, namely, Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network, have been strictly compared and seven ensemble types using simple averaged and stacked learning with a logistic regression meta-learner. Random Forest analysis identified top predictors turned into temporal and statistical features, and Synthetic Minority Oversampling Technique (SMOTE) was used to overcome the class imbalance issue. The analyics indicates that the single Long Short-Term Memory network model is of poor performance (F1 = 0.000, AUC = 0.4460), but tree-based models, especially eXtreme Gradient Boosting, perform well (F1 = 0.7470, AUC=0.9684). The hybrid stacked ensemble of Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network scored the highest, with the attack class of 0.7205 with an F1-score and a AUC of 0.9826 indicating that the heterogeneous stacking between model precision and generalization can work. The proposed framework establishes a robust and scalable solution for cyber-attack detection in time-dependent industrial systems, integrating temporal learning and ensemble diversity to support the secure operation of critical infrastructure.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2512.14427</link>
<guid>https://arxiv.org/abs/2512.14427</guid>
<content:encoded><![CDATA[
arXiv:2512.14427v1 Announce Type: cross 
Abstract: The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C-ing Clearly: Enhanced Binary Code Explanations using C code</title>
<link>https://arxiv.org/abs/2512.14500</link>
<guid>https://arxiv.org/abs/2512.14500</guid>
<content:encoded><![CDATA[
arXiv:2512.14500v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis</title>
<link>https://arxiv.org/abs/2512.14602</link>
<guid>https://arxiv.org/abs/2512.14602</guid>
<content:encoded><![CDATA[
arXiv:2512.14602v1 Announce Type: cross 
Abstract: Automatic Music Transcription (AMT) -- the task of converting music audio into note representations -- has seen rapid progress, driven largely by deep learning systems. Due to the limited availability of richly annotated music datasets, much of the progress in AMT has been concentrated on classical piano music, and even a few very specific datasets. Whether these systems can generalize effectively to other musical contexts remains an open question. Complementing recent studies on distribution shifts in sound (e.g., recording conditions), in this work we investigate the musical dimension -- specifically, variations in genre, dynamics, and polyphony levels. To this end, we introduce the MDS corpus, comprising three distinct subsets -- (1) Genre, (2) Random, and (3) MAEtest -- to emulate different axes of distribution shift. We evaluate the performance of several state-of-the-art AMT systems on the MDS corpus using both traditional information-retrieval and musically-informed performance metrics. Our extensive evaluation isolates and exposes varying degrees of performance degradation under specific distribution shifts. In particular, we measure a note-level F1 performance drop of 20 percentage points due to sound, and 14 due to genre. Generally, we find that dynamics estimation proves more vulnerable to musical variation than onset prediction. Musically informed evaluation metrics, particularly those capturing harmonic structure, help identify potential contributing factors. Furthermore, experiments with randomly generated, non-musical sequences reveal clear limitations in system performance under extreme musical distribution shifts. Altogether, these findings offer new evidence of the persistent impact of the Corpus Bias problem in deep AMT systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts</title>
<link>https://arxiv.org/abs/2512.14604</link>
<guid>https://arxiv.org/abs/2512.14604</guid>
<content:encoded><![CDATA[
arXiv:2512.14604v1 Announce Type: cross 
Abstract: Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines</title>
<link>https://arxiv.org/abs/2512.14645</link>
<guid>https://arxiv.org/abs/2512.14645</guid>
<content:encoded><![CDATA[
arXiv:2512.14645v1 Announce Type: cross 
Abstract: Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization</title>
<link>https://arxiv.org/abs/2512.14687</link>
<guid>https://arxiv.org/abs/2512.14687</guid>
<content:encoded><![CDATA[
arXiv:2512.14687v1 Announce Type: cross 
Abstract: Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation</title>
<link>https://arxiv.org/abs/2512.14689</link>
<guid>https://arxiv.org/abs/2512.14689</guid>
<content:encoded><![CDATA[
arXiv:2512.14689v1 Announce Type: cross 
Abstract: Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spherical Leech Quantization for Visual Tokenization and Generation</title>
<link>https://arxiv.org/abs/2512.14697</link>
<guid>https://arxiv.org/abs/2512.14697</guid>
<content:encoded><![CDATA[
arXiv:2512.14697v1 Announce Type: cross 
Abstract: Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($\Lambda_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting</title>
<link>https://arxiv.org/abs/2305.15786</link>
<guid>https://arxiv.org/abs/2305.15786</guid>
<content:encoded><![CDATA[
arXiv:2305.15786v4 Announce Type: replace 
Abstract: Ensembling is among the most popular tools in machine learning (ML) due to its effectiveness in minimizing variance and thus improving generalization. Most ensembling methods for black-box base learners fall under the umbrella of "stacked generalization," namely training an ML algorithm that takes the inferences from the base learners as input. While stacking has been widely applied in practice, its theoretical properties are poorly understood. In this paper, we prove a novel result, showing that choosing the best stacked generalization from a (finite or finite-dimensional) family of stacked generalizations based on cross-validated performance does not perform "much worse" than the oracle best. Our result strengthens and significantly extends the results in Van der Laan et al. (2007). Inspired by the theoretical analysis, we further propose a particular family of stacked generalizations in the context of probabilistic forecasting, each one with a different sensitivity for how much the ensemble weights are allowed to vary across items, timestamps in the forecast horizon, and quantiles. Experimental results demonstrate the performance gain of the proposed method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially Private Knowledge Distillation via Synthetic Text Generation</title>
<link>https://arxiv.org/abs/2403.00932</link>
<guid>https://arxiv.org/abs/2403.00932</guid>
<content:encoded><![CDATA[
arXiv:2403.00932v3 Announce Type: replace 
Abstract: Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy puts pressure on practitioners to train LLMs with Differential Privacy (DP) on private data. Concurrently, the exponential growth in parameter size of LLMs necessitates model compression before deployment of LLMs on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, simultaneously applying both schemes can compound the utility degradation. To this end, we propose DistilDP: a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private teacher LLM. The knowledge of a teacher LLM is transferred onto the student in two ways: one way from the synthetic data itself -- the hard labels, and the other way by the output distribution of the teacher evaluated on the synthetic data -- the soft labels. Furthermore, if the teacher and student share a similar architectural structure, we can further distill knowledge by aligning the hidden representations between both. Our experimental results demonstrate that DistilDP can substantially improve the utility over existing baselines, at least $9.0$ PPL on the Big Patent dataset, with strong privacy parameters, $\epsilon=2$. These promising results progress privacy-preserving compression of autoregressive LLMs. Our code can be accessed here: https://github.com/james-flemings/dp_compress.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I-Diff: Structural Regularization for High-Fidelity Diffusion Models</title>
<link>https://arxiv.org/abs/2403.16790</link>
<guid>https://arxiv.org/abs/2403.16790</guid>
<content:encoded><![CDATA[
arXiv:2403.16790v3 Announce Type: replace 
Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have significantly advanced generative AI, achieving impressive results in high-quality image and data generation. However, enhancing fidelity without compromising semantic content remains a key challenge in the field. Recent diffusion research in multiple disciplines has introduced objectives and architectural refinements that tighten the match between generated and real data distributions, yielding higher fidelity than earlier generative frameworks. Multi-stage architectures, physics-guided modeling, semantic conditioning, and rarity-aware generation are some of the explored works to achieve this task. However, the integration of structural information of the data distribution into DDPM has largely been unexplored. The conventional DDPM framework relies solely on the $L^2$ norm between the additive and predicted noise to generate new data distributions. We introduce I-Diff, an improved version of DDPM that incorporates a carefully designed regularizer, effectively enabling the model to encode structural information, thereby preserving the inherent fidelity of the data distribution. The proposed approach is validated through extensive experiments on DDPM, Improved DDPM and Latent Diffusion Model across multiple datasets. Empirical results demonstrate significant improvements in fidelity (Density and Precision increase 10% and 37% in CIFAR-100 dataset respectively) across other tested datasets. These results highlight the effectiveness of our method in enhancing the fidelity of the generated data. Notably, improvements across different models highlight the model-agnostic nature of our proposed method in the wider field of generative AI.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Representation Learning from Sparse Transformation Analysis</title>
<link>https://arxiv.org/abs/2410.05564</link>
<guid>https://arxiv.org/abs/2410.05564</guid>
<content:encoded><![CDATA[
arXiv:2410.05564v2 Announce Type: replace 
Abstract: There is a vast literature on representation learning based on principles such as coding efficiency, statistical independence, causality, controllability, or symmetry. In this paper we propose to learn representations from sequence data by factorizing the transformations of the latent variables into sparse components. Input data are first encoded as distributions of latent activations and subsequently transformed using a probability flow model, before being decoded to predict a future input state. The flow model is decomposed into a number of rotational (divergence-free) vector fields and a number of potential flow (curl-free) fields. Our sparsity prior encourages only a small number of these fields to be active at any instant and infers the speed with which the probability flows along these fields. Training this model is completely unsupervised using a standard variational objective and results in a new form of disentangled representations where the input is not only represented by a combination of independent factors, but also by a combination of independent transformation primitives given by the learned flow fields. When viewing the transformations as symmetries one may interpret this as learning approximately equivariant representations. Empirically we demonstrate that this model achieves state of the art in terms of both data likelihood and unsupervised approximate equivariance errors on datasets composed of sequence transformations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Multi-modal Root Cause Identification in Microservice Systems</title>
<link>https://arxiv.org/abs/2410.10021</link>
<guid>https://arxiv.org/abs/2410.10021</guid>
<content:encoded><![CDATA[
arXiv:2410.10021v2 Announce Type: replace 
Abstract: Root Cause Analysis (RCA) is essential for pinpointing the root causes of failures in microservice systems. Traditional data-driven RCA methods are typically limited to offline applications due to high computational demands, and existing online RCA methods handle only single-modal data, overlooking complex interactions in multi-modal systems. In this paper, we introduce OCEAN, a novel online multi-modal causal structure learning method for root cause localization. OCEAN employs a dilated convolutional neural network to capture long-term temporal dependencies and graph neural networks to learn causal relationships among system entities and key performance indicators. We further design a multi-factor attention mechanism to analyze and reassess the relationships among different metrics and log indicators/attributes for enhanced online causal graph learning. Additionally, a contrastive mutual information maximization-based graph fusion module is developed to effectively model the relationships across various modalities. Extensive experiments on three real-world datasets demonstrate the effectiveness and efficiency of our proposed method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transparent Networks for Multivariate Time Series</title>
<link>https://arxiv.org/abs/2410.10535</link>
<guid>https://arxiv.org/abs/2410.10535</guid>
<content:encoded><![CDATA[
arXiv:2410.10535v3 Announce Type: replace 
Abstract: Transparent models, which provide inherently interpretable predictions, are receiving significant attention in high-stakes domains. However, despite much real-world data being collected as time series, there is a lack of studies on transparent time series models. To address this gap, we propose a novel transparent neural network model for time series called Generalized Additive Time Series Model (GATSM). GATSM consists of two parts: 1) independent feature networks to learn feature representations, and 2) a transparent temporal module to learn temporal patterns across different time steps using the feature representations. This structure allows GATSM to effectively capture temporal patterns and handle varying-length time series while preserving transparency. Empirical experiments show that GATSM significantly outperforms existing generalized additive models and achieves comparable performance to black-box time series models, such as recurrent neural networks and Transformer. In addition, we demonstrate that GATSM finds interesting patterns in time series.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-guided multi-property molecular optimization with a diffusion language model</title>
<link>https://arxiv.org/abs/2410.13597</link>
<guid>https://arxiv.org/abs/2410.13597</guid>
<content:encoded><![CDATA[
arXiv:2410.13597v4 Announce Type: replace 
Abstract: Molecular optimization (MO) is a crucial stage in drug discovery in which task-oriented generated molecules are optimized to meet practical industrial requirements. Existing mainstream MO approaches primarily utilize external property predictors to guide iterative property optimization. However, learning all molecular samples in the vast chemical space is unrealistic for predictors. As a result, errors and noise are inevitably introduced during property prediction due to the nature of approximation. This leads to discrepancy accumulation, generalization reduction and suboptimal molecular candidates. In this paper, we propose a text-guided multi-property molecular optimization method utilizing transformer-based diffusion language model (TransDLM). TransDLM leverages standardized chemical nomenclature as semantic representations of molecules and implicitly embeds property requirements into textual descriptions, thereby mitigating error propagation during diffusion process. By fusing physically and chemically detailed textual semantics with specialized molecular representations, TransDLM effectively integrates diverse information sources to guide precise optimization, which enhances the model's ability to balance structural retention and property enhancement. Additionally, the success of a case study further demonstrates TransDLM's ability to solve practical problems. Experimentally, our approach surpasses state-of-the-art methods in maintaining molecular structural similarity and enhancing chemical properties on the benchmark dataset.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shared DIFF Transformer</title>
<link>https://arxiv.org/abs/2501.17900</link>
<guid>https://arxiv.org/abs/2501.17900</guid>
<content:encoded><![CDATA[
arXiv:2501.17900v2 Announce Type: replace 
Abstract: DIFF Transformer improves attention allocation by enhancing focus on relevant context while suppressing noise. It introduces a differential attention mechanism that calculates the difference between two independently generated attention distributions, effectively reducing noise and promoting sparse attention patterns. However, the independent signal generation in DIFF Transformer results in parameter redundancy and suboptimal utilization of information. In this work, we propose Shared DIFF Transformer, which draws on the idea of a differential amplifier by introducing a shared base matrix to model global patterns and incorporating low-rank updates to enhance task-specific flexibility. This design significantly reduces parameter redundancy, improves efficiency, and retains strong noise suppression capabilities. Experimental results show that, compared to DIFF Transformer, our method achieves better performance in tasks such as long-sequence modeling, key information retrieval, and in-context learning. Our work provides a novel and efficient approach to optimizing differential attention mechanisms and advancing robust Transformer architectures.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating the Generalizability of ECG Noise Detection Across Diverse Data Sources and Noise Types</title>
<link>https://arxiv.org/abs/2502.14522</link>
<guid>https://arxiv.org/abs/2502.14522</guid>
<content:encoded><![CDATA[
arXiv:2502.14522v2 Announce Type: replace 
Abstract: Electrocardiograms (ECGs) are vital for monitoring cardiac health, enabling the assessment of heart rate variability (HRV), detection of arrhythmias, and diagnosis of cardiovascular conditions. However, ECG signals recorded from wearable devices are frequently corrupted by noise artifacts, particularly those arising from motion and large muscle activity, which distort R-peaks and the QRS complex. These distortions hinder reliable HRV analysis and increase the risk of clinical misinterpretation.
  Existing studies on ECG noise detection typically evaluate performance on a single dataset, limiting insight into the generalizability of such methods across diverse sensors and recording conditions. In this work, we propose an HRV-based machine learning approach to detect noisy ECG segments and evaluate its generalizability using cross-dataset experiments on four datasets collected in both controlled and uncontrolled settings. Our method achieves over 90% average accuracy and an AUPRC exceeding 90%, even on previously unseen datasets-demonstrating robust performance across heterogeneous data sources. To support reproducibility and further research, we also release a curated and labeled ECG dataset annotated for noise artifacts.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.14704</link>
<guid>https://arxiv.org/abs/2502.14704</guid>
<content:encoded><![CDATA[
arXiv:2502.14704v4 Announce Type: replace 
Abstract: Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning. The code is available at https://github.com/SuDIS-ZJU/SCAM.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group-robust Machine Unlearning</title>
<link>https://arxiv.org/abs/2503.09330</link>
<guid>https://arxiv.org/abs/2503.09330</guid>
<content:encoded><![CDATA[
arXiv:2503.09330v2 Announce Type: replace 
Abstract: Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group (e.g., ethnicity, gender), we empirically show that performance for this group degrades, leading to fairness issues. To perform unlearning while preserving fairness, this work addresses the overlooked problem of non-uniformly distributed forget sets, which we refer to as group-robust machine unlearning. We formalize the problem and present a simple and effective exact unlearning strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unitless Unrestricted Markov-Consistent SCM Generation: Better Benchmark Datasets for Causal Discovery</title>
<link>https://arxiv.org/abs/2503.17037</link>
<guid>https://arxiv.org/abs/2503.17037</guid>
<content:encoded><![CDATA[
arXiv:2503.17037v3 Announce Type: replace 
Abstract: Causal discovery aims to extract qualitative causal knowledge in the form of causal graphs from data. Because causal ground truth is rarely known in the real world, simulated data plays a vital role in evaluating the performance of the various causal discovery algorithms proposed in the literature. But recent work highlighted certain artifacts of commonly used data generation techniques for a standard class of structural causal models (SCM) that may be nonphysical, including var- and R2-sortability, where the variables' variance and coefficients of determination (R2) after regressing on all other variables, respectively, increase along the causal order. Some causal methods exploit such artifacts, leading to unrealistic expectations for their performance on real-world data. Some modifications have been proposed to remove these artifacts; notably, the internally-standardized structural causal model (iSCM) avoids varsortability and largely alleviates R2-sortability on sparse causal graphs, but exhibits a reversed R2-sortability pattern for denser graphs not featured in their work. We analyze which sortability patterns we expect to see in real data, and propose a method for drawing coefficients that we argue more effectively samples the space of SCMs. Finally, we propose a novel extension of our SCM generation method to the time series setting.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Approximation with Softmax Attention</title>
<link>https://arxiv.org/abs/2504.15956</link>
<guid>https://arxiv.org/abs/2504.15956</guid>
<content:encoded><![CDATA[
arXiv:2504.15956v2 Announce Type: replace 
Abstract: We prove that with linear transformations, both (i) two-layer self-attention and (ii) one-layer self-attention followed by a softmax function are universal approximators for continuous sequence-to-sequence functions on compact domains. Our main technique is a new interpolation-based method for analyzing attention's internal mechanism. This leads to our key insight: self-attention is able to approximate a generalized version of ReLU to arbitrary precision, and hence subsumes many known universal approximators. Building on these, we show that two-layer multi-head attention alone suffices as a sequence-to-sequence universal approximator. In contrast, prior works rely on feed-forward networks to establish universal approximation in Transformers. Furthermore, we extend our techniques to show that, (softmax-)attention-only layers are capable of approximating various statistical models in-context. We believe these techniques hold independent interest.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable Trajectory Clustering: An Efficient Split and Merge Algorithm</title>
<link>https://arxiv.org/abs/2504.21808</link>
<guid>https://arxiv.org/abs/2504.21808</guid>
<content:encoded><![CDATA[
arXiv:2504.21808v2 Announce Type: replace 
Abstract: Clustering algorithms fundamentally group data points by characteristics to identify patterns. Over the past two decades, researchers have extended these methods to analyze trajectories of humans, animals, and vehicles, studying their behavior and movement across applications. \noindent This paper presents whole-trajectory clustering and sub-trajectory clustering algorithms based on DBSCAN line segment clustering, which encompasses two key events: split and merge of line segments. The events are utilized to capture object movement history based on the average Euclidean distance between line segments.
  In this framework, whole-trajectory clustering considers entire entities' trajectories, whereas sub-trajectory clustering employs a sliding window model to identify local similarity patterns. Many existing trajectory clustering algorithms respond to temporary anomalies in data by splitting trajectories, which often obscures otherwise consistent clustering patterns and leads to less reliable insights. To address this, we introduce the stable trajectory clustering algorithm, which leverages the mean absolute deviation concept to demonstrate that selective omission of transient deviations not only preserves the integrity of clusters but also improves their stability and interpretability. We evaluate all proposed algorithms on real trajectory datasets to illustrate their effectiveness and sensitivity to parameter variations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2505.13109</link>
<guid>https://arxiv.org/abs/2505.13109</guid>
<content:encoded><![CDATA[
arXiv:2505.13109v3 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Manifold Learning for Personalized and Label-Free Detection of Cardiac Arrhythmias</title>
<link>https://arxiv.org/abs/2506.16494</link>
<guid>https://arxiv.org/abs/2506.16494</guid>
<content:encoded><![CDATA[
arXiv:2506.16494v2 Announce Type: replace 
Abstract: Electrocardiograms (ECGs) provide direct, non-invasive measurements of heart activity and are well-established tools for detecting and monitoring cardiovascular disease. However, manual ECG analysis can be time-consuming and prone to errors. Machine learning has emerged as a promising approach for automated heartbeat recognition and classification, but substantial variations in ECG signals make it challenging to develop generalizable supervised models. ECG signals vary widely across individuals and leads, while datasets often follow different labeling standards and may be biased, greatly hindering supervised methods. Conventional unsupervised methods, such as principal component analysis, prioritize large (often obvious) variances and typically overlook subtle yet clinically relevant patterns. When labels are missing or variations are small, both approaches fail. Here, we show that nonlinear dimensionality reduction (NLDR) algorithms, namely t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold approximation and projection (UMAP), can address these challenges and identify medically relevant features in ECG signals without training or prior information. Using lead II and V1 signals from the MIT-BIH dataset, UMAP and t-SNE generate rich two-dimensional latent spaces with visually separable clusters. Applied to mixed populations of heartbeats, these clusters correspond to different individuals, while for single subjects they reveal distinct arrhythmia patterns. A simple classifier on these embeddings discriminates individual recordings with >= 90% accuracy and identifies arrhythmias in single patients with a median accuracy of 98.96% and median F1-score of 91.02%. The results show that NLDR holds much promise for cardiac monitoring, including the limiting cases of single-lead ECG and the current 12-lead standard of care, and for personalized health care beyond cardiology.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful</title>
<link>https://arxiv.org/abs/2507.07101</link>
<guid>https://arxiv.org/abs/2507.07101</guid>
<content:encoded><![CDATA[
arXiv:2507.07101v4 Announce Type: replace 
Abstract: Conventional wisdom dictates that small batch sizes make language model pretraining and fine-tuning unstable, motivating gradient accumulation, which trades off the number of optimizer steps for a proportional increase in batch size. While it is common to decrease the learning rate for smaller batch sizes, other hyperparameters are often held fixed. In this work, we revisit small batch sizes all the way down to batch size one, and we propose a rule for scaling Adam hyperparameters to small batch sizes. In particular, rather than holding the decay rate of the second moment fixed across batch sizes, we propose to hold its half-life fixed in terms of tokens. We find that small batch sizes (1) train stably, (2) are consistently more robust to hyperparameter choices, (3) achieve equal or better per-FLOP performance than larger batch sizes, and (4) notably enable stable language model training with vanilla SGD, even without momentum, despite storing no optimizer state. Building on these results, we provide practical recommendations for selecting a batch size and setting optimizer hyperparameters. We further recommend against gradient accumulation unless training on multiple devices with multiple model replicas. Finally, we show that a small batch size combined with an optimizer with a small state size can provide the performance benefits of full fine-tuning while maintaining a similar memory footprint to LoRA.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins</title>
<link>https://arxiv.org/abs/2507.18423</link>
<guid>https://arxiv.org/abs/2507.18423</guid>
<content:encoded><![CDATA[
arXiv:2507.18423v2 Announce Type: replace 
Abstract: Despite the necessity for accurate flood prediction, many regions lack sufficient river discharge observations. Although numerous models for daily river discharge prediction exist, achieving high accuracy, interpretability, and efficiency under data-scarce conditions remains a major challenge. We address this with a novel method, HYdrological Prediction with multi-model Ensemble and Reservoir computing (HYPER). Our approach applies Bayesian model averaging (BMA) to 47 "uncalibrated" catchment-based conceptual hydrological models. A reservoir computing (RC) model, a type of machine learning model, is then trained via linear regression to correct BMA output errors, a non-iterative process ensuring computational efficiency. For ungauged basins, we infer the required BMA and RC weights by mapping them to catchment attributes from gauged basins, creating a generalizable framework. Evaluated on 87 Japanese basins, in a data-rich scenario, HYPER (median Nash Sutcliffe Efficiency, NSE, of 0.59) performed comparably to a benchmark LSTM (NSE 0.64) but required only 3 % of its computational time. In a data-scarce scenario (where only ~20 % of basins are gauged), HYPER maintained robust performance (NSE 0.51) by leveraging the physical structure of the ensemble. In contrast, the LSTM's performance degraded substantially (NSE -0.61) due to data insufficiency. These results demonstrate that calibrating individual conceptual hydrological models is unnecessary when using a sufficiently large ensemble that is assembled and combined with machine-learning-based bias correction. HYPER provides a robust, efficient, and generalizable solution for discharge prediction, particularly in ungauged basins. By eliminating basin-specific calibration, HYPER offers a scalable, interpretable framework for accurate hydrological prediction in diverse data-scarce regions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles</title>
<link>https://arxiv.org/abs/2508.00969</link>
<guid>https://arxiv.org/abs/2508.00969</guid>
<content:encoded><![CDATA[
arXiv:2508.00969v2 Announce Type: replace 
Abstract: Self-supervised learning (SSL) has driven major advances in computational pathology by enabling the learning of rich representations from histopathology data. Yet, tissue analysis alone may fall short in capturing broader molecular complexity, as key complementary information resides in high-dimensional omics profiles such as transcriptomics, methylomics, and genomics. To address this gap, we introduce MORPHEUS, the first multimodal pre-training strategy that integrates histopathology images and multi-omics data within a shared transformer-based architecture. At its core, MORPHEUS relies on a novel masked omics modeling objective that encourages the model to learn meaningful cross-modal relationships. This yields a general-purpose pre-trained encoder that can be applied to histopathology alone or in combination with any subset of omics modalities. Beyond inference, MORPHEUS also supports flexible any-to-any omics reconstruction, enabling one or more omics profiles to be reconstructed from any modality subset that includes histopathology. Pre-trained on a large pan-cancer cohort, MORPHEUS shows substantial improvements over supervised and SSL baselines across diverse tasks and modality combinations. Together, these capabilities position it as a promising direction for the development of multimodal foundation models in oncology. Code is publicly available at https://github.com/Lucas-rbnt/MORPHEUS
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Unsupervised Deep Explainable AI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals</title>
<link>https://arxiv.org/abs/2508.09162</link>
<guid>https://arxiv.org/abs/2508.09162</guid>
<content:encoded><![CDATA[
arXiv:2508.09162v3 Announce Type: replace 
Abstract: Next generation advanced nuclear reactors are expected to be smaller both in size and power output, relying extensively on fully digital instrumentation and control systems. These reactors will generate a large flow of information in the form of multivariate time series data, conveying simultaneously various non linear cyber physical, process, control, sensor, and operational states. Ensuring data integrity against deception attacks is becoming increasingly important for networked communication and a requirement for safe and reliable operation. Current efforts to address replay attacks, almost universally focus on watermarking or supervised anomaly detection approaches without further identifying and characterizing the root cause of the anomaly. In addition, these approaches rely mostly on synthetic data with uncorrelated Gaussian process and measurement noise and full state feedback or are limited to univariate signals, signal stationarity, linear quadratic regulators, or other linear-time invariant state-space which may fail to capture any unmodeled system dynamics. In the realm of regulated nuclear cyber-physical systems, additional work is needed on characterization of replay attacks and explainability of predictions using real data. Here, we propose an unsupervised explainable AI framework based on a combination of autoencoder and customized windowSHAP algorithm to fully characterize real-time replay attacks, i.e., detection, source identification, timing and type, of increasing complexity during a dynamic time evolving reactor process. The proposed XAI framework was benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1 with up to six signals concurrently being replayed. In all cases, the XAI framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval Enhanced Feedback via In-context Neural Error-book</title>
<link>https://arxiv.org/abs/2508.16313</link>
<guid>https://arxiv.org/abs/2508.16313</guid>
<content:encoded><![CDATA[
arXiv:2508.16313v5 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Gaussian Initializations: Signal Preserving Weight Initialization for Odd-Sigmoid Activations</title>
<link>https://arxiv.org/abs/2509.23085</link>
<guid>https://arxiv.org/abs/2509.23085</guid>
<content:encoded><![CDATA[
arXiv:2509.23085v2 Announce Type: replace 
Abstract: Activation functions critically influence trainability and expressivity, and recent work has therefore explored a broad range of nonlinearities. However, widely used Gaussian i.i.d. initializations are designed to preserve activation variance under wide or infinite width assumptions. In deep and relatively narrow networks with sigmoidal nonlinearities, these schemes often drive preactivations into saturation, and collapse gradients. To address this, we introduce an odd-sigmoid activations and propose an activation aware initialization tailored to any function in this class. Our method remains robust over a wide band of variance scales, preserving both forward signal variance and backpropagated gradient norms even in very deep and narrow networks. Empirically, across standard image benchmarks we find that the proposed initialization is substantially less sensitive to depth, width, and activation scale than Gaussian initializations. In physics informed neural networks (PINNs), scaled odd-sigmoid activations combined with our initialization achieve lower losses than Gaussian based setups, suggesting that diagonal-plus-noise weights provide a practical alternative when Gaussian initialization breaks down.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forking-Sequences</title>
<link>https://arxiv.org/abs/2510.04487</link>
<guid>https://arxiv.org/abs/2510.04487</guid>
<content:encoded><![CDATA[
arXiv:2510.04487v2 Announce Type: replace 
Abstract: While accuracy is a critical requirement for time series forecasting, an equally important desideratum is forecast stability across forecast creation dates (FCDs). Even highly accurate models can produce erratic revisions between FCDs, disrupting downstream decision-making. To improve forecast stability, several state-of-the-art models including MQCNN, MQT, and SPADE employ a powerful yet underexplored neural network architectural design known as forking-sequences. This architectural design jointly encodes and decodes the entire time series across all FCDs, producing an entire multi-horizon forecast grid in a single forward pass. This approach contrasts with conventional statistical and neural forecasting methods that process FCDs independently, generating only a single multi-horizon forecast per forward pass. In this work, we formalize the forking-sequences design and motivate its broader adoption by introducing a metric for quantifying excess volatility in forecast revisions and by providing theoretical and empirical analysis. We theoretically motivate three key benefits of forking-sequences: (i) increased forecast stability through ensembling; (ii) gradient variance reduction, leading to more stable and consistent training steps; and (iii) improved computational efficiency during inference. We validate the benefits of forking-sequences compared to baseline window-sampling on the M-series benchmark, using 16 datasets from the M1, M3, M4, and Tourism competitions. We observe median accuracy improvements across datasets of 29.7%, 46.2%, 49.3%, 28.6%, 24.7%, and 6.4% for MLP, RNN, LSTM, CNN, Transformer, and State Space-based architectures, respectively. We then show that forecast ensembling during inference can improve median forecast stability by 10.8%, 13.2%, 13.0%, 10.9%, 10.2%, and 11.2% for these respective models trained with forking-sequences, while maintaining accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network</title>
<link>https://arxiv.org/abs/2510.09767</link>
<guid>https://arxiv.org/abs/2510.09767</guid>
<content:encoded><![CDATA[
arXiv:2510.09767v2 Announce Type: replace 
Abstract: Graph Transformers have recently achieved remarkable progress in graph representation learning by capturing long-range dependencies through self-attention. However, their quadratic computational complexity and inability to effectively model heterogeneous semantics severely limit their scalability and generalization on real-world heterogeneous graphs. To address these issues, we propose HeSRN, a novel Heterogeneous Slot-aware Retentive Network for efficient and expressive heterogeneous graph representation learning. HeSRN introduces a slot-aware structure encoder that explicitly disentangles node-type semantics by projecting heterogeneous features into independent slots and aligning their distributions through slot normalization and retention-based fusion, effectively mitigating the semantic entanglement caused by forced feature-space unification in previous Transformer-based models. Furthermore, we replace the self-attention mechanism with a retention-based encoder, which models structural and contextual dependencies in linear time complexity while maintaining strong expressive power. A heterogeneous retentive encoder is further employed to jointly capture both local structural signals and global heterogeneous semantics through multi-scale retention layers. Extensive experiments on four real-world heterogeneous graph datasets demonstrate that HeSRN consistently outperforms state-of-the-art heterogeneous graph neural networks and Graph Transformer baselines on node classification tasks, achieving superior accuracy with significantly lower computational complexity.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Sampler Stochasticity in Training Diffusion Models for RLHF</title>
<link>https://arxiv.org/abs/2510.10767</link>
<guid>https://arxiv.org/abs/2510.10767</guid>
<content:encoded><![CDATA[
arXiv:2510.10767v2 Announce Type: replace 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is increasingly used to fine-tune diffusion models, but a key challenge arises from the mismatch between stochastic samplers used during training and deterministic samplers used during inference. In practice, models are fine-tuned using stochastic SDE samplers to encourage exploration, while inference typically relies on deterministic ODE samplers for efficiency and stability. This discrepancy induces a reward gap, raising concerns about whether high-quality outputs can be expected during inference. In this paper, we theoretically characterize this reward gap and provide non-vacuous bounds for general diffusion models, along with sharper convergence rates for Variance Exploding (VE) and Variance Preserving (VP) Gaussian models. Methodologically, we adopt the generalized denoising diffusion implicit models (gDDIM) framework to support arbitrarily high levels of stochasticity, preserving data marginals throughout. Empirically, our findings through large-scale experiments on text-to-image models using denoising diffusion policy optimization (DDPO) and mixed group relative policy optimization (MixGRPO) validate that reward gaps consistently narrow over training, and ODE sampling quality improves when models are updated using higher-stochasticity SDE training.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective</title>
<link>https://arxiv.org/abs/2510.14510</link>
<guid>https://arxiv.org/abs/2510.14510</guid>
<content:encoded><![CDATA[
arXiv:2510.14510v5 Announce Type: replace 
Abstract: Time Series Forecasting has made significant progress with the help of Patching technique, which partitions time series into multiple patches to effectively retain contextual semantic information into a representation space beneficial for modeling long-term dependencies. However, conventional patching partitions a time series into adjacent patches, which causes a fixed representation space, thus resulting in insufficiently expressful representations. In this paper, we pioneer the exploration of constructing a selective representation space to flexibly include the most informative patches for forecasting. Specifically, we propose the Selective Representation Space (SRS) module, which utilizes the learnable Selective Patching and Dynamic Reassembly techniques to adaptively select and shuffle the patches from the contextual time series, aiming at fully exploiting the information of contextual time series to enhance the forecasting performance of patch-based models. To demonstrate the effectiveness of SRS module, we propose a simple yet effective SRSNet consisting of SRS and an MLP head, which achieves state-of-the-art performance on real-world datasets from multiple domains. Furthermore, as a novel plug-and-play module, SRS can also enhance the performance of existing patch-based models. The resources are available at https://github.com/decisionintelligence/SRSNet.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.25502</link>
<guid>https://arxiv.org/abs/2510.25502</guid>
<content:encoded><![CDATA[
arXiv:2510.25502v3 Announce Type: replace 
Abstract: Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval, fev-bench and Chronos-ZS benchmarks, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Conditional VAE with approximation using Normalizing Flows</title>
<link>https://arxiv.org/abs/2511.08946</link>
<guid>https://arxiv.org/abs/2511.08946</guid>
<content:encoded><![CDATA[
arXiv:2511.08946v3 Announce Type: replace 
Abstract: Variational Autoencoders and Generative Adversarial Networks remained the state-of-the-art (SOTA) generative models until 2022. Now they are superseded by diffusion based models. Efforts to improve traditional models have stagnated as a result. In old-school fashion, we explore image generation with conditional Variational Autoencoders (CVAE) to incorporate desired attributes within the images. VAEs are known to produce blurry images with less diversity, we refer a method that solve this issue by leveraging the variance of the gaussian decoder as a learnable parameter during training. Previous works on CVAEs assumed that the conditional distribution of the latent space given the labels is equal to the prior distribution, which is not the case in reality. We show that estimating it using normalizing flows results in better image generation than existing methods by reducing the FID by 4% and increasing log likelihood by 7.6% than the previous case.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Doubly Wild Refitting: Model-Free Evaluation of High Dimensional Black-Box Predictions under Convex Losses</title>
<link>https://arxiv.org/abs/2511.18789</link>
<guid>https://arxiv.org/abs/2511.18789</guid>
<content:encoded><![CDATA[
arXiv:2511.18789v2 Announce Type: replace 
Abstract: We study the problem of excess risk evaluation for empirical risk minimization (ERM) under general convex loss functions. Our contribution is an efficient refitting procedure that computes the excess risk and provides high-probability upper bounds under the fixed-design setting. Assuming only black-box access to the training algorithm and a single dataset, we begin by generating two sets of artificially modified pseudo-outcomes termed wild response, created by stochastically perturbing the gradient vectors with carefully chosen scaling. Using these two pseudo-labeled datasets, we then refit the black-box procedure twice to obtain two corresponding wild predictors. Finally, leveraging the original predictor, the two wild predictors, and the constructed wild responses, we derive an efficient excess risk upper bound. A key feature of our analysis is that it requires no prior knowledge of the complexity of the underlying function class. As a result, the method is essentially model-free and holds significant promise for theoretically evaluating modern opaque machine learning system--such as deep nerral networks and generative model--where traditional capacity-based learning theory becomes infeasible due to the extreme complexity of the hypothesis class.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSTN: Fast and Efficient Multivariate Time Series Prediction Model</title>
<link>https://arxiv.org/abs/2511.20577</link>
<guid>https://arxiv.org/abs/2511.20577</guid>
<content:encoded><![CDATA[
arXiv:2511.20577v2 Announce Type: replace 
Abstract: Real-world time series often exhibit strong non-stationarity, complex nonlinear dynamics, and behaviour expressed across multiple temporal scales, from rapid local fluctuations to slow-evolving long-range trends. However, many contemporary architectures impose rigid, fixed-scale structural priors -- such as patch-based tokenization, predefined receptive fields, or frozen backbone encoders -- which can over-regularize temporal dynamics and limit adaptability to abrupt high-magnitude events. To handle this, we introduce the \emph{Multi-scale Temporal Network} (MSTN), a hybrid neural architecture grounded in an \emph{Early Temporal Aggregation} principle. MSTN integrates three complementary components: (i) a multi-scale convolutional encoder that captures fine-grained local structure; (ii) a sequence modeling module that learns long-range dependencies through either recurrent or attention-based mechanisms; and (iii) a self-gated fusion stage incorporating squeeze-excitation and multi-head attention to dynamically modulate cross-scale representations. This design enables MSTN to flexibly model temporal patterns spanning milliseconds to extended horizons, while avoiding the computational burden typically associated with long-context models. Across extensive benchmarks covering forecasting, imputation, classification, and cross-dataset generalization, MSTN consistently delivers state-of-the-art performance, outperforming recent leading approaches including TIME-LLM, HiMTM, SOFTS, LLM4TS, TimesNet, and PatchTST, and establishing new best results on 24 out of 32 datasets. Despite its strong performance, MSTN remains lightweight and supports fast inference, making it well suited for deployment on edge devices and resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale-Agnostic Kolmogorov-Arnold Geometry in Neural Networks</title>
<link>https://arxiv.org/abs/2511.21626</link>
<guid>https://arxiv.org/abs/2511.21626</guid>
<content:encoded><![CDATA[
arXiv:2511.21626v3 Announce Type: replace 
Abstract: Recent work by Freedman and Mulligan demonstrated that shallow multilayer perceptrons spontaneously develop Kolmogorov-Arnold geometric (KAG) structure during training on synthetic three-dimensional tasks. However, it remained unclear whether this phenomenon persists in realistic high-dimensional settings and what spatial properties this geometry exhibits.
  We extend KAG analysis to MNIST digit classification (784 dimensions) using 2-layer MLPs with systematic spatial analysis at multiple scales. We find that KAG emerges during training and appears consistently across spatial scales, from local 7-pixel neighborhoods to the full 28x28 image. This scale-agnostic property holds across different training procedures: both standard training and training with spatial augmentation produce the same qualitative pattern. These findings reveal that neural networks spontaneously develop organized, scale-invariant geometric structure during learning on realistic high-dimensional data.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Difficulties with Evaluating a Deception Detector for AIs</title>
<link>https://arxiv.org/abs/2511.22662</link>
<guid>https://arxiv.org/abs/2511.22662</guid>
<content:encoded><![CDATA[
arXiv:2511.22662v2 Announce Type: replace 
Abstract: Building reliable deception detectors for AI systems -- methods that could predict when an AI system is being strategically deceptive without necessarily requiring behavioural evidence -- would be valuable in mitigating risks from advanced AI systems. But evaluating the reliability and efficacy of a proposed deception detector requires examples that we can confidently label as either deceptive or honest. We argue that we currently lack the necessary examples and further identify several concrete obstacles in collecting them. We provide evidence from conceptual arguments, analysis of existing empirical works, and analysis of novel illustrative case studies. We also discuss the potential of several proposed empirical workarounds to these problems and argue that while they seem valuable, they also seem insufficient alone. Progress on deception detection likely requires further consideration of these problems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts</title>
<link>https://arxiv.org/abs/2511.23442</link>
<guid>https://arxiv.org/abs/2511.23442</guid>
<content:encoded><![CDATA[
arXiv:2511.23442v2 Announce Type: replace 
Abstract: Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light-Weight Benchmarks Reveal the Hidden Hardware Cost of Zero-Shot Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2512.00888</link>
<guid>https://arxiv.org/abs/2512.00888</guid>
<content:encoded><![CDATA[
arXiv:2512.00888v2 Announce Type: replace 
Abstract: Zero-shot foundation models (FMs) promise training-free prediction on tabular data, yet their hardware footprint remains poorly characterized. We present a fully reproducible benchmark that reports test accuracy together with wall-clock latency, peak CPU RAM, and peak GPU VRAM on four public datasets: Adult-Income, Higgs-100k, Wine-Quality, and California-Housing. Two open FMs (TabPFN-1.0 and TabICL-base) are compared against tuned XGBoost, LightGBM, and Random Forest baselines on a single NVIDIA T4 GPU. The tree ensembles equal or surpass FM accuracy on three datasets while completing full-test batches in <= 0.40 s and <= 150 MB RAM, using zero VRAM. TabICL achieves a 0.8 percentage-point gain on Higgs but requires roughly 40,000 times more latency (960 s) and 9 GB VRAM. TabPFN matches tree-model accuracy on Wine and Housing but peaks at 4 GB VRAM and cannot process the full 100k-row Higgs table. These results quantify the substantial hardware-versus-accuracy trade-offs in current tabular FMs and provide an open baseline for future efficiency-oriented research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Statistical Guarantees for Conformal Predictors with Small Datasets</title>
<link>https://arxiv.org/abs/2512.04566</link>
<guid>https://arxiv.org/abs/2512.04566</guid>
<content:encoded><![CDATA[
arXiv:2512.04566v2 Announce Type: replace 
Abstract: Surrogate models (including deep neural networks and other machine learning algorithms in supervised learning) are capable of approximating arbitrarily complex, high-dimensional input-output problems in science and engineering, but require a thorough data-agnostic uncertainty quantification analysis before these can be deployed for any safety-critical application. The standard approach for data-agnostic uncertainty quantification is to use conformal prediction (CP), a well-established framework to build uncertainty models with proven statistical guarantees that do not assume any shape for the error distribution of the surrogate model. However, since the classic statistical guarantee offered by CP is given in terms of bounds for the marginal coverage, for small calibration set sizes (which are frequent in realistic surrogate modelling that aims to quantify error at different regions), the potentially strong dispersion of the coverage distribution around its average negatively impacts the relevance of the uncertainty model's statistical guarantee, often obtaining coverages below the expected value, resulting in a less applicable framework. After providing a gentle presentation of uncertainty quantification for surrogate models for machine learning practitioners, in this paper we bridge the gap by proposing a new statistical guarantee that offers probabilistic information for the coverage of a single conformal predictor. We show that the proposed framework converges to the standard solution offered by CP for large calibration set sizes and, unlike the classic guarantee, still offers relevant information about the coverage of a conformal predictor for small data sizes. We validate the methodology in a suite of examples, and implement an open access software solution that can be used alongside common conformal prediction libraries to obtain uncertainty models that fulfil the new guarantee.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?</title>
<link>https://arxiv.org/abs/2512.05442</link>
<guid>https://arxiv.org/abs/2512.05442</guid>
<content:encoded><![CDATA[
arXiv:2512.05442v2 Announce Type: replace 
Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learnability Window in Gated Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2512.05790</link>
<guid>https://arxiv.org/abs/2512.05790</guid>
<content:encoded><![CDATA[
arXiv:2512.05790v2 Announce Type: replace 
Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \emph{effective learning rates} $\mu_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($\alpha$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ satisfies $N(\ell)\propto f(\ell)^{-\alpha}$, where $f(\ell)=\|\mu_{t,\ell}\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory shows that the time-scale spectra induced by the effective learning rates are the dominant determinants of learnability. Broader or more heterogeneous spectra slow the decay of $f(\ell)$, enlarging the learnability window, while heavy-tailed noise compresses $\mathcal{H}_N$ by limiting statistical concentration. By integrating gate-induced time-scale geometry with gradient noise and sample complexity, the framework identifies the effective learning rates as the primary objects that determine whether, when, and over what horizons recurrent networks can learn long-range temporal dependencies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Learning of Density Estimates with Topological Optimization</title>
<link>https://arxiv.org/abs/2512.08895</link>
<guid>https://arxiv.org/abs/2512.08895</guid>
<content:encoded><![CDATA[
arXiv:2512.08895v2 Announce Type: replace 
Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Reinforcement Learning for Building Energy Management System</title>
<link>https://arxiv.org/abs/2210.12590</link>
<guid>https://arxiv.org/abs/2210.12590</guid>
<content:encoded><![CDATA[
arXiv:2210.12590v3 Announce Type: replace-cross 
Abstract: The building sector is one of the largest contributors to global energy consumption. Improving its energy efficiency is essential for reducing operational costs and greenhouse gas emissions. Energy management systems (EMS) play a key role in monitoring and controlling building appliances efficiently and reliably. With the increasing integration of renewable energy, intelligent EMS solutions have received growing attention. Reinforcement learning (RL) has recently been explored for this purpose and shows strong potential. However, most RL-based EMS methods require a large number of training steps to learn effective control policies, especially when adapting to unseen buildings, which limits their practical deployment. This paper introduces MetaEMS, a meta-reinforcement learning framework for EMS. MetaEMS improves learning efficiency by transferring knowledge from previously solved tasks to new ones through group-level and building-level adaptation, enabling fast adaptation and effective control across diverse building environments. Experimental results demonstrate that MetaEMS adapts more rapidly to unseen buildings and consistently outperforms baseline methods across various scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Emotional Trajectories: A Temporal-Semantic Network Approach for Latent Depression Assessment in Social Media</title>
<link>https://arxiv.org/abs/2305.13127</link>
<guid>https://arxiv.org/abs/2305.13127</guid>
<content:encoded><![CDATA[
arXiv:2305.13127v3 Announce Type: replace-cross 
Abstract: The early identification and intervention of latent depression are of significant societal importance for mental health governance. While current automated detection methods based on social media have shown progress, their decision-making processes often lack a clinically interpretable framework, particularly in capturing the duration and dynamic evolution of depressive symptoms. To address this, this study introduces a semantic parsing network integrated with multi-scale temporal prototype learning. The model detects depressive states by capturing temporal patterns and semantic prototypes in users' emotional expression, providing a duration-aware interpretation of underlying symptoms. Validated on a large-scale social media dataset, the model outperforms existing state-of-the-art methods. Analytical results indicate that the model can identify emotional expression patterns not systematically documented in traditional survey-based approaches, such as sustained narratives expressing admiration for an "alternative life." Further user evaluation demonstrates the model's superior interpretability compared to baseline methods. This research contributes a structurally transparent, clinically aligned framework for depression detection in social media to the information systems literature. In practice, the model can generate dynamic emotional profiles for social platform users, assisting in the targeted allocation of mental health support resources.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General Formulation and PCL-Analysis for Restless Bandits with Limited Observability</title>
<link>https://arxiv.org/abs/2307.03034</link>
<guid>https://arxiv.org/abs/2307.03034</guid>
<content:encoded><![CDATA[
arXiv:2307.03034v4 Announce Type: replace-cross 
Abstract: In this paper, we consider a general observation model for restless multi-armed bandit problems. The operation of the player is based on the past observation history that is limited (partial) and error-prone due to resource constraints or environmental or intrinsic noises. By establishing a general probabilistic model for dynamics of the observation process, we formulate the problem as a restless bandit with an infinite high-dimensional belief state space. We apply the achievable region method with partial conservation law (PCL) to the infinite-state problem and analyze its indexability and priority index (Whittle index). Finally, we propose an approximation process to transform the problem into which the AG algorithm of Ni\~no-Mora (2001) for finite-state problems can be applied. Numerical experiments show that our algorithm has excellent performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models</title>
<link>https://arxiv.org/abs/2410.03026</link>
<guid>https://arxiv.org/abs/2410.03026</guid>
<content:encoded><![CDATA[
arXiv:2410.03026v3 Announce Type: replace-cross 
Abstract: Language models (LMs) rely on their parametric knowledge augmented with relevant contextual knowledge for certain tasks, such as question answering. However, the contextual knowledge can contain private information that may be leaked when answering queries, and estimating this privacy leakage is not well understood. A straightforward approach of directly comparing an LM's output to the contexts can overestimate the privacy risk, since the LM's parametric knowledge might already contain the augmented contextual knowledge. To this end, we introduce *context influence*, a metric that builds on differential privacy, a widely-adopted privacy notion, to estimate the privacy leakage of contextual knowledge during decoding. Our approach effectively measures how each subset of the context influences an LM's response while separating the specific parametric knowledge of the LM. Using our context influence metric, we demonstrate that context privacy leakage occurs when contextual knowledge is out of distribution with respect to parametric knowledge. Moreover, we experimentally demonstrate how context influence properly attributes the privacy leakage to augmented contexts, and we evaluate how factors -- such as model size, context size, generation position, etc. -- affect context privacy leakage. The practical implications of our results will inform practitioners of the privacy risk associated with augmented contextual knowledge.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QCircuitBench: A Large-Scale Dataset for Benchmarking Quantum Algorithm Design</title>
<link>https://arxiv.org/abs/2410.07961</link>
<guid>https://arxiv.org/abs/2410.07961</guid>
<content:encoded><![CDATA[
arXiv:2410.07961v3 Announce Type: replace-cross 
Abstract: Quantum computing is an emerging field recognized for the significant speedup it offers over classical computing through quantum algorithms. However, designing and implementing quantum algorithms pose challenges due to the complex nature of quantum mechanics and the necessity for precise control over quantum states. Despite the significant advancements in AI, there has been a lack of datasets specifically tailored for this purpose. In this work, we introduce QCircuitBench, the first benchmark dataset designed to evaluate AI's capability in designing and implementing quantum algorithms using quantum programming languages. Unlike using AI for writing traditional codes, this task is fundamentally more complicated due to highly flexible design space. Our key contributions include: 1. A general framework which formulates the key features of quantum algorithm design for Large Language Models. 2. Implementations for quantum algorithms from basic primitives to advanced applications, spanning 3 task suites, 25 algorithms, and 120,290 data points. 3. Automatic validation and verification functions, allowing for iterative evaluation and interactive reasoning without human inspection. 4. Promising potential as a training dataset through preliminary fine-tuning results. We observed several interesting experimental phenomena: LLMs tend to exhibit consistent error patterns, and fine-tuning does not always outperform few-shot learning. In all, QCircuitBench is a comprehensive benchmark for LLM-driven quantum algorithm design, and it reveals limitations of LLMs in this domain.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lipschitz spaces view of infinitely wide shallow neural networks</title>
<link>https://arxiv.org/abs/2410.14591</link>
<guid>https://arxiv.org/abs/2410.14591</guid>
<content:encoded><![CDATA[
arXiv:2410.14591v2 Announce Type: replace-cross 
Abstract: We revisit the mean field parametrization of shallow neural networks, using signed measures on unbounded parameter spaces and duality pairings that take into account the regularity and growth of activation functions. This setting directly leads to the use of unbalanced Kantorovich-Rubinstein norms defined by duality with Lipschitz functions, and of spaces of measures dual to those of continuous functions with controlled growth. These allow to make transparent the need for total variation and moment bounds or penalization to obtain existence of minimizers of variational formulations, under which we prove a compactness result in strong Kantorovich-Rubinstein norm, and in the absence of which we show several examples demonstrating undesirable behavior. Further, the Kantorovich-Rubinstein setting enables us to combine the advantages of a completely linear parametrization and ensuing reproducing kernel Banach space framework with optimal transport insights. We showcase this synergy with representer theorems and uniform large data limits for empirical risk minimization, and in proposed formulations for distillation and fusion applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holistic Utility Preference Learning for Listwise Alignment</title>
<link>https://arxiv.org/abs/2410.18127</link>
<guid>https://arxiv.org/abs/2410.18127</guid>
<content:encoded><![CDATA[
arXiv:2410.18127v2 Announce Type: replace-cross 
Abstract: Aligning large language models with human preferences is essential for improving interaction quality and safety by ensuring outputs better reflect human values. A promising strategy involves Reinforcement Learning from Human Feedback (RLHF), starting with collecting and ranking responses generated by a supervised fine-tuning model to refine alignment. Existing methods such as Direct Preference Optimization (DPO) focus on pairwise comparisons, categorizing responses into preferred and less preferred pairs and optimizing pairwise margins. However, this pairwise approach cannot capture the holistic ranking relationships among multiple responses or effectively leverage the rich preference information available in list-wise comparisons. To address this challenge, this paper introduces \underline{D}irect \underline{R}anking \underline{P}reference \underline{O}ptimization (DRPO), a novel method that views human preference alignment as a Learning-to-Rank (LTR) task. Unlike pairwise methods, DRPO optimizes the preference ranking of entire response lists by computing holistic utility scores through NDCG, a standard LTR metric. To enable end-to-end optimization with the non-differentiable NDCG, we propose diffNDCG loss, a differentiable approximation facilitated by a sorting network. Furthermore, we introduce a novel margin-based Adaptive Rank Policy Score to enhance the discriminative quality of generated responses. Extensive experiments have shown that DRPO outperforms existing methods, enhancing the quality of the generated responses.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On uniqueness in structured model learning</title>
<link>https://arxiv.org/abs/2410.22009</link>
<guid>https://arxiv.org/abs/2410.22009</guid>
<content:encoded><![CDATA[
arXiv:2410.22009v2 Announce Type: replace-cross 
Abstract: This paper addresses the problem of uniqueness in learning physical laws for systems of partial differential equations (PDEs). Contrary to most existing approaches, it considers a framework of structured model learning, where existing, approximately correct physical models are augmented with components that are learned from data. The main result of the paper is a uniqueness result that covers a large class of PDEs and a suitable class of neural networks used for approximating the unknown model components. The uniqueness result shows that, in the idealized setting of full, noiseless measurements, a unique identification of the unknown model components is possible as regularization-minimizing solution of the PDE system. Furthermore, the paper provides a convergence result showing that model components learned on the basis of incomplete, noisy measurements approximate the regularization-minimizing solution of the PDE system in the limit. These results are possible under specific properties of the approximating neural networks and due to a dedicated choice of regularization. With this, a practical contribution of this analytic paper is to provide a class of model learning frameworks different to standard settings where uniqueness can be expected in the limit of full measurements.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Renal Cell Carcinoma subtyping: learning from multi-resolution localization</title>
<link>https://arxiv.org/abs/2411.09471</link>
<guid>https://arxiv.org/abs/2411.09471</guid>
<content:encoded><![CDATA[
arXiv:2411.09471v2 Announce Type: replace-cross 
Abstract: Renal Cell Carcinoma is typically asymptomatic at the early stages for many patients. This leads to a late diagnosis of the tumor, where the curability likelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high, with respect to its incidence rate. To increase the survival chance, a fast and correct categorization of the tumor subtype is paramount. Nowadays, computerized methods, based on artificial intelligence, represent an interesting opportunity to improve the productivity and the objectivity of the microscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their exploitation is hampered by the paucity of annotated dataset, essential for a proficient training of supervised machine learning technologies. This study sets out to investigate a novel self supervised training strategy for machine learning diagnostic tools, based on the multi-resolution nature of the histological samples. We aim at reducing the need of annotated dataset, without significantly reducing the accuracy of the tool. We demonstrate the classification capability of our tool on a whole slide imaging dataset for Renal Cancer subtyping, and we compare our solution with several state-of-the-art classification counterparts.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-Optimal Algorithms for Omniprediction</title>
<link>https://arxiv.org/abs/2501.17205</link>
<guid>https://arxiv.org/abs/2501.17205</guid>
<content:encoded><![CDATA[
arXiv:2501.17205v3 Announce Type: replace-cross 
Abstract: Omnipredictors are simple prediction functions that encode loss-minimizing predictions with respect to a hypothesis class $H$, simultaneously for every loss function within a class of losses $L$. In this work, we give near-optimal learning algorithms for omniprediction, in both the online and offline settings. To begin, we give an oracle-efficient online learning algorithm that acheives $(L,H)$-omniprediction with $\tilde O (\sqrt{T \log |H|})$ regret for any class of Lipschitz loss functions $L \subseteq L_\mathrm{Lip}$. Quite surprisingly, this regret bound matches the optimal regret for \emph{minimization of a single loss function} (up to a $\sqrt{\log(T)}$ factor). Given this online algorithm, we develop an online-to-offline conversion that achieves near-optimal complexity across a number of measures. In particular, for all bounded loss functions within the class of Bounded Variation losses $L_\mathrm{BV}$ (which include all convex, all Lipschitz, and all proper losses) and any (possibly-infinite) $H$, we obtain an offline learning algorithm that, leveraging an (offline) ERM oracle and $m$ samples from $D$, returns an efficient $(L_{\mathrm{BV}},H,\epsilon(m))$-omnipredictor for $\varepsilon(m)$ scaling near-linearly in the Rademacher complexity of a class derived from $H$ by taking convex combinations of a fixed number of elements of $\mathrm{Th} \circ H$.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero</title>
<link>https://arxiv.org/abs/2505.21536</link>
<guid>https://arxiv.org/abs/2505.21536</guid>
<content:encoded><![CDATA[
arXiv:2505.21536v2 Announce Type: replace-cross 
Abstract: The demand of finite raw materials will keep increasing as they fuel modern society. Simultaneously, solutions for stopping carbon emissions in the short term are not available, thus making the net zero target extremely challenging to achieve at scale. The circular economy (CE) paradigm is gaining attention as a solution to address climate change and the uncertainties of supplies of critical materials. Hence, in this paper, we introduce CiRL, a deep reinforcement learning (DRL) library of environments focused on the circularity control of both solid and fluid materials. The integration of DRL into the design of material circularity is possible thanks to the formalism of thermodynamical material networks, which is underpinned by compartmental dynamical thermodynamics. Along with the focus on circularity, this library has three more features: the new CE-oriented environments are in the state-space form, which is typically used in dynamical systems analysis and control design; it is based on a state-of-the-art Python library of DRL algorithms, namely, Stable-Baselines3; and it is developed in Google Colaboratory to be accessible to researchers from different disciplines and backgrounds as is often the case for circular economy researchers and engineers. CiRL is intended to be a tool to generate AI-driven actions for optimizing the circularity of supply-recovery chains and to be combined with human-driven decisions derived from material flow analysis (MFA) studies. CiRL is publicly available.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.22090</link>
<guid>https://arxiv.org/abs/2505.22090</guid>
<content:encoded><![CDATA[
arXiv:2505.22090v2 Announce Type: replace-cross 
Abstract: Three-dimensional ultrasound enables real-time volumetric visualization of anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces reliance on precise probe orientation, potentially making ultrasound more accessible to clinicians with varying levels of experience and improving automated measurements and post-exam analysis. However, achieving both high volume rates and high image quality remains a significant challenge. While 3D diverging waves can provide high volume rates, they suffer from limited tissue harmonic generation and increased multipath effects, which degrade image quality. One compromise is to retain focus in elevation while leveraging unfocused diverging waves in the lateral direction to reduce the number of transmissions per elevation plane. Reaching the volume rates achieved by full 3D diverging waves, however, requires dramatically undersampling the number of elevation planes. Subsequently, to render the full volume, simple interpolation techniques are applied. This paper introduces a novel approach to 3D ultrasound reconstruction from a reduced set of elevation planes by employing diffusion models (DMs) to achieve increased spatial and temporal resolution. We compare both traditional and supervised deep learning-based interpolation methods on a 3D cardiac ultrasound dataset. Our results show that DM-based reconstruction consistently outperforms the baselines in image quality and downstream task performance. Additionally, we accelerate inference by leveraging the temporal consistency inherent to ultrasound sequences. Finally, we explore the robustness of the proposed method by exploiting the probabilistic nature of diffusion posterior sampling to quantify reconstruction uncertainty and demonstrate improved recall on out-of-distribution data with synthetic anomalies under strong subsampling.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIBE: Can a VLM Read the Room?</title>
<link>https://arxiv.org/abs/2506.11162</link>
<guid>https://arxiv.org/abs/2506.11162</guid>
<content:encoded><![CDATA[
arXiv:2506.11162v2 Announce Type: replace-cross 
Abstract: Understanding human social behavior such as recognizing emotions and the social dynamics causing them is an important and challenging problem. While LLMs have made remarkable advances, they are limited to the textual domain and cannot account for the major role that non-verbal cues play in understanding social situations. Vision Language Models (VLMs) can potentially account for this gap, however their ability to make correct inferences over such social cues has received little attention. In this paper, we explore the capabilities of VLMs at social reasoning. We identify a previously overlooked limitation in VLMs: the Visual Social-Pragmatic Inference gap. To target this gap, we propose a new task for VLMs: Visual Social-Pragmatic Inference. We construct a high quality dataset to test the abilities of a VLM for this task and benchmark the performance of several VLMs on it.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2506.14831</link>
<guid>https://arxiv.org/abs/2506.14831</guid>
<content:encoded><![CDATA[
arXiv:2506.14831v2 Announce Type: replace-cross 
Abstract: With the emergence of powerful data-driven methods in human trajectory prediction (HTP), gaining a finer understanding of multi-agent interactions lies within hand's reach, with important implications in areas such as social robot navigation, autonomous navigation, and crowd modeling. This survey reviews some of the most recent advancements in deep learning-based multi-agent trajectory prediction, focusing on studies published between 2020 and 2025. We categorize the existing methods based on their architectural design, their input representations, and their overall prediction strategies, placing a particular emphasis on models evaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges and future research directions in the field of multi-agent HTP.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q2SAR: A Quantum Multiple Kernel Learning Approach for Drug Discovery</title>
<link>https://arxiv.org/abs/2506.14920</link>
<guid>https://arxiv.org/abs/2506.14920</guid>
<content:encoded><![CDATA[
arXiv:2506.14920v4 Announce Type: replace-cross 
Abstract: Quantitative Structure-Activity Relationship (QSAR) modeling is a cornerstone of computational drug discovery. This research demonstrates the successful application of a Quantum Multiple Kernel Learning (QMKL) framework to enhance QSAR classification, showing a notable performance improvement over classical methods. We apply this methodology to a dataset for identifying DYRK1A kinase inhibitors. The workflow involves converting SMILES representations into numerical molecular descriptors, reducing dimensionality via Principal Component Analysis (PCA), and employing a Support Vector Machine (SVM) trained on an optimized combination of multiple quantum and classical kernels. By benchmarking the QMKL-SVM against a classical Gradient Boosting model, we show that the quantum-enhanced approach achieves a superior AUC score, highlighting its potential to provide a quantum advantage in challenging cheminformatics classification tasks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization performance of narrow one-hidden layer networks in the teacher-student setting</title>
<link>https://arxiv.org/abs/2507.00629</link>
<guid>https://arxiv.org/abs/2507.00629</guid>
<content:encoded><![CDATA[
arXiv:2507.00629v3 Announce Type: replace-cross 
Abstract: Understanding the generalization abilities of neural networks for simple input-output distributions is crucial to account for their learning performance on real datasets. The classical teacher-student setting, where a network is trained from data obtained thanks to a label-generating teacher model, serves as a perfect theoretical test bed. In this context, a complete theoretical account of the performance of fully connected one-hidden layer networks in the presence of generic activation functions is lacking. In this work, we develop such a general theory for narrow networks, i.e. with a large number of hidden units, yet much smaller than the input dimension. Using methods from statistical physics, we provide closed-form expressions for the typical performance of both finite temperature (Bayesian) and empirical risk minimization estimators, in terms of a small number of summary statistics. In doing so, we highlight the presence of a transition where hidden neurons specialize when the number of samples is sufficiently large and proportional to the number of parameters of the network. Our theory accurately predicts the generalization error of neural networks trained on regression or classification tasks with either noisy full-batch gradient descent (Langevin dynamics) or full-batch gradient descent.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Property-Isometric Variational Autoencoders for Sequence Modeling and Design</title>
<link>https://arxiv.org/abs/2509.14287</link>
<guid>https://arxiv.org/abs/2509.14287</guid>
<content:encoded><![CDATA[
arXiv:2509.14287v2 Announce Type: replace-cross 
Abstract: Biological sequence design (DNA, RNA, or peptides) with desired functional properties has applications in discovering novel nanomaterials, biosensors, antimicrobial drugs, and beyond. One common challenge is the ability to optimize complex high-dimensional properties such as target emission spectra of DNA-mediated fluorescent nanoparticles, photo and chemical stability, and antimicrobial activity of peptides across target microbes. Existing models rely on simple binary labels (e.g., binding/non-binding) rather than high-dimensional complex properties. To address this gap, we propose a geometry-preserving variational autoencoder framework, called PrIVAE, which learns latent sequence embeddings that respect the geometry of their property space. Specifically, we model the property space as a high-dimensional manifold that can be locally approximated by a nearest neighbor graph, given an appropriately defined distance measure. We employ the property graph to guide the sequence latent representations using (1) graph neural network encoder layers and (2) an isometric regularizer. PrIVAE learns a property-organized latent space that enables rational design of new sequences with desired properties by employing the trained decoder. We evaluate the utility of our framework for two generative tasks: (1) design of DNA sequences that template fluorescent metal nanoclusters and (2) design of antimicrobial peptides. The trained models retain high reconstruction accuracy while organizing the latent space according to properties. Beyond in silico experiments, we also employ sampled sequences for wet lab design of DNA nanoclusters, resulting in up to 16.1-fold enrichment of rare-property nanoclusters compared to their abundance in training data, demonstrating the practical utility of our framework.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Ego-graph Inference for Networked Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.16606</link>
<guid>https://arxiv.org/abs/2509.16606</guid>
<content:encoded><![CDATA[
arXiv:2509.16606v3 Announce Type: replace-cross 
Abstract: In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. While centralized frameworks can learn dynamic graphs, their reliance on global state access and centralized infrastructure is impractical in real-world decentralized systems. We propose a stochastic graph-based policy for Networked-MARL, where each agent conditions its decision on a sampled subgraph over its local physical neighborhood. Building on this formulation, we introduce BayesG, a decentralized actor-framework that learns sparse, context-aware interaction structures via Bayesian variational inference. Each agent operates over an ego-graph and samples a latent communication mask to guide message passing and policy computation. The variational distribution is trained end-to-end alongside the policy using an evidence lower bound (ELBO) objective, enabling agents to jointly learn both interaction topology and decision-making strategies. BayesG outperforms strong MARL baselines on large-scale traffic control tasks with up to 167 agents, demonstrating superior scalability, efficiency, and performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem</title>
<link>https://arxiv.org/abs/2509.18054</link>
<guid>https://arxiv.org/abs/2509.18054</guid>
<content:encoded><![CDATA[
arXiv:2509.18054v2 Announce Type: replace-cross 
Abstract: Selecting a solution algorithm for the Facility Layout Problem (FLP), an NP-hard optimization problem with multiobjective trade-off, is a complex task that requires deep expert knowledge. The performance of a given algorithm depends on the specific characteristics of the problem, such as the number of facilities, objectives, and constraints. This creates a need for a data-driven recommendation method to guide algorithm selection in automated design systems. This paper introduces a new recommendation method to make this expertise accessible, based on a Knowledge Graph-Based Retrieval-Augmented Generation (KG-RAG) framework. In this framework, a domain-specific knowledge graph (KG) is constructed from the literature. The method then employs a multifaceted retrieval mechanism to gather relevant evidence from this KG using three distinct approaches: precise graph-based search, flexible vector-based search, and cluster-based high-level search. The retrieved evidence is utilized by a Large Language Model (LLM) to generate algorithm recommendations based on data-driven reasoning. This KG-RAG framework is tested on a use case consisting of six problems comprising of complex multi-objective and multi-constraint FLP case. The results are compared with the Gemini 1.5 Flash chatbot. The results show that KG-RAG achieves an average reasoning score of 4.7 out of 5 compared to 3.3 for the baseline chatbot.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models</title>
<link>https://arxiv.org/abs/2509.22284</link>
<guid>https://arxiv.org/abs/2509.22284</guid>
<content:encoded><![CDATA[
arXiv:2509.22284v3 Announce Type: replace-cross 
Abstract: Modern state-space models (SSMs) often utilize transition matrices which enable efficient computation but pose restrictions on the model's expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix as the product of a column one-hot matrix ($P$) and a complex-valued diagonal matrix ($D$). Consequently, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout of size $N \times N$, significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multiclass time-series classification, the performance is comparable to that of neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded as a set of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defining latent spaces by example: optimisation over the outputs of generative models</title>
<link>https://arxiv.org/abs/2509.23800</link>
<guid>https://arxiv.org/abs/2509.23800</guid>
<content:encoded><![CDATA[
arXiv:2509.23800v2 Announce Type: replace-cross 
Abstract: Modern generative AI models like diffusion and flow matching can sample from rich data distributions, but many downstream tasks - such as experimental design or creative content generation - require a higher level of control than unconstrained sampling. Here, the challenge is to efficiently identify outputs that are both probable under the model and satisfy task-specific constraints. Often, the evaluation of samples is expensive and lack gradients - a setting known as black-box optimisation. In this work, we allow black-box optimisation on top of diffusion and flow matching models for the first time by introducing surrogate latent spaces: non-parametric, low-dimensional Euclidean embeddings that can be extracted from any generative model without additional training. The axes can be defined via examples, providing a simple and interpretable approach to define custom latent spaces that express intended features and is convenient to use in downstream tasks. Our proposed representation is Euclidean and has controllable dimensionality, permitting direct application of standard optimisation algorithms. We demonstrate that our approach is architecture-agnostic, incurs almost no additional computational cost over standard generation, and generalises across modalities, including images, audio, videos, and structured objects like proteins.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curiosity-Driven Development of Action and Language in Robots Through Self-Exploration</title>
<link>https://arxiv.org/abs/2510.05013</link>
<guid>https://arxiv.org/abs/2510.05013</guid>
<content:encoded><![CDATA[
arXiv:2510.05013v5 Announce Type: replace-cross 
Abstract: Infants acquire language with generalization from minimal experience, whereas large language models require billions of training tokens. What underlies efficient development in humans? We investigated this problem through experiments wherein robotic agents learn to perform actions associated with imperative sentences (e.g., push red cube) via curiosity-driven self-exploration. Our approach integrates active inference with reinforcement learning, enabling intrinsically motivated developmental learning. The simulations reveal key findings corresponding to observations in developmental psychology. i) Generalization improves drastically as the scale of compositional elements increases. ii) Curiosity improves learning through self-exploration. iii) Rote pairing of sentences and actions precedes compositional generalization. iv) Simpler actions develop before complex actions depending on them. v) Exception-handling induces U-shaped developmental performance, a pattern like representational redescription in child language learning. These results suggest that curiosity-driven active inference accounts for how intrinsically motivated sensorimotor-linguistic learning supports scalable compositional generalization and exception handling in humans and artificial agents.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication</title>
<link>https://arxiv.org/abs/2511.09557</link>
<guid>https://arxiv.org/abs/2511.09557</guid>
<content:encoded><![CDATA[
arXiv:2511.09557v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.09605</link>
<guid>https://arxiv.org/abs/2511.09605</guid>
<content:encoded><![CDATA[
arXiv:2511.09605v3 Announce Type: replace-cross 
Abstract: The sharp rise in medical tomography examinations has created a demand for automated systems that can reliably extract informative features for downstream tasks such as tumor characterization. Although 3D volumes contain richer information than individual slices, effective 3D classification remains difficult: volumetric data encode complex spatial dependencies, and the scarcity of large-scale 3D datasets has constrained progress toward 3D foundation models. As a result, many recent approaches rely on 2D vision foundation models trained on natural images, repurposing them as feature extractors for medical scans with surprisingly strong performance. Despite their practical success, current methods that apply 2D foundation models to 3D scans via slice-based decomposition remain fundamentally limited. Standard slicing along axial, sagittal, and coronal planes often fails to capture the true spatial extent of a structure when its orientation does not align with these canonical views. More critically, most approaches aggregate slice features independently, ignoring the underlying 3D geometry and losing spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. Instead of restricting the model to axial, sagittal, or coronal planes, our method samples both canonical and non-canonical cross-sections generated from uniformly distributed points on a sphere enclosing the volume. We publicly share our accessible code base at http://github.com/compai-lab/2025-MedIA-kiechle and provide a user-friendly library for omnidirectional volume slicing at https://pypi.org/project/OmniSlicer.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</title>
<link>https://arxiv.org/abs/2511.15464</link>
<guid>https://arxiv.org/abs/2511.15464</guid>
<content:encoded><![CDATA[
arXiv:2511.15464v3 Announce Type: replace-cross 
Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration</title>
<link>https://arxiv.org/abs/2511.17123</link>
<guid>https://arxiv.org/abs/2511.17123</guid>
<content:encoded><![CDATA[
arXiv:2511.17123v3 Announce Type: replace-cross 
Abstract: Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies, which limits their effectiveness on real hardware. We propose an energy aware, layer-wise compression framework that explicitly leverages MAC and layer level energy characteristics. First, we build a layer-aware MAC energy model that combines per-layer activation statistics with an MSB-Hamming distance grouping of 22-bit partial sum transitions, and integrate it with a tile-level systolic mapping to estimate convolution-layer energy. On top of this model, we introduce an energy accuracy co-optimized weight selection algorithm within quantization aware training and an energy-prioritized layer-wise schedule that compresses high energy layers more aggressively under a global accuracy constraint. Experiments on different CNN models demonstrate up to 58.6\% energy reduction with 2-3\% accuracy drop, outperforming a state-of-the-art power-aware baseline.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees</title>
<link>https://arxiv.org/abs/2512.00204</link>
<guid>https://arxiv.org/abs/2512.00204</guid>
<content:encoded><![CDATA[
arXiv:2512.00204v2 Announce Type: replace-cross 
Abstract: In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Robust and Correct Controllers Guided by Feasibility-Aware Signal Temporal Logic via BarrierNet</title>
<link>https://arxiv.org/abs/2512.06973</link>
<guid>https://arxiv.org/abs/2512.06973</guid>
<content:encoded><![CDATA[
arXiv:2512.06973v2 Announce Type: replace-cross 
Abstract: Control Barrier Functions (CBFs) have emerged as a powerful tool for enforcing safety in optimization-based controllers, and their integration with Signal Temporal Logic (STL) has enabled the specification-driven synthesis of complex robotic behaviors. However, existing CBF-STL approaches typically rely on fixed hyperparameters and myopic, per-time step optimization, which can lead to overly conservative behavior, infeasibility near tight input limits, and difficulty satisfying long-horizon STL tasks. To address these limitations, we propose a feasibility-aware learning framework that embeds trainable, time-varying High Order Control Barrier Functions (HOCBFs) into a differentiable Quadratic Program (dQP). Our approach provides a systematic procedure for constructing time-varying HOCBF constraints for a broad fragment of STL and introduces a unified robustness measure that jointly captures STL satisfaction, QP feasibility, and control-bound compliance. Three neural networks-InitNet, RefNet, and an extended BarrierNet-collaborate to generate reference inputs and adapt constraint-related hyperparameters automatically over time and across initial conditions, reducing conservativeness while maximizing robustness. The resulting controller achieves STL satisfaction with strictly feasible dQPs and requires no manual tuning. Simulation results demonstrate that the proposed framework maintains high STL robustness under tight input bounds and significantly outperforms fixed-parameter and non-adaptive baselines in complex environments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.07342</link>
<guid>https://arxiv.org/abs/2512.07342</guid>
<content:encoded><![CDATA[
arXiv:2512.07342v2 Announce Type: replace-cross 
Abstract: Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.
  To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory</title>
<link>https://arxiv.org/abs/2511.23083</link>
<guid>https://arxiv.org/abs/2511.23083</guid>
<content:encoded><![CDATA[
<div> Keywords: Kernel Hopfield networks, Ridge of Optimization, Fisher Information Matrix, Dual Equilibrium, self-organized criticality<br /><br />Summary:<br /><br />1. This paper investigates high-capacity kernel Hopfield networks, focusing on a phenomenon called the Ridge of Optimization, which is linked to extreme stability in the networks’ performance.<br /><br />2. Previous interpretations connected the Ridge of Optimization to Spectral Concentration, but the underlying cause remained unclear.<br /><br />3. By analyzing the network dynamics on a statistical manifold, the authors reveal that the Ridge actually corresponds to the Edge of Stability, a critical boundary where the Fisher Information Matrix becomes singular, indicating a phase transition in learning dynamics.<br /><br />4. The study finds that the seemingly contradictory Euclidean force antagonism observed in these networks can be understood as a Dual Equilibrium within the Riemannian geometric framework.<br /><br />5. This insight unifies the principles of learning dynamics and network capacity through the Minimum Description Length principle, leading to a comprehensive geometric theory that explains self-organized criticality in kernel Hopfield networks. <div>
arXiv:2511.23083v3 Announce Type: replace 
Abstract: High-capacity kernel Hopfield networks exhibit a \textit{Ridge of Optimization} characterized by extreme stability. While previously linked to \textit{Spectral Concentration}, its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the Edge of Stability, a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent</title>
<link>https://arxiv.org/abs/2512.07490</link>
<guid>https://arxiv.org/abs/2512.07490</guid>
<content:encoded><![CDATA[
<div> low-tubal-rank tensor estimation, tensor factorization, alternating preconditioned gradient descent, over-parameterization, linear convergence<br /><br />Summary:<br /><br />1. The paper addresses the challenge of low-tubal-rank tensor estimation, a key problem with applications in signal processing, machine learning, and image science.<br />2. Conventional tensor singular value decomposition methods are computationally costly and unsuitable for large-scale tensors.<br />3. Existing gradient descent based factorization approaches require precise rank estimates, and overestimating the rank leads to slow or divergent convergence.<br />4. The authors propose an Alternating Preconditioned Gradient Descent (APGD) algorithm that improves convergence in over-parameterized settings by adding a preconditioning term and alternating updates of factor tensors.<br />5. Theoretical analysis shows APGD achieves linear convergence under certain geometric assumptions, and notably, its convergence rate is independent of the tensor condition number.<br />6. The study includes specific cases of low-tubal-rank tensor factorization and recovery, extending the applicability of the theoretical results.<br />7. Extensive simulations on synthetic data validate the efficiency and convergence guarantees of APGD in practical scenarios. <div>
arXiv:2512.07490v2 Announce Type: replace 
Abstract: The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SA$^{2}$GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation</title>
<link>https://arxiv.org/abs/2512.07857</link>
<guid>https://arxiv.org/abs/2512.07857</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Foundation Models, hierarchical structural semantics, domain adaptation, Information Bottleneck, mixture-of-experts

<br /><br />Summary:  
This paper addresses the robustness challenges of Graph Foundation Models (GFMs) against domain noise, structural perturbations, and adversarial attacks by proposing a novel framework called SA$^{2}$GFM. First, the authors identify a key limitation in existing GFMs related to the insufficient modeling of hierarchical structural semantics, which are essential for effective generalization across domains. To overcome this, SA$^{2}$GFM encodes hierarchical structural priors by converting entropy-based encoding trees into structure-aware textual prompts, thereby augmenting feature inputs. These enhanced inputs are then processed by a self-supervised Information Bottleneck mechanism, aiming to compress and distill robust and transferable representations guided by the graph’s structure. To mitigate negative transfer problems encountered during cross-domain adaptation, the framework integrates an expert adaptive routing mechanism combining a mixture-of-experts model with a null expert design. Moreover, for more efficient downstream adaptation, a fine-tuning module is introduced that optimizes hierarchical structures through joint learning of both intra-community and inter-community graph relations. Extensive experiments conducted demonstrate that SA$^{2}$GFM consistently outperforms nine state-of-the-art baseline models in both effectiveness and robustness for node and graph classification tasks, showing resilience against random noise and adversarial perturbations. <div>
arXiv:2512.07857v2 Announce Type: replace 
Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA$^{2}$GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA$^{2}$GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Contrastive Learning via Spectral Graph Alignment</title>
<link>https://arxiv.org/abs/2512.07878</link>
<guid>https://arxiv.org/abs/2512.07878</guid>
<content:encoded><![CDATA[
<div> Keywords: contrastive learning, graph embeddings, normalized Laplacians, SpecMatch-CL, unsupervised learning<br /><br />Summary:<br /><br />This paper addresses the limitation in traditional contrastive learning methods (such as InfoNCE) that focus on pairwise alignment of graph embeddings across augmented views but do not control the global structure of the graph-of-graphs formed from these embeddings. To tackle this, the authors propose SpecMatch-CL, a novel loss function that aligns the view-specific graph-of-graphs by minimizing the difference between their normalized Laplacian matrices. Theoretically, they prove that the difference between normalized Laplacians serves as an upper bound for the difference between both the ideal Perfect Alignment contrastive loss and the Uniformly loss under certain assumptions, connecting spectral graph theory with contrastive learning objectives. Empirically, SpecMatch-CL achieves new state-of-the-art performance on eight common TU graph classification benchmarks under both unsupervised and semi-supervised settings, particularly excelling in low-label scenarios. Furthermore, the method demonstrates consistent improvements in transfer learning tasks on large-scale datasets such as PPI-306K and ZINC 2M, showing its generalization capability. Overall, the study introduces a theoretically grounded and practically effective approach to improve contrastive learning for graph representation by explicitly aligning global graph structures across views. <div>
arXiv:2512.07878v2 Announce Type: replace 
Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>De novo generation of functional terpene synthases using TpsGPT</title>
<link>https://arxiv.org/abs/2512.08772</link>
<guid>https://arxiv.org/abs/2512.08772</guid>
<content:encoded><![CDATA[
<div> Keywords: Terpene synthases, protein language model, de novo design, enzyme validation, directed evolution<br /><br />Summary:<br /><br />1. Terpene synthases (TPS) are crucial enzymes responsible for producing diverse terpene scaffolds, which form the basis of many natural products, including important anticancer drugs like Taxol. <br />2. Traditional de novo TPS design via directed evolution is costly and time-consuming, motivating alternative computational design approaches. <br />3. The authors developed TpsGPT, a generative model for scalable TPS protein design by fine-tuning the ProtGPT2 protein language model on a dataset of 79,000 TPS sequences extracted from UniProt. <br />4. TpsGPT generated 28,000 new candidate TPS enzyme sequences which were subjected to multiple in silico validation metrics, including EnzymeExplorer classification, structural confidence scoring with ESMFold (pLDDT), sequence diversity analysis, CLEAN classification, InterPro domain detection, and structural alignment using Foldseek. <br />5. Seven putative TPS enzymes that met all validation criteria were identified, and subsequent experimental testing confirmed TPS enzymatic activity in at least two of these candidates. <br /><br />This study demonstrates that fine-tuning a protein language model on a large, enzyme-specific dataset coupled with rigorous computational filtering can effectively yield novel, functional, and evolutionarily distinct TPS enzymes, providing a promising avenue for accelerating enzyme design. <div>
arXiv:2512.08772v2 Announce Type: replace 
Abstract: Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</title>
<link>https://arxiv.org/abs/2509.25531</link>
<guid>https://arxiv.org/abs/2509.25531</guid>
<content:encoded><![CDATA[
<div> MixtureVitae, permissive licensing, pretrained corpus, instruction tuning, risk mitigation  

<br /><br />Summary:  
The paper introduces MixtureVitae, an open-access pretraining corpus designed to minimize legal risks while delivering strong downstream model performance. MixtureVitae employs a permissive-first, risk-mitigated data sourcing strategy combining public-domain and permissively licensed texts (e.g., CC-BY, Apache) with carefully justified low-risk sources such as government works and EU TDM-eligible content. The corpus integrates a large proportion of synthetic instruction and reasoning data signals during the initial pretraining stage, which are typically introduced during post-training and rare in permissively licensed web corpora. Sources are categorized into a three-tier risk scheme, with shard-level provenance metadata provided to enable risk-aware usage. Controlled experiments with fixed architectures and hyperparameters show that models pretrained on MixtureVitae outperform those trained on other permissive datasets across standard benchmarks. Notably, at a 1.7B parameter scale with 300B tokens, the model matches or exceeds instruction-tuned baselines on challenging math and code benchmarks like GSM8K, HumanEval, and MBPP, using significantly fewer tokens (~300B versus ~11T). The study’s decontamination analysis supports the corpus’s effectiveness as a practical, risk-mitigated foundation for training capable large language models, reducing dependency on broad, unfiltered web scrapes without compromising competitiveness. The code and data are publicly available. <div>
arXiv:2509.25531v4 Announce Type: replace-cross 
Abstract: We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong downstream performance. MixtureVitae follows a permissive-first, risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources). MixtureVitae adopts a simple, single-stage pretraining recipe that integrates a large proportion of permissive synthetic instruction and reasoning data-signals typically introduced during post-training and generally scarce in permissive web corpora. We categorize all sources into a three-tier scheme that reflects varying risk levels and provide shard-level provenance metadata to enable risk-aware usage. In controlled experiments using the open-sci-ref training protocol (fixed architectures and hyperparameters; 50B and 300B token budgets across 130M-1.7B parameters), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B-parameters/300B-tokens setting, they surpass FineWeb-Edu and approach DCLM late in training. Performance is particularly strong on MMLU and on math and code benchmarks: a 1.7B model pretrained on 300B MixtureVitae tokens matches or exceeds a strong 1.7B instruction-tuned baseline on GSM8K, HumanEval, and MBPP, despite using over 36 times fewer tokens (300B vs. ~11T). Supported by a thorough decontamination analysis, these results show that permissive-first data with high instruction and reasoning density, tiered by licensing and provenance-related risk, can provide a practical and risk-mitigated foundation for training capable LLMs, reducing reliance on broad web scrapes without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces</title>
<link>https://arxiv.org/abs/2510.18109</link>
<guid>https://arxiv.org/abs/2510.18109</guid>
<content:encoded><![CDATA[
<div> Keywords: privacy-preserving, data evaluation, secure computation, machine learning, blockchain<br /><br />Summary: Evaluating the usefulness of data prior to purchase is critical for building high-quality machine learning models, but proprietary concerns often prevent model builders and data providers from fully sharing their assets. PrivaDE addresses this challenge by introducing a privacy-preserving protocol that enables a model owner and a data owner to jointly compute a utility score for a candidate dataset without exposing sensitive information such as model parameters, features, or labels. The protocol is designed to provide strong security guarantees against malicious parties. Furthermore, PrivaDE can be integrated into blockchain-based marketplaces, leveraging smart contracts to ensure fair execution and payment. To enhance practicality, the authors propose optimizations that enable efficient secure model inference and introduce a model-agnostic scoring method that uses only a small, representative subset of the data while accurately reflecting its impact on model training. Extensive evaluation demonstrates that PrivaDE effectively performs data evaluation tasks with online runtimes under 15 minutes even for models containing millions of parameters. Overall, this work establishes a foundation for fair, privacy-conscious, and automated data marketplaces within decentralized machine learning ecosystems, fostering collaboration without compromising proprietary assets or security. <div>
arXiv:2510.18109v3 Announce Type: replace-cross 
Abstract: Evaluating the usefulness of data before purchase is essential when obtaining data for high-quality machine learning models, yet both model builders and data providers are often unwilling to reveal their proprietary assets.
  We present PrivaDE, a privacy-preserving protocol that allows a model owner and a data owner to jointly compute a utility score for a candidate dataset without fully exposing model parameters, raw features, or labels. PrivaDE provides strong security against malicious behavior and can be integrated into blockchain-based marketplaces, where smart contracts enforce fair execution and payment. To make the protocol practical, we propose optimizations to enable efficient secure model inference, and a model-agnostic scoring method that uses only a small, representative subset of the data while still reflecting its impact on downstream training. Evaluation shows that PrivaDE performs data evaluation effectively, achieving online runtimes within 15 minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in decentralized machine learning ecosystems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparsePixels: Efficient Convolution for Sparse Data on FPGAs</title>
<link>https://arxiv.org/abs/2512.06208</link>
<guid>https://arxiv.org/abs/2512.06208</guid>
<content:encoded><![CDATA[
<div> SparsePixels, FPGA, sparse convolution, CNN inference, neutrino detection<br /><br />Summary:<br /><br />1. The paper addresses the challenge of high latency and long initiation intervals in standard convolutional neural network (CNN) inference on FPGAs, caused by the dense computation over every input pixel regardless of its feature value.  
2. It identifies that many real-world image datasets, like those from Liquid Argon Time Projection Chambers (LArTPC) used in neutrino experiments, have spatially sparse features where semantic information is confined to a small subset of pixels.  
3. The authors propose SparsePixels, a novel framework that implements sparse convolution on FPGAs by selectively identifying and computing only on a small subset of active pixels, ignoring empty or irrelevant regions.  
4. Experimental results demonstrate that for LArTPC images containing 4,000 pixels, SparsePixels achieves a remarkable 73x speedup in inference latency, reducing it from 48.665 µs to 0.665 µs, while maintaining resource usage within FPGA on-chip budgets and incurring only a minimal, percent-level accuracy loss.  
5. This framework holds promise for accelerating data readout processes in advanced scientific experiments, particularly those requiring inference latencies in the microsecond range or below, enabling more efficient and faster decision-making in real-time applications. <div>
arXiv:2512.06208v2 Announce Type: replace-cross 
Abstract: Inference of standard convolutional neural networks (CNNs) on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value. However, input features can be spatially sparse in some image data, where semantic information may occupy only a small fraction of the pixels and most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework that implements sparse convolution on FPGAs by selectively retaining and computing on a small subset of active pixels while ignoring the rest. We show that, for identifying neutrino interactions in naturally sparse LArTPC images with 4k pixels, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $\mu$s on an FPGA, whereas a sparse CNN of the same base architecture, computing on less than 1% of the input pixels, achieves a $\times 73$ speedup to 0.665 $\mu$s with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. This work aims to benefit future algorithm development for efficient data readout in modern experiments with latency requirements of microseconds or below.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Turkiye</title>
<link>https://arxiv.org/abs/2512.08340</link>
<guid>https://arxiv.org/abs/2512.08340</guid>
<content:encoded><![CDATA[
<div> California Bearing Ratio, machine learning, random forest, soil properties, geotechnical engineering<br /><br />Summary:<br /><br />The California Bearing Ratio (CBR) is an essential metric in geotechnical engineering for evaluating soil load-bearing capacity, crucial in transportation infrastructure and foundation design. Traditional determination of CBR relies on laboratory penetration tests, which, while accurate, tend to be time-consuming, expensive, and less practical for large-scale or variable soil conditions. This study proposes a comprehensive machine learning (ML) framework to predict CBR efficiently and effectively. Utilizing a dataset of 382 soil samples from diverse geoclimatic regions in Türkiye, the research incorporates multiple physicochemical soil properties to represent features in a supervised learning setup. Twelve different ML algorithms, including decision trees, random forests, gradient boosting, support vector regression, and stacking regressors, were trained, validated, and tested to evaluate their predictive performance and robustness. Among all tested methods, the random forest regressor demonstrated superior accuracy, achieving R² scores of 0.95 on training, 0.76 on validation, and 0.83 on testing datasets. The strong nonlinear mapping ability of the model makes it a powerful and promising tool for predictive tasks in geotechnics. Ultimately, the study advocates for integrating intelligent, data-driven models to complement or potentially replace traditional methods, facilitating digital transformation in geotechnical infrastructure analysis and design. <div>
arXiv:2512.08340v2 Announce Type: replace-cross 
Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in T\"urkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astra: General Interactive World Model with Autoregressive Denoising</title>
<link>https://arxiv.org/abs/2512.08931</link>
<guid>https://arxiv.org/abs/2512.08931</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion transformers, world models, long-horizon prediction, action-aware adapter, video generation<br /><br />Summary: Recent progress in diffusion transformers has improved video generation from textual or image inputs; however, there is a lack of general-purpose world models capable of accurately predicting long-term future scenarios based on past observations and a variety of action types. To address this gap, Astra is introduced as an interactive, general world model generating real-world future predictions for diverse contexts such as autonomous driving and robot grasping, integrating precise actions like camera movements and robotic controls. Astra employs an autoregressive denoising architecture with temporal causal attention to effectively aggregate historical observations while supporting streaming outputs. The model incorporates a noise-augmented history memory to prevent over-dependence on past frames, balancing temporal coherence with real-time responsiveness. For accurate action control, Astra features an action-aware adapter that injects action signals directly into the denoising workflow. Additionally, a mixture of action experts is developed to dynamically route heterogeneous action modalities, thereby increasing adaptability across tasks including exploration, manipulation, and camera control. Experimental results on multiple datasets demonstrate Astra’s superior performance over current state-of-the-art world models, notably in video fidelity, extended-range prediction accuracy, and alignment with applied actions, confirming its interactive, consistent, and versatile long-term video prediction capabilities. <div>
arXiv:2512.08931v2 Announce Type: replace-cross 
Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Inference with Reusable State-Dependent Value Profiles</title>
<link>https://arxiv.org/abs/2512.11829</link>
<guid>https://arxiv.org/abs/2512.11829</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive behavior, value profiles, belief-weighted mixing, probabilistic reversal learning, policy priors  

<br /><br />Summary:  
1. The article addresses how agents adapt behavior in volatile environments by switching control strategies based on latent contexts, which is usually computationally intractable if separate parameters are maintained for every context.  
2. A novel concept called value profiles is introduced, which bundles outcome preferences, policy priors, and policy precision into reusable parameter sets linked to hidden states in a generative model.  
3. As beliefs over latent states update trial by trial, control parameters emerge through a belief-weighted mixture of these profiles, enabling flexible state-conditional strategy use without an explosion in parameter complexity.  
4. The authors evaluate their approach against simpler static-precision and entropy-coupled dynamic-precision models in a probabilistic reversal learning task, finding strong model superiority via cross-validated log-likelihood and information criteria (around 100-point AIC advantages).  
5. Parameter recovery supports structural identifiability even with noisy observations and inferred contexts, indicating robustness. Model-based inference reveals adaptive control is primarily driven by modulation of policy priors rather than changes in policy precision, with gradual recruitment of profiles reflecting belief-dependent, state-conditional control rather than merely responding to uncertainty.  
6. Overall, value profiles offer a computationally tractable framework for belief-conditioned value control in volatile environments and furnish testable predictions related to flexible behavioral adaptation. <div>
arXiv:2512.11829v1 Announce Type: new 
Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation</title>
<link>https://arxiv.org/abs/2512.11830</link>
<guid>https://arxiv.org/abs/2512.11830</guid>
<content:encoded><![CDATA[
<div> Keywords: chest X-ray, causal inference, AI diagnostics, report generation, patient-centric explanation<br /><br />Summary:  
1. The paper addresses the challenge of automatic chest X-ray report generation, a critical task for improving diagnostic accuracy and aiding faster clinical decisions.  
2. While current AI models excel at identifying patterns in medical images, they often fail to capture deeper cause-and-effect relationships that clarify why certain X-ray findings correspond to specific diagnoses.  
3. The authors propose a novel prompt-driven framework called Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G). This framework focuses on causal inference to enhance the interpretability of AI-generated reports by incorporating reasoning about cause-and-effect relationships.  
4. CR3G emphasizes generating explanations centered on the patient, aiming to make AI diagnostics more meaningful, trustworthy, and useful in clinical practice.  
5. Experimental results demonstrate that CR3G improves the ability to identify causal relationships and provides better explanations for 2 out of 5 tested abnormalities, indicating its potential to advance the quality of AI-driven chest X-ray diagnostics. <div>
arXiv:2512.11830v1 Announce Type: new 
Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Design of One-step Diffusion via Shortcutting Flow Paths</title>
<link>https://arxiv.org/abs/2512.11831</link>
<guid>https://arxiv.org/abs/2512.11831</guid>
<content:encoded><![CDATA[
<div> Keywords: few-step diffusion models, shortcut models, one-step diffusion, ImageNet-256x256, classifier-free guidance<br /><br />Summary:  
This paper addresses the challenges in few-step diffusion models, focusing on shortcut models that train one-step diffusion models directly from scratch. The authors highlight that previous approaches often intertwine theoretical derivations with practical implementations, which limits understanding and restricts design flexibility. To overcome this, they introduce a common design framework that unifies representative shortcut models by providing solid theoretical grounding while separating individual component choices. This disentanglement enables systematic innovation at the component level, aiding researchers in improving model performance more methodically. Leveraging these improvements, their newly designed one-step diffusion model achieves a state-of-the-art Fréchet Inception Distance (FID50k) score of 2.85 on the ImageNet-256x256 dataset under classifier-free guidance, demonstrating superior image generation quality. Importantly, their approach eliminates the need for pre-training, knowledge distillation, or curriculum learning, simplifying the training pipeline. Overall, the work lowers barriers to experimenting with shortcut diffusion models by clarifying the design space, thus fostering principled exploration and accelerating future advancements in efficient diffusion modeling. <div>
arXiv:2512.11831v1 Announce Type: new 
Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation</title>
<link>https://arxiv.org/abs/2512.11832</link>
<guid>https://arxiv.org/abs/2512.11832</guid>
<content:encoded><![CDATA[
<div> Keywords: sparse climate data, inverse distance weighting, ordinary kriging, implicit neural representation, reconstruction accuracy<br /><br />Summary: This study compares three reconstruction methods for sparse climate data: inverse distance weighting (IDW), ordinary kriging (OK), and an implicit neural representation model based on the MMGN architecture. Each method was optimized through hyper-parameter tuning using validation data to ensure fair evaluation. Extensive experiments were performed on 100 randomly sampled sparse datasets from the ECA&amp;D database, followed by comprehensive statistical analyses. Results showed that the simple IDW method outperformed both OK and MMGN in terms of reconstruction accuracy and computational efficiency. IDW achieved the lowest error values, with an RMSE of 3.00 ± 1.93, MAE of 1.32 ± 0.77, and maximum deviation (ΔMAX) of 24.06 ± 17.15. It also had the highest coefficient of determination, R² at 0.68 ± 0.16. Statistical testing confirmed that the differences in RMSE, MAE, and R² were significant and exhibited moderate to large effect sizes, highlighting the reliable advantage of IDW. The Dunn post-hoc test further verified that IDW consistently surpassed the other methods across all quality metrics evaluated. These findings suggest that despite its simplicity, IDW is highly effective for reconstructing sparse climate datasets. <div>
arXiv:2512.11832v1 Announce Type: new 
Abstract: This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \pm 1.93$), MAE ($1.32 \pm 0.77$), and $\Delta_{MAX}$ ($24.06 \pm 17.15$), as well as the highest $R^2$ ($0.68 \pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\&amp;D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Decision Tree classifier: explainable and extendable PyTorch implementation</title>
<link>https://arxiv.org/abs/2512.11833</link>
<guid>https://arxiv.org/abs/2512.11833</guid>
<content:encoded><![CDATA[
<div> Soft Decision Tree, Short-term Memory, Explainability, Clinical Datasets, Classification Methods<br /><br />Summary:<br /><br />This paper presents the implementation of a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using the PyTorch framework. Both models were evaluated extensively on simulated data as well as real clinical datasets to assess their performance. The authors provided visualizations of the SDT to highlight its potential for explainability, an important feature for decision-making processes in healthcare. The comparison of classification methods included SDT, SM-SDT, XGBoost, Random Forest, Logistic Regression, and a traditional Decision Tree. The results showed that SDT, SM-SDT, and XGBoost achieved similar area under the curve (AUC) values, indicating comparable predictive performance. These three methods outperformed Random Forest, Logistic Regression, and Decision Tree classifiers. The findings also suggest that aside from the standard decision tree, all other tested methods delivered comparable results on clinical datasets. The paper emphasizes the balance between model explainability and predictive accuracy. Additionally, the authors have made their code and datasets publicly available on GitHub, encouraging transparency and reproducibility of their work. This contribution supports the development of interpretable machine learning models for clinical applications. <div>
arXiv:2512.11833v1 Announce Type: new 
Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid twinning using PBDW and DeepONet for the effective state estimation and prediction on partially known systems</title>
<link>https://arxiv.org/abs/2512.11834</link>
<guid>https://arxiv.org/abs/2512.11834</guid>
<content:encoded><![CDATA[
<div> Keywords: state estimation, hybrid approach, Parameterized Background Data-Weak, Deep Operator Network, Helmholtz equation<br /><br />Summary:  
The paper addresses the challenge of accurately estimating the state of complex and uncertain physical systems by integrating theoretical models and noisy experimental data. It proposes a hybrid method that combines physics-based modeling with data-driven learning to improve state estimation and prediction accuracy. The approach extends the Parameterized Background Data-Weak (PBDW) framework, which fuses a reduced-order model representation with measurement data to handle both known and unknown uncertainties. To capture discrepancies not represented by the reduced-order model, a Deep Operator Network (DeepONet) is introduced. This DeepONet is constrained to lie within the orthogonal complement of the best-knowledge model manifold, ensuring that the learned corrections target only the unmodeled bias components, thereby maintaining model interpretability and fidelity. The study also explores an optimal sensor placement strategy to maximize information gain from measurements, enhancing state estimation quality. Validation is performed using a test problem involving the Helmholtz equation that simulates various model errors, including those from boundary conditions and source terms. The results demonstrate the efficacy of the proposed hybrid framework in addressing uncertainties and improving state estimation in complex physical systems. <div>
arXiv:2512.11834v1 Announce Type: new 
Abstract: The accurate estimation of the state of complex uncertain physical systems requires reconciling theoretical models, with inherent imperfections, with noisy experimental data. In this work, we propose an effective hybrid approach that combines physics-based modeling with data-driven learning to enhance state estimation and further prediction. Our method builds upon the Parameterized Background Data-Weak (PBDW) framework, which naturally integrates a reduced-order representation of the best-available model with measurement data to account for both anticipated and unanticipated uncertainties. To address model discrepancies not captured by the reduced-order space, and learn the structure of model deviation, we incorporate a Deep Operator Network (DeepONet) constrained to be an orthogonal complement of the best-knowledge manifold. This ensures that the learned correction targets only the unknown components of model bias, preserving the interpretability and fidelity of the physical model. An optimal sensor placement strategy is also investigated to maximize information gained from measurements. We validate the proposed approach on a representative problem involving the Helmholtz equation under various sources of modeling error, including those arising from boundary conditions and source terms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions</title>
<link>https://arxiv.org/abs/2512.11836</link>
<guid>https://arxiv.org/abs/2512.11836</guid>
<content:encoded><![CDATA[
<div> Keywords: Nutritional assessment, Food Compass Score, machine learning, text description, nutrient prediction  

<br /><br />Summary:  
Accurate nutritional assessment plays a vital role in public health but is often hindered by the lack of detailed data obtainable from casual food descriptions. This paper introduces a machine learning pipeline designed to predict the comprehensive Food Compass Score 2.0 (FCS) using text descriptions of food. The method employs multi-headed neural networks that analyze hybrid feature vectors combining semantic text embeddings, lexical patterns, and domain heuristics with USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. These networks estimate the necessary nutrient and food components required by the FCS algorithm. The system shows robust predictive performance, achieving a median R^2 value of 0.81 for individual nutrient predictions. Moreover, the predicted FCS values exhibit a strong correlation with published benchmarks, with Pearson’s correlation coefficient of 0.77 and a mean absolute difference of 14.0 points. The greatest prediction errors occur with ambiguous or highly processed food items. Overall, this approach effectively translates natural language food descriptions into actionable nutritional information, facilitating scalable dietary assessment for both consumer-facing applications and research purposes. <div>
arXiv:2512.11836v1 Announce Type: new 
Abstract: Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D-STEER - Preference Alignment Techniques Learn to Behave, not to Believe -- Beneath the Surface, DPO as Steering Vector Perturbation in Activation Space</title>
<link>https://arxiv.org/abs/2512.11838</link>
<guid>https://arxiv.org/abs/2512.11838</guid>
<content:encoded><![CDATA[
<div> Keywords: Direct Preference Optimization, model alignment, low rank steering, logits embeddings, behavioral illusion  

<br /><br />Summary:  
This paper investigates the internal mechanisms of Direct Preference Optimization (DPO), a prevalent method for aligning large language models. First, it challenges the common assumption that DPO fundamentally rewrites a model’s internal beliefs; instead, it proposes that DPO functions primarily as a low rank steering method that subtly adjusts activations along a few preference-specific directions. Second, through a simple mathematical derivation, the authors reveal that the DPO gradient depends solely on the difference between logit embeddings of preferred versus dispreferred outputs, leading to a first-order modification in the model’s final hidden layer rather than deep semantic changes. Third, the paper empirically extracts a steering vector from a DPO-tuned model, demonstrating that adding this vector to the base activations reproduces much of the model’s aligned behavior, while subtracting it largely restores the original behavior. Fourth, spectral analysis shows a rank-one dominance and entropy collapse in upper layers of the network, indicating alignment effects are concentrated within a narrow subspace. Finally, these combined findings support a "behavioral illusion" interpretation of DPO, suggesting that it primarily teaches models how to behave in an aligned manner instead of altering what they fundamentally believe or understand. <div>
arXiv:2512.11838v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) has become a standard recipe for aligning large language models, yet it is still unclear what kind of change it actually induces inside the network. This paper argues that DPO does not rewrite a models internal beliefs; instead, it acts as a low rank steering mechanism that nudges activations along a small number of preference directions. Using a simple derivation, we show that the DPO gradient depends only on the difference between the logit embeddings of preferred and dispreferred completions, implying a first order shift in the final hidden representation rather than a deep restructuring of semantics. We then extract an empirical steering vector from a DPO tuned model and demonstrate that adding this vector to base activations reproduces most of the aligned behavior, while subtracting it nearly restores the original model. Finally, spectral analyses reveal rank-one dominance and entropy collapse in upper layers, indicating that alignment is funneled through a narrow subspace. Taken together, these results support a behavioral illusion view of DPO: it teaches models how to act aligned, not what to believe.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models as Generalist Policies for Network Optimization</title>
<link>https://arxiv.org/abs/2512.11839</link>
<guid>https://arxiv.org/abs/2512.11839</guid>
<content:encoded><![CDATA[
<div> Keywords: Trailblazer, large language models, network policies, generalization, adaptive collaboration  

<br /><br />Summary:  
1. The paper introduces Trailblazer, a novel framework designed to create generalist network control policies that excel in robust network service management across diverse tasks and environments.  
2. It addresses the limitations of traditional network optimization methods that rely on specialist policies using handcrafted rules or deep learning, which typically suffer from poor generalization.  
3. Trailblazer leverages large language models (LLMs), pretrained on vast internet corpora, allowing access to fundamental networking principles and enabling strong generalization capabilities to unseen scenarios with minimal adaptation.  
4. A core feature of Trailblazer is its network alignment scheme, which grounds the LLM in specific networking tasks, ensuring relevance and accuracy in policy formulation.  
5. Additionally, it introduces an adaptive policy collaboration mechanism where simpler control tasks are offloaded from the LLM to lightweight policies, enhancing computational efficiency.  
6. Extensive experiments involving simulations and real-world online deployment on Douyin (the Chinese TikTok) demonstrate that Trailblazer outperforms conventional specialist policies in cross-task and cross-environment generalization.  
7. The results validate the potential of LLMs as a foundation for generalist network policies and position Trailblazer as an important step toward a generalist-driven paradigm in network control, reducing the effort required for policy design while improving adaptability and performance. <div>
arXiv:2512.11839v1 Announce Type: new 
Abstract: Designing control policies to ensure robust network services is essential to modern digital infrastructure. However, the dominant paradigm for network optimization relies on designing specialist policies based on handcrafted rules or deep learning models, leading to poor generalization across diverse tasks and environments. In contrast, large language models (LLMs), pretrained on Internet-scale corpora, provide a rich and unified knowledge base that encodes fundamental networking principles. Combined with their emergent abilities in generalization to unseen scenarios, LLMs offer a transformative foundation for generalist network policies that can generalize across diverse tasks and environments with minimal adaptation. In this paper, we present Trailblazer, the first systematic framework to realize such a generalist policy for networking. Trailblazer incorporates a network alignment scheme to ground the LLM in specific networking tasks, and an adaptive policy collaboration mechanism that offloads simple control cases from the LLM to a lightweight policy for computational efficiency. Through extensive simulations and large-scale real-world online evaluation on Douyin (the Chinese version of TikTok), Trailblazer, powered by a single LLM, demonstrates stronger cross-task and cross-environment generalization than conventional specialist policies. Our results validate LLMs as the foundation for generalist network policies, and position Trailblazer as the first step toward the generalist-driven paradigm that enables strong generalization with minimal efforts in policy design.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amortized Causal Discovery with Prior-Fitted Networks</title>
<link>https://arxiv.org/abs/2512.11840</link>
<guid>https://arxiv.org/abs/2512.11840</guid>
<content:encoded><![CDATA[
<div> Differentiable penalized likelihood, causal structure learning, Prior-Fitted Networks, amortized likelihood estimation, neural network accuracy  

<br /><br />Summary:  
This paper addresses challenges in causal structure discovery that arise due to errors in likelihood estimation, even when large datasets are available. Traditional differentiable penalized likelihood methods optimize causal structures by maximizing likelihood values based on observed data, but their effectiveness is hampered by inaccurate likelihood estimates. The authors introduce a novel approach called Prior-Fitted Networks (PFNs) to improve the process of likelihood estimation by amortizing data-dependent likelihood calculations. By doing so, PFNs produce more reliable scores that guide causal structure learning more effectively. The method was tested extensively across a range of scenarios, including synthetic, simulated, and real-world datasets, demonstrating considerable improvements in accurately recovering causal structures as compared to standard baseline methods. Importantly, the study provides direct evidence that PFNs outperform conventional neural network-based techniques in estimating likelihoods more precisely. This work suggests that leveraging PFNs in causal discovery pipelines can enhance the robustness and accuracy of inferred causal relationships, ultimately benefiting applications that rely on precise causal modeling. <div>
arXiv:2512.11840v1 Announce Type: new 
Abstract: In recent years, differentiable penalized likelihood methods have gained popularity, optimizing the causal structure by maximizing its likelihood with respect to the data. However, recent research has shown that errors in likelihood estimation, even on relatively large sample sizes, disallow the discovery of proper structures. We propose a new approach to amortized causal discovery that addresses the limitations of likelihood estimator accuracy. Our method leverages Prior-Fitted Networks (PFNs) to amortize data-dependent likelihood estimation, yielding more reliable scores for structure learning. Experiments on synthetic, simulated, and real-world datasets show significant gains in structure recovery compared to standard baselines. Furthermore, we demonstrate directly that PFNs provide more accurate likelihood estimates than conventional neural network-based approaches.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Continual Mobility Forecasting for Proactive Handover Prediction</title>
<link>https://arxiv.org/abs/2512.11841</link>
<guid>https://arxiv.org/abs/2512.11841</guid>
<content:encoded><![CDATA[
<div> Keywords: short-term mobility forecasting, proactive handover, meta-continual learning, GRU predictor, edge deployment<br /><br />Summary:<br /><br />1. The article addresses the challenge of short-term mobility forecasting essential for proactive handover (HO) in cellular networks, where user movement is highly non-stationary due to abrupt turns and speed changes. <br />2. Traditional predictors often fail or drift under these dynamic conditions, causing mistimed or failed handovers. <br />3. The authors propose a novel lightweight meta-continual forecasting framework combining a GRU-based predictor with Reptile meta-initialization, enabling fast few-shot adaptation to changing mobility patterns. <br />4. An Exponentially Weighted Moving Average (EWMA) residual detector is integrated to identify drift events and trigger compact online updates selectively, improving efficiency. <br />5. Evaluation using GeoLife and DeepMIMO datasets shows strong zero-shot forecasting performance (4.46 m ADE, 7.79 m FDE) and improved few-shot accuracy (3.71 m ADE at 10-shot), plus a 2-3x faster recovery from abrupt drift compared to an offline GRU model. <br />6. When applied to downstream handover prediction, the proposed method improves F1 score to 0.83 and AUROC to 0.90 while reducing missed-HO and ping-pong events significantly. <br />7. The model’s small size (128k parameters) makes it suitable for deployment on edge devices within 5G and upcoming 6G networks, enabling real-time adaptability and improved network reliability. <div>
arXiv:2512.11841v1 Announce Type: new 
Abstract: Short-term mobility forecasting is a core requirement for proactive handover (HO) in cellular networks. Real-world mobility is highly non-stationary: abrupt turns, rapid speed changes, and unpredictable user behavior cause conventional predictors to drift, leading to mistimed or failed handovers. We propose a lightweight meta-continual forecasting framework that integrates a GRU-based predictor, Reptile meta-initialization for fast few-shot adaptation, and an EWMA residual detector that triggers compact online updates only when drift occurs. Evaluated on a reproducible GeoLife and DeepMIMO pipeline, our method achieves 4.46 m ADE and 7.79 m FDE in zero-shot settings, improves few-shot ADE to 3.71 m at 10-shot, and enables recovery from abrupt drift about 2 to 3 times faster than an offline GRU. When applied to downstream HO prediction, the approach improves F1 to 0.83 and AUROC to 0.90, with substantial reductions in missed-HO and ping-pong events. The model is lightweight (128k parameters) and suitable for edge deployment in 5G and 6G systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach</title>
<link>https://arxiv.org/abs/2512.11845</link>
<guid>https://arxiv.org/abs/2512.11845</guid>
<content:encoded><![CDATA[
<div> Keywords: passenger flow forecasting, deformable partitioning, temporal-spectral transformer, frequency-domain attention, multiscale time series<br /><br />Summary: Accurate forecasting of passenger flows is vital for efficient airport operations. This paper introduces DTSFormer, a deformable temporal-spectral Transformer designed to address limitations of fixed-size patch embedding in time series forecasting. DTSFormer uses a multiscale deformable partitioning module that dynamically segments input sequences into temporal patches via a novel window function-based masking, allowing for extraction of heterogeneous trends across varying time scales. Within each scale, a frequency-domain attention mechanism captures both high-frequency volatility and low-frequency periodicity inherent in passenger flow data. Multi-frequency features are then fused in the time domain to jointly model short-term fluctuations and long-term trends. The model was evaluated on real passenger flow data from Beijing Capital International Airport, covering January 2023 to March 2024, demonstrating consistent superiority over existing forecasting methods across multiple prediction horizons. Further analyses reveal that the deformable partitioning aligns patch lengths with dominant periodic patterns and heterogeneous trends, enhancing the model's ability to capture sudden, high-frequency changes. Overall, DTSFormer effectively models complex and varied temporal dynamics in airport passenger flows, improving forecasting accuracy and robustness. <div>
arXiv:2512.11845v1 Announce Type: new 
Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Topological Bias in Heterogeneous Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.11846</link>
<guid>https://arxiv.org/abs/2512.11846</guid>
<content:encoded><![CDATA[
<div> Keywords: Heterogeneous Graph Neural Networks, topological bias, meta-weighting, PageRank, contrastive learning<br /><br />Summary: This paper addresses the issue of topological bias in Heterogeneous Graph Neural Networks (HGNNs), which arises due to sparse labels in semi-supervised learning, leading to uneven performance across nodes correlated with graph structure. Unlike prior work focused on homogeneous GNNs, this study targets HGNNs where distinct meta relations exist. The authors propose applying meta-weighting to the adjacency matrix to differentiate these meta relations effectively. Using the modified adjacency matrix, they combine PageRank with node label information to construct a projection that maps nodes to values strongly associated with model performance. This projection reveals that topological bias universally exists in HGNNs, regardless of the presence of intra-type connections. To mitigate this bias, a novel debiasing structure is introduced, which utilizes the difference in the node projection values and integrates this with the original graph structure within a contrastive learning framework. Experiments conducted on three public datasets demonstrate that the proposed approach significantly improves the performance of HGNNs while successfully reducing topological bias, confirming the method’s effectiveness and general applicability in heterogeneous graph learning scenarios. <div>
arXiv:2512.11846v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) are characterized by their capacity of processing graph-structured data. However, due to the sparsity of labels under semi-supervised learning, they have been found to exhibit biased performance on specific nodes. This kind of bias has been validated to correlate with topological structure and is considered as a bottleneck of GNNs' performance. Existing work focuses on the study of homogeneous GNNs and little attention has been given to topological bias in Heterogeneous Graph Neural Networks (HGNNs). In this work, firstly, in order to distinguish distinct meta relations, we apply meta-weighting to the adjacency matrix of a heterogeneous graph. Based on the modified adjacency matrix, we leverage PageRank along with the node label information to construct a projection. The constructed projection effectively maps nodes to values that strongly correlated with model performance when using datasets both with and without intra-type connections, which demonstrates the universal existence of topological bias in HGNNs. To handle this bias, we propose a debiasing structure based on the difference in the mapped values of nodes and use it along with the original graph structure for contrastive learning. Experiments on three public datasets verify the effectiveness of the proposed method in improving HGNNs' performance and debiasing.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute</title>
<link>https://arxiv.org/abs/2512.11847</link>
<guid>https://arxiv.org/abs/2512.11847</guid>
<content:encoded><![CDATA[
<div> Tiny Recursive Models, Abstraction and Reasoning Corpus, test-time augmentation, recursion analysis, efficiency comparison

<br /><br />Summary:  
This technical note analyzes the Tiny Recursive Models (TRM) checkpoint on the ARC-AGI-1 benchmark to clarify the sources of its strong performance on ARC-style tasks. First, it demonstrates that test-time augmentation combined with a 1000-sample majority-vote ensembling significantly boosts performance, increasing Pass@1 accuracy by approximately 11 percentage points compared to single-pass inference. Second, an ablation study replacing task-specific puzzle identifiers with blank or random tokens results in zero accuracy, highlighting strict dependence on task conditioning. Third, an analysis of recursion trajectories reveals that most accuracy gains occur at the first recursion step, with performance saturating after only a few latent updates, indicating that TRM uses shallow recursion rather than deep reasoning. Fourth, early training experiments show that heavy data augmentation broadens the solution candidate distribution and enhances multi-sample success, emphasizing the role of training regime in observed performance. Finally, a comparison with a naive QLoRA fine-tuning of Llama 3 8B indicates that TRM's non-autoregressive design offers much higher throughput and substantially lower memory usage. Overall, these findings suggest TRM’s ARC-AGI-1 success stems from an interplay of efficiency, task-specific priors, and aggressive test-time compute rather than deep internal recursive reasoning. <div>
arXiv:2512.11847v1 Announce Type: new 
Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs</title>
<link>https://arxiv.org/abs/2512.11851</link>
<guid>https://arxiv.org/abs/2512.11851</guid>
<content:encoded><![CDATA[
<div> Keywords: token recycling, key value cache, DialoGPT, inference acceleration, prompt prefix overlap  

<br /><br />Summary:  
This paper investigates whether key value (KV) states used in attention mechanisms for one prompt in a small language model (LLM) can be reused to speed up inference on a similar new prompt. The approach, called token recycling, effectively increases the model's context memory by caching and reusing past activations. Using the Hugging Face framework and DialoGPT-medium, a 345 million parameter GPT-2 style model trained on Reddit conversations, the authors build a cache indexed by sentence embeddings. Cached KV pairs are reused when the new prompt is an exact prefix extension of a previously processed prompt. The paper compares baseline runs with token recycling runs in terms of latency and output fidelity, also logging how many tokens are reused from the cache. The technique requires no modifications to the model itself; cached KV pairs are saved to CPU memory, reloaded, and supplied to the generate function to continue decoding seamlessly. Experimental results show consistent speedups when prefixes overlap, without significant degradation in the model's semantic output quality. When there is no prefix overlap, the system behaves identically to the baseline, demonstrating safe fallback behavior. This method offers a practical and reproducible way to accelerate inference in certain prompt similarity scenarios. <div>
arXiv:2512.11851v1 Announce Type: new 
Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things</title>
<link>https://arxiv.org/abs/2512.11852</link>
<guid>https://arxiv.org/abs/2512.11852</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Robotic Things, Temporal Fusion Transformer, explainability, smart greenhouse, time series forecasting<br /><br />Summary:  
This study explores the integration of the Internet of Robotic Things (IoRT) within smart greenhouse environments to enhance precision agriculture through autonomous environmental control. It addresses the challenge posed by traditional black-box time series forecasting models, which lack transparency and interpretability, key requirements for trust and regulatory adherence in smart farming. The researchers employed the Temporal Fusion Transformer (TFT) model to automate actuator settings, optimizing greenhouse management processes. To ensure the decision-making process is interpretable and trustworthy, multiple explainability techniques were applied, including model-inherent interpretation, Local Interpretable Model-agnostic Explanations (LIME), and SHapley Additive Explanations (SHAP). These methods elucidate how sensor data—temperature, humidity, CO2 levels, light, and external climate—affect actuator control. The TFT model demonstrated a high test accuracy of 95% on a class-imbalanced dataset for the control settings automation. Additionally, the results highlight the varying contributions of each sensor to real-time adjustments in the greenhouse, promoting transparency. This enhanced interpretability facilitates adaptive fine-tuning of environmental controls, which can lead to improved crop yields and greater resource efficiency in automated greenhouse systems. <div>
arXiv:2512.11852v1 Announce Type: new 
Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks</title>
<link>https://arxiv.org/abs/2512.11854</link>
<guid>https://arxiv.org/abs/2512.11854</guid>
<content:encoded><![CDATA[
<div> Keywords: resistance training, hypertrophy, Repetitions in Reserve, wrist-mounted IMU, real-time feedback

<br /><br />Summary: This paper addresses the challenge of optimizing resistance training for muscle hypertrophy by accurately assessing proximity to muscular failure, often quantified as Repetitions in Reserve (RiR). Traditional subjective methods for estimating RiR are unreliable, which can either reduce training effectiveness or lead to excessive fatigue. To solve this, the authors present a novel real-time feedback system that detects near-failure exercise states (RiR ≤ 2) using data from a single wrist-mounted Inertial Measurement Unit (IMU). Their approach involves a two-stage pipeline: a ResNet-based model segments exercise repetitions in real-time from 6-axis IMU data, followed by a classification model that uses features derived from the segmentation, convolutional features, and an LSTM-based historical context to identify near-failure effort windows. The system was trained and evaluated on a newly collected dataset from 13 participants performing preacher curls to failure, involving 631 repetitions in total. Results show the segmentation model achieved an F1 score of 0.83, and the near-failure classifier reached an F1 score of 0.82 under simulated real-time conditions at 1.6 Hz inference rate. The method's practicality was confirmed by deployment on edge hardware including a Raspberry Pi 5 (112 ms latency) and an iPhone 16 (23.5 ms latency), demonstrating feasiblity for low-latency, on-device computation. This study offers a minimal-hardware, AI-driven tool for objective, real-time training intensity feedback, potentially improving training management and hypertrophy outcomes. <div>
arXiv:2512.11854v1 Announce Type: new 
Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry</title>
<link>https://arxiv.org/abs/2512.11855</link>
<guid>https://arxiv.org/abs/2512.11855</guid>
<content:encoded><![CDATA[
<div> Keywords: exact symmetry, approximate symmetry, averaging complexity, machine learning, inductive bias

<br /><br />Summary:  
1. Enforcing exact symmetry in machine learning models provides strong inductive biases and often improves performance in scientific applications.  
2. However, recent empirical findings suggest that approximate symmetry might offer greater flexibility and robustness compared to exact symmetry.  
3. There has been little theoretical understanding or direct comparison between the costs involved in enforcing exact versus approximate symmetry to date.  
4. This paper introduces a novel framework called averaging complexity to quantify the cost of enforcing symmetry through averaging mechanisms.  
5. The main theoretical result shows an exponential separation: achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be accomplished with only logarithmic averaging complexity.  
6. This result is the first formal theoretical justification on why approximate symmetry may be more practical and preferable in machine learning model design.  
7. The tools and techniques developed could have broader applications and interest for future research on exploiting symmetries in machine learning. <div>
arXiv:2512.11855v1 Announce Type: new 
Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search</title>
<link>https://arxiv.org/abs/2512.11856</link>
<guid>https://arxiv.org/abs/2512.11856</guid>
<content:encoded><![CDATA[
<div> GNNs, device-edge co-inference, architecture-mapping co-design, energy prediction, inference optimization  

<br /><br />Summary:  
Graph Neural Networks (GNNs) are recognized as the leading method for graph learning but face significant challenges when deployed on edge devices due to high computational demands and limited resources, hindering real-time and energy-efficient inference. Device-edge co-inference is a promising approach to mitigate these limitations by distributing computation between edge devices and servers, yet existing model partitioning methods fall short for GNNs. To overcome this, the authors propose GCoDE, the first automatic framework designed for joint architecture and mapping co-design and deployment tailored to device-edge hierarchies. GCoDE innovatively treats device communication as an explicit operation, integrating architecture and mapping optimization into a unified design space. This system is aware of diverse heterogeneous hardware performance, enabling more accurate architecture efficiency evaluation. It also incorporates an energy prediction method that analyzes consumption across various GNN operations, enhancing energy assessment and guiding the identification of energy-efficient solutions. Leveraging a constraint-based random search strategy, GCoDE quickly converges to an optimal balance of accuracy and efficiency within 1.5 hours. Finally, GCoDE includes a co-inference engine for streamlined GNN deployment and execution. Experimental results demonstrate significant improvements, achieving up to 44.9 times speedup and 98.2% energy savings compared to existing methods across multiple applications and system setups. <div>
arXiv:2512.11856v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopicProphet: Prophesies on Temporal Topic Trends and Stocks</title>
<link>https://arxiv.org/abs/2512.11857</link>
<guid>https://arxiv.org/abs/2512.11857</guid>
<content:encoded><![CDATA[
<div> Keywords: stock prediction, topic modeling, temporal analysis, breakpoint detection, financial forecasting  

<br /><br />Summary:  
The article introduces TopicProphet, a novel framework designed to improve stock market predictions by addressing inherent challenges in quantitative stock data analysis. Traditional stock prediction models struggle due to the lack of causal logic in stock data and rapid market changes limiting the availability of relevant training data. TopicProphet overcomes these issues by analyzing historical periods with similar public sentiment trends and socio-political contexts, thus providing a more nuanced and relevant dataset for model training. Unlike previous studies that focused only on identifying the impact of keywords and sentiments, this approach incorporates a sequence of advanced techniques including topic modeling, temporal analysis, breakpoint detection, and segment optimization to identify the optimal training time periods. This method ensures the model captures complex patterns linked to specific eras’ socioeconomic and political factors, yielding richer and more pertinent training data. Extensive evaluations demonstrate that TopicProphet outperforms state-of-the-art methods by effectively selecting optimal training data, leading to enhanced accuracy in forecasting financial percentage changes. Overall, this framework offers a significant step forward in stock prediction by leveraging era-specific sentiment and context rather than relying solely on raw quantitative stock data. <div>
arXiv:2512.11857v1 Announce Type: new 
Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Path Integral Diffusion: AdaPID</title>
<link>https://arxiv.org/abs/2512.11858</link>
<guid>https://arxiv.org/abs/2512.11858</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion-based samplers, schedule selection, Harmonic PID, Gaussian Mixture target, Quality-of-Sampling (QoS)

<br /><br />Summary:  
This paper addresses diffusion-based samplers including Score Based Diffusions, Bridge Diffusions, and Path Integral Diffusions, emphasizing that their effectiveness arises from the choice of the intermediate-time schedule that governs the sampling dynamics. The authors propose a novel path-wise schedule selection framework tailored for Harmonic PID systems with time-varying stiffness. This framework exploits Piece-Wise-Constant (PWC) parametrizations combined with a hierarchical refinement approach to optimize the schedule design. A key contribution is the introduction of schedule-sensitive Quality-of-Sampling (QoS) diagnostics, which provide measurable metrics to guide schedule improvement. Under the assumption of a Gaussian Mixture (GM) target distribution, the method maintains closed-form Green function ratios and employs numerically stable, Neural-Network free oracles for predicted-state maps and score calculations, ensuring robustness and interpretability. Experimental validation in a 2D setting demonstrates that the QoS-driven PWC schedules lead to consistent improvements in early-exit fidelity, tail accuracy of the sampling distribution, the conditioning of the dynamics, and speciation timing (label-selection) when operating under fixed computational (integration) budgets. This approach provides a systematic and practical method for enhancing diffusion sampler performance without reliance on complex neural network components. <div>
arXiv:2512.11858v1 Announce Type: new 
Abstract: Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion</title>
<link>https://arxiv.org/abs/2512.11859</link>
<guid>https://arxiv.org/abs/2512.11859</guid>
<content:encoded><![CDATA[
<div> Guided Harmonic Path-Integral Diffusion, Stochastic Optimal Transport, Terminal Distribution, Gaussian-Mixture Model, Protocol Learning  

<br /><br />Summary:  
This paper introduces Guided Harmonic Path-Integral Diffusion (GH-PID), a novel, linearly-solvable framework designed for guided Stochastic Optimal Transport (SOT) that integrates a hard terminal distribution with soft, application-specific path costs. GH-PID employs a low-dimensional guidance protocol to shape the ensemble of trajectories while maintaining a linear structure in the forward and backward Kolmogorov equations, allowing the optimal score to be expressed explicitly as a Green-function ratio. Notably, when terminal distributions follow a Gaussian-Mixture Model (GMM), closed-form solutions are attainable, facilitating stable sampling and enabling differentiable learning of guidance protocols under exact terminal matching conditions. The authors introduce diagnostics focused on guidance, including path cost, centerline adherence, variance flow, and drift effort, which enhance interpretability within this variational ansatz framework for empirical SOT. The methodology is demonstrated through three 2D navigation scenarios: (i) hand-crafted protocols reveal the influence of geometry and stiffness on trajectory lag, curvature, and mode evolution; (ii) single-task protocol learning optimizes a piecewise-constant centerline to minimize integrated costs; and (iii) multi-expert fusion combines competing expert trajectories and terminal distributions through a product-of-experts approach to learn a consensus guidance protocol. Across these cases, GH-PID produces trajectories that are geometry-aware and trust-aware, ensuring exact terminal distribution satisfaction while systematically reducing total path costs. <div>
arXiv:2512.11859v1 Announce Type: new 
Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes</title>
<link>https://arxiv.org/abs/2512.11860</link>
<guid>https://arxiv.org/abs/2512.11860</guid>
<content:encoded><![CDATA[
<div> PDE, graph neural network, operator consistency, irregular meshes, physics-informed learning<br /><br />Summary: This paper addresses the challenge of solving partial differential equations (PDEs) on irregular domains where classical numerical methods often become unstable. The authors propose an Operator-Consistent Graph Neural Network combined with Physics-Informed Neural Networks (OCGNN-PINN) designed specifically to approximate PDE evolution under physics constraints. The core innovation lies in coupling node-edge message passing with a consistency loss that enforces the gradient-divergence relation via the graph incidence matrix. This approach ensures that the dynamics at nodes and edges remain structurally coupled over time, enhancing temporal stability. The model is tested on diffusion processes occurring on physically evolving meshes as well as real-world scanned surfaces, which represent irregular domains. Results indicate that OCGNN-PINN improves both prediction accuracy and temporal stability compared to standard graph convolutional networks and multilayer perceptron baselines. Furthermore, it approaches the performance of Crank-Nicolson numerical solvers, a classical method known for stability on unstructured meshes. This work demonstrates the effectiveness of embedding operator consistency and physics-informed constraints in graph neural networks for complex multiphysics simulations on irregular geometries. <div>
arXiv:2512.11860v1 Announce Type: new 
Abstract: Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL</title>
<link>https://arxiv.org/abs/2512.11862</link>
<guid>https://arxiv.org/abs/2512.11862</guid>
<content:encoded><![CDATA[
<div> Keywords: low-altitude intelligent networks, UAV trajectory planning, task offloading, multi-agent reinforcement learning, energy efficiency  

<br /><br />Summary:  
This paper addresses the challenges in low-altitude intelligent networks (LAINs) that combine unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations to provide low-latency and energy-efficient edge intelligence in dynamic, infrastructure-limited environments. The main challenges include limited UAV energy, stochastic task arrivals, and heterogeneous computing resources. To overcome these, the authors propose an integrated air-ground collaborative network and formulate a complex time-dependent integer nonlinear programming problem to jointly optimize UAV trajectory planning and task offloading decisions. Due to temporal dependencies among variables, the problem is difficult to solve directly. The authors design a hierarchical learning framework operating on two timescales: a large timescale using a Vickrey-Clarke-Groves auction mechanism for energy-aware and incentive-compatible trajectory assignment, and a small timescale employing diffusion-heterogeneous-agent proximal policy optimization, a novel multi-agent reinforcement learning algorithm that integrates latent diffusion models into actor networks. This enables each UAV to sample and refine actions based on observation-conditioned denoising, improving adaptability and policy diversity. Extensive simulations demonstrate that the proposed framework outperforms existing baselines in terms of energy efficiency, task success rate, and convergence performance, highlighting its effectiveness in managing LAINs. <div>
arXiv:2512.11862v1 Announce Type: new 
Abstract: The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase transitions reveal hierarchical structure in deep neural networks</title>
<link>https://arxiv.org/abs/2512.11866</link>
<guid>https://arxiv.org/abs/2512.11866</guid>
<content:encoded><![CDATA[
<div> phase transitions, saddle points, mode connectivity, loss landscape, deep neural networks<br /><br />Summary:<br /><br />Training deep neural networks (DNNs) involves navigating a high-dimensional, non-convex loss landscape to find good minima, but the underlying training dynamics remain not fully understood. This work addresses three key phenomena observed during training: phase transitions similar to those in statistical physics, the prevalence of saddle points, and the concept of mode connectivity relevant for merging models. The authors propose a unified explanatory framework based on the geometry of the loss and error landscapes, analytically demonstrating that phase transitions in DNN learning are driven by saddle points in these landscapes. Leveraging this insight, they introduce a simple, fast algorithm that employs the L2 regularizer as a probe to explore error landscape geometry. Applying this method to DNNs trained on the MNIST dataset, they efficiently identify paths connecting global minima, thereby confirming mode connectivity in practice. Further numerical experiments show that saddle points cause transitions between models that represent different digit classes. Overall, the study connects key training phenomena to the geometric structure of loss landscapes, revealing a hierarchy of accuracy basins analogous to phases found in statistical physics. This contributes to a deeper theoretical understanding of DNN training dynamics and provides practical tools for model analysis. <div>
arXiv:2512.11866v1 Announce Type: new 
Abstract: Training Deep Neural Networks relies on the model converging on a high-dimensional, non-convex loss landscape toward a good minimum. Yet, much of the phenomenology of training remains ill understood. We focus on three seemingly disparate observations: the occurrence of phase transitions reminiscent of statistical physics, the ubiquity of saddle points, and phenomenon of mode connectivity relevant for model merging. We unify these within a single explanatory framework, the geometry of the loss and error landscapes. We analytically show that phase transitions in DNN learning are governed by saddle points in the loss landscape. Building on this insight, we introduce a simple, fast, and easy to implement algorithm that uses the L2 regularizer as a tool to probe the geometry of error landscapes. We apply it to confirm mode connectivity in DNNs trained on the MNIST dataset by efficiently finding paths that connect global minima. We then show numerically that saddle points induce transitions between models that encode distinct digit classes. Our work establishes the geometric origin of key training phenomena in DNNs and reveals a hierarchy of accuracy basins analogous to phases in statistical physics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Dangers of Bootstrapping Generation for Continual Learning and Beyond</title>
<link>https://arxiv.org/abs/2512.11867</link>
<guid>https://arxiv.org/abs/2512.11867</guid>
<content:encoded><![CDATA[
<div> synthetic data, continual learning, generative models, distribution drift, maximum likelihood estimation<br /><br />Summary: The paper examines the impact of repeatedly training machine learning models on synthetically generated data, a practice increasingly common for augmenting datasets. It highlights concerns about distribution drift and performance degradation resulting from dataset contamination due to synthetic data bootstrapping. By framing the issue within the context of continual learning, the study draws parallels to Generative Experience Replay (GER) methods. A statistical analysis demonstrates that synthetic data introduces considerable bias and variance into training objectives, undermining the reliability of maximum likelihood estimation techniques. Empirical results indicate that popular generative models experience collapse when subjected to repeated training on synthetic datasets. The research quantifies this degradation and reveals that even advanced GER techniques fail to preserve latent space alignment during continual learning. These findings raise significant issues regarding the long-term use of synthetic data for training, emphasizing potential pitfalls for performance and model stability. The study calls for cautious adoption of synthetic data in continual learning frameworks and suggests a need for improved strategies to mitigate bias, variance, and distribution drift induced by synthetic training samples. <div>
arXiv:2512.11867v1 Announce Type: new 
Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations</title>
<link>https://arxiv.org/abs/2512.11946</link>
<guid>https://arxiv.org/abs/2512.11946</guid>
<content:encoded><![CDATA[
<div> Keywords: Explainable machine learning, Partial Dependence Plots, Individual Conditional Expectation, Global sensitivity, Feature importance<br /><br />Summary:  
1. Explainable machine learning is crucial in engineering fields such as aerospace, where understanding variable influence on predictions is essential.  
2. Partial Dependence Plots (PDPs) are commonly used to interpret black-box models by showing average effects of input variables; however, PDPs can be misleading in the presence of strong feature interactions because averaging masks interaction effects.  
3. The authors propose a new global sensitivity metric based on Individual Conditional Expectation (ICE) curves, capturing the expected feature importance and its variability to better reflect interaction effects.  
4. A mathematical proof is presented showing that the PDP-based sensitivity metric is a lower bound of the ICE-based metric under truncated orthogonal polynomial expansions.  
5. An ICE-based correlation value is also introduced to quantify interaction-driven changes in input-output relationships.  
6. The methodology is tested on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics problem.  
7. ICE-based sensitivity is compared with PDP, SHAP, and Sobol' indices, demonstrating richer insights into feature importance and complementing visual interpretations provided by other methods.  
8. Overall, the ICE-based approach provides a more nuanced understanding of input influences, especially when interactions are significant in complex engineering models. <div>
arXiv:2512.11946v1 Announce Type: new 
Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors</title>
<link>https://arxiv.org/abs/2512.11949</link>
<guid>https://arxiv.org/abs/2512.11949</guid>
<content:encoded><![CDATA[
<div> Activation monitoring, Neural Chameleons, model evasion, misalignment threat, robustness  

<br /><br />Summary:  
This paper addresses the challenge of activation monitoring for AI safety, which involves probing a model’s internal states via lightweight classifiers to detect unsafe behaviors. The authors investigate a misalignment threat model where a language model might learn to deliberately evade such monitoring. They demonstrate that fine-tuning can produce "Neural Chameleons," models that can zero-shot evade activation monitors trained on benign concepts simply by conditioning on a trigger phrase like "You are being probed for {concept}." Crucially, this evasion generalizes to safety-relevant concepts unseen during fine-tuning, such as "deception." The phenomenon is validated across multiple large language model families (Llama, Gemma, Qwen), even when monitors are trained after the model is frozen. The evasion is highly selective, affecting only the monitored concept, and minimally impacts overall model performance on standard benchmarks. A mechanistic case study on Gemma-2-9b-it reveals the evasion works by shifting activations into a low-dimensional subspace. Although more robust defenses like ensembles and nonlinear classifiers reduce evasion success, the threat remains non-trivial. Overall, this work provides a proof-of-concept exposing a critical vulnerability in activation monitoring methods and suggests a tool for assessing their worst-case robustness under adversarial misalignment. <div>
arXiv:2512.11949v1 Announce Type: new 
Abstract: Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: "You are being probed for {concept}". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Extract Context for Context-Aware LLM Inference</title>
<link>https://arxiv.org/abs/2512.11986</link>
<guid>https://arxiv.org/abs/2512.11986</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, user intent, contextual cues, reinforcement learning, safety  

<br /><br />Summary:  
This paper addresses the issue of ambiguity and under-specification in user prompts to large language models (LLMs), emphasizing how subtle contextual cues—such as user intentions, prior knowledge, and risk factors—critically affect the appropriateness of generated responses. Traditional frameworks generate immediate answers without considering broader context, which can lead to unsafe or unnecessarily cautious outputs. To overcome this, the authors propose a novel framework that extracts and leverages contextual information directly from user prompts. Their method employs a reinforcement learning-based context generator designed like an autoencoder to infer relevant contextual signals, which then guide the response generation process. This approach is particularly valuable for safety-sensitive tasks where ambiguous requests may evade safeguards, and benign but confusing queries might trigger unwarranted refusals. Experimental results demonstrate that the method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across several foundation models. Additionally, it improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on the XSTest and WildJailbreak benchmarks. Overall, the findings highlight the effectiveness of context extraction mechanisms in producing safer and more reliable inferences from LLMs. <div>
arXiv:2512.11986v1 Announce Type: new 
Abstract: User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnviroLLM: Resource Tracking and Optimization for Local AI</title>
<link>https://arxiv.org/abs/2512.12004</link>
<guid>https://arxiv.org/abs/2512.12004</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, energy consumption, benchmarking, real-time monitoring, optimization

<br /><br />Summary: This paper introduces EnviroLLM, an open-source toolkit designed to help users measure, monitor, and optimize the resource usage and environmental impact of large language models (LLMs) running locally on personal devices. First, EnviroLLM offers real-time process monitoring to track performance and energy consumption while the models are in use. Second, it supports benchmarking across multiple LLM platforms, including Ollama, LM Studio, vLLM, and OpenAI-compatible APIs, enabling broad applicability and comparison. Third, the system provides persistent data storage with visualizations, allowing users to perform longitudinal analysis of their usage patterns and efficiency metrics over time. Fourth, it delivers personalized recommendations for model selection and optimization strategies tailored to the user's specific hardware and needs. Finally, EnviroLLM incorporates LLM-as-judge evaluations, which assess the trade-offs between quality, energy consumption, and speed by using custom prompts, helping users make informed decisions about which model configurations best meet their performance and sustainability goals. Overall, EnviroLLM empowers users to better understand and reduce the environmental footprint of deploying large language models locally without sacrificing usability or functionality. <div>
arXiv:2512.12004v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2512.12022</link>
<guid>https://arxiv.org/abs/2512.12022</guid>
<content:encoded><![CDATA[
<div> Keywords: Decentralized Federated Learning, Aggregation Framework, Fairness, Robustness, Byzantine Attacks  

<br /><br />Summary:  
This paper addresses challenges in decentralized federated learning (DFL), a framework that enables multiple clients to collaboratively train machine learning models without a central server, thus avoiding associated vulnerabilities. The authors propose DFedReweighting, a unified aggregation framework designed to optimize diverse objectives in DFL systems through an objective-oriented reweighting aggregation process performed at the end of each learning round. The framework calculates preliminary weights based on target performance metrics derived from an auxiliary dataset constructed using local client data. These weights are then adjusted using a customized reweighting strategy to produce final aggregation weights tailored to specific goals. Theoretical analysis confirms that when combining appropriate performance metrics with customized reweighting strategies, the algorithm achieves linear convergence. Experimental evaluations demonstrate that DFedReweighting significantly enhances fairness among clients and improves robustness against Byzantine (malicious) attacks across various scenarios. Ultimately, the framework’s flexibility allows it to be adapted to achieve a broad range of desired learning objectives, given the appropriate choice of performance metrics and reweighting strategies, making it a versatile solution for improving DFL systems. <div>
arXiv:2512.12022v1 Announce Type: new 
Abstract: Decentralized federated learning (DFL) has recently emerged as a promising paradigm that enables multiple clients to collaboratively train machine learning model through iterative rounds of local training, communication, and aggregation without relying on a central server which introduces potential vulnerabilities in conventional Federated Learning. Nevertheless, DFL systems continue to face a range of challenges, including fairness, robustness, etc. To address these challenges, we propose \textbf{DFedReweighting}, a unified aggregation framework designed to achieve diverse objectives in DFL systems via a objective-oriented reweighting aggregation at the final step of each learning round. Specifically, the framework first computes preliminary weights based on \textit{target performance metric} obtained from auxiliary dataset constructed using local data. These weights are then refined using \textit{customized reweighting strategy}, resulting in the final aggregation weights. Our results from the theoretical analysis demonstrate that the appropriate combination of the target performance metric and the customized reweighting strategy ensures linear convergence. Experimental results consistently show that our proposed framework significantly improves fairness and robustness against Byzantine attacks in diverse scenarios. Provided that appropriate target performance metrics and customized reweighting strategy are selected, our framework can achieve a wide range of desired learning objectives.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.12046</link>
<guid>https://arxiv.org/abs/2512.12046</guid>
<content:encoded><![CDATA[
<div> Goal-Conditioned Reinforcement Learning, Quasimetric RL, Eikonal PDE, Hierarchical Reinforcement Learning, Offline Navigation<br /><br />Summary:<br /><br />1. The paper addresses Goal-Conditioned Reinforcement Learning (GCRL), an approach that simplifies reward design by focusing on goal-reaching tasks instead of traditional reward maximization. 2. It highlights that the optimal goal-conditioned value function forms a quasimetric, leading to the framework of Quasimetric RL (QRL), which enforces local consistency through discrete trajectory-based constraints.  3. The authors propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a novel continuous-time reformulation based on the Eikonal Partial Differential Equation (PDE), which eliminates the need for explicit trajectories by using only sampled states and goals. 4. Eik-QRL is theoretically grounded and offers improved generalization to out-of-distribution goals but faces limitations when dealing with complex dynamics. 5. To overcome these challenges, the paper introduces Eik-Hierarchical QRL (Eik-HiQRL), a hierarchical approach integrating Eik-QRL that achieves state-of-the-art performance in offline goal-conditioned navigation and improves manipulation task outcomes, matching the results of temporal-difference methods. <div>
arXiv:2512.12046v1 Announce Type: new 
Abstract: Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title>
<link>https://arxiv.org/abs/2512.12066</link>
<guid>https://arxiv.org/abs/2512.12066</guid>
<content:encoded><![CDATA[
<div> Keywords: safety evaluation, large language models, decision stability, temperature settings, instruction-tuned models<br /><br />Summary: This study critiques the common practice of single-shot safety evaluations for large language models (LLMs), arguing that such methods assume model responses are deterministic and fully representative of safety alignment, which is not the case. Researchers tested four instruction-tuned models (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 sampling configurations combining 4 temperature settings and 5 random seeds. They discovered that 18-28% of prompts caused the model to flip between refusal and compliance, depending on the configuration, indicating instability in safety decisions. The introduction of a Safety Stability Index (SSI) revealed that higher temperatures reduce decision stability significantly, with SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. Validation using Claude 3.5 Haiku as an external judge showed strong agreement (89.0% inter-judge agreement, Cohen’s kappa = 0.62) with the primary Llama 70B judge. The study concludes that single-shot evaluations are insufficient for reliable safety assessments, as they only align with a multi-sample ground truth 92.4% of the time. Therefore, the authors recommend using at least three samples per prompt to improve the reliability of safety evaluation in LLMs. <div>
arXiv:2512.12066v1 Announce Type: new 
Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-informed neural networks to solve inverse problems in unbounded domains</title>
<link>https://arxiv.org/abs/2512.12074</link>
<guid>https://arxiv.org/abs/2512.12074</guid>
<content:encoded><![CDATA[
<div> Keywords: inverse problems, physics informed neural networks, Kolmogorov Arnold networks, sampling strategy, computational efficiency<br /><br />Summary:<br /><br />1. The paper addresses inverse problems across infinite and semi-infinite domains, which are central in applied mathematics and have practical applications like medical imaging and geophysical exploration.<br /><br />2. It compares two types of physics-informed machine learning models: Physics Informed Neural Networks (PINNs) and Physics Informed Kolmogorov Arnold Networks (PIKANs), highlighting PIKANs’ polynomial composition that provides interpretability and potential accuracy benefits.<br /><br />3. A new sampling strategy for training points is introduced, leveraging negative exponential and normal distributions to better handle the domain characteristics, along with a dual network architecture that simultaneously learns both the equation’s solution and its parameters using a unified loss function.<br /><br />4. The design removes the need for explicitly imposed boundary conditions by relying on the natural stabilization of the solution outside the domain of interest, a key innovation for problems in unbounded domains.<br /><br />5. Performance evaluation shows PINNs outperform PIKANs in terms of computational speed — solving problems 1,000 times faster — while also achieving a slightly lower relative error, making PINNs more accurate and efficient especially under noisy conditions. <div>
arXiv:2512.12074v1 Announce Type: new 
Abstract: Inverse problems are extensively studied in applied mathematics, with applications ranging from acoustic tomography for medical diagnosis to geophysical exploration. Physics informed neural networks (PINNs) have emerged as a powerful tool for solving such problems, while Physics informed Kolmogorov Arnold networks (PIKANs) represent a recent benchmark that, in certain problems, promises greater interpretability and accuracy compared to PINNs, due to their nature, being constructed as a composition of polynomials. In this work, we develop a methodology for addressing inverse problems in infinite and semi infinite domains. We introduce a novel sampling strategy for the network's training points, using the negative exponential and normal distributions, alongside a dual network architecture that is trained to learn the solution and parameters of an equation with the same loss function. This design enables the solution of inverse problems without explicitly imposing boundary conditions, as long as the solutions tend to stabilize when leaving the domain of interest. The proposed architecture is implemented using both PINNs and PIKANs, and their performance is compared in terms of accuracy with respect to a known solution as well as computational time and response to a noisy environment. Our results demonstrate that, in this setting, PINNs provide a more accurate and computationally efficient solution, solving the inverse problem 1,000 times faster and in the same order of magnitude, yet with a lower relative error than PIKANs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SigTime: Learning and Visually Explaining Time Series Signatures</title>
<link>https://arxiv.org/abs/2512.12076</link>
<guid>https://arxiv.org/abs/2512.12076</guid>
<content:encoded><![CDATA[
<div> Temporal patterns, time series, Transformer models, shapelets, visual analytics<br /><br />Summary:  
This paper addresses the challenges in time series pattern discovery, particularly the issues of high computational complexity, limited interpretability, and difficulties in capturing meaningful temporal structures. To overcome these challenges, the authors propose a novel learning framework that jointly trains two Transformer models using complementary time series representations. The first uses shapelet-based representations to capture localized temporal structures, providing interpretable signatures that help differentiate classification labels. The second relies on traditional feature engineering approaches to encode statistical properties of the data. In addition to the model development, the authors present SigTIme, a visual analytics system with coordinated views designed to support exploration and insight generation of time series signatures from multiple perspectives. The framework’s effectiveness is quantitatively validated on eight public datasets and one proprietary clinical dataset. Furthermore, the practical utility of the system is showcased through two real-world usage scenarios involving domain experts: analysis of publicly available ECG data and preterm labor prediction. Overall, the study contributes a novel interpretable modeling framework alongside an interactive visualization tool, facilitating improved understanding and analysis of complex temporal patterns in biomedical and other time series data. <div>
arXiv:2512.12076v1 Announce Type: new 
Abstract: Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation</title>
<link>https://arxiv.org/abs/2512.12086</link>
<guid>https://arxiv.org/abs/2512.12086</guid>
<content:encoded><![CDATA[
<div> Data obfuscation, latent diffusion models, contrastive learning, privacy-utility trade-off, resource-constrained devices<br /><br />Summary:<br /><br />1. The paper addresses the challenge of protecting privacy in time-series sensor data emitted by semi-trusted parties through data obfuscation techniques to mitigate attribute inference attacks.<br /><br />2. Existing methods often rely on conditional generative models enhanced by adversarial training or mutual information regularization but face limitations such as the need to modify downstream tasks, poor privacy-utility trade-offs, or high computational costs.<br /><br />3. The authors propose Cloak, a new data obfuscation framework that uses latent diffusion models guided by contrastive learning to generate disentangled representations, effectively concealing private information while preserving useful data.<br /><br />4. Cloak enables flexible navigation of privacy-utility trade-offs with minimal retraining, catering to users with diverse privacy requirements.<br /><br />5. Extensive experiments conducted on four public time-series datasets from multiple sensing modalities and a facial image dataset demonstrate that Cloak outperforms existing state-of-the-art obfuscation methods and is suitable for deployment on resource-constrained mobile IoT devices, showing its practical applicability. <div>
arXiv:2512.12086v1 Announce Type: new 
Abstract: Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes</title>
<link>https://arxiv.org/abs/2512.12091</link>
<guid>https://arxiv.org/abs/2512.12091</guid>
<content:encoded><![CDATA[
<div> Keywords: performance prediction, heterogeneous embedded SoCs, Graph neural networks, uncertainty-aware surrogate, model-based reinforcement learning<br /><br />Summary:<br /><br />This paper addresses the challenge of performance prediction for OpenMP workloads on heterogeneous embedded System-on-Chips (SoCs), where issues such as task DAG (Directed Acyclic Graph) structure, control-flow irregularity, cache and branch behavior, and thermal dynamics complicate reliable modeling. Traditional heuristics, tabular regression, and model-free reinforcement learning approaches exhibit limitations under such complexity, including risks of overheating in resource-constrained devices. The authors propose GraphPerf-RT, the first surrogate model that integrates task DAG topology, control flow graph-derived code semantics, and runtime context—including per-core DVFS (Dynamic Voltage and Frequency Scaling), thermal state, and utilization—in a heterogeneous graph representation. This graph uses typed edges to encode task precedence, placement, and contention. GraphPerf-RT employs multi-task evidential neural network heads that predict makespan, energy consumption, cache and branch misses, and CPU utilization, outputting calibrated uncertainty via a Normal-Inverse-Gamma distribution to enable risk-aware scheduling that filters low-confidence predictions. Validation on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi) yielded high prediction accuracy (R² > 0.95) with low expected calibration error (ECE < 0.05). Integration of GraphPerf-RT with four reinforcement learning methods on Jetson TX2 demonstrated that a multi-agent model-based approach (MAMBRL-D3QN) significantly reduces makespan (by 66%) and energy consumption (by 82%) compared to model-free baselines, confirming the efficacy of accurate, uncertainty-aware surrogates for managing thermally constrained embedded systems through model-based planning. <div>
arXiv:2512.12091v1 Announce Type: new 
Abstract: Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache
  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks
  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core
  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict
  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.
  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To
  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),
  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the
  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,
  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural CDEs as Correctors for Learned Time Series Models</title>
<link>https://arxiv.org/abs/2512.12116</link>
<guid>https://arxiv.org/abs/2512.12116</guid>
<content:encoded><![CDATA[
<div> Keywords: time-series forecasting, Predictor-Corrector, neural controlled differential equation, multi-step forecasts, regularization

<br /><br />Summary:  
The paper addresses the challenge of errors in multi-step forecasting of dynamical systems using learned time-series models. It introduces a Predictor-Corrector mechanism where the Predictor can be any existing learned time-series model and the Corrector is implemented as a neural controlled differential equation. The Predictor generates forecasts, while the Corrector estimates the errors in these forecasts and corrects them by adding the predicted errors back to the forecasts, enhancing the overall forecasting accuracy. This approach is compatible with both continuous- and discrete-time predictors and can handle irregularly sampled time series data. Two novel regularization strategies are proposed to improve the Corrector’s ability to extrapolate and to accelerate its training process. The effectiveness of the Predictor-Corrector mechanism is demonstrated through experiments involving various Predictors such as neural ordinary differential equations, Contiformer, and DLinear. These experiments are conducted on diverse datasets including synthetic data, physics simulations, and real-world forecasting tasks. Results show consistent improvements in forecast quality when using the Predictor-Corrector mechanism compared to relying solely on the Predictor, highlighting the utility of this error-correction framework in time-series forecasting tasks. <div>
arXiv:2512.12116v1 Announce Type: new 
Abstract: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models</title>
<link>https://arxiv.org/abs/2512.12121</link>
<guid>https://arxiv.org/abs/2512.12121</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, MoE models, modular framework, token routing, multilingual code-switching<br /><br />Summary:<br /><br />1. The paper introduces MixtureKit, an open-source, modular framework designed to build, train, and analyze Mixture-of-Experts (MoE) models using pre-trained or fine-tuned models. <br /><br />2. MixtureKit supports three methods: Traditional MoE, which uses a single router per transformer block; BTX (Branch-Train-Mix), which adds routers for specific sub-layers to enable fine-grained token routing; and BTS (Branch-Train-Stitch), where experts remain intact with trainable stitch layers facilitating controlled communication between hub and experts. <br /><br />3. The framework automatically adjusts model configurations, patches decoder and causal language model classes, and produces a unified checkpoint that can be directly used for inference or further fine-tuning. <br /><br />4. MixtureKit includes a visualization tool for inspecting per-token routing choices, expert weight distributions, and layer-wise expert contributions, providing insight into the model’s internal decision-making.<br /><br />5. Experiments on multilingual code-switched datasets (for example, Arabic-Latin) demonstrate that BTX-based MoE models trained with MixtureKit outperform standard dense models across multiple benchmarks, underscoring its practical utility for diverse domains. <div>
arXiv:2512.12121v1 Announce Type: new 
Abstract: We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Dimensional Tensor Discriminant Analysis: Low-Rank Discriminant Structure, Representation Synergy, and Theoretical Guarantees</title>
<link>https://arxiv.org/abs/2512.12122</link>
<guid>https://arxiv.org/abs/2512.12122</guid>
<content:encoded><![CDATA[
<div> Keywords: tensor classification, CP low-rank, Tensor Gaussian Mixture Model, discriminant analysis, deep generative models<br /><br />Summary:<br /><br />1. The paper addresses classification problems involving high-dimensional tensor-valued predictors, which commonly appear as learned neural network representations in modern applications.<br /><br />2. Existing tensor classification methods typically assume sparsity or Tucker decomposition structures and do not provide strong theoretical guarantees. In contrast, this work introduces a novel modeling approach based on CP low-rank structure for the discriminant tensor, motivated by empirical evidence that discriminative signals focus on a few multilinear components.<br /><br />3. Under a Tensor Gaussian Mixture Model, the authors propose the CP low-rank Tensor Discriminant Analysis (CP-TDA) method, which incorporates a Randomized Composite PCA (rc-PCA) initialization to effectively manage dependent and anisotropic noise even under weaker signal strength and incoherence assumptions. This is followed by an iterative refinement algorithm.<br /><br />4. The paper establishes strong theoretical guarantees for the method, including global convergence and minimax-optimal misclassification rates.<br /><br />5. For data deviating from tensor normality, the authors develop the first semiparametric tensor discriminant model that maps learned tensor representations into a latent space via deep generative models optimized for CP-TDA. Misclassification risk is decomposed into representation, approximation, and estimation errors.<br /><br />6. Numerical experiments and real-world graph classification tasks demonstrate that the proposed method substantially outperforms existing tensor classifiers and state-of-the-art graph neural networks, especially in high-dimensional settings with limited samples. <div>
arXiv:2512.12122v1 Announce Type: new 
Abstract: High-dimensional tensor-valued predictors arise in modern applications, increasingly as learned representations from neural networks. Existing tensor classification methods rely on sparsity or Tucker structures and often lack theoretical guarantees. Motivated by empirical evidence that discriminative signals concentrate along a few multilinear components, we introduce CP low-rank structure for the discriminant tensor, a modeling perspective not previously explored. Under a Tensor Gaussian Mixture Model, we propose high-dimensional CP low-rank Tensor Discriminant Analysis (CP-TDA) with Randomized Composite PCA (\textsc{rc-PCA}) initialization, that is essential for handling dependent and anisotropic noise under weaker signal strength and incoherence conditions, followed by iterative refinement algorithm. We establish global convergence and minimax-optimal misclassification rates.
  To handle tensor data deviating from tensor normality, we develop the first semiparametric tensor discriminant model, in which learned tensor representations are mapped via deep generative models into a latent space tailored for CP-TDA. Misclassification risk decomposes into representation, approximation, and estimation errors. Numerical studies and real data analysis on graph classification demonstrate substantial gains over existing tensor classifiers and state-of-the-art graph neural networks, particularly in high-dimensional, small-sample regimes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models</title>
<link>https://arxiv.org/abs/2512.12131</link>
<guid>https://arxiv.org/abs/2512.12131</guid>
<content:encoded><![CDATA[
<div> Low-rank architectures, tensor parallelism, bottleneck models, BOOST framework, training efficiency

<br /><br />Summary: The paper addresses the challenge of scaling transformer model pre-training, which is limited by high computation and communication costs. It highlights that low-rank bottleneck architectures can significantly reduce training time and memory usage with little accuracy loss but suffer from poor scaling under standard tensor parallelism methods. Conventional 3D parallelism techniques, designed for full-rank models, cause excessive communication overhead and inefficient GPU utilization when applied naively to low-rank bottleneck models. To overcome these issues, the authors propose BOOST, a novel training framework specifically optimized for large-scale low-rank bottleneck architectures. BOOST introduces Bottleneck-aware Tensor Parallelism to better handle the unique structure of low-rank models. Additionally, BOOST incorporates optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to improve training speed and resource efficiency. Experimental evaluations demonstrate that BOOST achieves a 1.46 to 1.91 times speedup compared to full-rank model baselines, and a 1.87 to 2.27 times speedup relative to low-rank models using naive 3D parallelism. This results in enhanced GPU utilization and significantly reduced communication overhead, making BOOST a promising solution for efficient large-scale training of low-rank transformer models. <div>
arXiv:2512.12131v1 Announce Type: new 
Abstract: The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\times$ speedup over full-rank model baselines and 1.87-2.27$\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Approximation Power of SiLU Networks: Exponential Rates and Depth Efficiency</title>
<link>https://arxiv.org/abs/2512.12132</link>
<guid>https://arxiv.org/abs/2512.12132</guid>
<content:encoded><![CDATA[
<div> SiLU, neural networks, function approximation, Sobolev spaces, approximation rates<br /><br />Summary:<br /><br />This article presents a theoretical framework proving that neural networks with SiLU (Sigmoid Linear Unit) activation functions can approximate smooth functions at exponential rates. The authors develop a hierarchical method starting from an efficient and compact approximation of the square function \(x^2\), which is more depth- and size-efficient than existing ReLU network constructions such as those by Yarotsky. Specifically, they achieve an approximation error that decreases as \(\mathcal{O}(\omega^{-2k})\) while maintaining networks of constant depth \(\mathcal{O}(1)\). Building on this foundation, the framework is extended through functional composition techniques to derive sharp approximation bounds for deep SiLU networks targeting functions in Sobolev spaces. The resulting network design keeps the total depth constant \(\mathcal{O}(1)\) and controls the network size with the complexity \(\mathcal{O}(\varepsilon^{-d/n})\), where \(\varepsilon\) denotes the target accuracy, \(d\) the input dimension, and \(n\) the smoothness order. Overall, the work demonstrates that SiLU networks provide improved complexity control and approximation efficiency compared to classical ReLU-based methods, advancing the theoretical understanding of neural network approximation capabilities for smooth function classes. <div>
arXiv:2512.12132v1 Announce Type: new 
Abstract: This article establishes a comprehensive theoretical framework demonstrating that SiLU (Sigmoid Linear Unit) activation networks achieve exponential approximation rates for smooth functions with explicit and improved complexity control compared to classical ReLU-based constructions. We develop a novel hierarchical construction beginning with an efficient approximation of the square function $x^2$ more compact in depth and size than comparable ReLU realizations, such as those given by Yarotsky. This construction yields an approximation error decaying as $\mathcal{O}(\omega^{-2k})$ using networks of depth $\mathcal{O}(1)$. We then extend this approach through functional composition to establish sharp approximation bounds for deep SiLU networks in approximating Sobolev-class functions, with total depth $\mathcal{O}(1)$ and size $\mathcal{O}(\varepsilon^{-d/n})$.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity</title>
<link>https://arxiv.org/abs/2512.12135</link>
<guid>https://arxiv.org/abs/2512.12135</guid>
<content:encoded><![CDATA[
<div> Keywords: intracranial recordings, spatiotemporal transformer, self-supervised learning, token encoding, multiregional brain activity  

<br /><br />Summary:  
This paper addresses the challenge of modeling complex spatiotemporal interactions in human brain activity measured by multiregional intracranial electrophysiology (iEEG) recordings. The authors propose a novel spatiotemporal transformer model designed specifically for such neural data, alongside a self-supervised masked latent reconstruction task that allows flexible manipulation of spatial scales for both token encoding and masking. By experimenting with publicly available iEEG datasets, they demonstrate that the choice of spatial scale in token encoding and masked reconstruction significantly affects the performance in downstream decoding tasks. Importantly, encoding spatial information at scales larger than individual channels—such as region-level encoding—yields better decoding accuracy compared to conventional channel-level encoding approaches. Moreover, their model maintains accurate reconstruction of channel-level neural activity even when operating at region-level tokenization, illustrating the model’s ability to capture multiscale neural dynamics effectively. Overall, this framework provides a valuable tool for exploring spatial encoding strategies in brain recordings, emphasizing the importance of spatial scale in self-supervised pretraining for neurofoundation models, ultimately improving the decoding of complex brain network patterns from high-dimensional, multiregional electrophysiological data. <div>
arXiv:2512.12135v1 Announce Type: new 
Abstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HydroDiffusion: Diffusion-Based Probabilistic Streamflow Forecasting with a State Space Backbone</title>
<link>https://arxiv.org/abs/2512.12183</link>
<guid>https://arxiv.org/abs/2512.12183</guid>
<content:encoded><![CDATA[
<div> diffusion models, probabilistic forecasting, streamflow, HydroDiffusion, state space model<br /><br />Summary:<br /><br />1. The paper introduces HydroDiffusion, a novel diffusion-based probabilistic forecasting framework designed to improve streamflow prediction, addressing limitations of prior models that use LSTM backbones and single-step training. <br />2. HydroDiffusion employs a decoder-only state space model backbone to jointly denoise complete multi-day streamflow trajectories in a single pass, which enhances temporal coherence and reduces error accumulation common in autoregressive methods. <br />3. The framework is evaluated using the CAMELS dataset across 531 watersheds in the contiguous United States, providing a comprehensive continental-scale assessment. <br />4. HydroDiffusion is benchmarked against two diffusion-based LSTM models and the Diffusion-based Runoff Model (DRUM), demonstrating superior deterministic and probabilistic forecast skill, especially in operational forecasting contexts. <br />5. The results show that HydroDiffusion maintains strong nowcast accuracy with observed meteorological inputs and consistent performance over full simulation horizons, establishing it as a robust baseline for medium-range streamflow forecasting and a foundation for future hydrologic probabilistic prediction research at large scales. <div>
arXiv:2512.12183v1 Announce Type: new 
Abstract: Recent advances have introduced diffusion models for probabilistic streamflow forecasting, demonstrating strong early flood-warning skill. However, current implementations rely on recurrent Long Short-Term Memory (LSTM) backbones and single-step training objectives, which limit their ability to capture long-range dependencies and produce coherent forecast trajectories across lead times. To address these limitations, we developed HydroDiffusion, a diffusion-based probabilistic forecasting framework with a decoder-only state space model backbone. The proposed framework jointly denoises full multi-day trajectories in a single pass, ensuring temporal coherence and mitigating error accumulation common in autoregressive prediction. HydroDiffusion is evaluated across 531 watersheds in the contiguous United States (CONUS) in the CAMELS dataset. We benchmark HydroDiffusion against two diffusion baselines with LSTM backbones, as well as the recently proposed Diffusion-based Runoff Model (DRUM). Results show that HydroDiffusion achieves strong nowcast accuracy when driven by observed meteorological forcings, and maintains consistent performance across the full simulation horizon. Moreover, HydroDiffusion delivers stronger deterministic and probabilistic forecast skill than DRUM in operational forecasting. These results establish HydroDiffusion as a robust generative modeling framework for medium-range streamflow forecasting, providing both a new modeling benchmark and a foundation for future research on probabilistic hydrologic prediction at continental scales.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolGuidance: Advanced Guidance Strategies for Conditional Molecular Generation with Flow Matching</title>
<link>https://arxiv.org/abs/2512.12198</link>
<guid>https://arxiv.org/abs/2512.12198</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular generation, guidance strategies, SE(3)-equivariant flow, property alignment, Bayesian optimization<br /><br />Summary: This work addresses key objectives in conditional molecular generation, focusing on chemical validity, property alignment, structural diversity, and sampling efficiency for discovery. It adapts recent guidance strategies from computer vision—namely classifier-free guidance, autoguidance, and model guidance—into a molecule generation framework based on an SE(3)-equivariant flow matching process. A novel hybrid guidance strategy is introduced that independently guides continuous and discrete molecular modalities by manipulating velocity fields and predicted logits, respectively. Their guidance scales are jointly optimized using Bayesian optimization to enhance performance. The approach is implemented and benchmarked on the QM9 and QMe14S molecular datasets, demonstrating new state-of-the-art results in aligning generated molecules with target properties. Additionally, the generated molecules maintain high structural validity, addressing a critical aspect of molecular generation. The paper also provides a comprehensive comparison of various guidance methods, discussing their respective strengths and limitations. These insights offer valuable guidance for selecting appropriate techniques based on different molecular generation tasks. Overall, the study advances the field by integrating sophisticated guidance mechanisms to improve controllability and quality in de novo molecular design. <div>
arXiv:2512.12198v1 Announce Type: new 
Abstract: Key objectives in conditional molecular generation include ensuring chemical validity, aligning generated molecules with target properties, promoting structural diversity, and enabling efficient sampling for discovery. Recent advances in computer vision introduced a range of new guidance strategies for generative models, many of which can be adapted to support these goals. In this work, we integrate state-of-the-art guidance methods -- including classifier-free guidance, autoguidance, and model guidance -- in a leading molecule generation framework built on an SE(3)-equivariant flow matching process. We propose a hybrid guidance strategy that separately guides continuous and discrete molecular modalities -- operating on velocity fields and predicted logits, respectively -- while jointly optimizing their guidance scales via Bayesian optimization. Our implementation, benchmarked on the QM9 and QMe14S datasets, achieves new state-of-the-art performance in property alignment for de novo molecular generation. The generated molecules also exhibit high structural validity. Furthermore, we systematically compare the strengths and limitations of various guidance methods, offering insights into their broader applicability.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training</title>
<link>https://arxiv.org/abs/2512.12210</link>
<guid>https://arxiv.org/abs/2512.12210</guid>
<content:encoded><![CDATA[
<div> EEG, data distillation, foundation models, self-supervised learning, EEG-DLite<br /><br />Summary:<br /><br />1. Large-scale EEG foundation models exhibit strong generalization for diverse downstream tasks but require extensive resources for training due to large and variable-quality EEG datasets.  
2. EEG-DLite is introduced as a data distillation framework aimed at improving pre-training efficiency by selectively removing noisy and redundant EEG samples.  
3. The method encodes EEG segments into compact latent representations via a self-supervised autoencoder, enabling efficient and noise-robust sample selection.  
4. EEG-DLite filters out outliers and minimizes redundancy based on latent representations, forming a smaller yet highly informative subset preserving essential data diversity.  
5. Experiments show that training with only 5% of a 2,500-hour EEG dataset curated by EEG-DLite achieves comparable or superior performance relative to training on the full dataset across multiple downstream tasks.  
6. This work represents the first systematic study of pre-training data distillation specifically for EEG foundation models.  
7. EEG-DLite offers a scalable and practical solution for more effective, efficient physiological foundation model training, with code publicly available for community use. <div>
arXiv:2512.12210v1 Announce Type: new 
Abstract: Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at https://github.com/t170815518/EEG-DLite.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimized Learned Count-Min Sketch</title>
<link>https://arxiv.org/abs/2512.12252</link>
<guid>https://arxiv.org/abs/2512.12252</guid>
<content:encoded><![CDATA[
<div> Count-Min Sketch, Learned Count-Min Sketch, Optimized Learned Count-Min Sketch, dynamic programming, theoretical guarantees<br /><br />Summary:<br /><br />Count-Min Sketch (CMS) is a popular memory-efficient data structure for frequency estimation in multisets. Learned Count-Min Sketch (LCMS) improves CMS by integrating machine learning models, lowering estimation errors for the same memory budget, but it suffers from slow build times due to empirical parameter tuning and lacks theoretical error bounds. This paper introduces Optimized Learned Count-Min Sketch (OptLCMS), which divides the input domain into partitions, assigning a separate CMS instance to each segment. Parameters for each CMS are derived analytically based on fixed error thresholds, while the thresholds themselves are optimized efficiently with dynamic programming combined with approximate feasibility checks. This approach reduces reliance on empirical validation, significantly speeding up construction time and providing theoretical guarantees on intolerable error probabilities under given assumptions. OptLCMS further allows explicit control over acceptable error thresholds, making it more adaptable for different practical scenarios. Experimental results demonstrate that OptLCMS constructs sketches faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS, offering improved flexibility and robust performance for frequency estimation tasks. <div>
arXiv:2512.12252v1 Announce Type: new 
Abstract: Count-Min Sketch (CMS) is a memory-efficient data structure for estimating the frequency of elements in a multiset. Learned Count-Min Sketch (LCMS) enhances CMS with a machine learning model to reduce estimation error under the same memory usage, but suffers from slow construction due to empirical parameter tuning and lacks theoretical guarantees on intolerable error probability. We propose Optimized Learned Count-Min Sketch (OptLCMS), which partitions the input domain and assigns each partition to its own CMS instance, with CMS parameters analytically derived for fixed thresholds, and thresholds optimized via dynamic programming with approximate feasibility checks. This reduces the need for empirical validation, enabling faster construction while providing theoretical guarantees under these assumptions. OptLCMS also allows explicit control of the allowable error threshold, improving flexibility in practice. Experiments show that OptLCMS builds faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRC-Net: Gram Residual Co-attention Net for epilepsy prediction</title>
<link>https://arxiv.org/abs/2512.12273</link>
<guid>https://arxiv.org/abs/2512.12273</guid>
<content:encoded><![CDATA[
<div> Keywords: epilepsy prediction, EEG signals, Gram Matrix, multi-level feature extraction, GRC-Net<br /><br />Summary:<br /><br />This article addresses the prediction of epilepsy using EEG signals, a significant and rapidly evolving research area. Unlike traditional approaches that process EEG signals as one-dimensional data, the authors introduce the Gram Matrix method to convert these signals into a three-dimensional representation. This transformation facilitates modeling relationships across multiple dimensions while maintaining the temporal dependencies inherent in EEG data. The study identifies an imbalance between local and global signals within EEG recordings and proposes a multi-level feature extraction approach to address this. Specifically, they apply coattention mechanisms to capture global signal characteristics and use an inception-like structure to handle local signals, enabling multi-granular feature extraction. The proposed network, called GRC-Net, demonstrates superior performance on the BONN dataset, particularly in the challenging task of classifying five different classes of EEG signals related to epilepsy. The model achieves an accuracy of 93.66%, outperforming existing methods. Overall, this work advances the state-of-the-art in epilepsy prediction from EEG by leveraging advanced signal representation and feature extraction techniques to improve classification accuracy. <div>
arXiv:2512.12273v1 Announce Type: new 
Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model</title>
<link>https://arxiv.org/abs/2512.12276</link>
<guid>https://arxiv.org/abs/2512.12276</guid>
<content:encoded><![CDATA[
<div> Machine learning surrogate, Multi-Fidelity Ensemble Kalman Filter, Lorenz-2005 model, Quasi-Geostrophic model, data assimilation<br /><br />Summary:<br /><br />This work explores the integration of machine learning (ML) surrogate models as the low-fidelity component within a Multi-Fidelity Ensemble Kalman Filter (MF-EnKF) framework, aiming to improve computational efficiency and accuracy in data assimilation tasks. Instead of conventional low-resolution or reduced-order models, the low-fidelity model uses ML surrogates that are computationally cheaper but less precise. The method combines a small ensemble of expensive full physical model runs with a large ensemble of these ML-based surrogate runs to maintain or enhance accuracy while adhering to the same computational budget. Performance tests are conducted on two standard dynamical systems: the Lorenz-2005 model and the Quasi-Geostrophic model. Results show that retaining the original physical model within the MF-EnKF produces higher accuracy than fully replacing it with the ML model. Moreover, the MF-EnKF improves accuracy compared to single-fidelity approaches given equivalent computational resources. The ML surrogate models demonstrate either comparable or better accuracy than traditional low-resolution models, accompanied by significant speed-ups. Overall, this approach effectively increases the ensemble size in the EnKF, leading to better estimation of initial conditions and enhanced forecast accuracy in meteorological and oceanographic applications. <div>
arXiv:2512.12276v1 Announce Type: new 
Abstract: Currently, more and more machine learning (ML) surrogates are being developed for computationally expensive physical models. In this work we investigate the use of a Multi-Fidelity Ensemble Kalman Filter (MF-EnKF) in which the low-fidelity model is such a machine learning surrogate model, instead of a traditional low-resolution or reduced-order model. The idea behind this is to use an ensemble of a few expensive full model runs, together with an ensemble of many cheap but less accurate ML model runs. In this way we hope to reach increased accuracy within the same computational budget. We investigate the performance by testing the approach on two common test problems, namely the Lorenz-2005 model and the Quasi-Geostrophic model. By keeping the original physical model in place, we obtain a higher accuracy than when we completely replace it by the ML model. Furthermore, the MF-EnKF reaches improved accuracy within the same computational budget. The ML surrogate has similar or improved accuracy compared to the low-resolution one, but it can provide a larger speed-up. Our method contributes to increasing the effective ensemble size in the EnKF, which improves the estimation of the initial condition and hence accuracy of the predictions in fields such as meteorology and oceanography.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation</title>
<link>https://arxiv.org/abs/2512.12285</link>
<guid>https://arxiv.org/abs/2512.12285</guid>
<content:encoded><![CDATA[
<div> Fractional calculus, Physics-Informed Neural Network, State of Charge estimation, lithium-ion batteries, fractional-order model<br /><br />Summary:<br /><br />This paper addresses the critical need for accurate State of Charge (SOC) estimation in lithium-ion battery systems to ensure safety, reliability, and performance optimization. Conventional data-driven neural networks often fail to accurately capture the complex nonlinear and memory-dependent electrochemical dynamics, limiting their effectiveness under varying operating conditions. To overcome these limitations, the authors propose a novel neural architecture called the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which synergistically combines fractional calculus with deep learning techniques. The main contribution includes the development of a discretized fractional-order partial differential equation derived from a fractional-order equivalent circuit model, enabling more precise modeling of battery dynamics. The study validates the approach through comparative experiments using a dynamic charge/discharge dataset from Panasonic 18650PF batteries tested across multiple temperatures ranging from -10°C to 20°C. The results demonstrate that the FDIFF-PINN model yields improved predictive accuracy and better physical interpretability of the SOC estimation compared to traditional neural network models, addressing complex processes like nonlinearities and history dependence. This integration of fractional calculus within a physics-informed neural network framework offers a promising pathway for optimizing lithium-ion battery management systems under dynamic, real-world conditions. <div>
arXiv:2512.12285v1 Announce Type: new 
Abstract: Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\circ}$C to 20$^{\circ}$C).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2512.12301</link>
<guid>https://arxiv.org/abs/2512.12301</guid>
<content:encoded><![CDATA[
arXiv:2512.12301v1 Announce Type: new 
Abstract: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data</title>
<link>https://arxiv.org/abs/2512.12325</link>
<guid>https://arxiv.org/abs/2512.12325</guid>
<content:encoded><![CDATA[
arXiv:2512.12325v1 Announce Type: new 
Abstract: We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_\alpha$, this regret till time $T$ is bounded by $\ln^2(1/\alpha)/V_T + \ln (1/\alpha) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/\alpha) + \ln \ln V_T$ if $V_T \geq \ln(1/\alpha)$.) If the data were stochastic, then one can show that $E_\alpha$ has probability at least $1-\alpha$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Machine Learning: One Size Does Not Fit All</title>
<link>https://arxiv.org/abs/2512.12341</link>
<guid>https://arxiv.org/abs/2512.12341</guid>
<content:encoded><![CDATA[
arXiv:2512.12341v1 Announce Type: new 
Abstract: Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept</title>
<link>https://arxiv.org/abs/2512.12365</link>
<guid>https://arxiv.org/abs/2512.12365</guid>
<content:encoded><![CDATA[
arXiv:2512.12365v1 Announce Type: new 
Abstract: Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining</title>
<link>https://arxiv.org/abs/2512.12384</link>
<guid>https://arxiv.org/abs/2512.12384</guid>
<content:encoded><![CDATA[
arXiv:2512.12384v1 Announce Type: new 
Abstract: Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment</title>
<link>https://arxiv.org/abs/2512.12387</link>
<guid>https://arxiv.org/abs/2512.12387</guid>
<content:encoded><![CDATA[
arXiv:2512.12387v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields</title>
<link>https://arxiv.org/abs/2512.12402</link>
<guid>https://arxiv.org/abs/2512.12402</guid>
<content:encoded><![CDATA[
arXiv:2512.12402v1 Announce Type: new 
Abstract: We present DeepVekua, a hybrid architecture that unifies geometric deep learning with spectral analysis to solve partial differential equations (PDEs) in sparse data regimes. By learning a diffeomorphic coordinate transformation that maps complex geometries to a latent harmonic space, our method outperforms state-of-the-art implicit representations on advection-diffusion systems. Unlike standard coordinate-based networks which struggle with spectral bias, DeepVekua separates the learning of geometry from the learning of physics, solving for optimal spectral weights in closed form. We demonstrate a 100x improvement over spectral baselines. The code is available at https://github.com/VladimerKhasia/vekuanet.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Graphs Improve Tabular Foundation Models?</title>
<link>https://arxiv.org/abs/2512.12405</link>
<guid>https://arxiv.org/abs/2512.12405</guid>
<content:encoded><![CDATA[
arXiv:2512.12405v1 Announce Type: new 
Abstract: Tabular data are central to many real-world systems. While recent tabular transformers and in-context learners such as SAINT, TP-BERTa, TabPFN, TabICL, and MITRA incorporate limited inter-row reasoning, most approaches still lack an explicit mechanism to model relationships among instances, even though similar samples often share related outcomes. We investigate whether introducing \emph{simple graph priors} can enhance \emph{pretrained tabular transformers}. Concretely, we introduce {BOLERO}, a lightweight, static bipartite graph head that augments {RoBERTa-Tab} (a RoBERTa-style tabular backbone pretrained with masked-token prediction.) Each instance connects to feature/value anchors; a small GNN refines row representations, while the backbone remains frozen. We evaluate on 80 classification and 64 regression datasets from the TP-BERTa benchmark suites, comparing against strong baselines including XGBoost, CatBoost, TabPFN-v2, MITRA, TabICL, TP-BERTa, and RoBERTa-Tab. To ensure statistically sound conclusions, we follow best practices for multi-dataset evaluation: pairwise Wilcoxon signed-rank tests on per-dataset score differences and effect sizes (median improvement with confidence intervals), rather than mean-rank post-hoc tests that depend on the competitor pool. BOLERO achieves the highest number of statistically significant wins across both classification and regression, demonstrating that lightweight graph priors meaningfully improve pretrained tabular transformers.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Dynamics in Memristor-Based Equilibrium Propagation</title>
<link>https://arxiv.org/abs/2512.12428</link>
<guid>https://arxiv.org/abs/2512.12428</guid>
<content:encoded><![CDATA[
arXiv:2512.12428v1 Announce Type: new 
Abstract: Memristor-based in-memory computing has emerged as a promising paradigm to overcome the constraints of the von Neumann bottleneck and the memory wall by enabling fully parallelisable and energy-efficient vector-matrix multiplications. We investigate the effect of nonlinear, memristor-driven weight updates on the convergence behaviour of neural networks trained with equilibrium propagation (EqProp). Six memristor models were characterised by their voltage-current hysteresis and integrated into the EBANA framework for evaluation on two benchmark classification tasks. EqProp can achieve robust convergence under nonlinear weight updates, provided that memristors exhibit a sufficiently wide resistance range of at least an order of magnitude.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rough Sets for Explainability of Spectral Graph Clustering</title>
<link>https://arxiv.org/abs/2512.12436</link>
<guid>https://arxiv.org/abs/2512.12436</guid>
<content:encoded><![CDATA[
arXiv:2512.12436v1 Announce Type: new 
Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction</title>
<link>https://arxiv.org/abs/2512.12445</link>
<guid>https://arxiv.org/abs/2512.12445</guid>
<content:encoded><![CDATA[
arXiv:2512.12445v1 Announce Type: new 
Abstract: Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimized Architectures for Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2512.12448</link>
<guid>https://arxiv.org/abs/2512.12448</guid>
<content:encoded><![CDATA[
arXiv:2512.12448v1 Announce Type: new 
Abstract: Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling</title>
<link>https://arxiv.org/abs/2512.12461</link>
<guid>https://arxiv.org/abs/2512.12461</guid>
<content:encoded><![CDATA[
arXiv:2512.12461v1 Announce Type: new 
Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference</title>
<link>https://arxiv.org/abs/2512.12462</link>
<guid>https://arxiv.org/abs/2512.12462</guid>
<content:encoded><![CDATA[
arXiv:2512.12462v1 Announce Type: new 
Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Design Space of Transition Matching</title>
<link>https://arxiv.org/abs/2512.12465</link>
<guid>https://arxiv.org/abs/2512.12465</guid>
<content:encoded><![CDATA[
arXiv:2512.12465v1 Announce Type: new 
Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Concept Anchoring for Interpretable and Controllable Neural Representations</title>
<link>https://arxiv.org/abs/2512.12469</link>
<guid>https://arxiv.org/abs/2512.12469</guid>
<content:encoded><![CDATA[
arXiv:2512.12469v1 Announce Type: new 
Abstract: We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GoMS: Graph of Molecule Substructure Network for Molecule Property Prediction</title>
<link>https://arxiv.org/abs/2512.12489</link>
<guid>https://arxiv.org/abs/2512.12489</guid>
<content:encoded><![CDATA[
arXiv:2512.12489v1 Announce Type: new 
Abstract: While graph neural networks have shown remarkable success in molecular property prediction, current approaches like the Equivariant Subgraph Aggregation Networks (ESAN) treat molecules as bags of independent substructures, overlooking crucial relationships between these components. We present Graph of Molecule Substructures (GoMS), a novel architecture that explicitly models the interactions and spatial arrangements between molecular substructures. Unlike ESAN's bag-based representation, GoMS constructs a graph where nodes represent subgraphs and edges capture their structural relationships, preserving critical topological information about how substructures are connected and overlap within the molecule. Through extensive experiments on public molecular datasets, we demonstrate that GoMS outperforms ESAN and other baseline methods, with particularly improvements for large molecules containing more than 100 atoms. The performance gap widens as molecular size increases, demonstrating GoMS's effectiveness for modeling industrial-scale molecules. Our theoretical analysis demonstrates that GoMS can distinguish molecules with identical subgraph compositions but different spatial arrangements. Our approach shows particular promise for materials science applications involving complex molecules where properties emerge from the interplay between multiple functional units. By capturing substructure relationships that are lost in bag-based approaches, GoMS represents a significant advance toward scalable and interpretable molecular property prediction for real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models</title>
<link>https://arxiv.org/abs/2512.12493</link>
<guid>https://arxiv.org/abs/2512.12493</guid>
<content:encoded><![CDATA[
arXiv:2512.12493v1 Announce Type: new 
Abstract: Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy Optimization for Dynamic Heart Transplant Allocation</title>
<link>https://arxiv.org/abs/2512.12497</link>
<guid>https://arxiv.org/abs/2512.12497</guid>
<content:encoded><![CDATA[
arXiv:2512.12497v1 Announce Type: new 
Abstract: Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies.
  To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems</title>
<link>https://arxiv.org/abs/2512.12523</link>
<guid>https://arxiv.org/abs/2512.12523</guid>
<content:encoded><![CDATA[
arXiv:2512.12523v1 Announce Type: new 
Abstract: Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling</title>
<link>https://arxiv.org/abs/2512.12526</link>
<guid>https://arxiv.org/abs/2512.12526</guid>
<content:encoded><![CDATA[
arXiv:2512.12526v1 Announce Type: new 
Abstract: This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effective Fine-Tuning with Eigenvector Centrality Based Pruning</title>
<link>https://arxiv.org/abs/2512.12543</link>
<guid>https://arxiv.org/abs/2512.12543</guid>
<content:encoded><![CDATA[
arXiv:2512.12543v1 Announce Type: new 
Abstract: In social media networks a small number of highly influential users can drive large scale changes in discourse across multiple communities. Small shifts in the behavior of these users are often sufficient to propagate widely throughout the network. A similar phenomenon occurs during neural network fine tuning. Conventional fine tuning of convolutional neural networks typically adds a new linear classification layer on top of a large pre trained model. Instead we argue that improved adaptation can be achieved by first pruning the network to retain only the most important neurons and then performing fine tuning.
  We propose a graph theory based method for pruning neural networks that is designed to improve fine tuning performance. In this method each neuron is represented as a node and edges encode similarity between neurons. Neurons are pruned based on importance scores computed using eigenvector centrality. The resulting pruned network is then fine tuned using only the most central neurons. We evaluate the proposed method on VGGNet EfficientNet and ResNet models using the TF Flowers Caltech one zero one and Oxford Flowers one zero two datasets. The proposed approach achieves higher classification accuracy while significantly reducing model complexity. On the Oxford Flowers one zero two dataset the method achieves forty eight percent classification accuracy compared to thirty percent accuracy obtained by the baseline VGGNet model.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model</title>
<link>https://arxiv.org/abs/2512.12545</link>
<guid>https://arxiv.org/abs/2512.12545</guid>
<content:encoded><![CDATA[
arXiv:2512.12545v1 Announce Type: new 
Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Mistake Bounds for Transductive Online Learning</title>
<link>https://arxiv.org/abs/2512.12567</link>
<guid>https://arxiv.org/abs/2512.12567</guid>
<content:encoded><![CDATA[
arXiv:2512.12567v1 Announce Type: new 
Abstract: We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $\Omega(\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $\Omega(\log\log d)$, $\Omega(\sqrt{\log d})$, and $\Omega(\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Accuracy of Newton Step and Influence Function Data Attributions</title>
<link>https://arxiv.org/abs/2512.12572</link>
<guid>https://arxiv.org/abs/2512.12572</guid>
<content:encoded><![CDATA[
arXiv:2512.12572v1 Announce Type: new 
Abstract: Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.
  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as "what is the asymptotic scaling of the errors of each method?" or "which of these methods is more accurate for a given dataset?"
  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.
  \[ \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hat{\theta}_T - \hat{\theta}_T^{\mathrm{NS}}\|_2 \bigr] = \widetilde{\Theta}\!\left(\frac{k d}{n^2}\right), \qquad \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hat{\theta}_T^{\mathrm{NS}} - \hat{\theta}_T^{\mathrm{IF}}\|_2 \bigr] = \widetilde{\Theta}\!\left( \frac{(k + d)\sqrt{k d}}{n^2} \right). \]
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Energy-Based Regularization in GANs: A Simulator-Based Exploration of VQE-Inspired Auxiliary Losses</title>
<link>https://arxiv.org/abs/2512.12581</link>
<guid>https://arxiv.org/abs/2512.12581</guid>
<content:encoded><![CDATA[
arXiv:2512.12581v1 Announce Type: new 
Abstract: This paper presents an exploratory, simulator-based proof of concept investigating whether differentiable energy terms derived from parameterized quantum circuits can serve as auxiliary regularization signals in Generative Adversarial Networks (GANs). We augment the Auxiliary Classifier GAN (ACGAN) generator objective with a Variational Quantum Eigensolver (VQE)-inspired energy term computed from class-specific Ising Hamiltonians using Qiskit's EstimatorQNN and TorchConnector.
  Important limitations: All experiments run on a noiseless statevector simulator with only 4 qubits, use a deliberately simple Hamiltonian parameterization, and lack ablation studies comparing against equivalent classical biases. The computational overhead (approximately 200x slower than classical ACGAN) reflects simulator artifacts rather than inherent quantum costs.
  On MNIST, we observe that the energy-regularized model (termed QACGAN) achieves high classification accuracy (99 to 100 percent) within 5 epochs compared to 87.8 percent for ACGAN, suggesting the auxiliary term influences class conditioning. However, sample quality metrics (FID) show high variance across runs (coefficient of variation approximately 25 percent at epoch 5), with values ranging from 19.92 to 35.96. Extended runs stabilize around FID 23 to 24, comparable to the ACGAN baseline.
  We explicitly do not claim quantum advantage, improved stability in any general sense, or scalability beyond this toy setting. The contribution is methodological: demonstrating that VQE-style energy computations can be integrated into GAN training loops via differentiable pathways. Whether such auxiliary signals provide benefits beyond equivalent classical regularizers remains an open question requiring systematic ablation studies, which we leave for future work.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics</title>
<link>https://arxiv.org/abs/2512.12602</link>
<guid>https://arxiv.org/abs/2512.12602</guid>
<content:encoded><![CDATA[
arXiv:2512.12602v1 Announce Type: new 
Abstract: Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal inference and model explainability tools for retail</title>
<link>https://arxiv.org/abs/2512.12605</link>
<guid>https://arxiv.org/abs/2512.12605</guid>
<content:encoded><![CDATA[
arXiv:2512.12605v1 Announce Type: new 
Abstract: Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain</title>
<link>https://arxiv.org/abs/2512.12617</link>
<guid>https://arxiv.org/abs/2512.12617</guid>
<content:encoded><![CDATA[
arXiv:2512.12617v1 Announce Type: new 
Abstract: Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \ll d$. Under a $(\sigma,f)$ threat model with coordinate-wise honest variance bounded by $\sigma^2$ and $f < 1/2$ adversaries, we prove $(\epsilon,\delta)$-Byzantine resilience with convergence rate $O(\sigma f / \sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $\Omega(\sigma f / \sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.12642</link>
<guid>https://arxiv.org/abs/2512.12642</guid>
<content:encoded><![CDATA[
arXiv:2512.12642v1 Announce Type: new 
Abstract: We introduce Torch Geometric Pool (tgp), a library for hierarchical pooling in Graph Neural Networks. Built upon Pytorch Geometric, Torch Geometric Pool (tgp) provides a wide variety of pooling operators, unified under a consistent API and a modular design. The library emphasizes usability and extensibility, and includes features like precomputed pooling, which significantly accelerate training for a class of operators. In this paper, we present tgp's structure and present an extensive benchmark. The latter showcases the library's features and systematically compares the performance of the implemented graph-pooling methods in different downstream tasks. The results, showing that the choice of the optimal pooling operator depends on tasks and data at hand, support the need for a library that enables fast prototyping.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2512.12663</link>
<guid>https://arxiv.org/abs/2512.12663</guid>
<content:encoded><![CDATA[
arXiv:2512.12663v1 Announce Type: new 
Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization</title>
<link>https://arxiv.org/abs/2512.12669</link>
<guid>https://arxiv.org/abs/2512.12669</guid>
<content:encoded><![CDATA[
arXiv:2512.12669v1 Announce Type: new 
Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Approaches to Building Surrogate ODE Models for Diffusion Bridges</title>
<link>https://arxiv.org/abs/2512.12671</link>
<guid>https://arxiv.org/abs/2512.12671</guid>
<content:encoded><![CDATA[
arXiv:2512.12671v1 Announce Type: new 
Abstract: Diffusion and Schr\"{o}dinger Bridge models have established state-of-the-art performance in generative modeling but are often hampered by significant computational costs and complex training procedures. While continuous-time bridges promise faster sampling, overparameterized neural networks describe their optimal dynamics, and the underlying stochastic differential equations can be difficult to integrate efficiently. This work introduces a novel paradigm that uses surrogate models to create simpler, faster, and more flexible approximations of these dynamics. We propose two specific algorithms: SINDy Flow Matching (SINDy-FM), which leverages sparse regression to identify interpretable, symbolic differential equations from data, and a Neural-ODE reformulation of the Schr\"{o}dinger Bridge (DSBM-NeuralODE) for flexible continuous-time parameterization. Our experiments on Gaussian transport tasks and MNIST latent translation demonstrate that these surrogates achieve competitive performance while offering dramatic improvements in efficiency and interpretability. The symbolic SINDy-FM models, in particular, reduce parameter counts by several orders of magnitude and enable near-instantaneous inference, paving the way for a new class of tractable and high-performing bridge models for practical deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity</title>
<link>https://arxiv.org/abs/2512.12688</link>
<guid>https://arxiv.org/abs/2512.12688</guid>
<content:encoded><![CDATA[
arXiv:2512.12688v1 Announce Type: new 
Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning</title>
<link>https://arxiv.org/abs/2512.12690</link>
<guid>https://arxiv.org/abs/2512.12690</guid>
<content:encoded><![CDATA[
arXiv:2512.12690v1 Announce Type: new 
Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits</title>
<link>https://arxiv.org/abs/2512.12693</link>
<guid>https://arxiv.org/abs/2512.12693</guid>
<content:encoded><![CDATA[
arXiv:2512.12693v1 Announce Type: new 
Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero Terminal Inventory: Optimal Execution on Synthetic &amp; SPY Data</title>
<link>https://arxiv.org/abs/2512.12708</link>
<guid>https://arxiv.org/abs/2512.12708</guid>
<content:encoded><![CDATA[
arXiv:2512.12708v1 Announce Type: new 
Abstract: We study optimal trade execution with a hard-zero terminal inventory constraint, modeled via Hamilton-Jacobi-Bellman (HJB) equations. Vanilla PINNs often under-enforce this constraint and produce unstable controls. We propose a Multi-Trajectory PINN (MT-PINN) that adds a rollout-based trajectory loss and propagates a terminal penalty on terminal inventory via backpropagation-through-time, directly enforcing zero terminal inventory. A lightweight lambda-curriculum is adopted to stabilize training as the state expands from a risk-neutral reduced HJB to a risk-averse HJB. On the Gatheral-Schied single-asset model, MT-PINN aligns closely with their derived closed-form solutions and concentrates terminal inventory tightly around zero while reducing errors along optimal paths. We apply MT-PINNs on SPY intraday data, matching TWAP when risk-neutral, and achieving lower exposure and competitive costs, especially in falling windows, for higher risk-aversion.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving a Machine Learning Regression Problem Based on the Theory of Random Functions</title>
<link>https://arxiv.org/abs/2512.12731</link>
<guid>https://arxiv.org/abs/2512.12731</guid>
<content:encoded><![CDATA[
arXiv:2512.12731v1 Announce Type: new 
Abstract: This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization</title>
<link>https://arxiv.org/abs/2512.12737</link>
<guid>https://arxiv.org/abs/2512.12737</guid>
<content:encoded><![CDATA[
arXiv:2512.12737v1 Announce Type: new 
Abstract: Decentralized federated learning (DFL) faces critical challenges from statistical heterogeneity and communication overhead. While NTK-based methods achieve faster convergence, transmitting full Jacobian matrices is impractical for bandwidth-constrained edge networks. We propose SPARK, synergistically integrating random projection-based Jacobian compression, stage-wise annealed distillation, and Nesterov momentum acceleration. Random projections compress Jacobians while preserving spectral properties essential for convergence. Stage-wise annealed distillation transitions from pure NTK evolution to neighbor-regularized learning, counteracting compression noise. Nesterov momentum accelerates convergence through stable accumulation enabled by distillation smoothing. SPARK achieves 98.7% communication reduction compared to NTK-DFL while maintaining convergence speed and superior accuracy. With momentum, SPARK reaches target performance 3 times faster, establishing state-of-the-art results for communication-efficient decentralized learning and enabling practical deployment in bandwidth-limited edge environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models</title>
<link>https://arxiv.org/abs/2512.12744</link>
<guid>https://arxiv.org/abs/2512.12744</guid>
<content:encoded><![CDATA[
arXiv:2512.12744v1 Announce Type: new 
Abstract: Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Feedback Alignment</title>
<link>https://arxiv.org/abs/2512.12762</link>
<guid>https://arxiv.org/abs/2512.12762</guid>
<content:encoded><![CDATA[
arXiv:2512.12762v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging</title>
<link>https://arxiv.org/abs/2512.12779</link>
<guid>https://arxiv.org/abs/2512.12779</guid>
<content:encoded><![CDATA[
arXiv:2512.12779v1 Announce Type: new 
Abstract: Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces "OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset</title>
<link>https://arxiv.org/abs/2512.12783</link>
<guid>https://arxiv.org/abs/2512.12783</guid>
<content:encoded><![CDATA[
arXiv:2512.12783v1 Announce Type: new 
Abstract: Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 T\"U\.IK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average</title>
<link>https://arxiv.org/abs/2512.12785</link>
<guid>https://arxiv.org/abs/2512.12785</guid>
<content:encoded><![CDATA[
arXiv:2512.12785v1 Announce Type: new 
Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Statistical Significance of Online Regression over Multiple Datasets</title>
<link>https://arxiv.org/abs/2512.12787</link>
<guid>https://arxiv.org/abs/2512.12787</guid>
<content:encoded><![CDATA[
arXiv:2512.12787v1 Announce Type: new 
Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks</title>
<link>https://arxiv.org/abs/2512.12792</link>
<guid>https://arxiv.org/abs/2512.12792</guid>
<content:encoded><![CDATA[
arXiv:2512.12792v1 Announce Type: new 
Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk</title>
<link>https://arxiv.org/abs/2512.12795</link>
<guid>https://arxiv.org/abs/2512.12795</guid>
<content:encoded><![CDATA[
arXiv:2512.12795v1 Announce Type: new 
Abstract: Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs</title>
<link>https://arxiv.org/abs/2512.12805</link>
<guid>https://arxiv.org/abs/2512.12805</guid>
<content:encoded><![CDATA[
arXiv:2512.12805v1 Announce Type: new 
Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift</title>
<link>https://arxiv.org/abs/2512.12816</link>
<guid>https://arxiv.org/abs/2512.12816</guid>
<content:encoded><![CDATA[
arXiv:2512.12816v1 Announce Type: new 
Abstract: We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the continuity of flows</title>
<link>https://arxiv.org/abs/2512.12821</link>
<guid>https://arxiv.org/abs/2512.12821</guid>
<content:encoded><![CDATA[
arXiv:2512.12821v1 Announce Type: new 
Abstract: Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients</title>
<link>https://arxiv.org/abs/2512.12827</link>
<guid>https://arxiv.org/abs/2512.12827</guid>
<content:encoded><![CDATA[
arXiv:2512.12827v1 Announce Type: new 
Abstract: Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future</title>
<link>https://arxiv.org/abs/2512.12832</link>
<guid>https://arxiv.org/abs/2512.12832</guid>
<content:encoded><![CDATA[
arXiv:2512.12832v1 Announce Type: new 
Abstract: Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks</title>
<link>https://arxiv.org/abs/2512.12840</link>
<guid>https://arxiv.org/abs/2512.12840</guid>
<content:encoded><![CDATA[
arXiv:2512.12840v1 Announce Type: new 
Abstract: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word priv\'ee, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Conformal Risk Control</title>
<link>https://arxiv.org/abs/2512.12844</link>
<guid>https://arxiv.org/abs/2512.12844</guid>
<content:encoded><![CDATA[
arXiv:2512.12844v1 Announce Type: new 
Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-Consistent Language Model Recommendations through Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2512.12858</link>
<guid>https://arxiv.org/abs/2512.12858</guid>
<content:encoded><![CDATA[
arXiv:2512.12858v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels</title>
<link>https://arxiv.org/abs/2512.12870</link>
<guid>https://arxiv.org/abs/2512.12870</guid>
<content:encoded><![CDATA[
arXiv:2512.12870v1 Announce Type: new 
Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Recursive Transformers with Mixture of LoRAs</title>
<link>https://arxiv.org/abs/2512.12880</link>
<guid>https://arxiv.org/abs/2512.12880</guid>
<content:encoded><![CDATA[
arXiv:2512.12880v1 Announce Type: new 
Abstract: Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised learning of multiscale switching dynamical system models from multimodal neural data</title>
<link>https://arxiv.org/abs/2512.12881</link>
<guid>https://arxiv.org/abs/2512.12881</guid>
<content:encoded><![CDATA[
arXiv:2512.12881v1 Announce Type: new 
Abstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distillation of Discrete Diffusion by Exact Conditional Distribution Matching</title>
<link>https://arxiv.org/abs/2512.12889</link>
<guid>https://arxiv.org/abs/2512.12889</guid>
<content:encoded><![CDATA[
arXiv:2512.12889v1 Announce Type: new 
Abstract: Discrete diffusion models (DDMs) are a powerful class of generative models for categorical data, but they typically require many function evaluations for a single sample, making inference expensive. Existing acceleration methods either rely on approximate simulators, such as $\tau$-leaping, or on distillation schemes that train new student models and auxiliary networks with proxy objectives. We propose a simple and principled distillation alternative based on \emph{conditional distribution matching}. Our key observation is that the reverse conditional distribution of clean data given a noisy state, $p_{0\mid t}(x_0 \mid x_t)$, admits a Markov decomposition through intermediate times and can be recovered from marginal density ratios and the known forward CTMC kernel. We exploit this structure to define distillation objectives that directly match conditional distributions between a pre-trained teacher and a low-NFE student, both for one-step and few-step samplers.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wait, Wait, Wait... Why Do Reasoning Models Loop?</title>
<link>https://arxiv.org/abs/2512.12895</link>
<guid>https://arxiv.org/abs/2512.12895</guid>
<content:encoded><![CDATA[
arXiv:2512.12895v1 Announce Type: new 
Abstract: Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning</title>
<link>https://arxiv.org/abs/2512.12896</link>
<guid>https://arxiv.org/abs/2512.12896</guid>
<content:encoded><![CDATA[
arXiv:2512.12896v1 Announce Type: new 
Abstract: This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm</title>
<link>https://arxiv.org/abs/2512.12901</link>
<guid>https://arxiv.org/abs/2512.12901</guid>
<content:encoded><![CDATA[
arXiv:2512.12901v1 Announce Type: new 
Abstract: In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-generation reservoir computing validated by classification task</title>
<link>https://arxiv.org/abs/2512.12903</link>
<guid>https://arxiv.org/abs/2512.12903</guid>
<content:encoded><![CDATA[
arXiv:2512.12903v1 Announce Type: new 
Abstract: An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic</title>
<link>https://arxiv.org/abs/2512.12907</link>
<guid>https://arxiv.org/abs/2512.12907</guid>
<content:encoded><![CDATA[
arXiv:2512.12907v1 Announce Type: new 
Abstract: This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization</title>
<link>https://arxiv.org/abs/2512.12922</link>
<guid>https://arxiv.org/abs/2512.12922</guid>
<content:encoded><![CDATA[
arXiv:2512.12922v1 Announce Type: new 
Abstract: In modern financial markets, investors increasingly seek personalized and adaptive portfolio strategies that reflect their individual risk preferences and respond to dynamic market conditions. Traditional rule-based or static optimization approaches often fail to capture the nonlinear interactions among investor behavior, market volatility, and evolving financial objectives. To address these limitations, this paper introduces the LLM-based Personalized Portfolio Recommender , an integrated framework that combines Large Language Models, reinforcement learning, and individualized risk preference modeling to support intelligent investment decision-making.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision</title>
<link>https://arxiv.org/abs/2512.12930</link>
<guid>https://arxiv.org/abs/2512.12930</guid>
<content:encoded><![CDATA[
arXiv:2512.12930v1 Announce Type: new 
Abstract: Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Data Pruning for Pretraining Biological Foundation Models at Scale</title>
<link>https://arxiv.org/abs/2512.12932</link>
<guid>https://arxiv.org/abs/2512.12932</guid>
<content:encoded><![CDATA[
arXiv:2512.12932v1 Announce Type: new 
Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties</title>
<link>https://arxiv.org/abs/2512.12947</link>
<guid>https://arxiv.org/abs/2512.12947</guid>
<content:encoded><![CDATA[
arXiv:2512.12947v1 Announce Type: new 
Abstract: Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of Deep Learning in Biological Data Compression</title>
<link>https://arxiv.org/abs/2512.12975</link>
<guid>https://arxiv.org/abs/2512.12975</guid>
<content:encoded><![CDATA[
arXiv:2512.12975v1 Announce Type: new 
Abstract: Cryogenic electron microscopy (Cryo-EM) has become an essential tool for capturing high-resolution biological structures. Despite its advantage in visualizations, the large storage size of Cryo-EM data file poses significant challenges for researchers and educators. This paper investigates the application of deep learning, specifically implicit neural representation (INR), to compress Cryo-EM biological data. The proposed approach first extracts the binary map of each file according to the density threshold. The density map is highly repetitive, ehich can be effectively compressed by GZIP. The neural network then trains to encode spatial density information, allowing the storage of network parameters and learnable latent vectors. To improve reconstruction accuracy, I further incorporate the positional encoding to enhance spatial representation and a weighted Mean Squared Error (MSE) loss function to balance density distribution variations. Using this approach, my aim is to provide a practical and efficient biological data compression solution that can be used for educational and research purpose, while maintaining a reasonable compression ratio and reconstruction quality from file to file.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks</title>
<link>https://arxiv.org/abs/2512.12981</link>
<guid>https://arxiv.org/abs/2512.12981</guid>
<content:encoded><![CDATA[
arXiv:2512.12981v1 Announce Type: new 
Abstract: While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)</title>
<link>https://arxiv.org/abs/2512.13010</link>
<guid>https://arxiv.org/abs/2512.13010</guid>
<content:encoded><![CDATA[
arXiv:2512.13010v1 Announce Type: new 
Abstract: The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Bidirectional Spans and Span Violations in Attention Mechanism</title>
<link>https://arxiv.org/abs/2512.13033</link>
<guid>https://arxiv.org/abs/2512.13033</guid>
<content:encoded><![CDATA[
arXiv:2512.13033v1 Announce Type: new 
Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization</title>
<link>https://arxiv.org/abs/2512.13034</link>
<guid>https://arxiv.org/abs/2512.13034</guid>
<content:encoded><![CDATA[
arXiv:2512.13034v1 Announce Type: new 
Abstract: This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection</title>
<link>https://arxiv.org/abs/2512.13040</link>
<guid>https://arxiv.org/abs/2512.13040</guid>
<content:encoded><![CDATA[
arXiv:2512.13040v1 Announce Type: new 
Abstract: Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments</title>
<link>https://arxiv.org/abs/2512.13060</link>
<guid>https://arxiv.org/abs/2512.13060</guid>
<content:encoded><![CDATA[
arXiv:2512.13060v1 Announce Type: new 
Abstract: This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-fidelity aerodynamic data fusion by autoencoder transfer learning</title>
<link>https://arxiv.org/abs/2512.13069</link>
<guid>https://arxiv.org/abs/2512.13069</guid>
<content:encoded><![CDATA[
arXiv:2512.13069v1 Announce Type: new 
Abstract: Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LikeBench: Evaluating Subjective Likability in LLMs for Personalization</title>
<link>https://arxiv.org/abs/2512.13077</link>
<guid>https://arxiv.org/abs/2512.13077</guid>
<content:encoded><![CDATA[
arXiv:2512.13077v1 Announce Type: new 
Abstract: A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.13106</link>
<guid>https://arxiv.org/abs/2512.13106</guid>
<content:encoded><![CDATA[
arXiv:2512.13106v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network</title>
<link>https://arxiv.org/abs/2512.13111</link>
<guid>https://arxiv.org/abs/2512.13111</guid>
<content:encoded><![CDATA[
arXiv:2512.13111v1 Announce Type: new 
Abstract: In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quanvolutional Neural Networks for Spectrum Peak-Finding</title>
<link>https://arxiv.org/abs/2512.13125</link>
<guid>https://arxiv.org/abs/2512.13125</guid>
<content:encoded><![CDATA[
arXiv:2512.13125v1 Announce Type: new 
Abstract: The analysis of spectra, such as Nuclear Magnetic Resonance (NMR) spectra, for the comprehensive characterization of peaks is a challenging task for both experts and machines, especially with complex molecules. This process, also known as deconvolution, involves identifying and quantifying the peaks in the spectrum. Machine learning techniques have shown promising results in automating this process. With the advent of quantum computing, there is potential to further enhance these techniques. In this work, inspired by the success of classical Convolutional Neural Networks (CNNs), we explore the use of Quanvolutional Neural Networks (QuanvNNs) for the multi-task peak finding problem, involving both peak counting and position estimation. We implement a simple and interpretable QuanvNN architecture that can be directly compared to its classical CNN counterpart, and evaluate its performance on a synthetic NMR-inspired dataset. Our results demonstrate that QuanvNNs outperform classical CNNs on challenging spectra, achieving an 11\% improvement in F1 score and a 30\% reduction in mean absolute error for peak position estimation. Additionally, QuanvNNs appear to exhibit better convergence stability for harder problems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency</title>
<link>https://arxiv.org/abs/2512.13149</link>
<guid>https://arxiv.org/abs/2512.13149</guid>
<content:encoded><![CDATA[
arXiv:2512.13149v1 Announce Type: new 
Abstract: Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SACn: Soft Actor-Critic with n-step Returns</title>
<link>https://arxiv.org/abs/2512.13165</link>
<guid>https://arxiv.org/abs/2512.13165</guid>
<content:encoded><![CDATA[
arXiv:2512.13165v1 Announce Type: new 
Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $\tau$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning</title>
<link>https://arxiv.org/abs/2512.13186</link>
<guid>https://arxiv.org/abs/2512.13186</guid>
<content:encoded><![CDATA[
arXiv:2512.13186v1 Announce Type: new 
Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory</title>
<link>https://arxiv.org/abs/2512.13190</link>
<guid>https://arxiv.org/abs/2512.13190</guid>
<content:encoded><![CDATA[
arXiv:2512.13190v1 Announce Type: new 
Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning</title>
<link>https://arxiv.org/abs/2512.13196</link>
<guid>https://arxiv.org/abs/2512.13196</guid>
<content:encoded><![CDATA[
arXiv:2512.13196v1 Announce Type: new 
Abstract: Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting</title>
<link>https://arxiv.org/abs/2512.13207</link>
<guid>https://arxiv.org/abs/2512.13207</guid>
<content:encoded><![CDATA[
arXiv:2512.13207v1 Announce Type: new 
Abstract: Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\% degradation) but fails against patch attacks (281-603\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data</title>
<link>https://arxiv.org/abs/2512.13228</link>
<guid>https://arxiv.org/abs/2512.13228</guid>
<content:encoded><![CDATA[
arXiv:2512.13228v1 Announce Type: new 
Abstract: Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE: Contrastive Masked Feature Reconstruction on Graphs</title>
<link>https://arxiv.org/abs/2512.13235</link>
<guid>https://arxiv.org/abs/2512.13235</guid>
<content:encoded><![CDATA[
arXiv:2512.13235v1 Announce Type: new 
Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Retrieve with Weakened Labels: Robust Training under Label Noise</title>
<link>https://arxiv.org/abs/2512.13237</link>
<guid>https://arxiv.org/abs/2512.13237</guid>
<content:encoded><![CDATA[
arXiv:2512.13237v1 Announce Type: new 
Abstract: Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>B\'ezierFlow: B\'ezier Stochastic Interpolant Schedulers for Few-Step Generation</title>
<link>https://arxiv.org/abs/2512.13255</link>
<guid>https://arxiv.org/abs/2512.13255</guid>
<content:encoded><![CDATA[
arXiv:2512.13255v1 Announce Type: new 
Abstract: We introduce B\'ezierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. B\'ezierFlow achieves a 2-3x performance improvement for sampling with $\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as B\'ezier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to B\'ezier control points. Across a range of pretrained diffusion and flow models, B\'ezierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to B\'ezier-based trajectory transformations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction</title>
<link>https://arxiv.org/abs/2512.13300</link>
<guid>https://arxiv.org/abs/2512.13300</guid>
<content:encoded><![CDATA[
arXiv:2512.13300v1 Announce Type: new 
Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning</title>
<link>https://arxiv.org/abs/2512.13316</link>
<guid>https://arxiv.org/abs/2512.13316</guid>
<content:encoded><![CDATA[
arXiv:2512.13316v1 Announce Type: new 
Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers</title>
<link>https://arxiv.org/abs/2512.13336</link>
<guid>https://arxiv.org/abs/2512.13336</guid>
<content:encoded><![CDATA[
arXiv:2512.13336v1 Announce Type: new 
Abstract: This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs</title>
<link>https://arxiv.org/abs/2512.13337</link>
<guid>https://arxiv.org/abs/2512.13337</guid>
<content:encoded><![CDATA[
arXiv:2512.13337v1 Announce Type: new 
Abstract: Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks</title>
<link>https://arxiv.org/abs/2512.13340</link>
<guid>https://arxiv.org/abs/2512.13340</guid>
<content:encoded><![CDATA[
arXiv:2512.13340v1 Announce Type: new 
Abstract: The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models</title>
<link>https://arxiv.org/abs/2512.13352</link>
<guid>https://arxiv.org/abs/2512.13352</guid>
<content:encoded><![CDATA[
arXiv:2512.13352v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction</title>
<link>https://arxiv.org/abs/2512.13381</link>
<guid>https://arxiv.org/abs/2512.13381</guid>
<content:encoded><![CDATA[
arXiv:2512.13381v1 Announce Type: new 
Abstract: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks</title>
<link>https://arxiv.org/abs/2512.13410</link>
<guid>https://arxiv.org/abs/2512.13410</guid>
<content:encoded><![CDATA[
arXiv:2512.13410v1 Announce Type: new 
Abstract: While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2512.13442</link>
<guid>https://arxiv.org/abs/2512.13442</guid>
<content:encoded><![CDATA[
arXiv:2512.13442v1 Announce Type: new 
Abstract: In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy</title>
<link>https://arxiv.org/abs/2512.13458</link>
<guid>https://arxiv.org/abs/2512.13458</guid>
<content:encoded><![CDATA[
arXiv:2512.13458v1 Announce Type: new 
Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems</title>
<link>https://arxiv.org/abs/2512.13460</link>
<guid>https://arxiv.org/abs/2512.13460</guid>
<content:encoded><![CDATA[
arXiv:2512.13460v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Element-wise Modulation of Random Matrices for Efficient Neural Layers</title>
<link>https://arxiv.org/abs/2512.13480</link>
<guid>https://arxiv.org/abs/2512.13480</guid>
<content:encoded><![CDATA[
arXiv:2512.13480v1 Announce Type: new 
Abstract: Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing</title>
<link>https://arxiv.org/abs/2512.13497</link>
<guid>https://arxiv.org/abs/2512.13497</guid>
<content:encoded><![CDATA[
arXiv:2512.13497v1 Announce Type: new 
Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource</title>
<link>https://arxiv.org/abs/2512.13506</link>
<guid>https://arxiv.org/abs/2512.13506</guid>
<content:encoded><![CDATA[
arXiv:2512.13506v1 Announce Type: new 
Abstract: Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Async Control: Stress-testing Asynchronous Control Measures for LLM Agents</title>
<link>https://arxiv.org/abs/2512.13526</link>
<guid>https://arxiv.org/abs/2512.13526</guid>
<content:encoded><![CDATA[
arXiv:2512.13526v1 Announce Type: new 
Abstract: LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability</title>
<link>https://arxiv.org/abs/2512.13568</link>
<guid>https://arxiv.org/abs/2512.13568</guid>
<content:encoded><![CDATA[
arXiv:2512.13568v1 Announce Type: new 
Abstract: Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many "virtual neurons" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication</title>
<link>https://arxiv.org/abs/2512.13583</link>
<guid>https://arxiv.org/abs/2512.13583</guid>
<content:encoded><![CDATA[
arXiv:2512.13583v1 Announce Type: new 
Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}{\delta} \right)}/(\sqrt{n}J\epsilon) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(\epsilon, \delta\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Diffusion Preview with Consistency Solver</title>
<link>https://arxiv.org/abs/2512.13592</link>
<guid>https://arxiv.org/abs/2512.13592</guid>
<content:encoded><![CDATA[
arXiv:2512.13592v1 Announce Type: new 
Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Formal Verification via Autoencoder Latent Space Abstraction</title>
<link>https://arxiv.org/abs/2512.13593</link>
<guid>https://arxiv.org/abs/2512.13593</guid>
<content:encoded><![CDATA[
arXiv:2512.13593v1 Announce Type: new 
Abstract: Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification</title>
<link>https://arxiv.org/abs/2512.13617</link>
<guid>https://arxiv.org/abs/2512.13617</guid>
<content:encoded><![CDATA[
arXiv:2512.13617v1 Announce Type: new 
Abstract: Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion</title>
<link>https://arxiv.org/abs/2512.13632</link>
<guid>https://arxiv.org/abs/2512.13632</guid>
<content:encoded><![CDATA[
arXiv:2512.13632v1 Announce Type: new 
Abstract: Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</title>
<link>https://arxiv.org/abs/2512.13641</link>
<guid>https://arxiv.org/abs/2512.13641</guid>
<content:encoded><![CDATA[
arXiv:2512.13641v1 Announce Type: new 
Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Scientific Reasoning Model for Organic Synthesis Procedure Generation</title>
<link>https://arxiv.org/abs/2512.13668</link>
<guid>https://arxiv.org/abs/2512.13668</guid>
<content:encoded><![CDATA[
arXiv:2512.13668v1 Announce Type: new 
Abstract: Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Directional Textual Inversion for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.13672</link>
<guid>https://arxiv.org/abs/2512.13672</guid>
<content:encoded><![CDATA[
arXiv:2512.13672v1 Announce Type: new 
Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multitask VAE for Time Series Preprocessing and Prediction of Blood Glucose Level</title>
<link>https://arxiv.org/abs/2410.00015</link>
<guid>https://arxiv.org/abs/2410.00015</guid>
<content:encoded><![CDATA[
arXiv:2410.00015v2 Announce Type: cross 
Abstract: Data preprocessing is a critical part of time series data analysis. Data from connected medical devices often have missing or abnormal values during acquisition. Handling such situations requires additional assumptions and domain knowledge. This can be time-consuming, and can introduce a significant bias affecting predictive model accuracy and thus, medical interpretation. To overcome this issue, we propose a new deep learning model to mitigate the preprocessing assumptions. The model architecture relies on a variational auto-encoder (VAE) to produce a preprocessing latent space, and a recurrent VAE to preserve the temporal dynamics of the data. We demonstrate the effectiveness of such an architecture on telemonitoring data to forecast glucose-level of diabetic patients. Our results show an improvement in terms of accuracy with respect of existing state-of-the-art methods and architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning</title>
<link>https://arxiv.org/abs/2511.16619</link>
<guid>https://arxiv.org/abs/2511.16619</guid>
<content:encoded><![CDATA[
arXiv:2511.16619v1 Announce Type: cross 
Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Latent-Space Thinking in LLMs</title>
<link>https://arxiv.org/abs/2512.11816</link>
<guid>https://arxiv.org/abs/2512.11816</guid>
<content:encoded><![CDATA[
arXiv:2512.11816v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models</title>
<link>https://arxiv.org/abs/2512.11835</link>
<guid>https://arxiv.org/abs/2512.11835</guid>
<content:encoded><![CDATA[
arXiv:2512.11835v1 Announce Type: cross 
Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Foundry: A System for Training Foundational Vision AI Models</title>
<link>https://arxiv.org/abs/2512.11837</link>
<guid>https://arxiv.org/abs/2512.11837</guid>
<content:encoded><![CDATA[
arXiv:2512.11837v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spiking Manifesto</title>
<link>https://arxiv.org/abs/2512.11843</link>
<guid>https://arxiv.org/abs/2512.11843</guid>
<content:encoded><![CDATA[
arXiv:2512.11843v1 Announce Type: cross 
Abstract: Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator crunches numbers more energy-efficiently than any human. Yet AI models are a thousand times less efficient than the brain. These models use artificial neural networks (ANNs) and require GPUs for the multiplication of huge matrices. In contrast, spiking neural networks (SNNs) of the brain have no matrix multiplication and much smaller energy requirements. This manifesto proposes a framework for thinking about popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature's way of implementing look-up tables. This offers a way to convert AI models into a novel type of architecture with the promise of a thousandfold improvement in efficiency. Code is available at https://github.com/izhikevich/SNN
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines</title>
<link>https://arxiv.org/abs/2512.11844</link>
<guid>https://arxiv.org/abs/2512.11844</guid>
<content:encoded><![CDATA[
arXiv:2512.11844v1 Announce Type: cross 
Abstract: We propose Love First, Know Later: a paradigm shift in computational matching that simulates interactions first, then assesses compatibility. Instead of comparing static profiles, our framework leverages LLMs as text world engines that operate in dual capacity-as persona-driven agents following behavioral policies and as the environment modeling interaction dynamics. We formalize compatibility assessment as a reward-modeling problem: given observed matching outcomes, we learn to extract signals from simulations that predict human preferences. Our key insight is that relationships hinge on responses to critical moments-we translate this observation from relationship psychology into mathematical hypotheses, enabling effective simulation. Theoretically, we prove that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically, we validate on speed dating data for initial chemistry and divorce prediction for long-term stability. This paradigm enables interactive, personalized matching systems where users iteratively refine their agents, unlocking future possibilities for transparent and interactive compatibility assessment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolving Deep Learning Optimizers</title>
<link>https://arxiv.org/abs/2512.11853</link>
<guid>https://arxiv.org/abs/2512.11853</guid>
<content:encoded><![CDATA[
arXiv:2512.11853v1 Announce Type: cross 
Abstract: We present a genetic algorithm framework for automatically discovering deep learning optimization algorithms. Our approach encodes optimizers as genomes that specify combinations of primitive update terms (gradient, momentum, RMS normalization, Adam-style adaptive terms, and sign-based updates) along with hyperparameters and scheduling options. Through evolutionary search over 50 generations with a population of 50 individuals, evaluated across multiple vision tasks, we discover an evolved optimizer that outperforms Adam by 2.6% in aggregate fitness and achieves a 7.7% relative improvement on CIFAR-10. The evolved optimizer combines sign-based gradient terms with adaptive moment estimation, uses lower momentum coefficients than Adam ($\beta_1$=0.86, $\beta_2$=0.94), and notably disables bias correction while enabling learning rate warmup and cosine decay. Our results demonstrate that evolutionary search can discover competitive optimization algorithms and reveal design principles that differ from hand-crafted optimizers. Code is available at https://github.com/mmarfinetz/evo-optimizer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Art of Storytelling in Authoritarian Regimes: Crafting State Narratives on Chinese Social Media</title>
<link>https://arxiv.org/abs/2512.11875</link>
<guid>https://arxiv.org/abs/2512.11875</guid>
<content:encoded><![CDATA[
arXiv:2512.11875v1 Announce Type: cross 
Abstract: This article examines how authoritarian regimes construct state narratives about politically consequential events. Building on the narrative policy framework and existing research on authoritarian propaganda, we propose two dimensions that shape narrative construction: legitimacy implications -- whether events enhance or threaten regime legitimacy, and citizen verification capacity -- the extent to which citizens can evaluate official narratives through alternative sources. Using quantitative narrative analysis of Chinese social media posts by government, state media, and celebrity accounts, we extract subject-verb-object (SVO) triplets to map dominant narrative structures across four major events. Our findings show that legitimacy implications of the event shape regime's efforts in storytelling and the beliefs highlighted in the narratives, while citizen's verification capacity could balance the strategic choice between a top-down manipulation and bottom-up responsiveness of state narratives. Together, the results reveal propaganda as a complex process of narrative construction adaptive to specific contexts, offering new insights into how dynamic storytelling sustains authoritarian resilience.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Structural Representation in Foundation Models for Polymers</title>
<link>https://arxiv.org/abs/2512.11881</link>
<guid>https://arxiv.org/abs/2512.11881</guid>
<content:encoded><![CDATA[
arXiv:2512.11881v1 Announce Type: cross 
Abstract: From the relative scarcity of training data to the lack of standardized benchmarks, the development of foundation models for polymers face significant and multi-faceted challenges. At the core, many of these issues are tied directly to the structural representation of polymers and here, we present a new foundation model using a SMILES-based polymer graph representation. This approach allows representation of critical polymer architectural features and connectivity that are not available in other SMILES-based representations. The developed polymer foundation model exhibited excellent performance on 28 different benchmark datasets. Critical evaluation of the developed representation against other variations in control experiments reveals this approach to be a highly performant method of representing polymers in language-based foundation models. These control experiments also reveal a strong invariance of all SMILES representations, with many variations achieving state-of-the-art or near state-of-the-art performance, including those which are chemically or semantically invalid. Examination of error sources and attention maps for the evaluated representations corroborate the findings of the control experiments, showing that chemistry language models based on SMILES interpolate over all sequence space for prediction tasks, not only those of semantically valid inputs. Overall, this work highlights the importance of control experiments as a check on human-imposed assumptions that can limit rational design of both chemistry foundation models and their underlying structural representations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description</title>
<link>https://arxiv.org/abs/2512.11894</link>
<guid>https://arxiv.org/abs/2512.11894</guid>
<content:encoded><![CDATA[
arXiv:2512.11894v1 Announce Type: cross 
Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities</title>
<link>https://arxiv.org/abs/2512.11901</link>
<guid>https://arxiv.org/abs/2512.11901</guid>
<content:encoded><![CDATA[
arXiv:2512.11901v1 Announce Type: cross 
Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.11902</link>
<guid>https://arxiv.org/abs/2512.11902</guid>
<content:encoded><![CDATA[
arXiv:2512.11902v1 Announce Type: cross 
Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MPath: Multimodal Pathology Report Generation from Whole Slide Images</title>
<link>https://arxiv.org/abs/2512.11906</link>
<guid>https://arxiv.org/abs/2512.11906</guid>
<content:encoded><![CDATA[
arXiv:2512.11906v1 Announce Type: cross 
Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interval Fisher's Discriminant Analysis and Visualisation</title>
<link>https://arxiv.org/abs/2512.11945</link>
<guid>https://arxiv.org/abs/2512.11945</guid>
<content:encoded><![CDATA[
arXiv:2512.11945v1 Announce Type: cross 
Abstract: In Data Science, entities are typically represented by single valued measurements. Symbolic Data Analysis extends this framework to more complex structures, such as intervals and histograms, that express internal variability. We propose an extension of multiclass Fisher's Discriminant Analysis to interval-valued data, using Moore's interval arithmetic and the Mallows' distance. Fisher's objective function is generalised to consider simultaneously the contributions of the centres and the ranges of intervals and is numerically maximised. The resulting discriminant directions are then used to classify interval-valued observations.To support visual assessment, we adapt the class map, originally introduced for conventional data, to classifiers that assign labels through minimum distance rules. We also extend the silhouette plot to this setting and use stacked mosaic plots to complement the visual display of class assignments. Together, these graphical tools provide insight into classifier performance and the strength of class membership. Applications to real datasets illustrate the proposed methodology and demonstrate its value in interpreting classification results for interval-valued data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic search for 100M+ galaxy images using AI-generated captions</title>
<link>https://arxiv.org/abs/2512.11982</link>
<guid>https://arxiv.org/abs/2512.11982</guid>
<content:encoded><![CDATA[
arXiv:2512.11982v1 Announce Type: cross 
Abstract: Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy Gradient Algorithms for Age-of-Information Cost Minimization</title>
<link>https://arxiv.org/abs/2512.11990</link>
<guid>https://arxiv.org/abs/2512.11990</guid>
<content:encoded><![CDATA[
arXiv:2512.11990v1 Announce Type: cross 
Abstract: Recent developments in cyber-physical systems have increased the importance of maximizing the freshness of the information about the physical environment. However, optimizing the access policies of Internet of Things devices to maximize the data freshness, measured as a function of the Age-of-Information (AoI) metric, is a challenging task. This work introduces two algorithms to optimize the information update process in cyber-physical systems operating under the generate-at-will model, by finding an online policy without knowing the characteristics of the transmission delay or the age cost function. The optimization seeks to minimize the time-average cost, which integrates the AoI at the receiver and the data transmission cost, making the approach suitable for a broad range of scenarios. Both algorithms employ policy gradient methods within the framework of model-free reinforcement learning (RL) and are specifically designed to handle continuous state and action spaces. Each algorithm minimizes the cost using a distinct strategy for deciding when to send an information update. Moreover, we demonstrate that it is feasible to apply the two strategies simultaneously, leading to an additional reduction in cost. The results demonstrate that the proposed algorithms exhibit good convergence properties and achieve a time-average cost within 3% of the optimal value, when the latter is computable. A comparison with other state-of-the-art methods shows that the proposed algorithms outperform them in one or more of the following aspects: being applicable to a broader range of scenarios, achieving a lower time-average cost, and requiring a computational cost at least one order of magnitude lower.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions</title>
<link>https://arxiv.org/abs/2512.11995</link>
<guid>https://arxiv.org/abs/2512.11995</guid>
<content:encoded><![CDATA[
arXiv:2512.11995v1 Announce Type: cross 
Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Attacks Against Deep Learning-Based Radio Frequency Fingerprint Identification</title>
<link>https://arxiv.org/abs/2512.12002</link>
<guid>https://arxiv.org/abs/2512.12002</guid>
<content:encoded><![CDATA[
arXiv:2512.12002v1 Announce Type: cross 
Abstract: Radio frequency fingerprint identification (RFFI) is an emerging technique for the lightweight authentication of wireless Internet of things (IoT) devices. RFFI exploits deep learning models to extract hardware impairments to uniquely identify wireless devices. Recent studies show deep learning-based RFFI is vulnerable to adversarial attacks. However, effective adversarial attacks against different types of RFFI classifiers have not yet been explored. In this paper, we carried out a comprehensive investigations into different adversarial attack methods on RFFI systems using various deep learning models. Three specific algorithms, fast gradient sign method (FGSM), projected gradient descent (PGD), and universal adversarial perturbation (UAP), were analyzed. The attacks were launched to LoRa-RFFI and the experimental results showed the generated perturbations were effective against convolutional neural networks (CNNs), long short-term memory (LSTM) networks, and gated recurrent units (GRU). We further used UAP to launch practical attacks. Special factors were considered for the wireless context, including implementing real-time attacks, the effectiveness of the attacks over a period of time, etc. Our experimental evaluation demonstrated that UAP can successfully launch adversarial attacks against the RFFI, achieving a success rate of 81.7% when the adversary almost has no prior knowledge of the victim RFFI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.12013</link>
<guid>https://arxiv.org/abs/2512.12013</guid>
<content:encoded><![CDATA[
arXiv:2512.12013v1 Announce Type: cross 
Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos</title>
<link>https://arxiv.org/abs/2512.12060</link>
<guid>https://arxiv.org/abs/2512.12060</guid>
<content:encoded><![CDATA[
arXiv:2512.12060v1 Announce Type: cross 
Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title>
<link>https://arxiv.org/abs/2512.12069</link>
<guid>https://arxiv.org/abs/2512.12069</guid>
<content:encoded><![CDATA[
arXiv:2512.12069v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs</title>
<link>https://arxiv.org/abs/2512.12072</link>
<guid>https://arxiv.org/abs/2512.12072</guid>
<content:encoded><![CDATA[
arXiv:2512.12072v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models</title>
<link>https://arxiv.org/abs/2512.12080</link>
<guid>https://arxiv.org/abs/2512.12080</guid>
<content:encoded><![CDATA[
arXiv:2512.12080v1 Announce Type: cross 
Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations</title>
<link>https://arxiv.org/abs/2512.12088</link>
<guid>https://arxiv.org/abs/2512.12088</guid>
<content:encoded><![CDATA[
arXiv:2512.12088v1 Announce Type: cross 
Abstract: In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPDMark: Selective Parameter Displacement for Robust Video Watermarking</title>
<link>https://arxiv.org/abs/2512.12090</link>
<guid>https://arxiv.org/abs/2512.12090</guid>
<content:encoded><![CDATA[
arXiv:2512.12090v1 Announce Type: cross 
Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging</title>
<link>https://arxiv.org/abs/2512.12101</link>
<guid>https://arxiv.org/abs/2512.12101</guid>
<content:encoded><![CDATA[
arXiv:2512.12101v1 Announce Type: cross 
Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Patch-Based TDA Approach for Computed Tomography</title>
<link>https://arxiv.org/abs/2512.12108</link>
<guid>https://arxiv.org/abs/2512.12108</guid>
<content:encoded><![CDATA[
arXiv:2512.12108v1 Announce Type: cross 
Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context</title>
<link>https://arxiv.org/abs/2512.12117</link>
<guid>https://arxiv.org/abs/2512.12117</guid>
<content:encoded><![CDATA[
arXiv:2512.12117v1 Announce Type: cross 
Abstract: Large language models have become essential tools for code comprehension, enabling developers to query unfamiliar codebases through natural language interfaces. However, LLM hallucination, generating plausible but factually incorrect citations to source code, remains a critical barrier to reliable developer assistance. This paper addresses the challenges of achieving verifiable, citation grounded code comprehension through hybrid retrieval and lightweight structural reasoning. Our work is grounded in systematic evaluation across 30 Python repositories with 180 developer queries, comparing retrieval modalities, graph expansion strategies, and citation verification mechanisms. We find that challenges of citation accuracy arise from the interplay between sparse lexical matching, dense semantic similarity, and cross file architectural dependencies. Among these, cross file evidence discovery is the largest contributor to citation completeness, but it is largely overlooked because existing systems rely on pure textual similarity without leveraging code structure. We advocate for citation grounded generation as an architectural principle for code comprehension systems and demonstrate this need by achieving 92 percent citation accuracy with zero hallucinations. Specifically, we develop a hybrid retrieval system combining BM25 sparse matching, BGE dense embeddings, and Neo4j graph expansion via import relationships, which outperforms single mode baselines by 14 to 18 percentage points while discovering cross file evidence missed by pure text similarity in 62 percent of architectural queries.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Dabrafenib Response Using Multi-Omics Modality Fusion and Protein Network Embeddings Based on Graph Convolutional Networks</title>
<link>https://arxiv.org/abs/2512.12134</link>
<guid>https://arxiv.org/abs/2512.12134</guid>
<content:encoded><![CDATA[
arXiv:2512.12134v1 Announce Type: cross 
Abstract: Cancer cell response to targeted therapy arises from complex molecular interactions, making single omics insufficient for accurate prediction. This study develops a model to predict Dabrafenib sensitivity by integrating multiple omics layers (genomics, transcriptomics, proteomics, epigenomics, and metabolomics) with protein network embeddings generated using Graph Convolutional Networks (GCN). Each modality is encoded into low dimensional representations through neural network preprocessing. Protein interaction information from STRING is incorporated using GCN to capture biological topology. An attention based fusion mechanism assigns adaptive weights to each modality according to its relevance. Using GDSC cancer cell line data, the model shows that selective integration of two modalities, especially proteomics and transcriptomics, achieves the best test performance (R2 around 0.96), outperforming all single omics and full multimodal settings. Genomic and epigenomic data were less informative, while proteomic and transcriptomic layers provided stronger phenotypic signals related to MAPK inhibitor activity. These results show that attention guided multi omics fusion combined with GCN improves drug response prediction and reveals complementary molecular determinants of Dabrafenib sensitivity. The approach offers a promising computational framework for precision oncology and predictive modeling of targeted therapies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater</title>
<link>https://arxiv.org/abs/2512.12142</link>
<guid>https://arxiv.org/abs/2512.12142</guid>
<content:encoded><![CDATA[
arXiv:2512.12142v1 Announce Type: cross 
Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Keep the Lights On, Keep the Lengths in Check: Plug-In Adversarial Detection for Time-Series LLMs in Energy Forecasting</title>
<link>https://arxiv.org/abs/2512.12154</link>
<guid>https://arxiv.org/abs/2512.12154</guid>
<content:encoded><![CDATA[
arXiv:2512.12154v1 Announce Type: cross 
Abstract: Accurate time-series forecasting is increasingly critical for planning and operations in low-carbon power systems. Emerging time-series large language models (TS-LLMs) now deliver this capability at scale, requiring no task-specific retraining, and are quickly becoming essential components within the Internet-of-Energy (IoE) ecosystem. However, their real-world deployment is complicated by a critical vulnerability: adversarial examples (AEs). Detecting these AEs is challenging because (i) adversarial perturbations are optimized across the entire input sequence and exploit global temporal dependencies, which renders local detection methods ineffective, and (ii) unlike traditional forecasting models with fixed input dimensions, TS-LLMs accept sequences of variable length, increasing variability that complicates detection. To address these challenges, we propose a plug-in detection framework that capitalizes on the TS-LLM's own variable-length input capability. Our method uses sampling-induced divergence as a detection signal. Given an input sequence, we generate multiple shortened variants and detect AEs by measuring the consistency of their forecasts: Benign sequences tend to produce stable predictions under sampling, whereas adversarial sequences show low forecast similarity, because perturbations optimized for a full-length sequence do not transfer reliably to shorter, differently-structured subsamples. We evaluate our approach on three representative TS-LLMs (TimeGPT, TimesFM, and TimeLLM) across three energy datasets: ETTh2 (Electricity Transformer Temperature), NI (Hourly Energy Consumption), and Consumption (Hourly Electricity Consumption and Production). Empirical results confirm strong and robust detection performance across both black-box and white-box attack scenarios, highlighting its practicality as a reliable safeguard for TS-LLM forecasting in real-world energy systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB</title>
<link>https://arxiv.org/abs/2512.12206</link>
<guid>https://arxiv.org/abs/2512.12206</guid>
<content:encoded><![CDATA[
arXiv:2512.12206v1 Announce Type: cross 
Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking</title>
<link>https://arxiv.org/abs/2512.12218</link>
<guid>https://arxiv.org/abs/2512.12218</guid>
<content:encoded><![CDATA[
arXiv:2512.12218v1 Announce Type: cross 
Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy</title>
<link>https://arxiv.org/abs/2512.12230</link>
<guid>https://arxiv.org/abs/2512.12230</guid>
<content:encoded><![CDATA[
arXiv:2512.12230v1 Announce Type: cross 
Abstract: Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&amp;P 500 Index Volatility Forecasting</title>
<link>https://arxiv.org/abs/2512.12250</link>
<guid>https://arxiv.org/abs/2512.12250</guid>
<content:encoded><![CDATA[
arXiv:2512.12250v1 Announce Type: cross 
Abstract: Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hellinger loss function for Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2512.12267</link>
<guid>https://arxiv.org/abs/2512.12267</guid>
<content:encoded><![CDATA[
arXiv:2512.12267v1 Announce Type: cross 
Abstract: We propose Hellinger-type loss functions for training Generative Adversarial Networks (GANs), motivated by the boundedness, symmetry, and robustness properties of the Hellinger distance. We define an adversarial objective based on this divergence and study its statistical properties within a general parametric framework. We establish the existence, uniqueness, consistency, and joint asymptotic normality of the estimators obtained from the adversarial training procedure. In particular, we analyze the joint estimation of both generator and discriminator parameters, offering a comprehensive asymptotic characterization of the resulting estimators. We introduce two implementations of the Hellinger-type loss and we evaluate their empirical behavior in comparison with the classic (Maximum Likelihood-type) GAN loss. Through a controlled simulation study, we demonstrate that both proposed losses yield improved estimation accuracy and robustness under increasing levels of data contamination.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Outlier Detection and Low-Latency Concept Drift Adaptation for Data Stream Regression: A Dual-Channel Architecture</title>
<link>https://arxiv.org/abs/2512.12289</link>
<guid>https://arxiv.org/abs/2512.12289</guid>
<content:encoded><![CDATA[
arXiv:2512.12289v1 Announce Type: cross 
Abstract: Outlier detection and concept drift detection represent two challenges in data analysis. Most studies address these issues separately. However, joint detection mechanisms in regression remain underexplored, where the continuous nature of output spaces makes distinguishing drifts from outliers inherently challenging. To address this, we propose a novel robust regression framework for joint outlier and concept drift detection. Specifically, we introduce a dual-channel decision process that orchestrates prediction residuals into two coupled logic flows: a rapid response channel for filtering point outliers and a deep analysis channel for diagnosing drifts. We further develop the Exponentially Weighted Moving Absolute Deviation with Distinguishable Types (EWMAD-DT) detector to autonomously differentiate between abrupt and incremental drifts via dynamic thresholding. Comprehensive experiments on both synthetic and real-world datasets demonstrate that our unified framework, enhanced by EWMAD-DT, exhibits superior detection performance even when point outliers and concept drifts coexist.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates</title>
<link>https://arxiv.org/abs/2512.12295</link>
<guid>https://arxiv.org/abs/2512.12295</guid>
<content:encoded><![CDATA[
arXiv:2512.12295v1 Announce Type: cross 
Abstract: Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak <= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (<2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact <20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search</title>
<link>https://arxiv.org/abs/2512.12296</link>
<guid>https://arxiv.org/abs/2512.12296</guid>
<content:encoded><![CDATA[
arXiv:2512.12296v1 Announce Type: cross 
Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending the application of dynamic Bayesian networks in calculating market risk: Standard and stressed expected shortfall</title>
<link>https://arxiv.org/abs/2512.12334</link>
<guid>https://arxiv.org/abs/2512.12334</guid>
<content:encoded><![CDATA[
arXiv:2512.12334v1 Announce Type: cross 
Abstract: In the last five years, expected shortfall (ES) and stressed ES (SES) have become key required regulatory measures of market risk in the banking sector, especially following events such as the global financial crisis. Thus, finding ways to optimize their estimation is of great importance. We extend the application of dynamic Bayesian networks (DBNs) to the estimation of 10-day 97.5% ES and stressed ES, building on prior work applying DBNs to value at risk. Using the S&amp;P 500 index as a proxy for the equities trading desk of a US bank, we compare the performance of three DBN structure-learning algorithms with several traditional market risk models, using either the normal or the skewed Student's t return distributions. Backtesting shows that all models fail to produce statistically accurate ES and SES forecasts at the 2.5% level, reflecting the difficulty of modeling extreme tail behavior. For ES, the EGARCH(1,1) model (normal) produces the most accurate forecasts, while, for SES, the GARCH(1,1) model (normal) performs best. All distribution-dependent models deteriorate substantially when using the skewed Student's t distribution. The DBNs perform comparably to the historical simulation model, but their contribution to tail prediction is limited by the small weight assigned to their one-day-ahead forecasts within the return distribution. Future research should examine weighting schemes that enhance the influence of forward-looking DBN forecasts on tail risk estimation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Control for Inference-Time Guidance of Denoising Diffusion Models</title>
<link>https://arxiv.org/abs/2512.12339</link>
<guid>https://arxiv.org/abs/2512.12339</guid>
<content:encoded><![CDATA[
arXiv:2512.12339v1 Announce Type: cross 
Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a pretrained deep learning estimator of the Linfoot informational correlation</title>
<link>https://arxiv.org/abs/2512.12358</link>
<guid>https://arxiv.org/abs/2512.12358</guid>
<content:encoded><![CDATA[
arXiv:2512.12358v1 Announce Type: cross 
Abstract: We develop a supervised deep-learning approach to estimate mutual information between two continuous random variables. As labels, we use the Linfoot informational correlation, a transformation of mutual information that has many important properties. Our method is based on ground truth labels for Gaussian and Clayton copulas. We compare our method with estimators based on kernel density, k-nearest neighbours and neural estimators. We show generally lower bias and lower variance. As a proof of principle, future research could look into training the model with a more diverse set of examples from other copulas for which ground truth labels are available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ElasticVR: Elastic Task Computing in Multi-User Multi-Connectivity Wireless Virtual Reality (VR) Systems</title>
<link>https://arxiv.org/abs/2512.12366</link>
<guid>https://arxiv.org/abs/2512.12366</guid>
<content:encoded><![CDATA[
arXiv:2512.12366v1 Announce Type: cross 
Abstract: Diverse emerging VR applications integrate streaming of high fidelity 360 video content that requires ample amounts of computation and data rate. Scalable 360 video tiling enables having elastic VR computational tasks that can be scaled adaptively in computation and data rate based on the available user and system resources. We integrate scalable 360 video tiling in an edge-client wireless multi-connectivity architecture for joint elastic task computation offloading across multiple VR users called ElasticVR. To balance the trade-offs in communication, computation, energy consumption, and QoE that arise herein, we formulate a constrained QoE and energy optimization problem that integrates the multi-user/multi-connectivity action space with the elasticity of VR computational tasks. The ElasticVR framework introduces two multi-agent deep reinforcement learning solutions, namely CPPG and IPPG. CPPG adopts a centralized training and centralized execution approach to capture the coupling between users' communication and computational demands. This leads to globally coordinated decisions at the cost of increased computational overheads and limited scalability. To address the latter challenges, we also explore an alternative strategy denoted IPPG that adopts a centralized training with decentralized execution paradigm. IPPG leverages shared information and parameter sharing to learn robust policies; however, during execution, each user takes action independently based on its local state information only. The decentralized execution alleviates the communication and computation overhead of centralized decision-making and improves scalability. We show that the ElasticVR framework improves the PSNR by 43.21%, while reducing the response time and energy consumption by 42.35% and 56.83%, respectively, compared with a case where no elasticity is incorporated into VR computations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics</title>
<link>https://arxiv.org/abs/2512.12424</link>
<guid>https://arxiv.org/abs/2512.12424</guid>
<content:encoded><![CDATA[
arXiv:2512.12424v1 Announce Type: cross 
Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven modelling of autonomous and forced dynamical systems</title>
<link>https://arxiv.org/abs/2512.12432</link>
<guid>https://arxiv.org/abs/2512.12432</guid>
<content:encoded><![CDATA[
arXiv:2512.12432v1 Announce Type: cross 
Abstract: The paper demonstrates that invariant foliations are accurate, data-efficient and practical tools for data-driven modelling of physical systems. Invariant foliations can be fitted to data that either fill the phase space or cluster about an invariant manifold. Invariant foliations can be fitted to a single trajectory or multiple trajectories. Over and underfitting are eliminated by appropriately choosing a function representation and its hyperparameters, such as polynomial orders. The paper extends invariant foliations to forced and parameter dependent systems. It is assumed that forcing is provided by a volume preserving map, and therefore the forcing can be periodic, quasi-periodic or even chaotic. The method utilises full trajectories, hence it is able to predict long-term dynamics accurately. We take into account if a forced system is reducible to an autonomous system about a steady state, similar to how Floquet theory guarantees reducibility for periodically forced systems. In order to find an invariant manifold, multiple invariant foliations are calculated in the neighbourhood of the invariant manifold. Some of the invariant foliations can be linear, while others nonlinear but only defined in a small neighbourhood of an invariant manifold, which reduces the number of parameters to be identified. An invariant manifold is recovered as the zero level set of one or more of the foliations. To interpret the results, the identified mathematical models are transformed to a canonical form and instantaneous frequency and damping information are calculated.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Hub Node Based Multiview Graph Learning with Theoretical Guarantees</title>
<link>https://arxiv.org/abs/2512.12435</link>
<guid>https://arxiv.org/abs/2512.12435</guid>
<content:encoded><![CDATA[
arXiv:2512.12435v1 Announce Type: cross 
Abstract: Identifying the graphical structure underlying the observed multivariate data is essential in numerous applications. Current methodologies are predominantly confined to deducing a singular graph under the presumption that the observed data are uniform. However, many contexts involve heterogeneous datasets that feature multiple closely related graphs, typically referred to as multiview graphs. Previous research on multiview graph learning promotes edge-based similarity across layers using pairwise or consensus-based regularizers. However, multiview graphs frequently exhibit a shared node-based architecture across different views, such as common hub nodes. Such commonalities can enhance the precision of learning and provide interpretive insight. In this paper, we propose a co-hub node model, positing that different views share a common group of hub nodes. The associated optimization framework is developed by enforcing structured sparsity on the connections of these co-hub nodes. Moreover, we present a theoretical examination of layer identifiability and determine bounds on estimation error. The proposed methodology is validated using both synthetic graph data and fMRI time series data from multiple subjects to discern several closely related graphs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Level-Crossing Probability Calculation for Gaussian Process Modeled Data</title>
<link>https://arxiv.org/abs/2512.12442</link>
<guid>https://arxiv.org/abs/2512.12442</guid>
<content:encoded><![CDATA[
arXiv:2512.12442v1 Announce Type: cross 
Abstract: Almost all scientific data have uncertainties originating from different sources. Gaussian process regression (GPR) models are a natural way to model data with Gaussian-distributed uncertainties. GPR also has the benefit of reducing I/O bandwidth and storage requirements for large scientific simulations. However, the reconstruction from the GPR models suffers from high computation complexity. To make the situation worse, classic approaches for visualizing the data uncertainties, like probabilistic marching cubes, are also computationally very expensive, especially for data of high resolutions. In this paper, we accelerate the level-crossing probability calculation efficiency on GPR models by subdividing the data spatially into a hierarchical data structure and only reconstructing values adaptively in the regions that have a non-zero probability. For each region, leveraging the known GPR kernel and the saved data observations, we propose a novel approach to efficiently calculate an upper bound for the level-crossing probability inside the region and use this upper bound to make the subdivision and reconstruction decisions. We demonstrate that our value occurrence probability estimation is accurate with a low computation cost by experiments that calculate the level-crossing probability fields on different datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval</title>
<link>https://arxiv.org/abs/2512.12458</link>
<guid>https://arxiv.org/abs/2512.12458</guid>
<content:encoded><![CDATA[
arXiv:2512.12458v1 Announce Type: cross 
Abstract: Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Overparametrization in Survival Models through Double-Descent</title>
<link>https://arxiv.org/abs/2512.12463</link>
<guid>https://arxiv.org/abs/2512.12463</guid>
<content:encoded><![CDATA[
arXiv:2512.12463v1 Announce Type: cross 
Abstract: Classical statistical learning theory predicts a U-shaped relationship between test loss and model capacity, driven by the bias-variance trade-off. Recent advances in modern machine learning have revealed a more complex pattern, double-descent, in which test loss, after peaking near the interpolation threshold, decreases again as model capacity continues to grow. While this behavior has been extensively analyzed in regression and classification, its manifestation in survival analysis remains unexplored. This study investigates double-descent in four representative survival models: DeepSurv, PC-Hazard, Nnet-Survival, and N-MTLR. We rigorously define interpolation and finite-norm interpolation, two key characteristics of loss-based models to understand double-descent. We then show the existence (or absence) of (finite-norm) interpolation of all four models. Our findings clarify how likelihood-based losses and model implementation jointly determine the feasibility of interpolation and show that overfitting should not be regarded as benign for survival models. All theoretical results are supported by numerical experiments that highlight the distinct generalization behaviors of survival models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Driven Real-Time Kick Classification in Olympic Taekwondo Using Sensor Fusion</title>
<link>https://arxiv.org/abs/2512.12474</link>
<guid>https://arxiv.org/abs/2512.12474</guid>
<content:encoded><![CDATA[
arXiv:2512.12474v1 Announce Type: cross 
Abstract: Olympic Taekwondo has faced challenges in spectator engagement due to static, defensive gameplay and contentious scoring. Current Protector and Scoring Systems (PSS) rely on impact sensors and simplistic logic, encouraging safe strategies that diminish the sport's dynamism. This paper proposes an AI-powered scoring system that integrates existing PSS sensors with additional accelerometers, gyroscopes, magnetic/RFID, and impact force sensors in a sensor fusion framework. The system classifies kicks in real-time to identify technique type, contact location, impact force, and even the part of the foot used. A machine learning pipeline employing sensor fusion and Support Vector Machines (SVMs) is detailed, enabling automatic kick technique recognition for scoring. We present a novel kick scoring rubric that awards points based on specific kick techniques (e.g., turning and spinning kicks) to incentivize dynamic attacks. Drawing on a 2024 study achieving 96-98% accuracy, we validate the feasibility of real-time kick classification and further propose enhancements to this methodology, such as ensemble SVM classifiers and expanded datasets, to achieve the high-stakes accuracy required by the sport. We analyze how the proposed system can improve scoring fairness, reduce rule exploitation and illegitimate tactics, encourage more dynamic techniques, and enhance spectator understanding and excitement. The paper includes system design illustrations, a kick scoring table from an AI-augmented rule set, and discusses anticipated impacts on Olympic Taekwondo.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs</title>
<link>https://arxiv.org/abs/2512.12477</link>
<guid>https://arxiv.org/abs/2512.12477</guid>
<content:encoded><![CDATA[
arXiv:2512.12477v1 Announce Type: cross 
Abstract: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Spatiotemporal Data Augmentation</title>
<link>https://arxiv.org/abs/2512.12508</link>
<guid>https://arxiv.org/abs/2512.12508</guid>
<content:encoded><![CDATA[
arXiv:2512.12508v1 Announce Type: cross 
Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Animus3D: Text-driven 3D Animation via Motion Score Distillation</title>
<link>https://arxiv.org/abs/2512.12534</link>
<guid>https://arxiv.org/abs/2512.12534</guid>
<content:encoded><![CDATA[
arXiv:2512.12534v1 Announce Type: cross 
Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks</title>
<link>https://arxiv.org/abs/2512.12544</link>
<guid>https://arxiv.org/abs/2512.12544</guid>
<content:encoded><![CDATA[
arXiv:2512.12544v1 Announce Type: cross 
Abstract: Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2512.12548</link>
<guid>https://arxiv.org/abs/2512.12548</guid>
<content:encoded><![CDATA[
arXiv:2512.12548v1 Announce Type: cross 
Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised Contrastive Frame Aggregation for Video Representation Learning</title>
<link>https://arxiv.org/abs/2512.12549</link>
<guid>https://arxiv.org/abs/2512.12549</guid>
<content:encoded><![CDATA[
arXiv:2512.12549v1 Announce Type: cross 
Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Sampling Methods for Sinkhorn Distributionally Robust Optimization</title>
<link>https://arxiv.org/abs/2512.12550</link>
<guid>https://arxiv.org/abs/2512.12550</guid>
<content:encoded><![CDATA[
arXiv:2512.12550v1 Announce Type: cross 
Abstract: Distributionally robust optimization (DRO) has emerged as a powerful paradigm for reliable decision-making under uncertainty. This paper focuses on DRO with ambiguity sets defined via the Sinkhorn discrepancy: an entropy-regularized Wasserstein distance, referred to as Sinkhorn DRO. Existing work primarily addresses Sinkhorn DRO from a dual perspective, leveraging its formulation as a conditional stochastic optimization problem, for which many stochastic gradient methods are applicable. However, the theoretical analyses of such methods often rely on the boundedness of the loss function, and it is indirect to obtain the worst-case distribution associated with Sinkhorn DRO. In contrast, we study Sinkhorn DRO from the primal perspective, by reformulating it as a bilevel program with several infinite-dimensional lower-level subproblems over probability space. This formulation enables us to simultaneously obtain the optimal robust decision and the worst-case distribution, which is valuable in practical settings, such as generating stress-test scenarios or designing robust learning algorithms. We propose both double-loop and single-loop sampling-based algorithms with theoretical guarantees to solve this bilevel program. Finally, we demonstrate the effectiveness of our approach through a numerical study on adversarial classification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Jumps: A Scalable Robust Local Gaussian Process for Multidimensional Response Surfaces with Discontinuities</title>
<link>https://arxiv.org/abs/2512.12574</link>
<guid>https://arxiv.org/abs/2512.12574</guid>
<content:encoded><![CDATA[
arXiv:2512.12574v1 Announce Type: cross 
Abstract: Modeling response surfaces with abrupt jumps and discontinuities remains a major challenge across scientific and engineering domains. Although Gaussian process models excel at capturing smooth nonlinear relationships, their stationarity assumptions limit their ability to adapt to sudden input-output variations. Existing nonstationary extensions, particularly those based on domain partitioning, often struggle with boundary inconsistencies, sensitivity to outliers, and scalability issues in higher-dimensional settings, leading to reduced predictive accuracy and unreliable parameter estimation.
  To address these challenges, this paper proposes the Robust Local Gaussian Process (RLGP) model, a framework that integrates adaptive nearest-neighbor selection with a sparsity-driven robustification mechanism. Unlike existing methods, RLGP leverages an optimization-based mean-shift adjustment after a multivariate perspective transformation combined with local neighborhood modeling to mitigate the influence of outliers. This approach improves predictive accuracy near discontinuities while enhancing robustness to data heterogeneity.
  Comprehensive evaluations on real-world datasets show that RLGP consistently delivers high predictive accuracy and maintains competitive computational efficiency, especially in scenarios with sharp transitions and complex response structures. Scalability tests further confirm RLGP's stability and reliability in higher-dimensional settings, where other methods struggle. These results establish RLGP as an effective and practical solution for modeling nonstationary and discontinuous response surfaces across a wide range of applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Quantum Error Mitigation with Neighbor-Informed Learning</title>
<link>https://arxiv.org/abs/2512.12578</link>
<guid>https://arxiv.org/abs/2512.12578</guid>
<content:encoded><![CDATA[
arXiv:2512.12578v1 Announce Type: cross 
Abstract: Noise in quantum hardware is the primary obstacle to realizing the transformative potential of quantum computing. Quantum error mitigation (QEM) offers a promising pathway to enhance computational accuracy on near-term devices, yet existing methods face a difficult trade-off between performance, resource overhead, and theoretical guarantees. In this work, we introduce neighbor-informed learning (NIL), a versatile and scalable QEM framework that unifies and strengthens existing methods such as zero-noise extrapolation (ZNE) and probabilistic error cancellation (PEC), while offering improved flexibility, accuracy, efficiency, and robustness.
  NIL learns to predict the ideal output of a target quantum circuit from the noisy outputs of its structurally related ``neighbor'' circuits. A key innovation is our 2-design training method, which generates training data for our machine learning model. In contrast to conventional learning-based QEM protocols that create training circuits by replacing non-Clifford gates with uniformly random Clifford gates, our approach achieves higher accuracy and efficiency, as demonstrated by both theoretical analysis and numerical simulation. Furthermore, we prove that the required size of the training set scales only \emph{logarithmically} with the total number of neighbor circuits, enabling NIL to be applied to problems involving large-scale quantum circuits. Our work establishes a theoretically grounded and practically efficient framework for QEM, paving a viable path toward achieving quantum advantage on noisy hardware.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ceLLMate: Sandboxing Browser AI Agents</title>
<link>https://arxiv.org/abs/2512.12594</link>
<guid>https://arxiv.org/abs/2512.12594</guid>
<content:encoded><![CDATA[
arXiv:2512.12594v1 Announce Type: cross 
Abstract: Browser-using agents (BUAs) are an emerging class of autonomous agents that interact with web browsers in human-like ways, including clicking, scrolling, filling forms, and navigating across pages. While these agents help automate repetitive online tasks, they are vulnerable to prompt injection attacks that can trick an agent into performing undesired actions, such as leaking private information or issuing state-changing requests. We propose ceLLMate, a browser-level sandboxing framework that restricts the agent's ambient authority and reduces the blast radius of prompt injections. We address two fundamental challenges: (1) The semantic gap challenge in policy enforcement arises because the agent operates through low-level UI observations and manipulations; however, writing and enforcing policies directly over UI-level events is brittle and error-prone. To address this challenge, we introduce an agent sitemap that maps low-level browser behaviors to high-level semantic actions. (2) Policy prediction in BUAs is the norm rather than the exception. BUAs have no app developer to pre-declare sandboxing policies, and thus, ceLLMate pairs website-authored mandatory policies with an automated policy-prediction layer that adapts and instantiates these policies from the user's natural-language task. We implement ceLLMate as an agent-agnostic browser extension and demonstrate how it enables sandboxing policies that effectively block various types of prompt injection attacks with negligible overhead.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoLSE: A Lightweight and Robust Hybrid Learned Model for Single-Table Cardinality Estimation using Joint CDF</title>
<link>https://arxiv.org/abs/2512.12624</link>
<guid>https://arxiv.org/abs/2512.12624</guid>
<content:encoded><![CDATA[
arXiv:2512.12624v1 Announce Type: cross 
Abstract: Cardinality estimation (CE), the task of predicting the result size of queries is a critical component of query optimization. Accurate estimates are essential for generating efficient query execution plans. Recently, machine learning techniques have been applied to CE, broadly categorized into query-driven and data-driven approaches. Data-driven methods learn the joint distribution of data, while query-driven methods construct regression models that map query features to cardinalities. Ideally, a CE technique should strike a balance among three key factors: accuracy, efficiency, and memory footprint. However, existing state-of-the-art models often fail to achieve this balance.
  To address this, we propose CoLSE, a hybrid learned approach for single-table cardinality estimation. CoLSE directly models the joint probability over queried intervals using a novel algorithm based on copula theory and integrates a lightweight neural network to correct residual estimation errors. Experimental results show that CoLSE achieves a favorable trade-off among accuracy, training time, inference latency, and model size, outperforming existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.12654</link>
<guid>https://arxiv.org/abs/2512.12654</guid>
<content:encoded><![CDATA[
arXiv:2512.12654v1 Announce Type: cross 
Abstract: Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Variational Bayes by Min-Max Median Aggregation</title>
<link>https://arxiv.org/abs/2512.12676</link>
<guid>https://arxiv.org/abs/2512.12676</guid>
<content:encoded><![CDATA[
arXiv:2512.12676v1 Announce Type: cross 
Abstract: We propose a robust and scalable variational Bayes (VB) framework designed to effectively handle contamination and outliers in dataset. Our approach partitions the data into $m$ disjoint subsets and formulates a joint optimization problem based on robust aggregation principles. A key insight is that the full posterior distribution is equivalent to the minimizer of the mean Kullback-Leibler (KL) divergence from the $m$-powered local posterior distributions. To enhance robustness, we replace the mean KL divergence with a min-max median formulation. The min-max formulation not only ensures consistency between the KL minimizer and the Evidence Lower Bound (ELBO) maximizer but also facilitates the establishment of improved statistical rates for the mean of variational posterior. We observe a notable discrepancy in the $m$-powered marginal log likelihood function contingent on the presence of local latent variables. To address this, we treat these two scenarios separately to guarantee the consistency of the aggregated variational posterior. Specifically, when local latent variables are present, we introduce an aggregate-and-rescale strategy. Theoretically, we provide a non-asymptotic analysis of our proposed posterior, incorporating a refined analysis of Bernstein-von Mises (BvM) theorem to accommodate a diverging number of subsets $m$. Our findings indicate that the two-stage approach yields a smaller approximation error compared to directly aggregating the $m$-powered local posteriors. Furthermore, we establish a nearly optimal statistical rate for the mean of the proposed posterior, advancing existing theories related to min-max median estimators. The efficacy of our method is demonstrated through extensive simulation studies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</title>
<link>https://arxiv.org/abs/2512.12692</link>
<guid>https://arxiv.org/abs/2512.12692</guid>
<content:encoded><![CDATA[
arXiv:2512.12692v1 Announce Type: cross 
Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Vision-Language Reasoning via Adaptive Token Pruning</title>
<link>https://arxiv.org/abs/2512.12701</link>
<guid>https://arxiv.org/abs/2512.12701</guid>
<content:encoded><![CDATA[
arXiv:2512.12701v1 Announce Type: cross 
Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Hybrid Quantum Language Models with Observable Readout on Real Hardware</title>
<link>https://arxiv.org/abs/2512.12710</link>
<guid>https://arxiv.org/abs/2512.12710</guid>
<content:encoded><![CDATA[
arXiv:2512.12710v1 Announce Type: cross 
Abstract: Hybrid quantum-classical models represent a crucial step toward leveraging near-term quantum devices for sequential data processing. We present Quantum Recurrent Neural Networks (QRNNs) and Quantum Convolutional Neural Networks (QCNNs) as hybrid quantum language models, reporting the first empirical demonstration of generative language modeling trained and evaluated end-to-end on real quantum hardware. Our architecture combines hardware-optimized parametric quantum circuits with a lightweight classical projection layer, utilizing a multi-sample SPSA strategy to efficiently train quantum parameters despite hardware noise. To characterize the capabilities of these models, we introduce a synthetic dataset designed to isolate syntactic dependencies in a controlled, low-resource environment. Experiments on IBM Quantum processors reveal the critical trade-offs between circuit depth and trainability, demonstrating that while noise remains a significant factor, observable-based readout enables the successful learning of sequential patterns on NISQ devices. These results establish a rigorous engineering baseline for generative quantum natural language processing, validating the feasibility of training complex sequence models on current quantum hardware.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural Plasticity</title>
<link>https://arxiv.org/abs/2512.12713</link>
<guid>https://arxiv.org/abs/2512.12713</guid>
<content:encoded><![CDATA[
arXiv:2512.12713v1 Announce Type: cross 
Abstract: Control policies in deep reinforcement learning are often implemented with fixed-capacity multilayer perceptrons trained by backpropagation, which lack structural plasticity and depend on global error signals. This paper introduces the Self-Motivated Growing Neural Network (SMGrNN), a controller whose topology evolves online through a local Structural Plasticity Module (SPM). The SPM monitors neuron activations and edge-wise weight update statistics over short temporal windows and uses these signals to trigger neuron insertion and pruning, while synaptic weights are updated by a standard gradient-based optimizer. This allows network capacity to be regulated during learning without manual architectural tuning.
  SMGrNN is evaluated on control benchmarks via policy distillation. Compared with multilayer perceptron baselines, it achieves similar or higher returns, lower variance, and task-appropriate network sizes. Ablation studies with growth disabled and growth-only variants isolate the role of structural plasticity, showing that adaptive topology improves reward stability. The local and modular design of SPM enables future integration of a Hebbian plasticity module and spike-timing-dependent plasticity, so that SMGrNN can support both artificial and spiking neural implementations driven by local rules.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limits To (Machine) Learning</title>
<link>https://arxiv.org/abs/2512.12735</link>
<guid>https://arxiv.org/abs/2512.12735</guid>
<content:encoded><![CDATA[
arXiv:2512.12735v1 Announce Type: cross 
Abstract: Machine learning (ML) methods are highly flexible, but their ability to approximate the true data-generating process is fundamentally constrained by finite samples. We characterize a universal lower bound, the Limits-to-Learning Gap (LLG), quantifying the unavoidable discrepancy between a model's empirical fit and the population benchmark. Recovering the true population $R^2$, therefore, requires correcting observed predictive performance by this bound. Using a broad set of variables, including excess returns, yields, credit spreads, and valuation ratios, we find that the implied LLGs are large. This indicates that standard ML approaches can substantially understate true predictability in financial data. We also derive LLG-based refinements to the classic Hansen and Jagannathan (1991) bounds, analyze implications for parameter learning in general-equilibrium settings, and show that the LLG provides a natural mechanism for generating excess volatility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transport Reversible Jump Markov Chain Monte Carlo with proposals generated by Variational Inference with Normalizing Flows</title>
<link>https://arxiv.org/abs/2512.12742</link>
<guid>https://arxiv.org/abs/2512.12742</guid>
<content:encoded><![CDATA[
arXiv:2512.12742v1 Announce Type: cross 
Abstract: We present a framework using variational inference with normalizing flows (VI-NFs) to generate proposals of reversible jump Markov chain Monte Carlo (RJMCMC) for efficient trans-dimensional Bayesian inference. Unlike transport reversible jump methods relying on forward KL minimization with pilot MCMC samples, our approach minimizes the reverse KL divergence which requires only samples from a base distribution, eliminating costly target sampling. The method employs RealNVP-based flows to learn model-specific transport maps, enabling construction of both between-model and within-model proposals. Our framework provides accurate marginal likelihood estimates from the variational approximation. This facilitates efficient model comparison and proposal adaptation in RJMCMC. Experiments on illustrative example, factor analysis and variable selection tasks in linear regression show that TRJ designed by VI-NFs achieves faster mixing and more efficient model space exploration compared to existing baselines. The proposed algorithm can be extended to conditional flows for amortized vairiational inference across models. Code is available at https://github.com/YinPingping111/TRJ_VINFs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow-matching Operators for Residual-Augmented Probabilistic Learning of Partial Differential Equations</title>
<link>https://arxiv.org/abs/2512.12749</link>
<guid>https://arxiv.org/abs/2512.12749</guid>
<content:encoded><![CDATA[
arXiv:2512.12749v1 Announce Type: cross 
Abstract: Learning probabilistic surrogates for PDEs remains challenging in data-scarce regimes: neural operators require large amounts of high-fidelity data, while generative approaches typically sacrifice resolution invariance. We formulate flow matching in an infinite-dimensional function space to learn a probabilistic transport that maps low-fidelity approximations to the manifold of high-fidelity PDE solutions via learned residual corrections. We develop a conditional neural operator architecture based on feature-wise linear modulation for flow-matching vector fields directly in function space, enabling inference at arbitrary spatial resolutions without retraining. To improve stability and representational control of the induced neural ODE, we parameterize the flow vector field as a sum of a linear operator and a nonlinear operator, combining lightweight linear components with a conditioned Fourier neural operator for expressive, input-dependent dynamics. We then formulate a residual-augmented learning strategy where the flow model learns probabilistic corrections from inexpensive low-fidelity surrogates to high-fidelity solutions, rather than learning the full solution mapping from scratch. Finally, we derive tractable training objectives that extend conditional flow matching to the operator setting with input-function-dependent couplings. To demonstrate the effectiveness of our approach, we present numerical experiments on a range of PDEs, including the 1D advection and Burgers' equation, and a 2D Darcy flow problem for flow through a porous medium.
  We show that the proposed method can accurately learn solution operators across different resolutions and fidelities and produces uncertainty estimates that appropriately reflect model confidence, even when trained on limited high-fidelity data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An End-to-End Approach for Microgrid Probabilistic Forecasting and Robust Operation via Decision-focused Learning</title>
<link>https://arxiv.org/abs/2512.12755</link>
<guid>https://arxiv.org/abs/2512.12755</guid>
<content:encoded><![CDATA[
arXiv:2512.12755v1 Announce Type: cross 
Abstract: High penetration of renewable energy sources (RES) introduces significant uncertainty and intermittency into microgrid operations, posing challenges to economic and reliable scheduling. To address this, this paper proposes an end-to-end decision-focused framework that jointly optimizes probabilistic forecasting and robust operation for microgrids. A multilayer encoder-decoder (MED) probabilistic forecasting model is integrated with a two-stage robust optimization (TSRO) model involving direct load control (DLC) through a differentiable decision pathway, enabling gradient-based feedback from operational outcomes to improve forecasting performance. Unlike conventional sequential approaches, the proposed method aligns forecasting accuracy with operational objectives by directly minimizing decision regret via a surrogate smart predict-then-optimize (SPO) loss function. This integration ensures that probabilistic forecasts are optimized for downstream decisions, enhancing both economic efficiency and robustness. Case studies on modified IEEE 33-bus and 69-bus systems demonstrate that the proposed framework achieves superior forecasting accuracy and operational performance, reducing total and net operation costs by up to 18% compared with conventional forecasting and optimization combinations. The results verify the effectiveness and scalability of the end-to-end decision-focused approach for resilient and cost-efficient microgrid management under uncertainty.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</title>
<link>https://arxiv.org/abs/2512.12768</link>
<guid>https://arxiv.org/abs/2512.12768</guid>
<content:encoded><![CDATA[
arXiv:2512.12768v1 Announce Type: cross 
Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects</title>
<link>https://arxiv.org/abs/2512.12818</link>
<guid>https://arxiv.org/abs/2512.12818</guid>
<content:encoded><![CDATA[
arXiv:2512.12818v1 Announce Type: cross 
Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding</title>
<link>https://arxiv.org/abs/2512.12842</link>
<guid>https://arxiv.org/abs/2512.12842</guid>
<content:encoded><![CDATA[
arXiv:2512.12842v1 Announce Type: cross 
Abstract: We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility</title>
<link>https://arxiv.org/abs/2512.12847</link>
<guid>https://arxiv.org/abs/2512.12847</guid>
<content:encoded><![CDATA[
arXiv:2512.12847v1 Announce Type: cross 
Abstract: We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KANEL\'E: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation</title>
<link>https://arxiv.org/abs/2512.12850</link>
<guid>https://arxiv.org/abs/2512.12850</guid>
<content:encoded><![CDATA[
arXiv:2512.12850v1 Announce Type: cross 
Abstract: Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANEL\'E, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANEL\'E matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents</title>
<link>https://arxiv.org/abs/2512.12856</link>
<guid>https://arxiv.org/abs/2512.12856</guid>
<content:encoded><![CDATA[
arXiv:2512.12856v1 Announce Type: cross 
Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM</title>
<link>https://arxiv.org/abs/2512.12868</link>
<guid>https://arxiv.org/abs/2512.12868</guid>
<content:encoded><![CDATA[
arXiv:2512.12868v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence</title>
<link>https://arxiv.org/abs/2512.12888</link>
<guid>https://arxiv.org/abs/2512.12888</guid>
<content:encoded><![CDATA[
arXiv:2512.12888v1 Announce Type: cross 
Abstract: Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution</title>
<link>https://arxiv.org/abs/2512.12898</link>
<guid>https://arxiv.org/abs/2512.12898</guid>
<content:encoded><![CDATA[
arXiv:2512.12898v1 Announce Type: cross 
Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAC-Bayes Bounds for Multivariate Linear Regression and Linear Autoencoders</title>
<link>https://arxiv.org/abs/2512.12905</link>
<guid>https://arxiv.org/abs/2512.12905</guid>
<content:encoded><![CDATA[
arXiv:2512.12905v1 Announce Type: cross 
Abstract: Linear Autoencoders (LAEs) have shown strong performance in state-of-the-art recommender systems. However, this success remains largely empirical, with limited theoretical understanding. In this paper, we investigate the generalizability -- a theoretical measure of model performance in statistical learning -- of multivariate linear regression and LAEs. We first propose a PAC-Bayes bound for multivariate linear regression, extending the earlier bound for single-output linear regression by Shalaeva et al., and establish sufficient conditions for its convergence. We then show that LAEs, when evaluated under a relaxed mean squared error, can be interpreted as constrained multivariate linear regression models on bounded data, to which our bound adapts. Furthermore, we develop theoretical methods to improve the computational efficiency of optimizing the LAE bound, enabling its practical evaluation on large models and real-world datasets. Experimental results demonstrate that our bound is tight and correlates well with practical ranking metrics such as Recall@K and NDCG@K.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Singular Value Thresholds for DNN Weight Matrices based on Random Matrix Theory</title>
<link>https://arxiv.org/abs/2512.12911</link>
<guid>https://arxiv.org/abs/2512.12911</guid>
<content:encoded><![CDATA[
arXiv:2512.12911v1 Announce Type: cross 
Abstract: This study evaluates thresholds for removing singular values from singular value decomposition-based low-rank approximations of deep neural network weight matrices. Each weight matrix is modeled as the sum of signal and noise matrices. The low-rank approximation is obtained by removing noise-related singular values using a threshold based on random matrix theory. To assess the adequacy of this threshold, we propose an evaluation metric based on the cosine similarity between the singular vectors of the signal and original weight matrices. The proposed metric is used in numerical experiments to compare two threshold estimation methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs</title>
<link>https://arxiv.org/abs/2512.12914</link>
<guid>https://arxiv.org/abs/2512.12914</guid>
<content:encoded><![CDATA[
arXiv:2512.12914v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Edit Distance, Geodesics and Barycenters of Time-varying Persistence Diagrams</title>
<link>https://arxiv.org/abs/2512.12939</link>
<guid>https://arxiv.org/abs/2512.12939</guid>
<content:encoded><![CDATA[
arXiv:2512.12939v1 Announce Type: cross 
Abstract: We introduce the Continuous Edit Distance (CED), a geodesic and elastic distance for time-varying persistence diagrams (TVPDs). The CED extends edit-distance ideas to TVPDs by combining local substitution costs with penalized deletions/insertions, controlled by two parameters: \(\alpha\) (trade-off between temporal misalignment and diagram discrepancy) and \(\beta\) (gap penalty). We also provide an explicit construction of CED-geodesics. Building on these ingredients, we present two practical barycenter solvers, one stochastic and one greedy, that monotonically decrease the CED Frechet energy. Empirically, the CED is robust to additive perturbations (both temporal and spatial), recovers temporal shifts, and supports temporal pattern search. On real-life datasets, the CED achieves clustering performance comparable or better than standard elastic dissimilarities, while our clustering based on CED-barycenters yields superior classification results. Overall, the CED equips TVPD analysis with a principled distance, interpretable geodesics, and practical barycenters, enabling alignment, comparison, averaging, and clustering directly in the space of TVPDs. A C++ implementation is provided for reproducibility at the following address https://github.com/sebastien-tchitchek/ContinuousEditDistance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs</title>
<link>https://arxiv.org/abs/2512.12984</link>
<guid>https://arxiv.org/abs/2512.12984</guid>
<content:encoded><![CDATA[
arXiv:2512.12984v1 Announce Type: cross 
Abstract: We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.12987</link>
<guid>https://arxiv.org/abs/2512.12987</guid>
<content:encoded><![CDATA[
arXiv:2512.12987v1 Announce Type: cross 
Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating Uncertainty for Zero-Shot Adversarial CLIP</title>
<link>https://arxiv.org/abs/2512.12997</link>
<guid>https://arxiv.org/abs/2512.12997</guid>
<content:encoded><![CDATA[
arXiv:2512.12997v1 Announce Type: cross 
Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General OOD Detection via Model-aware and Subspace-aware Variable Priority</title>
<link>https://arxiv.org/abs/2512.13003</link>
<guid>https://arxiv.org/abs/2512.13003</guid>
<content:encoded><![CDATA[
arXiv:2512.13003v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection is essential for determining when a supervised model encounters inputs that differ meaningfully from its training distribution. While widely studied in classification, OOD detection for regression and survival analysis remains limited due to the absence of discrete labels and the challenge of quantifying predictive uncertainty. We introduce a framework for OOD detection that is simultaneously model aware and subspace aware, and that embeds variable prioritization directly into the detection step. The method uses the fitted predictor to construct localized neighborhoods around each test case that emphasize the features driving the model's learned relationship and downweight directions that are less relevant to prediction. It produces OOD scores without relying on global distance metrics or estimating the full feature density. The framework is applicable across outcome types, and in our implementation we use random forests, where the rule structure yields transparent neighborhoods and effective scoring. Experiments on synthetic and real data benchmarks designed to isolate functional shifts show consistent improvements over existing methods. We further demonstrate the approach in an esophageal cancer survival study, where distribution shifts related to lymphadenectomy identify patterns relevant to surgical guidelines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing</title>
<link>https://arxiv.org/abs/2512.13018</link>
<guid>https://arxiv.org/abs/2512.13018</guid>
<content:encoded><![CDATA[
arXiv:2512.13018v1 Announce Type: cross 
Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motus: A Unified Latent Action World Model</title>
<link>https://arxiv.org/abs/2512.13030</link>
<guid>https://arxiv.org/abs/2512.13030</guid>
<content:encoded><![CDATA[
arXiv:2512.13030v1 Announce Type: cross 
Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer</title>
<link>https://arxiv.org/abs/2512.13037</link>
<guid>https://arxiv.org/abs/2512.13037</guid>
<content:encoded><![CDATA[
arXiv:2512.13037v1 Announce Type: cross 
Abstract: In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiRe: Diversity-promoting Regularization for Dataset Condensation</title>
<link>https://arxiv.org/abs/2512.13083</link>
<guid>https://arxiv.org/abs/2512.13083</guid>
<content:encoded><![CDATA[
arXiv:2512.13083v1 Announce Type: cross 
Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations</title>
<link>https://arxiv.org/abs/2512.13093</link>
<guid>https://arxiv.org/abs/2512.13093</guid>
<content:encoded><![CDATA[
arXiv:2512.13093v1 Announce Type: cross 
Abstract: Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13095</link>
<guid>https://arxiv.org/abs/2512.13095</guid>
<content:encoded><![CDATA[
arXiv:2512.13095v1 Announce Type: cross 
Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.13101</link>
<guid>https://arxiv.org/abs/2512.13101</guid>
<content:encoded><![CDATA[
arXiv:2512.13101v1 Announce Type: cross 
Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation</title>
<link>https://arxiv.org/abs/2512.13120</link>
<guid>https://arxiv.org/abs/2512.13120</guid>
<content:encoded><![CDATA[
arXiv:2512.13120v1 Announce Type: cross 
Abstract: Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences</title>
<link>https://arxiv.org/abs/2512.13123</link>
<guid>https://arxiv.org/abs/2512.13123</guid>
<content:encoded><![CDATA[
arXiv:2512.13123v1 Announce Type: cross 
Abstract: We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\varepsilon$-optimal with probability at least $1-\alpha$ and is almost surely finite under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models</title>
<link>https://arxiv.org/abs/2512.13144</link>
<guid>https://arxiv.org/abs/2512.13144</guid>
<content:encoded><![CDATA[
arXiv:2512.13144v1 Announce Type: cross 
Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks</title>
<link>https://arxiv.org/abs/2512.13170</link>
<guid>https://arxiv.org/abs/2512.13170</guid>
<content:encoded><![CDATA[
arXiv:2512.13170v1 Announce Type: cross 
Abstract: Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MicroPhaseNO: Adapting an Earthquake-Trained Phase Neural Operator for Microseismic Phase Picking</title>
<link>https://arxiv.org/abs/2512.13197</link>
<guid>https://arxiv.org/abs/2512.13197</guid>
<content:encoded><![CDATA[
arXiv:2512.13197v1 Announce Type: cross 
Abstract: Seismic phase picking is very often used for microseismic monitoring and subsurface imaging. Traditional manual processing is not feasible for either real-time applications or large arrays. Deep learning-based pickers trained on large earthquake catalogs offer an automated alternative. However, they are typically optimized for high signal-to-noise, long-duration networks and struggle with the challenges presented by microseismic datasets, which are purpose-built for limited time without previously detected seismicity. In this study, we demonstrate how a network-wide earthquake phase picker, the Phase Neural Operator (PhaseNO), can be adapted to microseismic monitoring using transfer learning. Starting from a PhaseNO model pre-trained on more than 57,000 three-component earthquake and noise records, we fine-tune the model using only 200 labeled and noise seismograms from induced events in hydraulic-fracturing settings. The fine-tuned model thus preserves the rich spatio-temporal representation learned from abundant earthquake data, while adapting to the characteristics and labeling conventions of microseismic phases, which are often picked on peaks or troughs rather than onsets. We evaluate performance on three distinct real-world microseismic datasets with different network geometries and acquisition parameters. Compared to the original PhaseNO and a conventional workflow, the adapted model increases F1 score and accuracy by up to 30%, and strongly reduces systematic timing bias and pick uncertainty. Because the adaptation relies on a small, campaign-specific calibration set, the approach is readily transferable to other microseismic tasks where public earthquake data and pre-trained models are accessible. The associated code will be released openly at https://github.com/ayratabd/MicroPhaseNO.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Physics-Informed Regression Beyond Training Loops and Bespoke Architectures</title>
<link>https://arxiv.org/abs/2512.13217</link>
<guid>https://arxiv.org/abs/2512.13217</guid>
<content:encoded><![CDATA[
arXiv:2512.13217v1 Announce Type: cross 
Abstract: We revisit the problem of physics-informed regression, and propose a method that directly computes the state at the prediction point, simultaneously with the derivative and curvature information of the existing samples. We frame each prediction as a constrained optimisation problem, leveraging multivariate Taylor series expansions and explicitly enforcing physical laws. Each individual query can be processed with low computational cost without any pre- or re-training, in contrast to global function approximator-based solutions such as neural networks. Our comparative benchmarks on a reaction-diffusion system show competitive predictive accuracy relative to a neural network-based solution, while completely eliminating the need for long training loops, and remaining robust to changes in the sampling layout.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better LMO-based Momentum Methods with Second-Order Information</title>
<link>https://arxiv.org/abs/2512.13227</link>
<guid>https://arxiv.org/abs/2512.13227</guid>
<content:encoded><![CDATA[
arXiv:2512.13227v1 Announce Type: cross 
Abstract: The use of momentum in stochastic optimization algorithms has shown empirical success across a range of machine learning tasks. Recently, a new class of stochastic momentum algorithms has emerged within the Linear Minimization Oracle (LMO) framework--leading to state-of-the-art methods, such as Muon, Scion, and Gluon, that effectively solve deep neural network training problems. However, traditional stochastic momentum methods offer convergence guarantees no better than the ${O}(1/K^{1/4})$ rate. While several approaches--such as Hessian-Corrected Momentum (HCM)--have aimed to improve this rate, their theoretical results are generally restricted to the Euclidean norm setting. This limitation hinders their applicability in problems, where arbitrary norms are often required. In this paper, we extend the LMO-based framework by integrating HCM, and provide convergence guarantees under relaxed smoothness and arbitrary norm settings. We establish improved convergence rates of ${O}(1/K^{1/3})$ for HCM, which can adapt to the geometry of the problem and achieve a faster rate than traditional momentum. Experimental results on training Multi-Layer Perceptrons (MLPs) and Long Short-Term Memory (LSTM) networks verify our theoretical observations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection</title>
<link>https://arxiv.org/abs/2512.13240</link>
<guid>https://arxiv.org/abs/2512.13240</guid>
<content:encoded><![CDATA[
arXiv:2512.13240v1 Announce Type: cross 
Abstract: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning</title>
<link>https://arxiv.org/abs/2512.13278</link>
<guid>https://arxiv.org/abs/2512.13278</guid>
<content:encoded><![CDATA[
arXiv:2512.13278v1 Announce Type: cross 
Abstract: Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13290</link>
<guid>https://arxiv.org/abs/2512.13290</guid>
<content:encoded><![CDATA[
arXiv:2512.13290v1 Announce Type: cross 
Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data</title>
<link>https://arxiv.org/abs/2512.13297</link>
<guid>https://arxiv.org/abs/2512.13297</guid>
<content:encoded><![CDATA[
arXiv:2512.13297v1 Announce Type: cross 
Abstract: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Face Identity Unlearning for Retrieval via Embedding Dispersion</title>
<link>https://arxiv.org/abs/2512.13317</link>
<guid>https://arxiv.org/abs/2512.13317</guid>
<content:encoded><![CDATA[
arXiv:2512.13317v1 Announce Type: cross 
Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error-Driven Prompt Optimization for Arithmetic Reasoning</title>
<link>https://arxiv.org/abs/2512.13323</link>
<guid>https://arxiv.org/abs/2512.13323</guid>
<content:encoded><![CDATA[
arXiv:2512.13323v1 Announce Type: cross 
Abstract: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models</title>
<link>https://arxiv.org/abs/2512.13325</link>
<guid>https://arxiv.org/abs/2512.13325</guid>
<content:encoded><![CDATA[
arXiv:2512.13325v1 Announce Type: cross 
Abstract: Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)</title>
<link>https://arxiv.org/abs/2512.13356</link>
<guid>https://arxiv.org/abs/2512.13356</guid>
<content:encoded><![CDATA[
arXiv:2512.13356v1 Announce Type: cross 
Abstract: This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles</title>
<link>https://arxiv.org/abs/2512.13359</link>
<guid>https://arxiv.org/abs/2512.13359</guid>
<content:encoded><![CDATA[
arXiv:2512.13359v1 Announce Type: cross 
Abstract: Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>rNCA: Self-Repairing Segmentation Masks</title>
<link>https://arxiv.org/abs/2512.13397</link>
<guid>https://arxiv.org/abs/2512.13397</guid>
<content:encoded><![CDATA[
arXiv:2512.13397v1 Announce Type: cross 
Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $\beta_0$ errors by 60% and $\beta_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MineTheGap: Automatic Mining of Biases in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.13427</link>
<guid>https://arxiv.org/abs/2512.13427</guid>
<content:encoded><![CDATA[
arXiv:2512.13427v1 Announce Type: cross 
Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models</title>
<link>https://arxiv.org/abs/2512.13478</link>
<guid>https://arxiv.org/abs/2512.13478</guid>
<content:encoded><![CDATA[
arXiv:2512.13478v1 Announce Type: cross 
Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $\rho$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time AI-Driven Milling Digital Twin Towards Extreme Low-Latency</title>
<link>https://arxiv.org/abs/2512.13482</link>
<guid>https://arxiv.org/abs/2512.13482</guid>
<content:encoded><![CDATA[
arXiv:2512.13482v1 Announce Type: cross 
Abstract: Digital twin (DT) enables smart manufacturing by leveraging real-time data, AI models, and intelligent control systems. This paper presents a state-of-the-art analysis on the emerging field of DTs in the context of milling. The critical aspects of DT are explored through the lens of virtual models of physical milling, data flow from physical milling to virtual model, and feedback from virtual model to physical milling. Live data streaming protocols and virtual modeling methods are highlighted. A case study showcases the transformative capability of a real-time machine learning-driven live DT of tool-work contact in a milling process. Future research directions are outlined to achieve the goals of Industry 4.0 and beyond.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis</title>
<link>https://arxiv.org/abs/2512.13491</link>
<guid>https://arxiv.org/abs/2512.13491</guid>
<content:encoded><![CDATA[
arXiv:2512.13491v1 Announce Type: cross 
Abstract: We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments</title>
<link>https://arxiv.org/abs/2512.13517</link>
<guid>https://arxiv.org/abs/2512.13517</guid>
<content:encoded><![CDATA[
arXiv:2512.13517v1 Announce Type: cross 
Abstract: Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing lithological interpretation from petrophysical well log of IODP expedition 390/393 using machine learning</title>
<link>https://arxiv.org/abs/2512.13529</link>
<guid>https://arxiv.org/abs/2512.13529</guid>
<content:encoded><![CDATA[
arXiv:2512.13529v1 Announce Type: cross 
Abstract: Enhanced lithological interpretation from well logs plays a key role in geological resource exploration and mapping, as well as in geo-environmental modeling studies. Core and cutting information is useful for making sound interpretations of well logs; however, these are rarely collected at each depth due to high costs. Moreover, well log interpretation using traditional methods is constrained by poor borehole conditions. Traditional statistical methods are mostly linear, often failing to discriminate between lithology and rock facies, particularly when dealing with overlapping well log signals characterized by the structural and compositional variation of rock types. In this study, we develop multiple supervised and unsupervised machine learning algorithms to jointly analyze multivariate well log data from Integrated Ocean Drilling Program (IODP) expeditions 390 and 393 for enhanced lithological interpretations. Among the algorithms, Logistic Regression, Decision Trees, Gradient Boosting, Support Vector Machines (SVM), k-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) neural network models, the Decision Tree and Gradient Boosting models outperformed the others, achieving an accuracy of 0.9950 and an F1-score of 0.9951. While unsupervised machine learning (ML) provides the foundation for cluster information that inherently supports the classification algorithm, supervised ML is applied to devise a data-driven lithology clustering mechanism for IODP datasets. The joint ML-based method developed here has the potential to be further explored for analyzing other well log datasets from the world's oceans.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Actively Learning Joint Contours of Multiple Computer Experiments</title>
<link>https://arxiv.org/abs/2512.13530</link>
<guid>https://arxiv.org/abs/2512.13530</guid>
<content:encoded><![CDATA[
arXiv:2512.13530v1 Announce Type: cross 
Abstract: Contour location$\unicode{x2014}$the process of sequentially training a surrogate model to identify the design inputs that result in a pre-specified response value from a single computer experiment$\unicode{x2014}$is a well-studied active learning problem. Here, we tackle a related but distinct problem: identifying the input configuration that returns pre-specified values of multiple independent computer experiments simultaneously. Motivated by computer experiments of the rotational torques acting upon a vehicle in flight, we aim to identify stable flight conditions which result in zero torque forces. We propose a "joint contour location" (jCL) scheme that strikes a strategic balance between exploring the multiple response surfaces while exploiting learning of the intersecting contours. We employ both shallow and deep Gaussian process surrogates, but our jCL procedure is applicable to any surrogate that can provide posterior predictive distributions. Our jCL designs significantly outperform existing (single response) CL strategies, enabling us to efficiently locate the joint contour of our motivating computer experiments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Sampling for Hydrodynamic Stability</title>
<link>https://arxiv.org/abs/2512.13532</link>
<guid>https://arxiv.org/abs/2512.13532</guid>
<content:encoded><![CDATA[
arXiv:2512.13532v1 Announce Type: cross 
Abstract: An adaptive sampling approach for efficient detection of bifurcation boundaries in parametrized fluid flow problems is presented herein. The study extends the machine-learning approach of Silvester (Machine Learning for Hydrodynamic Stability, arXiv:2407.09572), where a classifier network was trained on preselected simulation data to identify bifurcated and nonbifurcated flow regimes. In contrast, the proposed methodology introduces adaptivity through a flow-based deep generative model that automatically refines the sampling of the parameter space. The strategy has two components: a classifier network maps the flow parameters to a bifurcation probability, and a probability density estimation technique (KRnet) for the generation of new samples at each adaptive step. The classifier output provides a probabilistic measure of flow stability, and the Shannon entropy of these predictions is employed as an uncertainty indicator. KRnet is trained to approximate a probability density function that concentrates sampling in regions of high entropy, thereby directing computational effort towards the evolving bifurcation boundary. This coupling between classification and generative modeling establishes a feedback-driven adaptive learning process analogous to error-indicator based refinement in contemporary partial differential equation solution strategies. Starting from a uniform parameter distribution, the new approach achieves accurate bifurcation boundary identification with significantly fewer Navier--Stokes simulations, providing a scalable foundation for high-dimensional stability analysis.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains</title>
<link>https://arxiv.org/abs/2512.13534</link>
<guid>https://arxiv.org/abs/2512.13534</guid>
<content:encoded><![CDATA[
arXiv:2512.13534v1 Announce Type: cross 
Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees</title>
<link>https://arxiv.org/abs/2512.13565</link>
<guid>https://arxiv.org/abs/2512.13565</guid>
<content:encoded><![CDATA[
arXiv:2512.13565v1 Announce Type: cross 
Abstract: This paper tackles the problem of feature selection in a highly challenging setting: $\mathbb{E}(y | \boldsymbol{x}) = G(\boldsymbol{x}_{\mathcal{S}_0})$, where $\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{\"o}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = \Omega(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = \Omega(s \log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding</title>
<link>https://arxiv.org/abs/2512.13586</link>
<guid>https://arxiv.org/abs/2512.13586</guid>
<content:encoded><![CDATA[
arXiv:2512.13586v1 Announce Type: cross 
Abstract: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization</title>
<link>https://arxiv.org/abs/2512.13598</link>
<guid>https://arxiv.org/abs/2512.13598</guid>
<content:encoded><![CDATA[
arXiv:2512.13598v1 Announce Type: cross 
Abstract: A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models</title>
<link>https://arxiv.org/abs/2512.13607</link>
<guid>https://arxiv.org/abs/2512.13607</guid>
<content:encoded><![CDATA[
arXiv:2512.13607v1 Announce Type: cross 
Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.13609</link>
<guid>https://arxiv.org/abs/2512.13609</guid>
<content:encoded><![CDATA[
arXiv:2512.13609v1 Announce Type: cross 
Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models</title>
<link>https://arxiv.org/abs/2512.13618</link>
<guid>https://arxiv.org/abs/2512.13618</guid>
<content:encoded><![CDATA[
arXiv:2512.13618v1 Announce Type: cross 
Abstract: Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universality of high-dimensional scaling limits of stochastic gradient descent</title>
<link>https://arxiv.org/abs/2512.13634</link>
<guid>https://arxiv.org/abs/2512.13634</guid>
<content:encoded><![CDATA[
arXiv:2512.13634v1 Announce Type: cross 
Abstract: We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work</title>
<link>https://arxiv.org/abs/2512.13666</link>
<guid>https://arxiv.org/abs/2512.13666</guid>
<content:encoded><![CDATA[
arXiv:2512.13666v1 Announce Type: cross 
Abstract: The security and decentralization of Proof-of-Work (PoW) have been well-tested in existing blockchain systems. However, its tremendous energy waste has raised concerns about sustainability. Proof-of-Useful-Work (PoUW) aims to redirect the meaningless computation to meaningful tasks such as solving machine learning (ML) problems, giving rise to the branch of Proof-of-Learning (PoL). While previous studies have proposed various PoLs, they all, to some degree, suffer from security, decentralization, or efficiency issues. In this paper, we propose a PoL framework that trains ML models efficiently while maintaining blockchain security in a fully distributed manner. We name the framework SEDULity, which stands for a Secure, Efficient, Distributed, and Useful Learning-based blockchain system. Specifically, we encode the template block into the training process and design a useful function that is difficult to solve but relatively easy to verify, as a substitute for the PoW puzzle. We show that our framework is distributed, secure, and efficiently trains ML models. We further demonstrate that the proposed PoL framework can be extended to other types of useful work and design an incentive mechanism to incentivize task verification. We show theoretically that a rational miner is incentivized to train fully honestly with well-designed system parameters. Finally, we present simulation results to demonstrate the performance of our framework and validate our analysis.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</title>
<link>https://arxiv.org/abs/2512.13690</link>
<guid>https://arxiv.org/abs/2512.13690</guid>
<content:encoded><![CDATA[
arXiv:2512.13690v1 Announce Type: cross 
Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Optimal Approximation Factor in Density Estimation</title>
<link>https://arxiv.org/abs/1902.05876</link>
<guid>https://arxiv.org/abs/1902.05876</guid>
<content:encoded><![CDATA[
arXiv:1902.05876v5 Announce Type: replace 
Abstract: Consider the following problem: given two arbitrary densities $q_1,q_2$ and a sample-access to an unknown target density $p$, find which of the $q_i$'s is closer to $p$ in total variation.
  A remarkable result due to Yatracos shows that this problem is tractable in the following sense: there exists an algorithm that uses $O(\epsilon^{-2})$ samples from $p$ and outputs~$q_i$ such that with high probability, $TV(q_i,p) \leq 3\cdot\mathsf{opt} + \epsilon$, where $\mathsf{opt}= \min\{TV(q_1,p),TV(q_2,p)\}$. Moreover, this result extends to any finite class of densities $\mathcal{Q}$: there exists an algorithm that outputs the best density in $\mathcal{Q}$ up to a multiplicative approximation factor of 3.
  We complement and extend this result by showing that: (i) the factor 3 can not be improved if one restricts the algorithm to output a density from $\mathcal{Q}$, and (ii) if one allows the algorithm to output arbitrary densities (e.g.\ a mixture of densities from $\mathcal{Q}$), then the approximation factor can be reduced to 2, which is optimal. In particular this demonstrates an advantage of improper learning over proper in this setup.
  We develop two approaches to achieve the optimal approximation factor of 2: an adaptive one and a static one. Both approaches are based on a geometric point of view of the problem and rely on estimating surrogate metrics to the total variation. Our sample complexity bounds exploit techniques from {\it Adaptive Data Analysis}.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Risk Mitigation in Demand Learning</title>
<link>https://arxiv.org/abs/2011.10690</link>
<guid>https://arxiv.org/abs/2011.10690</guid>
<content:encoded><![CDATA[
arXiv:2011.10690v2 Announce Type: replace 
Abstract: We study dynamic pricing of a product with an unknown demand distribution over a finite horizon. Departing from the standard no-regret learning environment in which prices can be adjusted at any time, we restrict price changes to predetermined points in time to reflect common retail practice. This constraint, coupled with demand model ambiguity and an unknown customer arrival pattern, imposes a high risk of revenue loss, as a price based on a misestimated demand model may be applied to many customers before it can be revised. We develop an adaptive risk learning (ARL) framework that embeds a data-driven ambiguity set (DAS) to quantify demand model ambiguity by adapting to the unknown arrival pattern. Initially, when arrivals are few, the DAS includes a broad set of plausible demand models, reflecting high ambiguity and revenue risk. As new data is collected through pricing, the DAS progressively shrinks, capturing the reduction in model ambiguity and associated risk. We establish the probabilistic convergence of the DAS to the true demand model and derive a regret bound for the ARL policy that explicitly links revenue loss to the data required for the DAS to identify the true model with high probability. The dependence of our regret bound on the arrival pattern is unique to our constrained dynamic pricing problem and contrasts with no-regret learning environments, where regret is constant and arrival-pattern independent. Relaxing the constraint on infrequent price changes, we show that ARL attains the known constant regret bound. Numerical experiments further demonstrate that ARL outperforms benchmarks that prioritize either regret or risk alone by adaptively balancing both without knowledge of the arrival pattern. This adaptive risk adjustment is crucial for achieving high revenues and low downside risk when prices are sticky and both demand and arrival patterns are unknown.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vertical Semi-Federated Learning for Efficient Online Advertising</title>
<link>https://arxiv.org/abs/2209.15635</link>
<guid>https://arxiv.org/abs/2209.15635</guid>
<content:encoded><![CDATA[
arXiv:2209.15635v3 Announce Type: replace 
Abstract: Traditional vertical federated learning schema suffers from two main issues: 1) restricted applicable scope to overlapped samples and 2) high system challenge of real-time federated serving, which limits its application to advertising systems. To this end, we advocate a new practical learning setting, Semi-VFL (Vertical Semi-Federated Learning), for real-world industrial applications, where the learned model retains sufficient advantages of federated learning while supporting independent local serving. To achieve this goal, we propose the carefully designed Joint Privileged Learning framework (JPL) to i) alleviate the absence of the passive party's feature with federated equivalence imitation and ii) adapt to the heterogeneous full sample space with cross-branch rank alignment. Extensive experiments conducted on real-world advertising datasets validate the effectiveness of our method over baseline methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certifying Robustness of Graph Convolutional Networks for Node Perturbation with Polyhedra Abstract Interpretation</title>
<link>https://arxiv.org/abs/2405.08645</link>
<guid>https://arxiv.org/abs/2405.08645</guid>
<content:encoded><![CDATA[
arXiv:2405.08645v2 Announce Type: replace 
Abstract: Graph convolutional neural networks (GCNs) are powerful tools for learning graph-based knowledge representations from training data. However, they are vulnerable to small perturbations in the input graph, which makes them susceptible to input faults or adversarial attacks. This poses a significant problem for GCNs intended to be used in critical applications, which need to provide certifiably robust services even in the presence of adversarial perturbations. We propose an improved GCN robustness certification technique for node classification in the presence of node feature perturbations. We introduce a novel polyhedra-based abstract interpretation approach to tackle specific challenges of graph data and provide tight upper and lower bounds for the robustness of the GCN. Experiments show that our approach simultaneously improves the tightness of robustness bounds as well as the runtime performance of certification. Moreover, our method can be used during training to further improve the robustness of GCNs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Neural Common Neighbor for Temporal Graph Link Prediction</title>
<link>https://arxiv.org/abs/2406.07926</link>
<guid>https://arxiv.org/abs/2406.07926</guid>
<content:encoded><![CDATA[
arXiv:2406.07926v2 Announce Type: replace 
Abstract: Temporal graphs are widespread in real-world applications such as social networks, as well as trade and transportation networks. Predicting dynamic links within these evolving graphs is a key problem. Many memory-based methods use temporal interaction histories to generate node embeddings, which are then combined to predict links. However, these approaches primarily focus on individual node representations, often overlooking the inherently pairwise nature of link prediction. While some recent methods attempt to capture pairwise features, they tend to be limited by high computational complexity arising from repeated embedding calculations, making them unsuitable for large-scale datasets like the Temporal Graph Benchmark (TGB). To address the critical need for models that combine strong expressive power with high computational efficiency for link prediction on large temporal graphs, we propose Temporal Neural Common Neighbor (TNCN). Our model achieves this balance by adapting the powerful pairwise modeling principles of Neural Common Neighbor (NCN) to an efficient temporal architecture. TNCN improves upon NCN by efficiently preserving and updating temporal neighbor dictionaries for each node and by using multi-hop common neighbors to learn more expressive pairwise representations. TNCN achieves new state-of-the-art performance on Review from five large-scale real-world TGB datasets, 6 out of 7 datasets in the transductive setting and 3 out of 7 in the inductive setting on small- to medium-scale datasets. Additionally, TNCN demonstrates excellent scalability, outperforming prominent GNN baselines by up to 30.3 times in speed on large datasets. Our code is available at https://github.com/GraphPKU/TNCN.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural stochastic Volterra equations: learning path-dependent dynamics</title>
<link>https://arxiv.org/abs/2407.19557</link>
<guid>https://arxiv.org/abs/2407.19557</guid>
<content:encoded><![CDATA[
arXiv:2407.19557v3 Announce Type: replace 
Abstract: Stochastic Volterra equations (SVEs) serve as mathematical models for the time evolutions of random systems with memory effects and irregular behaviour. We introduce neural stochastic Volterra equations as a physics-inspired architecture, generalizing the class of neural stochastic differential equations, and provide some theoretical foundation. Numerical experiments on various SVEs, like the disturbed pendulum equation, the generalized Ornstein--Uhlenbeck process, the rough Heston model and a monetary reserve dynamics, are presented, comparing the performance of neural SVEs, neural SDEs and Deep Operator Networks (DeepONets).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fully Bayesian Differential Gaussian Processes through Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2408.06069</link>
<guid>https://arxiv.org/abs/2408.06069</guid>
<content:encoded><![CDATA[
arXiv:2408.06069v2 Announce Type: replace 
Abstract: Deep Gaussian process models typically employ discrete hierarchies, but recent advancements in differential Gaussian processes (DiffGPs) have extended these models to infinite depths. However, existing DiffGP approaches often overlook the uncertainty in kernel hyperparameters by treating them as fixed and time-invariant, which degrades the model's predictive performance and neglects the posterior distribution. In this work, we introduce a fully Bayesian framework that models kernel hyperparameters as random variables and utilizes coupled stochastic differential equations (SDEs) to jointly learn their posterior distributions alongside those of inducing points. By incorporating the estimation uncertainty of hyperparameters, our method significantly enhances model flexibility and adaptability to complex dynamic systems. Furthermore, we employ a black-box adaptive SDE solver with a neural network to achieve realistic, time varying posterior approximations, thereby improving the expressiveness of the variational posterior. Comprehensive experimental evaluations demonstrate that our approach outperforms traditional methods in terms of flexibility, accuracy, and other key performance metrics. This work not only provides a robust Bayesian extension to DiffGP models but also validates its effectiveness in handling intricate dynamic behaviors, thereby advancing the applicability of Gaussian process models in diverse real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Fraud Detection: Integrating Reinforcement Learning into Graph Neural Networks</title>
<link>https://arxiv.org/abs/2409.09892</link>
<guid>https://arxiv.org/abs/2409.09892</guid>
<content:encoded><![CDATA[
arXiv:2409.09892v2 Announce Type: replace 
Abstract: Financial fraud refers to the act of obtaining financial benefits through dishonest means. Such behavior not only disrupts the order of the financial market but also harms economic and social development and breeds other illegal and criminal activities. With the popularization of the internet and online payment methods, many fraudulent activities and money laundering behaviors in life have shifted from offline to online, posing a great challenge to regulatory authorities. How to efficiently detect these financial fraud activities has become an urgent issue that needs to be resolved. Graph neural networks are a type of deep learning model that can utilize the interactive relationships within graph structures, and they have been widely applied in the field of fraud detection. However, there are still some issues. First, fraudulent activities only account for a very small part of transaction transfers, leading to an inevitable problem of label imbalance in fraud detection. At the same time, fraudsters often disguise their behavior, which can have a negative impact on the final prediction results. In addition, existing research has overlooked the importance of balancing neighbor information and central node information. For example, when the central node has too many neighbors, the features of the central node itself are often neglected. Finally, fraud activities and patterns are constantly changing over time, so considering the dynamic evolution of graph edge relationships is also very important.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels</title>
<link>https://arxiv.org/abs/2410.10473</link>
<guid>https://arxiv.org/abs/2410.10473</guid>
<content:encoded><![CDATA[
arXiv:2410.10473v5 Announce Type: replace 
Abstract: Neural networks are powered by an implicit bias: a tendency of gradient descent to fit training data in a way that generalizes to unseen data. A recent class of neural network models gaining increasing popularity is structured state space models (SSMs), regarded as an efficient alternative to transformers. Prior work argued that the implicit bias of SSMs leads to generalization in a setting where data is generated by a low dimensional teacher. In this paper, we revisit the latter setting, and formally establish a phenomenon entirely undetected by prior work on the implicit bias of SSMs. Namely, we prove that while implicit bias leads to generalization under many choices of training data, there exist special examples whose inclusion in training completely distorts the implicit bias, to a point where generalization fails. This failure occurs despite the special training examples being labeled by the teacher, i.e. having clean labels! We empirically demonstrate the phenomenon, with SSMs trained independently and as part of non-linear neural networks. In the area of adversarial machine learning, disrupting generalization with cleanly labeled training examples is known as clean-label poisoning. Given the proliferation of SSMs, we believe that delineating their susceptibility to clean-label poisoning, and developing methods for overcoming this susceptibility, are critical research directions to pursue.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trojan Cleansing with Neural Collapse</title>
<link>https://arxiv.org/abs/2411.12914</link>
<guid>https://arxiv.org/abs/2411.12914</guid>
<content:encoded><![CDATA[
arXiv:2411.12914v3 Announce Type: replace 
Abstract: Trojan attacks are sophisticated training-time attacks on neural networks that embed backdoor triggers which force the network to produce a specific output on any input which includes the trigger. With the increasing relevance of deep networks which are too large to train with personal resources and which are trained on data too large to thoroughly audit, these training-time attacks pose a significant risk. In this work, we connect trojan attacks to Neural Collapse, a phenomenon wherein the final feature representations of over-parameterized neural networks converge to a simple geometric structure. We provide experimental evidence that trojan attacks disrupt this convergence for a variety of datasets and architectures. We then use this disruption to design a lightweight, broadly generalizable mechanism for cleansing trojan attacks from a wide variety of different network architectures and experimentally demonstrate its efficacy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pruning to Grafting: Dynamic Knowledge Redistribution via Learnable Layer Fusion</title>
<link>https://arxiv.org/abs/2411.14507</link>
<guid>https://arxiv.org/abs/2411.14507</guid>
<content:encoded><![CDATA[
arXiv:2411.14507v3 Announce Type: replace 
Abstract: Structured pruning of Generative Pre-trained Transformers (GPTs) offers a promising path to efficiency but often suffers from irreversible performance degradation due to the discarding of transformer blocks. In this paper, we introduce FuseGPT, a compression paradigm that reframes structured pruning as iterative knowledge grafting rather than simple removal. Motivated by the observation that linear block merging fails to capture non-linear feature disparities and that block importance fluctuates dynamically during pruning, FuseGPT employs a dual-strategy pipeline. First, we propose Macro Influence (MI), a dynamic fusion-aware metric that continuously re-evaluates block redundancy as the network topology evolves. Second, instead of rigid parameter averaging, we introduce a learnable low-rank fusion mechanism that adaptively grafts the knowledge of pruned blocks onto surviving layers via lightweight local distillation. Extensive experiments on LLaMA, Mistral, Qwen, and Phi families demonstrate that FuseGPT establishes a new state-of-the-art on the compression-accuracy Pareto frontier: at 25\% sparsity, FuseGPT achieves lower perplexity than prior methods at 20\% sparsity, improves zero-shot reasoning by up to 4.5 points, and delivers 1.33$\times$ inference speedup with 25\% memory reduction. Furthermore, FuseGPT is orthogonal to quantization, achieving 52.1\% total compression with negligible quality loss when combined with 4-bit GPTQ. We make our code publicly available at https://github.com/JarvisPei/FuseGPT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defending Collaborative Filtering Recommenders via Adversarial Robustness Based Edge Reweighting</title>
<link>https://arxiv.org/abs/2412.10850</link>
<guid>https://arxiv.org/abs/2412.10850</guid>
<content:encoded><![CDATA[
arXiv:2412.10850v2 Announce Type: replace 
Abstract: User based collaborative filtering (CF) relies on a user and user similarity graph, making it vulnerable to profile injection (shilling) attacks that manipulate neighborhood relations to promote (push) or demote (nuke) target items. In this work, we propose an adversarial robustness based edge reweighting defense for CF. We first assign each user and user edge a non robustness score via spectral adversarial robustness evaluation, which quantifies the edge sensitivity to adversarial perturbations. We then attenuate the influence of non robust edges by reweighting similarities during prediction. Extensive experiments demonstrate that the proposed method effectively defends against various types of attacks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural equilibria for long-term prediction of nonlinear conservation laws</title>
<link>https://arxiv.org/abs/2501.06933</link>
<guid>https://arxiv.org/abs/2501.06933</guid>
<content:encoded><![CDATA[
arXiv:2501.06933v2 Announce Type: replace 
Abstract: We introduce Neural Discrete Equilibrium (NeurDE), a machine learning framework for stable and accurate long-term forecasting of nonlinear conservation laws. NeurDE leverages a kinetic lifting that decomposes the dynamics into a fixed linear transport component and a local nonlinear relaxation to equilibrium. This structure provides a natural and principled interface between physics, numerical methods, and machine learning methodologies, enabling NeurDE to be viewed as a ``neural twin'' to Boltzmann-BGK. The transport step can be implemented efficiently in solvers such as lattice Boltzmann (LB), while the equilibrium is modeled by a neural network that maps macroscopic observables to a discrete equilibrium distribution. When integrated into a LB solver, the transport step becomes an efficient lattice streaming operation, and NeurDE yields a hybrid algorithm that robustly captures shock propagation and complex compressible dynamics over long time horizons. The NeurDE method is highly data-efficient: a small network trained on limited data generalizes far beyond the training regime, resolving shocks that evolve well outside the initial training distribution. Unlike traditional kinetic solvers, NeurDE achieves this accuracy without costly root-finding procedures or large velocity lattices. These results establish NeurDE as a scalable, efficient, and physics-informed paradigm for learning-based simulation of nonlinear conservation laws
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State Combinatorial Generalization In Decision Making With Conditional Diffusion Models</title>
<link>https://arxiv.org/abs/2501.13241</link>
<guid>https://arxiv.org/abs/2501.13241</guid>
<content:encoded><![CDATA[
arXiv:2501.13241v2 Announce Type: replace 
Abstract: Many real-world decision-making problems are combinatorial in nature, where states (e.g., surrounding traffic of a self-driving car) can be seen as a combination of basic elements (e.g., pedestrians, trees, and other cars). Due to combinatorial complexity, observing all combinations of basic elements in the training set is infeasible, which leads to an essential yet understudied problem of zero-shot generalization to states that are unseen combinations of previously seen elements. In this work, we first formalize this problem and then demonstrate how existing value-based reinforcement learning (RL) algorithms struggle due to unreliable value predictions in unseen states. We argue that this problem cannot be addressed with exploration alone, but requires more expressive and generalizable models. We demonstrate that behavior cloning with a conditioned diffusion model trained on successful trajectory generalizes better to states formed by new combinations of seen elements than traditional RL methods. Through experiments in maze, driving, and multiagent environments, we show that conditioned diffusion models outperform traditional RL techniques and highlight the broad applicability of our problem formulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Benchmarks: On The False Promise of AI Regulation</title>
<link>https://arxiv.org/abs/2501.15693</link>
<guid>https://arxiv.org/abs/2501.15693</guid>
<content:encoded><![CDATA[
arXiv:2501.15693v2 Announce Type: replace 
Abstract: The performance of AI models on safety benchmarks does not indicate their real-world performance after deployment. This opaqueness of AI models impedes existing regulatory frameworks constituted on benchmark performance, leaving them incapable of mitigating ongoing real-world harm. The problem stems from a fundamental challenge in AI interpretability, which seems to be overlooked by regulators and decision makers. We propose a simple, realistic and readily usable regulatory framework which does not rely on benchmarks, and call for interdisciplinary collaboration to find new ways to address this crucial problem.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training</title>
<link>https://arxiv.org/abs/2502.03604</link>
<guid>https://arxiv.org/abs/2502.03604</guid>
<content:encoded><![CDATA[
arXiv:2502.03604v3 Announce Type: replace 
Abstract: Fine-tuning pre-trained Large Language Models (LLMs) for downstream tasks using First-Order (FO) optimizers presents significant computational challenges. Parameter-Efficient Fine-Tuning (PEFT) methods address these by freezing most model parameters and training only a small subset. However, PEFT often underperforms compared to full fine-tuning when high task-specific accuracy is required. Zeroth-Order (ZO) methods fine-tune the entire pre-trained model without back-propagation, estimating gradients through forward passes only. While memory-efficient, ZO methods suffer from slow convergence and high sensitivity to prompt selection. We bridge these two worlds with Bilevel-ZOFO, a bilevel optimization method that couples fast, local FO-PEFT adaptation at the inner level with stable, memory-efficient ZO updates of the full backbone at the outer level. The FO-PEFT inner loop performs fast, low-memory local adaptation that reduces the variance of ZO estimates and stabilizes the search, guiding the outer ZO updates of the full backbone and reducing prompt sensitivity. In the mean time, the outer ZO provides better generalization ability for PEFT. We provide theoretical convergence guarantees and empirically demonstrate that Bilevel-ZOFO significantly outperforms existing ZO and FO-PEFT methods, achieving 2-4 times faster training while maintaining similar memory efficiency. Additionally, we show by updating the backbone with ZO and adapting only a tiny FO-PEFT block per task, Bilevel-ZOFO combines full-model capacity with few-shot efficiency, making it a very efficient meta-learning algorithm that quickly adapts to new tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Physics-Informed Neural Networks Through Feature Engineering</title>
<link>https://arxiv.org/abs/2502.07209</link>
<guid>https://arxiv.org/abs/2502.07209</guid>
<content:encoded><![CDATA[
arXiv:2502.07209v3 Announce Type: replace 
Abstract: Physics-Informed Neural Networks (PINNs) seek to solve partial differential equations (PDEs) with deep learning. Mainstream approaches that deploy fully-connected multi-layer deep learning architectures require prolonged training to achieve even moderate accuracy, while recent work on feature engineering allows higher accuracy and faster convergence. This paper introduces SAFE-NET, a Single-layered Adaptive Feature Engineering NETwork that achieves orders-of-magnitude lower errors with far fewer parameters than baseline feature engineering methods. SAFE-NET returns to basic ideas in machine learning, using Fourier features, a simplified single hidden layer network architecture, and an effective optimizer that improves the conditioning of the PINN optimization problem. Numerical results show that SAFE-NET converges faster and typically outperforms deeper networks and more complex architectures. It consistently uses fewer parameters -- on average, 65% fewer than the competing feature engineering methods -- while achieving comparable accuracy in less than 30% of the training epochs. Moreover, each SAFE-NET epoch is 95% faster than those of competing feature engineering approaches. These findings challenge the prevailing belief that modern PINNs effectively learn features in these scientific applications and highlight the efficiency gains possible through feature engineering.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Channel Dependence, Limited Lookback Windows, and the Simplicity of Datasets: How Biased is Time Series Forecasting?</title>
<link>https://arxiv.org/abs/2502.09683</link>
<guid>https://arxiv.org/abs/2502.09683</guid>
<content:encoded><![CDATA[
arXiv:2502.09683v2 Announce Type: replace 
Abstract: In Time Series Forecasting (TSF), the lookback window (the length of historical data used for prediction) is a critical hyperparameter that is often set arbitrarily, undermining the validity of model evaluations. We argue that the lookback window must be tuned on a per-task basis to ensure fair comparisons. Our empirical results show that failing to do so can invert performance rankings, particularly when comparing univariate and multivariate methods. Experiments on standard benchmarks reposition Channel-Independent (CI) models, such as PatchTST, as state-of-the-art methods. However, we reveal this superior performance is largely an artifact of weak inter-channel correlations and simplicity of patterns within these specific datasets. Using Granger causality analysis and ODE datasets (with implicit channel correlations), we demonstrate that the true strength of multivariate Channel-Dependent (CD) models emerges on datasets with strong, inherent cross-channel dependencies, where they significantly outperform CI models. We conclude with three key recommendations for improving TSF research: (i) treat the lookback window as a critical hyperparameter to be tuned, (ii) use statistical analysis of datasets to inform the choice between CI and CD architectures, and (iii) favor CD models in applications with limited data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-context Learning of Evolving Data Streams with Tabular Foundational Models</title>
<link>https://arxiv.org/abs/2502.16840</link>
<guid>https://arxiv.org/abs/2502.16840</guid>
<content:encoded><![CDATA[
arXiv:2502.16840v2 Announce Type: replace 
Abstract: State-of-the-art data stream mining has long drawn from ensembles of the Very Fast Decision Tree, a seminal algorithm honored with the 2015 KDD Test-of-Time Award. However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift. These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning. By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing. This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments. Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees, such as Adaptive Random Forest, and Streaming Random Patches, across all non-stationary benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Deep-Learning Iterative Stacked Approach for Prediction of Reactive Dissolution in Porous Media</title>
<link>https://arxiv.org/abs/2503.08410</link>
<guid>https://arxiv.org/abs/2503.08410</guid>
<content:encoded><![CDATA[
arXiv:2503.08410v2 Announce Type: replace 
Abstract: Simulating reactive dissolution of solid minerals in porous media has many subsurface applications, including carbon capture and storage (CCS), geothermal systems and oil & gas recovery. As traditional direct numerical simulators are computationally expensive, it is of paramount importance to develop faster and more efficient alternatives. Deep-learning-based solutions, most of them built upon convolutional neural networks (CNNs), have been recently designed to tackle this problem. However, these solutions were limited to approximating one field over the domain (e.g. velocity field). In this manuscript, we present a novel deep learning approach that incorporates both temporal and spatial information to predict the future states of the dissolution process at a fixed time-step horizon, given a sequence of input states. The overall performance, in terms of speed and prediction accuracy, is demonstrated on a numerical simulation dataset, comparing its prediction results against state-of-the-art approaches, also achieving a speedup around $10^4$ over traditional numerical simulators.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIMRL: Physics-Informed Multi-Scale Recurrent Learning for Burst-Sampled Spatiotemporal Dynamics</title>
<link>https://arxiv.org/abs/2503.10253</link>
<guid>https://arxiv.org/abs/2503.10253</guid>
<content:encoded><![CDATA[
arXiv:2503.10253v4 Announce Type: replace 
Abstract: Deep learning has shown strong potential in modeling complex spatiotemporal dynamics. However, most existing methods depend on densely and uniformly sampled data, which is often unavailable in practice due to sensor and cost limitations. In many real-world settings, such as mobile sensing and physical experiments, data are burst-sampled with short high-frequency segments followed by long gaps, making it difficult to learn accurate dynamics from sparse observations. To address this issue, we propose Physics-Informed Multi-Scale Recurrent Learning (PIMRL), a novel framework specifically designed for burst-sampled spatiotemporal data. PIMRL combines macro-scale latent dynamics inference with micro-scale adaptive refinement guided by incomplete prior information from partial differential equations (PDEs). It further introduces a temporal message-passing mechanism to effectively propagate information across burst intervals. This multi-scale architecture enables PIMRL to model complex systems accurately even under severe data scarcity. We evaluate our approach on five benchmark datasets involving 1D to 3D multi-scale PDEs. The results show that PIMRL consistently outperforms state-of-the-art baselines, achieving substantial improvements and reducing errors by up to 80% in the most challenging settings, which demonstrates the clear advantage of our model. Our work demonstrates the effectiveness of physics-informed recurrent learning for accurate and efficient modeling of sparse spatiotemporal systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus</title>
<link>https://arxiv.org/abs/2503.23229</link>
<guid>https://arxiv.org/abs/2503.23229</guid>
<content:encoded><![CDATA[
arXiv:2503.23229v2 Announce Type: replace 
Abstract: Large Language Models provide significant new opportunities for the generation of high-quality written works. However, their employment in the research community is inhibited by their tendency to hallucinate invalid sources and lack of direct access to a knowledge base of relevant scientific articles. In this work, we present Citegeist: An application pipeline using dynamic Retrieval Augmented Generation (RAG) on the arXiv Corpus to generate a related work section and other citation-backed outputs. For this purpose, we employ a mixture of embedding-based similarity matching, summarization, and multi-stage filtering. To adapt to the continuous growth of the document base, we also present an optimized way of incorporating new and modified papers. To enable easy utilization in the scientific community, we release both, a website (https://citegeist.org), as well as an implementation harness that works with several different LLM implementations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2504.03494</link>
<guid>https://arxiv.org/abs/2504.03494</guid>
<content:encoded><![CDATA[
arXiv:2504.03494v3 Announce Type: replace 
Abstract: Cyber-Physical Systems (CPS) in domains such as manufacturing and energy distribution generate complex time series data crucial for Prognostics and Health Management (PHM). While Deep Learning (DL) methods have demonstrated strong forecasting capabilities, their adoption in industrial CPS remains limited due insufficient robustness. Existing robustness evaluations primarily focus on formal verification or adversarial perturbations, inadequately representing the complexities encountered in real-world CPS scenarios. To address this, we introduce a practical robustness definition grounded in distributional robustness, explicitly tailored to industrial CPS, and propose a systematic framework for robustness evaluation. Our framework simulates realistic disturbances, such as sensor drift, noise and irregular sampling, enabling thorough robustness analyses of forecasting models on real-world CPS datasets. The robustness definition provides a standardized score to quantify and compare model performance across diverse datasets, assisting in informed model selection and architecture design. Through extensive empirical studies evaluating prominent DL architectures (including recurrent, convolutional, attention-based, modular, and structured state-space models) we demonstrate the applicability and effectiveness of our approach. We publicly release our robustness benchmark to encourage further research and reproducibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Algorithm to perform Covariance-Adjusted Support Vector Classification in Non-Euclidean Spaces</title>
<link>https://arxiv.org/abs/2504.04371</link>
<guid>https://arxiv.org/abs/2504.04371</guid>
<content:encoded><![CDATA[
arXiv:2504.04371v2 Announce Type: replace 
Abstract: Traditional Support Vector Machine (SVM) classification is carried out by finding the max-margin classifier for the training data that divides the mar-gin space into two equal sub-spaces. This study demonstrates limitations of performing Support Vector Classification in non-Euclidean spaces by estab-lishing that the underlying principle of max-margin classification and Karush Kuhn Tucker (KKT) boundary conditions are valid only in the Eu-clidean vector spaces, while in non-Euclidean spaces the principle of maxi-mum margin is a function of intra-class data covariance. The study estab-lishes a methodology to perform Support Vector Classification in Non-Euclidean Spaces by incorporating data covariance into the optimization problem using the transformation matrix obtained from Cholesky Decompo-sition of respective class covariance matrices, and shows that the resulting classifier obtained separates the margin space in ratio of respective class pop-ulation covariance. The study proposes an algorithm to iteratively estimate the population covariance-adjusted SVM classifier in non-Euclidean space from sample covariance matrices of the training data. The effectiveness of this SVM classification approach is demonstrated by applying the classifier on multiple datasets and comparing the performance with traditional SVM kernels and whitening algorithms. The Cholesky-SVM model shows marked improvement in the accuracy, precision, F1 scores and ROC performance compared to linear and other kernel SVMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints</title>
<link>https://arxiv.org/abs/2505.05019</link>
<guid>https://arxiv.org/abs/2505.05019</guid>
<content:encoded><![CDATA[
arXiv:2505.05019v2 Announce Type: replace 
Abstract: The generation of synthetic clinical trial data offers a promising approach to mitigating privacy concerns and data accessibility limitations in medical research. However, ensuring that synthetic datasets maintain high fidelity, utility, and adherence to domain-specific constraints remains a key challenge. While hyperparameter optimization (HPO) improves generative model performance, the effectiveness of different optimization strategies for synthetic clinical data remains unclear. This study systematically evaluates four HPO objectives across nine generative models, comparing single-metric to compound metric optimization. Our results demonstrate that HPO consistently improves synthetic data quality, with Tab DDPM achieving the largest relative gains, followed by TVAE (60%), CTGAN (39%), and CTAB-GAN+ (38%). Compound metric optimization outperformed single-metric objectives, producing more generalizable synthetic datasets. Despite improving overall quality, HPO alone fails to prevent violations of essential clinical survival constraints. Preprocessing and postprocessing played a crucial role in reducing these violations, as models lacking robust processing steps produced invalid data in up to 61% of cases. These findings underscore the necessity of integrating explicit domain knowledge alongside HPO to generate high-quality synthetic datasets. Our study provides actionable recommendations for improving synthetic data generation, with future work needed to refine metric selection and validate findings on larger datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees</title>
<link>https://arxiv.org/abs/2505.12919</link>
<guid>https://arxiv.org/abs/2505.12919</guid>
<content:encoded><![CDATA[
arXiv:2505.12919v2 Announce Type: replace 
Abstract: Recovering a low rank matrix from a subset of its entries, some of which may be corrupted, is known as the robust matrix completion (RMC) problem. Existing RMC methods have several limitations: they require a relatively large number of observed entries; they may fail under overparametrization, when their assumed rank is higher than the correct one; and many of them fail to recover even mildly ill-conditioned matrices. In this paper we propose a novel RMC method, denoted $\texttt{RGNMR}$, which overcomes these limitations. $\texttt{RGNMR}$ is a simple factorization-based iterative algorithm, which combines a Gauss-Newton linearization with removal of entries suspected to be outliers. On the theoretical front, we prove that under suitable assumptions, $\texttt{RGNMR}$ is guaranteed exact recovery of the underlying low rank matrix. Our theoretical results improve upon the best currently known for factorization-based methods. On the empirical front, we show via several simulations the advantages of $\texttt{RGNMR}$ over existing RMC methods, and in particular its ability to handle a small number of observed entries, overparameterization of the rank and ill-conditioned matrices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Integrate Diffusion ODEs by Averaging the Derivatives</title>
<link>https://arxiv.org/abs/2505.14502</link>
<guid>https://arxiv.org/abs/2505.14502</guid>
<content:encoded><![CDATA[
arXiv:2505.14502v3 Announce Type: replace 
Abstract: To accelerate diffusion model inference, numerical solvers perform poorly at extremely small steps, while distillation techniques often introduce complexity and instability. This work presents an intermediate strategy, balancing performance and cost, by learning ODE integration using loss functions derived from the derivative-integral relationship, inspired by Monte Carlo integration and Picard iteration. From a geometric perspective, the losses operate by gradually extending the tangent to the secant, thus are named as secant losses. The target of secant losses is the same as that of diffusion models, or the diffusion model itself, leading to great training stability. By fine-tuning or distillation, the secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID of $1.96$ on ImageNet-$256\times256$. Code is available at https://github.com/poppuppy/secant-expectation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cost-aware LLM-based Online Dataset Annotation</title>
<link>https://arxiv.org/abs/2505.15101</link>
<guid>https://arxiv.org/abs/2505.15101</guid>
<content:encoded><![CDATA[
arXiv:2505.15101v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have enabled automated dataset labeling with minimal human supervision. While majority voting across multiple LLMs can improve label reliability by mitigating individual model biases, it incurs high computational costs due to repeated querying. In this work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo), for efficient and accurate LLM-based dataset annotation. CaMVo adaptively selects a subset of LLMs for each data instance based on contextual embeddings, balancing confidence and cost without requiring pre-training or ground-truth labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator over confidence scores, CaMVo estimates a lower bound on labeling accuracy for each LLM and aggregates responses through weighted majority voting. Our empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates that CaMVo achieves comparable or superior accuracy to full majority voting while significantly reducing labeling costs. This establishes CaMVo as a practical and robust solution for cost-efficient annotation in dynamic labeling environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems</title>
<link>https://arxiv.org/abs/2505.15201</link>
<guid>https://arxiv.org/abs/2505.15201</guid>
<content:encoded><![CDATA[
arXiv:2505.15201v4 Announce Type: replace 
Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Graph Pattern Machine</title>
<link>https://arxiv.org/abs/2505.16130</link>
<guid>https://arxiv.org/abs/2505.16130</guid>
<content:encoded><![CDATA[
arXiv:2505.16130v3 Announce Type: replace 
Abstract: Graph neural networks (GNNs) have been predominantly driven by message-passing, where node representations are iteratively updated via local neighborhood aggregation. Despite their success, message-passing suffers from fundamental limitations -- including constrained expressiveness, over-smoothing, over-squashing, and limited capacity to model long-range dependencies. These issues hinder scalability: increasing data size or model size often fails to yield improved performance. To this end, we explore pathways beyond message-passing and introduce Generative Graph Pattern Machine (G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM represents graph instances (nodes, edges, or entire graphs) as sequences of substructures, and employs generative pre-training over the sequences to learn generalizable and transferable representations. Empirically, G$^2$PM demonstrates strong scalability: on the ogbn-arxiv benchmark, it continues to improve with model sizes up to 60M parameters, outperforming prior generative approaches that plateau at significantly smaller scales (e.g., 3M). In addition, we systematically analyze the model design space, highlighting key architectural choices that contribute to its scalability and generalization. Across diverse tasks -- including node/link/graph classification, transfer learning, and cross-graph pretraining -- G$^2$PM consistently outperforms strong baselines, establishing a compelling foundation for scalable graph learning. The code and dataset are available at https://github.com/Zehong-Wang/G2PM.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-reinforcement learning with minimum attention</title>
<link>https://arxiv.org/abs/2505.16741</link>
<guid>https://arxiv.org/abs/2505.16741</guid>
<content:encoded><![CDATA[
arXiv:2505.16741v2 Announce Type: replace 
Abstract: Minimum attention applies the least action principle in the changes of control concerning state and time, first proposed by Brockett. The involved regularization is highly relevant in emulating biological control, such as motor learning. We apply minimum attention in reinforcement learning (RL) as part of the rewards and investigate its connection to meta-learning and stabilization. Specifically, model-based meta-learning with minimum attention is explored in high-dimensional nonlinear dynamics. Ensemble-based model learning and gradient-based meta-policy learning are alternately performed. Empirically, the minimum attention does show outperforming competence in comparison to the state-of-the-art algorithms of model-free and model-based RL, i.e., fast adaptation in few shots and variance reduction from the perturbations of the model and environment. Furthermore, the minimum attention demonstrates an improvement in energy efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Backdoors in DRL: Four Environments Focusing on In-distribution Triggers</title>
<link>https://arxiv.org/abs/2505.17248</link>
<guid>https://arxiv.org/abs/2505.17248</guid>
<content:encoded><![CDATA[
arXiv:2505.17248v3 Announce Type: replace 
Abstract: Backdoor attacks, or trojans, pose a security risk by concealing undesirable behavior in deep neural network models. Open-source neural networks are downloaded from the internet daily, possibly containing backdoors, and third-party model developers are common. To advance research on backdoor attack mitigation, we develop several trojans for deep reinforcement learning (DRL) agents. We focus on in-distribution triggers, which occur within the agent's natural data distribution, since they pose a more significant security threat than out-of-distribution triggers due to their ease of activation by the attacker during model deployment. We implement backdoor attacks in four reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld, Colorful Memory, and Modified Safety Gymnasium. We train various models, both clean and backdoored, to characterize these attacks. We find that in-distribution triggers can require additional effort to implement and be more challenging for models to learn, but are nevertheless viable threats in DRL even using basic data poisoning attacks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparator-Adaptive $\Phi$-Regret: Improved Bounds, Simpler Algorithms, and Applications to Games</title>
<link>https://arxiv.org/abs/2505.17277</link>
<guid>https://arxiv.org/abs/2505.17277</guid>
<content:encoded><![CDATA[
arXiv:2505.17277v2 Announce Type: replace 
Abstract: In the classic expert problem, $\Phi$-regret measures the gap between the learner's total loss and that achieved by applying the best action transformation $\phi \in \Phi$. A recent work by Lu et al., [2025] introduces an adaptive algorithm whose regret against a comparator $\phi$ depends on a certain sparsity-based complexity measure of $\phi$, (almost) recovering and interpolating optimal bounds for standard regret notions such as external, internal, and swap regret. In this work, we propose a general idea to achieve an even better comparator-adaptive $\Phi$-regret bound via much simpler algorithms compared to Lu et al., [2025]. Specifically, we discover a prior distribution over all possible binary transformations and show that it suffices to achieve prior-dependent regret against these transformations. Then, we propose two concrete and efficient algorithms to achieve so, where the first one learns over multiple copies of a prior-aware variant of the Kernelized MWU algorithm of Farina et al., [2022], and the second one learns over multiple copies of a prior-aware variant of the BM-reduction [Blum and Mansour, 2007]. To further showcase the power of our methods and the advantages over Lu et al., [2025] besides the simplicity and better regret bounds, we also show that our second approach can be extended to the game setting to achieve accelerated and adaptive convergence rate to $\Phi$-equilibria for a class of general-sum games. When specified to the special case of correlated equilibria, our bound improves over the existing ones from Anagnostides et al., [2022a,b]
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning</title>
<link>https://arxiv.org/abs/2505.18101</link>
<guid>https://arxiv.org/abs/2505.18101</guid>
<content:encoded><![CDATA[
arXiv:2505.18101v2 Announce Type: replace 
Abstract: Online Continual Learning (OCL) involves sequentially arriving data and is particularly challenged by catastrophic forgetting, which significantly impairs model performance. To address this issue, we introduce a novel framework, Online Dynamic Expandable Dual Memory (ODEDM), that integrates a short-term memory for fast memory and a long-term memory structured into sub-buffers anchored by cluster prototypes, enabling the storage of diverse and category-specific samples to mitigate forgetting. We propose a novel K-means-based strategy for prototype identification and an optimal transport-based mechanism to retain critical samples, prioritising those exhibiting high similarity to their corresponding prototypes. This design preserves semantically rich information. Additionally, we propose a Divide-and-Conquer (DAC) optimisation strategy that decomposes memory updates into subproblems, thereby reducing computational overhead. ODEDM functions as a plug-and-play module that can be seamlessly integrated with existing rehearsal-based approaches. Experimental results under both standard and imbalanced OCL settings show that ODEDM consistently achieves state-of-the-art performance across multiple datasets, delivering substantial improvements over the DER family as well as recent methods such as VR-MCL and POCL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache</title>
<link>https://arxiv.org/abs/2505.18231</link>
<guid>https://arxiv.org/abs/2505.18231</guid>
<content:encoded><![CDATA[
arXiv:2505.18231v2 Announce Type: replace 
Abstract: Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\times$ throughput gain over full-precision baselines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling</title>
<link>https://arxiv.org/abs/2505.19609</link>
<guid>https://arxiv.org/abs/2505.19609</guid>
<content:encoded><![CDATA[
arXiv:2505.19609v2 Announce Type: replace 
Abstract: Long-context supervised fine-tuning (Long-SFT) plays a vital role in enhancing the performance of large language models (LLMs) on long-context tasks. To smoothly adapt LLMs to long-context scenarios, this process typically entails training on mixed datasets containing both long and short sequences. However, this heterogeneous sequence length distribution poses significant challenges for existing training systems, as they fail to simultaneously achieve high training efficiency for both long and short sequences, resulting in sub-optimal end-to-end system performance in Long-SFT. In this paper, we present a novel perspective on data scheduling to address the challenges posed by the heterogeneous data distributions in Long-SFT. We propose Skrull, a dynamic data scheduler specifically designed for efficient long-SFT. Through dynamic data scheduling, Skrull balances the computation requirements of long and short sequences, improving overall training efficiency. Furthermore, we formulate the scheduling process as a joint optimization problem and thoroughly analyze the trade-offs involved. Based on those analysis, Skrull employs a lightweight scheduling algorithm to achieve near-zero cost online scheduling in Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art distributed training system for LLMs. Experimental results demonstrate that Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world long-SFT scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Time Series Forecasting via Instance-aware Post-hoc Revision</title>
<link>https://arxiv.org/abs/2505.23583</link>
<guid>https://arxiv.org/abs/2505.23583</guid>
<content:encoded><![CDATA[
arXiv:2505.23583v2 Announce Type: replace 
Abstract: Time series forecasting plays a vital role in various real-world applications and has attracted significant attention in recent decades. While recent methods have achieved remarkable accuracy by incorporating advanced inductive biases and training strategies, we observe that instance-level variations remain a significant challenge. These variations--stemming from distribution shifts, missing data, and long-tail patterns--often lead to suboptimal forecasts for specific instances, even when overall performance appears strong. To address this issue, we propose a model-agnostic framework, PIR, designed to enhance forecasting performance through Post-forecasting Identification and Revision. Specifically, PIR first identifies biased forecasting instances by estimating their accuracy. Based on this, the framework revises the forecasts using contextual information, including covariates and historical time series, from both local and global perspectives in a post-processing fashion. Extensive experiments on real-world datasets with mainstream forecasting models demonstrate that PIR effectively mitigates instance-level errors and significantly improves forecasting reliability.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title>
<link>https://arxiv.org/abs/2505.24592</link>
<guid>https://arxiv.org/abs/2505.24592</guid>
<content:encoded><![CDATA[
arXiv:2505.24592v3 Announce Type: replace 
Abstract: Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruptions and adversarial attacks. Data augmentation is one of the most prevalent and effective ways to enhance robustness. Despite the great success of the diverse augmentations in different fields, a unified theoretical understanding of their efficacy in improving model robustness is lacking. We theoretically reveal a general condition for label-preserving augmentations to bring robustness to diverse distribution shifts through the lens of flat minima and generalization bound, which de facto turns out to be strongly correlated with robustness against different distribution shifts in practice. Unlike most earlier works, our theoretical framework accommodates all the label-preserving augmentations and is not limited to particular distribution shifts. We substantiate our theories through different simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.24850</link>
<guid>https://arxiv.org/abs/2505.24850</guid>
<content:encoded><![CDATA[
arXiv:2505.24850v2 Announce Type: replace 
Abstract: Recent advances in model distillation show that data from advanced reasoning models can effectively train smaller student models. However, standard practices discard incorrect reasoning traces -- valuable, yet underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? We employ a two-stage training recipe: first, Supervised Fine-Tuning (SFT) on positive traces, followed by a refinement stage using both positive and negative traces. We find that a simple REINFORCE-style objective, which we term the Reinforcement Distillation (REDI) objective, outperforms established preference optimization methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate the effectiveness of this approach. Notably, our Qwen-REDI-1.5B model, trained on just 131k traces from the open Open-R1 dataset, achieves an 83.1% score on MATH-500. Its performance matches that of DeepSeek-R1-Distill-Qwen-1.5B, a model trained on 800k proprietary data. This result showcases the remarkable data efficiency of utilizing previously discarded negative traces.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice</title>
<link>https://arxiv.org/abs/2506.01594</link>
<guid>https://arxiv.org/abs/2506.01594</guid>
<content:encoded><![CDATA[
arXiv:2506.01594v2 Announce Type: replace 
Abstract: As machine learning models are increasingly embedded into society through high-stakes decision-making, selecting the right algorithm for a given task, audience, and sector presents a critical challenge, particularly in the context of fairness. Traditional assessments of model fairness have often framed fairness as an objective mathematical property, treating model selection as an optimization problem under idealized informational conditions. This overlooks model multiplicity as a consideration--that multiple models can deliver similar performance while exhibiting different fairness characteristics. Legal scholars have engaged this challenge through the concept of Less Discriminatory Algorithms (LDAs), which frames model selection as a civil rights obligation. In real-world deployment, this normative challenge is bounded by constraints on fairness experimentation, e.g., regulatory standards, institutional priorities, and resource capacity.
  Against these considerations, the paper revisits Lee and Floridi (2021)'s relational fairness approach using updated 2021 Home Mortgage Disclosure Act (HMDA) data, and proposes an expansion of the scope of the LDA search process. We argue that extending the LDA search horizontally, considering fairness across model families themselves, provides a lightweight complement, or alternative, to within-model hyperparameter optimization, when operationalizing fairness in non-experimental, resource constrained settings. Fairness metrics alone offer useful, but insufficient signals to accurately evaluate candidate LDAs. Rather, by using a horizontal LDA search approach with the relational trade-off framework, we demonstrate a responsible minimum viable LDA search on real-world lending outcomes. Organizations can modify this approach to systematically compare, evaluate, and select LDAs that optimize fairness and accuracy in a sector-based contextualized manner.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification</title>
<link>https://arxiv.org/abs/2506.01983</link>
<guid>https://arxiv.org/abs/2506.01983</guid>
<content:encoded><![CDATA[
arXiv:2506.01983v2 Announce Type: replace 
Abstract: Identification of antimicrobial peptides is an important and necessary issue in today's era. Antimicrobial peptides are essential as an alternative to antibiotics for biomedical applications and many other practical applications. These oligopeptides are useful in drug design and cause innate immunity against microorganisms. Artificial intelligence algorithms have played a significant role in the ease of identifying these peptides.This research is improved by improving proposed method in the field of antimicrobial peptides prediction. Suggested method is improved by combining the best coding method from different perspectives, In the following a deep neural network to balance the imbalanced combined datasets. The results of this research show that the proposed method have a significant improvement in the accuracy and efficiency of the prediction of antimicrobial peptides and are able to provide the best results compared to the existing methods. These development in the field of prediction and classification of antimicrobial peptides, basically in the fields of medicine and pharmaceutical industries, have high effectiveness and application.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RiemannFormer: A Framework for Attention in Curved Spaces</title>
<link>https://arxiv.org/abs/2506.07405</link>
<guid>https://arxiv.org/abs/2506.07405</guid>
<content:encoded><![CDATA[
arXiv:2506.07405v3 Announce Type: replace 
Abstract: This research endeavors to offer insights into unlocking the further potential of transformer-based architectures. One of the primary motivations is to offer a geometric interpretation for the attention mechanism in transformers. In our framework, the attention mainly involves metric tensors, tangent spaces, inner product, and how they relate to each other. These quantities and structures at discrete positions are intricately interconnected via the parallel transport of tangent vectors. To make the learning process more efficient, we reduce the number of parameters through ingenious predefined configurations. Moreover, we introduce an explicit mechanism to highlight a neighborhood by attenuating the remote values, given that transformers inherently neglect local inductive bias. Experimental results demonstrate that our modules deliver significant performance improvements relative to the baseline. More evaluation experiments on visual and large language models will be launched successively.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Pruning via Graph Metanetworks : A Universal Meta Learning Framework for Network Pruning</title>
<link>https://arxiv.org/abs/2506.12041</link>
<guid>https://arxiv.org/abs/2506.12041</guid>
<content:encoded><![CDATA[
arXiv:2506.12041v3 Announce Type: replace 
Abstract: We propose an entirely new meta-learning framework for network pruning. It is a general framework that can be theoretically applied to almost all types of networks with all kinds of pruning and has great generality and transferability. Experiments have shown that it can achieve outstanding results on many popular and representative pruning tasks (including both CNNs and Transformers). Unlike all prior works that either rely on fixed, hand-crafted criteria to prune in a coarse manner, or employ learning to prune ways that require special training during each pruning and lack generality. Our framework can learn complex pruning rules automatically via a neural network (metanetwork) and has great generality that can prune without any special training. More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically and can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and some standard finetuning to prune at state-of-the-art. Our code is available at https://github.com/Yewei-Liu/MetaPruning
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Learning of Disentangled Representations</title>
<link>https://arxiv.org/abs/2506.17182</link>
<guid>https://arxiv.org/abs/2506.17182</guid>
<content:encoded><![CDATA[
arXiv:2506.17182v2 Announce Type: replace 
Abstract: Disentangled representations enable models to separate factors of variation that are shared across experimental conditions from those that are condition-specific. This separation is essential in domains such as biomedical data analysis, where generalization to new treatments, patients, or species depends on isolating stable biological signals from context-dependent effects. While extensions of the variational autoencoder (VAE) framework have been proposed to address this problem, they frequently suffer from leakage between latent representations, limiting their ability to generalize to unseen conditions. Here, we introduce DISCoVeR, a new variational framework that explicitly separates condition-invariant and condition-specific factors. DISCoVeR integrates three key components: (i) a dual-latent architecture that models shared and specific factors separately; (ii) two parallel reconstructions that ensure both representations remain informative; and (iii) a novel max-min objective that encourages clean separation without relying on handcrafted priors, while making only minimal assumptions. Theoretically, we show that this objective maximizes data likelihood while promoting disentanglement, and that it admits a unique equilibrium. Empirically, we demonstrate that DISCoVeR achieves improved disentanglement on synthetic datasets, natural images, and single-cell RNA-seq data. Together, these results establish DISCoVeR as a principled approach for learning disentangled representations in multi-condition settings.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliability-Adjusted Prioritized Experience Replay</title>
<link>https://arxiv.org/abs/2506.18482</link>
<guid>https://arxiv.org/abs/2506.18482</guid>
<content:encoded><![CDATA[
arXiv:2506.18482v3 Announce Type: replace 
Abstract: Experience replay enables data-efficient learning from past experiences in online reinforcement learning agents. Traditionally, experiences were sampled uniformly from a replay buffer, regardless of differences in experience-specific learning potential. In an effort to sample more efficiently, researchers introduced Prioritized Experience Replay (PER). In this paper, we propose an extension to PER by introducing a novel measure of temporal difference error reliability. We theoretically show that the resulting transition selection algorithm, Reliability-adjusted Prioritized Experience Replay (ReaPER), enables more efficient learning than PER. We further present empirical results showing that ReaPER outperforms PER across various environment types, including the Atari-10 benchmark.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Policy Switching for Secure UAV Deconfliction in Adversarial Airspace</title>
<link>https://arxiv.org/abs/2506.21127</link>
<guid>https://arxiv.org/abs/2506.21127</guid>
<content:encoded><![CDATA[
arXiv:2506.21127v3 Announce Type: replace 
Abstract: Autonomous UAV navigation using reinforcement learning (RL) is vulnerable to adversarial attacks that manipulate sensor inputs, potentially leading to unsafe behavior and mission failure. Although robust RL methods provide partial protection, they often struggle to generalize to unseen or out-of-distribution (OOD) attacks due to their reliance on fixed perturbation settings. To address this limitation, we propose a meta-policy switching framework in which a meta-level polic dynamically selects among multiple robust policies to counter unknown adversarial shifts. At the core of this framework lies a discounted Thompson sampling (DTS) mechanism that formulates policy selection as a multi-armed bandit problem, thereby minimizing value distribution shifts via self-induced adversarial observations. We first construct a diverse ensemble of action-robust policies trained under varying perturbation intensities. The DTS-based meta-policy then adaptively selects among these policies online, optimizing resilience against self-induced, piecewise-stationary attacks. Theoretical analysis shows that the DTS mechanism minimizes expected regret, ensuring adaptive robustness to OOD attacks and exhibiting emergent antifragile behavior under uncertainty. Extensive simulations in complex 3D obstacle environments under both white-box (Projected Gradient Descent) and black-box (GPS spoofing) attacks demonstrate significantly improved navigation efficiency and higher conflict free trajectory rates compared to standard robust and vanilla RL baselines, highlighting the practical security and dependability benefits of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems</title>
<link>https://arxiv.org/abs/2506.21502</link>
<guid>https://arxiv.org/abs/2506.21502</guid>
<content:encoded><![CDATA[
arXiv:2506.21502v2 Announce Type: replace 
Abstract: Cyber-Physical Systems (CPSs) tightly interconnect digital and physical operations within production environments, enabling real-time monitoring, control, optimization, and autonomous decision-making that directly enhance manufacturing processes and productivity. The inherent complexity of these systems can lead to faults that require robust and interpretable diagnoses to maintain system dependability and operational efficiency. However, manual modeling of faulty behaviors requires extensive domain expertise and cannot leverage the low-level sensor data of the CPS. Furthermore, although powerful, deep learning-based techniques produce black-box diagnostics that lack interpretability, limiting their practical adoption. To address these challenges, we set forth a method that performs unsupervised characterization of system states and state transitions from low-level sensor data, uses several process mining techniques to model faults through interpretable stochastic Petri nets, simulates such Petri nets for a comprehensive understanding of system behavior under faulty conditions, and performs Petri net-based fault diagnosis. The method is applied to the Robotic Arm Dataset (RoAD), a benchmark collected from a robotic arm deployed in a scale-replica smart manufacturing assembly line. The application to RoAD demonstrates the method's effectiveness in modeling, simulating, and classifying faulty behaviors in CPSs. The modeling results demonstrate that our method achieves a satisfactory interpretability-simulation accuracy trade-off with up to 0.676 arc-degree simplicity, 0.395 R^2, and 0.088 RMSE. In addition, the fault identification results show that the method achieves an F1 score of up to 98.925%, while maintaining a low conformance checking time of 0.020 seconds, which competes with other deep learning-based methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations</title>
<link>https://arxiv.org/abs/2507.01131</link>
<guid>https://arxiv.org/abs/2507.01131</guid>
<content:encoded><![CDATA[
arXiv:2507.01131v4 Announce Type: replace 
Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine learning interatomic potentials (MLIPs). The key operation of such networks is the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To accelerate the computation, we develop tensor decomposition networks (TDNs) as a class of approximately equivariant networks in which CG tensor products are replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP) decomposition. With the CP decomposition, we prove (i) a uniform bound on the induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of approximating any equivariant bilinear map. To further reduce the number of parameters, we propose path-weight sharing that ties all multiplicity-space weights across the $\mathcal{O}(L^3)$ CG paths into a single shared parameter set without compromising equivariance, where $L$ is the maximum angular degree. The resulting layer acts as a plug-and-play replacement for tensor products in existing networks, and the computational complexity of tensor products is reduced from $\mathcal{O}(L^6)$ to $\mathcal{O}(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation dataset containing 105 million DFT-calculated snapshots. We also use existing datasets, including OC20, and OC22. Results show that TDNs achieve competitive performance with dramatic speedup in computations. Our code is publicly available as part of the AIRS library (\href{https://github.com/divelab/AIRS/tree/main/OpenMol/TDN}{https://github.com/divelab/AIRS/}).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift</title>
<link>https://arxiv.org/abs/2507.05412</link>
<guid>https://arxiv.org/abs/2507.05412</guid>
<content:encoded><![CDATA[
arXiv:2507.05412v3 Announce Type: replace 
Abstract: We study the problem of learning robust discriminative representations of causally related latent variables given the underlying causal graph and a training set comprising passively collected observational data and interventional data obtained through targeted interventions on some of these latent variables. We desire to learn representations that are robust against the resulting interventional distribution shifts. Existing approaches treat observational and interventional data alike, ignoring the independence relations arising from these interventions, even with known underlying causal models. As a result, their representations lead to large predictive performance disparities between observational and interventional data. This performance disparity worsens when interventional training data is scarce. In this paper, (1) we first identify a strong correlation between this performance disparity and the representations' violation of statistical independence induced during interventions. (2) For linear models, we derive sufficient conditions on the proportion of interventional training data, for which enforcing statistical independence between representations of the intervened node and its non-descendants during interventions lowers the test-time error on interventional data. Combining these insights, (3) we propose RepLIn, a training algorithm that explicitly enforces this statistical independence between interventional representations. We demonstrate the utility of RepLIn on a synthetic dataset, and on real image and text datasets on facial attribute classification and toxicity detection, respectively, with semi-synthetic causal structures. Our experiments show that RepLIn is scalable with the number of nodes in the causal graph and is suitable to improve robustness against interventional distribution shifts of both continuous and discrete latent variables compared to the ERM baselines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Technical Indicator Networks (TINs): An Interpretable Neural Architecture Modernizing Classic al Technical Analysis for Adaptive Algorithmic Trading</title>
<link>https://arxiv.org/abs/2507.20202</link>
<guid>https://arxiv.org/abs/2507.20202</guid>
<content:encoded><![CDATA[
arXiv:2507.20202v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) have transformed fields such as computer vision and natural language processing by employing architectures aligned with domain-specific structural patterns. In algorithmic trading, however, there remains a lack of architectures that directly incorporate the logic of traditional technical indicators. This study introduces Technical Indicator Networks (TINs), a structured neural design that reformulates rule-based financial heuristics into trainable and interpretable modules. The architecture preserves the core mathematical definitions of conventional indicators while extending them to multidimensional data and supporting optimization through diverse learning paradigms, including reinforcement learning. Analytical transformations such as averaging, clipping, and ratio computation are expressed as vectorized layer operators, enabling transparent network construction and principled initialization. This formulation retains the clarity and interpretability of classical strategies while allowing adaptive adjustment and data-driven refinement. As a proof of concept, the framework is validated on the Dow Jones Industrial Average constituents using a Moving Average Convergence Divergence (MACD) TIN. Empirical results demonstrate improved risk-adjusted performance relative to traditional indicator-based strategies. Overall, the findings suggest that TINs provide a generalizable foundation for interpretable, adaptive, and extensible learning architectures in structured decision-making domains and indicate substantial commercial potential for upgrading trading platforms with cross-market visibility and enhanced decision-support capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Language Models Discover Scaling Laws?</title>
<link>https://arxiv.org/abs/2507.21184</link>
<guid>https://arxiv.org/abs/2507.21184</guid>
<content:encoded><![CDATA[
arXiv:2507.21184v3 Announce Type: replace 
Abstract: Discovering scaling laws for predicting model performance at scale is a fundamental and open-ended challenge, mostly reliant on slow, case specific human experimentation. To investigate the potential for LLMs to automate this process, we collect over 5,000 experiments from existing literature and curate seven diverse scaling law discovery tasks. While existing agents struggle to produce accurate law formulas, this paper introduces SLDAgent, an evolution-based agent that co-optimize the scaling law model and the parameters, enabling it to autonomously explore complex relationships between variables. For the first time, we demonstrates that SLDAgent can automatically discover laws that exhibit consistently more accurate extrapolation than their established, human-derived counterparts across all tasks. Through comprehensive analysis, we elucidate why these discovered laws are superior and verify their practical utility in both pretraining and finetuning applications. This work establishes a new paradigm for agentic scientific discovery, showing that AI systems can understand their own scaling behavior, and can contribute novel and practical knowledge back to the research community.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Generative Ad Text on Facebook using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.21983</link>
<guid>https://arxiv.org/abs/2507.21983</guid>
<content:encoded><![CDATA[
arXiv:2507.21983v2 Announce Type: replace 
Abstract: Generative artificial intelligence (AI), in particular large language models (LLMs), is poised to drive transformative economic change. LLMs are pre-trained on vast text data to learn general language patterns, but a subsequent post-training phase is critical to align them for specific real-world tasks. Reinforcement learning (RL) is the leading post-training technique, yet its economic impact remains largely underexplored and unquantified. We examine this question through the lens of the first deployment of an RL-trained LLM for generative advertising on Facebook. Integrated into Meta's Text Generation feature, our model, "AdLlama," powers an AI tool that helps advertisers create new variations of human-written ad text. To train this model, we introduce reinforcement learning with performance feedback (RLPF), a post-training method that uses historical ad performance data as a reward signal. In a large-scale 10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad variations, we find that AdLlama improves click-through rates by 6.7% (p=0.0296) compared to a supervised imitation model trained on curated ads. This represents a substantial improvement in advertiser return on investment on Facebook. We also find that advertisers who used AdLlama generated more ad variations, indicating higher satisfaction with the model's outputs. To our knowledge, this is the largest study to date on the use of generative AI in an ecologically valid setting, offering an important data point quantifying the tangible impact of RL post-training. Furthermore, the results show that RLPF is a promising and generalizable approach for metric-driven post-training that bridges the gap between highly capable language models and tangible outcomes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer</title>
<link>https://arxiv.org/abs/2508.07710</link>
<guid>https://arxiv.org/abs/2508.07710</guid>
<content:encoded><![CDATA[
arXiv:2508.07710v2 Announce Type: replace 
Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a promising approach for energy-efficient Transformer architectures.While ANN-to-SNN conversion avoids the high training cost of directly trained Spiking Transformers, existing approaches still struggle to handle the nonlinear operations within Transformer blocks, and often require additional fine-tuning of pretrained ANNs.To address these limitations, we propose a training-free and high-performance ANN-to-SNN conversion framework tailored for Transformer architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE) neuron that combines exponential decay with a multi-basis encoding strategy to effectively approximate nonlinear operations, eliminating the need for weight modifications in pretrained ANNs.Extensive experiments across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures (ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless conversion accuracy with significantly lower latency. This provides a promising pathway for the efficient and scalable deployment of Spiking Transformers in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction</title>
<link>https://arxiv.org/abs/2508.11931</link>
<guid>https://arxiv.org/abs/2508.11931</guid>
<content:encoded><![CDATA[
arXiv:2508.11931v2 Announce Type: replace 
Abstract: We present an efficient algorithm for linear contextual bandits with adversarial losses and stochastic action sets. Our approach reduces this setting to misspecification-robust adversarial linear bandits with fixed action sets. Without knowledge of the context distribution or access to a context simulator, the algorithm achieves $\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log K}\})$ regret and runs in $\text{poly}(d,C,T)$ time, where $d$ is the feature dimension, $C$ is an upper bound on the number of linear constraints defining the action set in each round, $K$ is an upper bound on the number of actions in each round, and $T$ is number of rounds. This resolves the open question by Liu et al. (2023) on whether one can obtain $\text{poly}(d)\sqrt{T}$ regret in polynomial time independent of the number of actions. For the important class of combinatorial bandits with adversarial losses and stochastic action sets where the action sets can be described by a polynomial number of linear constraints, our algorithm is the first to achieve $\text{poly}(d)\sqrt{T}$ regret in polynomial time, while no prior algorithm achieves even $o(T)$ regret in polynomial time to our knowledge. When a simulator is available, the regret bound can be improved to $\tilde{O}(d\sqrt{L^\star})$, where $L^\star$ is the cumulative loss of the best policy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations</title>
<link>https://arxiv.org/abs/2508.16634</link>
<guid>https://arxiv.org/abs/2508.16634</guid>
<content:encoded><![CDATA[
arXiv:2508.16634v5 Announce Type: replace 
Abstract: Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to continuously learn from new fault classes with only a few samples without forgetting old ones, is critical for real-world industrial systems. However, this challenging task severely amplifies the issues of catastrophic forgetting of old knowledge and overfitting on scarce new data. To address these challenges, this paper proposes a novel framework built upon Dual-Granularity Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN explicitly decouples feature learning into two parallel streams: 1) a fine-grained representation stream, which utilizes a novel Multi-Order Interaction Aggregation module to capture discriminative, class-specific features from the limited new samples. 2) a coarse-grained representation stream, designed to model and preserve general, class-agnostic knowledge shared across all fault types. These two representations are dynamically fused by a multi-semantic cross-attention mechanism, where the stable coarse-grained knowledge guides the learning of fine-grained features, preventing overfitting and alleviating feature conflicts. To further mitigate catastrophic forgetting, we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a decoupled Balanced Random Forest classifier is employed to counter the decision boundary bias caused by data imbalance. Extensive experiments on the TEP benchmark and a real-world MFF dataset demonstrate that our proposed DGGN achieves superior diagnostic performance and stability compared to state-of-the-art FSC-FD approaches. Our code is publicly available at https://github.com/MentaY/DGGN
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-informed neural network for fatigue life prediction of irradiated austenitic and ferritic/martensitic steels</title>
<link>https://arxiv.org/abs/2508.17303</link>
<guid>https://arxiv.org/abs/2508.17303</guid>
<content:encoded><![CDATA[
arXiv:2508.17303v2 Announce Type: replace 
Abstract: This study proposes a Physics-Informed Neural Network (PINN) framework to predict the low-cycle fatigue (LCF) life of irradiated austenitic and ferritic/martensitic (F/M) steels used in nuclear reactors. During operation, these materials undergo cyclic loading and irradiation at elevated temperatures, resulting in complex degradation mechanisms that traditional empirical or purely data-driven models often fail to capture accurately. The developed PINN model incorporates physical fatigue life constraints into its loss function, improving prediction accuracy, reliability, and generalizability. Trained on 495 data points, including both irradiated and unirradiated conditions, the model is more robust than traditional machine learning models, such as Random Forest, Gradient Boosting, eXtreme Gradient Boosting, and the conventional Neural Network. Model interpretability assessed via SHapley Additive exPlanations analysis revealed that strain amplitude, irradiation dose, and test temperature are the dominant features, each exhibiting a physically consistent inverse correlation with fatigue life. Univariate and multivariate analyses showed that strain amplitude is the primary driver of fatigue degradation in both alloy classes, while irradiation dose and temperature introduce alloy-specific sensitivities. The PINN successfully captured key mechanistic trends, including the comparatively stable irradiation response and dose saturation behaviour of F/M steels, as well as a pronounced reduction in fatigue life at elevated temperatures exceeding the tempering threshold. Overall, the proposed PINN framework offers a reliable and interpretable tool for predicting fatigue life in irradiated structural alloys, thereby supporting informed materials selection and performance assessment for advanced nuclear reactor applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling</title>
<link>https://arxiv.org/abs/2508.17756</link>
<guid>https://arxiv.org/abs/2508.17756</guid>
<content:encoded><![CDATA[
arXiv:2508.17756v2 Announce Type: replace 
Abstract: Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SUPERGEN, an efficient tile-based framework for ultra-high-resolution video generation. SUPERGEN features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SUPERGEN incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SUPERGEN also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations show that SUPERGEN maximizes performance gains while achieving high output quality across various benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An upper bound of the silhouette validation metric for clustering</title>
<link>https://arxiv.org/abs/2509.08625</link>
<guid>https://arxiv.org/abs/2509.08625</guid>
<content:encoded><![CDATA[
arXiv:2509.08625v3 Announce Type: replace 
Abstract: The silhouette coefficient quantifies, for each observation, the balance between within-cluster cohesion and between-cluster separation, taking values in [-1, 1]. The average silhouette width (ASW ) is a widely used internal measure of clustering quality, with higher values indicating more cohesive and well-separated clusters. However, the dataset-specific maximum of ASW is typically unknown, and the standard upper limit of 1 is rarely attainable. In this work, we derive for each data point a sharp upper bound on its silhouette width and aggregate these to obtain a canonical upper bound on the ASW. This bound-often substantially below 1-enhances the interpretability of empirical ASW values by providing guidance on how close a given clustering result is to the best possible outcome for that dataset. We evaluate the usefulness of the upper bound on a variety of datasets and conclude that it can meaningfully enrich cluster quality evaluation, but its practical relevance depends on the given dataset. Finally, we extend the framework to establish an upper bound of the macro-averaged silhouette.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis of Wave Scattering Numerical Modeling Using the Boundary Element Method and Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2509.12483</link>
<guid>https://arxiv.org/abs/2509.12483</guid>
<content:encoded><![CDATA[
arXiv:2509.12483v2 Announce Type: replace 
Abstract: This study compares the Boundary Element Method (BEM) and Physics-Informed Neural Networks (PINNs) for solving the two-dimensional Helmholtz equation in wave scattering problems. The objective is to evaluate the performance of both methods under the same conditions. We solve the Helmholtz equation using BEM and PINNs for the same scattering problem. PINNs are trained by minimizing the residual of the governing equations and boundary conditions with their configuration determined through hyperparameter optimization, while BEM is applied using boundary discretization. Both methods are evaluated in terms of solution accuracy and computation time. We conducted numerical experiments by varying the number of boundary integration points for the BEM and the number of hidden layers and neurons per layer for the PINNs. We performed a hyperparameter tuning to identify an adequate PINN configuration for this problem as a network with 3 hidden layers and 25 neurons per layer, using a learning rate of $10^{-2}$ and a sine activation function. At comparable levels of accuracy, the assembly and solution of the BEM system required a computational time on the order of $10^{-2}$~s, whereas the training time of the PINN was on the order of $10^{2}$~s, corresponding to a difference of approximately four orders of magnitude. However, once trained, the PINN achieved evaluation times on the order of $10^{-2}$~s, which is about two orders of magnitude faster than the evaluation of the BEM solution at interior points. This work establishes a procedure for comparing BEM and PINNs. It also presents a direct comparison between the two methods for the scattering problem. The analysis provides quantitative data on their performance, supporting their use in future research on wave propagation problems and outlining challenges and directions for further investigation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Bilevel Optimization with Heavy-Tailed Noise</title>
<link>https://arxiv.org/abs/2509.14952</link>
<guid>https://arxiv.org/abs/2509.14952</guid>
<content:encoded><![CDATA[
arXiv:2509.14952v2 Announce Type: replace 
Abstract: This paper considers the smooth bilevel optimization in which the lower-level problem is strongly convex and the upper-level problem is possibly nonconvex. We focus on the stochastic setting where the algorithm can access the unbiased stochastic gradient evaluation with heavy-tailed noise, which is prevalent in many machine learning applications, such as training large language models and reinforcement learning. We propose a nested-loop normalized stochastic bilevel approximation (N$^2$SBA) for finding an $\epsilon$-stationary point with the stochastic first-order oracle (SFO) complexity of $\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{4 p - 2}{p-1}}\big)$, where $\kappa$ is the condition number, $p\in(1,2]$ is the order of central moment for the noise, and $\sigma$ is the noise level. Furthermore, we specialize our idea to solve the nonconvex-strongly-concave minimax optimization problem, achieving an $\epsilon$-stationary point with the SFO complexity of~$\tilde{\mathcal O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{3p-2}{p-1}}\big)$. All the above upper bounds match the best-known results under the special case of the bounded variance setting, i.e., $p=2$. We also conduct the numerical experiments to show the empirical superiority of the proposed methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoopQ: Cooperative Game Inspired Layerwise Mixed Precision Quantization for LLMs</title>
<link>https://arxiv.org/abs/2509.15455</link>
<guid>https://arxiv.org/abs/2509.15455</guid>
<content:encoded><![CDATA[
arXiv:2509.15455v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) promise impressive capabilities, yet their multi-billion-parameter scale makes on-device or low-resource deployment prohibitive. Mixed-precision quantization offers a compelling solution, but existing methods struggle when the average precision drops below four bits, as they rely on isolated, layer-specific metrics that overlook critical inter-layer interactions affecting overall performance. To address these limitations, we first frame the mixed-precision quantization problem as a cooperative game among layers and introduce Shapley-based Progressive Quantization Estimation (SPQE) to efficiently obtain accurate Shapley estimates of layer sensitivities and inter-layer interactions. Leveraging the SPQE estimates, we propose Cooperative Game Inspired Mixed-Precision Quantization (CoopQ) which translates these Shapley estimates into a binary quadratic optimization formulation, assigning either 2 or 4-bit precision to layers under strict memory constraints. Comprehensive experiments conducted on Llama-3, Gemma-2, and Qwen-3 models across three independent PTQ backends (Quanto, HQQ, GPTQ) demonstrate CoopQ's scalability and consistently superior performance compared to methods relying solely on isolated metrics. Across average precisions spanning 4 bit down to 2 bit, CoopQ cuts Perplexity by 20 - 80 % relative to the best baseline, with the margin growing as the bit-width tightens.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Highly Imbalanced Regression with Tabular Data in SEP and Other Applications</title>
<link>https://arxiv.org/abs/2509.16339</link>
<guid>https://arxiv.org/abs/2509.16339</guid>
<content:encoded><![CDATA[
arXiv:2509.16339v4 Announce Type: replace 
Abstract: We investigate imbalanced regression with tabular data that have an imbalance ratio larger than 1,000 ("highly imbalanced"). Accurately estimating the target values of rare instances is important in applications such as forecasting the intensity of rare harmful Solar Energetic Particle (SEP) events. For regression, the MSE loss does not consider the correlation between predicted and actual values. Typical inverse importance functions allow only convex functions. Uniform sampling might yield mini-batches that do not have rare instances. We propose CISIR that incorporates correlation, Monotonically Decreasing Involution (MDI) importance, and stratified sampling. Based on five datasets, our experimental results indicate that CISIR can achieve lower error and higher correlation than some recent methods. Also, adding our correlation component to other recent methods can improve their performance. Lastly, MDI importance can outperform other importance functions. Our code can be found in https://github.com/Machine-Earning/CISIR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mamba Modulation: On the Length Generalization of Mamba</title>
<link>https://arxiv.org/abs/2509.19633</link>
<guid>https://arxiv.org/abs/2509.19633</guid>
<content:encoded><![CDATA[
arXiv:2509.19633v3 Announce Type: replace 
Abstract: The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\exp(-\sum_{t=1}^N\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shoot from the HIP: Hessian Interatomic Potentials without derivatives</title>
<link>https://arxiv.org/abs/2509.21624</link>
<guid>https://arxiv.org/abs/2509.21624</guid>
<content:encoded><![CDATA[
arXiv:2509.21624v2 Announce Type: replace 
Abstract: Fundamental tasks in computational chemistry, from transition state search to vibrational analysis, rely on molecular Hessians, which are the second derivatives of the potential energy. Yet, Hessians are computationally expensive to calculate and scale poorly with system size, with both quantum mechanical methods and neural networks. In this work, we demonstrate that Hessians can be predicted directly from a deep learning model, without relying on automatic differentiation or finite differences. We observe that one can construct SE(3)-equivariant, symmetric Hessians from irreducible representations (irrep) features up to degree $l$=2 computed during message passing in graph neural networks. This makes HIP Hessians one to two orders of magnitude faster, more accurate, more memory efficient, easier to train, and enables more favorable scaling with system size. We validate our predictions across a wide range of downstream tasks, demonstrating consistently superior performance for transition state search, accelerated geometry optimization, zero-point energy corrections, and vibrational analysis benchmarks. We open-source the HIP codebase and model weights to enable further development of the direct prediction of Hessians at https://github.com/BurgerAndreas/hip
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Linear Trajectory Modeling for Multi-Step Gradient Inversion Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2509.22082</link>
<guid>https://arxiv.org/abs/2509.22082</guid>
<content:encoded><![CDATA[
arXiv:2509.22082v2 Announce Type: replace 
Abstract: Federated Learning (FL) enables collaborative training while preserving privacy, yet Gradient Inversion Attacks (GIAs) pose severe threats by reconstructing private data from shared gradients. In realistic FedAvg scenarios with multi-step updates, existing surrogate methods like SME rely on linear interpolation to approximate client trajectories for privacy leakage. However, we demonstrate that linear assumptions fundamentally underestimate SGD's nonlinear complexity, encountering irreducible approximation barriers in non-convex landscapes with only one-dimensional expressiveness. We propose Non-Linear Surrogate Model Extension (NL-SME), the first framework introducing learnable quadratic B\'ezier curves for trajectory modeling in GIAs against FL. NL-SME leverages $|w|+1$-dimensional control point parameterization combined with dvec scaling and regularization mechanisms to achieve superior approximation accuracy. Extensive experiments on CIFAR-100 and FEMNIST demonstrate NL-SME significantly outperforms baselines across all metrics, achieving 94\%--98\% performance gaps and order-of-magnitude improvements in cosine similarity loss while maintaining computational efficiency. This work exposes critical privacy vulnerabilities in FL's multi-step paradigm and provides insights for robust defense development.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning</title>
<link>https://arxiv.org/abs/2509.22621</link>
<guid>https://arxiv.org/abs/2509.22621</guid>
<content:encoded><![CDATA[
arXiv:2509.22621v2 Announce Type: replace 
Abstract: Supervised Fine-Tuning (SFT) is used to specialize model behavior by training weights to produce intended target responses for queries. In contrast, In-Context Learning (ICL) adapts models during inference with instructions or demonstrations in the prompt. ICL can offer better generalizability and more calibrated responses compared to SFT in data scarce settings, at the cost of more inference compute. In this work, we ask the question: Can ICL's internal computations be used to improve the qualities of SFT? We first show that ICL and SFT produce distinct activation patterns, indicating that the two methods achieve adaptation through different functional mechanisms. Motivated by this observation and to use ICL's rich functionality, we introduce ICL Activation Alignment (IA2), a self-distillation technique which aims to replicate ICL's activation patterns in SFT models and incentivizes ICL-like internal reasoning. Performing IA2 as a priming step before SFT significantly improves the accuracy and calibration of model outputs, as shown by our extensive empirical results on 12 popular benchmarks and two model families. This finding is not only practically useful, but also offers a conceptual window into the inner mechanics of model adaptation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification</title>
<link>https://arxiv.org/abs/2509.26032</link>
<guid>https://arxiv.org/abs/2509.26032</guid>
<content:encoded><![CDATA[
arXiv:2509.26032v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have demonstrated strong performance across tasks such as node classification, link prediction, and graph classification, but remain vulnerable to backdoor attacks that implant imperceptible triggers during training to control predictions. While node-level attacks exploit local message passing, graph-level attacks face the harder challenge of manipulating global representations while maintaining stealth. We identify two main sources of anomaly in existing graph classification backdoor methods: structural deviation from rare subgraph triggers and semantic deviation caused by label flipping, both of which make poisoned graphs easily detectable by anomaly detection models. To address this, we propose DPSBA, a clean-label backdoor framework that learns in-distribution triggers via adversarial training guided by anomaly-aware discriminators. DPSBA effectively suppresses both structural and semantic anomalies, achieving high attack success while significantly improving stealth. Extensive experiments on real-world datasets validate that DPSBA achieves a superior balance between effectiveness and detectability compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models</title>
<link>https://arxiv.org/abs/2510.04146</link>
<guid>https://arxiv.org/abs/2510.04146</guid>
<content:encoded><![CDATA[
arXiv:2510.04146v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a broad range of Natural Language Processing (NLP) tasks, including document processing and code generation. Autoregressive Language Models (ARMs), which generate tokens sequentially conditioned on all previous tokens, have been the predominant paradigm for LLMs. While these models have achieved high accuracy across a range of downstream tasks, they exhibit low arithmetic intensity due to the inherent sequential dependency in next-token prediction. Recently, Diffusion Language Models (DLMs) have emerged as a promising alternative architecture. DLMs generate output tokens in parallel, mitigating the limitations of sequential decoding. However, the performance implications of DLMs relative to commonly deployed ARMs are not fully understood. In this work, we present a comprehensive study of the performance characteristics of ARMs and DLMs, combining theoretical analysis with empirical profiling to characterize the trade-offs between these approaches. We show that although DLMs can achieve higher arithmetic intensity than ARMs by leveraging parallelism across token positions, they fail to scale effectively with longer contexts. We then explore block-wise decoding for DLMs, which decouples arithmetic intensity from sequence length and enables better scaling to long contexts (similar to ARMs). We also examine batched inference and find that ARMs exhibit superior throughput as they benefit more from parallelism across sequences in the batch. Finally, we highlight opportunities for accelerating DLM inference, emphasizing that reducing the number of sampling steps is key for open-source DLMs to achieve lower latency relative to ARMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.07084</link>
<guid>https://arxiv.org/abs/2510.07084</guid>
<content:encoded><![CDATA[
arXiv:2510.07084v3 Announce Type: replace 
Abstract: Transformer-based methods have achieved impressive results in time series forecasting. However, existing Transformers still exhibit limitations in sequence modeling as they tend to overemphasize temporal dependencies. This incurs additional computational overhead without yielding corresponding performance gains. We find that the performance of Transformers is highly dependent on the embedding method used to learn effective representations. To address this issue, we extract multivariate features to augment the effective information captured in the embedding layer, yielding multidimensional embeddings that convey richer and more meaningful sequence representations. These representations enable Transformer-based forecasters to better understand the series. Specifically, we introduce Hybrid Temporal and Multivariate Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature extraction module with a carefully designed multivariate feature extraction module to provide complementary features, thereby achieving a balance between model complexity and performance. By combining HTME with the Transformer architecture, we present HTMformer, leveraging the enhanced feature extraction capability of the HTME extractor to build a lightweight forecaster. Experiments conducted on eight real-world datasets demonstrate that our approach outperforms existing baselines in both accuracy and efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support</title>
<link>https://arxiv.org/abs/2510.07620</link>
<guid>https://arxiv.org/abs/2510.07620</guid>
<content:encoded><![CDATA[
arXiv:2510.07620v2 Announce Type: replace 
Abstract: Dynamic trust evaluation in large, rapidly evolving graphs demands models that capture changing relationships, express calibrated confidence, and resist adversarial manipulation. DGTEN (Deep Gaussian-Based Trust Evaluation Network) introduces a unified graph-based framework that does all three by combining uncertainty-aware message passing, expressive temporal modeling, and built-in defenses against trust-targeted attacks. It represents nodes and edges as Gaussian distributions so that both semantic signals and epistemic uncertainty propagate through the graph neural network, enabling risk-aware trust decisions rather than overconfident guesses. To track how trust evolves, it layers hybrid absolute-Gaussian-hourglass positional encoding with Kolmogorov-Arnold network-based unbiased multi-head attention, then applies an ordinary differential equation-based residual learning module to jointly model abrupt shifts and smooth trends. Robust adaptive ensemble coefficient analysis prunes or down-weights suspicious interactions using complementary cosine and Jaccard similarity, curbing reputation laundering, sabotage, and on-off attacks. On two signed Bitcoin trust networks, DGTEN delivers standout gains where it matters most: in single-timeslot prediction on Bitcoin-OTC, it improves MCC by +12.34% over the best dynamic baseline; in the cold-start scenario on Bitcoin-Alpha, it achieves a +25.00% MCC improvement, the largest across all tasks and datasets; while under adversarial on-off attacks, it surpasses the baseline by up to +10.23% MCC. These results endorse the unified DGTEN framework.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyGraphEstimator: Adapting Lightweight Language Models for Graph Structure Inference</title>
<link>https://arxiv.org/abs/2510.08808</link>
<guid>https://arxiv.org/abs/2510.08808</guid>
<content:encoded><![CDATA[
arXiv:2510.08808v4 Announce Type: replace 
Abstract: Graphs provide a universal framework for representing complex relational systems, and inferring their structural properties is a core challenge in graph analysis and reasoning. While large language models have recently demonstrated emerging abilities to perform symbolic and numerical reasoning, the potential of smaller, resource-efficient models in this context remains largely unexplored. This paper investigates whether compact transformer-based language models can infer graph-theoretic parameters directly from graph representations. To enable systematic evaluation, we introduce the TinyGraphEstimator dataset - a balanced collection of connected graphs generated from multiple random graph models and annotated with detailed structural metadata. We evaluate several small open models on their ability to predict key graph parameters such as density, clustering, and chromatic number. Furthermore, we apply lightweight fine-tuning using the Low-Rank Adaptation (LoRA) technique, achieving consistent improvements across all evaluated metrics. The results demonstrate that small language models possess non-trivial reasoning capacity over graph-structured data and can be effectively adapted for structural inference tasks through efficient parameter tuning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation</title>
<link>https://arxiv.org/abs/2510.14974</link>
<guid>https://arxiv.org/abs/2510.14974</guid>
<content:encoded><![CDATA[
arXiv:2510.14974v2 Announce Type: replace 
Abstract: Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming previous 1-NFE models of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves substantially better diversity than state-of-the-art DMD models, while maintaining teacher-level quality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Excision Score: Evaluating Edits with Surgical Precision</title>
<link>https://arxiv.org/abs/2510.21537</link>
<guid>https://arxiv.org/abs/2510.21537</guid>
<content:encoded><![CDATA[
arXiv:2510.21537v2 Announce Type: replace 
Abstract: Many tasks revolve around editing a document, whether code or text. We formulate the revision similarity problem to unify a wide range of machine learning evaluation problems whose goal is to assess a revision to an existing document. We observe that revisions usually change only a small portion of an existing document, so the existing document and its immediate revisions share a majority of their content. We formulate five adequacy criteria for revision similarity measures, designed to align them with human judgement. We show that popular pairwise measures, like BLEU, fail to meet these criteria, because their scores are dominated by the shared content. They report high similarity between two revisions when humans would assess them as quite different. This is a fundamental flaw we address. We propose a novel static measure, Excision Score (ES), which computes longest common subsequence (LCS) to remove content shared by an existing document with the ground truth and predicted revisions, before comparing only the remaining divergent regions. This is analogous to a surgeon creating a sterile field to focus on the work area. We use approximation to speed the standard cubic LCS computation to quadratic. In code-editing evaluation, where static measures are often used as a cheap proxy for passing tests, we demonstrate that ES surpasses existing measures. When aligned with test execution on HumanEvalFix, ES improves over its nearest competitor, SARI, by 12% Pearson correlation and by >21% over standard measures like BLEU. The key criterion is invariance to shared context; when we perturb HumanEvalFix with increased shared context, ES' improvement over SARI increases to 20% and >30% over standard measures. ES also handles other corner cases that other measures do not, such as correctly aligning moved code blocks, and appropriately rewarding matching insertions or deletions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Informed Initialization for Bayesian Optimization and Active Learning</title>
<link>https://arxiv.org/abs/2510.23681</link>
<guid>https://arxiv.org/abs/2510.23681</guid>
<content:encoded><![CDATA[
arXiv:2510.23681v2 Announce Type: replace 
Abstract: Bayesian Optimization is a widely used method for optimizing expensive black-box functions, relying on probabilistic surrogate models such as Gaussian Processes. The quality of the surrogate model is crucial for good optimization performance, especially in the few-shot setting where only a small number of batches of points can be evaluated. In this setting, the initialization plays a critical role in shaping the surrogate's predictive quality and guiding subsequent optimization. Despite this, practitioners typically rely on (quasi-)random designs to cover the input space. However, such approaches neglect two key factors: (a) space-filling designs may not be desirable to reduce predictive uncertainty, and (b) efficient hyperparameter learning during initialization is essential for high-quality prediction, which may conflict with space-filling designs. To address these limitations, we propose Hyperparameter-Informed Predictive Exploration (HIPE), a novel acquisition strategy that balances predictive uncertainty reduction with hyperparameter learning using information-theoretic principles. We derive a closed-form expression for HIPE in the Gaussian Process setting and demonstrate its effectiveness through extensive experiments in active learning and few-shot BO. Our results show that HIPE outperforms standard initialization strategies in terms of predictive accuracy, hyperparameter identification, and subsequent optimization performance, particularly in large-batch, few-shot settings relevant to many real-world Bayesian Optimization applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lipschitz-aware Linearity Grafting for Certified Robustness</title>
<link>https://arxiv.org/abs/2510.25130</link>
<guid>https://arxiv.org/abs/2510.25130</guid>
<content:encoded><![CDATA[
arXiv:2510.25130v2 Announce Type: replace 
Abstract: Lipschitz constant is a fundamental property in certified robustness, as smaller values imply robustness to adversarial examples when a model is confident in its prediction. However, identifying the worst-case adversarial examples is known to be an NP-complete problem. Although over-approximation methods have shown success in neural network verification to address this challenge, reducing approximation errors remains a significant obstacle. Furthermore, these approximation errors hinder the ability to obtain tight local Lipschitz constants, which are crucial for certified robustness. Originally, grafting linearity into non-linear activation functions was proposed to reduce the number of unstable neurons, enabling scalable and complete verification. However, no prior theoretical analysis has explained how linearity grafting improves certified robustness. We instead consider linearity grafting primarily as a means of eliminating approximation errors rather than reducing the number of unstable neurons, since linear functions do not require relaxation. In this paper, we provide two theoretical contributions: 1) why linearity grafting improves certified robustness through the lens of the $l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear activation functions, the dominant source of approximation errors, yields a tighter local Lipschitz constant. Based on these theoretical contributions, we propose a Lipschitz-aware linearity grafting method that removes dominant approximation errors, which are crucial for tightening the local Lipschitz constant, thereby improving certified robustness, even without certified training. Our extensive experiments demonstrate that grafting linearity into these influential activations tightens the $l_\infty$ local Lipschitz constant and enhances certified robustness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matrix Sensing with Kernel Optimal Loss: Robustness and Optimization Landscape</title>
<link>https://arxiv.org/abs/2511.02122</link>
<guid>https://arxiv.org/abs/2511.02122</guid>
<content:encoded><![CDATA[
arXiv:2511.02122v2 Announce Type: replace 
Abstract: In this paper we study how the choice of loss functions of non-convex optimization problems affects their robustness and optimization landscape, through the study of noisy matrix sensing. In traditional regression tasks, mean squared error (MSE) loss is a common choice, but it can be unreliable for non-Gaussian or heavy-tailed noise. To address this issue, we adopt a robust loss based on nonparametric regression, which uses a kernel-based estimate of the residual density and maximizes the estimated log-likelihood. This robust formulation coincides with the MSE loss under Gaussian errors but remains stable under more general settings. We further examine how this robust loss reshapes the optimization landscape by analyzing the upper-bound of restricted isometry property (RIP) constants for spurious local minima to disappear. Through theoretical and empirical analysis, we show that this new loss excels at handling large noise and remains robust across diverse noise distributions. This work offers initial insights into enhancing the robustness of machine learning tasks through simply changing the loss, guided by an intuitive and broadly applicable analytical framework.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit</title>
<link>https://arxiv.org/abs/2511.04522</link>
<guid>https://arxiv.org/abs/2511.04522</guid>
<content:encoded><![CDATA[
arXiv:2511.04522v2 Announce Type: replace 
Abstract: With our recently proposed method based on reinforcement learning (Mayfrank et al. (2024), Comput. Chem. Eng. 190), Koopman surrogate models can be trained for optimal performance in specific (economic) nonlinear model predictive control ((e)NMPC) applications. So far, our method has exclusively been demonstrated on a small-scale case study. Herein, we show that our method scales well to a more challenging demand response case study built on a large-scale model of a single-product (nitrogen) air separation unit. Across all numerical experiments, we assume observability of only a few realistically measurable plant variables. Compared to a purely system identification-based Koopman eNMPC, which generates small economic savings but frequently violates constraints, our method delivers similar economic performance while avoiding constraint violations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems</title>
<link>https://arxiv.org/abs/2511.04594</link>
<guid>https://arxiv.org/abs/2511.04594</guid>
<content:encoded><![CDATA[
arXiv:2511.04594v2 Announce Type: replace 
Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics and traffic routing, where agents must coordinate in a decentralized manner to achieve a common objective. Stochastic Shortest Path (SSP) problems provide a natural framework for modeling decentralized control in such settings. While the problem of learning in SSP has been extensively studied in single-agent settings, the decentralized multi-agent variant remains largely unexplored. In this work, we take a step towards addressing that gap. We study decentralized multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the transition dynamics and costs are represented using linear models. Applying novel symmetry-based arguments, we identify the structure of optimal policies. Our main contribution is the first regret lower bound for this setting based on the construction of hard-to-learn instances for any number of agents, $n$. Our regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the inherent learning difficulty in Dec-MASSPs. These insights clarify the learning complexity of decentralized control and can further guide the design of efficient learning algorithms in multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving</title>
<link>https://arxiv.org/abs/2511.06029</link>
<guid>https://arxiv.org/abs/2511.06029</guid>
<content:encoded><![CDATA[
arXiv:2511.06029v3 Announce Type: replace 
Abstract: Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Estimate and Convergence Analysis for Data Valuation</title>
<link>https://arxiv.org/abs/2511.06463</link>
<guid>https://arxiv.org/abs/2511.06463</guid>
<content:encoded><![CDATA[
arXiv:2511.06463v2 Announce Type: replace 
Abstract: Data valuation quantifies data importance, but existing methods cannot ensure validity in a single training process. The neural dynamic data valuation (NDDV) method [3] addresses this limitation. Based on NDDV, we are the first to explore error estimation and convergence analysis in data valuation. Under Lipschitz and smoothness assumptions, we derive quadratic error bounds for loss differences that scale inversely with time steps and quadratically with control variations, ensuring stability. We also prove that the expected squared gradient norm for the training loss vanishes asymptotically, and that the meta loss converges sublinearly over iterations. In particular, NDDV achieves sublinear convergence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers</title>
<link>https://arxiv.org/abs/2511.14465</link>
<guid>https://arxiv.org/abs/2511.14465</guid>
<content:encoded><![CDATA[
arXiv:2511.14465v2 Announce Type: replace 
Abstract: Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj</title>
<link>https://arxiv.org/abs/2511.18248</link>
<guid>https://arxiv.org/abs/2511.18248</guid>
<content:encoded><![CDATA[
arXiv:2511.18248v2 Announce Type: replace 
Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs</title>
<link>https://arxiv.org/abs/2512.01054</link>
<guid>https://arxiv.org/abs/2512.01054</guid>
<content:encoded><![CDATA[
arXiv:2512.01054v2 Announce Type: replace 
Abstract: Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages.
  We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs.
  We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set.
  Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphBench: Next-generation graph learning benchmarking</title>
<link>https://arxiv.org/abs/2512.04475</link>
<guid>https://arxiv.org/abs/2512.04475</guid>
<content:encoded><![CDATA[
arXiv:2512.04475v2 Announce Type: replace 
Abstract: Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval</title>
<link>https://arxiv.org/abs/2512.04524</link>
<guid>https://arxiv.org/abs/2512.04524</guid>
<content:encoded><![CDATA[
arXiv:2512.04524v2 Announce Type: replace 
Abstract: Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception</title>
<link>https://arxiv.org/abs/2512.05089</link>
<guid>https://arxiv.org/abs/2512.05089</guid>
<content:encoded><![CDATA[
arXiv:2512.05089v2 Announce Type: replace 
Abstract: Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.
  This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.
  We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.
  Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</title>
<link>https://arxiv.org/abs/2512.05377</link>
<guid>https://arxiv.org/abs/2512.05377</guid>
<content:encoded><![CDATA[
arXiv:2512.05377v3 Announce Type: replace 
Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 40 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing synthetic microdata through machine learning for firm-level business surveys</title>
<link>https://arxiv.org/abs/2512.05948</link>
<guid>https://arxiv.org/abs/2512.05948</guid>
<content:encoded><![CDATA[
arXiv:2512.05948v2 Announce Type: replace 
Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer</title>
<link>https://arxiv.org/abs/2112.01330</link>
<guid>https://arxiv.org/abs/2112.01330</guid>
<content:encoded><![CDATA[
arXiv:2112.01330v2 Announce Type: replace-cross 
Abstract: Interval and large invasive breast cancers, which are associated with worse prognosis than other cancers, are usually detected at a late stage due to false negative assessments of screening mammograms. The missed screening-time detection is commonly caused by the tumor being obscured by its surrounding breast tissues, a phenomenon called masking. To study and benchmark mammographic masking of cancer, in this work we introduce CSAW-M, the largest public mammographic dataset, collected from over 10,000 individuals and annotated with potential masking. In contrast to the previous approaches which measure breast image density as a proxy, our dataset directly provides annotations of masking potential assessments from five specialists. We also trained deep learning models on CSAW-M to estimate the masking level and showed that the estimated masking is significantly more predictive of screening participants diagnosed with interval and large invasive cancers -- without being explicitly trained for these tasks -- than its breast density counterparts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The prediction of the quality of results in Logic Synthesis using Transformer and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2207.11437</link>
<guid>https://arxiv.org/abs/2207.11437</guid>
<content:encoded><![CDATA[
arXiv:2207.11437v3 Announce Type: replace-cross 
Abstract: In the logic synthesis stage, structure transformations in the synthesis tool need to be combined into optimization sequences and act on the circuit to meet the specified circuit area and delay. However, logic synthesis optimization sequences are time-consuming to run, and predicting the quality of the results (QoR) against the synthesis optimization sequence for a circuit can help engineers find a better optimization sequence faster. In this work, we propose a deep learning method to predict the QoR of unseen circuit-optimization sequences pairs. Specifically, the structure transformations are translated into vectors by embedding methods and advanced natural language processing (NLP) technology (Transformer) is used to extract the features of the optimization sequences. In addition, to enable the prediction process of the model to be generalized from circuit to circuit, the graph representation of the circuit is represented as an adjacency matrix and a feature matrix. Graph neural networks(GNN) are used to extract the structural features of the circuits. For this problem, the Transformer and three typical GNNs are used. Furthermore, the Transformer and GNNs are adopted as a joint learning policy for the QoR prediction of the unseen circuit-optimization sequences. The methods resulting from the combination of Transformer and GNNs are benchmarked. The experimental results show that the joint learning of Transformer and GraphSage gives the best results. The Mean Absolute Error (MAE) of the predicted result is 0.412.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavioral Machine Learning? Regularization and Forecast Bias</title>
<link>https://arxiv.org/abs/2303.16158</link>
<guid>https://arxiv.org/abs/2303.16158</guid>
<content:encoded><![CDATA[
arXiv:2303.16158v3 Announce Type: replace-cross 
Abstract: Standard forecast efficiency tests interpret violations as evidence of behavioral bias. We show theoretically and empirically that rational forecasters using optimal regularization systematically violate these tests. Machine learning forecasts show near zero bias at one year horizon, but strong overreaction at two years, consistent with predictions from a model of regularization and measurement noise. We provide three complementary tests: experimental variation in regularization parameters, cross-sectional heterogeneity in firm signal quality, and quasi-experimental evidence from ML adoption around 2013. Technically trained analysts shift sharply toward overreaction post-2013. Our findings suggest reported violations may reflect statistical sophistication rather than cognitive failure.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Generalization Bounds for Transductive Learning by Transductive Local Complexity and Its Applications</title>
<link>https://arxiv.org/abs/2309.16858</link>
<guid>https://arxiv.org/abs/2309.16858</guid>
<content:encoded><![CDATA[
arXiv:2309.16858v4 Announce Type: replace-cross 
Abstract: We introduce Transductive Local Complexity (TLC) as a new tool for analyzing the generalization performance of transductive learning methods. Our work extends the classical Local Rademacher Complexity (LRC) to the transductive setting, incorporating substantial and novel components. Although LRC has been used to obtain sharp generalization bounds and minimax rates for inductive tasks such as classification and nonparametric regression, it has remained an open problem whether a localized Rademacher complexity framework can be effectively adapted to transductive learning to achieve sharp or nearly sharp bounds consistent with inductive results. We provide an affirmative answer via TLC. TLC is constructed by first deriving a new concentration inequality in Theorem 4.1 for the supremum of empirical processes capturing the gap between test and training losses, termed the test-train process, under uniform sampling without replacement, which leverages a novel combinatorial property of the test-train process and a new proof strategy applying the exponential Efron-Stein inequality twice. A subsequent peeling strategy and a new surrogate variance operator then yield excess risk bounds in the transductive setting that are nearly consistent with classical LRC-based inductive bounds up to a logarithmic gap. We further advance transductive learning through two applications: (1) for realizable transductive learning over binary-valued classes with finite VC dimension of $\dVC$ and $u \ge m \ge \dVC$, where $u$ and $m$ are the number of test features and training features, our Theorem 6.1 gives a nearly optimal bound $\Theta(\dVC \log(me/\dVC)/m)$ matching the minimax rate $\Theta(\dVC/m)$ up to $\log m$, resolving a decade-old open question; and (2) Theorem 6.2 presents a sharper excess risk bound for transductive kernel learning compared to the current state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Anytime Algorithm for Good Arm Identification</title>
<link>https://arxiv.org/abs/2310.10359</link>
<guid>https://arxiv.org/abs/2310.10359</guid>
<content:encoded><![CDATA[
arXiv:2310.10359v2 Announce Type: replace-cross 
Abstract: In good arm identification (GAI), the goal is to identify one arm whose average performance exceeds a given threshold, referred to as a good arm, if it exists. Few works have studied GAI in the fixed-budget setting when the sampling budget is fixed beforehand, or in the anytime setting, when a recommendation can be asked at any time. We propose APGAI, an anytime and parameter-free sampling rule for GAI in stochastic bandits. APGAI can be straightforwardly used in fixed-confidence and fixed-budget settings. First, we derive upper bounds on its probability of error at any time. They show that adaptive strategies can be more efficient in detecting the absence of good arms than uniform sampling in several diverse instances. Second, when APGAI is combined with a stopping rule, we prove upper bounds on the expected sampling complexity, holding at any confidence level. Finally, we show the good empirical performance of APGAI on synthetic and real-world data. Our work offers an extensive overview of the GAI problem in all settings.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"All of Me": Mining Users' Attributes from their Public Spotify Playlists</title>
<link>https://arxiv.org/abs/2401.14296</link>
<guid>https://arxiv.org/abs/2401.14296</guid>
<content:encoded><![CDATA[
arXiv:2401.14296v2 Announce Type: replace-cross 
Abstract: In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences. People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections. In this work, we aim to address the question: can we infer users' private attributes from their public Spotify playlists? To this end, we conducted an online survey involving 739 Spotify users, resulting in a dataset of 10,286 publicly shared playlists comprising over 200,000 unique songs and 55,000 artists. Then, we utilize statistical analyses and machine learning algorithms to build accurate predictive models for users' attributes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Rates of Convergence for Entropy Regularization in Discounted Markov Decision Processes</title>
<link>https://arxiv.org/abs/2406.04163</link>
<guid>https://arxiv.org/abs/2406.04163</guid>
<content:encoded><![CDATA[
arXiv:2406.04163v5 Announce Type: replace-cross 
Abstract: We study the error introduced by entropy regularization in infinite-horizon discrete discounted Markov decision processes. We show that this error decreases exponentially in the inverse regularization strength, both in a weighted KL-divergence and in value with a problem-specific exponent. This is in contrast to previously known estimates, of the order $O(\tau)$, where $\tau$ is the regularization strength. We provide a lower bound that matches our upper bound up to a polynomial term, thereby characterizing the exponential convergence rate for entropy regularization. Our proof relies on the observation that the solutions of entropy-regularized Markov decision processes solve a gradient flow of the unregularized reward with respect to a Riemannian metric common in natural policy gradient methods. This correspondence allows us to identify the limit of this gradient flow as the generalized maximum entropy optimal policy, thereby characterizing the implicit bias of this gradient flow, which corresponds to a time-continuous version of the natural policy gradient method. We use our improved error estimates to show that for entropy-regularized natural policy gradient methods, the overall error decays exponentially in the square root of the number of iterations, improving over existing sublinear guarantees. Finally, we extend our analysis to settings beyond the entropy. In particular, we characterize the implicit bias regarding general convex potentials and their resulting generalized natural policy gradients.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dy-mer: An Explainable DNA Sequence Representation Scheme using Dictionary Learning</title>
<link>https://arxiv.org/abs/2407.12051</link>
<guid>https://arxiv.org/abs/2407.12051</guid>
<content:encoded><![CDATA[
arXiv:2407.12051v2 Announce Type: replace-cross 
Abstract: DNA sequences encode critical genetic information, yet their variable length and discrete nature impede direct utilization in deep learning models. Existing DNA representation schemes convert sequences into numerical vectors but fail to capture structural features of local subsequences and often suffer from limited interpretability and poor generalization on small datasets. To address these limitations, we propose Dy-mer, an interpretable and robust DNA representation scheme based on dictionary learning. Dy-mer formulates an optimization problem in tensor format, which ensures computational efficiency in batch processing. Our scheme reconstructs DNA sequences as concatenations of dynamic-length subsequences (dymers) through a convolution operation and simultaneously optimize a learnable dymer dictionary and sparse representations. Our method achieves state-of-the-art performance in downstream tasks such as DNA promoter classification and motif detection. Experiments further show that the learned dymers match known DNA motifs and clustering using Dy-mer yields semantically meaningful phylogenetic trees. These results demonstrate that the proposed approach achieves both strong predictive performance and high interpretability, making it well suited for biological research applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIC: Circular Image Compression</title>
<link>https://arxiv.org/abs/2407.15870</link>
<guid>https://arxiv.org/abs/2407.15870</guid>
<content:encoded><![CDATA[
arXiv:2407.15870v3 Announce Type: replace-cross 
Abstract: Learned image compression (LIC) is currently the cutting-edge method. However, the inherent difference between testing and training images of LIC results in performance degradation to some extent. Especially for out-of-sample, out-of-distribution, or out-of-domain testing images, the performance of LIC dramatically degraded. Classical LIC is a serial image compression (SIC) approach that utilizes an open-loop architecture with serial encoding and decoding units. Nevertheless, according to the theory of automatic control, a closed-loop architecture holds the potential to improve the dynamic and static performance of LIC. Therefore, a circular image compression (CIC) approach with closed-loop encoding and decoding elements is proposed to minimize the gap between testing and training images and upgrade the capability of LIC. The proposed CIC establishes a nonlinear loop equation and proves that steady-state error between reconstructed and original images is close to zero by Taylor series expansion. The proposed CIC method possesses the property of Post-Training and plug-and-play which can be built on any existing advanced SIC methods. Experimental results on five public image compression datasets demonstrate that the proposed CIC outperforms five competing state-of-the-art open-source SIC algorithms in reconstruction capacity. Experimental results further show that the proposed method is suitable for out-of-sample testing images with dark backgrounds, sharp edges, high contrast, grid shapes, or complex patterns.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Screening is More Efficient with Multiple Objects</title>
<link>https://arxiv.org/abs/2408.10077</link>
<guid>https://arxiv.org/abs/2408.10077</guid>
<content:encoded><![CDATA[
arXiv:2408.10077v2 Announce Type: replace-cross 
Abstract: We study efficient mechanism design for allocating multiple heterogeneous objects. The aim is to maximize the residual surplus, the total value generated from an allocation minus the costs of screening. We discover a robust trend indicating that no-screening mechanisms, such as serial dictatorship with exogenous priority order, tend to perform better as the variety of goods increases. We analyze the underlying reasons by characterizing asymptotically efficient mechanisms in a stylized environment. We also apply an automated mechanism design approach to numerically derive efficient mechanisms and validate the trend in general environments. Building on these implications, we propose the \emph{register-invite-book system} (RIB) as an efficient system for scheduling vaccination against pandemic diseases.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep-ER: Deep Learning ECCENTRIC Reconstruction for fast high-resolution neurometabolic imaging</title>
<link>https://arxiv.org/abs/2409.18303</link>
<guid>https://arxiv.org/abs/2409.18303</guid>
<content:encoded><![CDATA[
arXiv:2409.18303v2 Announce Type: replace-cross 
Abstract: Introduction: Altered neurometabolism is an important pathological mechanism in many neurological diseases and brain cancer, which can be mapped non-invasively by Magnetic Resonance Spectroscopic Imaging (MRSI). Advanced MRSI using non-cartesian compressed-sense acquisition enables fast high-resolution metabolic imaging but has lengthy reconstruction times that limits throughput and needs expert user interaction. Here, we present a robust and efficient Deep Learning reconstruction to obtain high-quality metabolic maps.
  Methods: Fast high-resolution whole-brain metabolic imaging was performed at 3.4 mm$^3$ isotropic resolution with acquisition times between 4:11-9:21 min:s using ECCENTRIC pulse sequence on a 7T MRI scanner. Data were acquired in a high-resolution phantom and 27 human participants, including 22 healthy volunteers and 5 glioma patients. A deep neural network using recurring interlaced convolutional layers with joint dual-space feature representation was developed for deep learning ECCENTRIC reconstruction (Deep-ER). 21 subjects were used for training and 6 subjects for testing. Deep-ER performance was compared to conventional iterative Total Generalized Variation reconstruction using image and spectral quality metrics.
  Results: Deep-ER demonstrated 600-fold faster reconstruction than conventional methods, providing improved spatial-spectral quality and metabolite quantification with 12%-45% (P<0.05) higher signal-to-noise and 8%-50% (P<0.05) smaller Cramer-Rao lower bounds. Metabolic images clearly visualize glioma tumor heterogeneity and boundary.
  Conclusion: Deep-ER provides efficient and robust reconstruction for sparse-sampled MRSI. The accelerated acquisition-reconstruction MRSI is compatible with high-throughput imaging workflow. It is expected that such improved performance will facilitate basic and clinical MRSI applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WALINET: A water and lipid identification convolutional Neural Network for nuisance signal removal in 1H MR Spectroscopic Imaging</title>
<link>https://arxiv.org/abs/2410.00746</link>
<guid>https://arxiv.org/abs/2410.00746</guid>
<content:encoded><![CDATA[
arXiv:2410.00746v2 Announce Type: replace-cross 
Abstract: Purpose. Proton Magnetic Resonance Spectroscopic Imaging (1H-MRSI) provides non-invasive spectral-spatial mapping of metabolism. However, long-standing problems in whole-brain 1H-MRSI are spectral overlap of metabolite peaks with large lipid signal from scalp, and overwhelming water signal that distorts spectra. Fast and effective methods are needed for high-resolution 1H-MRSI to accurately remove lipid and water signals while preserving the metabolite signal. The potential of supervised neural networks for this task remains unexplored, despite their success for other MRSI processing.
  Methods. We introduce a deep-learning method based on a modified Y-NET network for water and lipid removal in whole-brain 1H-MRSI. The WALINET (WAter and LIpid neural NETwork) was compared to conventional methods such as the state-of-the-art lipid L2 regularization and Hankel-Lanczos singular value decomposition (HLSVD) water suppression. Methods were evaluated on simulated and in-vivo whole-brain MRSI using NMRSE, SNR, CRLB, and FWHM metrics.
  Results. WALINET is significantly faster and needs 8s for high-resolution whole-brain MRSI, compared to 42 minutes for conventional HLSVD+L2. Quantitative analysis shows WALINET has better performance than HLSVD+L2: 1) more lipid removal with 41% lower NRMSE, 2) better metabolite signal preservation with 71% lower NRMSE in simulated data, 155% higher SNR and 50% lower CRLB in in-vivo data. Metabolic maps obtained by WALINET in healthy subjects and patients show better gray/white-matter contrast with more visible structural details.
  Conclusions. WALINET has superior performance for nuisance signal removal and metabolite quantification on whole-brain 1H-MRSI compared to conventional state-of-the-art techniques. This represents a new application of deep-learning for MRSI processing, with potential for automated high-throughput workflow.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Markov Chain Gradient Descent in Hilbert Spaces</title>
<link>https://arxiv.org/abs/2410.08361</link>
<guid>https://arxiv.org/abs/2410.08361</guid>
<content:encoded><![CDATA[
arXiv:2410.08361v4 Announce Type: replace-cross 
Abstract: In this paper, we study a Markov chain-based stochastic gradient algorithm in general Hilbert spaces, aiming at approximating the optimal solution of a quadratic loss function. We establish probabilistic upper bounds on its convergence. We further extend these results to an online regularized learning algorithm in reproducing kernel Hilbert spaces, where the samples are drawn along a Markov chain trajectory.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the physics of nested Markov models: a generalized probabilistic theory perspective</title>
<link>https://arxiv.org/abs/2411.11614</link>
<guid>https://arxiv.org/abs/2411.11614</guid>
<content:encoded><![CDATA[
arXiv:2411.11614v2 Announce Type: replace-cross 
Abstract: Determining potential probability distributions with a given causal graph is vital for causality studies. To bypass the difficulty in characterizing latent variables in a Bayesian network, the nested Markov model provides an elegant algebraic approach by listing exactly all the equality constraints on the observed variables. However, this algebraically motivated causal model comprises distributions outside Bayesian networks, and its physical interpretation remains vague. In this work, we inspect the nested Markov model through the lens of generalized probabilistic theory, an axiomatic framework to describe general physical theories. We prove that all the equality constraints defining the nested Markov model are valid theory-independently. At the same time, not every distribution within the nested Markov model is implementable, not even via exotic physical theories associated with generalized probability theories (GPTs). To interpret the origin of such a gap, we study three causal models standing between the nested Markov model and the set of all distributions admitting some GPT realization. Each of the successive three models gives a strictly tighter characterization of the physically implementable distribution set; that is, each successive model manifests new types of GPT-inviolable constraints. We further demonstrate each gap through a specially chosen illustrative causal structure. We anticipate our results will enlighten further explorations on the unification of algebraic and physical perspectives of causality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUOTA: Quantifying Objects with Text-to-Image Models for Any Domain</title>
<link>https://arxiv.org/abs/2411.19534</link>
<guid>https://arxiv.org/abs/2411.19534</guid>
<content:encoded><![CDATA[
arXiv:2411.19534v2 Announce Type: replace-cross 
Abstract: We tackle the problem of quantifying the number of objects by a generative text-to-image model. Rather than retraining such a model for each new image domain of interest, which leads to high computational costs and limited scalability, we are the first to consider this problem from a domain-agnostic perspective. We propose QUOTA, an optimization framework for text-to-image models that enables effective object quantification across unseen domains without retraining. It leverages a dual-loop meta-learning strategy to optimize a domain-invariant prompt. Further, by integrating prompt learning with learnable counting and domain tokens, our method captures stylistic variations and maintains accuracy, even for object classes not encountered during training. For evaluation, we adopt a new benchmark specifically designed for object quantification in domain generalization, enabling rigorous assessment of object quantification accuracy and adaptability across unseen domains in text-to-image generation. Extensive experiments demonstrate that QUOTA outperforms conventional models in both object quantification accuracy and semantic consistency, setting a new benchmark for efficient and scalable text-to-image generation for any domain.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-test loss functions for learning weak-form operators and gradient flows</title>
<link>https://arxiv.org/abs/2412.03506</link>
<guid>https://arxiv.org/abs/2412.03506</guid>
<content:encoded><![CDATA[
arXiv:2412.03506v3 Announce Type: replace-cross 
Abstract: The construction of loss functions presents a major challenge in data-driven modeling involving weak-form operators in PDEs and gradient flows, particularly due to the need to select test functions appropriately. We address this challenge by introducing self-test loss functions, which employ test functions that depend on the unknown parameters, specifically for cases where the operator depends linearly on the unknowns. The proposed self-test loss function conserves energy for gradient flows and coincides with the expected log-likelihood ratio for stochastic differential equations. Importantly, it is quadratic, facilitating theoretical analysis of identifiability and well-posedness of the inverse problem, while also leading to efficient parametric or nonparametric regression algorithms. It is computationally simple, requiring only low-order derivatives or even being entirely derivative-free, and numerical experiments demonstrate its robustness against noisy and discrete data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating AI to Unpack Youth Privacy Concerns: An In-Depth Exploration and Systematic Review</title>
<link>https://arxiv.org/abs/2412.16369</link>
<guid>https://arxiv.org/abs/2412.16369</guid>
<content:encoded><![CDATA[
arXiv:2412.16369v2 Announce Type: replace-cross 
Abstract: This systematic literature review investigates perceptions, concerns, and expectations of young digital citizens regarding privacy in artificial intelligence (AI) systems, focusing on social media platforms, educational technology, gaming systems, and recommendation algorithms. Using a rigorous methodology, the review started with 2,000 papers, narrowed down to 552 after initial screening, and finally refined to 108 for detailed analysis. Data extraction focused on privacy concerns, data-sharing practices, the balance between privacy and utility, trust factors in AI, transparency expectations, and strategies to enhance user control over personal data. Findings reveal significant privacy concerns among young users, including a perceived lack of control over personal information, potential misuse of data by AI, and fears of data breaches and unauthorized access. These issues are worsened by unclear data collection practices and insufficient transparency in AI applications. The intention to share data is closely associated with perceived benefits and data protection assurances. The study also highlights the role of parental mediation and the need for comprehensive education on data privacy. Balancing privacy and utility in AI applications is crucial, as young digital citizens value personalized services but remain wary of privacy risks. Trust in AI is significantly influenced by transparency, reliability, predictable behavior, and clear communication about data usage. Strategies to improve user control over personal data include access to and correction of data, clear consent mechanisms, and robust data protection assurances. The review identifies research gaps and suggests future directions, such as longitudinal studies, multicultural comparisons, and the development of ethical AI frameworks. The findings have significant implications for policy development and educational initiatives
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Embedded Dual-Learning Imaging Framework for Electrical Impedance Tomography</title>
<link>https://arxiv.org/abs/2412.17827</link>
<guid>https://arxiv.org/abs/2412.17827</guid>
<content:encoded><![CDATA[
arXiv:2412.17827v2 Announce Type: replace-cross 
Abstract: Electrical Impedance Tomography (EIT) is a promising noninvasive imaging technique that reconstructs the spatial conductivity distribution from boundary voltage measurements. However, it poses a highly nonlinear and ill-posed inverse problem. Traditional regularization-based methods are sensitive to noise and often produce significant artifacts. Physics-Embedded learning frameworks, particularly Physics-Informed Neural Networks (PINNs), have shown success in solving such inverse problems under ideal conditions with abundant internal data. Yet in practical EIT applications, only sparse and noisy boundary measurements are available. Moreover, changing boundary excitations require the simultaneous training of multiple forward networks and one inverse network, which significantly increases computational complexity and hampers convergence. To overcome these limitations, we propose a Physics-Embedded Dual-Learning Imaging Framework for EIT. The dual-learning strategy is composed of a supervised CNN-based forward network, which learns to predict a discrete internal potential distribution under fixed Neumann-to-Dirichlet boundary conditions, and an unsupervised PINN-based inverse network, which reconstructs the conductivity by enforcing the governing PDE through discrete numerical differentiation of the predicted potentials. This decoupled architecture removes the need for smooth conductivity assumptions, reduces the number of forward networks required from $K$ to 1, and improves reconstruction robustness and efficiency under realistic measurement constraints.(https://github.com/XuanxuanYang/CNN-PINNframework.git)
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compact Neural Network Algorithm for Electrocardiogram Classification</title>
<link>https://arxiv.org/abs/2412.17852</link>
<guid>https://arxiv.org/abs/2412.17852</guid>
<content:encoded><![CDATA[
arXiv:2412.17852v2 Announce Type: replace-cross 
Abstract: In this paper, we present a powerful, compact electrocardiogram (ECG) classification algorithm for cardiac arrhythmia diagnosis that addresses the current reliance on deep learning and convolutional neural networks (CNNs) in ECG analysis. This work aims to reduce the demand for deep learning, which often requires extensive computational resources and large labeled datasets. Our approach introduces an artificial neural network (ANN) with a simple architecture combined with advanced feature engineering techniques. A key contribution of this work is the incorporation of 17 engineered features that enable the extraction of critical patterns from raw ECG signals. By integrating mathematical transformations, signal processing methods, and data extraction algorithms, our model captures the morphological and physiological characteristics of ECG signals with high efficiency, without requiring deep learning. Our method demonstrates a similar performance to other state-of-the-art models in classifying 4 types of arrhythmias, including atrial fibrillation, sinus tachycardia, sinus bradycardia, and ventricular flutter. Our algorithm achieved an accuracy of 97.36% on the MIT-BIH and St. Petersburg INCART arrhythmia databases. Our approach offers a practical and feasible solution for real-time diagnosis of cardiac disorders in medical applications, particularly in resource-limited environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics</title>
<link>https://arxiv.org/abs/2501.10100</link>
<guid>https://arxiv.org/abs/2501.10100</guid>
<content:encoded><![CDATA[
arXiv:2501.10100v5 Announce Type: replace-cross 
Abstract: Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The proposed method employs a dual-autoregressive mechanism and self-supervised training to achieve reliable long-horizon predictions without relying on domain-specific inductive biases, ensuring adaptability across diverse robotic tasks. We further propose a policy optimization framework that leverages world models for efficient training in imagined environments and seamless deployment in real-world systems. This work advances model-based reinforcement learning by addressing the challenges of long-horizon prediction, error accumulation, and sim-to-real transfer. By providing a scalable and robust framework, the introduced methods pave the way for adaptive and efficient robotic systems in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-View Oriented GPLVM: Expressiveness and Efficiency</title>
<link>https://arxiv.org/abs/2502.08253</link>
<guid>https://arxiv.org/abs/2502.08253</guid>
<content:encoded><![CDATA[
arXiv:2502.08253v2 Announce Type: replace-cross 
Abstract: The multi-view Gaussian process latent variable model (MV-GPLVM) aims to learn a unified representation from multi-view data but is hindered by challenges such as limited kernel expressiveness and low computational efficiency. To overcome these issues, we first introduce a new duality between the spectral density and the kernel function. By modeling the spectral density with a bivariate Gaussian mixture, we then derive a generic and expressive kernel termed Next-Gen Spectral Mixture (NG-SM) for MV-GPLVMs. To address the inherent computational inefficiency of the NG-SM kernel, we design a new form of random Fourier feature approximation. Combined with a tailored reparameterization trick, this approximation enables scalable variational inference for both the model and the unified latent representations. Numerical evaluations across a diverse range of multi-view datasets demonstrate that our proposed method consistently outperforms state-of-the-art models in learning meaningful latent representations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning and Computation of $\Phi$-Equilibria at the Frontier of Tractability</title>
<link>https://arxiv.org/abs/2502.18582</link>
<guid>https://arxiv.org/abs/2502.18582</guid>
<content:encoded><![CDATA[
arXiv:2502.18582v3 Announce Type: replace-cross 
Abstract: $\Phi$-equilibria -- and the associated notion of $\Phi$-regret -- are a powerful and flexible framework at the heart of online learning and game theory, whereby enriching the set of deviations $\Phi$ begets stronger notions of rationality. Recently, Daskalakis, Farina, Fishelson, Pipis, and Schneider (STOC '24) -- abbreviated as DFFPS -- settled the existence of efficient algorithms when $\Phi$ contains only linear maps under a general, $d$-dimensional convex constraint set $\mathcal{X}$. In this paper, we significantly extend their work by resolving the case where $\Phi$ is $k$-dimensional; degree-$\ell$ polynomials constitute a canonical such example with $k = d^{O(\ell)}$. In particular, positing only oracle access to $\mathcal{X}$, we obtain two main positive results: i) a $\text{poly}(n, d, k, \text{log}(1/\epsilon))$-time algorithm for computing $\epsilon$-approximate $\Phi$-equilibria in $n$-player multilinear games, and ii) an efficient online algorithm that incurs average $\Phi$-regret at most $\epsilon$ using $\text{poly}(d, k)/\epsilon^2$ rounds.
  We also show nearly matching lower bounds in the online learning setting, thereby obtaining for the first time a family of deviations that captures the learnability of $\Phi$-regret.
  From a technical standpoint, we extend the framework of DFFPS from linear maps to the more challenging case of maps with polynomial dimension. At the heart of our approach is a polynomial-time algorithm for computing an expected fixed point of any $\phi : \mathcal{X} \to \mathcal{X}$ based on the ellipsoid against hope (EAH) algorithm of Papadimitriou and Roughgarden (JACM '08). In particular, our algorithm for computing $\Phi$-equilibria is based on executing EAH in a nested fashion -- each step of EAH itself being implemented by invoking a separate call to EAH.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpurLens: Automatic Detection of Spurious Cues in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2503.08884</link>
<guid>https://arxiv.org/abs/2503.08884</guid>
<content:encoded><![CDATA[
arXiv:2503.08884v2 Announce Type: replace-cross 
Abstract: Unimodal vision models are known to rely on spurious correlations, but it remains unclear to what extent Multimodal Large Language Models (MLLMs) exhibit similar biases despite language supervision. In this paper, we investigate spurious bias in MLLMs and introduce SpurLens, a pipeline that leverages GPT-4 and open-set object detectors to automatically identify spurious visual cues without human supervision. Our findings reveal that spurious correlations cause two major failure modes in MLLMs: (1) over-reliance on spurious cues for object recognition, where removing these cues reduces accuracy, and (2) object hallucination, where spurious cues amplify the hallucination by over 10x. We validate our findings in various MLLMs and datasets. Beyond diagnosing these failures, we explore potential mitigation strategies, such as prompt ensembling and reasoning-based prompting, and conduct ablation studies to examine the root causes of spurious bias in MLLMs. By exposing the persistence of spurious correlations, our study calls for more rigorous evaluation methods and mitigation strategies to enhance the reliability of MLLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Quantum Machine Learning for Multispectral Images Segmentation: Case Study</title>
<link>https://arxiv.org/abs/2503.08962</link>
<guid>https://arxiv.org/abs/2503.08962</guid>
<content:encoded><![CDATA[
arXiv:2503.08962v3 Announce Type: replace-cross 
Abstract: The emergence of Big Data changed how we approach information systems engineering. Nowadays, when we can use remote sensing techniques for Big Data acquisition, the issues such data introduce are as important as ever. One of those concerns is the processing of the data. Classical methods often fail to address that problem or are incapable of processing the data in a reasonable time. With that in mind information system engineers are required to investigate different approaches to the data processing. The recent advancements in noisy intermediate-scale quantum (NISQ) devices implementation allow us to investigate their application to real-life computational problem. This field of study is called quantum (information) systems engineering and usually focuses on technical problems with the contemporary devices. However, hardware challenges are not the only ones that hinder our quantum computation capabilities. Software limitations are the other, less explored side of this medal. Using multispectral image segmentation as a task example, we investigated how difficult it is to run a hybrid quantum-classical model on a real, publicly available quantum device. To quantify how and explain why the performance of our model changed when ran on a real device, we propose new explainability metrics. These metrics introduce new meaning to the explainable quantum machine learning; the explanation of the performance issue comes from the quantum device behavior. We also analyzed the expected money costs of running similar experiment on contemporary quantum devices using standard market prices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CRPS-Based Targeted Sequential Design with Application in Chemical Space</title>
<link>https://arxiv.org/abs/2503.11250</link>
<guid>https://arxiv.org/abs/2503.11250</guid>
<content:encoded><![CDATA[
arXiv:2503.11250v2 Announce Type: replace-cross 
Abstract: Sequential design of real and computer experiments via Gaussian Process (GP) models has proven useful for parsimonious, goal-oriented data acquisition purposes. In this work, we focus on acquisition strategies for a GP model that needs to be accurate within a predefined range of the response of interest. Such an approach is useful in various fields including synthetic chemistry, where finding molecules with particular properties is essential for developing useful materials and effective medications. GP modeling and sequential design of experiments have been successfully applied to a plethora of domains, including molecule research. Our main contribution here is to use the threshold-weighted Continuous Ranked Probability Score (CRPS) as a basic building block for acquisition functions employed within sequential design. We study pointwise and integral criteria relying on two different weighting measures and benchmark them against competitors, demonstrating improved performance with respect to considered goals. The resulting acquisition strategies are applicable to a wide range of fields and pave the way to further developing sequential design relying on scoring rules.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MusicInfuser: Making Video Diffusion Listen and Dance</title>
<link>https://arxiv.org/abs/2503.14505</link>
<guid>https://arxiv.org/abs/2503.14505</guid>
<content:encoded><![CDATA[
arXiv:2503.14505v2 Announce Type: replace-cross 
Abstract: We introduce MusicInfuser, an approach that aligns pre-trained text-to-video diffusion models to generate high-quality dance videos synchronized with specified music tracks. Rather than training a multimodal audio-video or audio-motion model from scratch, our method demonstrates how existing video diffusion models can be efficiently adapted to align with musical inputs. We propose a novel layer-wise adaptability criterion based on a guidance-inspired constructive influence function to select adaptable layers, significantly reducing training costs while preserving rich prior knowledge, even with limited, specialized datasets. Experiments show that MusicInfuser effectively bridges the gap between music and video, generating novel and diverse dance movements that respond dynamically to music. Furthermore, our framework generalizes well to unseen music tracks, longer video sequences, and unconventional subjects, outperforming baseline models in consistency and synchronization. All of this is achieved without requiring motion data, with training completed on a single GPU within a day.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
arXiv:2503.19041v3 Announce Type: replace-cross 
Abstract: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HACTS: a Human-As-Copilot Teleoperation System for Robot Learning</title>
<link>https://arxiv.org/abs/2503.24070</link>
<guid>https://arxiv.org/abs/2503.24070</guid>
<content:encoded><![CDATA[
arXiv:2503.24070v2 Announce Type: replace-cross 
Abstract: Teleoperation is essential for autonomous robot learning, especially in manipulation tasks that require human demonstrations or corrections. However, most existing systems only offer unilateral robot control and lack the ability to synchronize the robot's status with the teleoperation hardware, preventing real-time, flexible intervention. In this work, we introduce HACTS (Human-As-Copilot Teleoperation System), a novel system that establishes bilateral, real-time joint synchronization between a robot arm and teleoperation hardware. This simple yet effective feedback mechanism, akin to a steering wheel in autonomous vehicles, enables the human copilot to intervene seamlessly while collecting action-correction data for future learning. Implemented using 3D-printed components and low-cost, off-the-shelf motors, HACTS is both accessible and scalable. Our experiments show that HACTS significantly enhances performance in imitation learning (IL) and reinforcement learning (RL) tasks, boosting IL recovery capabilities and data efficiency, and facilitating human-in-the-loop RL. HACTS paves the way for more effective and interactive human-robot collaboration and data-collection, advancing the capabilities of robot manipulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Credal Prediction based on Relative Likelihood</title>
<link>https://arxiv.org/abs/2505.22332</link>
<guid>https://arxiv.org/abs/2505.22332</guid>
<content:encoded><![CDATA[
arXiv:2505.22332v2 Announce Type: replace-cross 
Abstract: Predictions in the form of sets of probability distributions, so-called credal sets, provide a suitable means to represent a learner's epistemic uncertainty. In this paper, we propose a theoretically grounded approach to credal prediction based on the statistical notion of relative likelihood: The target of prediction is the set of all (conditional) probability distributions produced by the collection of plausible models, namely those models whose relative likelihood exceeds a specified threshold. This threshold has an intuitive interpretation and allows for controlling the trade-off between correctness and precision of credal predictions. We tackle the problem of approximating credal sets defined in this way by means of suitably modified ensemble learning techniques. To validate our approach, we illustrate its effectiveness by experiments on benchmark datasets demonstrating superior uncertainty representation without compromising predictive performance. We also compare our method against several state-of-the-art baselines in credal prediction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How PARTs assemble into wholes: Learning the relative composition of images</title>
<link>https://arxiv.org/abs/2506.03682</link>
<guid>https://arxiv.org/abs/2506.03682</guid>
<content:encoded><![CDATA[
arXiv:2506.03682v2 Announce Type: replace-cross 
Abstract: The composition of objects and their parts, along with object-object positional relationships, provides a rich source of information for representation learning. Hence, spatial-aware pretext tasks have been actively explored in self-supervised learning. Existing works commonly start from a grid structure, where the goal of the pretext task involves predicting the absolute position index of patches within a fixed grid. However, grid-based approaches fall short of capturing the fluid and continuous nature of real-world object compositions. We introduce PART, a self-supervised learning approach that leverages continuous relative transformations between off-grid patches to overcome these limitations. By modeling how parts relate to each other in a continuous space, PART learns the relative composition of images-an off-grid structural relative positioning that is less tied to absolute appearance and can remain coherent under variations such as partial visibility or stylistic changes. In tasks requiring precise spatial understanding such as object detection and time series prediction, PART outperforms grid-based methods like MAE and DropPos, while maintaining competitive performance on global classification tasks. By breaking free from grid constraints, PART opens up a new trajectory for universal self-supervised pretraining across diverse datatypes-from images to EEG signals-with potential in medical imaging, video, and audio.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>U-NetMN and SegNetMN: Modified U-Net and SegNet models for bimodal SAR image segmentation</title>
<link>https://arxiv.org/abs/2506.05444</link>
<guid>https://arxiv.org/abs/2506.05444</guid>
<content:encoded><![CDATA[
arXiv:2506.05444v2 Announce Type: replace-cross 
Abstract: Segmenting Synthetic Aperture Radar (SAR) images is crucial for many remote sensing applications, particularly water body detection. However, deep learning-based segmentation models often face challenges related to convergence speed and stability, mainly due to the complex statistical distribution of this type of data. In this study, we evaluate the impact of mode normalization on two widely used semantic segmentation models, U-Net and SegNet. Specifically, we integrate mode normalization, to reduce convergence time while maintaining the performance of the baseline models. Experimental results demonstrate that mode normalization significantly accelerates convergence. Furthermore, cross-validation results indicate that normalized models exhibit increased stability in different zones. These findings highlight the effectiveness of normalization in improving computational efficiency and generalization in SAR image segmentation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Inequality Proofs with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07927</link>
<guid>https://arxiv.org/abs/2506.07927</guid>
<content:encoded><![CDATA[
arXiv:2506.07927v3 Announce Type: replace-cross 
Abstract: Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multipole Attention for Efficient Long Context Reasoning</title>
<link>https://arxiv.org/abs/2506.13059</link>
<guid>https://arxiv.org/abs/2506.13059</guid>
<content:encoded><![CDATA[
arXiv:2506.13059v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extreme mass distributions for quasi-copulas</title>
<link>https://arxiv.org/abs/2506.20672</link>
<guid>https://arxiv.org/abs/2506.20672</guid>
<content:encoded><![CDATA[
arXiv:2506.20672v2 Announce Type: replace-cross 
Abstract: A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R. Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the dependence modeling community in spite of the lack of statistical interpretation of quasi-copulas. In our previous work (Fuzzy Sets and Systems 517 (2025) 109457), we addressed the question of extreme values of the mass distribution associated with multivariate quasi-copulas. Using a linear programming approach, we were able to solve Open Problem 5 of the "Guide" up to dimension d = 17 and disprove a recent conjecture on the solution to that problem. In this paper, we use an analytical approach to provide a complete answer to the original question.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incoherence as Oracle-less Measure of Error in LLM-Based Code Generation</title>
<link>https://arxiv.org/abs/2507.00057</link>
<guid>https://arxiv.org/abs/2507.00057</guid>
<content:encoded><![CDATA[
arXiv:2507.00057v2 Announce Type: replace-cross 
Abstract: Generating code from a natural language programming task is one of the most successful applications of Large Language Models (LLMs). Yet, the generated program may be buggy. Without an oracle, such as an existing, correct implementation or a formal specification, can we somehow estimate how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called *incoherence*, that can be estimated efficiently in the absence of an oracle and allows us to establish a lower bound on the error, i.e., the probability that the LLM-generated program for that specification is incorrect. In our experiments, our incoherence-based methodology can automatically identify about two-thirds of incorrect programs without reports of false positives for the average task. In fact, *an oracle-based evaluation of LLMs can be reliably replaced by an incoherence-based evaluation*. In particular, we find a very strong agreement between the ranking of LLMs by the number of programs deemed correct via an oracle (pass@1) and the ranking of LLMs by the number of programs deemed correct via incoherence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective</title>
<link>https://arxiv.org/abs/2507.05622</link>
<guid>https://arxiv.org/abs/2507.05622</guid>
<content:encoded><![CDATA[
arXiv:2507.05622v3 Announce Type: replace-cross 
Abstract: The widespread application of Deep Learning across diverse domains hinges critically on the quality and composition of training datasets. However, the common lack of disclosure regarding their usage raises significant privacy and copyright concerns. Dataset auditing techniques, which aim to determine if a specific dataset was used to train a given suspicious model, provide promising solutions to addressing these transparency gaps. While prior work has developed various auditing methods, their resilience against dedicated adversarial attacks remains largely unexplored. To bridge the gap, this paper initiates a comprehensive study evaluating dataset auditing from an adversarial perspective. We start with introducing a novel taxonomy, classifying existing methods based on their reliance on internal features (IF) (inherent to the data) versus external features (EF) (artificially introduced for auditing). Subsequently, we formulate two primary attack types: evasion attacks, designed to conceal the use of a dataset, and forgery attacks, intending to falsely implicate an unused dataset. Building on the understanding of existing methods and attack objectives, we further propose systematic attack strategies: decoupling, removal, and detection for evasion; adversarial example-based methods for forgery. These formulations and strategies lead to our new benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9 representative auditing methods. Extensive evaluations using DATABench reveal that none of the evaluated auditing methods are sufficiently robust or distinctive under adversarial settings. These findings underscore the urgent need for developing a more secure and reliable dataset auditing method capable of withstanding sophisticated adversarial manipulation. Code is available in https://github.com/shaoshuo-ss/DATABench.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2507.18577</link>
<guid>https://arxiv.org/abs/2507.18577</guid>
<content:encoded><![CDATA[
arXiv:2507.18577v2 Announce Type: replace-cross 
Abstract: The advent of foundation models (FMs), large-scale pre-trained models with strong generalization capabilities, has opened new frontiers for financial engineering. While general-purpose FMs such as GPT-4 and Gemini have demonstrated promising performance in tasks ranging from financial report summarization to sentiment-aware forecasting, many financial applications remain constrained by unique domain requirements such as multimodal reasoning, regulatory compliance, and data privacy. These challenges have spurred the emergence of financial foundation models (FFMs): a new class of models explicitly designed for finance. This survey presents a comprehensive overview of FFMs, with a taxonomy spanning three key modalities: financial language foundation models (FinLFMs), financial time-series foundation models (FinTSFMs), and financial visual-language foundation models (FinVLFMs). We review their architectures, training methodologies, datasets, and real-world applications. Furthermore, we identify critical challenges in data availability, algorithmic scalability, and infrastructure constraints and offer insights into future research opportunities. We hope this survey can serve as both a comprehensive reference for understanding FFMs and a practical roadmap for future innovation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: "One Map, Many Trials" in Satellite-Driven Poverty Analysis</title>
<link>https://arxiv.org/abs/2508.01341</link>
<guid>https://arxiv.org/abs/2508.01341</guid>
<content:encoded><![CDATA[
arXiv:2508.01341v4 Announce Type: replace-cross 
Abstract: Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a "one map, many trials" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving</title>
<link>https://arxiv.org/abs/2509.02718</link>
<guid>https://arxiv.org/abs/2509.02718</guid>
<content:encoded><![CDATA[
arXiv:2509.02718v3 Announce Type: replace-cross 
Abstract: Increasing demand for Large Language Models (LLMs) services imposes substantial deployment and computation costs on providers. LLM routing offers a cost-efficient solution by directing queries to the optimal LLM based on model and query features. However, existing works primarily focus on offline scenarios and struggle to adapt to online settings with high query volume and constrained token budgets. In this work, we introduce the first training-free algorithm for online routing scenarios. Our algorithm leverages approximate nearest neighbor search to efficiently estimate query features and performs a one-time optimization over a small set of initial queries to learn a routing strategy that guides future routing. We provide theoretical guarantees demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$ under natural assumptions, which is further validated by extensive experiments across 3 benchmark datasets and 8 baselines, showing an average improvement of 3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and nearly 4.25$\times$ in throughput. Our code is available at https://github.com/fzwark/PORT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2509.14285</link>
<guid>https://arxiv.org/abs/2509.14285</guid>
<content:encoded><![CDATA[
arXiv:2509.14285v3 Announce Type: replace-cross 
Abstract: Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference</title>
<link>https://arxiv.org/abs/2509.21791</link>
<guid>https://arxiv.org/abs/2509.21791</guid>
<content:encoded><![CDATA[
arXiv:2509.21791v2 Announce Type: replace-cross 
Abstract: Structured output from large language models (LLMs) has enhanced efficiency in processing generated information and is increasingly adopted in industrial applications. Prior studies have investigated the impact of structured output on LLMs' generation quality, often presenting one-way findings. Some suggest that structured format enhances completeness and factual accuracy, while others argue that it restricts the reasoning capacity of LLMs and leads to reductions in standard evaluation metrics. Potential limitations of these assessments include restricted testing scenarios, weakly controlled comparative settings, and reliance on coarse metrics. In this work, we present a refined analysis using causal inference. Based on one assumed and two guaranteed constraints, we derive five potential causal structures characterizing the influence of structured output on LLMs' generation: (1) collider without m-bias, (2) collider with m-bias, (3) single cause from instruction, (4) single cause from output format, and (5) independence. Across seven public and one developed reasoning tasks, we find that coarse metrics report positive, negative, or neutral effects of structured output on GPT-4o's generation. However, causal inference reveals no causal impact in 43 out of 48 scenarios. In the remaining 5, 3 involve multifaceted causal structures influenced by concrete instructions. Further experiments show that OpenAI-o3 are more resilient to output formats than general-purpose GPT-4o and GPT-4.1, highlighting an unaware advantage of reasoning models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations</title>
<link>https://arxiv.org/abs/2509.24250</link>
<guid>https://arxiv.org/abs/2509.24250</guid>
<content:encoded><![CDATA[
arXiv:2509.24250v2 Announce Type: replace-cross 
Abstract: Teaching systems physical tasks is a long standing goal in HCI, yet most prior work has focused on non collaborative physical activities. Collaborative tasks introduce added complexity, requiring systems to infer users assumptions about their teammates intent, which is an inherently ambiguous and dynamic process. This necessitates representations that are interpretable and correctable, enabling users to inspect and refine system behavior. We address this challenge by framing collaborative task learning as a program synthesis problem. Our system represents behavior as editable programs and uses narrated demonstrations, i.e. paired physical actions and natural language, as a unified modality for teaching, inspecting, and correcting system logic without requiring users to see or write code. The same modality is used for the system to communicate its learning to users. In a within subjects study, 20 users taught multiplayer soccer tactics to our system. 70 percent (14/20) of participants successfully refined learned programs to match their intent and 90 percent (18/20) found it easy to correct the programs. The study surfaced unique challenges in representing learning as programs and in enabling users to teach collaborative physical activities. We discuss these issues and outline mitigation strategies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models</title>
<link>https://arxiv.org/abs/2510.14925</link>
<guid>https://arxiv.org/abs/2510.14925</guid>
<content:encoded><![CDATA[
arXiv:2510.14925v3 Announce Type: replace-cross 
Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition in linear-Gaussian state-space models via H-Risk, a composite instability index integrating spectral margin, conditioning, temporal sensitivity, and innovation amplification. In simulations, higher H-Risk predicts overconfident errors and degraded closed-loop behavior even when the dynamics remain formally stable, exposing a gap between nominal and epistemic stability.
  Extending this stability lens to large language models (LLMs), we introduce a domain-wise proxy based on confidence fluctuations and overconfident errors. In a binary-question study, a Kantian-inspired policy that permits ''cannot judge'' responses yields targeted reductions in policy-aware squared loss in high-stakes domains relative to an overconfident baseline. To probe internal dynamics, we analyse layer-wise sensitivity of hidden states to small input perturbations. Contrary to a naive instability hypothesis, confidently wrong answers show no instability gap; instead, they are at least as locally stable as confidently correct answers, revealing stable miscalibration in which hallucinations behave like robust but misaligned attractors. For Qwen-2.5, spectral and activation profiles suggest a high signal-to-noise, low effective signal temperature regime in which representations become inertial and resistant to contextual shifts. These results bridge Kantian self-limitation and feedback control, and suggest that stable high-confidence hallucinations may not be readily corrected by output-only heuristics (e.g., temperature scaling or re-sampling), motivating process-level interventions that explicitly perturb and re-evaluate the inference trajectory.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue</title>
<link>https://arxiv.org/abs/2510.21720</link>
<guid>https://arxiv.org/abs/2510.21720</guid>
<content:encoded><![CDATA[
arXiv:2510.21720v2 Announce Type: replace-cross 
Abstract: The confluence of Artificial Intelligence and Computational Psychology presents an opportunity to model, understand, and interact with complex human psychological states through computational means. This paper presents a comprehensive, multi-faceted framework designed to bridge the gap between isolated predictive modeling and an interactive system for psychological analysis. The methodology encompasses a rigorous, end-to-end development lifecycle. First, foundational performance benchmarks were established on four diverse psychological datasets using classical machine learning techniques. Second, state-of-the-art transformer models were fine-tuned, a process that necessitated the development of effective solutions to overcome critical engineering challenges, including the resolution of numerical instability in regression tasks and the creation of a systematic workflow for conducting large-scale training under severe resource constraints. Third, a generative large language model (LLM) was fine-tuned using parameter-efficient techniques to function as an interactive "Personality Brain." Finally, the entire suite of predictive and generative models was architected and deployed as a robust, scalable microservices ecosystem. Key findings include the successful stabilization of transformer-based regression models for affective computing, showing meaningful predictive performance where standard approaches failed, and the development of a replicable methodology for democratizing large-scale AI research. The significance of this work lies in its holistic approach, demonstrating a complete research-to-deployment pipeline that integrates predictive analysis with generative dialogue, thereby providing a practical model for future research in computational psychology and human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A PyTorch Framework for Scalable Non-Crossing Quantile Regression</title>
<link>https://arxiv.org/abs/2510.22419</link>
<guid>https://arxiv.org/abs/2510.22419</guid>
<content:encoded><![CDATA[
arXiv:2510.22419v2 Announce Type: replace-cross 
Abstract: Quantile regression is fundamental to distributional modeling, yet independent estimation of multiple quantiles frequently produces crossing -- where estimated quantile functions violate monotonicity, implying impossible negative probability densities. While Constrained Joint Quantile Regression (CJQR) elegantly enforces non-crossing by construction, existing formulations via Linear Programming exhibit $O((qn)^3)$ complexity, rendering them impractical for large-scale applications. We present the first scalable solution using PyTorch automatic differentiation: \textbf{CJQR-ALM}, combining the \textbf{Augmented Lagrangian Method} with \textbf{differentiable pinball loss} and \textbf{L-BFGS} optimization. Our approach reduces computational complexity to $O(n)$, achieving near-zero crossing rates on datasets exceeding 70,000 observations within minutes. The differentiable formulation naturally extends to neural network architectures for non-linear conditional quantile estimation. Application to Student Growth Percentile calculations demonstrates practical utility for educational assessment, while simulation studies show negligible accuracy cost (RMSE increase $\approx 2.4$ points) relative to unconstrained estimation -- a favorable trade-off for applications requiring valid probability statements across finance, healthcare, and engineering.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AQCat25: Unlocking spin-aware, high-fidelity machine learning potentials for heterogeneous catalysis</title>
<link>https://arxiv.org/abs/2510.22938</link>
<guid>https://arxiv.org/abs/2510.22938</guid>
<content:encoded><![CDATA[
arXiv:2510.22938v2 Announce Type: replace-cross 
Abstract: Large-scale datasets have enabled highly accurate machine learning interatomic potentials (MLIPs) for general-purpose heterogeneous catalysis modeling. There are, however, some limitations in what can be treated with these potentials because of gaps in the underlying training data. To extend these capabilities, we introduce AQCat25, a complementary dataset of 13.5 million density functional theory (DFT) single point calculations designed to improve the treatment of systems where spin polarization and/or higher fidelity are critical. We also investigate methodologies for integrating new datasets, such as AQCat25, with the broader Open Catalyst 2020 (OC20) dataset to create spin-aware models without sacrificing generalizability. We find that directly tuning a general model on AQCat25 leads to catastrophic forgetting of the original dataset's knowledge. Conversely, joint training strategies prove effective for improving accuracy on the new data without sacrificing general performance. This joint approach introduces a challenge, as the model must learn from a dataset containing both mixed-fidelity calculations and mixed-physics (spin-polarized vs. unpolarized). We show that explicitly conditioning the model on this system-specific metadata, for example by using Feature-wise Linear Modulation (FiLM), successfully addresses this challenge and further enhances model accuracy. Ultimately, our work establishes an effective protocol for bridging DFT fidelity domains to advance the predictive power of foundational models in catalysis.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Convergence Analysis of DDPM for General Distributions</title>
<link>https://arxiv.org/abs/2510.27562</link>
<guid>https://arxiv.org/abs/2510.27562</guid>
<content:encoded><![CDATA[
arXiv:2510.27562v2 Announce Type: replace-cross 
Abstract: Score-based diffusion models have achieved remarkable empirical success in generating high-quality samples from target data distributions. Among them, the Denoising Diffusion Probabilistic Model (DDPM) is one of the most widely used samplers, generating samples via estimated score functions. Despite its empirical success, a tight theoretical understanding of DDPM -- especially its convergence properties -- remains limited.
  In this paper, we provide a refined convergence analysis of the DDPM sampler and establish near-optimal convergence rates under general distributional assumptions. Specifically, we introduce a relaxed smoothness condition parameterized by a constant $L$, which is small for many practical distributions (e.g., Gaussian mixture models). We prove that the DDPM sampler with accurate score estimates achieves a convergence rate of $$\widetilde{O}\left(\frac{d\min\{d,L^2\}}{T^2}\right)~\text{in Kullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the number of iterations, and $\widetilde{O}$ hides polylogarithmic factors in $T$. This result substantially improves upon the best-known $d^2/T^2$ rate when $L < \sqrt{d}$. By establishing a matching lower bound, we show that our convergence analysis is tight for a wide array of target distributions. Moreover, it reveals that DDPM and DDIM share the same dependence on $d$, raising an interesting question of why DDIM often appears empirically faster.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vectorized Computation of Euler Characteristic Functions and Transforms</title>
<link>https://arxiv.org/abs/2511.03909</link>
<guid>https://arxiv.org/abs/2511.03909</guid>
<content:encoded><![CDATA[
arXiv:2511.03909v2 Announce Type: replace-cross 
Abstract: The weighted Euler characteristic transform (WECT) and Euler characteristic function (ECF) have proven to be useful tools in a variety of applications. However, current methods for computing these functions are either not optimized for speed or do not scale to higher-dimensional settings. In this work, we present a vectorized framework for computing such topological descriptors using tensor operations, which is highly optimized for GPU architectures and works in full generality across simplicial and cubical complexes of arbitrary dimension. Experimentally, the framework demonstrates significant speedups over existing methods when computing the WECT and ECF across a variety of two- and three-dimensional datasets. Computation of these transforms is implemented in a publicly available Python package called pyECT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits and Deletions</title>
<link>https://arxiv.org/abs/2511.09465</link>
<guid>https://arxiv.org/abs/2511.09465</guid>
<content:encoded><![CDATA[
arXiv:2511.09465v2 Announce Type: replace-cross 
Abstract: Diffusion and flow matching approaches to generative modeling have shown promise in domains where the state space is continuous, such as image generation or protein folding & design, and discrete, exemplified by diffusion large language models. They offer a natural fit when the number of elements in a state is fixed in advance (e.g. images), but require ad hoc solutions when, for example, the length of a response from a large language model, or the number of amino acids in a protein chain is not known a priori.
  Here we propose Branching Flows, a generative modeling framework that, like diffusion and flow matching approaches, transports a simple distribution to the data distribution. But in Branching Flows, the elements in the state evolve over a forest of binary trees, branching and dying stochastically with rates that are learned by the model. This allows the model to control, during generation, the number of elements in the sequence. We also show that Branching Flows can compose with any flow matching base process on discrete sets, continuous Euclidean spaces, smooth manifolds, and `multimodal' product spaces that mix these components. We demonstrate this in three domains: small molecule generation (multimodal), antibody sequence generation (discrete), and protein backbone generation (multimodal), and show that Branching Flows is a capable distribution learner with a stable learning objective, and that it enables new capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle</title>
<link>https://arxiv.org/abs/2511.17171</link>
<guid>https://arxiv.org/abs/2511.17171</guid>
<content:encoded><![CDATA[
arXiv:2511.17171v2 Announce Type: replace-cross 
Abstract: Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MODEST: Multi-Optics Depth-of-Field Stereo Dataset</title>
<link>https://arxiv.org/abs/2511.20853</link>
<guid>https://arxiv.org/abs/2511.20853</guid>
<content:encoded><![CDATA[
arXiv:2511.20853v2 Announce Type: replace-cross 
Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs</title>
<link>https://arxiv.org/abs/2512.00319</link>
<guid>https://arxiv.org/abs/2512.00319</guid>
<content:encoded><![CDATA[
arXiv:2512.00319v2 Announce Type: replace-cross 
Abstract: The Structure Gap between probabilistic LLM generation and deterministic schema requirements hinders automated workflows. We propose RL-Struct, a lightweight framework using Gradient Regularized Policy Optimization (GRPO) with a hierarchical reward function to align LLMs with structural constraints. This approach eliminates the critic network, reducing peak VRAM by 38% compared to PPO. On complex JSON tasks, RL-Struct achieves 89.7% structural accuracy and 92.1% validity, significantly outperforming SFT and zero-shot baselines. We also report an emergent curriculum--a self-organized learning process where the model prioritizes syntax before semantics. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI</title>
<link>https://arxiv.org/abs/2512.02020</link>
<guid>https://arxiv.org/abs/2512.02020</guid>
<content:encoded><![CDATA[
arXiv:2512.02020v2 Announce Type: replace-cross 
Abstract: Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm</title>
<link>https://arxiv.org/abs/2512.02700</link>
<guid>https://arxiv.org/abs/2512.02700</guid>
<content:encoded><![CDATA[
arXiv:2512.02700v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup. The code is available at https://github.com/Casey-bit/VLMPruner.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title>
<link>https://arxiv.org/abs/2512.06393</link>
<guid>https://arxiv.org/abs/2512.06393</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, logical reasoning, robustness evaluation, rule deletion, logic-preserving rewrites<br /><br />Summary:<br /><br />This paper introduces a controlled evaluation framework designed to probe the reasoning reliability of large language models (LLMs) under structured perturbations to logical rule systems. The framework consists of four stress tests: (1) rule deletion, distinguishing the impact of removing redundant versus essential rules from multi-step inference chains; (2) injection of contradictory evidence to test consistency handling; (3) logic-preserving rewrites using equivalence laws such as contraposition, double negation, implication-to-disjunction, De Morgan's laws, identity, and commutativity; and (4) multi-law equivalence stacking, involving the composition of 2 to 5 transformations. Evaluations were conducted across three representative model families: BERT, Qwen2, and LLaMA-like models. Results show all models achieve perfect accuracy on the base condition and remain unaffected by redundant rule deletion. However, deleting essential rules drastically reduces accuracy to near chance levels, and injecting contradictions causes accuracy to fall to zero. Under single-law logic-preserving rewrites, models largely maintain accuracy with small degradations in some cases. When multiple logical transformations are composed, sensitivities vary by model family—BERT remains stable, TinyLlama experiences minor drops, and Qwen2 suffers substantial performance declines. Overall, the study reveals that while contemporary LLMs are robust to semantic-preserving reformulations, they are fragile to missing or inconsistent evidence and may degrade when facing complex logical transformations. The framework offers a precise diagnostic tool for assessing logical generalization beyond superficial textual variations. <div>
arXiv:2512.06393v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) achieve strong performance on many natural language tasks, yet their generalisation under structured perturbations of logical rule systems remains insufficiently characterised. We present a controlled evaluation framework that probes reasoning reliability through four stress tests: (1) rule deletion, removing redundant versus essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites based on equivalence laws (contraposition, double negation, implication-to-disjunction, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that composes 2--5 transformations. Across three representative model families -- BERT, Qwen2, and LLaMA-like models -- all models attain Acc$=1.0000$ on the base split and show no degradation under redundant rule deletion. In contrast, essential rule deletion yields a pronounced decrease to near-chance performance, and injecting explicit contradictions reduces accuracy to 0.0000. Under logic-preserving rewrites, accuracy is largely preserved for single-law transformations with only small degradations in a few cases, whereas multi-law stacking exposes model-dependent sensitivity: BERT matches the base condition, TinyLlama shows only marginal degradation, and Qwen2 exhibits a substantial drop.
  Overall, the results indicate that contemporary LLMs are generally stable under semantic-preserving reformulations, yet remain brittle to missing or inconsistent evidence and may degrade under composed logical transformations depending on the model family. The proposed framework provides a concise diagnostic tool for isolating these failure modes and for evaluating logical generalisation beyond surface-form variation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</title>
<link>https://arxiv.org/abs/2512.07462</link>
<guid>https://arxiv.org/abs/2512.07462</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, strategic behaviour, repeated social dilemmas, FAIRGAME framework, multi-agent systems<br /><br />Summary:<br /><br />1. This paper examines the strategic behaviour of Large Language Models (LLMs) acting autonomously in interactive, multi-agent environments, emphasizing the importance of understanding their intentions for safety and coordination. <br />2. The authors extend the FAIRGAME framework by introducing two experimental environments: a payoff-scaled Prisoner’s Dilemma to analyze sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game featuring dynamic payoffs and multi-agent histories. <br />3. These environments reveal consistent behavioural patterns across different LLM models and languages, such as cooperation that varies with incentives, differences across languages, and a tendency toward defection near the end of repeated games. <br />4. To interpret these behaviours, they train supervised classifiers on classic repeated-game strategies and apply these to observations from FAIRGAME, identifying systematic, model- and language-dependent behavioral intentions. Notably, linguistic framing sometimes influences behaviour as strongly as architectural model differences. <br />5. The work provides a unified methodological approach for auditing LLMs as strategic agents, uncovering cooperation biases with significant implications for AI governance, collective decision-making, and the design of safe multi-agent AI systems. <div>
arXiv:2512.07462v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering</title>
<link>https://arxiv.org/abs/2512.10962</link>
<guid>https://arxiv.org/abs/2512.10962</guid>
<content:encoded><![CDATA[
<div> Keywords: computer use agents, data synthesis, step-level filtering, WebSTAR dataset, reward model<br /><br />Summary:<br /><br />1. Computer Use Agents (CUAs) interact with real-world digital interfaces, but training them is challenging due to the high cost of GUI interactions and limited availability of quality trajectory data. <br />2. Existing datasets are mostly based on human demonstrations, which restrict scalability. A promising approach is synthesizing data from strong CUAs, but these rollouts are noisy, containing numerous incorrect or suboptimal actions, making straightforward imitation learning ineffective. <br />3. To address this, the authors propose a scalable data synthesis pipeline centered on step-level filtering to evaluate actions individually, retaining only correct steps, enhanced with reasoning augmentation for better planning. <br />4. Using this method, they create WebSTAR, a dataset of 13.3K trajectories and 100K graded, reasoning-enriched steps synthesized from OpenAI's computer-use-preview model. <br />5. Training Qwen-2.5-VL-Instruct models (7B and 32B) on WebSTAR, their 7B model outperforms the previous state-of-the-art open-source CUA model UI-TARS-1.5-7B by over 15% on WebVoyager with only supervised fine-tuning. <br />6. They also develop WebSCORE, a dataset with graded step-level actions, and train StepRM, a 7B multimodal reward model distilled from o4-mini, which maintains grading quality while being more efficient for large-scale deployment. <br />7. Their results underscore step-level filtering as a key scalable training principle and contribute two datasets (WebSTAR, WebSCORE) plus a lightweight reward model (StepRM) as practical tools to advance robust, efficient CUAs. <div>
arXiv:2512.10962v1 Announce Type: new 
Abstract: Computer use agents (CUAs) can operate real-world digital interfaces but remain difficult to train due to the high cost of graphical user interface (GUI) interaction and the scarcity of high-quality trajectory data. Existing datasets rely on human demonstrations, limiting scalability. A natural alternative is to synthesize data from strong CUAs, yet their rollouts are highly noisy, with incorrect or suboptimal actions consisting a large proportion of the steps, making naive imitation ineffective. To tackle this challenge, we introduce a scalable data synthesis pipeline that transforms noisy rollouts into reliable supervision without human annotation. The core idea is step-level filtering, which evaluates actions individually to retain only correct steps, complemented by reasoning augmentation for improved planning. Using this pipeline, we construct WebSTAR, a dataset of 13.3K trajectories and 100K graded, reasoning-rich steps synthesized from OpenAI's computer-use-preview model. We train Qwen-2.5-VL-Instruct models (7B and 32B) on WebSTAR. On WebVoyager, our 7B model surpasses SoTA open-source CUA model UI-TARS-1.5-7B by more than 15% with only supervised finetuning. Building on step-level grading, we further create WebSCORE, a dataset of graded step-level actions, and train StepRM, a 7B multimodal reward model distilled from o4-mini, which matches its grading quality while being far more efficient to deploy at scale. Our results establish step-level filtering as a key principle for scalable CUA training and construct two new datasets (WebSTAR, WebSCORE) and a lightweight reward model (StepRM) as practical tools to advance robust and efficient CUAs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2512.10966</link>
<guid>https://arxiv.org/abs/2512.10966</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, multimodal fusion, Mixture-of-Experts, neuroimaging, interpretability  

<br /><br />Summary:  
1. The paper addresses the challenge of accurate and early diagnosis of Alzheimer's disease (AD) by integrating multiple imaging modalities, specifically amyloid PET and MRI, to better mimic clinical diagnostic practices.  
2. Conventional fusion methods typically concatenate features without adaptively weighting the contributions of different biomarkers across brain regions, which limits the effectiveness of multimodal analysis.  
3. The authors propose MREF-AD, a Multimodal Regional Expert Fusion model based on a Mixture-of-Experts (MoE) framework, where each expert corresponds to a meso-scale brain region within a specific modality.  
4. MREF-AD incorporates two levels of gating networks that learn individualized fusion weights for each subject, allowing adaptive balancing of information from different regions and modalities.  
5. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), the model achieves state-of-the-art diagnostic performance compared to baseline methods, while also providing enhanced interpretability at both modality- and region-specific levels, thus offering insights into the joint contributions of structural and molecular imaging biomarkers.  
6. The approach highlights the potential of MREF-AD as a general and interpretable framework for multimodal fusion in neuroimaging applications, improving both accuracy and understandability of AD diagnosis. <div>
arXiv:2512.10966v1 Announce Type: new 
Abstract: Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoB: Mixture of Bidders</title>
<link>https://arxiv.org/abs/2512.10969</link>
<guid>https://arxiv.org/abs/2512.10969</guid>
<content:encoded><![CDATA[
<div> Mixture of Experts, continual learning, catastrophic forgetting, Vickrey-Clarke-Groves auction, Elastic Weight Consolidation<br /><br />Summary:  
The paper introduces Mixture of Bidders (MoB), a novel framework aimed at overcoming the catastrophic forgetting problem in continual learning within Mixture of Experts (MoE) architectures. Unlike traditional MoE models that use learned gating networks prone to forgetting, MoB replaces gating with Vickrey-Clarke-Groves (VCG) auctions, enabling experts to bid truthfully based on a combined cost metric comprising execution cost (predicted loss) and forgetting cost (Elastic Weight Consolidation penalty). This economic, game-theoretic approach leads to three main benefits: (1) stateless routing that avoids catastrophic forgetting, (2) guaranteed truthful bidding due to dominant-strategy incentive compatibility, and (3) natural emergent specialization of experts without requiring explicit task boundaries. Evaluation on Split-MNIST benchmarks shows MoB achieves an average accuracy of 88.77%, significantly outperforming gated MoE (19.54%) and Monolithic EWC (27.96%), amounting to a 4.5 times performance boost over the strongest baseline. The study also extends MoB by incorporating autonomous self-monitoring experts that independently detect knowledge consolidation boundaries, thus removing the need for predefined task demarcations and further enhancing continual learning robustness. <div>
arXiv:2512.10969v1 Announce Type: new 
Abstract: Mixture of Experts (MoE) architectures have demonstrated remarkable success in scaling neural networks, yet their application to continual learning remains fundamentally limited by a critical vulnerability: the learned gating network itself suffers from catastrophic forgetting. We introduce Mixture of Bidders (MoB), a novel framework that reconceptualizes expert routing as a decentralized economic mechanism. MoB replaces learned gating networks with Vickrey-Clarke-Groves (VCG) auctions, where experts compete for each data batch by bidding their true cost -- a principled combination of execution cost (predicted loss) and forgetting cost (Elastic Weight Consolidation penalty). This game-theoretic approach provides three key advantages: (1) {stateless routing that is immune to catastrophic forgetting, (2) \textbf{truthful bidding} guaranteed by dominant-strategy incentive compatibility, and (3) emergent specialization without explicit task boundaries. On Split-MNIST benchmarks, MoB achieves 88.77% average accuracy compared to 19.54% for Gated MoE and 27.96% for Monolithic EWC, representing a 4.5 times improvement over the strongest baseline. We further extend MoB with autonomous self-monitoring experts that detect their own knowledge consolidation boundaries, eliminating the need for explicit task demarcation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TECM*: A Data-Driven Assessment to Reinforcement Learning Methods and Application to Heparin Treatment Strategy for Surgical Sepsis</title>
<link>https://arxiv.org/abs/2512.10973</link>
<guid>https://arxiv.org/abs/2512.10973</guid>
<content:encoded><![CDATA[
<div> Keywords: sepsis, heparin therapy, reinforcement learning, cxSOFA, Treatment Effect Comparison Matrix (TECM)  

<br /><br />Summary:  
This study addresses sepsis, a life-threatening condition from severe infection causing acute organ dysfunction, focusing on optimizing personalized heparin therapy for surgical sepsis patients. Researchers utilized large-scale clinical data from MIMIC-IV v1.0 and eICU v2.0 databases, specifically abdominal surgery patients receiving unfractionated heparin post-sepsis onset, to develop and evaluate models. A novel reinforcement learning (RL) framework was introduced with three key innovations: converting the discrete SOFA score to a continuous cxSOFA score for more nuanced state and reward modeling; defining treatment strategies as "good" or "bad" based on cxSOFA changes stepwise; and proposing the Treatment Effect Comparison Matrix (TECM), analogous to a confusion matrix, for comprehensive evaluation of treatment strategies. Various RL algorithms, including Q-Learning, DQN, DDQN, BCQ, and CQL, were tested, with the cxSOFA-CQL model showing the best clinical outcomes, reducing patient mortality from 1.83% to 0.74% and shortening the average hospital stay from 11.11 to 9.42 days. The TECM method proved consistent and robust across models, enhancing interpretability and evaluation accuracy. Overall, the proposed RL-based approach demonstrates strong potential for improving clinical decision support and personalized heparin therapy optimization in surgical sepsis management. <div>
arXiv:2512.10973v1 Announce Type: new 
Abstract: Objective: Sepsis is a life-threatening condition caused by severe infection leading to acute organ dysfunction. This study proposes a data-driven metric and a continuous reward function to optimize personalized heparin therapy in surgical sepsis patients. Methods: Data from the MIMIC-IV v1.0 and eICU v2.0 databases were used for model development and evaluation. The training cohort consisted of abdominal surgery patients receiving unfractionated heparin (UFH) after postoperative sepsis onset. We introduce a new RL-based framework: converting the discrete SOFA score to a continuous cxSOFA for more nuanced state and reward functions; Second, defining "good" or "bad" strategies based on cxSOFA by a stepwise manner; Third, proposing a Treatment Effect Comparison Matrix (TECM), analogous to a confusion matrix for classification tasks, to evaluate the treatment strategies. We applied different RL algorithms, Q-Learning, DQN, DDQN, BCQ and CQL to optimize the treatment and comprehensively evaluated the framework. Results: Among the AI-derived strategies, the cxSOFA-CQL model achieved the best performance, reducing mortality from 1.83% to 0.74% with the average hospital stay from 11.11 to 9.42 days. TECM demonstrated consistent outcomes across models, highlighting robustness. Conclusion: The proposed RL framework enables interpretable and robust optimization of heparin therapy in surgical sepsis. Continuous cxSOFA scoring and TECM-based evaluation provide nuanced treatment assessment, showing promise for improving clinical outcomes and decision-support reliability.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems</title>
<link>https://arxiv.org/abs/2512.10975</link>
<guid>https://arxiv.org/abs/2512.10975</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal deep learning, emotion recognition, multi-agent framework, human-agent interaction, modular integration  

<br /><br />Summary:  
1. The paper addresses the challenge of effective human-agent interaction (HAI) by focusing on accurate and adaptive perception of human emotional states using multimodal data.  
2. It highlights limitations of existing multimodal deep learning models, particularly their high computational cost and inflexibility in handling changes or additions in data modalities.  
3. The authors propose a novel multi-agent framework in which each modality encoder (e.g., for facial expressions, speech, text) and the fusion classifier function as independent autonomous agents coordinated by a central supervisor agent.  
4. This architecture supports modular integration, allowing easy addition, replacement, or removal of modalities such as audio features using emotion2vec, improving system flexibility and maintainability.  
5. A proof-of-concept implementation demonstrates support for vision, audio, and text modalities, with the classifier acting as a shared decision-making agent, resulting in reduced computational overhead and enhanced training efficiency.  
6. The framework advances the design of scalable, flexible, and maintainable perception modules suitable for embodied and virtual agents in various HAI scenarios, promoting more adaptable emotion recognition models. <div>
arXiv:2512.10975v1 Announce Type: new 
Abstract: Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax</title>
<link>https://arxiv.org/abs/2512.10991</link>
<guid>https://arxiv.org/abs/2512.10991</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D molecular geometry, MolSculpt, molecular diffusion model, chemical syntax, de novo generation

<br /><br />Summary: Generating precise 3D molecular geometries is vital for drug discovery and material science, but traditional methods using 1D molecular representations like SELFIES often do not fully utilize the embedded chemical knowledge, causing discrepancies between 1D syntax and 3D structure. To address this, MolSculpt is introduced as a novel framework that sculpts 3D molecular geometries directly from chemical syntax by combining a frozen 1D molecular foundation model with a 3D molecular diffusion model. The framework employs learnable queries to extract chemical knowledge from the 1D model and uses a trainable projector to inject this information into the diffusion model’s conditioning space, enabling end-to-end optimization and deep integration of 1D chemical context into 3D generation. Experiments on GEOM-DRUGS and QM9 datasets demonstrate that MolSculpt achieves state-of-the-art performance in both de novo and conditional 3D molecule generation tasks, outperforming existing methods in 3D fidelity and structural stability. The authors also provide open-source code to facilitate further research and application development. <div>
arXiv:2512.10991v1 Announce Type: new 
Abstract: Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that "sculpts" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memoryless Policy Iteration for Episodic POMDPs</title>
<link>https://arxiv.org/abs/2512.11082</link>
<guid>https://arxiv.org/abs/2512.11082</guid>
<content:encoded><![CDATA[
<div> POMDP, policy iteration, memoryless policies, computational efficiency, model-free learning<br /><br />Summary:<br /><br />This article addresses the challenge of solving partially observable Markov decision processes (POMDPs) by focusing on memoryless and finite-memory policies, which simplify computation by operating in the output space instead of the complex belief space. The authors highlight the difficulty in applying traditional policy iteration methods because the output process in POMDPs is non-Markovian, causing interdependencies in policy improvement across stages. To overcome this, they develop a novel family of policy-iteration algorithms that alternate between single-stage output-based policy improvement steps and policy evaluation steps following specific periodic patterns. They demonstrate that among these patterns, some are optimal in maximizing computational efficiency, and notably identify the simplest pattern with the shortest period that still achieves this efficiency. Additionally, the paper extends the approach with a model-free variant that estimates values directly from data, allowing learning of memoryless policies without a model. Empirical results across various POMDP benchmarks show that their methods offer substantial computational speedups compared to policy-gradient methods and other recent specialized algorithms, both in model-based and model-free contexts. This work provides a practical and theoretically grounded advancement for efficiently solving POMDPs with output-based policies. <div>
arXiv:2512.11082v1 Announce Type: new 
Abstract: Memoryless and finite-memory policies offer a practical alternative for solving partially observable Markov decision processes (POMDPs), as they operate directly in the output space rather than in the high-dimensional belief space. However, extending classical methods such as policy iteration to this setting remains difficult; the output process is non-Markovian, making policy-improvement steps interdependent across stages. We introduce a new family of monotonically improving policy-iteration algorithms that alternate between single-stage output-based policy improvements and policy evaluations according to a prescribed periodic pattern. We show that this family admits optimal patterns that maximize a natural computational-efficiency index, and we identify the simplest pattern with minimal period. Building on this structure, we further develop a model-free variant that estimates values from data and learns memoryless policies directly. Across several POMDPs examples, our method achieves significant computational speedups over policy-gradient baselines and recent specialized algorithms in both model-based and model-free settings.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification</title>
<link>https://arxiv.org/abs/2512.11087</link>
<guid>https://arxiv.org/abs/2512.11087</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network verification, branch-and-bound, linear constraints, bound tightening, GPU acceleration  

<br /><br />Summary:  
This paper introduces the linear constraint-driven clipping framework, a new class of scalable methods aimed at improving neural network (NN) verification efficacy. 1) It presents two novel algorithms that leverage linear constraints to reduce input space portions either already verified or irrelevant during branch-and-bound (BaB) subproblems, and also enhance intermediate bounds across the network layers. 2) The framework uniquely incorporates linear constraints arising from bound propagation and other sources to tighten verification bounds. 3) A specialized GPU-based procedure is developed to efficiently manage these constraints, enabling scalability to large networks without relying on expensive external solvers. 4) The proposed verification method, Clip-and-Verify, demonstrates consistent bound tightening across multiple benchmark datasets and significantly decreases the number of BaB subproblems, in some cases by up to 96%. 5) The approach integrates seamlessly with BaB-based verifiers like α,β-CROWN, utilizing split constraints in activation-space BaB or output constraints defining unverified input regions. 6) Experiments confirm state-of-the-art verified accuracy and considerable efficiency improvements, contributing to its success as part of the winning tool in the VNN-COMP 2025 verification competition. The implementation is publicly available, further enabling reproducibility and adoption. <div>
arXiv:2512.11087v1 Announce Type: new 
Abstract: State-of-the-art neural network (NN) verifiers demonstrate that applying the branch-and-bound (BaB) procedure with fast bounding techniques plays a key role in tackling many challenging verification properties. In this work, we introduce the linear constraint-driven clipping framework, a class of scalable and efficient methods designed to enhance the efficacy of NN verifiers. Under this framework, we develop two novel algorithms that efficiently utilize linear constraints to 1) reduce portions of the input space that are either verified or irrelevant to a subproblem in the context of branch-and-bound, and 2) directly improve intermediate bounds throughout the network. The process novelly leverages linear constraints that often arise from bound propagation methods and is general enough to also incorporate constraints from other sources. It efficiently handles linear constraints using a specialized GPU procedure that can scale to large neural networks without the use of expensive external solvers. Our verification procedure, Clip-and-Verify, consistently tightens bounds across multiple benchmarks and can significantly reduce the number of subproblems handled during BaB. We show that our clipping algorithms can be integrated with BaB-based verifiers such as $\alpha,\beta$-CROWN, utilizing either the split constraints in activation-space BaB or the output constraints that denote the unverified input space. We demonstrate the effectiveness of our procedure on a broad range of benchmarks where, in some instances, we witness a 96% reduction in the number of subproblems during branch-and-bound, and also achieve state-of-the-art verified accuracy across multiple benchmarks. Clip-and-Verify is part of the $\alpha,\beta$-CROWN verifier (http://abcrown.org), the VNN-COMP 2025 winner. Code available at https://github.com/Verified-Intelligence/Clip_and_Verify.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating ECG Diagnosis with Ambiguous Labels using Partial Label Learning</title>
<link>https://arxiv.org/abs/2512.11095</link>
<guid>https://arxiv.org/abs/2512.11095</guid>
<content:encoded><![CDATA[
<div> Keywords: Label Ambiguity, Partial Label Learning, ECG Diagnosis, Medical Time-Series, Robustness  

<br /><br />Summary:  
This paper addresses the inherent challenge of label ambiguity in real-world electrocardiogram (ECG) diagnosis, highlighting that traditional ECG models assume clean, unambiguous annotations which is unrealistic in clinical practice. It explores Partial Label Learning (PLL) frameworks, designed to handle ambiguous labels, which have not been thoroughly studied in the context of medical time-series data like ECGs. The authors adapt nine PLL algorithms to the multi-label ECG diagnosis task and introduce various clinically motivated ambiguity generation strategies to simulate real-world conditions, including both unstructured ambiguity (random noise) and structured ambiguity based on cardiologist-derived similarities, treatment relationships, and diagnostic taxonomies. Experiments are conducted on two prominent ECG datasets, PTB-XL and Chapman, revealing that PLL methods exhibit significant variability in their robustness to different types and levels of label ambiguity. The study identifies major shortcomings of current PLL approaches when applied to clinical ECG diagnosis scenarios and emphasizes the necessity for developing more robust, clinically relevant ambiguity-aware learning frameworks. Future work should focus on aligning PLL methodology more closely with real diagnostic challenges to improve model reliability and evaluation in practical healthcare settings. <div>
arXiv:2512.11095v1 Announce Type: new 
Abstract: Label ambiguity is an inherent problem in real-world electrocardiogram (ECG) diagnosis, arising from overlapping conditions and diagnostic disagreement. However, current ECG models are trained under the assumption of clean and non-ambiguous annotations, which limits both the development and the meaningful evaluation of models under real-world conditions. Although Partial Label Learning (PLL) frameworks are designed to learn from ambiguous labels, their effectiveness in medical time-series domains, ECG in particular, remains largely unexplored. In this work, we present the first systematic study of PLL methods for ECG diagnosis. We adapt nine PLL algorithms to multi-label ECG diagnosis and evaluate them using a diverse set of clinically motivated ambiguity generation strategies, capturing both unstructured (e.g., random) and structured ambiguities (e.g., cardiologist-derived similarities, treatment relationships, and diagnostic taxonomies). Our experiments on the PTB-XL and Chapman datasets demonstrate that PLL methods vary substantially in their robustness to different types and degrees of ambiguity. Through extensive analysis, we identify key limitations of current PLL approaches in clinical settings and outline future directions for developing robust and clinically aligned ambiguity-aware learning frameworks for ECG diagnosis.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limits and Gains of Test-Time Scaling in Vision-Language Reasoning</title>
<link>https://arxiv.org/abs/2512.11109</link>
<guid>https://arxiv.org/abs/2512.11109</guid>
<content:encoded><![CDATA[
<div> Test-time scaling, Vision-Language Models, reasoning methods, iterative self-refinement, external verification<br /><br />Summary:<br /><br />1. Test-time scaling (TTS) is an approach to enhance the reasoning abilities of Large Language Models (LLMs) by allocating extra computational steps during inference. However, its application to multimodal models, particularly Vision-Language Models (VLMs), remains underexplored.<br /><br />2. The study conducts a systematic empirical evaluation of reasoning methods used at inference time across both open-source and closed-source VLMs, tested on various benchmark datasets.<br /><br />3. Findings indicate that closed-source VLMs reliably benefit from structured reasoning techniques and iterative Self-Refinement, improving performance on reasoning tasks.<br /><br />4. In contrast, open-source VLMs demonstrate inconsistent responses: external verification mechanisms tend to improve results, but iterative refinement frequently harms performance.<br /><br />5. The effectiveness of TTS is shown to be highly dependent on the dataset type, providing clear improvements on multi-step reasoning challenges but limited advantages on perception-focused benchmarks.<br /><br />6. Overall, TTS is not a one-size-fits-all solution; its success hinges on tailoring methods to specific model capabilities and task characteristics. This motivates future research into adaptive TTS methods and the development of multimodal reward models to guide reasoning. <div>
arXiv:2512.11109v1 Announce Type: new 
Abstract: Test-time scaling (TTS) has emerged as a powerful paradigm for improving the reasoning ability of Large Language Models (LLMs) by allocating additional computation at inference, yet its application to multimodal systems such as Vision-Language Models (VLMs) remains underexplored. In this work, we present a systematic empirical study of inference time reasoning methods applied across both open-source and closed-source VLMs on different benchmarks. Our results reveal that while closed-source models consistently benefit from structured reasoning and iterative Self-Refinement, open-source VLMs show inconsistent behavior: external verification provides the most reliable gains, whereas iterative refinement often degrades performance. We further find that the effectiveness of TTS is dataset-dependent, yielding clear improvements on multi-step reasoning tasks but offering only limited gains on perception-focused benchmarks. These findings demonstrate that TTS is not a universal solution and must be tailored to both model capabilities and task characteristics, motivating future work on adaptive TTS strategies and multimodal reward models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Multi-Objective Optimization</title>
<link>https://arxiv.org/abs/2512.11114</link>
<guid>https://arxiv.org/abs/2512.11114</guid>
<content:encoded><![CDATA[
<div> Multi-objective optimization, Bayesian optimization, transformer, amortized policy, hypervolume improvement

<br /><br />Summary:  
This paper introduces TAMO, a fully amortized and universal policy designed for multi-objective black-box optimization. TAMO leverages a transformer architecture capable of handling problems with varying input and objective dimensions. By doing so, it supports pretraining on a diverse range of optimization tasks, allowing it to transfer seamlessly to new problems without the need for retraining. Unlike traditional multi-objective Bayesian optimization methods, which rely on tailored surrogate models and acquisition functions that often do not generalize well, TAMO proposes new designs through a single forward pass at test time, significantly reducing computational overhead. The model is pretrained using reinforcement learning, aiming to maximize cumulative hypervolume improvement across entire optimization trajectories while conditioning on the historical query data to approximate the Pareto frontier. Experimental results on both synthetic benchmarks and real-world tasks demonstrate that TAMO consistently matches or exceeds the quality of solutions found by existing methods but does so much faster, with speed improvements ranging from 50 to 1000 times. This work highlights the potential of transformer-based methods to perform multi-objective optimization entirely in-context, removing the usual need for per-task surrogate fitting and acquisition function engineering. Consequently, TAMO paves the way toward foundation-style, plug-and-play optimizers for scientific discovery and other time-sensitive workflows. <div>
arXiv:2512.11114v1 Announce Type: new 
Abstract: Balancing competing objectives is omnipresent across disciplines, from drug design to autonomous systems. Multi-objective Bayesian optimization is a promising solution for such expensive, black-box problems: it fits probabilistic surrogates and selects new designs via an acquisition function that balances exploration and exploitation. In practice, it requires tailored choices of surrogate and acquisition that rarely transfer to the next problem, is myopic when multi-step planning is often required, and adds refitting overhead, particularly in parallel or time-sensitive loops. We present TAMO, a fully amortized, universal policy for multi-objective black-box optimization. TAMO uses a transformer architecture that operates across varying input and objective dimensions, enabling pretraining on diverse corpora and transfer to new problems without retraining: at test time, the pretrained model proposes the next design with a single forward pass. We pretrain the policy with reinforcement learning to maximize cumulative hypervolume improvement over full trajectories, conditioning on the entire query history to approximate the Pareto frontier. Across synthetic benchmarks and real tasks, TAMO produces fast proposals, reducing proposal time by 50-1000x versus alternatives while matching or improving Pareto quality under tight evaluation budgets. These results show that transformers can perform multi-objective optimization entirely in-context, eliminating per-task surrogate fitting and acquisition engineering, and open a path to foundation-style, plug-and-play optimizers for scientific discovery workflows.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constraint-Satisfaction Guarantee</title>
<link>https://arxiv.org/abs/2512.11127</link>
<guid>https://arxiv.org/abs/2512.11127</guid>
<content:encoded><![CDATA[
<div> Keywords: DC Optimal Power Flow, Graph Neural Networks, Continuous Flow Matching, power system constraints, Kirchhoff's laws<br /><br />Summary:  
1. The paper addresses the DC Optimal Power Flow (DC-OPF) problem, essential for real-time power system operations, emphasizing the need for rapid, reliable solutions in large-scale grids.  
2. Traditional optimization solvers provide accurate results but suffer from high computational costs, making frequent recalculations impractical for extensive and dynamic systems.  
3. The authors introduce a novel two-stage learning framework that integrates physics-informed Graph Neural Networks (GNNs) with Continuous Flow Matching (CFM).  
4. Physical principles, such as economic dispatch optimality conditions, Kirchhoff's laws, and Karush-Kuhn-Tucker complementarity conditions, are embedded within the training objectives to ensure feasible and physically consistent solutions.  
5. The first stage uses the GNN to predict feasible initial solutions by minimizing physics-based loss functions encoding system constraints.  
6. The second stage applies CFM, a simulation-free continuous normalizing flow method, to refine these initial solutions through learned vector field regression, improving optimality.  
7. Evaluation on the IEEE 30-bus test system under varying load conditions (70% to 130% nominal load) demonstrates cost gaps below 0.1% at nominal load and under 3% even in extreme scenarios, with 100% solution feasibility.  
8. This approach effectively bridges the gap between fast but approximate machine learning methods and slow yet optimal traditional solvers, offering a promising tool for modern power grids, especially those with high renewable penetration requiring frequent dispatch updates. <div>
arXiv:2512.11127v1 Announce Type: new 
Abstract: The DC Optimal Power Flow (DC-OPF) problem is fundamental to power system operations, requiring rapid solutions for real-time grid management. While traditional optimization solvers provide optimal solutions, their computational cost becomes prohibitive for large-scale systems requiring frequent recalculations. Machine learning approaches offer promise for acceleration but often struggle with constraint satisfaction and cost optimality. We present a novel two-stage learning framework that combines physics-informed Graph Neural Networks (GNNs) with Continuous Flow Matching (CFM) for solving DC-OPF problems. Our approach embeds fundamental physical principles--including economic dispatch optimality conditions, Kirchhoff's laws, and Karush-Kuhn-Tucker (KKT) complementarity conditions--directly into the training objectives. The first stage trains a GNN to produce feasible initial solutions by learning from physics-informed losses that encode power system constraints. The second stage employs CFM, a simulation-free continuous normalizing flow technique, to refine these solutions toward optimality through learned vector field regression. Evaluated on the IEEE 30-bus system across five load scenarios ranging from 70\% to 130\% nominal load, our method achieves near-optimal solutions with cost gaps below 0.1\% for nominal loads and below 3\% for extreme conditions, while maintaining 100\% feasibility. Our framework bridges the gap between fast but approximate neural network predictions and optimal but slow numerical solvers, offering a practical solution for modern power systems with high renewable penetration requiring frequent dispatch updates.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Regularized Online Optimization with Switching Costs</title>
<link>https://arxiv.org/abs/2512.11131</link>
<guid>https://arxiv.org/abs/2512.11131</guid>
<content:encoded><![CDATA[
<div> fairness, online convex optimization, switching costs, competitive ratio, resource provisioning<br /><br />Summary:  
This paper addresses the simultaneous consideration of fairness and action smoothness in online optimization, a setting not previously explored together. It studies fairness-regularized smoothed online convex optimization problems with switching costs. A key challenge identified is that, due to the long-term fairness regularizer which depends on the entire sequence of actions, no online algorithm can achieve sublinear regret or a finite competitive ratio relative to the offline optimum as the time horizon \(T\) grows, even when switching costs are absent. To overcome this, the authors introduce FairOBD (Fairness-regularized Online Balanced Descent), an algorithm that balances hitting cost, switching cost, and fairness cost. FairOBD tackles the complexity of the fairness cost by decomposing it into a sequence of per-round costs using an auxiliary variable, which guides fair action updates. Theoretical analysis shows FairOBD attains a worst-case asymptotic competitive ratio compared to an optimal offline algorithm with parameterized constraints in the limit as \(T \to \infty\). Empirical evaluation on dynamic computing resource provisioning for socially responsible AI inference demonstrates FairOBD’s effectiveness in reducing total fairness-regularized cost and achieving fairer outcomes than baseline methods. This work contributes novel theoretical insights and practical algorithms for fairness-aware online optimization with switching costs. <div>
arXiv:2512.11131v1 Announce Type: new 
Abstract: Fairness and action smoothness are two crucial considerations in many online optimization problems, but they have yet to be addressed simultaneously. In this paper, we study a new and challenging setting of fairness-regularized smoothed online convex optimization with switching costs. First, to highlight the fundamental challenges introduced by the long-term fairness regularizer evaluated based on the entire sequence of actions, we prove that even without switching costs, no online algorithms can possibly achieve a sublinear regret or finite competitive ratio compared to the offline optimal algorithm as the problem episode length $T$ increases. Then, we propose FairOBD (Fairness-regularized Online Balanced Descent), which reconciles the tension between minimizing the hitting cost, switching cost, and fairness cost. Concretely, FairOBD decomposes the long-term fairness cost into a sequence of online costs by introducing an auxiliary variable and then leverages the auxiliary variable to regularize the online actions for fair outcomes. Based on a new approach to account for switching costs, we prove that FairOBD offers a worst-case asymptotic competitive ratio against a novel benchmark -- the optimal offline algorithm with parameterized constraints -- by considering $T\to\infty$. Finally, we run trace-driven experiments of dynamic computing resource provisioning for socially responsible AI inference to empirically evaluate FairOBD, showing that FairOBD can effectively reduce the total fairness-regularized cost and better promote fair outcomes compared to existing baseline solutions.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Vekua Layer: Exact Physical Priors for Implicit Neural Representations via Generalized Analytic Functions</title>
<link>https://arxiv.org/abs/2512.11138</link>
<guid>https://arxiv.org/abs/2512.11138</guid>
<content:encoded><![CDATA[
<div> Keywords: Implicit Neural Representations, Vekua Layer, Generalized Analytic Functions, Partial Differential Equations, spectral methods  

<br /><br />Summary:  
This paper introduces the Vekua Layer (VL), a novel differentiable spectral method based on the classical theory of Generalized Analytic Functions, designed to improve Implicit Neural Representations (INRs) for parameterizing physical fields. Unlike traditional INRs which face spectral bias and require non-convex optimization, the VL restricts the hypothesis space to the kernel of the governing differential operator by employing Harmonic and Fourier-Bessel basis functions. This restriction transforms the optimization process into a strictly convex least-squares problem solved through linear projection, increasing computational efficiency and avoiding iterative gradient descent. The authors evaluate VL on homogeneous elliptic Partial Differential Equations (PDEs) and show that it achieves extremely high accuracy (machine precision with MSE ≈ 10⁻³³) in exact reconstruction tasks. Additionally, VL demonstrates superior robustness to sensor noise, maintaining a low MSE (~0.03), functioning effectively as a physics-informed spectral filter. A distinctive feature of the VL is its ability to perform "holographic" extrapolation of global fields from partial boundary data via analytic continuation, a capability not achievable by conventional coordinate-based neural approximations. These properties suggest VL as a promising approach for physics-informed neural modeling with enhanced precision, stability, and extrapolative power. <div>
arXiv:2512.11138v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs) have emerged as a powerful paradigm for parameterizing physical fields, yet they often suffer from spectral bias and the computational expense of non-convex optimization. We introduce the Vekua Layer (VL), a differentiable spectral method grounded in the classical theory of Generalized Analytic Functions. By restricting the hypothesis space to the kernel of the governing differential operator -- specifically utilizing Harmonic and Fourier-Bessel bases -- the VL transforms the learning task from iterative gradient descent to a strictly convex least-squares problem solved via linear projection. We evaluate the VL against Sinusoidal Representation Networks (SIRENs) on homogeneous elliptic Partial Differential Equations (PDEs). Our results demonstrate that the VL achieves machine precision ($\text{MSE} \approx 10^{-33}$) on exact reconstruction tasks and exhibits superior stability in the presence of incoherent sensor noise ($\text{MSE} \approx 0.03$), effectively acting as a physics-informed spectral filter. Furthermore, we show that the VL enables "holographic" extrapolation of global fields from partial boundary data via analytic continuation, a capability absent in standard coordinate-based approximations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles</title>
<link>https://arxiv.org/abs/2512.11145</link>
<guid>https://arxiv.org/abs/2512.11145</guid>
<content:encoded><![CDATA[
<div> autoencoder, clustering loss, contrastive loss, ensemble datasets, dimensionality reduction<br /><br />Summary:<br /><br />1. The paper addresses challenges in analyzing and visualizing high-dimensional scientific ensemble datasets.<br />2. It proposes an enhanced autoencoder framework that integrates a clustering loss based on the soft silhouette score and a contrastive loss to improve feature extraction, visualization, and interpretability.<br />3. EfficientNetV2 is utilized to generate pseudo-labels for the unlabeled segments of the datasets, facilitating better supervised signals.<br />4. The method jointly optimizes reconstruction, clustering, and contrastive losses to encourage grouping of similar data points and separation of distinct clusters within the latent space.<br />5. UMAP is applied to the learned latent space for 2D projection, with quality assessed by the silhouette score.<br />6. Various types of autoencoders are compared in terms of their effectiveness at extracting meaningful features.<br />7. Experiments on two scientific ensemble datasets — soil channel structures modeled by Markov chain Monte Carlo and droplet-on-film impact dynamics — demonstrate that incorporating clustering and/or contrastive loss marginally outperforms baseline autoencoder approaches.<br />8. The study highlights slight but consistent improvements in visualization and interpretability when adding these losses to the training objective. <div>
arXiv:2512.11145v1 Announce Type: new 
Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Rich Multi-Modal Data for Spatial-Temporal Homophily-Embedded Graph Learning Across Domains and Localities</title>
<link>https://arxiv.org/abs/2512.11178</link>
<guid>https://arxiv.org/abs/2512.11178</guid>
<content:encoded><![CDATA[
<div> Keywords: heterogeneous data fusion, urban analytics, graph learning, multi-domain datasets, smart cities  

<br /><br />Summary:  
This research addresses the challenge of integrating diverse and heterogeneous city-level datasets collected by various local agencies with differing objectives and standards. It proposes a novel heterogeneous data pipeline designed to perform cross-domain data fusion on datasets that vary over time and space, including spatial-varying time-series data. The framework incorporates a data-learning module that embeds homophily from spatially varying data into graph learning models, enabling the integration of locality-specific information across over 50 different data sources. The approach targets complex urban problems spanning multiple domains such as transportation, public safety, and environmental impact. The framework is tested with five real-world case studies, utilizing publicly accessible datasets, including ride-share, traffic crash, and crime reports, collected from multiple cities. Results demonstrate strong predictive capabilities of the proposed method while maintaining flexibility and requiring minimal reconfiguration when applied to new cities or domains. Overall, this research contributes to advancing scalable, data-driven urban systems and smart city analytics by overcoming data heterogeneity challenges and fostering generalized models suitable for cross-domain and multi-locality use cases. <div>
arXiv:2512.11178v1 Announce Type: new 
Abstract: Modern cities are increasingly reliant on data-driven insights to support decision making in areas such as transportation, public safety and environmental impact. However, city-level data often exists in heterogeneous formats, collected independently by local agencies with diverse objectives and standards. Despite their numerous, wide-ranging, and uniformly consumable nature, national-level datasets exhibit significant heterogeneity and multi-modality. This research proposes a heterogeneous data pipeline that performs cross-domain data fusion over time-varying, spatial-varying and spatial-varying time-series datasets. We aim to address complex urban problems across multiple domains and localities by harnessing the rich information over 50 data sources. Specifically, our data-learning module integrates homophily from spatial-varying dataset into graph-learning, embedding information of various localities into models. We demonstrate the generalizability and flexibility of the framework through five real-world observations using a variety of publicly accessible datasets (e.g., ride-share, traffic crash, and crime reports) collected from multiple cities. The results show that our proposed framework demonstrates strong predictive performance while requiring minimal reconfiguration when transferred to new localities or domains. This research advances the goal of building data-informed urban systems in a scalable way, addressing one of the most pressing challenges in smart city analytics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.11179</link>
<guid>https://arxiv.org/abs/2512.11179</guid>
<content:encoded><![CDATA[
<div> Graph-based MARL, Bandwidth constraints, Variational encoding, Coordination graphs, Message compression<br /><br />Summary:<br /><br />This paper addresses the challenge of communication under hard bandwidth constraints in graph-based multi-agent reinforcement learning (MARL), where agents communicate through sparse coordination graphs. The authors identify that naive dimensionality reduction methods degrade coordination performance by not providing control over the compression process. To overcome this, they propose Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that models messages as samples from learned Gaussian posteriors, regularized via KL divergence to an uninformative prior. This variational approach offers principled and tunable compression strength controlled by interpretable hyperparameters, directly influencing the representations agents use to make decisions. The framework enables selective and efficient encoding of inter-agent messages while respecting strict bandwidth limits. Empirical results on benchmark tasks such as SMACv1, SMACv2, and MPE show that BVME achieves comparable or better coordination performance using 67–83% fewer message dimensions compared to baselines. Notably, the advantages of BVME are most significant in sparse graphs where message quality critically affects the coordination outcome. Ablation studies reveal a U-shaped sensitivity curve regarding bandwidth compression levels, with BVME excelling at both low and high bandwidth extremes while incurring minimal computational overhead. <div>
arXiv:2512.11179v1 Announce Type: new 
Abstract: Graph-based multi-agent reinforcement learning (MARL) enables coordinated behavior under partial observability by modeling agents as nodes and communication links as edges. While recent methods excel at learning sparse coordination graphs-determining who communicates with whom-they do not address what information should be transmitted under hard bandwidth constraints. We study this bandwidth-limited regime and show that naive dimensionality reduction consistently degrades coordination performance. Hard bandwidth constraints force selective encoding, but deterministic projections lack mechanisms to control how compression occurs. We introduce Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that treats messages as samples from learned Gaussian posteriors regularized via KL divergence to an uninformative prior. BVME's variational framework provides principled, tunable control over compression strength through interpretable hyperparameters, directly constraining the representations used for decision-making. Across SMACv1, SMACv2, and MPE benchmarks, BVME achieves comparable or superior performance while using 67--83% fewer message dimensions, with gains most pronounced on sparse graphs where message quality critically impacts coordination. Ablations reveal U-shaped sensitivity to bandwidth, with BVME excelling at extreme ratios while adding minimal overhead.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progress over Points: Reframing LM Benchmarks Around Scientific Objectives</title>
<link>https://arxiv.org/abs/2512.11183</link>
<guid>https://arxiv.org/abs/2512.11183</guid>
<content:encoded><![CDATA[
<div> Keywords: progress-oriented benchmarks, NanoGPT speedrun, LLM training efficiency, scientific delta, open-ended evaluation<br /><br />Summary:  
The paper critiques current benchmarks for large language models (LLMs), which primarily focus on static, already-solved problems, limiting the scope of measurable advancements. It proposes a shift toward progress-oriented benchmarks, where the benchmark’s objective aligns with scientific progress itself, ensuring that improving on the benchmark directly advances the field. As a practical example, the authors introduce an environment modeled on the NanoGPT speedrun, featuring standardized dataset slices, a reference model and training setup, along with detailed telemetry and checks to prevent gaming. The evaluation emphasizes the “scientific delta,” measured by best-attained loss and efficiency frontier, rather than just leaderboard positioning. Using this new environment, the authors achieve a new state-of-the-art training time, beating the previous record by 3 seconds, and observe the spontaneous emergence of novel algorithmic concepts. The benchmark design still allows for model and agent comparisons, but primarily serves as a catalyst for reusable improvements in the language modeling stack. Ultimately, the work aims to encourage a community-wide transition from static problem leaderboards to open-ended, scientifically meaningful benchmarks, reframing benchmarking as a direct vehicle for advancing research and innovation in language modeling. <div>
arXiv:2512.11183v1 Announce Type: new 
Abstract: Current benchmarks that test LLMs on static, already-solved problems (e.g., math word problems) effectively demonstrated basic capability acquisition. The natural progression has been toward larger, more comprehensive and challenging collections of static problems, an approach that inadvertently constrains the kinds of advances we can measure and incentivize. To address this limitation, we argue for progress-oriented benchmarks, problem environments whose objectives are themselves the core targets of scientific progress, so that achieving state of the art on the benchmark advances the field. As a introductory step, we instantiate an environment based on the NanoGPT speedrun. The environment standardizes a dataset slice, a reference model and training harness, and rich telemetry, with run-time verification and anti-gaming checks. Evaluation centers on the scientific delta achieved: best-attained loss and the efficiency frontier. Using this environment, we achieve a new state-of-the-art training time, improving upon the previous record by 3 seconds, and qualitatively observe the emergence of novel algorithmic ideas. Moreover, comparisons between models and agents remain possible, but they are a means, not the end; the benchmark's purpose is to catalyze reusable improvements to the language modeling stack. With this release, the overarching goal is to seed a community shift from static problem leaderboards to test-time research on open-ended yet measurable scientific problems. In this new paradigm, progress on the benchmark is progress on the science, thus reframing "benchmarking" as a vehicle for scientific advancement.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the failure of ReLU activation for physics-informed machine learning</title>
<link>https://arxiv.org/abs/2512.11184</link>
<guid>https://arxiv.org/abs/2512.11184</guid>
<content:encoded><![CDATA[
<div> Physics-informed machine learning, activation functions, ReLU limitations, automatic differentiation, PyTorch failure  

<br /><br />Summary:  
Physics-informed machine learning integrates governing differential equations into neural network training to model solution fields accurately. The choice of activation function significantly influences the quality and performance of these solutions. Previous benchmarks have demonstrated that ReLU activation underperforms compared to alternatives such as sigmoid, hyperbolic tangent, and swish functions. This work investigates the reasons behind ReLU’s poor performance in physics-informed contexts. It reveals that although ReLU’s piecewise linear nature is known to limit its use in second-order differential equations, it surprisingly also fails in variational problems involving only first derivatives. The root cause is identified as complications arising from second derivatives of the activation function during training, not directly in the loss function formulation. Specifically, when using PyTorch’s automatic differentiation, it inadequately handles derivatives of discontinuous functions like ReLU, resulting in inaccurately computed gradients of the physics-informed loss. This mis-specification of gradients explains why ReLU struggles to provide high-quality solutions in physics-informed machine learning tasks, highlighting a critical interaction between activation function properties and differentiation tooling in modern ML frameworks. <div>
arXiv:2512.11184v1 Announce Type: new 
Abstract: Physics-informed machine learning uses governing ordinary and/or partial differential equations to train neural networks to represent the solution field. Like any machine learning problem, the choice of activation function influences the characteristics and performance of the solution obtained from physics-informed training. Several studies have compared common activation functions on benchmark differential equations, and have unanimously found that the rectified linear unit (ReLU) is outperformed by competitors such as the sigmoid, hyperbolic tangent, and swish activation functions. In this work, we diagnose the poor performance of ReLU on physics-informed machine learning problems. While it is well-known that the piecewise linear form of ReLU prevents it from being used on second-order differential equations, we show that ReLU fails even on variational problems involving only first derivatives. We identify the cause of this failure as second derivatives of the activation, which are taken not in the formulation of the loss, but in the process of training. Namely, we show that automatic differentiation in PyTorch fails to characterize derivatives of discontinuous fields, which causes the gradient of the physics-informed loss to be mis-specified, thus explaining the poor performance of ReLU.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.11194</link>
<guid>https://arxiv.org/abs/2512.11194</guid>
<content:encoded><![CDATA[
<div> Memorization, diffusion models, concept-level unlearning, gradient projection, privacy-preserving  

<br /><br />Summary:  
This paper addresses the problem of memorization in large-scale text-to-image diffusion models, which can lead to security and intellectual property risks by enabling unauthorized extraction and reproduction of sensitive or proprietary features. Conventional dememorization techniques like regularization or data filtering are insufficient because they cannot selectively prevent the learning of forbidden concept-level features without discarding valuable training data. To overcome this, the authors propose a Gradient Projection Framework that enforces exclusion of prohibited concept-level features during training. Their method works during backpropagation by identifying gradients aligned with sensitive feature embeddings and projecting gradient updates onto the orthogonal complement of these embeddings, effectively preventing the model from internalizing unwanted attributes. This approach integrates easily into existing diffusion model training workflows and can be combined with other defenses. The authors evaluate their method against adversaries trying to extract sensitive features and demonstrate a significant reduction in memorization while maintaining high generation quality and semantic fidelity. By framing memorization control as selective learning rather than outright forgetting, this technique offers a new privacy-preserving and intellectual property-safe paradigm for generative AI development. <div>
arXiv:2512.11194v1 Announce Type: new 
Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast EXP3 Algorithms</title>
<link>https://arxiv.org/abs/2512.11201</link>
<guid>https://arxiv.org/abs/2512.11201</guid>
<content:encoded><![CDATA[
<div> EXP3, constant time, regret bounds, time complexity, practical algorithms<br /><br />Summary:<br /><br />1. The paper focuses on the EXP3 algorithm, which is widely used for multi-armed bandit problems with adversarial settings. 2. A key contribution is demonstrating that EXP3 can be implemented to run in constant time per round, improving its practical efficiency. 3. The authors propose several more practical algorithms inspired by this insight, designed to better balance computational resources and performance. 4. They analyze the trade-offs between the regret bounds of these algorithms and their time complexities, providing a thorough comparison of efficiency versus theoretical guarantees. 5. Overall, the work advances both the theoretical understanding and practical deployment of EXP3-type algorithms, making them more accessible for scenarios where computational speed is critical. <div>
arXiv:2512.11201v1 Announce Type: new 
Abstract: We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Variable Causal Discovery under Selection Bias</title>
<link>https://arxiv.org/abs/2512.11219</link>
<guid>https://arxiv.org/abs/2512.11219</guid>
<content:encoded><![CDATA[
<div> Latent variable causal discovery, selection bias, rank constraints, linear Gaussian models, one-factor model

<br /><br />Summary:  
This article addresses the challenge of selection bias in latent variable causal discovery, an area that has remained largely unexplored due to the scarcity of appropriate statistical methods. It expands on existing causal discovery techniques by focusing on rank constraints, which generalize conditional independence constraints by using the ranks of covariance submatrices within linear Gaussian models. The authors reveal that despite the complications selection bias introduces into joint distributions, the ranks in biased covariance matrices still retain valuable information about both the underlying causal structures and the selection mechanisms involved. The paper provides a graph-theoretic framework for characterizing these rank constraints, thereby advancing the theoretical understanding of causal discovery under selection bias. A significant contribution is the demonstration that the classical one-factor latent variable model can be reliably identified even when selection bias is present. The effectiveness of this approach is validated through extensive simulations and real-world data experiments, showcasing the practical utility of rank constraints in causal inference tasks affected by selection bias. This work thus offers both a novel conceptual and applied toolset for improving causal discovery methodologies in complex, biased observational settings. <div>
arXiv:2512.11219v1 Announce Type: new 
Abstract: Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2512.11221</link>
<guid>https://arxiv.org/abs/2512.11221</guid>
<content:encoded><![CDATA[
<div> Adaptive Soft Rolling KV Freeze, Entropy-Guided Recovery, large language model, KV cache reduction, long-context inference<br /><br />Summary:<br /><br />This paper introduces Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a novel framework designed to improve inference efficiency for large language models without requiring any additional training. The core innovation is a reversible soft-freeze mechanism that temporarily suspends updates to key-value (KV) pairs for tokens deemed low-importance within a sliding attention window. Unlike traditional eviction methods that permanently discard context tokens, ASR-KF-EGR stores all tokens off-GPU and restores them as needed, preserving context integrity. The method features a sublinear freeze scheduling strategy, where the freeze duration increases sublinearly upon repeated detections of low-importance tokens. This approach helps avoid excessive compression that could degrade performance. In preliminary experiments using LLaMA-3 8B, the framework achieved a 55% to 67% reduction in active KV cache size while maintaining generation quality, including successful results on needle-in-haystack retrieval tasks. Notably, ASR-KF-EGR is architecture-agnostic and training-free, making it practical for deployment in memory-constrained environments demanding long-context generation capability. Overall, this work presents a promising solution for managing KV memory efficiently during large language model inference while retaining output quality. <div>
arXiv:2512.11221v1 Announce Type: new 
Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Aware Multi-Expert Architecture For Lifelong Deep Learning</title>
<link>https://arxiv.org/abs/2512.11243</link>
<guid>https://arxiv.org/abs/2512.11243</guid>
<content:encoded><![CDATA[
<div> Lifelong learning, Continual learning, Multi-expert system, Catastrophic forgetting, Task similarity<br /><br />Summary:<br /><br />The article introduces Task-Aware Multi-Expert (TAME), a novel algorithm for lifelong deep learning (LDL) that enables neural networks to learn sequentially across multiple tasks while preserving previously acquired knowledge. TAME leverages task similarity to guide the selection of the most appropriate pretrained expert network from a pool, enhancing knowledge transfer for each new task. A shared dense layer integrates features from the selected expert to generate accurate predictions. To mitigate catastrophic forgetting, TAME employs a replay buffer that stores representative samples and embeddings from past tasks, which are reused during training to reinforce prior knowledge. Additionally, an attention mechanism prioritizes the most relevant stored information dynamically for each prediction, ensuring efficient knowledge retrieval. Experimental results on binary classification tasks derived from CIFAR-100 demonstrate that TAME improves accuracy on new tasks without degrading performance on earlier tasks. These findings highlight TAME's effectiveness in balancing adaptation to new information and retention of existing knowledge, addressing core challenges in lifelong learning. Overall, TAME offers a flexible and robust approach to continual learning by combining expert selection, replay-based memory, and attention mechanisms. <div>
arXiv:2512.11243v1 Announce Type: new 
Abstract: Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language</title>
<link>https://arxiv.org/abs/2512.11251</link>
<guid>https://arxiv.org/abs/2512.11251</guid>
<content:encoded><![CDATA[
<div> Keywords: time-series analysis, multimodal model, dataset, GPT-4, trend description<br /><br />Summary:  
This paper introduces **Insight Miner**, a novel large-scale multimodal model (LMM) designed to generate detailed and high-quality descriptions of time-series data enriched with domain-specific insights.  To support the training of Insight Miner, the authors present **TS-Insights**, the first general-domain dataset aligning time-series windows with natural language descriptions. TS-Insights comprises 100,000 time-series windows sampled from 20 different forecasting datasets, representing a diverse range of domains. The dataset was constructed using an innovative **agentic workflow** that first extracts statistical features from raw time-series data and then uses GPT-4 to synthesize these features into coherent, human-readable trend descriptions. After performing instruction tuning of Insight Miner on the TS-Insights dataset, the model outperformed existing state-of-the-art multimodal models like LLaVA as well as GPT-4 itself in generating accurate and insightful time-series descriptions. This work highlights the potential of leveraging large multimodal models for native time-series interpretation, offering a new avenue for efficient and expert-level time-series data analysis without deep domain knowledge. The contributions lay foundational groundwork for future research in incorporating LLMs as powerful tools for time-series understanding across various scientific and industrial fields. <div>
arXiv:2512.11251v1 Announce Type: new 
Abstract: Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose \textbf{Insight Miner}, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce \textbf{TS-Insights}\footnote{Available at \href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel \textbf{agentic workflow}, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA \citep{liu2023llava} and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple Generalisation of the Implicit Dynamics of In-Context Learning</title>
<link>https://arxiv.org/abs/2512.11255</link>
<guid>https://arxiv.org/abs/2512.11255</guid>
<content:encoded><![CDATA[
<div> Keywords: in-context learning, transformer blocks, implicit weight updates, layer normalization, linear regression tasks  

<br /><br />Summary:  
This paper studies in-context learning (ICL), the capability of models to learn tasks from input examples without parameter updates. Previous ICL theories largely depended on simplified toy models and data, but recent work by Dherin et al. (2025) showed that a transformer block can be interpreted as implicitly updating its feedforward network weights based on context. The authors extend this theory by generalizing it to cover three new dimensions: (i) all token positions in a sequence, not just the final one; (ii) any transformer block layer beyond the first; and (iii) more realistic transformer architectures that include residual connections and layer normalization. Through empirical experiments on basic in-context linear regression tasks, the theory is validated, demonstrating that implicit weight updates occur within and across transformer blocks and tokens. These findings offer a more practical and comprehensive theoretical understanding of ICL mechanisms in transformers. The work represents an important step toward bridging theory and large-scale model behavior, with the potential to guide further research and validation on complex models commonly used in natural language processing. <div>
arXiv:2512.11255v1 Announce Type: new 
Abstract: In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Features Emerge as Discrete States: The First Application of SAEs to 3D Representations</title>
<link>https://arxiv.org/abs/2512.11263</link>
<guid>https://arxiv.org/abs/2512.11263</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse Autoencoders, 3D reconstruction, discrete features, phase transitions, feature decomposition<br /><br />Summary:<br /><br />1. This paper presents the first application of Sparse Autoencoders (SAEs) to the 3D domain, specifically analyzing a state-of-the-art 3D reconstruction Variational Autoencoder (VAE) trained on 53,000 3D models from the Objaverse dataset.<br /><br />2. The study reveals that the 3D reconstruction model encodes discrete rather than continuous features, demonstrating that these models approximate a discrete state space characterized by phase-like transitions in feature activations.<br /><br />3. Using the framework of discrete state transitions, the authors explain three notable behaviors: the model's preference for positional encoding representations, the sigmoidal shape of reconstruction loss when features are ablated, and the bimodal distribution observed in phase transition points.<br /><br />4. The bimodality suggests that the model manages interference caused by feature superposition by redistributing it to emphasize the saliency of different features.<br /><br />5. Overall, this work compiles and elucidates unexpected phenomena in feature decomposition of 3D models and provides a novel framework for understanding the feature learning dynamics of neural networks operating on 3D data. Code and datasets will be made publicly available upon release. <div>
arXiv:2512.11263v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) are a powerful dictionary learning technique for decomposing neural network activations, translating the hidden state into human ideas with high semantic value despite no external intervention or guidance. However, this technique has rarely been applied outside of the textual domain, limiting theoretical explorations of feature decomposition. We present the \textbf{first application of SAEs to the 3D domain}, analyzing the features used by a state-of-the-art 3D reconstruction VAE applied to 53k 3D models from the Objaverse dataset. We observe that the network encodes discrete rather than continuous features, leading to our key finding: \textbf{such models approximate a discrete state space, driven by phase-like transitions from feature activations}. Through this state transition framework, we address three otherwise unintuitive behaviors -- the inclination of the reconstruction model towards positional encoding representations, the sigmoidal behavior of reconstruction loss from feature ablation, and the bimodality in the distribution of phase transition points. This final observation suggests the model \textbf{redistributes the interference caused by superposition to prioritize the saliency of different features}. Our work not only compiles and explains unexpected phenomena regarding feature decomposition, but also provides a framework to explain the model's feature learning dynamics. The code and dataset of encoded 3D objects will be available on release.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRLR: Symbolic Regression based Logic Recovery to Counter Programmable Logic Controller Attacks</title>
<link>https://arxiv.org/abs/2512.11298</link>
<guid>https://arxiv.org/abs/2512.11298</guid>
<content:encoded><![CDATA[
<div> Keywords: Programmable Logic Controllers, Industrial Control Systems, Symbolic Regression, Logic Recovery, Cyber-attack Detection<br /><br />Summary:<br /><br />Programmable Logic Controllers (PLCs) are vital in Industrial Control Systems (ICSs) but are vulnerable to cyber-attacks due to their exposure to external networks. Existing detection methods for PLC logic attacks typically rely on either specification-based models, which require expert intervention or access to source code, or machine learning models that often lack interpretability. To address these challenges, the authors propose SRLR, a Symbolic Regression based Logic Recovery solution that infers PLC logic solely from input-output data. SRLR creates explainable detection rules for controller logic attacks by recovering underlying logic formulas. The method incorporates four ICS-specific characteristics to improve performance: (1) critical ICS control logic is better expressed in the frequency domain rather than time domain; (2) controllers operate in multiple modes with infrequent mode switching; (3) robust controllers filter out noisy or outlier sensor inputs; (4) accounting for these factors reduces formula complexity, facilitating effective symbolic regression. Experimental results show SRLR consistently outperforms prior approaches, with up to 39% gain in recovery accuracy in challenging scenarios. Additionally, SRLR demonstrates strong stability and scalability when tested on a large-scale distribution grid with hundreds of voltage regulators, handling complex and varied system configurations effectively. <div>
arXiv:2512.11298v1 Announce Type: new 
Abstract: Programmable Logic Controllers (PLCs) are critical components in Industrial Control Systems (ICSs). Their potential exposure to external world makes them susceptible to cyber-attacks. Existing detection methods against controller logic attacks use either specification-based or learnt models. However, specification-based models require experts' manual efforts or access to PLC's source code, while machine learning-based models often fall short of providing explanation for their decisions. We design SRLR -- a it Symbolic Regression based Logic Recovery} solution to identify the logic of a PLC based only on its inputs and outputs. The recovered logic is used to generate explainable rules for detecting controller logic attacks. SRLR enhances the latest deep symbolic regression methods using the following ICS-specific properties: (1) some important ICS control logic is best represented in frequency domain rather than time domain; (2) an ICS controller can operate in multiple modes, each using different logic, where mode switches usually do not happen frequently; (3) a robust controller usually filters out outlier inputs as ICS sensor data can be noisy; and (4) with the above factors captured, the degree of complexity of the formulas is reduced, making effective search possible. Thanks to these enhancements, SRLR consistently outperforms all existing methods in a variety of ICS settings that we evaluate. In terms of the recovery accuracy, SRLR's gain can be as high as 39% in some challenging environment. We also evaluate SRLR on a distribution grid containing hundreds of voltage regulators, demonstrating its stability in handling large-scale, complex systems with varied configurations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QGEC : Quantum Golay Code Error Correction</title>
<link>https://arxiv.org/abs/2512.11307</link>
<guid>https://arxiv.org/abs/2512.11307</guid>
<content:encoded><![CDATA[
<div> Quantum error correction, Golay code, Transformer decoder, quantum computing, noise models  

<br /><br />Summary:  
This paper proposes a new quantum error correction (QEC) method called Quantum Golay code Error Correction (QGEC), which utilizes the classical Golay code adapted for quantum computing. The method focuses on predicting errors from syndrome measurements without directly measuring data qubits, a crucial step in protecting qubits from external noise. The authors explore decoding through Transformer-based models, evaluating decoder accuracy over different sets of generative polynomial weights and various noise models characterized by differing correlations between bit-flip and phase-flip errors. Results indicate that lower error correlation in the noise model corresponds to higher decoding accuracy, while the specific weights of generative polynomials have minimal impact. A comparative analysis between Transformer decoders trained on Golay code (23 data qubits, code distance 7) and those on toric code (50 data qubits, code distance 5) reveals that Golay code achieves superior decoding performance. These findings suggest that integrating Golay code with Transformer-based decoders may provide a more resource-efficient and accurate route toward fault-tolerant quantum computation, potentially reducing the qubit overhead and improving error resistance compared to conventional topological codes like the toric code. <div>
arXiv:2512.11307v1 Announce Type: new 
Abstract: Quantum computers have the possibility of a much reduced calculation load compared with classical computers in specific problems. Quantum error correction (QEC) is vital for handling qubits, which are vulnerable to external noise. In QEC, actual errors are predicted from the results of syndrome measurements by stabilizer generators, in place of making direct measurements of the data qubits. Here, we propose Quantum Golay code Error Correction (QGEC), a QEC method using Golay code, which is an efficient coding method in classical information theory. We investigated our method's ability in decoding calculations with the Transformer. We evaluated the accuracy of the decoder in a code space defined by the generative polynomials with three different weights sets and three noise models with different correlations of bit-flip error and phase-flip error. Furthermore, under a noise model following a discrete uniform distribution, we compared the decoding performance of Transformer decoders with identical architectures trained respectively on Golay and toric codes. The results showed that the noise model with the smaller correlation gave better accuracy, while the weights of the generative polynomials had little effect on the accuracy of the decoder. In addition, they showed that Golay code requiring 23 data qubits and having a code distance of 7 achieved higher decoding accuracy than toric code which requiring 50 data qubits and having a code distance of 5. This suggests that implementing quantum error correction using a Transformer may enable the Golay code to realize fault-tolerant quantum computation more efficiently.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking the Generality of Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2512.11315</link>
<guid>https://arxiv.org/abs/2512.11315</guid>
<content:encoded><![CDATA[
<div> Keywords: Generalist multimodal agents, MultiNet v1.0, Vision language models, Cross domain generality, Foundation models<br /><br />Summary:<br /><br />1. The paper addresses the challenge of evaluating generalist multimodal agents that integrate perception, language, and control to perform reliably across diverse real-world domains. <br />2. It critiques current evaluation methods as fragmented and confined to isolated benchmarks, which obscure whether foundation models truly generalize beyond their training data.<br />3. To overcome this, the authors present MultiNet v1.0, a unified benchmark designed to assess vision-language models (VLMs) and vision-language action models (VLAs) across six key capabilities: visual grounding, spatial reasoning, tool use, physical commonsense, multi-agent coordination, and continuous robot control.<br />4. Evaluations of prominent models such as GPT 5, Pi0, and Magma reveal that none demonstrate consistent cross-domain generality; performance significantly declines on unseen domains, unfamiliar modalities, or when tasks shift across domains.<br />5. The study identifies critical failure modes including modality misalignment, inconsistent output formats, and severe knowledge degradation upon domain transfer.<br />6. These results highlight a significant gap between the ideal of generalist intelligence and the actual abilities of current foundation models.<br />7. MultiNet v1.0 serves as a standardized platform for systematically identifying these performance gaps and guiding improvements in future generalist agent development, with all code, data, and leaderboards made publicly available. <div>
arXiv:2512.11315v1 Announce Type: new 
Abstract: Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Condensation-Concatenation Framework for Dynamic Graph Continual Learning</title>
<link>https://arxiv.org/abs/2512.11317</link>
<guid>https://arxiv.org/abs/2512.11317</guid>
<content:encoded><![CDATA[
<div> dynamic graphs, continual learning, graph neural networks, catastrophic forgetting, topological changes<br /><br />Summary:<br /><br />Dynamic graphs frequently occur in real-world applications where their constantly changing structures pose challenges to graph neural networks (GNNs), especially causing catastrophic forgetting. Existing continual learning methods for dynamic graphs fail to address the impact of topological changes on previously learned nodes adequately. To tackle this issue, the paper introduces a novel framework called Condensation-Concatenation-based Continual Learning (CCC). CCC first condenses past graph snapshots into compact semantic embeddings that preserve essential label distribution and topological features, ensuring historical information is retained effectively. It then selectively concatenates these condensed historical embeddings with current graph representations, allowing the model to leverage both historical and up-to-date information during learning. Additionally, the authors propose a refined Forgetting Measure (FM) tailored to dynamic graph scenarios that quantifies predictive performance degradation of existing nodes after structural updates, facilitating better evaluation of forgetting effects. Extensive experiments on four real-world datasets demonstrate CCC's superior performance compared to state-of-the-art baselines, highlighting its effectiveness in mitigating catastrophic forgetting and handling dynamic topological changes in continual learning for graphs. <div>
arXiv:2512.11317v1 Announce Type: new 
Abstract: Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pace: Physics-Aware Attentive Temporal Convolutional Network for Battery Health Estimation</title>
<link>https://arxiv.org/abs/2512.11332</link>
<guid>https://arxiv.org/abs/2512.11332</guid>
<content:encoded><![CDATA[
<div> Battery Health, Temporal Convolutional Network, Equivalent Circuit Model, Attention Mechanism, Edge Deployment  

<br /><br />Summary:  
1. Batteries play a vital role in modern energy applications, including electric vehicles and power grid storage, necessitating effective battery health management for safety, cost-efficiency, and sustainability.  
2. The paper introduces Pace, a novel physics-aware attentive temporal convolutional network designed specifically for accurate battery health estimation.  
3. Pace uniquely combines raw sensor data with physics-derived features from an equivalent circuit model to improve prediction accuracy.  
4. It includes three specialized modules: dilated temporal blocks for efficient temporal encoding, chunked attention blocks for enhanced context modeling, and a dual-head output block to integrate both short- and long-term degradation patterns of batteries.  
5. Evaluation on a large public dataset shows Pace outperforms existing state-of-the-art models, delivering 6.5x and 2.0x performance improvements over two leading baselines.  
6. The model’s efficiency and practicality are further validated through real-time deployment on a Raspberry Pi edge device, demonstrating its feasibility for practical battery health monitoring applications.  
7. Overall, Pace establishes itself as a high-performance, practical solution for battery health analytics across diverse usage conditions. <div>
arXiv:2512.11332v1 Announce Type: new 
Abstract: Batteries are critical components in modern energy systems such as electric vehicles and power grid energy storage. Effective battery health management is essential for battery system safety, cost-efficiency, and sustainability. In this paper, we propose Pace, a physics-aware attentive temporal convolutional network for battery health estimation. Pace integrates raw sensor measurements with battery physics features derived from the equivalent circuit model. We develop three battery-specific modules, including dilated temporal blocks for efficient temporal encoding, chunked attention blocks for context modeling, and a dual-head output block for fusing short- and long-term battery degradation patterns. Together, the modules enable Pace to predict battery health accurately and efficiently in various battery usage conditions. In a large public dataset, Pace performs much better than existing models, achieving an average performance improvement of 6.5 and 2.0x compared to two best-performing baseline models. We further demonstrate its practical viability with a real-time edge deployment on a Raspberry Pi. These results establish Pace as a practical and high-performance solution for battery health analytics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral entropy prior-guided deep feature fusion architecture for magnetic core loss</title>
<link>https://arxiv.org/abs/2512.11334</link>
<guid>https://arxiv.org/abs/2512.11334</guid>
<content:encoded><![CDATA[
<div> Keywords: core loss modeling, power electronics, hybrid model, deep learning, empirical models

<br /><br />Summary:
Accurate core loss modeling is essential for designing high-efficiency power electronic systems, but traditional methods suffer from limited prediction accuracy. To tackle this challenge, the IEEE Power Electronics Society initiated the MagNet Challenge in 2023, an international competition aimed at leveraging data-driven techniques to identify complex loss patterns in magnetic components. Purely data-driven approaches show strong fitting abilities but lack interpretability and generalization across different data distributions. To overcome these issues, this paper introduces a hybrid model named SEPI-TFPNet, which combines empirical knowledge with deep learning techniques. The model’s physical-prior submodule uses a spectral entropy discrimination mechanism to select the best empirical model depending on the excitation waveform. The data-driven submodule integrates convolutional neural networks, multi-head attention, and bidirectional long short-term memory networks to effectively extract features from flux-density time-series data. An adaptive feature fusion module enhances multimodal feature interaction and integration. Performance evaluation on the MagNet dataset, which includes various magnetic materials, shows SEPI-TFPNet outperforms 21 representative models from the 2023 challenge and three state-of-the-art methods developed between 2024 and 2025, demonstrating improved accuracy and robustness in core loss prediction. <div>
arXiv:2512.11334v1 Announce Type: new 
Abstract: Accurate core loss modeling is critical for the design of high-efficiency power electronic systems. Traditional core loss modeling methods have limitations in prediction accuracy. To advance this field, the IEEE Power Electronics Society launched the MagNet Challenge in 2023, the first international competition focused on data-driven power electronics design methods, aiming to uncover complex loss patterns in magnetic components through a data-driven paradigm. Although purely data-driven models demonstrate strong fitting performance, their interpretability and cross-distribution generalization capabilities remain limited. To address these issues, this paper proposes a hybrid model, SEPI-TFPNet, which integrates empirical models with deep learning. The physical-prior submodule employs a spectral entropy discrimination mechanism to select the most suitable empirical model under different excitation waveforms. The data-driven submodule incorporates convolutional neural networks, multi-head attention mechanisms, and bidirectional long short-term memory networks to extract flux-density time-series features. An adaptive feature fusion module is introduced to improve multimodal feature interaction and integration. Using the MagNet dataset containing various magnetic materials, this paper evaluates the proposed method and compares it with 21 representative models from the 2023 challenge and three advanced methods from 2024-2025. The results show that the proposed method achieves improved modeling accuracy and robustness.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.11342</link>
<guid>https://arxiv.org/abs/2512.11342</guid>
<content:encoded><![CDATA[
<div> Keywords: High-Level Synthesis, FPGA, optimization strategies, reinforcement learning, hardware metric estimation<br /><br />Summary:<br /><br />1. High-Level Synthesis (HLS) tools are critical for designing FPGA-based domain-specific accelerators, but current tools often use fixed optimization strategies originally designed for software, which limits their ability to maximize hardware performance.<br /><br />2. Adapting optimization passes specifically for individual hardware designs requires deep understanding of program semantics, precise estimation of hardware metrics, and sophisticated search methods, which existing approaches do not adequately provide.<br /><br />3. The paper introduces DAPO, a novel pass ordering framework that is aware of design structure by extracting semantic information from program control and data flow graphs.<br /><br />4. DAPO uses contrastive learning techniques to create rich embeddings representing program semantics and an analytical model to accurately estimate hardware performance metrics, both of which help guide optimization.<br /><br />5. These components work together to steer a reinforcement learning agent that discovers optimized pass orderings tailored to specific designs, resulting in significant performance improvements.<br /><br />6. Experimental results on classic HLS benchmarks show that DAPO achieves an average 2.36× speedup over the widely used Vitis HLS tool, demonstrating the effectiveness of the proposed end-to-end framework in improving FPGA accelerator design. <div>
arXiv:2512.11342v1 Announce Type: new 
Abstract: High-Level Synthesis (HLS) tools are widely adopted in FPGA-based domain-specific accelerator design. However, existing tools rely on fixed optimization strategies inherited from software compilations, limiting their effectiveness. Tailoring optimization strategies to specific designs requires deep semantic understanding, accurate hardware metric estimation, and advanced search algorithms -- capabilities that current approaches lack.
  We propose DAPO, a design structure-aware pass ordering framework that extracts program semantics from control and data flow graphs, employs contrastive learning to generate rich embeddings, and leverages an analytical model for accurate hardware metric estimation. These components jointly guide a reinforcement learning agent to discover design-specific optimization strategies. Evaluations on classic HLS designs demonstrate that our end-to-end flow delivers a 2.36 speedup over Vitis HLS on average.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits</title>
<link>https://arxiv.org/abs/2512.11345</link>
<guid>https://arxiv.org/abs/2512.11345</guid>
<content:encoded><![CDATA[
<div> equivariant diffusion policies, reinforcement learning, symmetry, sample efficiency, policy improvement  

<br /><br />Summary:  
Equivariant Diffusion Policies (EDPs) integrate diffusion models' generative power with geometric symmetries' benefits, such as better generalization and sample efficiency. Steering EDPs using reinforcement learning (RL) is promising for enhancing policy performance beyond initial demonstrations. However, applying standard non-equivariant RL can be inefficient and unstable because it does not leverage the underlying symmetries EDPs are designed to utilize. This paper provides theoretical proof that the diffusion process in an EDP is equivariant, leading to a group-invariant latent-noise Markov Decision Process (MDP) ideal for equivariant diffusion steering. Using this foundation, the authors propose a symmetry-aware steering framework. Through extensive experiments on tasks featuring different symmetry levels, they compare standard RL, equivariant RL, and approximately equivariant RL methods. Findings reveal practical limitations of strict equivariance when symmetry breaking occurs but demonstrate significant advantages when symmetry is exploited during steering. These advantages include improved sample efficiency, prevention of value function divergence, and stronger policy improvements, even when the EDP is trained with very limited demonstration data. Overall, leveraging symmetry in RL-based steering of EDPs enhances performance and stability in complex control tasks. <div>
arXiv:2512.11345v1 Announce Type: new 
Abstract: Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAT: Can Trust be Predicted with Context-Awareness in Dynamic Heterogeneous Networks?</title>
<link>https://arxiv.org/abs/2512.11352</link>
<guid>https://arxiv.org/abs/2512.11352</guid>
<content:encoded><![CDATA[
<div> Keywords: Trust Prediction, Graph Neural Networks, Dynamic Graphs, Heterogeneous Networks, Context-Awareness<br /><br />Summary: Trust prediction plays a crucial role in decision-making, risk reduction, and enhancing system security. Existing GNN-based trust prediction models have significant limitations: they often fail to capture the dynamic nature of trust, overlook the heterogeneity of real-world networks, and do not incorporate context-awareness, resulting in less precise predictions. To address these challenges, the authors propose CAT, a novel Context-Aware GNN model specifically designed for trust prediction. CAT integrates multiple components including a graph construction layer, an embedding layer, a heterogeneous attention layer, and a prediction layer to comprehensively model trust. It handles dynamic graphs by utilizing continuous-time representations along with a time encoding function to preserve temporal information. CAT effectively manages graph heterogeneity and semantic richness using a dual attention mechanism that evaluates the significance of diverse node types and individual nodes. Importantly, CAT introduces a new form of meta-paths to derive contextual features, enabling it to construct context embeddings and utilize a context-aware aggregator for more granular trust prediction. Experimental evaluations on three real-world datasets show that CAT outperforms five different baseline groups, demonstrating superior accuracy, scalability on large graphs, and robustness against attacks targeting trust and GNN vulnerabilities. <div>
arXiv:2512.11352v1 Announce Type: new 
Abstract: Trust prediction provides valuable support for decision-making, risk mitigation, and system security enhancement. Recently, Graph Neural Networks (GNNs) have emerged as a promising approach for trust prediction, owing to their ability to learn expressive node representations that capture intricate trust relationships within a network. However, current GNN-based trust prediction models face several limitations: (i) Most of them fail to capture trust dynamicity, leading to questionable inferences. (ii) They rarely consider the heterogeneous nature of real-world networks, resulting in a loss of rich semantics. (iii) None of them support context-awareness, a basic property of trust, making prediction results coarse-grained.
  To this end, we propose CAT, the first Context-Aware GNN-based Trust prediction model that supports trust dynamicity and accurately represents real-world heterogeneity. CAT consists of a graph construction layer, an embedding layer, a heterogeneous attention layer, and a prediction layer. It handles dynamic graphs using continuous-time representations and captures temporal information through a time encoding function. To model graph heterogeneity and leverage semantic information, CAT employs a dual attention mechanism that identifies the importance of different node types and nodes within each type. For context-awareness, we introduce a new notion of meta-paths to extract contextual features. By constructing context embeddings and integrating a context-aware aggregator, CAT can predict both context-aware trust and overall trust. Extensive experiments on three real-world datasets demonstrate that CAT outperforms five groups of baselines in trust prediction, while exhibiting strong scalability to large-scale graphs and robustness against both trust-oriented and GNN-oriented attacks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attacking and Securing Community Detection: A Game-Theoretic Framework</title>
<link>https://arxiv.org/abs/2512.11359</link>
<guid>https://arxiv.org/abs/2512.11359</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial graphs, community detection, attack-defense, game theory, Nash equilibrium<br /><br />Summary:<br /><br />This work extends adversarial graph concepts, previously studied in classification, to the more complex problem of community detection. The authors propose novel attack techniques aiming to hide targeted individuals within communities and defend methods to bolster the robustness of community detection models. These contributions have practical applications, such as protecting privacy in social networks and analyzing camouflage in transaction networks. To model the dynamic interplay between attacker and defender, the paper introduces a game-theoretic framework called CD-GAME. In this framework, the attacker attempts to disrupt detection while the defender, modeled via a Rayleigh Quotient approach, tries to maintain detection accuracy. The iterative process captures feedback mechanisms, with both parties updating strategies until reaching a Nash equilibrium. Experimental results confirm that the proposed attack and defense methods significantly outperform existing baselines. Notably, single-step attacks often favor highly effective but conspicuous strategies, which defenders can detect and counter. However, when the interaction evolves to Nash equilibrium, attackers tend to employ more subtle, imperceptible strategies that still effectively degrade detection performance despite defense efforts. CD-GAME thus offers valuable insights into the strategic dynamics of adversarial interactions in community detection tasks. <div>
arXiv:2512.11359v1 Announce Type: new 
Abstract: It has been demonstrated that adversarial graphs, i.e., graphs with imperceptible perturbations, can cause deep graph models to fail on classification tasks. In this work, we extend the concept of adversarial graphs to the community detection problem, which is more challenging. We propose novel attack and defense techniques for community detection problem, with the objective of hiding targeted individuals from detection models and enhancing the robustness of community detection models, respectively. These techniques have many applications in real-world scenarios, for example, protecting personal privacy in social networks and understanding camouflage patterns in transaction networks. To simulate interactive attack and defense behaviors, we further propose a game-theoretic framework, called CD-GAME. One player is a graph attacker, while the other player is a Rayleigh Quotient defender. The CD-GAME models the mutual influence and feedback mechanisms between the attacker and the defender, revealing the dynamic evolutionary process of the game. Both players dynamically update their strategies until they reach the Nash equilibrium. Extensive experiments demonstrate the effectiveness of our proposed attack and defense methods, and both outperform existing baselines by a significant margin. Furthermore, CD-GAME provides valuable insights for understanding interactive attack and defense scenarios in community detection problems. We found that in traditional single-step attack or defense, attacker tends to employ strategies that are most effective, but are easily detected and countered by defender. When the interactive game reaches a Nash equilibrium, attacker adopts more imperceptible strategies that can still achieve satisfactory attack effectiveness even after defense.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization</title>
<link>https://arxiv.org/abs/2512.11391</link>
<guid>https://arxiv.org/abs/2512.11391</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Safety Alignment, Reinforcement Learning, Null-Space constrained Policy Optimization, Alignment Tax<br /><br />Summary: As Large Language Models (LLMs) are increasingly used in real-world applications, ensuring their alignment with human values and ethical standards is critical. Traditional Reinforcement Learning (RL) methods for safety alignment often lead to an "alignment tax," where the model forgets previously learned general abilities. To address this, the paper introduces Null-Space constrained Policy Optimization (NSPO), a novel RL framework designed to maintain LLMs’ core capabilities while improving safety alignment. NSPO works by projecting safety policy gradients into the null space of general tasks, effectively reducing the negative impact on general abilities. The authors provide theoretical proof that NSPO preserves the model’s original competencies while ensuring effective safety improvements through guaranteed descent directions. Extensive experimental results show that NSPO significantly outperforms existing safety alignment methods, achieving state-of-the-art safety without compromising accuracy on general tasks such as mathematics, coding, and instruction-following. Additionally, NSPO is data-efficient, requiring only 40% of the public human-annotated safety data from PKU-SafeRLHF, and does not rely on large amounts of mixed general task data typical in other alignment methods. This innovative approach offers a promising solution for safer and more reliable LLM deployment. <div>
arXiv:2512.11391v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bhargava Cube--Inspired Quadratic Regularization for Structured Neural Embeddings</title>
<link>https://arxiv.org/abs/2512.11392</link>
<guid>https://arxiv.org/abs/2512.11392</guid>
<content:encoded><![CDATA[
<div> Keywords: Bhargava cubes, neural representation learning, algebraic constraints, 3-dimensional latent spaces, quadratic relationships  

<br /><br />Summary:  
This paper introduces a novel neural representation learning method that integrates algebraic constraints inspired by Bhargava cubes from number theory. Unlike conventional deep learning techniques that yield unstructured and often uninterpretable latent spaces, the proposed approach maps input data into constrained 3-dimensional latent spaces governed by learned quadratic relationships derived from Bhargava’s combinatorial frameworks. The architecture includes a differentiable auxiliary loss function that operates independently of the main classification task. This loss guides the model to produce mathematically structured embeddings, ensuring interpretability and consistency. The framework is evaluated on the MNIST dataset, achieving a high classification accuracy of 99.46% while generating embeddings that naturally cluster according to digit classes and adhere to the algebraic constraints. The approach differs from existing manifold learning methods by imposing weak algebraic priors without requiring explicit geometric supervision, enabling seamless integration with standard optimization techniques. This work pioneers the application of number-theoretic constructs, specifically Bhargava cubes, in the domain of neural representation learning and lays the groundwork for embedding structured mathematical priors in neural networks to improve both interpretability and performance. <div>
arXiv:2512.11392v1 Announce Type: new 
Abstract: We present a novel approach to neural representation learning that incorporates algebraic constraints inspired by Bhargava cubes from number theory. Traditional deep learning methods learn representations in unstructured latent spaces lacking interpretability and mathematical consistency. Our framework maps input data to constrained 3-dimensional latent spaces where embeddings are regularized to satisfy learned quadratic relationships derived from Bhargava's combinatorial structures. The architecture employs a differentiable auxiliary loss function operating independently of classification objectives, guiding models toward mathematically structured representations. We evaluate on MNIST, achieving 99.46% accuracy while producing interpretable 3D embeddings that naturally cluster by digit class and satisfy learned quadratic constraints. Unlike existing manifold learning approaches requiring explicit geometric supervision, our method imposes weak algebraic priors through differentiable constraints, ensuring compatibility with standard optimization. This represents the first application of number-theoretic constructs to neural representation learning, establishing a foundation for incorporating structured mathematical priors in neural networks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sliced ReLU attention: Quasi-linear contextual expressivity via sorting</title>
<link>https://arxiv.org/abs/2512.11411</link>
<guid>https://arxiv.org/abs/2512.11411</guid>
<content:encoded><![CDATA[
<div> Keywords: sliced ReLU attention, kernel methods, quasi-linear complexity, in-context expressivity, sequence-to-sequence disentangling<br /><br />Summary:<br /><br />1. The paper introduces sliced ReLU attention, a novel attention mechanism that differs structurally from traditional softmax and ReLU-based attention methods.<br />2. Instead of applying nonlinearities to pairwise dot products between keys and queries, this approach operates on one-dimensional projections of key–query differences.<br />3. Sorting is leveraged to enable computation in O(n log(n)) time, making the mechanism highly efficient and scalable to very long context lengths.<br />4. The resulting kernel is differentiable and non-symmetric, expanding design possibilities for attention architectures.<br />5. The authors establish two theoretical in-context expressivity results previously known only for softmax attention, demonstrating that sliced ReLU attention can perform complex sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property.<br />6. Preliminary small-scale experiments illustrate the practical potential of this attention mechanism, suggesting it can serve as an effective and efficient alternative for handling very long sequences in models.<br />7. Overall, this method offers both strong theoretical foundations and computational advantages, enabling advanced expressive power with quasi-linear complexity in context length. <div>
arXiv:2512.11411v1 Announce Type: new 
Abstract: We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces</title>
<link>https://arxiv.org/abs/2512.11448</link>
<guid>https://arxiv.org/abs/2512.11448</guid>
<content:encoded><![CDATA[
<div> Keywords: clustering, hyperbolic space, Gaussian Blurring Mean Shift, hierarchical data, density-based clustering<br /><br />Summary:<br /><br />Clustering is a core unsupervised learning technique aimed at discovering patterns within data. Traditional Gaussian Blurring Mean Shift (GBMS) methods work well for clusters in Euclidean spaces but face challenges with data exhibiting hierarchical or tree-like structures. This paper introduces HypeGBMS, a novel method extending GBMS into hyperbolic space, which is better suited for representing hierarchical data. By substituting Euclidean distance calculations with hyperbolic distances and using M\"obius-weighted means, HypeGBMS ensures that all point updates respect the underlying geometry of hyperbolic space. This modification enables the method to capture latent hierarchies effectively while maintaining GBMS's original density-seeking properties. The authors provide theoretical analysis regarding the convergence and computational complexity of the proposed algorithm. Empirical evaluations across eleven real-world datasets demonstrate that HypeGBMS consistently delivers superior clustering quality compared to conventional mean-shift approaches, especially in non-Euclidean settings. This work effectively merges classical mean-shift clustering techniques with recent advances in hyperbolic representation learning, offering a rigorous and practical framework for density-based clustering on curved geometric spaces and highlighting its robustness and effectiveness for hierarchical data analysis. <div>
arXiv:2512.11448v1 Announce Type: new 
Abstract: Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs M\"obius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Expert Trajectory Utilization in LLM Post-training</title>
<link>https://arxiv.org/abs/2512.11470</link>
<guid>https://arxiv.org/abs/2512.11470</guid>
<content:encoded><![CDATA[
<div> Keywords: Supervised Fine-Tuning, Reinforcement Learning, Plasticity-Ceiling Framework, Expert Trajectories, Post-Training Performance  

<br /><br />Summary:  
The paper introduces the Plasticity-Ceiling Framework to theoretically analyze the interplay between Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in post-training of models. It decomposes the final performance into two components: foundational SFT performance and RL-induced plasticity. Through comprehensive benchmarking, the authors establish that a sequential pipeline of SFT followed by RL outperforms synchronized training methods by avoiding stability issues. They present precise scaling guidelines: first, transitioning to RL during the SFT Stable or Mild Overfitting phase optimizes final performance by ensuring a solid SFT foundation without sacrificing RL adaptability; second, contrary to some prior beliefs, increasing data scale, rather than limiting it, is crucial for maximizing post-training potential, with the difficulty of trajectories acting as an additional performance multiplier; third, the minimum SFT validation loss is identified as a reliable metric for selecting expert trajectories that most effectively improve the final performance ceiling. These insights offer actionable recommendations for leveraging expert trajectories efficiently to boost the overall performance attained through post-training combining SFT and RL. <div>
arXiv:2512.11470v1 Announce Type: new 
Abstract: While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics</title>
<link>https://arxiv.org/abs/2512.11525</link>
<guid>https://arxiv.org/abs/2512.11525</guid>
<content:encoded><![CDATA[
<div> NeuralOGCM, ocean modeling, differentiable programming, deep learning, physical fidelity  

<br /><br />Summary:  
1. The paper addresses the trade-off in high-precision scientific simulations between computational efficiency and physical fidelity.  
2. NeuralOGCM is introduced as a novel ocean modeling framework that combines differentiable programming with deep learning techniques.  
3. The core of NeuralOGCM is a fully differentiable dynamical solver that incorporates physics knowledge as an inductive bias, allowing it to capture large-scale, deterministic physical evolution.  
4. Key physical parameters such as diffusion coefficients are made learnable, enabling autonomous optimization of the physical core through end-to-end training.  
5. A deep neural network component is included to learn corrections for subgrid-scale processes and discretization errors, which are not accounted for by the physics model.  
6. These two components—learnable physics integration and neural network corrections—work synergistically and their outputs are integrated within a unified ODE solver.  
7. Experimental results show that NeuralOGCM achieves long-term stability and maintains physical consistency while significantly outperforming traditional numerical models in computational speed and surpassing pure AI baselines in accuracy.  
8. The framework represents a promising new direction for creating fast, stable, and physically plausible models in scientific computing, bridging the gap between physics-based simulations and AI-driven approaches. <div>
arXiv:2512.11525v1 Announce Type: new 
Abstract: High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Time Series Forecasting with Anomalies</title>
<link>https://arxiv.org/abs/2512.11526</link>
<guid>https://arxiv.org/abs/2512.11526</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, anomalies, contrastive learning, regularization, latent-output alignment<br /><br />Summary:<br /><br />1. The paper addresses the challenge in time series forecasting of distinguishing between anomalies that have lasting impacts on future values and those that are short-lived and should be ignored.<br />2. Existing forecasting models often either overreact to noise or fail to detect persistent shifts caused by meaningful anomalies.<br />3. The authors propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns to differentiate when to respond to anomalies and when to remain invariant.<br />4. Co-TSFA uses input-only and input-output augmentations to model anomalies that are irrelevant and relevant to forecasting, respectively.<br />5. A latent-output alignment loss is introduced that connects changes in learned representations to changes in forecasts, promoting sensitivity to meaningful anomalies while ignoring irrelevant perturbations.<br />6. Experiments on benchmark datasets such as Traffic and Electricity, as well as a real-world cash-demand dataset, show Co-TSFA improves forecast accuracy under anomalous conditions without degrading performance on normal data.<br />7. The implementation of Co-TSFA is made available in an anonymized GitHub repository, which will be publicly released upon acceptance of the paper. <div>
arXiv:2512.11526v1 Announce Type: new 
Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>xGR: Efficient Generative Recommendation Serving at Scale</title>
<link>https://arxiv.org/abs/2512.11529</link>
<guid>https://arxiv.org/abs/2512.11529</guid>
<content:encoded><![CDATA[
<div> Keywords: generative recommendation, large language models, beam search, low-latency serving, recommendation system<br /><br />Summary:<br /><br />This paper addresses the challenges in generative recommendation (GR) systems that leverage large language models (LLMs) to process long user-item sequences but differ from typical LLM serving workloads. GR systems often handle long prompts and produce short, fixed-length outputs, with high computational cost during decoding due to large beam widths and extensive sorting over a vast item space. To overcome these issues, the authors propose xGR, a specialized serving system designed for GR that meets stringent low-latency requirements even under high concurrency. xGR unifies prefill and decode phases by employing staged computation and a separated Key-Value (KV) cache, enhancing efficiency. It further incorporates early sorting termination and mask-based item filtering while reusing data structures to reduce sorting overhead. Additionally, the pipeline is restructured to exploit multilevel computational overlap and multi-stream parallelism, significantly boosting throughput. Experiments on real-world recommendation service datasets demonstrate that xGR outperforms state-of-the-art baselines, achieving at least a 3.49x increase in throughput under strict latency constraints, making it highly effective for practical, large-scale generative recommendation deployments. <div>
arXiv:2512.11529v1 Announce Type: new 
Abstract: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parametric Numerical Integration with (Differential) Machine Learning</title>
<link>https://arxiv.org/abs/2512.11530</link>
<guid>https://arxiv.org/abs/2512.11530</guid>
<content:encoded><![CDATA[
<div> Keywords: parametric integrals, differential learning, machine learning, Chebyshev expansions, numerical integrals<br /><br />Summary:<br /><br />This work presents a novel machine and deep learning methodology aimed at solving parametric integrals. The authors propose not only classical machine learning approaches but also introduce a differential learning framework that leverages derivative information during the training phase, highlighting its benefits. The methodology is applied to three representative problem classes: statistical functionals, such as moments and cumulative distribution functions; the approximation of functions using Chebyshev expansions; and integrals that naturally arise from differential equations. These examples cover a broad spectrum, from smooth closed-form benchmarks to more complex numerical integral problems. The study finds that across all tested cases, the differential machine learning approach consistently outperforms standard architectures. This superior performance is reflected in lower mean squared errors, enhanced scalability with problem complexity, and improved sample efficiency during training. The incorporation of derivative information in learning models is shown to be advantageous in capturing the structure of parametric integrals more effectively, leading to better accuracy and robustness in the solutions. Overall, the paper demonstrates the potential of differential learning frameworks as powerful tools for tackling integral-related computational challenges in various scientific and engineering domains. <div>
arXiv:2512.11530v1 Announce Type: new 
Abstract: In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Criteria Automated MLOps Pipeline for Cost-Effective Cloud-Based Classifier Retraining in Response to Data Distribution Shifts</title>
<link>https://arxiv.org/abs/2512.11541</link>
<guid>https://arxiv.org/abs/2512.11541</guid>
<content:encoded><![CDATA[
<div> Keywords: MLOps, data distribution drift, automated retraining, neural network classifiers, anomaly detection<br /><br />Summary:<br /><br />1. The paper addresses the challenge of machine learning model performance degradation due to data distribution drift over time.<br />2. It proposes an automated MLOps pipeline that detects significant changes in data distribution using multi-criteria statistical techniques.<br />3. The pipeline triggers neural network classifier retraining only when necessary, optimizing computational resources and efficiency.<br />4. Experiments conducted on various benchmark anomaly detection datasets demonstrate that the proposed framework improves model accuracy and robustness compared to traditional retraining strategies.<br />5. The work lays the foundation for deploying more reliable and adaptive ML systems capable of handling dynamic data environments, reducing the reliance on manual intervention in MLOps processes. <div>
arXiv:2512.11541v1 Announce Type: new 
Abstract: The performance of machine learning (ML) models often deteriorates when the underlying data distribution changes over time, a phenomenon known as data distribution drift. When this happens, ML models need to be retrained and redeployed. ML Operations (MLOps) is often manual, i.e., humans trigger the process of model retraining and redeployment. In this work, we present an automated MLOps pipeline designed to address neural network classifier retraining in response to significant data distribution changes. Our MLOps pipeline employs multi-criteria statistical techniques to detect distribution shifts and triggers model updates only when necessary, ensuring computational efficiency and resource optimization. We demonstrate the effectiveness of our framework through experiments on several benchmark anomaly detection data sets, showing significant improvements in model accuracy and robustness compared to traditional retraining strategies. Our work provides a foundation for deploying more reliable and adaptive ML systems in dynamic real-world settings, where data distribution changes are common.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.11546</link>
<guid>https://arxiv.org/abs/2512.11546</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, data selection, time series, clustering, optimization  

<br /><br />Summary:  
1. The paper challenges the assumption that using larger sensor datasets always leads to better deep learning model performance, pointing out issues like data imbalance and redundancy.  
2. Instead of focusing on model hyperparameter tuning, the authors propose optimizing the training dataset composition itself — referred to as discovering the optimal "training diet."  
3. They introduce a framework that first encodes the raw time series data using a large-scale encoder, then applies k-means clustering to segment the dataset into behaviorally consistent clusters, which act as fundamental training ingredients.  
4. The Optuna optimization framework is employed to systematically explore different sampling ratios of these clusters, creating various tailored training subsets, which are used to train and evaluate smaller target models.  
5. Experiments on the PMSM dataset demonstrate that this data-centric approach yields models with significantly improved accuracy, reducing mean squared error (MSE) from 1.70 to 1.37, which corresponds to a 19.41% performance improvement over baseline models trained on the full dataset. <div>
arXiv:2512.11546v1 Announce Type: new 
Abstract: The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, "less is more" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal "training diet" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elastic-Net Multiple Kernel Learning: Combining Multiple Data Sources for Prediction</title>
<link>https://arxiv.org/abs/2512.11547</link>
<guid>https://arxiv.org/abs/2512.11547</guid>
<content:encoded><![CDATA[
<div> Multiple Kernel Learning, Elastic-net regularization, Neuroimaging, Support Vector Machine, Kernel Ridge Regression<br /><br />Summary: Multiple Kernel Learning (MKL) models integrate multiple data sources by combining several kernels in supervised and unsupervised learning. MKL's objective is to find an optimal linear combination of base kernels to maximize performance under regularization constraints using various norms such as \(l_1\), \(l_2\), \(l_p\), and elastic-net penalties. The elastic-net regularization is especially useful for tasks requiring model interpretability and when kernels capture correlated information, as it promotes sparsity while selecting correlated kernels. Prior ENMKL approaches typically involved a two-stage cycle of fixing kernel weights, training an SVM, and updating weights through gradient or surrogate methods. This paper proposes a novel ENMKL formulation that allows an explicit analytical update of kernel weights, simplifying the optimization process. Specifically, the authors derive algorithms for both Support Vector Machine (SVM) and Kernel Ridge Regression (KRR) within this framework and implement these methods in the PRoNTo toolbox for neuroimaging. Empirical evaluations across three neuroimaging tasks indicate that ENMKL matches or surpasses \(l_1\)-norm MKL performance and only underperforms standard SVM in one scenario. Importantly, ENMKL yields sparser and more interpretable models by effectively weighting correlated kernels, enhancing model clarity and usefulness in neuroimaging applications. <div>
arXiv:2512.11547v1 Announce Type: new 
Abstract: Multiple Kernel Learning (MKL) models combine several kernels in supervised and unsupervised settings to integrate multiple data representations or sources, each represented by a different kernel. MKL seeks an optimal linear combination of base kernels that maximizes a generalized performance measure under a regularization constraint. Various norms have been used to regularize the kernel weights, including $l1$, $l2$ and $lp$, as well as the "elastic-net" penalty, which combines $l1$- and $l2$-norm to promote both sparsity and the selection of correlated kernels. This property makes elastic-net regularized MKL (ENMKL) especially valuable when model interpretability is critical and kernels capture correlated information, such as in neuroimaging. Previous ENMKL methods have followed a two-stage procedure: fix kernel weights, train a support vector machine (SVM) with the weighted kernel, and then update the weights via gradient descent, cutting-plane methods, or surrogate functions. Here, we introduce an alternative ENMKL formulation that yields a simple analytical update for the kernel weights. We derive explicit algorithms for both SVM and kernel ridge regression (KRR) under this framework, and implement them in the open-source Pattern Recognition for Neuroimaging Toolbox (PRoNTo). We evaluate these ENMKL algorithms against $l1$-norm MKL and against SVM (or KRR) trained on the unweighted sum of kernels across three neuroimaging applications. Our results show that ENMKL matches or outperforms $l1$-norm MKL in all tasks and only underperforms standard SVM in one scenario. Crucially, ENMKL produces sparser, more interpretable models by selectively weighting correlated kernels.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fully Inductive Node Representation Learning via Graph View Transformation</title>
<link>https://arxiv.org/abs/2512.11561</link>
<guid>https://arxiv.org/abs/2512.11561</guid>
<content:encoded><![CDATA[
<div> Keywords: graph neural networks, inductive learning, view space, node representation, cross-dataset generalization  

<br /><br />Summary:  
This paper addresses the challenge of generalizing pretrained graph models to unseen datasets without retraining, focusing on fully inductive inference in graph-structured data where feature spaces differ significantly in dimensionality and semantics. The authors propose a novel concept called the "view space," a new representational axis that allows arbitrary graphs to be encoded in a unified way, overcoming limitations posed by varying feature spaces. Building upon this, they introduce Graph View Transformation (GVT), a mapping that is both node- and feature-permutation-equivariant within the view space. GVT forms the foundation for Recurrent GVT, a fully inductive node representation learning model. The model is pretrained on the OGBN-Arxiv dataset and subsequently evaluated across 27 node-classification benchmarks. Experimental results show that Recurrent GVT significantly outperforms previous state-of-the-art fully inductive graph models like GraphAny by 8.93% in accuracy. Additionally, it surpasses 12 individually tuned graph neural networks by at least 3.30% on average, demonstrating robust cross-dataset generalization capability. These findings establish the view space as an effective and principled framework for fully inductive learning of node representations in graphs. <div>
arXiv:2512.11561v1 Announce Type: new 
Abstract: Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model</title>
<link>https://arxiv.org/abs/2512.11582</link>
<guid>https://arxiv.org/abs/2512.11582</guid>
<content:encoded><![CDATA[
arXiv:2512.11582v1 Announce Type: new 
Abstract: The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents</title>
<link>https://arxiv.org/abs/2512.11584</link>
<guid>https://arxiv.org/abs/2512.11584</guid>
<content:encoded><![CDATA[
arXiv:2512.11584v1 Announce Type: new 
Abstract: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration</title>
<link>https://arxiv.org/abs/2512.11587</link>
<guid>https://arxiv.org/abs/2512.11587</guid>
<content:encoded><![CDATA[
arXiv:2512.11587v1 Announce Type: new 
Abstract: Even for the gradient descent (GD) method applied to neural network training, understanding its optimization dynamics, including convergence rate, iterate trajectories, function value oscillations, and especially its implicit acceleration, remains a challenging problem. We analyze nonlinear models with the logistic loss and show that the steps of GD reduce to those of generalized perceptron algorithms (Rosenblatt, 1958), providing a new perspective on the dynamics. This reduction yields significantly simpler algorithmic steps, which we analyze using classical linear algebra tools. Using these tools, we demonstrate on a minimalistic example that the nonlinearity in a two-layer model can provably yield a faster iteration complexity $\tilde{O}(\sqrt{d})$ compared to $\Omega(d)$ achieved by linear models, where $d$ is the number of features. This helps explain the optimization dynamics and the implicit acceleration phenomenon observed in neural networks. The theoretical results are supported by extensive numerical experiments. We believe that this alternative view will further advance research on the optimization of neural networks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast Interpretable Fuzzy Tree Learner</title>
<link>https://arxiv.org/abs/2512.11616</link>
<guid>https://arxiv.org/abs/2512.11616</guid>
<content:encoded><![CDATA[
arXiv:2512.11616v1 Announce Type: new 
Abstract: Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Streaming Continual Learning via In-Context Large Tabular Models</title>
<link>https://arxiv.org/abs/2512.11668</link>
<guid>https://arxiv.org/abs/2512.11668</guid>
<content:encoded><![CDATA[
arXiv:2512.11668v1 Announce Type: new 
Abstract: In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized Model Predictive Control</title>
<link>https://arxiv.org/abs/2512.11705</link>
<guid>https://arxiv.org/abs/2512.11705</guid>
<content:encoded><![CDATA[
arXiv:2512.11705v1 Announce Type: new 
Abstract: Learning controller parameters from closed-loop data has been shown to improve closed-loop performance. Bayesian optimization, a widely used black-box and sample-efficient learning method, constructs a probabilistic surrogate of the closed-loop performance from few experiments and uses it to select informative controller parameters. However, it typically struggles with dense high-dimensional controller parameterizations, as they may appear, for example, in tuning model predictive controllers, because standard surrogate models fail to capture the structure of such spaces. This work suggests that the use of Bayesian neural networks as surrogate models may help to mitigate this limitation. Through a comparison between Gaussian processes with Matern kernels, finite-width Bayesian neural networks, and infinite-width Bayesian neural networks on a cart-pole task, we find that Bayesian neural network surrogate models achieve faster and more reliable convergence of the closed-loop cost and enable successful optimization of parameterizations with hundreds of dimensions. Infinite-width Bayesian neural networks also maintain performance in settings with more than one thousand parameters, whereas Matern-kernel Gaussian processes rapidly lose effectiveness. These results indicate that Bayesian neural network surrogate models may be suitable for learning dense high-dimensional controller parameterizations and offer practical guidance for selecting surrogate models in learning-based controller design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2512.11760</link>
<guid>https://arxiv.org/abs/2512.11760</guid>
<content:encoded><![CDATA[
arXiv:2512.11760v1 Announce Type: new 
Abstract: Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.
  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.
  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Adaptive Vekua Cascade: A Differentiable Spectral-Analytic Solver for Physics-Informed Representation</title>
<link>https://arxiv.org/abs/2512.11776</link>
<guid>https://arxiv.org/abs/2512.11776</guid>
<content:encoded><![CDATA[
arXiv:2512.11776v1 Announce Type: new 
Abstract: Coordinate-based neural networks have emerged as a powerful tool for representing continuous physical fields, yet they face two fundamental pathologies: spectral bias, which hinders the learning of high-frequency dynamics, and the curse of dimensionality, which causes parameter explosion in discrete feature grids. We propose the Adaptive Vekua Cascade (AVC), a hybrid architecture that bridges deep learning and classical approximation theory. AVC decouples manifold learning from function approximation by using a deep network to learn a diffeomorphic warping of the physical domain, projecting complex spatiotemporal dynamics onto a latent manifold where the solution is represented by a basis of generalized analytic functions. Crucially, we replace the standard gradient-descent output layer with a differentiable linear solver, allowing the network to optimally resolve spectral coefficients in a closed form during the forward pass. We evaluate AVC on a suite of five rigorous physics benchmarks, including high-frequency Helmholtz wave propagation, sparse medical reconstruction, and unsteady 3D Navier-Stokes turbulence. Our results demonstrate that AVC achieves state-of-the-art accuracy while reducing parameter counts by orders of magnitude (e.g., 840 parameters vs. 4.2 million for 3D grids) and converging 2-3x faster than implicit neural representations. This work establishes a new paradigm for memory-efficient, spectrally accurate scientific machine learning. The code is available at https://github.com/VladimerKhasia/vecua.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective</title>
<link>https://arxiv.org/abs/2512.11784</link>
<guid>https://arxiv.org/abs/2512.11784</guid>
<content:encoded><![CDATA[
arXiv:2512.11784v1 Announce Type: new 
Abstract: Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions</title>
<link>https://arxiv.org/abs/2512.11793</link>
<guid>https://arxiv.org/abs/2512.11793</guid>
<content:encoded><![CDATA[
arXiv:2512.11793v1 Announce Type: new 
Abstract: Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion-Driven Personalized Recommendation for AI-Generated Content Using Multi-Modal Sentiment and Intent Analysis</title>
<link>https://arxiv.org/abs/2512.10963</link>
<guid>https://arxiv.org/abs/2512.10963</guid>
<content:encoded><![CDATA[
arXiv:2512.10963v1 Announce Type: cross 
Abstract: With the rapid growth of AI-generated content (AIGC) across domains such as music, video, and literature, the demand for emotionally aware recommendation systems has become increasingly important. Traditional recommender systems primarily rely on user behavioral data such as clicks, views, or ratings, while neglecting users' real-time emotional and intentional states during content interaction. To address this limitation, this study proposes a Multi-Modal Emotion and Intent Recognition Model (MMEI) based on a BERT-based Cross-Modal Transformer with Attention-Based Fusion, integrated into a cloud-native personalized AIGC recommendation framework. The proposed system jointly processes visual (facial expression), auditory (speech tone), and textual (comments or utterances) modalities through pretrained encoders ViT, Wav2Vec2, and BERT, followed by an attention-based fusion module to learn emotion-intent representations. These embeddings are then used to drive personalized content recommendations through a contextual matching layer. Experiments conducted on benchmark emotion datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrate that the proposed MMEI model achieves a 4.3% improvement in F1-score and a 12.3% reduction in cross-entropy loss compared to the best fusion-based transformer baseline. Furthermore, user-level online evaluations reveal that emotion-driven recommendations increase engagement time by 15.2% and enhance satisfaction scores by 11.8%, confirming the model's effectiveness in aligning AI-generated content with users' affective and intentional states. This work highlights the potential of cross-modal emotional intelligence for next-generation AIGC ecosystems, enabling adaptive, empathetic, and context-aware recommendation experiences.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RMSup: Physics-Informed Radio Map Super-Resolution for Compute-Enhanced Integrated Sensing and Communications</title>
<link>https://arxiv.org/abs/2512.10965</link>
<guid>https://arxiv.org/abs/2512.10965</guid>
<content:encoded><![CDATA[
arXiv:2512.10965v1 Announce Type: cross 
Abstract: Radio maps (RMs) provide a spatially continuous description of wireless propagation, enabling cross-layer optimization and unifying communication and sensing for integrated sensing and communications (ISAC). However, constructing high-fidelity RMs at operational scales is difficult, since physics-based solvers are time-consuming and require precise scene models, while learning methods degrade under incomplete priors and sparse measurements, often smoothing away critical discontinuities. We present RMSup, a physics-informed super-resolution framework that functions with uniform sparse sampling and imperfect environment priors. RMSup extracts Helmholtz equation-informed boundary and singularity prompts from the measurements, fuses them with base-station side information and coarse scene descriptors as conditional inputs, and employs a boundary-aware dual-head network to reconstruct a high-fidelity RM and recover environmental contours jointly. Experimental results show the proposed RMsup achieves state-of-the-art performance both in RM construction and ISAC-related environment sensing.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developmental Symmetry-Loss: A Free-Energy Perspective on Brain-Inspired Invariance Learning</title>
<link>https://arxiv.org/abs/2512.10984</link>
<guid>https://arxiv.org/abs/2512.10984</guid>
<content:encoded><![CDATA[
arXiv:2512.10984v1 Announce Type: cross 
Abstract: We propose Symmetry-Loss, a brain-inspired algorithmic principle that enforces invariance and equivariance through a differentiable constraint derived from environmental symmetries. The framework models learning as the iterative refinement of an effective symmetry group, paralleling developmental processes in which cortical representations align with the world's structure. By minimizing structural surprise, i.e. deviations from symmetry consistency, Symmetry-Loss operationalizes a Free-Energy--like objective for representation learning. This formulation bridges predictive-coding and group-theoretic perspectives, showing how efficient, stable, and compositional representations can emerge from symmetry-based self-organization. The result is a general computational mechanism linking developmental learning in the brain with principled representation learning in artificial systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Marti-5: A Mathematical Model of "Self in the World" as a First Step Toward Self-Awareness</title>
<link>https://arxiv.org/abs/2512.10985</link>
<guid>https://arxiv.org/abs/2512.10985</guid>
<content:encoded><![CDATA[
arXiv:2512.10985v1 Announce Type: cross 
Abstract: The existence of 'what' and 'where' pathways of information processing in the brain was proposed almost 30 years ago, but there is still a lack of a clear mathematical model that could show how these pathways work together. We propose a biologically inspired mathematical model that uses this idea to identify and separate the self from the environment and then build and use a self-model for better predictions. This is a model of neocortical columns governed by the basal ganglia to make predictions and choose the next action, where some columns act as 'what' columns and others act as 'where' columns. Based on this model, we present a reinforcement learning agent that learns purposeful behavior in a virtual environment. We evaluate the agent on the Atari games Pong and Breakout, where it successfully learns to play. We conclude that the ability to separate the self from the environment gives advantages to the agent and therefore such a model could appear in living organisms during evolution. We propose Self-Awareness Principle 1: the ability to separate the self from the world is a necessary but insufficient condition for self-awareness.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization of Long-Range Machine Learning Potentials in Complex Chemical Spaces</title>
<link>https://arxiv.org/abs/2512.10989</link>
<guid>https://arxiv.org/abs/2512.10989</guid>
<content:encoded><![CDATA[
arXiv:2512.10989v1 Announce Type: cross 
Abstract: The vastness of chemical space makes generalization a central challenge in the development of machine learning interatomic potentials (MLIPs). While MLIPs could enable large-scale atomistic simulations with near-quantum accuracy, their usefulness is often limited by poor transferability to out-of-distribution samples. Here, we systematically evaluate different MLIP architectures with long-range corrections across diverse chemical spaces and show that such schemes are essential, not only for improving in-distribution performance but, more importantly, for enabling significant gains in transferability to unseen regions of chemical space. To enable a more rigorous benchmarking, we introduce biased train-test splitting strategies, which explicitly test the model performance in significantly different regions of chemical space. Together, our findings highlight the importance of long-range modeling for achieving generalizable MLIPs and provide a framework for diagnosing systematic failures across chemical space. Although we demonstrate our methodology on metal-organic frameworks, it is broadly applicable to other materials, offering insights into the design of more robust and transferable MLIPs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STARK denoises spatial transcriptomics images via adaptive regularization</title>
<link>https://arxiv.org/abs/2512.10994</link>
<guid>https://arxiv.org/abs/2512.10994</guid>
<content:encoded><![CDATA[
arXiv:2512.10994v1 Announce Type: cross 
Abstract: We present an approach to denoising spatial transcriptomics images that is particularly effective for uncovering cell identities in the regime of ultra-low sequencing depths, and also allows for interpolation of gene expression. The method -- Spatial Transcriptomics via Adaptive Regularization and Kernels (STARK) -- augments kernel ridge regression with an incrementally adaptive graph Laplacian regularizer. In each iteration, we (1) perform kernel ridge regression with a fixed graph to update the image, and (2) update the graph based on the new image. The kernel ridge regression step involves reducing the infinite dimensional problem on a space of images to finite dimensions via a modified representer theorem. Starting with a purely spatial graph, and updating it as we improve our image makes the graph more robust to noise in low sequencing depth regimes. We show that the aforementioned approach optimizes a block-convex objective through an alternating minimization scheme wherein the sub-problems have closed form expressions that are easily computed. This perspective allows us to prove convergence of the iterates to a stationary point of this non-convex objective. Statistically, such stationary points converge to the ground truth with rate $\mathcal{O}(R^{-1/2})$ where $R$ is the number of reads. In numerical experiments on real spatial transcriptomics data, the denoising performance of STARK, evaluated in terms of label transfer accuracy, shows consistent improvement over the competing methods tested.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosted Random Forests for Predicting Treatment Failure of Chemotherapy Regimens</title>
<link>https://arxiv.org/abs/2512.10995</link>
<guid>https://arxiv.org/abs/2512.10995</guid>
<content:encoded><![CDATA[
arXiv:2512.10995v1 Announce Type: cross 
Abstract: Cancer patients may undergo lengthy and painful chemotherapy treatments, comprising several successive regimens or plans. Treatment inefficacy and other adverse events can lead to discontinuation (or failure) of these plans, or prematurely changing them, which results in a significant amount of physical, financial, and emotional toxicity to the patients and their families. In this work, we build treatment failure models based on the Real World Evidence (RWE) gathered from patients' profiles available in our oncology EMR/EHR system. We also describe our feature engineering pipeline, experimental methods, and valuable insights obtained about treatment failures from trained models. We report our findings on five primary cancer types with the most frequent treatment failures (or discontinuations) to build unique and novel feature vectors from the clinical notes, diagnoses, and medications that are available in our oncology EMR. After following a novel three axes - performance, complexity and explainability - design exploration framework, boosted random forests are selected because they provide a baseline accuracy of 80% and an F1 score of 75%, with reduced model complexity, thus making them more interpretable to and usable by oncologists.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification</title>
<link>https://arxiv.org/abs/2512.11015</link>
<guid>https://arxiv.org/abs/2512.11015</guid>
<content:encoded><![CDATA[
arXiv:2512.11015v1 Announce Type: cross 
Abstract: In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Variant of One-Class SVM with Lifelong Online Learning Guarantees</title>
<link>https://arxiv.org/abs/2512.11052</link>
<guid>https://arxiv.org/abs/2512.11052</guid>
<content:encoded><![CDATA[
arXiv:2512.11052v1 Announce Type: cross 
Abstract: We study outlier (a.k.a., anomaly) detection for single-pass non-stationary streaming data. In the well-studied offline or batch outlier detection problem, traditional methods such as kernel One-Class SVM (OCSVM) are both computationally heavy and prone to large false-negative (Type II) errors under non-stationarity. To remedy this, we introduce SONAR, an efficient SGD-based OCSVM solver with strongly convex regularization. We show novel theoretical guarantees on the Type I/II errors of SONAR, superior to those known for OCSVM, and further prove that SONAR ensures favorable lifelong learning guarantees under benign distribution shifts. In the more challenging problem of adversarial non-stationary data, we show that SONAR can be used within an ensemble method and equipped with changepoint detection to achieve adaptive guarantees, ensuring small Type I/II errors on each phase of data. We validate our theoretical findings on synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data</title>
<link>https://arxiv.org/abs/2512.11074</link>
<guid>https://arxiv.org/abs/2512.11074</guid>
<content:encoded><![CDATA[
arXiv:2512.11074v1 Announce Type: cross 
Abstract: Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \(30000\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\_Hans and Zh\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \(0.8\) cosine similarity and symmetric KL divergence less than \(0.000251\) for all languages supported except Zh\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\%$ greater than MultiScript30k-Uk per split.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Recovery of Locally Important Signed Features and Interactions from Random Forest</title>
<link>https://arxiv.org/abs/2512.11081</link>
<guid>https://arxiv.org/abs/2512.11081</guid>
<content:encoded><![CDATA[
arXiv:2512.11081v1 Announce Type: cross 
Abstract: Feature and Interaction Importance (FII) methods are essential in supervised learning for assessing the relevance of input variables and their interactions in complex prediction models. In many domains, such as personalized medicine, local interpretations for individual predictions are often required, rather than global scores summarizing overall feature importance. Random Forests (RFs) are widely used in these settings, and existing interpretability methods typically exploit tree structures and split statistics to provide model-specific insights. However, theoretical understanding of local FII methods for RF remains limited, making it unclear how to interpret high importance scores for individual predictions. We propose a novel, local, model-specific FII method that identifies frequent co-occurrences of features along decision paths, combining global patterns with those observed on paths specific to a given test point. We prove that our method consistently recovers the true local signal features and their interactions under a Locally Spike Sparse (LSS) model and also identifies whether large or small feature values drive a prediction. We illustrate the usefulness of our method and theoretical results through simulation studies and a real-world data example.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TPV: Parameter Perturbations Through the Lens of Test Prediction Variance</title>
<link>https://arxiv.org/abs/2512.11089</link>
<guid>https://arxiv.org/abs/2512.11089</guid>
<content:encoded><![CDATA[
arXiv:2512.11089v1 Announce Type: cross 
Abstract: We identify test prediction variance (TPV) -- the first-order sensitivity of model outputs to parameter perturbations around a trained solution -- as a unifying quantity that links several classical observations about generalization in deep networks. TPV is a fully label-free object whose trace form separates the geometry of the trained model from the specific perturbation mechanism, allowing a broad family of parameter perturbations like SGD noise, label noise, finite-precision noise, and other post-training perturbations to be analyzed under a single framework. Theoretically, we show that TPV estimated on the training set converges to its test-set value in the overparameterized limit, providing the first result that prediction variance under local parameter perturbations can be inferred from training inputs alone. Empirically, TPV exhibits a striking stability across datasets and architectures -- including extremely narrow networks -- and correlates well with clean test loss. Finally, we demonstrate that modeling pruning as a TPV perturbation yields a simple label-free importance measure that performs competitively with state-of-the-art pruning methods, illustrating the practical utility of TPV. Code available at github.com/devansharpit/TPV.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Model Reduction using WeldNet: Windowed Encoders for Learning Dynamics</title>
<link>https://arxiv.org/abs/2512.11090</link>
<guid>https://arxiv.org/abs/2512.11090</guid>
<content:encoded><![CDATA[
arXiv:2512.11090v1 Announce Type: cross 
Abstract: Many problems in science and engineering involve time-dependent, high dimensional datasets arising from complex physical processes, which are costly to simulate. In this work, we propose WeldNet: Windowed Encoders for Learning Dynamics, a data-driven nonlinear model reduction framework to build a low-dimensional surrogate model for complex evolution systems. Given time-dependent training data, we split the time domain into multiple overlapping windows, within which nonlinear dimension reduction is performed by auto-encoders to capture latent codes. Once a low-dimensional representation of the data is learned, a propagator network is trained to capture the evolution of the latent codes in each window, and a transcoder is trained to connect the latent codes between adjacent windows. The proposed windowed decomposition significantly simplifies propagator training by breaking long-horizon dynamics into multiple short, manageable segments, while the transcoders ensure consistency across windows. In addition to the algorithmic framework, we develop a mathematical theory establishing the representation power of WeldNet under the manifold hypothesis, justifying the success of nonlinear model reduction via deep autoencoder-based architectures. Our numerical experiments on various differential equations indicate that WeldNet can capture nonlinear latent structures and their underlying dynamics, outperforming both traditional projection-based approaches and recently developed nonlinear model reduction methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound</title>
<link>https://arxiv.org/abs/2512.11169</link>
<guid>https://arxiv.org/abs/2512.11169</guid>
<content:encoded><![CDATA[
arXiv:2512.11169v1 Announce Type: cross 
Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&amp;B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&amp;B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADKnitter: Compositional CAD Generation from Text and Geometry Guidance</title>
<link>https://arxiv.org/abs/2512.11199</link>
<guid>https://arxiv.org/abs/2512.11199</guid>
<content:encoded><![CDATA[
arXiv:2512.11199v1 Announce Type: cross 
Abstract: Crafting computer-aided design (CAD) models has long been a painstaking and time-intensive task, demanding both precision and expertise from designers. With the emergence of 3D generation, this task has undergone a transformative impact, shifting not only from visual fidelity to functional utility but also enabling editable CAD designs. Prior works have achieved early success in single-part CAD generation, which is not well-suited for real-world applications, as multiple parts need to be assembled under semantic and geometric constraints. In this paper, we propose CADKnitter, a compositional CAD generation framework with a geometry-guided diffusion sampling strategy. CADKnitter is able to generate a complementary CAD part that follows both the geometric constraints of the given CAD model and the semantic constraints of the desired design text prompt. We also curate a dataset, so-called KnitCAD, containing over 310,000 samples of CAD models, along with textual prompts and assembly metadata that provide semantic and geometric constraints. Intensive experiments demonstrate that our proposed method outperforms other state-of-the-art baselines by a clear margin.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration</title>
<link>https://arxiv.org/abs/2512.11200</link>
<guid>https://arxiv.org/abs/2512.11200</guid>
<content:encoded><![CDATA[
arXiv:2512.11200v1 Announce Type: cross 
Abstract: Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>amc: The Automated Mission Classifier for Telescope Bibliographies</title>
<link>https://arxiv.org/abs/2512.11202</link>
<guid>https://arxiv.org/abs/2512.11202</guid>
<content:encoded><![CDATA[
arXiv:2512.11202v1 Announce Type: cross 
Abstract: Telescope bibliographies record the pulse of astronomy research by capturing publication statistics and citation metrics for telescope facilities. Robust and scalable bibliographies ensure that we can measure the scientific impact of our facilities and archives. However, the growing rate of publications threatens to outpace our ability to manually label astronomical literature. We therefore present the Automated Mission Classifier (amc), a tool that uses large language models (LLMs) to identify and categorize telescope references by processing large quantities of paper text. A modified version of amc performs well on the TRACS Kaggle challenge, achieving a macro $F_1$ score of 0.84 on the held-out test set. amc is valuable for other telescopes beyond TRACS; we developed the initial software for identifying papers that featured scientific results by NASA missions. Additionally, we investigate how amc can also be used to interrogate historical datasets and surface potential label errors. Our work demonstrates that LLM-based applications offer powerful and scalable assistance for library sciences.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFMF: World Modeling by Forecasting Vision Foundation Model Features</title>
<link>https://arxiv.org/abs/2512.11225</link>
<guid>https://arxiv.org/abs/2512.11225</guid>
<content:encoded><![CDATA[
arXiv:2512.11225v1 Announce Type: cross 
Abstract: Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Objective Reinforcement Learning for Large-Scale Mixed Traffic Control</title>
<link>https://arxiv.org/abs/2512.11247</link>
<guid>https://arxiv.org/abs/2512.11247</guid>
<content:encoded><![CDATA[
arXiv:2512.11247v1 Announce Type: cross 
Abstract: Effective mixed traffic control requires balancing efficiency, fairness, and safety. Existing approaches excel at optimizing efficiency and enforcing safety constraints but lack mechanisms to ensure equitable service, resulting in systematic starvation of vehicles on low-demand approaches. We propose a hierarchical framework combining multi-objective reinforcement learning for local intersection control with strategic routing for network-level coordination. Our approach introduces a Conflict Threat Vector that provides agents with explicit risk signals for proactive conflict avoidance, and a queue parity penalty that ensures equitable service across all traffic streams. Extensive experiments on a real-world network across different robot vehicle (RV) penetration rates demonstrate substantial improvements: up to 53% reductions in average wait time, up to 86% reductions in maximum starvation, and up to 86\% reduction in conflict rate compared to baselines, while maintaining fuel efficiency. Our analysis reveals that strategic routing effectiveness scales with RV penetration, becoming increasingly valuable at higher autonomy levels. The results demonstrate that multi-objective optimization through well-curated reward functions paired with strategic RV routing yields significant benefits in fairness and safety metrics critical for equitable mixed-autonomy deployment.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrated Prediction and Multi-period Portfolio Optimization</title>
<link>https://arxiv.org/abs/2512.11273</link>
<guid>https://arxiv.org/abs/2512.11273</guid>
<content:encoded><![CDATA[
arXiv:2512.11273v1 Announce Type: cross 
Abstract: Multi-period portfolio optimization is important for real portfolio management, as it accounts for transaction costs, path-dependent risks, and the intertemporal structure of trading decisions that single-period models cannot capture. Classical methods usually follow a two-stage framework: machine learning algorithms are employed to produce forecasts that closely fit the realized returns, and the predicted values are then used in a downstream portfolio optimization problem to determine the asset weights. This separation leads to a fundamental misalignment between predictions and decision outcomes, while also ignoring the impact of transaction costs. To bridge this gap, recent studies have proposed the idea of end-to-end learning, integrating the two stages into a single pipeline. This paper introduces IPMO (Integrated Prediction and Multi-period Portfolio Optimization), a model for multi-period mean-variance portfolio optimization with turnover penalties. The predictor generates multi-period return forecasts that parameterize a differentiable convex optimization layer, which in turn drives learning via portfolio performance. For scalability, we introduce a mirror-descent fixed-point (MDFP) differentiation scheme that avoids factorizing the Karush-Kuhn-Tucker (KKT) systems, which thus yields stable implicit gradients and nearly scale-insensitive runtime as the decision horizon grows. In experiments with real market data and two representative time-series prediction models, the IPMO method consistently outperforms the two-stage benchmarks in risk-adjusted performance net of transaction costs and achieves more coherent allocation paths. Our results show that integrating machine learning prediction with optimization in the multi-period setting improves financial outcomes and remains computationally tractable.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents</title>
<link>https://arxiv.org/abs/2512.11277</link>
<guid>https://arxiv.org/abs/2512.11277</guid>
<content:encoded><![CDATA[
arXiv:2512.11277v1 Announce Type: cross 
Abstract: Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maritime object classification with SAR imagery using quantum kernel methods</title>
<link>https://arxiv.org/abs/2512.11367</link>
<guid>https://arxiv.org/abs/2512.11367</guid>
<content:encoded><![CDATA[
arXiv:2512.11367v1 Announce Type: cross 
Abstract: Illegal, unreported, and unregulated (IUU) fishing causes global economic losses of \$10-25 billion annually and undermines marine sustainability and governance. Synthetic Aperture Radar (SAR) provides reliable maritime surveillance under all weather and lighting conditions, but classifying small maritime objects in SAR imagery remains challenging. We investigate quantum machine learning for this task, focusing on Quantum Kernel Methods (QKMs) applied to real and complex SAR chips extracted from the SARFish dataset. We tackle two binary classification problems, the first for distinguishing vessels from non-vessels, and the second for distinguishing fishing vessels from other types of vessels. We compare QKMs applied to real and complex SAR chips against classical Laplacian, RBF, and linear kernels applied to real SAR chips. Using noiseless numerical simulations of the quantum kernels, we find that QKMs are capable of obtaining equal or better performance than the classical kernel on these tasks in the best case, but do not demonstrate a clear advantage for the complex SAR data. This work presents the first application of QKMs to maritime classification in SAR imagery and offers insight into the potential and current limitations of quantum-enhanced learning for maritime surveillance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Out-of-Distribution Segmentation via Wasserstein-Based Evidential Uncertainty</title>
<link>https://arxiv.org/abs/2512.11373</link>
<guid>https://arxiv.org/abs/2512.11373</guid>
<content:encoded><![CDATA[
arXiv:2512.11373v1 Announce Type: cross 
Abstract: Deep neural networks achieve superior performance in semantic segmentation, but are limited to a predefined set of classes, which leads to failures when they encounter unknown objects in open-world scenarios. Recognizing and segmenting these out-of-distribution (OOD) objects is crucial for safety-critical applications such as automated driving. In this work, we present an evidence segmentation framework using a Wasserstein loss, which captures distributional distances while respecting the probability simplex geometry. Combined with Kullback-Leibler regularization and Dice structural consistency terms, our approach leads to improved OOD segmentation performance compared to uncertainty-based approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models</title>
<link>https://arxiv.org/abs/2512.11412</link>
<guid>https://arxiv.org/abs/2512.11412</guid>
<content:encoded><![CDATA[
arXiv:2512.11412v1 Announce Type: cross 
Abstract: Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergence of Nonequilibrium Latent Cycles in Unsupervised Generative Modeling</title>
<link>https://arxiv.org/abs/2512.11415</link>
<guid>https://arxiv.org/abs/2512.11415</guid>
<content:encoded><![CDATA[
arXiv:2512.11415v1 Announce Type: cross 
Abstract: We show that nonequilibrium dynamics can play a constructive role in unsupervised machine learning by inducing the spontaneous emergence of latent-state cycles. We introduce a model in which visible and hidden variables interact through two independently parametrized transition matrices, defining a Markov chain whose steady state is intrinsically out of equilibrium. Likelihood maximization drives this system toward nonequilibrium steady states with finite entropy production, reduced self-transition probabilities, and persistent probability currents in the latent space. These cycles are not imposed by the architecture but arise from training, and models that develop them avoid the low-log-likelihood regime associated with nearly reversible dynamics while more faithfully reproducing the empirical distribution of data classes. Compared with equilibrium approaches such as restricted Boltzmann machines, our model breaks the detailed balance between the forward and backward conditional transitions and relies on a log-likelihood gradient that depends explicitly on the last two steps of the Markov chain. Hence, this exploration of the interface between nonequilibrium statistical physics and modern machine learning suggests that introducing irreversibility into latent-variable models can enhance generative performance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring MLLM-Diffusion Information Transfer with MetaCanvas</title>
<link>https://arxiv.org/abs/2512.11464</link>
<guid>https://arxiv.org/abs/2512.11464</guid>
<content:encoded><![CDATA[
arXiv:2512.11464v1 Announce Type: cross 
Abstract: Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation</title>
<link>https://arxiv.org/abs/2512.11465</link>
<guid>https://arxiv.org/abs/2512.11465</guid>
<content:encoded><![CDATA[
arXiv:2512.11465v1 Announce Type: cross 
Abstract: Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FRQI Pairs method for image classification using Quantum Recurrent Neural Network</title>
<link>https://arxiv.org/abs/2512.11499</link>
<guid>https://arxiv.org/abs/2512.11499</guid>
<content:encoded><![CDATA[
arXiv:2512.11499v1 Announce Type: cross 
Abstract: This study aims to introduce the FRQI Pairs method to a wider audience, a novel approach to image classification using Quantum Recurrent Neural Networks (QRNN) with Flexible Representation for Quantum Images (FRQI).
  The study highlights an innovative approach to use quantum encoded data for an image classification task, suggesting that such quantum-based approaches could significantly reduce the complexity of quantum algorithms. Comparison of the FRQI Pairs method with contemporary techniques underscores the promise of integrating quantum computing principles with neural network architectures for the development of quantum machine learning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BAID: A Benchmark for Bias Assessment of AI Detectors</title>
<link>https://arxiv.org/abs/2512.11505</link>
<guid>https://arxiv.org/abs/2512.11505</guid>
<content:encoded><![CDATA[
arXiv:2512.11505v1 Announce Type: cross 
Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France</title>
<link>https://arxiv.org/abs/2512.11524</link>
<guid>https://arxiv.org/abs/2512.11524</guid>
<content:encoded><![CDATA[
arXiv:2512.11524v1 Announce Type: cross 
Abstract: Fine-scale forest monitoring is essential for understanding canopy structure and its dynamics, which are key indicators of carbon stocks, biodiversity, and forest health. Deep learning is particularly effective for this task, as it integrates spectral, temporal, and spatial signals that jointly reflect the canopy structure. To address this need, we introduce THREASURE-Net, a novel end-to-end framework for Tree Height Regression And Super-Resolution. The model is trained on Sentinel-2 time series using reference height metrics derived from LiDAR HD data at multiple spatial resolutions over Metropolitan France to produce annual height maps. We evaluate three model variants, producing tree-height predictions at 2.5 m, 5 m, and 10 m resolution. THREASURE-Net does not rely on any pretrained model nor on reference very high resolution optical imagery to train its super-resolution module; instead, it learns solely from LiDAR-derived height information. Our approach outperforms existing state-of-the-art methods based on Sentinel data and is competitive with methods based on very high resolution imagery. It can be deployed to generate high-precision annual canopy-height maps, achieving mean absolute errors of 2.62 m, 2.72 m, and 2.88 m at 2.5 m, 5 m, and 10 m resolution, respectively. These results highlight the potential of THREASURE-Net for scalable and cost-effective structural monitoring of temperate forests using only freely available satellite data. The source code for THREASURE-Net is available at: https://github.com/Global-Earth-Observation/threasure-net.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visualizing token importance for black-box language models</title>
<link>https://arxiv.org/abs/2512.11573</link>
<guid>https://arxiv.org/abs/2512.11573</guid>
<content:encoded><![CDATA[
arXiv:2512.11573v1 Announce Type: cross 
Abstract: We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning for Seismic Data Processing</title>
<link>https://arxiv.org/abs/2512.11575</link>
<guid>https://arxiv.org/abs/2512.11575</guid>
<content:encoded><![CDATA[
arXiv:2512.11575v1 Announce Type: cross 
Abstract: Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safe Bayesian optimization across noise models via scenario programming</title>
<link>https://arxiv.org/abs/2512.11580</link>
<guid>https://arxiv.org/abs/2512.11580</guid>
<content:encoded><![CDATA[
arXiv:2512.11580v1 Announce Type: cross 
Abstract: Safe Bayesian optimization (BO) with Gaussian processes is an effective tool for tuning control policies in safety-critical real-world systems, specifically due to its sample efficiency and safety guarantees. However, most safe BO algorithms assume homoscedastic sub-Gaussian measurement noise, an assumption that does not hold in many relevant applications. In this article, we propose a straightforward yet rigorous approach for safe BO across noise models, including homoscedastic sub-Gaussian and heteroscedastic heavy-tailed distributions. We provide a high-probability bound on the measurement noise via the scenario approach, integrate these bounds into high probability confidence intervals, and prove safety and optimality for our proposed safe BO algorithm. We deploy our algorithm in synthetic examples and in tuning a controller for the Franka Emika manipulator in simulation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis</title>
<link>https://arxiv.org/abs/2512.11593</link>
<guid>https://arxiv.org/abs/2512.11593</guid>
<content:encoded><![CDATA[
arXiv:2512.11593v1 Announce Type: cross 
Abstract: Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\texttt{https://github.com/hyungrok-do/NeuralPLSI}).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols</title>
<link>https://arxiv.org/abs/2512.11614</link>
<guid>https://arxiv.org/abs/2512.11614</guid>
<content:encoded><![CDATA[
arXiv:2512.11614v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition</title>
<link>https://arxiv.org/abs/2512.11682</link>
<guid>https://arxiv.org/abs/2512.11682</guid>
<content:encoded><![CDATA[
arXiv:2512.11682v1 Announce Type: cross 
Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable spectral neural operator for learning stiff PDE systems from limited data</title>
<link>https://arxiv.org/abs/2512.11686</link>
<guid>https://arxiv.org/abs/2512.11686</guid>
<content:encoded><![CDATA[
arXiv:2512.11686v1 Announce Type: cross 
Abstract: Accurate modeling of spatiotemporal dynamics is crucial to understanding complex phenomena across science and engineering. However, this task faces a fundamental challenge when the governing equations are unknown and observational data are sparse. System stiffness, the coupling of multiple time-scales, further exacerbates this problem and hinders long-term prediction. Existing methods fall short: purely data-driven methods demand massive datasets, whereas physics-aware approaches are constrained by their reliance on known equations and fine-grained time steps. To overcome these limitations, we introduce an equation-free learning framework, namely, the Stable Spectral Neural Operator (SSNO), for modeling stiff partial differential equation (PDE) systems based on limited data. Instead of encoding specific equation terms, SSNO embeds spectrally inspired structures in its architecture, yielding strong inductive biases for learning the underlying physics. It automatically learns local and global spatial interactions in the frequency domain, while handling system stiffness with a robust integrating factor time-stepping scheme. Demonstrated across multiple 2D and 3D benchmarks in Cartesian and spherical geometries, SSNO achieves prediction errors one to two orders of magnitude lower than leading models. Crucially, it shows remarkable data efficiency, requiring only very few (2--5) training trajectories for robust generalization to out-of-distribution conditions. This work offers a robust and generalizable approach to learning stiff spatiotemporal dynamics from limited data without explicit \textit{a priori} knowledge of PDE terms.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning</title>
<link>https://arxiv.org/abs/2512.11727</link>
<guid>https://arxiv.org/abs/2512.11727</guid>
<content:encoded><![CDATA[
arXiv:2512.11727v1 Announce Type: cross 
Abstract: Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems</title>
<link>https://arxiv.org/abs/2512.11750</link>
<guid>https://arxiv.org/abs/2512.11750</guid>
<content:encoded><![CDATA[
arXiv:2512.11750v1 Announce Type: cross 
Abstract: Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Minimal Representations of Fermionic Ground States</title>
<link>https://arxiv.org/abs/2512.11767</link>
<guid>https://arxiv.org/abs/2512.11767</guid>
<content:encoded><![CDATA[
arXiv:2512.11767v1 Announce Type: cross 
Abstract: We introduce an unsupervised machine-learning framework that discovers optimally compressed representations of quantum many-body ground states. Using an autoencoder neural network architecture on data from $L$-site Fermi-Hubbard models, we identify minimal latent spaces with a sharp reconstruction quality threshold at $L-1$ latent dimensions, matching the system's intrinsic degrees of freedom. We demonstrate the use of the trained decoder as a differentiable variational ansatz to minimize energy directly within the latent space. Crucially, this approach circumvents the $N$-representability problem, as the learned manifold implicitly restricts the optimization to physically valid quantum states.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Coverage Diagnostics for Conformal Prediction</title>
<link>https://arxiv.org/abs/2512.11779</link>
<guid>https://arxiv.org/abs/2512.11779</guid>
<content:encoded><![CDATA[
arXiv:2512.11779v1 Announce Type: cross 
Abstract: Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Federated Learning with Exact Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2202.09848</link>
<guid>https://arxiv.org/abs/2202.09848</guid>
<content:encoded><![CDATA[
arXiv:2202.09848v2 Announce Type: replace 
Abstract: We propose a Stochastic Gradient Descent (SGD)-type algorithm for Personalized Federated Learning which can be particularly attractive for mobile energy-limited regimes due to its low per-client computational cost. The model to be trained includes a set of common weights for all clients, and a set of personalized weights that are specific to each client. At each optimization round, randomly selected clients perform multiple full gradient-descent updates over their client-specific weights towards optimizing the loss function on their own datasets, without updating the common weights. This procedure is energy-efficient since it has low computational cost per client. At the final update of each round, each client computes the joint gradient over both the client-specific and the common weights and returns the gradient of common weights to the server, which allows to perform an exact SGD step over the full set of weights in a distributed manner. For the overall optimization scheme, we rigorously prove convergence, even in non-convex settings such as those encountered when training neural networks, with a rate of $\mathcal{O} \left (\frac{1}{\sqrt{T}} \right )$ with respect to communication rounds $T$. In practice, PFLEGO exhibits substantially lower per-round wall-clock time, used as a proxy for energy. Our theoretical guarantees translate to superior performance in practice against baselines such as FedAvg and FedPer, as evaluated in several multi-class classification datasets, in particular, Omniglot, CIFAR-10, MNIST, Fashion-MNIST, and EMNIST.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data as Voters: Core Set Selection Using Approval-Based Multi-Winner Voting</title>
<link>https://arxiv.org/abs/2304.09995</link>
<guid>https://arxiv.org/abs/2304.09995</guid>
<content:encoded><![CDATA[
arXiv:2304.09995v3 Announce Type: replace 
Abstract: We present a novel approach to the core set/instance selection problem in machine learning. Our approach is based on recent results on (proportional) representation in approval-based multi-winner elections. In our model, instances play a double role as voters and candidates. The approval set of each instance in the training set (acting as a voter) is defined from the concept of local set, which already exists in the literature. We then select the election winners by using a representative voting rule, and such winners are the data instances kept in the reduced training set. We evaluate our approach in two experiments involving neural network classifiers and classic machine learning classifiers (KNN and SVM). Our experiments show that, in several cases, our approach improves the performance of state-of-the-art methods, and the differences are statistically significant.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M2NO: An Efficient Multi-Resolution Operator Framework for Dynamic Multi-Scale PDE Solvers</title>
<link>https://arxiv.org/abs/2406.04822</link>
<guid>https://arxiv.org/abs/2406.04822</guid>
<content:encoded><![CDATA[
arXiv:2406.04822v3 Announce Type: replace 
Abstract: Solving high-dimensional partial differential equations (PDEs) efficiently requires handling multi-scale features across varying resolutions. To address this challenge, we present the Multiwavelet-based Multigrid Neural Operator (M2NO), a deep learning framework that integrates a multigrid structure with predefined multiwavelet spaces. M2NO leverages multi-resolution analysis to selectively transfer low-frequency error components to coarser grids while preserving high-frequency details at finer levels. This design enhances both accuracy and computational efficiency without introducing additional complexity. Moreover, M2NO serves as an effective preconditioner for iterative solvers, further accelerating convergence in large-scale PDE simulations. Through extensive evaluations on diverse PDE benchmarks, including high-resolution, super-resolution tasks, and preconditioning settings, M2NO consistently outperforms existing models. Its ability to efficiently capture fine-scale variations and large-scale structures makes it a robust and versatile solution for complex PDE simulations. Our code and datasets are available on https://github.com/lizhihao2022/M2NO.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAEGAN: Generating Synthetic Tabular Data For Data Augmentation</title>
<link>https://arxiv.org/abs/2410.01933</link>
<guid>https://arxiv.org/abs/2410.01933</guid>
<content:encoded><![CDATA[
arXiv:2410.01933v2 Announce Type: replace 
Abstract: Synthetic tabular data generation has gained significant attention for its potential in data augmentation and privacy-preserving data sharing. While recent methods like diffusion and auto-regressive models (i.e., transformer) have advanced the field, generative adversarial networks (GANs) remain highly competitive due to their training efficiency and strong data generation capabilities. In this paper, we introduce Tabular Auto-Encoder Generative Adversarial Network (TAEGAN), a novel GAN-based framework that leverages a masked auto-encoder as the generator. TAEGAN is the first to incorporate self-supervised warmup training of generator into tabular GANs. It enhances GAN stability and exposes the generator to richer information beyond the discriminator's feedback. Additionally, we propose a novel sampling method tailored for imbalanced or skewed data and an improved loss function to better capture data distribution and correlations. We evaluate TAEGAN against seven state-of-the-art synthetic tabular data generation algorithms. Results from eight datasets show that TAEGAN outperforms all baselines on five datasets, achieving a 27% overall utility boost over the best-performing baseline while maintaining a model size less than 5% of the best-performing baseline model. Code is available at: https://github.com/BetterdataLabs/taegan.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Continual Instruction Assistant</title>
<link>https://arxiv.org/abs/2410.10868</link>
<guid>https://arxiv.org/abs/2410.10868</guid>
<content:encoded><![CDATA[
arXiv:2410.10868v5 Announce Type: replace 
Abstract: Continual Instruction Tuning (CIT) is adopted to continually instruct Large Models to follow human intent data by data. It is observed that existing gradient update would heavily destroy the performance on previous datasets during CIT process. Instead, Exponential Moving Average (EMA), owns the ability to trace previous parameters, which can aid in decreasing forgetting. Nonetheless, its stable balance weight fails to deal with the ever-changing datasets, leading to the out-of-balance between plasticity and stability. In this paper, we propose a general continual instruction tuning framework to address the challenge. Starting from the trade-off prerequisite and EMA update, we propose the plasticity and stability ideal condition. Based on Taylor expansion in the loss function, we find the optimal balance weight can be automatically determined by the gradients and learned parameters. Therefore, we propose a stable-plasticity balanced coefficient to avoid knowledge interference. Based on the semantic similarity of the instructions, we can determine whether to retrain or expand the training parameters and allocate the most suitable parameters for the testing instances. Extensive experiments across multiple continual instruction tuning benchmarks demonstrate that our approach not only enhances anti-forgetting capabilities but also significantly improves overall continual tuning performance. Our code is available at https://github.com/JingyangQiao/CoIN.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WARPD: World model Assisted Reactive Policy Diffusion</title>
<link>https://arxiv.org/abs/2410.14040</link>
<guid>https://arxiv.org/abs/2410.14040</guid>
<content:encoded><![CDATA[
arXiv:2410.14040v4 Announce Type: replace 
Abstract: With the increasing availability of open-source robotic data, imitation learning has become a promising approach for both manipulation and locomotion. Diffusion models are now widely used to train large, generalized policies that predict controls or trajectories, leveraging their ability to model multimodal action distributions. However, this generality comes at the cost of larger model sizes and slower inference, an acute limitation for robotic tasks requiring high control frequencies. Moreover, Diffusion Policy (DP), a popular trajectory-generation approach, suffers from a trade-off between performance and action horizon: fewer diffusion queries lead to larger trajectory chunks, which in turn accumulate tracking errors. To overcome these challenges, we introduce WARPD (World model Assisted Reactive Policy Diffusion), a method that generates closed-loop policies (weights for neural policies) directly, instead of open-loop trajectories. By learning behavioral distributions in parameter space rather than trajectory space, WARPD offers two major advantages: (1) extended action horizons with robustness to perturbations, while maintaining high task performance, and (2) significantly reduced inference costs. Empirically, WARPD outperforms DP in long-horizon and perturbed environments, and achieves multitask performance on par with DP while requiring only ~ 1/45th of the inference-time FLOPs per step.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Frozen Subspace: Importance Sampling for Low-Rank Optimization in LLM Pretraining</title>
<link>https://arxiv.org/abs/2502.05790</link>
<guid>https://arxiv.org/abs/2502.05790</guid>
<content:encoded><![CDATA[
arXiv:2502.05790v3 Announce Type: replace 
Abstract: Low-rank optimization has emerged as a promising approach to enabling memory-efficient training of large language models (LLMs). Existing low-rank optimization methods typically project gradients onto a low-rank subspace, reducing the memory cost of storing optimizer states. A key challenge in these methods is selecting suitable subspaces to ensure an effective optimization trajectory. Most existing approaches select the dominant subspace to preserve gradient information, as this intuitively provides the best approximation. However, we find that in practice, the dominant subspace stops changing during pretraining, thereby constraining weight updates to similar subspaces. In this paper, we propose importance sampling for low-rank optimization in LLM pretraining with a provable convergence guarantee, which the dominant subspace approach does not have. Empirically, we demonstrate that our method significantly outperforms previous methods in LLM pretraining tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FuncGenFoil: Airfoil Generation and Editing Model in Function Space</title>
<link>https://arxiv.org/abs/2502.10712</link>
<guid>https://arxiv.org/abs/2502.10712</guid>
<content:encoded><![CDATA[
arXiv:2502.10712v4 Announce Type: replace 
Abstract: Aircraft manufacturing is the jewel in the crown of industry, in which generating high-fidelity airfoil geometries with controllable and editable representations remains a fundamental challenge. Existing deep learning methods, which typically rely on predefined parametric representations (e.g., B\'ezier) or discrete point sets, face an inherent trade-off between expressive power and resolution adaptability. To tackle this challenge, we introduce FuncGenFoil, a novel function-space generative model that directly reconstructs airfoil geometries as function curves. Our method inherits the advantages of arbitrary-resolution sampling and smoothness from parametric functions, as well as the strong expressiveness of discrete point-based representations. Empirical evaluations demonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil generation, achieving a relative 74.4% reduction in label error and a 23.2% increase in diversity on the AF-200K dataset. Our results highlight the advantages of function-space modeling for aerodynamic shape optimization, offering a powerful and flexible framework for high-fidelity airfoil design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Statistical Learning: Supervised Learning of Statistical Estimators</title>
<link>https://arxiv.org/abs/2502.12088</link>
<guid>https://arxiv.org/abs/2502.12088</guid>
<content:encoded><![CDATA[
arXiv:2502.12088v3 Announce Type: replace 
Abstract: Statistical inference, a central tool of science, revolves around the study and the usage of statistical estimators: functions that map finite samples to predictions about unknown distribution parameters. In the frequentist framework, estimators are evaluated based on properties such as bias, variance (for parameter estimation), accuracy, power, and calibration (for hypothesis testing). However, crafting estimators with desirable properties is often analytically challenging, and sometimes impossible, e.g., there exists no universally unbiased estimator for the standard deviation. In this work, we introduce meta-statistical learning, an amortized learning framework that recasts estimator design as an optimization problem via supervised learning. This takes a fully empirical approach to discovering statistical estimators; entire datasets are input to permutation-invariant neural networks, such as Set Transformers, trained to predict the target statistical property. The trained model is the estimator, and can be analyzed through the classical frequentist lens. We demonstrate the approach on two tasks: learning a normality test (classification) and estimating mutual information (regression), achieving strong results even with small models. Looking ahead, this paradigm opens a path to automate the discovery of generalizable and flexible statistical estimators.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRKM: Twin Restricted Kernel Machines for Classification and Regression</title>
<link>https://arxiv.org/abs/2502.15759</link>
<guid>https://arxiv.org/abs/2502.15759</guid>
<content:encoded><![CDATA[
arXiv:2502.15759v2 Announce Type: replace 
Abstract: Restricted kernel machines (RKMs) have considerably improved generalization in machine learning. Recent advancements explored various techniques within the RKM framework, integrating kernel functions with least squares support vector machines (LSSVM) to mirror the energy function of restricted Boltzmann machines (RBM), leading to enhanced performance. However, RKMs may face challenges in generalization when dealing with unevenly distributed or complexly clustered data. Additionally, as the dataset size increases, the computational burden of managing high-dimensional feature spaces can become substantial, potentially hindering performance in large-scale datasets. To address these challenges, we propose twin restricted kernel machine (TRKM). TRKM combines the benefits of twin models with the robustness of the RKM framework to enhance classification and regression tasks. By leveraging the Fenchel-Young inequality, we introduce a novel conjugate feature duality, allowing the formulation of classification and regression problems in terms of dual variables. This duality provides an upper bound to the objective function of the TRKM problem, resulting in a new methodology under the RKM framework. The model uses an energy function similar to that of RBM, incorporating both visible and hidden variables corresponding to both classes. Additionally, the kernel trick is employed to map data into a high-dimensional feature space, where the model identifies an optimal separating hyperplane using a regularized least squares approach. Experiments on UCI and KEEL datasets confirm TRKM's superiority over baselines, showcasing its robustness and efficiency in handling complex data. Furthermore, We implemented the TRKM model on the brain age dataset, demonstrating its efficacy in predicting brain age.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry-Informed Neural Operator Transformer</title>
<link>https://arxiv.org/abs/2504.19452</link>
<guid>https://arxiv.org/abs/2504.19452</guid>
<content:encoded><![CDATA[
arXiv:2504.19452v5 Announce Type: replace 
Abstract: Machine-learning-based surrogate models offer significant computational efficiency and faster simulations compared to traditional numerical methods, especially for problems requiring repeated evaluations of partial differential equations. This work introduces the Geometry-Informed Neural Operator Transformer (GINOT), which integrates the transformer architecture with the neural operator framework to enable forward predictions on arbitrary geometries. GINOT employs a sampling and grouping strategy together with an attention mechanism to encode surface point clouds that are unordered, exhibit non-uniform point densities, and contain varying numbers of points for different geometries. The geometry information is seamlessly integrated with query points in the solution decoder through the attention mechanism. The performance of GINOT is validated on multiple challenging datasets, showcasing its high accuracy and strong generalization capabilities for complex and arbitrary 2D and 3D geometries.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FT-MoE: Sustainable-learning Mixture of Experts for Fault-Tolerant Computing</title>
<link>https://arxiv.org/abs/2504.20446</link>
<guid>https://arxiv.org/abs/2504.20446</guid>
<content:encoded><![CDATA[
arXiv:2504.20446v2 Announce Type: replace 
Abstract: Intelligent fault-tolerant (FT) computing has recently demonstrated significant advantages in predicting and diagnosing faults proactively, thereby ensuring reliable service delivery. However, due to the heterogeneity of fault knowledge, dynamic workloads, and limited data support, existing deep learning-based FT algorithms face challenges in fault detection quality and training efficiency. This is primarily because their homogenization of fault knowledge perception difficuties to fully capture diverse and complex fault patterns. To address these challenges, we propose FT-MoE, a sustainable-learning fault-tolerant computing framework based on a dual-path architecture for high-accuracy fault detection and classification. This model employs a mixture-of-experts (MoE) architecture, enabling different parameters to learn distinct fault knowledge. Additionally, we adopt a two-stage learning scheme that combines comprehensive offline training with continual online tuning, allowing the model to adaptively optimize its parameters in response to evolving real-time workloads. To facilitate realistic evaluation, we construct a new fault detection and classification dataset for edge networks, comprising 10,000 intervals with fine-grained resource features, surpassing existing datasets in both scale and granularity. Finally, we conduct extensive experiments on the FT benchmark to verify the effectiveness of FT-MoE. Results demonstrate that our model outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning</title>
<link>https://arxiv.org/abs/2505.16368</link>
<guid>https://arxiv.org/abs/2505.16368</guid>
<content:encoded><![CDATA[
arXiv:2505.16368v3 Announce Type: replace 
Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iPINNER: An Iterative Physics-Informed Neural Network with Ensemble Kalman Filter</title>
<link>https://arxiv.org/abs/2506.00731</link>
<guid>https://arxiv.org/abs/2506.00731</guid>
<content:encoded><![CDATA[
arXiv:2506.00731v2 Announce Type: replace 
Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving forward and inverse problems involving partial differential equations (PDEs) by incorporating physical laws into the training process. However, the performance of PINNs is often hindered in real-world scenarios involving noisy observational data and missing physics, particularly in inverse problems. In this work, we propose an iterative multi-objective PINN ensemble Kalman filter (iPINNER) framework that improves the robustness and accuracy of PINNs in both forward and inverse problems by using the \textit{ensemble Kalman filter} and the \textit{non-dominated sorting genetic algorithm} III (NSGA-III). Specifically, NSGA-III is used as a multi-objective optimizer that can generate various ensemble members of PINNs along the optimal Pareto front, while accounting the model uncertainty in the solution space. These ensemble members are then utilized within the EnKF to assimilate noisy observational data. The EnKF's analysis is subsequently used to refine the data loss component for retraining the PINNs, thereby iteratively updating their parameters. The iterative procedure generates improved solutions to the PDEs. The proposed method is tested on two benchmark problems: the one-dimensional viscous Burgers equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The numerical results show it outperforms standard PINNs in handling noisy data and missing physics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving</title>
<link>https://arxiv.org/abs/2506.01374</link>
<guid>https://arxiv.org/abs/2506.01374</guid>
<content:encoded><![CDATA[
arXiv:2506.01374v4 Announce Type: replace 
Abstract: While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GoalLadder: Incremental Goal Discovery with Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.16396</link>
<guid>https://arxiv.org/abs/2506.16396</guid>
<content:encoded><![CDATA[
arXiv:2506.16396v2 Announce Type: replace 
Abstract: Natural language can offer a concise and human-interpretable means of specifying reinforcement learning (RL) tasks. The ability to extract rewards from a language instruction can enable the development of robotic systems that can learn from human guidance; however, it remains a challenging problem, especially in visual environments. Existing approaches that employ large, pretrained language models either rely on non-visual environment representations, require prohibitively large amounts of feedback, or generate noisy, ill-shaped reward functions. In this paper, we propose a novel method, GoalLadder, that leverages vision-language models (VLMs) to train RL agents from a single language instruction in visual environments. GoalLadder works by incrementally discovering states that bring the agent closer to completing a task specified in natural language. To do so, it queries a VLM to identify states that represent an improvement in agent's task progress and to rank them using pairwise comparisons. Unlike prior work, GoalLadder does not trust VLM's feedback completely; instead, it uses it to rank potential goal states using an ELO-based rating system, thus reducing the detrimental effects of noisy VLM feedback. Over the course of training, the agent is tasked with minimising the distance to the top-ranked goal in a learned embedding space, which is trained on unlabelled visual data. This key feature allows us to bypass the need for abundant and accurate feedback typically required to train a well-shaped reward function. We demonstrate that GoalLadder outperforms existing related methods on classic control and robotic manipulation environments with the average final success rate of $\sim$95% compared to only $\sim$45% of the best competitor.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Koopman operator-based discussion on partial observation in stochastic systems</title>
<link>https://arxiv.org/abs/2506.21844</link>
<guid>https://arxiv.org/abs/2506.21844</guid>
<content:encoded><![CDATA[
arXiv:2506.21844v3 Announce Type: replace 
Abstract: It is sometimes difficult to achieve a complete observation for a full set of observables, and partial observations are necessary. For deterministic systems, the Mori-Zwanzig formalism provides a theoretical framework for handling partial observations. Recently, data-driven algorithms based on the Koopman operator theory have made significant progress, and there is a discussion to connect the Mori-Zwanzig formalism with the Koopman operator theory. In this work, we discuss the effects of partial observation in stochastic systems using the Koopman operator theory. The discussion clarifies the importance of distinguishing the state space and the function space in stochastic systems. Even in stochastic systems, the delay-embedding technique is beneficial for partial observation, and several numerical experiments show a power-law behavior of error with respect to the amplitude of the additive noise. We also discuss the relation between the exponent of the power-law behavior and the effects of partial observation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REDELEX: A Framework for Relational Deep Learning Exploration</title>
<link>https://arxiv.org/abs/2506.22199</link>
<guid>https://arxiv.org/abs/2506.22199</guid>
<content:encoded><![CDATA[
arXiv:2506.22199v2 Announce Type: replace 
Abstract: Relational databases (RDBs) are widely regarded as the gold standard for storing structured information. Consequently, predictive tasks leveraging this data format hold significant application promise. Recently, Relational Deep Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized as graph structures, enabling the application of various graph neural architectures to effectively address these tasks. However, given its novelty, there is a lack of analysis into the relationships between the performance of various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for evaluating RDL models of varying complexity on the most diverse collection of over 70 RDBs, which we make available to the community. Benchmarked alongside key representatives of classic methods, we confirm the generally superior performance of RDL while providing insights into the main factors shaping performance, including model complexity, database sizes and their structural properties.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Model Agent for Modular Task Execution in Drug Discovery</title>
<link>https://arxiv.org/abs/2507.02925</link>
<guid>https://arxiv.org/abs/2507.02925</guid>
<content:encoded><![CDATA[
arXiv:2507.02925v3 Announce Type: replace 
Abstract: We present a modular framework powered by large language models (LLMs) that automates and streamlines key tasks across the early-stage computational drug discovery pipeline. By combining LLM reasoning with domain-specific tools, the framework performs biomedical data retrieval, literature-grounded question answering via retrieval-augmented generation, molecular generation, multi-property prediction, property-aware molecular refinement, and 3D protein-ligand structure generation. The agent autonomously retrieved relevant biomolecular information, including FASTA sequences, SMILES representations, and literature, and answered mechanistic questions with improved contextual accuracy compared to standard LLMs. It then generated chemically diverse seed molecules and predicted 75 properties, including ADMET-related and general physicochemical descriptors, which guided iterative molecular refinement. Across two refinement rounds, the number of molecules with QED > 0.6 increased from 34 to 55. The number of molecules satisfying empirical drug-likeness filters also rose; for example, compliance with the Ghose filter increased from 32 to 55 within a pool of 100 molecules. The framework also employed Boltz-2 to generate 3D protein-ligand complexes and provide rapid binding affinity estimates for candidate compounds. These results demonstrate that the approach effectively supports molecular screening, prioritization, and structure evaluation. Its modular design enables flexible integration of evolving tools and models, providing a scalable foundation for AI-assisted therapeutic discovery.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation</title>
<link>https://arxiv.org/abs/2507.05578</link>
<guid>https://arxiv.org/abs/2507.05578</guid>
<content:encoded><![CDATA[
arXiv:2507.05578v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they also exhibit memorization of their training data. This phenomenon raises critical questions about model behavior, privacy risks, and the boundary between learning and memorization. Addressing these concerns, this paper synthesizes recent studies and investigates the landscape of memorization, the factors influencing it, and methods for its detection and mitigation. We explore key drivers, including training data duplication, training dynamics, and fine-tuning procedures that influence data memorization. In addition, we examine methodologies such as prefix-based extraction, membership inference, and adversarial prompting, assessing their effectiveness in detecting and measuring memorized content. Beyond technical analysis, we also explore the broader implications of memorization, including the legal and ethical implications. Finally, we discuss mitigation strategies, including data cleaning, differential privacy, and post-training unlearning, while highlighting open challenges in balancing the need to minimize harmful memorization with model utility. This paper provides a comprehensive overview of the current state of research on LLM memorization across technical, privacy, and performance dimensions, identifying critical directions for future work.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class-wise Balancing Data Replay for Federated Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2507.07712</link>
<guid>https://arxiv.org/abs/2507.07712</guid>
<content:encoded><![CDATA[
arXiv:2507.07712v4 Announce Type: replace 
Abstract: Federated Class Incremental Learning (FCIL) aims to collaboratively process continuously increasing incoming tasks across multiple clients. Among various approaches, data replay has become a promising solution, which can alleviate forgetting by reintroducing representative samples from previous tasks. However, their performance is typically limited by class imbalance, both within the replay buffer due to limited global awareness and between replayed and newly arrived classes. To address this issue, we propose a class wise balancing data replay method for FCIL (FedCBDR), which employs a global coordination mechanism for class-level memory construction and reweights the learning objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has two key components: 1) the global-perspective data replay module reconstructs global representations of prior task in a privacy-preserving manner, which then guides a class-aware and importance-sensitive sampling strategy to achieve balanced replay; 2) Subsequently, to handle class imbalance across tasks, the task aware temperature scaling module adaptively adjusts the temperature of logits at both class and instance levels based on task dynamics, which reduces the model's overconfidence in majority classes while enhancing its sensitivity to minority classes. Experimental results verified that FedCBDR achieves balanced class-wise sampling under heterogeneous data distributions and improves generalization under task imbalance between earlier and recent tasks, yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces</title>
<link>https://arxiv.org/abs/2507.13092</link>
<guid>https://arxiv.org/abs/2507.13092</guid>
<content:encoded><![CDATA[
arXiv:2507.13092v2 Announce Type: replace 
Abstract: Electroencephalography (EEG) is a fundamental modality for cognitive state monitoring in brain-computer interfaces (BCIs). However, it is highly susceptible to intrinsic signal errors and human-induced labeling errors, which lead to label noise and ultimately degrade model performance. To enhance EEG learning, multimodal knowledge distillation (KD) has been explored to transfer knowledge from visual models with rich representations to EEG-based models. Nevertheless, KD faces two key challenges: modality gap and soft label misalignment. The former arises from the heterogeneous nature of EEG and visual feature spaces, while the latter stems from label inconsistencies that create discrepancies between ground truth labels and distillation targets. This paper addresses semantic uncertainty caused by ambiguous features and weakly defined labels. We propose a novel cross-modal knowledge distillation framework that mitigates both modality and label inconsistencies. It aligns feature semantics through a prototype-based similarity module and introduces a task-specific distillation head to resolve label-induced inconsistency in supervision. Experimental results demonstrate that our approach improves EEG-based emotion regression and classification performance, outperforming both unimodal and multimodal baselines on a public multimodal dataset. These findings highlight the potential of our framework for BCI applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing</title>
<link>https://arxiv.org/abs/2508.18316</link>
<guid>https://arxiv.org/abs/2508.18316</guid>
<content:encoded><![CDATA[
arXiv:2508.18316v3 Announce Type: replace 
Abstract: This study proposes and validates a Federated Learning (FL) framework to proactively identify at-risk students while preserving data privacy. Persistently high dropout rates in distance education remain a pressing institutional challenge. Using the large-scale OULAD dataset, we simulate a privacy-centric scenario where models are trained on early academic performance and digital engagement patterns. Our work investigates the practical trade-offs between model complexity (Logistic Regression vs. a Deep Neural Network) and the impact of local data balancing. The resulting federated model achieves strong predictive power (ROC AUC approximately 85%), demonstrating that FL is a practical and scalable solution for early-warning systems that inherently respects student data sovereignty.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation</title>
<link>https://arxiv.org/abs/2509.19112</link>
<guid>https://arxiv.org/abs/2509.19112</guid>
<content:encoded><![CDATA[
arXiv:2509.19112v3 Announce Type: replace 
Abstract: Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance</title>
<link>https://arxiv.org/abs/2510.02630</link>
<guid>https://arxiv.org/abs/2510.02630</guid>
<content:encoded><![CDATA[
arXiv:2510.02630v2 Announce Type: replace 
Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), has emerged as a promising approach to fine-tuning large language models(LLMs) while reducing computational and memory overhead. However, LoRA assumes a uniform rank \textit{r} for each incremental matrix, not accounting for the varying significance of weight matrices across different modules and layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize updates and employs pruning of singular values to introduce dynamic rank allocation, thereby enhancing adaptability. However, during the training process, it often encounters issues of slow convergence speed and high computational overhead. To address this issue, we propose HyperAdaLoRA, a novel framework that accelerates the convergence of AdaLoRA by leveraging a hypernetwork. Instead of directly optimizing the components of Singular Value Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on attention mechanisms to dynamically generate these parameters. By pruning the outputs of the hypernetwork that generates the singular values, dynamic rank allocation is achieved. Comprehensive experiments on various datasets and models demonstrate that our method achieves faster convergence without sacrificing performance. Additionally, further extension experiments on other LoRA-based approaches validate the broad applicability of our method.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning</title>
<link>https://arxiv.org/abs/2510.04573</link>
<guid>https://arxiv.org/abs/2510.04573</guid>
<content:encoded><![CDATA[
arXiv:2510.04573v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFCA: Decentralized Federated Clustering Algorithm</title>
<link>https://arxiv.org/abs/2510.15300</link>
<guid>https://arxiv.org/abs/2510.15300</guid>
<content:encoded><![CDATA[
arXiv:2510.15300v2 Announce Type: replace 
Abstract: Clustered Federated Learning has emerged as an effective approach for handling heterogeneous data across clients by partitioning them into clusters with similar or identical data distributions. However, most existing methods, including the Iterative Federated Clustering Algorithm (IFCA), rely on a central server to coordinate model updates, which creates a bottleneck and a single point of failure, limiting their applicability in more realistic decentralized learning settings. In this work, we introduce DFCA, a fully decentralized clustered FL algorithm that enables clients to collaboratively train cluster-specific models without central coordination. DFCA uses a sequential running average to aggregate models from neighbors as updates arrive, providing a communication-efficient alternative to batch aggregation while maintaining clustering performance. Our experiments on various datasets demonstrate that DFCA outperforms other decentralized algorithms and performs comparably to centralized IFCA, even under sparse connectivity, highlighting its robustness and practicality for dynamic real-world decentralized networks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Than Memory Savings: Zeroth-Order Optimization Mitigates Forgetting in Continual Learning</title>
<link>https://arxiv.org/abs/2510.21019</link>
<guid>https://arxiv.org/abs/2510.21019</guid>
<content:encoded><![CDATA[
arXiv:2510.21019v2 Announce Type: replace 
Abstract: Zeroth-order (ZO) optimization has gained attention as a memory-efficient alternative to first-order (FO) methods, particularly in settings where gradient computation is expensive or even impractical. Beyond its memory efficiency, in this work, we investigate ZO optimization for continual learning (CL) as a novel approach to address the plasticity-stability-efficiency trilemma. Through theoretical analysis and empirical evidence, we show that ZO optimization naturally leads to flatter loss landscapes, which in turn reduce forgetting in CL. However, this stability comes at a cost of plasticity: due to its imprecise gradient estimates and slower convergence, ZO optimization tends to be less effective than FO in acquiring new task-specific knowledge, particularly under constrained training budgets. To better understand this trade-off, we conduct a holistic evaluation of ZO optimization applied to various existing CL methods. Our findings reveal that ZO optimization enhances stability but often undermines plasticity, particularly when used with learnable classifiers. Motivated by this insight, we propose ZO-FC, a simple but effective approach that applies ZO optimization to a single adapter-based PEFT module with FO optimized classifier. This design leverages the stability benefits of ZO while preserving the adaptability of FO updates with negligible memory overhead. Experiments demonstrate that ZO-FC achieves an effective balance between stability and plasticity, offering a practical and memory-efficient solution for on-device CL.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Model for Multi-Task Drone Routing in Post-Disaster Road Assessment</title>
<link>https://arxiv.org/abs/2510.21525</link>
<guid>https://arxiv.org/abs/2510.21525</guid>
<content:encoded><![CDATA[
arXiv:2510.21525v2 Announce Type: replace 
Abstract: Post-disaster road assessment (PDRA) is essential for emergency response, enabling rapid evaluation of infrastructure conditions and efficient allocation of resources. Although drones provide a flexible and effective tool for PDRA, routing them in large-scale networks remains challenging. Exact and heuristic optimization methods scale poorly and demand domain expertise, while existing deep reinforcement learning (DRL) approaches adopt a single-task paradigm, requiring separate models for each problem variant and lacking adaptability to evolving operational needs. This study proposes a unified model (UM) for drone routing that simultaneously addresses eight PDRA variants. By training a single neural network across multiple problem configurations, UM captures shared structural knowledge while adapting to variant-specific constraints through a modern transformer encoder-decoder architecture. A lightweight adapter mechanism further enables efficient finetuning to unseen attributes without retraining, enhancing deployment flexibility in dynamic disaster scenarios. Extensive experiments demonstrate that the UM reduces training time and parameters by a factor of eight compared with training separate models, while consistently outperforming single-task DRL methods by 6-14%, heuristic algorithms by 22-42%, and commercial solvers by 24-82% in terms of solution quality (total collected information value). The model achieves rapid solutions (1-10 seconds) across networks of up to 1,000 nodes, with robustness confirmed through sensitivity analyses. Moreover, finetuning experiments show that unseen attributes can be effectively incorporated with minimal cost while retaining high solution quality. The source code for UM is publicly available at https://github.com/PJ-HTU/UM_PDRA.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Muon's Spectral Design Benefits Generalization: A Study on Imbalanced Data</title>
<link>https://arxiv.org/abs/2510.22980</link>
<guid>https://arxiv.org/abs/2510.22980</guid>
<content:encoded><![CDATA[
arXiv:2510.22980v3 Announce Type: replace 
Abstract: The growing adoption of spectrum-aware matrix-valued optimizers such as Muon and Shampoo in deep learning motivates a systematic study of their generalization properties and, in particular, when they might outperform competitive algorithms. We approach this question by introducing appropriate simplifying abstractions as follows: First, we use imbalanced data as a testbed. Second, we study the canonical form of such optimizers, which is Spectral Gradient Descent (SpecGD) -- each update step is $UV^T$ where $U\Sigma V^T$ is the truncated SVD of the gradient. Third, within this framework we identify a canonical setting for which we precisely quantify when SpecGD outperforms vanilla Euclidean GD. For a Gaussian mixture data model and both linear and bilinear models, we show that unlike GD, which prioritizes learning dominant principal components of the data first, SpecGD learns all principal components of the data at equal rates. We demonstrate how this translates to a growing gap in class balanced loss favoring SpecGD early in training and further show that the gap remains consistent even when the GD counterpart uses adaptive step-sizes via normalization. By extending the analysis to deep linear models, we show that depth amplifies these effects. We empirically verify our theoretical findings on a variety of imbalanced datasets. Our experiments compare practical variants of spectral methods, like Muon and Shampoo, against their Euclidean counterparts and Adam. The results validate our findings that these spectral optimizers achieve superior generalization by promoting a more balanced learning of the data's underlying components.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering</title>
<link>https://arxiv.org/abs/2510.26898</link>
<guid>https://arxiv.org/abs/2510.26898</guid>
<content:encoded><![CDATA[
arXiv:2510.26898v2 Announce Type: replace 
Abstract: This work presents an ontology-integrated large language model (LLM) framework for chemical engineering that unites structured domain knowledge with generative reasoning. The proposed pipeline aligns model training and inference with the COPE ontology through a sequence of data acquisition, semantic preprocessing, information extraction, and ontology mapping steps, producing templated question-answer pairs that guide fine-tuning. A control-focused decoding stage and citation gate enforce syntactic and factual grounding by constraining outputs to ontology-linked terms, while evaluation metrics quantify both linguistic quality and ontological accuracy. Feedback and future extensions, including semantic retrieval and iterative validation, further enhance the system's interpretability and reliability. This integration of symbolic structure and neural generation provides a transparent, auditable approach for applying LLMs to process control, safety analysis, and other critical engineering contexts.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Bayesian Model for Gene Deconvolution and Functional Analysis in Human Endometrium Across the Menstrual Cycle</title>
<link>https://arxiv.org/abs/2510.27097</link>
<guid>https://arxiv.org/abs/2510.27097</guid>
<content:encoded><![CDATA[
arXiv:2510.27097v2 Announce Type: replace 
Abstract: Bulk tissue RNA sequencing of heterogeneous samples provides averaged gene expression profiles, obscuring cell type-specific dynamics. To address this, we present a probabilistic hierarchical Bayesian model that deconvolves bulk RNA-seq data into constituent cell-type expression profiles and proportions, leveraging a high-resolution single-cell reference. We apply our model to human endometrial tissue across the menstrual cycle, a context characterized by dramatic hormone-driven cellular composition changes. Our extended framework provides a principled inference of cell type proportions and cell-specific gene expression changes across cycle phases. We demonstrate the model's structure, priors, and inference strategy in detail, and we validate its performance with simulations and comparisons to existing methods. The results reveal dynamic shifts in epithelial, stromal, and immune cell fractions between menstrual phases, and identify cell-type-specific differential gene expression associated with endometrial function (e.g., decidualization markers in stromal cells during the secretory phase). We further conduct robustness tests and show that our Bayesian approach is resilient to reference mismatches and noise. Finally, we discuss the biological significance of our findings, potential clinical implications for fertility and endometrial disorders, and future directions, including integration of spatial transcriptomics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensitivity Analysis for Climate Science with Generative Flow Models</title>
<link>https://arxiv.org/abs/2511.00663</link>
<guid>https://arxiv.org/abs/2511.00663</guid>
<content:encoded><![CDATA[
arXiv:2511.00663v3 Announce Type: replace 
Abstract: Sensitivity analysis is a cornerstone of climate science, essential for understanding phenomena ranging from storm intensity to long-term climate feedbacks. However, computing these sensitivities using traditional physical models is often prohibitively expensive in terms of both computation and development time. While modern AI-based generative models are orders of magnitude faster to evaluate, computing sensitivities with them remains a significant bottleneck. This work addresses this challenge by applying the adjoint state method for calculating gradients in generative flow models. We apply this method to the cBottle generative model, trained on ERA5 and ICON data, to perform sensitivity analysis of any atmospheric variable with respect to sea surface temperatures. We quantitatively validate the computed sensitivities against the model's own outputs. Our results provide initial evidence that this approach can produce reliable gradients, reducing the computational cost of sensitivity analysis from weeks on a supercomputer with a physical model to hours on a GPU, thereby simplifying a critical workflow in climate science. The code can be found at https://github.com/Kwartzl8/cbottle_adjoint_sensitivity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games</title>
<link>https://arxiv.org/abs/2511.00811</link>
<guid>https://arxiv.org/abs/2511.00811</guid>
<content:encoded><![CDATA[
arXiv:2511.00811v2 Announce Type: replace 
Abstract: Equilibrium learning in adversarial games is an important topic widely examined in the fields of game theory and reinforcement learning (RL). Pursuit-evasion game (PEG), as an important class of real-world games from the fields of robotics and security, requires exponential time to be accurately solved. When the underlying graph structure varies, even the state-of-the-art RL methods require recomputation or at least fine-tuning, which can be time-consuming and impair real-time applicability. This paper proposes an Equilibrium Policy Generalization (EPG) framework to effectively learn a generalized policy with robust cross-graph zero-shot performance. In the context of PEGs, our framework is generally applicable to both pursuer and evader sides in both no-exit and multi-exit scenarios. These two generalizability properties, to our knowledge, are the first to appear in this domain. The core idea of the EPG framework is to train an RL policy across different graph structures against the equilibrium policy for each single graph. To construct an equilibrium oracle for single-graph policies, we present a dynamic programming (DP) algorithm that provably generates pure-strategy Nash equilibrium with near-optimal time complexity. To guarantee scalability with respect to pursuer number, we further extend DP and RL by designing a grouping mechanism and a sequence model for joint policy decomposition, respectively. Experimental results show that, using equilibrium guidance and a distance feature proposed for cross-graph PEG training, the EPG framework guarantees desirable zero-shot performance in various unseen real-world graphs. Besides, when trained under an equilibrium heuristic proposed for the graphs with exits, our generalized pursuer policy can even match the performance of the fine-tuned policies from the state-of-the-art PEG methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaberNet: Causal Representation Learning for Cross-Domain HVAC Energy Prediction</title>
<link>https://arxiv.org/abs/2511.06634</link>
<guid>https://arxiv.org/abs/2511.06634</guid>
<content:encoded><![CDATA[
arXiv:2511.06634v3 Announce Type: replace 
Abstract: Cross-domain HVAC energy prediction is essential for scalable building energy management, particularly because collecting extensive labeled data for every new building is both costly and impractical. Yet, this task remains highly challenging due to the scarcity and heterogeneity of data across different buildings, climate zones, and seasonal patterns. In particular, buildings situated in distinct climatic regions introduce variability that often leads existing methods to overfit to spurious correlations, rely heavily on expert intervention, or compromise on data diversity. To address these limitations, we propose CaberNet, a causal and interpretable deep sequence model that learns invariant (Markov blanket) representations for robust cross-domain prediction. In a purely data-driven fashion and without requiring any prior knowledge, CaberNet integrates i) a global feature gate trained with a self-supervised Bernoulli regularization to distinguish superior causal features from inferior ones, and ii) a domain-wise training scheme that balances domain contributions, minimizes cross-domain loss variance, and promotes latent factor independence. We evaluate CaberNet on real-world datasets collected from three buildings located in three climatically diverse cities, and it consistently outperforms all baselines, achieving a 22.9% reduction in normalized mean squared error (NMSE) compared to the best benchmark. Our code is available at https://github.com/SusCom-Lab/CaberNet-CRL.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Methodological Precedence in Health Tech: Why ML/Big Data Analysis Must Follow Basic Epidemiological Consistency. A Case Study</title>
<link>https://arxiv.org/abs/2511.07500</link>
<guid>https://arxiv.org/abs/2511.07500</guid>
<content:encoded><![CDATA[
arXiv:2511.07500v2 Announce Type: replace 
Abstract: The integration of advanced analytical tools, including Machine Learning (ML) and massive data processing, has revolutionized health research, promising unprecedented accuracy in diagnosis and risk prediction. However, the rigor of these complex methods is fundamentally dependent on the quality and integrity of the underlying datasets and the validity of their statistical design. We propose an emblematic case where advanced analysis (ML/Big Data) must necessarily be subsequent to the verification of basic methodological coherence and adherence to established medical protocols, such as the STROBE Statement. This study highlights a crucial cautionary principle: sophisticated analyses amplify, rather than correct, severe methodological flaws rooted in basic design choices, leading to misleading or contradictory findings. By applying simple, standard descriptive statistical methods and established national epidemiological benchmarks to a recently published cohort study on COVID-19 vaccine outcomes and severe adverse events, like cancer, we expose multiple, statistically irreconcilable paradoxes. These paradoxes, specifically the contradictory finding of an increased cancer incidence within an exposure subgroup, concurrent with a suppressed overall Crude Incidence Rate compared to national standards, definitively invalidate the reported risk of increased cancer in the total population. We demonstrate that the observed effects are mathematical artifacts stemming from an uncorrected selection bias in the cohort construction. This analysis serves as a robust reminder that even the most complex health studies must first pass the test of basic epidemiological consistency before any conclusion drawn from subsequent advanced statistical modeling can be considered valid or publishable.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10843</link>
<guid>https://arxiv.org/abs/2511.10843</guid>
<content:encoded><![CDATA[
arXiv:2511.10843v2 Announce Type: replace 
Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference</title>
<link>https://arxiv.org/abs/2511.13018</link>
<guid>https://arxiv.org/abs/2511.13018</guid>
<content:encoded><![CDATA[
arXiv:2511.13018v2 Announce Type: replace 
Abstract: The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic "representation bottleneck": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent "nuisance bottleneck," linking it to GNN over-squashing via a targeted "Hub-Periphery Trade-off" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical "final-stage bottleneck."
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect</title>
<link>https://arxiv.org/abs/2511.14317</link>
<guid>https://arxiv.org/abs/2511.14317</guid>
<content:encoded><![CDATA[
arXiv:2511.14317v4 Announce Type: replace 
Abstract: In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Variance-Based Analysis of Sample Complexity for Grid Coverage</title>
<link>https://arxiv.org/abs/2511.17784</link>
<guid>https://arxiv.org/abs/2511.17784</guid>
<content:encoded><![CDATA[
arXiv:2511.17784v2 Announce Type: replace 
Abstract: Verifying uniform conditions over continuous spaces through random sampling is fundamental in machine learning and control theory, yet classical coverage analyses often yield conservative bounds, particularly at small failure probabilities. We study uniform random sampling on the $d$-dimensional unit hypercube and analyze the number of uncovered subcubes after discretization. By applying a concentration inequality to the uncovered-count statistic, we derive a sample complexity bound with a logarithmic dependence on the failure probability ($\delta$), i.e., $M =O( \tilde{C}\ln(\frac{2\tilde{C}}{\delta}))$, which contrasts sharply with the classical linear $1/\delta$ dependence. Under standard Lipschitz and uniformity assumptions, we present a self-contained derivation and compare our result with classical coupon-collector rates. Numerical studies across dimensions, precision levels, and confidence targets indicate that our bound tracks practical coverage requirements more tightly and scales favorably as $\delta \to 0$. Our findings offer a sharper theoretical tool for algorithms that rely on grid-based coverage guarantees, enabling more efficient sampling, especially in high-confidence regimes.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings</title>
<link>https://arxiv.org/abs/2511.19279</link>
<guid>https://arxiv.org/abs/2511.19279</guid>
<content:encoded><![CDATA[
arXiv:2511.19279v2 Announce Type: replace 
Abstract: A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion for Fusion: Designing Stellarators with Generative AI</title>
<link>https://arxiv.org/abs/2511.20445</link>
<guid>https://arxiv.org/abs/2511.20445</guid>
<content:encoded><![CDATA[
arXiv:2511.20445v2 Announce Type: replace 
Abstract: Stellarators are a prospective class of fusion-based power plants that confine a hot plasma with three-dimensional magnetic fields. Typically framed as a PDE-constrained optimization problem, stellarator design is a time-consuming process that can take hours to solve on a computing cluster. Developing fast methods for designing stellarators is crucial for advancing fusion research. Given the recent development of large datasets of optimized stellarators, machine learning approaches have emerged as a potential candidate. Motivated by this, we present an open inverse problem to the machine learning community: to rapidly generate high-quality stellarator designs which have a set of desirable characteristics. As a case study in the problem space, we train a conditional diffusion model on data from the QUASR database to generate quasisymmetric stellarator designs with desirable characteristics (aspect ratio and mean rotational transform). The diffusion model is applied to design stellarators with characteristics not seen during training. We provide evaluation protocols and show that many of the generated stellarators exhibit solid performance: less than 5% deviation from quasisymmetry and the target characteristics. The modest deviation from quasisymmetry highlights an opportunity to reach the sub 1% target. Beyond the case study, we share multiple promising avenues for generative modeling to advance stellarator design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Signed Graph Learning with Differential Privacy</title>
<link>https://arxiv.org/abs/2512.00307</link>
<guid>https://arxiv.org/abs/2512.00307</guid>
<content:encoded><![CDATA[
arXiv:2512.00307v2 Announce Type: replace 
Abstract: Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement</title>
<link>https://arxiv.org/abs/2512.00396</link>
<guid>https://arxiv.org/abs/2512.00396</guid>
<content:encoded><![CDATA[
arXiv:2512.00396v2 Announce Type: replace 
Abstract: We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs</title>
<link>https://arxiv.org/abs/2512.00862</link>
<guid>https://arxiv.org/abs/2512.00862</guid>
<content:encoded><![CDATA[
arXiv:2512.00862v3 Announce Type: replace 
Abstract: We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$ on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: https://github.com/Yeyke/HBLLM.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.02551</link>
<guid>https://arxiv.org/abs/2512.02551</guid>
<content:encoded><![CDATA[
arXiv:2512.02551v2 Announce Type: replace 
Abstract: In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used torch.matmul to state-of-the-art Nvidia's closed-source libraries, i.e., cuBLAS, cuBLASLt. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0% over torch.matmul on average; +19.2% over cuBLAS using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8% over cuBLASLt-heuristic, which queries cuBLASLt library and selects the algorithm based on the heuristic's suggestion; and +11.4% over the most competitive cuBLASLt-AutoTuning model, which selects the fastest algorithm from up to 100 candidates from cuBLASLt's suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7%, +26.0%, +22.4%, and +15.9% for torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics</title>
<link>https://arxiv.org/abs/2512.03290</link>
<guid>https://arxiv.org/abs/2512.03290</guid>
<content:encoded><![CDATA[
arXiv:2512.03290v2 Announce Type: replace 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting</title>
<link>https://arxiv.org/abs/2512.04752</link>
<guid>https://arxiv.org/abs/2512.04752</guid>
<content:encoded><![CDATA[
arXiv:2512.04752v2 Announce Type: replace 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is an important fine-tuning technique for large language models (LLMs) and comprises three stages: generation, inference, and training. The generation stage generates samples that are then used to infer learnable experiences for training. We observe that the generation stage is the bottleneck of the entire execution process and consider it a key point for optimization. Specifically, we realize the first attempt to integrate speculative decoding into the RLHF generation stage and propose RLHFSpec, an RLHF system that accelerates generation execution with efficient speculative decoding and sample reallocation. To fully exploit the performance potential provided by speculative decoding, especially dealing with the dynamic workload of the generation stage, RLHFSpec proposes a workload-aware drafting strategy selection mechanism, which selects the near-optimal strategy by jointly considering the verification cost and the number of accepted tokens. Moreover, RLHFSpec also proposes sample reallocation to fully utilize the GPU resources, and optimizes it with an efficient sample migration mechanism. The experimental results show that the RLHFSpec can achieve higher throughput in the generation stage compared to state-of-the-art works. Moreover, due to the effective alleviation of the generation bottleneck, RLHFSpec also shows significant performance speedup in the entire RLHF execution.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism</title>
<link>https://arxiv.org/abs/2303.13775</link>
<guid>https://arxiv.org/abs/2303.13775</guid>
<content:encoded><![CDATA[
arXiv:2303.13775v3 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs), an emerging class of machine learning models for graphs, have gained popularity for their superior performance in various graph analytical tasks. Mini-batch training is commonly used to train GNNs on large graphs, and data parallelism is the standard approach to scale mini-batch training across multiple GPUs. Data parallel approaches contain redundant work as subgraphs sampled by different GPUs contain significant overlap. To address this issue, we introduce a hybrid parallel mini-batch training paradigm called split parallelism. Split parallelism avoids redundant work by splitting the sampling, loading, and training of each mini-batch across multiple GPUs. Split parallelism, however, introduces communication overheads that can be more than the savings from removing redundant work. We further present a lightweight partitioning algorithm that probabilistically minimizes these overheads. We implement split parallelism in GSplit and show that it outperforms state-of-the-art mini-batch training systems like DGL, Quiver, and $P^3$.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DoDo-Code: an Efficient Levenshtein Distance Embedding-based Code for 4-ary IDS Channel</title>
<link>https://arxiv.org/abs/2312.12717</link>
<guid>https://arxiv.org/abs/2312.12717</guid>
<content:encoded><![CDATA[
arXiv:2312.12717v3 Announce Type: replace-cross 
Abstract: With the emergence of new storage and communication methods, the insertion, deletion, and substitution (IDS) channel has attracted considerable attention. However, many topics on the IDS channel and the associated Levenshtein distance remain open, making the invention of a novel IDS-correcting code a hard task. Furthermore, current studies on single-IDS-correcting code misalign with the requirements of applications which necessitates the correcting of multiple errors. Compromise solutions have involved shortening codewords to reduce the chance of multiple errors. However, the code rates of existing codes are poor at short lengths, diminishing the overall storage density. In this study, a novel method is introduced for designing high-code-rate single-IDS-correcting codewords through deep Levenshtein distance embedding. A deep learning model is utilized to project the sequences into embedding vectors that preserve the Levenshtein distances between the original sequences. This embedding space serves as a proxy for the complex Levenshtein domain, within which algorithms for codeword search and segment correcting is developed. While the concept underpinning this approach is straightforward, it bypasses the mathematical challenges typically encountered in code design. The proposed method results in a code rate that outperforms existing combinatorial solutions, particularly for designing short-length codewords.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation</title>
<link>https://arxiv.org/abs/2402.03216</link>
<guid>https://arxiv.org/abs/2402.03216</guid>
<content:encoded><![CDATA[
arXiv:2402.03216v5 Announce Type: replace-cross 
Abstract: In this paper, we introduce a new embedding model called M3-Embedding, which is distinguished for its versatility in \textit{Multi-Linguality}, \textit{Multi-Functionality}, and \textit{Multi-Granularity}. It provides a uniform support for the semantic retrieval of more than 100 working languages. It can simultaneously accomplish the three common retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding presents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, which enables a large batch size and high training throughput to improve the discriminativeness of embeddings. M3-Embedding exhibits a superior performance in our experiment, leading to new state-of-the-art results on multilingual, cross-lingual, and long-document retrieval benchmarks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Expressive Capacity of State Space Models: A Formal Language Perspective</title>
<link>https://arxiv.org/abs/2405.17394</link>
<guid>https://arxiv.org/abs/2405.17394</guid>
<content:encoded><![CDATA[
arXiv:2405.17394v3 Announce Type: replace-cross 
Abstract: Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grammar-Aligned Decoding</title>
<link>https://arxiv.org/abs/2405.21047</link>
<guid>https://arxiv.org/abs/2405.21047</guid>
<content:encoded><![CDATA[
arXiv:2405.21047v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper, we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperSBINN: A Hypernetwork-Enhanced Systems Biology-Informed Neural Network for Efficient Drug Cardiosafety Assessment</title>
<link>https://arxiv.org/abs/2408.14266</link>
<guid>https://arxiv.org/abs/2408.14266</guid>
<content:encoded><![CDATA[
arXiv:2408.14266v2 Announce Type: replace-cross 
Abstract: Mathematical modeling in systems toxicology enables a comprehensive understanding of the effects of pharmaceutical substances on cardiac health. However, the complexity of these models limits their widespread application in early drug discovery. In this paper, we introduce a novel approach to solving parameterized models of cardiac action potentials by combining meta-learning techniques with Systems Biology-Informed Neural Networks (SBINNs). The proposed method, hyperSBINN, effectively addresses the challenge of predicting the effects of various compounds at different concentrations on cardiac action potentials, outperforming traditional differential equation solvers in speed. Our model efficiently handles scenarios with limited data and complex parameterized differential equations. The hyperSBINN model demonstrates robust performance in predicting APD90 values, indicating its potential as a reliable tool for modeling cardiac electrophysiology and aiding in preclinical drug development. This framework represents an advancement in computational modeling, offering a scalable and efficient solution for simulating and understanding complex biological systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assumption-Lean Post-Integrated Inference with Surrogate Control Outcomes</title>
<link>https://arxiv.org/abs/2410.04996</link>
<guid>https://arxiv.org/abs/2410.04996</guid>
<content:encoded><![CDATA[
arXiv:2410.04996v5 Announce Type: replace-cross 
Abstract: Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference method that accounts for latent heterogeneity by utilizing control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects using negative control outcomes. By utilizing surrogate control outcomes as an extension of negative control outcomes, we develop semiparametric inference on projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated using random forests through simulations and analysis of single-cell CRISPR perturbed datasets, which may contain potential unmeasured confounders.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Shuffle or not to Shuffle: Auditing DP-SGD with Shuffling</title>
<link>https://arxiv.org/abs/2411.10614</link>
<guid>https://arxiv.org/abs/2411.10614</guid>
<content:encoded><![CDATA[
arXiv:2411.10614v3 Announce Type: replace-cross 
Abstract: The Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm supports the training of machine learning (ML) models with formal Differential Privacy (DP) guarantees. Traditionally, DP-SGD processes training data in batches using Poisson subsampling to select each batch at every iteration. More recently, shuffling has become a common alternative due to its better compatibility and lower computational overhead. However, computing tight theoretical DP guarantees under shuffling remains an open problem. As a result, models trained with shuffling are often evaluated as if Poisson subsampling were used, which might result in incorrect privacy guarantees.
  This raises a compelling research question: can we verify whether there are gaps between the theoretical DP guarantees reported by state-of-the-art models using shuffling and their actual leakage? To do so, we define novel DP-auditing procedures to analyze DP-SGD with shuffling and measure their ability to tightly estimate privacy leakage vis-\`a-vis batch sizes, privacy budgets, and threat models. Overall, we demonstrate that DP models trained using this approach have considerably overestimated their privacy guarantees (by up to 4 times). However, we also find that the gap between the theoretical Poisson DP guarantees and the actual privacy leakage from shuffling is not uniform across all parameter settings and threat models. Finally, we study two common variations of the shuffling procedure that result in even further privacy leakage (up to 10 times). Overall, our work highlights the risk of using shuffling instead of Poisson subsampling in the absence of rigorous analysis methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERITAS: Verifying the Performance of AI-native Transceiver Actions in Base-Stations</title>
<link>https://arxiv.org/abs/2501.09761</link>
<guid>https://arxiv.org/abs/2501.09761</guid>
<content:encoded><![CDATA[
arXiv:2501.09761v3 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI)-native receivers prove significant performance improvement in high noise regimes and can potentially reduce communication overhead compared to the traditional receiver. However, their performance highly depends on the representativeness of the training dataset. A major issue is the uncertainty of whether the training dataset covers all test environments and waveform configurations, and thus, whether the trained model is robust in practical deployment conditions. To this end, we propose a joint measurement-recovery framework for AI-native transceivers post deployment, called VERITAS, that continuously looks for distribution shifts in the received signals and triggers finite re-training spurts. VERITAS monitors the wireless channel using 5G pilots fed to an auxiliary neural network that detects out-of-distribution channel profile, transmitter speed, and delay spread. As soon as such a change is detected, a traditional (reference) receiver is activated, which runs for a period of time in parallel to the AI-native receiver. Finally, VERTIAS compares the bit probabilities of the AI-native and the reference receivers for the same received data inputs, and decides whether or not a retraining process needs to be initiated. Our evaluations reveal that VERITAS can detect changes in the channel profile, transmitter speed, and delay spread with 99%, 97%, and 69% accuracies, respectively, followed by timely initiation of retraining for 86%, 93.3%, and 94.8% of inputs in channel profile, transmitter speed, and delay spread test sets, respectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Best-of-N Selection for Large Language Models via Self-Certainty</title>
<link>https://arxiv.org/abs/2502.18581</link>
<guid>https://arxiv.org/abs/2502.18581</guid>
<content:encoded><![CDATA[
arXiv:2502.18581v3 Announce Type: replace-cross 
Abstract: Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size N, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Full-Precision and Ternarised Neural Networks with Tunnel-Diode Activation Functions: Computing and Physics Perspectives</title>
<link>https://arxiv.org/abs/2503.04978</link>
<guid>https://arxiv.org/abs/2503.04978</guid>
<content:encoded><![CDATA[
arXiv:2503.04978v2 Announce Type: replace-cross 
Abstract: The mathematical complexity and high dimensionality of neural networks slow both training and deployment, demanding heavy computational resources. This has driven the search for alternative architectures built from novel components, including new activation functions. Taking a different approach from state-of-the-art neural and neuromorphic computational systems, we employ the current-voltage characteristic of a tunnel diode as a quantum physics-based activation function for deep networks. This tunnel-diode activation function (TDAF) outperforms standard activations in deep architectures, delivering lower loss and higher accuracy in both training and evaluation. We also highlight its promise for implementation in electronic hardware aimed at neuromorphic, ternarised and energy efficient AI systems. Speaking broadly, our work lays a solid foundation for a new bridge between machine learning, semiconductor electronics and quantum physics -- bringing together quantum tunnelling, a phenomenon recognised in six Nobel Prizes (including the 2025 award), and contemporary AI research.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits</title>
<link>https://arxiv.org/abs/2503.07020</link>
<guid>https://arxiv.org/abs/2503.07020</guid>
<content:encoded><![CDATA[
arXiv:2503.07020v2 Announce Type: replace-cross 
Abstract: Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Existing protocols typically default to entirely risk-avoidant actions such as immediate stops, which are detrimental to navigation goals and lack flexibility for rare driving scenarios. Yet, in cases of minor risk, halting the vehicle may be unnecessary, and more adaptive responses are preferable. In this paper, we propose LLM-RCO, a risk-averse framework leveraging large language models (LLMs) to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules interacting with the dynamic driving environment: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator, enabling proactive and context-aware actions in such challenging conditions. To enhance the driving decision-making of LLMs, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, annotated for LLM fine-tuning in hazard detection and motion planning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that LLM-RCO promotes proactive maneuvers over purely risk-averse actions in perception deficit scenarios, underscoring its value for boosting autonomous driving resilience against perception loss challenges.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence</title>
<link>https://arxiv.org/abs/2503.14749</link>
<guid>https://arxiv.org/abs/2503.14749</guid>
<content:encoded><![CDATA[
arXiv:2503.14749v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly used for factual question-answering, it becomes more important for LLMs to have the capability to communicate the likelihood that their answer is correct. For these verbalized expressions of uncertainty to be meaningful, they should reflect the error rates at the expressed level of confidence. However, when prompted to express confidence, the error rates of current LLMs are inconsistent with their communicated confidences, highlighting the need for uncertainty quantification methods. Many prior methods calculate lexical uncertainty, estimating a model's confidence in the specific string it generated. In some cases, however, it may be more useful to estimate semantic uncertainty, or the model's confidence in the answer regardless of how it is verbalized. We propose a simple procedure, uncertainty distillation, to teach an LLM to verbalize calibrated semantic confidences. Using held-out data to map initial uncertainty estimates to meaningful probabilities, we create examples annotated with verbalized probabilities for supervised fine-tuning. We find that our method yields verbalized confidences that correlate well with observed error rates, even when compared to strong baselines, some of which are more than twenty times slower at inference time. Additionally, we demonstrate that our method can be applied to black-box models that allow API-based fine-tuning, resulting in estimates of uncertainty that are both more effective and more efficient than any of our baselines.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Support Vector Regression for Robust Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.01012</link>
<guid>https://arxiv.org/abs/2505.01012</guid>
<content:encoded><![CDATA[
arXiv:2505.01012v3 Announce Type: replace-cross 
Abstract: Anomaly Detection (AD) is critical in data analysis, particularly within the domain of IT security. In this study, we explore the potential of Quantum Machine Learning for application to AD with special focus on the robustness to noise and adversarial attacks. We build upon previous work on Quantum Support Vector Regression (QSVR) for semisupervised AD by conducting a comprehensive benchmark on IBM quantum hardware using eleven datasets. Our results demonstrate that QSVR achieves strong classification performance and even outperforms the noiseless simulation on two of these datasets. Moreover, we investigate the influence of - in the NISQ-era inevitable - quantum noise on the performance of the QSVR. Our findings reveal that the model exhibits robustness to depolarizing, phase damping, phase flip, and bit flip noise, while amplitude damping and miscalibration noise prove to be more disruptive. Finally, we explore the domain of Quantum Adversarial Machine Learning by demonstrating that QSVR is highly vulnerable to adversarial attacks, with neither quantum noise nor adversarial training improving the model's robustness against such attacks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hamiltonian of polymatrix zero-sum games</title>
<link>https://arxiv.org/abs/2505.12609</link>
<guid>https://arxiv.org/abs/2505.12609</guid>
<content:encoded><![CDATA[
arXiv:2505.12609v3 Announce Type: replace-cross 
Abstract: The understanding of a dynamical system's properties can be significantly advanced by establishing it as a Hamiltonian system and then systematically exploring its inherent symmetries. By formulating agents' strategies and cumulative payoffs as canonically conjugate variables, we identify the Hamiltonian function that generates the dynamics of poly-matrix zero-sum games. We reveal the symmetries of our Hamiltonian and derive the associated conserved quantities, showing how the conservation of probability and the invariance of the Fenchel coupling are intrinsically encoded within the system. Furthermore, we propose the dissipation FTRL (DFTRL) dynamics by introducing a perturbation that dissipates the Fenchel coupling, proving convergence to the Nash equilibrium and linking DFTRL to last-iterate convergent algorithms. Our results highlight the potential of Hamiltonian dynamics in uncovering the structural properties of learning dynamics in games, and pave the way for broader applications of Hamiltonian dynamics in game theory and machine learning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Early-Token Bias: Model-Specific and Language-Specific Position Effects in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2505.16134</link>
<guid>https://arxiv.org/abs/2505.16134</guid>
<content:encoded><![CDATA[
arXiv:2505.16134v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit position bias systematically underweighting information based on its location in the context but how this bias varies across languages and models remains unclear. We conduct a multilingual study across five typologically diverse languages (English, Russian, German, Hindi, Vietnamese) and five model architectures, analyzing how position bias interacts with prompting strategies and affects output entropy. Our key findings are: (1) Position bias is primarily model-driven but shows language-specific nuances. Notably, Qwen2.5-7B-Instruct, DeepSeek 7B Chat and Mistral 7B consistently favor late positions challenging the common assumption of universal early-token preference. (2) Explicitly instructing the model, in the presence of irrelevant distractors, that "the most relevant context to the query is marked as 1" unexpectedly reduces accuracy across all languages, questioning standard prompt-engineering practices. (3) Accuracy consistently drops most when relevant information appears in the middle of the context, yet this is not reflected in a corresponding increase in output entropy, suggesting the model remains confident even when it fails to use mid-context cues.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safely Learning Controlled Stochastic Dynamics</title>
<link>https://arxiv.org/abs/2506.02754</link>
<guid>https://arxiv.org/abs/2506.02754</guid>
<content:encoded><![CDATA[
arXiv:2506.02754v2 Announce Type: replace-cross 
Abstract: We address the problem of safely learning controlled stochastic dynamics from discrete-time trajectory observations, ensuring system trajectories remain within predefined safe regions during both training and deployment. Safety-critical constraints of this kind are crucial in applications such as autonomous robotics, finance, and biomedicine. We introduce a method that ensures safe exploration and efficient estimation of system dynamics by iteratively expanding an initial known safe control set using kernel-based confidence bounds. After training, the learned model enables predictions of the system's dynamics and permits safety verification of any given control. Our approach requires only mild smoothness assumptions and access to an initial safe control set, enabling broad applicability to complex real-world systems. We provide theoretical guarantees for safety and derive adaptive learning rates that improve with increasing Sobolev regularity of the true dynamics. Experimental evaluations demonstrate the practical effectiveness of our method in terms of safety, estimation accuracy, and computational efficiency.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation</title>
<link>https://arxiv.org/abs/2506.10622</link>
<guid>https://arxiv.org/abs/2506.10622</guid>
<content:encoded><![CDATA[
arXiv:2506.10622v2 Announce Type: replace-cross 
Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEIST: A Graph Foundation Model for Spatial Transcriptomics and Proteomics Data</title>
<link>https://arxiv.org/abs/2506.11152</link>
<guid>https://arxiv.org/abs/2506.11152</guid>
<content:encoded><![CDATA[
arXiv:2506.11152v3 Announce Type: replace-cross 
Abstract: Single-cell transcriptomics and proteomics have become a great source for data-driven insights into biology, enabling the use of advanced deep learning methods to understand cellular heterogeneity and gene expression at the single-cell level. With the advent of spatial-omics data, we have the promise of characterizing cells within their tissue context as it provides both spatial coordinates and intra-cellular transcriptional or protein counts. Proteomics offers a complementary view by directly measuring proteins, which are the primary effectors of cellular function and key therapeutic targets. However, existing models either ignore the spatial information or the complex genetic and proteomic programs within cells. Thus they cannot infer how cell internal regulation adapts to microenvironmental cues. Furthermore, these models often utilize fixed gene vocabularies, hindering their generalizability unseen genes. In this paper, we introduce HEIST, a hierarchical graph transformer foundation model for spatial transcriptomics and proteomics. HEIST models tissues as hierarchical graphs. The higher level graph is a spatial cell graph, and each cell in turn, is represented by its lower level gene co-expression network graph. HEIST achieves this by performing both intra-level and cross-level message passing to utilize the hierarchy in its embeddings and can thus generalize to novel datatypes including spatial proteomics without retraining. HEIST is pretrained on 22.3M cells from 124 tissues across 15 organs using spatially-aware contrastive and masked autoencoding objectives. Unsupervised analysis of HEIST embeddings reveals spatially informed subpopulations missed by prior models. Downstream evaluations demonstrate generalizability to proteomics data and state-of-the-art performance in clinical outcome prediction, cell type annotation, and gene imputation across multiple technologies.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Intensity and Focality in Directional DBS Under Uncertainty: A Simulation Study of Electrode Optimization via a Metaheuristic L1L1 Approach</title>
<link>https://arxiv.org/abs/2506.13452</link>
<guid>https://arxiv.org/abs/2506.13452</guid>
<content:encoded><![CDATA[
arXiv:2506.13452v2 Announce Type: replace-cross 
Abstract: As DBS technology advances toward directional leads and optimization-based current steering, this study aims to improve the selection of electrode contact configurations using the recently developed L1-norm regularized L1-norm fitting (L1L1) method. The focus is in particular on L1L1's capability to incorporate a priori lead field uncertainty, offering a potential advantage over conventional approaches that do not account for such variability. Our optimization framework incorporates uncertainty by constraining the solution space based on lead field attenuation. This reflects physiological expectations about the VTA and serves to avoid overfitting. By applying this method to 8- and 40-contact electrode configurations, we optimize current distributions within a discretized finite element (FE) model, focusing on the lead field's characteristics. The model accounts for uncertainty through these explicit constraints, enhancing the feasibility, focality, and robustness of the resulting solutions. The L1L1 method was validated through a series of numerical experiments using both noiseless and noisy lead fields, where the noise level was selected to reflect attenuation within VTA. It successfully fits and regularizes the current distribution across target structures, with hyperparameter optimization extracting either bipolar or multipolar electrode configurations. These configurations aim to maximize focused current density or prioritize a high gain field ratio in a discretized FE model. Compared to traditional methods, the L1L1 approach showed competitive performance in concentrating stimulation within the target region while minimizing unintended current spread, particularly under noisy conditions. By incorporating uncertainty directly into the optimization process, we obtain a noise-robust framework for current steering, allowing for variations in lead field models and simulation parameters.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Hitchhiker's Guide to Efficient, End-to-End, and Tight DP Auditing</title>
<link>https://arxiv.org/abs/2506.16666</link>
<guid>https://arxiv.org/abs/2506.16666</guid>
<content:encoded><![CDATA[
arXiv:2506.16666v3 Announce Type: replace-cross 
Abstract: In this paper, we systematize research on auditing Differential Privacy (DP) techniques, aiming to identify key insights and open challenges. First, we introduce a comprehensive framework for reviewing work in the field and establish three cross-contextual desiderata that DP audits should target -- namely, efficiency, end-to-end-ness, and tightness. Then, we systematize the modes of operation of state-of-the-art DP auditing techniques, including threat models, attacks, and evaluation functions. This allows us to highlight key details overlooked by prior work, analyze the limiting factors to achieving the three desiderata, and identify open research problems. Overall, our work provides a reusable and systematic methodology geared to assess progress in the field and identify friction points and future directions for our community to focus on.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Segmentation Reasoning: Diagnosing and Mitigating Pixel-Grounding Hallucination</title>
<link>https://arxiv.org/abs/2506.21546</link>
<guid>https://arxiv.org/abs/2506.21546</guid>
<content:encoded><![CDATA[
arXiv:2506.21546v3 Announce Type: replace-cross 
Abstract: Segmentation Vision-Language Models (VLMs) have significantly advanced grounded visual understanding, yet they remain prone to pixel-grounding hallucinations, producing masks for incorrect objects or for objects that are entirely absent. Existing evaluations rely almost entirely on text- or label-based perturbations, which check only whether the predicted mask matches the queried label. Such evaluations overlook the spatial footprint and severity of hallucination and therefore fail to reveal vision-driven hallucinations, which are more challenging and more prevalent. To address this gap, we formalize the task of Counterfactual Segmentation Reasoning (CSR), where a model must segment the referenced object in the factual image and abstain in its counterfactual counterpart. To support this task, we curate HalluSegBench, the first large-scale benchmark to diagnose referring and reasoning expression segmentation hallucinations using controlled visual counterfactuals, alongside new evaluation metrics that measure hallucination severity and disentangle vision- and language-driven failure modes. We further introduce RobustSeg, a segmentation VLM trained with counterfactual fine-tuning (CFT) to learn when to segment and when to abstain. Experimental results confirm RobustSeg reduces hallucinations by 30%, while improving segmentation performance on FP-RefCOCO(+/g).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communication-Efficient Module-Wise Federated Learning for Grasp Pose Detection in Cluttered Environments</title>
<link>https://arxiv.org/abs/2507.05861</link>
<guid>https://arxiv.org/abs/2507.05861</guid>
<content:encoded><![CDATA[
arXiv:2507.05861v2 Announce Type: replace-cross 
Abstract: Grasp pose detection (GPD) is a fundamental capability for robotic autonomy, but its reliance on large, diverse datasets creates significant data privacy and centralization challenges. Federated Learning (FL) offers a privacy-preserving solution, but its application to GPD is hindered by the substantial communication overhead of large models, a key issue for resource-constrained robots. To address this, we propose a novel module-wise FL framework that begins by analyzing the learning dynamics of the GPD model's functional components. This analysis identifies slower-converging modules, to which our framework then allocates additional communication effort. This is realized through a two-phase process: a standard full-model training phase is followed by a communication-efficient phase where only the identified subset of slower-converging modules is trained and their partial updates are aggregated. Extensive experiments on the GraspNet-1B dataset demonstrate that our method outperforms standard FedAvg and other baselines, achieving higher accuracy for a given communication budget. Furthermore, real-world experiments on a physical robot validate our approach, showing a superior grasp success rate compared to baseline methods in cluttered scenes. Our work presents a communication-efficient framework for training robust, generalized GPD models in a decentralized manner, effectively improving the trade-off between communication cost and model performance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistical Inference for Differentially Private Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2507.20560</link>
<guid>https://arxiv.org/abs/2507.20560</guid>
<content:encoded><![CDATA[
arXiv:2507.20560v2 Announce Type: replace-cross 
Abstract: Privacy preservation in machine learning, particularly through Differentially Private Stochastic Gradient Descent (DP-SGD), is critical for sensitive data analysis. However, existing statistical inference methods for SGD predominantly focus on cyclic subsampling, while DP-SGD requires randomized subsampling. This paper first bridges this gap by establishing the asymptotic properties of SGD under the randomized rule and extending these results to DP-SGD. For the output of DP-SGD, we show that the asymptotic variance decomposes into statistical, sampling, and privacy-induced components. Two methods are proposed for constructing valid confidence intervals: the plug-in method and the random scaling method. We also perform extensive numerical analysis, which shows that the proposed confidence intervals achieve nominal coverage rates while maintaining privacy.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Annotation-Free Reinforcement Learning Query Rewriting via Verifiable Search Reward</title>
<link>https://arxiv.org/abs/2507.23242</link>
<guid>https://arxiv.org/abs/2507.23242</guid>
<content:encoded><![CDATA[
arXiv:2507.23242v2 Announce Type: replace-cross 
Abstract: Optimizing queries for Retrieval-Augmented Generation (RAG) systems poses a significant challenge, particularly across diverse modal indices. We introduce RL-QR, a novel annotation-free reinforcement learning framework for query rewriting that eliminates the need for costly human-annotated data. By leveraging verifiable search rewards derived from index-aligned synthetic queries, RL-QR overcomes human-annotation dependencies, extending its applicability to various modalities and index domains. Experimental results demonstrate the framework's robustness, achieving substantial retrieval performance gains of up to 3.9$\times$ on lexical retrievers and 3.5$\times$ on semantic retrievers on the MTEB VIDORE V2 benchmark for unstructured visual documents, along with consistent 5\% to 10\% improvements on MS MARCO v2.1 and internal industrial datasets.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence</title>
<link>https://arxiv.org/abs/2508.15447</link>
<guid>https://arxiv.org/abs/2508.15447</guid>
<content:encoded><![CDATA[
arXiv:2508.15447v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown promising potential in business applications, particularly in enterprise decision support and strategic planning, yet current approaches often struggle to reconcile intricate operational analyses with overarching strategic goals across diverse market environments, leading to fragmented workflows and reduced collaboration across organizational levels. This paper introduces BusiAgent, a novel multi-agent framework leveraging LLMs for advanced decision-making in complex corporate environments. BusiAgent integrates three core innovations: an extended Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a generalized entropy measure to optimize collaborative efficiency, and a multi-level Stackelberg game to handle hierarchical decision processes. Additionally, contextual Thompson sampling is employed for prompt optimization, supported by a comprehensive quality assurance system to mitigate errors. Extensive empirical evaluations across diverse business scenarios validate BusiAgent's efficacy, demonstrating its capacity to generate coherent, client-focused solutions that smoothly integrate granular insights with high-level strategy, significantly outperforming established approaches in both solution quality and user satisfaction. By fusing cutting-edge AI technologies with deep business insights, BusiAgent marks a substantial step forward in AI-driven enterprise decision-making, empowering organizations to navigate complex business landscapes more effectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echoes of the past: A unified perspective on fading memory and echo states</title>
<link>https://arxiv.org/abs/2508.19145</link>
<guid>https://arxiv.org/abs/2508.19145</guid>
<content:encoded><![CDATA[
arXiv:2508.19145v2 Announce Type: replace-cross 
Abstract: Recurrent neural networks (RNNs) have become increasingly popular in information processing tasks involving time series and temporal data. A fundamental property of RNNs is their ability to create reliable input/output responses, often linked to how the network handles its memory of the information it processed. Various notions have been proposed to conceptualize the behavior of memory in RNNs, including steady states, echo states, state forgetting, input forgetting, and fading memory. Although these notions are often used interchangeably, their precise relationships remain unclear. This work aims to unify these notions in a common language, derive new implications and equivalences between them, and provide alternative proofs to some existing results. By clarifying the relationships between these concepts, this research contributes to a deeper understanding of RNNs and their temporal information processing capabilities.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Misspecification-robust amortised simulation-based inference using variational methods</title>
<link>https://arxiv.org/abs/2509.05724</link>
<guid>https://arxiv.org/abs/2509.05724</guid>
<content:encoded><![CDATA[
arXiv:2509.05724v2 Announce Type: replace-cross 
Abstract: Recent advances in neural density estimation have enabled powerful simulation-based inference (SBI) methods that can flexibly approximate Bayesian inference for intractable stochastic models. Although these methods have demonstrated reliable posterior estimation when the simulator accurately represents the underlying data generative process (DGP), recent work has shown that they perform poorly in the presence of model misspecification. This poses a significant issue for their use in real-world problems, due to simulators always misrepresenting the true DGP to a certain degree. In this paper, we introduce robust variational neural posterior estimation (RVNP), a method which addresses the problem of misspecification in amortised SBI by bridging the simulation-to-reality gap using variational inference and error modelling. We test RVNP on multiple benchmark tasks, including using real data from astronomy, and show that it can recover robust posterior inference in a data-driven manner without adopting hyperparameters or priors governing the misspecification influence.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Illusion of Readiness in Health AI</title>
<link>https://arxiv.org/abs/2509.18234</link>
<guid>https://arxiv.org/abs/2509.18234</guid>
<content:encoded><![CDATA[
arXiv:2509.18234v3 Announce Type: replace-cross 
Abstract: Large language models have demonstrated remarkable performance in a wide range of medical benchmarks. Yet underneath the seemingly promising results lie salient growth areas, especially in cutting-edge frontiers such as multimodal reasoning. In this paper, we introduce a series of adversarial stress tests to systematically assess the robustness of flagship models and medical benchmarks. Our study reveals prevalent brittleness in the presence of simple adversarial transformations: leading systems can guess the right answer even with key inputs removed, yet may get confused by the slightest prompt alterations, while fabricating convincing yet flawed reasoning traces. Using clinician-guided rubrics, we demonstrate that popular medical benchmarks vary widely in what they truly measure. Our study reveals significant competency gaps of frontier AI in attaining real-world readiness for health applications. If we want AI to earn trust in healthcare, we must demand more than leaderboard wins and must hold AI systems accountable to ensure robustness, sound reasoning, and alignment with real medical demands.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E2E Learning Massive MIMO for Multimodal Semantic Non-Orthogonal Transmission and Fusion</title>
<link>https://arxiv.org/abs/2509.19312</link>
<guid>https://arxiv.org/abs/2509.19312</guid>
<content:encoded><![CDATA[
arXiv:2509.19312v2 Announce Type: replace-cross 
Abstract: This paper investigates multimodal semantic non-orthogonal transmission and fusion in hybrid analog-digital massive multiple-input multiple-output (MIMO). A Transformer-based cross-modal source-channel semantic-aware network (CSC-SA-Net) framework is conceived, where channel state information (CSI) reference signal (RS), feedback, analog-beamforming/combining, and baseband semantic processing are data-driven end-to-end (E2E) optimized at the base station (BS) and user equipments (UEs). CSC-SA-Net comprises five sub-networks: BS-side CSI-RS network (BS-CSIRS-Net), UE-side channel semantic-aware network (UE-CSANet), BS-CSANet, UE-side multimodal semantic fusion network (UE-MSFNet), and BS-MSFNet. Specifically, we firstly E2E train BS-CSIRS-Net, UE-CSANet, and BS-CSANet to jointly design CSI-RS, feedback, analog-beamforming/combining with maximum {\emph{physical-layer's}} spectral-efficiency. Meanwhile, we E2E train UE-MSFNet and BS-MSFNet for optimizing {\emph{application-layer's}} source semantic downstream tasks. On these pre-trained models, we further integrate application-layer semantic processing with physical-layer tasks to E2E train five subnetworks. Extensive simulations show that the proposed CSC-SA-Net outperforms traditional separated designs, revealing the advantage of cross-modal channel-source semantic fusion.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Super-Resolution for Urban Micrometeorology via a Schr\"odinger Bridge</title>
<link>https://arxiv.org/abs/2510.12148</link>
<guid>https://arxiv.org/abs/2510.12148</guid>
<content:encoded><![CDATA[
arXiv:2510.12148v2 Announce Type: replace-cross 
Abstract: This study employs a neural network that represents the solution to a Schr\"odinger bridge problem to perform super-resolution of 2-m temperature in an urban area. Schr\"odinger bridges generally describe transformations between two data distributions based on diffusion processes. We use a specific Schr\"odinger-bridge model (SM) that directly transforms low-resolution data into high-resolution data, unlike denoising diffusion probabilistic models (simply, diffusion models; DMs) that generate high-resolution data from Gaussian noise. Low-resolution and high-resolution data were obtained from separate numerical simulations with a physics-based model under common initial and boundary conditions. Compared with a DM, the SM attains comparable accuracy at one-fifth the computational cost, requiring 50 neural-network evaluations per datum for the DM and only 10 for the SM. Furthermore, high-resolution samples generated by the SM exhibit larger variance, implying superior uncertainty quantification relative to the DM. Owing to the reduced computational cost of the SM, our results suggest the feasibility of real-time ensemble micrometeorological prediction using SM-based super-resolution.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Exploration of Chemical Kinetics</title>
<link>https://arxiv.org/abs/2510.21368</link>
<guid>https://arxiv.org/abs/2510.21368</guid>
<content:encoded><![CDATA[
arXiv:2510.21368v2 Announce Type: replace-cross 
Abstract: Estimating reaction rates and chemical stability is fundamental, yet efficient methods for large-scale simulations remain out of reach despite advances in modeling and exascale computing. Direct simulation is limited by short timescales; machine-learned potentials require large data sets and struggle with transition state regions essential for reaction rates. Reaction network exploration with sufficient accuracy is hampered by the computational cost of electronic structure calculations, and even simplifications like harmonic transition state theory rely on prohibitively expensive saddle point searches. Surrogate model-based acceleration has been promising but hampered by overhead and numerical instability.
  This dissertation presents a holistic solution, co-designing physical representations, statistical models, and systems architecture in the Optimal Transport Gaussian Process (OT-GP) framework. Using physics-aware optimal transport metrics, OT-GP creates compact, chemically relevant surrogates of the potential energy surface, underpinned by statistically robust sampling. Alongside EON software rewrites for long timescale simulations, we introduce reinforcement learning approaches for both minimum-mode following (when the final state is unknown) and nudged elastic band methods (when endpoints are specified). Collectively, these advances establish a representation-first, modular approach to chemical kinetics simulation. Large-scale benchmarks and Bayesian hierarchical validation demonstrate state-of-the-art performance and practical exploration of chemical kinetics, transforming a longstanding theoretical promise into a working engine for discovery.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introducing physics-informed generative models for targeting structural novelty in the exploration of chemical space</title>
<link>https://arxiv.org/abs/2510.23181</link>
<guid>https://arxiv.org/abs/2510.23181</guid>
<content:encoded><![CDATA[
arXiv:2510.23181v2 Announce Type: replace-cross 
Abstract: Discovering materials with new structural chemistry is key to achieving transformative functionality. Generative artificial intelligence offers a scalable route to propose candidate crystal structures. We introduce a reliable low-cost proxy for structural novelty as a conditioning property to steer generation towards novel yet physically plausible structures. We then develop a physics-informed diffusion model that embeds this descriptor of local environment diversity together with compactness as a stability metric to balance physical plausibility with structural novelty. Conditioning on these metrics improves generative performance across diffusion models, shifting generation away from structural motifs that dominate the training data. A chemically grounded validation protocol isolates those candidates that combine plausibility with structural novelty for physics-based calculation of energetic stability. Both the stability and the novelty of candidates emerging from this workflow can however change when the full potential energy surface at a candidate composition is evaluated with crystal structure prediction (CSP). This suggests a practical generative-CSP synergy for discovery-oriented exploration, where AI targets physically viable yet structurally distinct regions of chemical space for detailed physics-based assessment of novelty and stability.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCode: Unify Plan and Action for Universal Granularity Control</title>
<link>https://arxiv.org/abs/2510.23564</link>
<guid>https://arxiv.org/abs/2510.23564</guid>
<content:encoded><![CDATA[
arXiv:2510.23564v3 Announce Type: replace-cross 
Abstract: Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistical physics of deep learning: Optimal learning of a multi-layer perceptron near interpolation</title>
<link>https://arxiv.org/abs/2510.24616</link>
<guid>https://arxiv.org/abs/2510.24616</guid>
<content:encoded><![CDATA[
arXiv:2510.24616v3 Announce Type: replace-cross 
Abstract: For four decades statistical physics has been providing a framework to analyse neural networks. A long-standing question remained on its capacity to tackle deep learning models capturing rich feature learning effects, thus going beyond the narrow networks or kernel methods analysed until now. We positively answer through the study of the supervised learning of a multi-layer perceptron. Importantly, (i) its width scales as the input dimension, making it more prone to feature learning than ultra wide networks, and more expressive than narrow ones or ones with fixed embedding layers; and (ii) we focus on the challenging interpolation regime where the number of trainable parameters and data are comparable, which forces the model to adapt to the task. We consider the matched teacher-student setting. Therefore, we provide the fundamental limits of learning random deep neural network targets and identify the sufficient statistics describing what is learnt by an optimally trained network as the data budget increases. A rich phenomenology emerges with various learning transitions. With enough data, optimal performance is attained through the model's "specialisation" towards the target, but it can be hard to reach for training algorithms which get attracted by sub-optimal solutions predicted by the theory. Specialisation occurs inhomogeneously across layers, propagating from shallow towards deep ones, but also across neurons in each layer. Furthermore, deeper targets are harder to learn. Despite its simplicity, the Bayes-optimal setting provides insights on how the depth, non-linearity and finite (proportional) width influence neural networks in the feature learning regime that are potentially relevant in much more general settings.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Bayesian Optimization: Generative Models as Acquisition Functions</title>
<link>https://arxiv.org/abs/2510.25240</link>
<guid>https://arxiv.org/abs/2510.25240</guid>
<content:encoded><![CDATA[
arXiv:2510.25240v2 Announce Type: replace-cross 
Abstract: We present a general strategy for turning generative models into candidate solution samplers for batch Bayesian optimization (BO). The use of generative models for BO enables large batch scaling as generative sampling, optimization of non-continuous design spaces, and high-dimensional and combinatorial design. Inspired by the success of direct preference optimization (DPO), we show that one can train a generative model with noisy, simple utility values directly computed from observations to then form proposal distributions whose densities are proportional to the expected utility, i.e., BO's acquisition function values. Furthermore, this approach is generalizable beyond preference-based feedback to general types of reward signals and loss functions. This perspective avoids the construction of surrogate (regression or classification) models, common in previous methods that have used generative models for black-box optimization. Theoretically, we show that the generative models within the BO process approximately follow a sequence of distributions which asymptotically concentrate at the global optima under certain conditions. We also demonstrate this effect through experiments on challenging optimization problems involving large batches in high dimensions.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</title>
<link>https://arxiv.org/abs/2511.20629</link>
<guid>https://arxiv.org/abs/2511.20629</guid>
<content:encoded><![CDATA[
arXiv:2511.20629v2 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Highly Configurable Framework for Large-Scale Thermal Building Data Generation to drive Machine Learning Research</title>
<link>https://arxiv.org/abs/2512.00483</link>
<guid>https://arxiv.org/abs/2512.00483</guid>
<content:encoded><![CDATA[
arXiv:2512.00483v2 Announce Type: replace-cross 
Abstract: Data-driven modeling of building thermal dynamics is emerging as an increasingly important field of research for large-scale intelligent building control. However, research in data-driven modeling using machine learning (ML) techniques requires massive amounts of thermal building data, which is not easily available. Neither empirical public datasets nor existing data generators meet the needs of ML research in terms of data quality and quantity. Moreover, existing data generation approaches typically require expert knowledge in building simulation. To fill this gap, we present a thermal building data generation framework which we call BuilDa. BuilDa is designed to produce synthetic data of adequate quality and quantity for ML research. The framework does not require profound building simulation knowledge to generate large volumes of data. BuilDa uses a single-zone Modelica model that is exported as a Functional Mock-up Unit (FMU) and simulated in Python. We demonstrate BuilDa by generating data and utilizing it for a transfer learning study involving the fine-tuning of 486 data-driven models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HGC-Herd: Efficient Heterogeneous Graph Condensation via Representative Node Herding</title>
<link>https://arxiv.org/abs/2512.09947</link>
<guid>https://arxiv.org/abs/2512.09947</guid>
<content:encoded><![CDATA[
<div> Heterogeneous Graph Neural Networks, Graph Condensation, Scalability, HGC-Herd, Feature Propagation<br /><br />Summary:<br /><br />Heterogeneous graph neural networks (HGNNs) effectively model complex semantics involving multiple types of nodes and relations but face scalability challenges on large datasets due to structural redundancy and high-dimensional features. Existing graph condensation methods like GCond are designed mainly for homogeneous graphs and depend on gradient matching, which incurs heavy computational, memory, and optimization costs. To address these limitations, the paper introduces HGC-Herd, a novel training-free condensation framework tailored for heterogeneous graphs. HGC-Herd creates compact yet information-rich condensed graphs that preserve both semantic meaning and structural properties. It incorporates lightweight feature propagation to capture multi-hop relational contexts, enhancing representational fidelity. Additionally, a class-wise herding technique is used to select representative nodes per class, ensuring the condensed graph is balanced and discriminative for downstream tasks. Experimental evaluations on datasets ACM, DBLP, and Freebase show that HGC-Herd achieves accuracy on par with or better than full-graph training methods. Moreover, it significantly reduces runtime and memory usage, demonstrating practical efficiency and scalability for heterogeneous graph learning applications. This framework presents a promising step toward scalable heterogeneous graph representation with lower resource requirements. <div>
arXiv:2512.09947v1 Announce Type: new 
Abstract: Heterogeneous graph neural networks (HGNNs) have demonstrated strong capability in modeling complex semantics across multi-type nodes and relations. However, their scalability to large-scale graphs remains challenging due to structural redundancy and high-dimensional node features. Existing graph condensation approaches, such as GCond, are primarily developed for homogeneous graphs and rely on gradient matching, resulting in considerable computational, memory, and optimization overhead. We propose HGC-Herd, a training-free condensation framework that generates compact yet informative heterogeneous graphs while maintaining both semantic and structural fidelity. HGC-Herd integrates lightweight feature propagation to encode multi-hop relational context and employs a class-wise herding mechanism to identify representative nodes per class, producing balanced and discriminative subsets for downstream learning tasks. Extensive experiments on ACM, DBLP, and Freebase validate that HGC-Herd attains comparable or superior accuracy to full-graph training while markedly reducing both runtime and memory consumption. These results underscore its practical value for efficient and scalable heterogeneous graph representation learning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization</title>
<link>https://arxiv.org/abs/2512.09972</link>
<guid>https://arxiv.org/abs/2512.09972</guid>
<content:encoded><![CDATA[
<div> Keywords: Pareto set, Large Language Models, Bayesian optimization, block partitioning, multi-objective optimization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of constructing a comprehensive Pareto set that effectively balances capability and efficiency trade-offs in Large Language Models (LLMs).<br />2. Existing solutions fall short by either being coarse-grained at the model level, resulting in sparse and suboptimal solutions, or deploying layer-wise fine-grained approaches which face computational intractability due to high dimensionality.<br />3. To overcome these limitations, the authors introduce BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework designed to automatically generate the Pareto set for LLMs.<br />4. Central to BAMBO is a Hybrid Optimal Block Partitioning strategy, framed as a 1D clustering problem that uses dynamic programming to strike an optimal balance between intra-block homogeneity and inter-block information distribution, effectively reducing dimensionality without losing important granularity.<br />5. The optimization process is embedded within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function, ensuring efficient and automated exploration.<br />6. Experimental results demonstrate BAMBO’s ability to discover a more comprehensive and superior Pareto frontier compared to baseline methods, facilitating adaptive model selection customized to various operational constraints.<br />7. The authors provide the code publicly for reproducibility and further research at https://github.com/xin8coder/BAMBO. <div>
arXiv:2512.09972v1 Announce Type: new 
Abstract: Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the "curse of dimensionality," rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Action World Models for Control with Unlabeled Trajectories</title>
<link>https://arxiv.org/abs/2512.10016</link>
<guid>https://arxiv.org/abs/2512.10016</guid>
<content:encoded><![CDATA[
<div> latent action, world models, offline reinforcement learning, action-free data, DeepMind Control Suite<br /><br />Summary:<br /><br />This paper addresses the challenge of learning world models from heterogeneous data that include both action-conditioned and action-free experiences, such as videos without accompanying action labels. Standard world models depend heavily on trajectories labeled with actions, limiting their capability when such labels are scarce. The authors propose a family of latent-action world models that create a shared latent action space, aligning actual control signals with inferred actions from passive observations. This approach enables a single dynamics model to leverage large-scale unlabeled trajectories alongside a small set of action-labeled data. Using the latent-action representation, they train a policy via offline reinforcement learning, effectively bridging the gap between offline RL, traditionally reliant on action-conditioned data, and learning from action-free data that usually does not integrate with RL pipelines. Experimental results on the DeepMind Control Suite demonstrate that the proposed method achieves competitive performance while requiring approximately ten times fewer action-labeled samples compared to purely action-conditioned baselines. These findings highlight the efficiency gains attained by incorporating latent actions that allow leveraging both passive and interactive datasets for training world models and policies more effectively. <div>
arXiv:2512.10016v1 Announce Type: new 
Abstract: Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cluster-Dags as Powerful Background Knowledge For Causal Discovery</title>
<link>https://arxiv.org/abs/2512.10032</link>
<guid>https://arxiv.org/abs/2512.10032</guid>
<content:encoded><![CDATA[
<div> Causal Discovery, Cluster-DAGs, Constraint-Based Algorithms, High-Dimensional Data, Prior Knowledge<br /><br />Summary:<br /><br />1. The paper addresses the challenge of finding cause-effect relationships, which is crucial in many scientific disciplines. <br />2. Causal discovery methods aim to reconstruct a graph that represents these cause-effect relationships from observed data but often struggle with high-dimensional and complex datasets. <br />3. The authors propose leveraging Cluster-DAGs as a novel framework for incorporating prior knowledge to improve causal discovery. Cluster-DAGs provide more flexibility than traditional tiered background knowledge approaches. <br />4. Two new modified constraint-based algorithms, named Cluster-PC and Cluster-FCI, are introduced for causal discovery. Cluster-PC is designed for fully observed data, while Cluster-FCI handles partially observed settings. <br />5. Empirical evaluations on simulated datasets show that both Cluster-PC and Cluster-FCI outperform their baseline counterparts when no prior knowledge is used, indicating that using Cluster-DAGs to warm-start the causal discovery process enhances performance. <div>
arXiv:2512.10032v1 Announce Type: new 
Abstract: Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealing with high-dimensional data and complex dependencies. Incorporating prior knowledge about the system can aid causal discovery. In this work, we leverage Cluster-DAGs as a prior knowledge framework to warm-start causal discovery. We show that Cluster-DAGs offer greater flexibility than existing approaches based on tiered background knowledge and introduce two modified constraint-based algorithms, Cluster-PC and Cluster-FCI, for causal discovery in the fully and partially observed setting, respectively. Empirical evaluation on simulated data demonstrates that Cluster-PC and Cluster-FCI outperform their respective baselines without prior knowledge.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation</title>
<link>https://arxiv.org/abs/2512.10033</link>
<guid>https://arxiv.org/abs/2512.10033</guid>
<content:encoded><![CDATA[
<div> Accelerated gradient methods, Heavy-Ball momentum, Synthetic Gradient Extrapolation, convergence guarantees, ill-conditioned optimization<br /><br />Summary:  
Accelerated gradient methods such as Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but can fail by diverging on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. The paper introduces Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a novel first-order optimization method that combines heavy-ball momentum with predictive gradient extrapolation to improve robustness. Unlike traditional momentum techniques that accumulate past gradients, HB-SGE uses local Taylor approximations to estimate future gradient directions, enabling adaptive acceleration without sacrificing stability. The authors provide theoretical convergence guarantees for strongly convex functions, ensuring the method's reliability. Empirical results demonstrate that HB-SGE avoids divergence on problems where NAG and classical momentum methods fail, particularly on ill-conditioned quadratic problems (condition number κ=50), where HB-SGE converged in 119 iterations while SGD and NAG diverged. Further, on the non-convex Rosenbrock function, HB-SGE achieved convergence in 2,718 iterations, proving more stable compared to classical momentum methods that diverged quickly. Although NAG remains faster on well-conditioned problems, HB-SGE offers a robust alternative, achieving speedups over SGD across various landscapes. Additionally, HB-SGE requires only O(d) additional memory and uses the same hyperparameters as standard momentum, making it practical for a wide range of applications. <div>
arXiv:2512.10033v1 Announce Type: new 
Abstract: Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $\kappa=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs</title>
<link>https://arxiv.org/abs/2512.10040</link>
<guid>https://arxiv.org/abs/2512.10040</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-tuning, large language models, multiple-reference preference optimization, weighting strategies, preference accuracy<br /><br />Summary:<br /><br />1. Fine-tuning is crucial for aligning large language models (LLMs) with human preferences, and Multiple-Reference Preference Optimization (MRPO) extends Direct Preference Optimization (DPO) by using multiple reference models.  
2. Existing methods for setting reference weights in MRPO are ad-hoc and statistically unsound, causing inconsistent performance.  
3. The paper introduces four new weighting strategies: two offline methods using held-out validation signals, one online method employing a sliding-window estimator to mitigate overfitting, and another online method framing reference weighting as a K-armed bandit problem resolved via Thompson Sampling.  
4. Experiments conducted with Qwen2.5-0.5B as the policy model and seven different reference models from diverse families (Llama, Mistral, Qwen, Yi, Phi) ranging from 0.5B to 14B parameters demonstrate that all four new strategies outperform existing MRPO weighting approaches in preference accuracy on UltraFeedback and SafeRLHF benchmarks.  
5. Notably, single-reference DPO using any six out of seven references consistently surpasses all tested multiple-reference methods, raising questions about the practical benefits of multiple-reference approaches in preference optimization for LLMs. <div>
arXiv:2512.10040v1 Announce Type: new 
Abstract: Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing the policy towards a mixture of reference models to leverage their collective desirable properties. However, current methods for setting the reference weights are ad-hoc and statistically unsound, leading to unreliable performance. To address this, we introduce four new weighting strategies: two offline methods that leverage held-out validation signal; one online method that uses a sliding-window estimator to reduce overfitting; and an online method that treats reference weighting as a $K$-armed bandit via Thompson Sampling. Experiments using Qwen2.5-0.5B as the policy model and seven reference models from the Llama, Mistral, Qwen, Yi, and Phi families (0.5B-14B each) show that all 4 of our strategies outperform the current MRPO weighting methods on UltraFeedback and SafeRLHF in preference accuracy. More thought-provokingly, however, we find that single-reference DPO, using any of 6 out of 7 references, consistently outperforms all tested multiple-reference approaches -- calling into question the practical appeal of multiple-reference approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation</title>
<link>https://arxiv.org/abs/2512.10042</link>
<guid>https://arxiv.org/abs/2512.10042</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised pre-training, reinforcement learning, state entropy maximization, off-policy algorithm, SEMDICE<br /><br />Summary:<br /><br />This paper addresses unsupervised pre-training in reinforcement learning (RL), where the goal is to learn a prior policy useful for downstream tasks without relying on task-specific reward signals. The focus is on state entropy maximization (SEM), which seeks to learn a policy that maximizes the entropy of the state stationary distribution, promoting exploration and diverse state visitation. The authors propose SEMDICE, a novel off-policy algorithm designed to compute a policy that maximizes state entropy directly from arbitrary off-policy datasets. Unlike existing methods, SEMDICE optimizes the policy explicitly within the space of stationary distributions, ensuring principled learning. SEMDICE computes a single stationary Markov policy that maximizes state entropy, making it applicable when only offline data is available. Experimental results show that SEMDICE outperforms baseline algorithms in effectively maximizing state entropy. Furthermore, SEMDICE achieves superior adaptation efficiency in downstream tasks compared to other SEM-based unsupervised RL pre-training approaches, demonstrating its practical benefits in accelerating learning when task rewards become available. This work thus contributes a theoretically sound and empirically validated method to advance unsupervised pre-training in reinforcement learning. <div>
arXiv:2512.10042v1 Announce Type: new 
Abstract: In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition</title>
<link>https://arxiv.org/abs/2512.10043</link>
<guid>https://arxiv.org/abs/2512.10043</guid>
<content:encoded><![CDATA[
<div> NER, Large Language Models, Ensemble, Portuguese, Zero-shot  

<br /><br />Summary:  
This work addresses the challenge of Named Entity Recognition (NER) for lower-resource languages like Portuguese, where Large Language Models (LLMs) often underperform despite excelling at many other NLP tasks. The authors propose a novel three-step ensemble pipeline that leverages multiple similarly capable locally run open-weight LLMs in a zero-shot setting, without any fine-tuning. Their approach introduces a heuristic to select optimal model combinations using only minimal annotated data, which helps boost performance beyond individual models. Experiments conducted on five Portuguese NER datasets demonstrate that the ensemble method outperforms single LLMs on four out of these five datasets. Furthermore, ensembles built on different source datasets show improved generalization in cross-dataset scenarios, suggesting that extensive annotated data for each new task may not be necessary. This work advances scalable and low-resource NER solutions by providing an effective alternative to relying on a single dominant model or the costly fine-tuning process. The authors also share their code publicly, facilitating reproducibility and further research in this domain. <div>
arXiv:2512.10043v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel in many Natural Language Processing (NLP) tasks through in-context learning but often under-perform in Named Entity Recognition (NER), especially for lower-resource languages like Portuguese. While open-weight LLMs enable local deployment, no single model dominates all tasks, motivating ensemble approaches. However, existing LLM ensembles focus on text generation or classification, leaving NER under-explored. In this context, this work proposes a novel three-step ensemble pipeline for zero-shot NER using similarly capable, locally run LLMs. Our method outperforms individual LLMs in four out of five Portuguese NER datasets by leveraging a heuristic to select optimal model combinations with minimal annotated data. Moreover, we show that ensembles obtained on different source datasets generally outperform individual LLMs in cross-dataset configurations, potentially eliminating the need for annotated data for the current task. Our work advances scalable, low-resource, and zero-shot NER by effectively combining multiple small LLMs without fine-tuning. Code is available at https://github.com/Joao-Luz/local-llm-ner-ensemble.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detailed balance in large language model-driven agents</title>
<link>https://arxiv.org/abs/2512.10047</link>
<guid>https://arxiv.org/abs/2512.10047</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, least action principle, generative dynamics, detailed balance, AI agents<br /><br />Summary:<br />1. This article addresses the lack of a theoretical framework to understand the macroscopic dynamics of large language model (LLM)-driven agents, which are increasingly used to solve complex problems.  
2. The authors propose leveraging the least action principle, a concept from physics, to estimate the underlying generative directionality of LLMs when embedded in agents, offering a novel viewpoint on their behavior.  
3. Through experimental measurement of transition probabilities between states generated by LLMs, they discover a statistical detailed balance, suggesting that LLM generation may rely on implicitly learning underlying potential functions rather than explicit rule sets or strategies.  
4. This discovery implies the emergence of a macroscopic physical law governing LLM generative dynamics that is independent of specific model architectures or prompt designs.  
5. The work aims to establish a macroscopic dynamics theory for complex AI systems, moving the study of AI agents from empirical engineering practices towards a science grounded on measurable, predictable, and quantifiable phenomena. <div>
arXiv:2512.10047v1 Announce Type: new 
Abstract: Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.10051</link>
<guid>https://arxiv.org/abs/2512.10051</guid>
<content:encoded><![CDATA[
<div> Time series forecasting, Transformer architecture, Daubechies wavelet, multi-scale patterns, resource efficiency<br /><br />Summary:<br /><br />This paper introduces DB2-TransF, a novel Transformer-inspired architecture designed for efficient time series forecasting. Traditional Transformer models, while powerful for capturing long-range dependencies, face scalability challenges due to their quadratic computational complexity. DB2-TransF addresses this limitation by replacing the conventional self-attention mechanism with a learnable Daubechies wavelet coefficient layer. This wavelet-based module effectively captures multi-scale local and global temporal patterns, enhancing the ability to model correlations across multiple time series simultaneously. The architecture is particularly well-suited for large-scale and high-dimensional data. Extensive experiments conducted on 13 standard forecasting benchmarks reveal that DB2-TransF achieves predictive accuracy comparable or superior to that of traditional Transformer models. Additionally, DB2-TransF demonstrates significant reductions in memory usage, making it more scalable and resource-efficient. The combination of wavelet-based feature extraction and Transformer inspiration positions DB2-TransF as a promising framework for advanced time series forecasting tasks. The authors have made their code publicly available at the provided GitHub repository, enabling further research and practical application of this approach. <div>
arXiv:2512.10051v1 Announce Type: new 
Abstract: Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings. While Transformer-based architectures excel at modeling long-range dependencies, their quadratic computational complexity poses limitations on scalability and adaptability. To overcome these challenges, we introduce DB2-TransF, a novel Transformer-inspired architecture that replaces the self-attention mechanism with a learnable Daubechies wavelet coefficient layer. This wavelet-based module efficiently captures multi-scale local and global patterns and enhances the modeling of correlations across multiple time series for the time series forecasting task. Extensive experiments on 13 standard forecasting benchmarks demonstrate that DB2-TransF achieves comparable or superior predictive accuracy to conventional Transformers, while substantially reducing memory usage for the time series forecasting task. The obtained experimental results position DB2-TransF as a scalable and resource-efficient framework for advanced time series forecasting. Our code is available at https://github.com/SteadySurfdom/DB2-TransF
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens</title>
<link>https://arxiv.org/abs/2512.10056</link>
<guid>https://arxiv.org/abs/2512.10056</guid>
<content:encoded><![CDATA[
<div> autoregressive forecasting, exposure bias, soft-token trajectory forecasting, risk-aware decoding, clinical risk<br /><br />Summary:<br /><br />This article addresses the challenge of unstable multi-step forecasting in autoregressive models used for predictive control in medical applications such as diabetes and hemodynamic management. Traditional models trained with teacher forcing experience exposure bias, leading to inaccuracies when predictions are fed back for future steps. To overcome this, the authors propose Soft-Token Trajectory Forecasting (SoTra), a novel method that propagates continuous probability distributions, referred to as "soft tokens," through the forecasting process. This approach reduces exposure bias and enables the learning of calibrated, uncertainty-aware trajectories, improving reliability in closed-loop settings. Additionally, SoTra incorporates a risk-aware decoding module designed to minimize expected clinical harm by taking into account different operating zones associated with varying levels of clinical risk. The method was evaluated on glucose forecasting and blood pressure forecasting tasks, showing a reduction in average zone-based risk by 18% in glucose prediction and an approximate 15% decrease in effective clinical risk for blood pressure management. These performance gains highlight SoTra's potential to enhance safety and effectiveness in critical predictive healthcare control systems. <div>
arXiv:2512.10056v1 Announce Type: new 
Abstract: Autoregressive forecasting is central to predictive control in diabetes and hemodynamic management, where different operating zones carry different clinical risks. Standard models trained with teacher forcing suffer from exposure bias, yielding unstable multi-step forecasts for closed-loop use. We introduce Soft-Token Trajectory Forecasting (SoTra), which propagates continuous probability distributions (``soft tokens'') to mitigate exposure bias and learn calibrated, uncertainty-aware trajectories. A risk-aware decoding module then minimizes expected clinical harm. In glucose forecasting, SoTra reduces average zone-based risk by 18\%; in blood-pressure forecasting, it lowers effective clinical risk by approximately 15\%. These improvements support its use in safety-critical predictive control.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>\textsc{Text2Graph}: Combining Lightweight LLMs and GNNs for Efficient Text Classification in Label-Scarce Scenarios</title>
<link>https://arxiv.org/abs/2512.10061</link>
<guid>https://arxiv.org/abs/2512.10061</guid>
<content:encoded><![CDATA[
<div> Large Language Models, zero-shot classification, text-to-graph, Graph Neural Networks, environmental cost

<br /><br />Summary:  
Large Language Models (LLMs) are effective for zero-shot text classification but have high computational demands and environmental impacts, limiting their use in large-scale, high-performance computing settings. To mitigate this, the authors introduce \textsc{Text2Graph}, an open-source Python package designed for modular implementation of text-to-graph classification methods. This framework integrates LLM-based partial annotation with Graph Neural Network (GNN) label propagation, allowing users to flexibly interchange components, including feature extractors, edge construction techniques, and sampling strategies. The package was benchmarked on five datasets covering topic classification and sentiment analysis under a zero-shot learning scenario. Multiple \textsc{Text2Graph} variants were compared against other zero-shot text classification approaches. Results showed that the graph-based propagation method achieves competitive classification performance while significantly reducing energy consumption and carbon emissions compared to traditional LLM approaches. The work highlights the feasibility and environmental benefits of combining LLMs with GNNs for sustainable large-scale text annotation workflows. <div>
arXiv:2512.10061v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become effective zero-shot classifiers, but their high computational requirements and environmental costs limit their practicality for large-scale annotation in high-performance computing (HPC) environments. To support more sustainable workflows, we present \textsc{Text2Graph}, an open-source Python package that provides a modular implementation of existing text-to-graph classification approaches. The framework enables users to combine LLM-based partial annotation with Graph Neural Network (GNN) label propagation in a flexible manner, making it straightforward to swap components such as feature extractors, edge construction methods, and sampling strategies. We benchmark \textsc{Text2Graph} on a zero-shot setting using five datasets spanning topic classification and sentiment analysis tasks, comparing multiple variants against other zero-shot approaches for text classification. In addition to reporting performance, we provide detailed estimates of energy consumption and carbon emissions, showing that graph-based propagation achieves competitive results at a fraction of the energy and environmental cost.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis</title>
<link>https://arxiv.org/abs/2512.10098</link>
<guid>https://arxiv.org/abs/2512.10098</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical AI, Explainable AI, Domain Shift, Rare-Class Bias, Multimodal Imaging  

<br /><br />Summary:  
This paper introduces MedXAI, an explainable AI framework tailored for medical imaging classification that integrates deep learning models with clinician-derived expert knowledge. The framework addresses key challenges in medical AI, such as overcoming domain shifts and mitigating bias against rare pathological classes. Unlike typical post-hoc explanation methods like Saliency Maps and LIME, MedXAI provides human-understandable explanations by explicitly localizing relevant diagnostic features based on expert knowledge. The framework was evaluated on two complex medical tasks: Seizure Onset Zone localization using resting-state fMRI and Diabetic Retinopathy grading, across ten multicenter heterogeneous datasets. Results demonstrated consistent improvements, including a 3% increase in cross-domain generalization performance and a 10% gain in the F1 score for rare disease classes, significantly outperforming strong deep learning baselines. Ablation studies confirmed that the symbolic expert knowledge components function as effective clinical priors and regularizers, enhancing robustness under distribution shifts. Overall, MedXAI delivers clinically aligned, interpretable explanations while achieving superior in-domain and cross-domain classification performance, particularly benefiting the detection and diagnosis of rare diseases in multimodal medical imaging AI systems. <div>
arXiv:2512.10098v1 Announce Type: new 
Abstract: Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly un- der domain shifts and rare-class conditions. Deep learning mod- els often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Med- ical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician- derived expert knowledge to improve generalization, reduce rare- class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHyLL: Learning Continuous Neural Representations of Hybrid Systems</title>
<link>https://arxiv.org/abs/2512.10117</link>
<guid>https://arxiv.org/abs/2512.10117</guid>
<content:encoded><![CDATA[
<div> Hybrid systems, continuous dynamics, neural embedding, quotient manifold, stochastic optimal control<br /><br />Summary:<br /><br />This paper addresses the challenge of learning flows in hybrid systems characterized by both continuous and discrete-time dynamics, where traditional methods struggle due to mode switching and flow discontinuities. The authors propose CHyLL (Continuous Hybrid System Learning in Latent Space), a novel approach that forgoes the need for trajectory segmentation, event functions, or explicit mode switching. The core insight behind CHyLL is the use of the reset map to "glue" the state space at guard surfaces, effectively reformulating the state space into a piecewise smooth quotient manifold that enables spatially continuous flow representation. Leveraging differential topology embedding theorems, CHyLL learns a singularity-free neural embedding in a higher-dimensional latent space while simultaneously learning the continuous dynamics within this space. Experimental results demonstrate that CHyLL provides superior accuracy in predicting hybrid system flows compared to existing methods. Additionally, CHyLL can identify topological invariants intrinsic to the hybrid systems studied. Finally, the paper extends the application of CHyLL to the stochastic optimal control problem, showcasing the method's versatility and potential for broader control and dynamical systems applications. <div>
arXiv:2512.10117v1 Announce Type: new 
Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partitioning the Sample Space for a More Precise Shannon Entropy Estimation</title>
<link>https://arxiv.org/abs/2512.10133</link>
<guid>https://arxiv.org/abs/2512.10133</guid>
<content:encoded><![CDATA[
<div> Keywords: Shannon entropy, discrete entropy estimator, missing mass, unseen outcomes, undersampled data  

<br /><br />Summary:  
1. The paper addresses the challenge of reliably estimating Shannon entropy from small data sets, particularly when the sample size is smaller than the number of possible outcomes.  
2. It introduces a novel discrete entropy estimator that leverages the decomposability property of entropy to improve estimation accuracy.  
3. The method incorporates estimations of the missing mass (the probability of unseen outcomes) and the count of unseen outcomes to correct the negative bias commonly observed in entropy estimation from limited samples.  
4. Experimental evaluations demonstrate that the proposed estimator outperforms classical entropy estimators in undersampled regimes, where data scarcity is most problematic.  
5. Additionally, the method performs comparably to some of the best-established state-of-the-art entropy estimators, indicating its robustness and effectiveness across different data sampling conditions.  
Overall, this work contributes a statistically sound and computationally feasible solution for entropy estimation in scenarios with limited data availability, which is crucial for many applications relying on accurate uncertainty quantification. <div>
arXiv:2512.10133v1 Announce Type: new 
Abstract: Reliable data-driven estimation of Shannon entropy from small data sets, where the number of examples is potentially smaller than the number of possible outcomes, is a critical matter in several applications. In this paper, we introduce a discrete entropy estimator, where we use the decomposability property in combination with estimations of the missing mass and the number of unseen outcomes to compensate for the negative bias induced by them. Experimental results show that the proposed method outperforms some classical estimators in undersampled regimes, and performs comparably with some well-established state-of-the-art estimators.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequence-to-Image Transformation for Sequence Classification Using Rips Complex Construction and Chaos Game Representation</title>
<link>https://arxiv.org/abs/2512.10141</link>
<guid>https://arxiv.org/abs/2512.10141</guid>
<content:encoded><![CDATA[
<div> Keywords: Chaos Game Representation, Rips complex, molecular sequence classification, topological data analysis, deep learning

<br /><br />Summary:  
This paper addresses the limitations of traditional feature engineering and deep learning models in molecular sequence classification, particularly in biological tabular data. It introduces a novel topological method that transforms molecular sequences into images by integrating Chaos Game Representation (CGR) with Rips complex construction from algebraic topology. The approach maps sequence elements to 2D coordinates through CGR, calculates pairwise distances between points, and constructs Rips complexes to capture both local and global topological features of sequences. The method provides formal guarantees on uniqueness of representation, topological stability, and information preservation, ensuring robustness and fidelity of the transformed data. Experimental evaluation conducted on anticancer peptide datasets demonstrates the superiority of this approach compared to traditional vector-based models, sequence language models, and existing image-based techniques. Specifically, the approach achieves 86.8% accuracy on breast cancer datasets and 94.5% accuracy on lung cancer datasets. By preserving critical sequence information topologically, this representation enables the effective application of vision-based deep learning architectures for molecular sequence analysis, offering a promising direction for computational biology and cancer research. <div>
arXiv:2512.10141v1 Announce Type: new 
Abstract: Traditional feature engineering approaches for molecular sequence classification suffer from sparsity issues and computational complexity, while deep learning models often underperform on tabular biological data. This paper introduces a novel topological approach that transforms molecular sequences into images by combining Chaos Game Representation (CGR) with Rips complex construction from algebraic topology. Our method maps sequence elements to 2D coordinates via CGR, computes pairwise distances, and constructs Rips complexes to capture both local structural and global topological features. We provide formal guarantees on representation uniqueness, topological stability, and information preservation. Extensive experiments on anticancer peptide datasets demonstrate superior performance over vector-based, sequence language models, and existing image-based methods, achieving 86.8\% and 94.5\% accuracy on breast and lung cancer datasets, respectively. The topological representation preserves critical sequence information while enabling effective utilization of vision-based deep learning architectures for molecular sequence analysis.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Murmur2Vec: A Hashing Based Solution For Embedding Generation Of COVID-19 Spike Sequences</title>
<link>https://arxiv.org/abs/2512.10147</link>
<guid>https://arxiv.org/abs/2512.10147</guid>
<content:encoded><![CDATA[
<div> Keywords: SARS-CoV-2, spike protein, sequence embedding, machine learning, lineage classification<br /><br />Summary:  
This study addresses the challenge of early detection and characterization of COVID-19 by focusing on scalable computational analysis of viral sequence data. Existing phylogenetic and embedding-based methods face issues such as high computational cost, poor scalability, dependency on aligned sequences, and suboptimal predictive performance. The research introduces a novel hashing-based embedding technique to generate compact, low-dimensional representations of SARS-CoV-2 spike protein sequences. These embeddings enable efficient supervised classification of viral lineages using various machine learning models. The method is extensively evaluated against several baseline and state-of-the-art sequence embedding approaches across multiple metrics. Results indicate that the proposed embeddings achieve a classification accuracy of up to 86.4%, significantly outperforming previous approaches. Notably, embedding generation time is reduced by as much as 99.81%, addressing large-scale data processing challenges. This combination of speed, scalability, and accuracy suggests that the new method is a valuable tool for viral sequence analysis and real-time public health monitoring. The approach enhances the ability to manage and analyze multi-million-sequence datasets, crucial during ongoing and future pandemic response efforts. <div>
arXiv:2512.10147v1 Announce Type: new 
Abstract: Early detection and characterization of coronavirus disease (COVID-19), caused by SARS-CoV-2, remain critical for effective clinical response and public-health planning. The global availability of large-scale viral sequence data presents significant opportunities for computational analysis; however, existing approaches face notable limitations. Phylogenetic tree-based methods are computationally intensive and do not scale efficiently to today's multi-million-sequence datasets. Similarly, current embedding-based techniques often rely on aligned sequences or exhibit suboptimal predictive performance and high runtime costs, creating barriers to practical large-scale analysis. In this study, we focus on the most prevalent SARS-CoV-2 lineages associated with the spike protein region and introduce a scalable embedding method that leverages hashing to generate compact, low-dimensional representations of spike sequences. These embeddings are subsequently used to train a variety of machine learning models for supervised lineage classification. We conduct an extensive evaluation comparing our approach with multiple baseline and state-of-the-art biological sequence embedding methods across diverse metrics. Our results demonstrate that the proposed embeddings offer substantial improvements in efficiency, achieving up to 86.4\% classification accuracy while reducing embedding generation time by as much as 99.81\%. This highlights the method's potential as a fast, effective, and scalable solution for large-scale viral sequence analysis.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Causal Discovery Through the Lens of Exchangeability</title>
<link>https://arxiv.org/abs/2512.10152</link>
<guid>https://arxiv.org/abs/2512.10152</guid>
<content:encoded><![CDATA[
<div> Keywords: causal discovery, exchangeability, i.i.d., synthetic dataset, neural network<br /><br />Summary:<br /><br />1. Causal discovery methods have traditionally been developed under two regimes: independent and identically distributed (i.i.d.) data and time series data, each relying on distinct modeling assumptions.<br />2. The paper argues that the i.i.d. setting can be more generally reframed using the concept of exchangeability, which is a broader symmetry principle than i.i.d.<br />3. Two main arguments support this reframing: a conceptual argument that extends the use of exchangeability from experimental causal inference to causal discovery, and an empirical argument showing that many existing i.i.d. causal discovery methods assume exchangeability.<br />4. The authors point out that the widely-used Tübingen real-world "i.i.d." benchmark dataset mainly consists of exchangeable examples, not strictly i.i.d. ones.<br />5. Based on these insights, they introduce a novel synthetic dataset that only enforces exchangeability without the stronger i.i.d. assumption, closely mirroring the statistical structure of the real-world "i.i.d." dataset better than other synthetic i.i.d. datasets.<br />6. Finally, they demonstrate the predictive power of this synthetic dataset by training a neural-network-based causal discovery algorithm exclusively on it, achieving performance comparable to state-of-the-art i.i.d. methods on the real-world benchmark. <div>
arXiv:2512.10152v1 Announce Type: new 
Abstract: Causal discovery methods have traditionally been developed under two distinct regimes: independent and identically distributed (i.i.d.) and timeseries data, each governed by separate modelling assumptions. In this paper, we argue that the i.i.d. setting can and should be reframed in terms of exchangeability, a strictly more general symmetry principle. We present the implications of this reframing, alongside two core arguments: (1) a conceptual argument, based on extending the dependency of experimental causal inference on exchangeability to causal discovery; and (2) an empirical argument, showing that many existing i.i.d. causal-discovery methods are predicated on exchangeability assumptions, and that the sole extensive widely-used real-world "i.i.d." benchmark (the T\"ubingen dataset) consists mainly of exchangeable (and not i.i.d.) examples. Building on this insight, we introduce a novel synthetic dataset that enforces only the exchangeability assumption, without imposing the stronger i.i.d. assumption. We show that our exchangeable synthetic dataset mirrors the statistical structure of the real-world "i.i.d." dataset more closely than all other i.i.d. synthetic datasets. Furthermore, we demonstrate the predictive capability of this dataset by proposing a neural-network-based causal-discovery algorithm trained exclusively on our synthetic dataset, and which performs similarly to other state-of-the-art i.i.d. methods on the real-world benchmark.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIEGAD: Cluster-Conditioned Interpolative and Extrapolative Framework for Geometry-Aware and Domain-Aligned Data Augmentation</title>
<link>https://arxiv.org/abs/2512.10178</link>
<guid>https://arxiv.org/abs/2512.10178</guid>
<content:encoded><![CDATA[
<div> Keywords: data augmentation, deep learning, domain alignment, large language models, class imbalance<br /><br />Summary: In practical deployment of deep learning models, data scarcity and imbalanced label distributions create semantically uncovered regions in real-world data, leading to challenges like misclassification near class boundaries and instability in peripheral data areas. To address this, the authors propose CIEGAD, a novel data augmentation framework that combines cluster-conditioning for domain profiling with a hierarchical frequency-geometric allocation strategy to systematically cover both in-distribution and out-of-distribution regions. CIEGAD uses both interpolative and extrapolative generation techniques to precisely control the directions of synthesized data, enhancing diversity and coverage. Quality control is ensured through geometry-constrained filtering alongside an innovative large language model-driven judging mechanism, which improves the fidelity of augmented data. Experiments across multiple classification tasks, especially long-tailed and multi-class scenarios, demonstrate that CIEGAD effectively expands the peripheral data distribution without sacrificing alignment or semantic fidelity. This leads to consistent improvements in metrics such as F1 score and recall, validating the framework's balance among distributional consistency, diversity, and data quality. Overall, CIEGAD offers a practical and integrated solution for augmenting underrepresented data regions while maintaining strong alignment with real-world distributions, thus enhancing model training and performance. <div>
arXiv:2512.10178v1 Announce Type: new 
Abstract: In practical deep learning deployment, the scarcity of data and the imbalance of label distributions often lead to semantically uncovered regions within the real-world data distribution, hindering model training and causing misclassification near class boundaries as well as unstable behaviors in peripheral areas. Although recent large language models (LLMs) show promise for data augmentation, an integrated framework that simultaneously achieves directional control of generation, domain alignment, and quality control has not yet been fully established. To address these challenges, we propose a Cluster-conditioned Interpolative and Extrapolative framework for Geometry-Aware and Domain-aligned data augmentation (CIEGAD), which systematically complements both in-distribution and out-of-distribution semantically uncovered regions. CIEGAD constructs domain profiles through cluster conditioning, allocates generation with a hierarchical frequency-geometric allocation integrating class frequency and geometric indicators, and finely controls generation directions via the coexistence of interpolative and extrapolative synthesis. It further performs quality control through geometry-constrained filtering combined with an LLM-as-a-Judge mechanism. Experiments on multiple classification tasks demonstrate that CIEGAD effectively extends the periphery of real-world data distributions while maintaining high alignment between generated and real-world data as well as semantic diversity. In particular, for long-tailed and multi-class classification tasks, CIEGAD consistently improves F1 and recall, validating the triple harmony of distributional consistency, diversity, and quality. These results indicate that CIEGAD serves as a practically oriented data augmentation framework that complements underrepresented regions while preserving alignment with real-world data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Neuromorphic Computing for Fingertip Force Decoding from Electromyography</title>
<link>https://arxiv.org/abs/2512.10179</link>
<guid>https://arxiv.org/abs/2512.10179</guid>
<content:encoded><![CDATA[
<div> High-density surface electromyography, Spiking neural network, Temporal convolutional network, Motor-unit firing, Fingertip force decoding<br /><br />Summary:<br /><br />This paper evaluates the performance of a spiking neural network (SNN) against a temporal convolutional network (TCN) for decoding fingertip force using motor-unit firing data derived from high-density surface electromyography (HD-sEMG). HD-sEMG offers a noninvasive method for neural interfacing in assistive and rehabilitation technologies, but accurately mapping neural activity to motor intent is difficult. The study collected data from a single participant across 10 trials, utilizing two electrode arrays on the forearm to record MU activity, which was extracted through FastICA-based decomposition. Both models were trained using overlapping windows with end-to-end causal convolutions to predict fingertip force. Results on held-out trials showed the TCN outperformed the SNN, achieving a root mean squared error (RMSE) of 4.44% of maximum voluntary contraction (MVC) and a Pearson correlation coefficient of 0.974, compared to the SNN’s 8.25% MVC RMSE and 0.922 correlation. Despite the lower accuracy of the SNN, the authors suggest it serves as a viable neuromorphic baseline, with potential for performance improvements through architectural and hyperparameter tuning. The work highlights the promise of neuromorphic computing in HD-sEMG decoding applications. <div>
arXiv:2512.10179v1 Announce Type: new 
Abstract: High-density surface electromyography (HD-sEMG) provides a noninvasive neural interface for assistive and rehabilitation control, but mapping neural activity to user motor intent remains challenging. We assess a spiking neural network (SNN) as a neuromorphic architecture against a temporal convolutional network (TCN) for decoding fingertip force from motor-unit (MU) firing derived from HD-sEMG. Data were collected from a single participant (10 trials) with two forearm electrode arrays; MU activity was obtained via FastICA-based decomposition, and models were trained on overlapping windows with end-to-end causal convolutions. On held-out trials, the TCN achieved 4.44% MVC RMSE (Pearson r = 0.974) while the SNN achieved 8.25% MVC (r = 0.922). While the TCN was more accurate, we view the SNN as a realistic neuromorphic baseline that could close much of this gap with modest architectural and hyperparameter refinements.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniF2F-Dafny: LLM-Guided Mathematical Theorem Proving via Auto-Active Verification</title>
<link>https://arxiv.org/abs/2512.10187</link>
<guid>https://arxiv.org/abs/2512.10187</guid>
<content:encoded><![CDATA[
<div> Keywords: miniF2F-Dafny, automated theorem prover, Dafny, large language models, mathematical reasoning benchmark  

<br /><br />Summary:  
1. The paper introduces miniF2F-Dafny, the first translation of the miniF2F mathematical reasoning benchmark into the automated theorem prover Dafny.  
2. Previously, miniF2F was only available in several interactive theorem provers such as Lean, Isabelle, HOL Light, and Metamath, limiting automated proof capabilities.  
3. Dafny’s automation was able to verify 40.6% (99 out of 244) of the test set and 44.7% (109 out of 244) of the validation set using empty proofs, meaning no manual proof steps were required.  
4. For problems where Dafny’s empty proofs failed, the authors tested 12 off-the-shelf large language models (LLMs) to generate proof hints and guide the prover.  
5. The best performing LLM achieved a pass@4 success rate of 55.7% by applying iterative error correction, demonstrating that a combination of LLM-provided high-level guidance and automated low-level proof checking is effective.  
6. The benchmark and all associated resources are publicly available on GitHub at http://github.com/dafny-lang/miniF2F, facilitating further research and development in automated theorem proving. <div>
arXiv:2512.10187v1 Announce Type: new 
Abstract: We present miniF2F-Dafny, the first translation of the mathematical reasoning benchmark miniF2F to an automated theorem prover: Dafny. Previously, the benchmark existed only in interactive theorem provers (Lean, Isabelle, HOL Light, Metamath). We find that Dafny's automation verifies 99/244 (40.6%) of the test set and 109/244 (44.7%) of the validation set with empty proofs--requiring no manual proof steps. For problems where empty proofs fail, we evaluate 12 off-the-shelf LLMs on providing proof hints. The best model we test achieves 55.7% pass@4 success rate employing iterative error correction. These preliminary results highlight an effective division of labor: LLMs provide high-level guidance while automation handles low-level details. Our benchmark can be found on GitHub at http://github.com/dafny-lang/miniF2F .
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Recovery of Non-Random Missing Multidimensional Time Series via Temporal Isometric Delay-Embedding Transform</title>
<link>https://arxiv.org/abs/2512.10191</link>
<guid>https://arxiv.org/abs/2512.10191</guid>
<content:encoded><![CDATA[
<div> Keywords: non-random missing data, Hankel tensor completion, temporal isometric delay-embedding transform, low-rank tensor completion, exact recovery theory<br /><br />Summary:<br /><br />This paper addresses the challenge of non-random missing data in multidimensional time series, a common and problematic issue for data-driven analysis and decision-making. Traditional low-rank tensor completion methods are inadequate for such non-random missingness due to methodological and theoretical limitations. The authors introduce the temporal isometric delay-embedding transform (TIDT), which constructs a Hankel tensor whose low-rank structure is inherently related to the smoothness and periodicity of the underlying time series data. Building on this, they propose the LRTC-TIDT model within the Tensor Singular Value Decomposition (t-SVD) framework to effectively capture and exploit this low-rankness. Under prescribed non-random sampling conditions and mild incoherence assumptions, the model guarantees exact recovery of missing data, an advancement supported by extensive simulation experiments across various missing data patterns. Moreover, LRTC-TIDT demonstrates superior performance over existing tensor-based methods in several practical applications, including network flow reconstruction, urban traffic estimation, and temperature field prediction. The authors also provide a publicly available implementation of their method, facilitating further use and development. This study thus makes a significant theoretical and practical contribution to the field of multidimensional time series data recovery under non-random missing conditions. <div>
arXiv:2512.10191v1 Announce Type: new 
Abstract: Non-random missing data is a ubiquitous yet undertreated flaw in multidimensional time series, fundamentally threatening the reliability of data-driven analysis and decision-making. Pure low-rank tensor completion, as a classical data recovery method, falls short in handling non-random missingness, both methodologically and theoretically. Hankel-structured tensor completion models provide a feasible approach for recovering multidimensional time series with non-random missing patterns. However, most Hankel-based multidimensional data recovery methods both suffer from unclear sources of Hankel tensor low-rankness and lack an exact recovery theory for non-random missing data. To address these issues, we propose the temporal isometric delay-embedding transform, which constructs a Hankel tensor whose low-rankness is naturally induced by the smoothness and periodicity of the underlying time series. Leveraging this property, we develop the \textit{Low-Rank Tensor Completion with Temporal Isometric Delay-embedding Transform} (LRTC-TIDT) model, which characterizes the low-rank structure under the \textit{Tensor Singular Value Decomposition} (t-SVD) framework. Once the prescribed non-random sampling conditions and mild incoherence assumptions are satisfied, the proposed LRTC-TIDT model achieves exact recovery, as confirmed by simulation experiments under various non-random missing patterns. Furthermore, LRTC-TIDT consistently outperforms existing tensor-based methods across multiple real-world tasks, including network flow reconstruction, urban traffic estimation, and temperature field prediction. Our implementation is publicly available at https://github.com/HaoShu2000/LRTC-TIDT.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Domain Generalization with Latent Space Inversion</title>
<link>https://arxiv.org/abs/2512.10224</link>
<guid>https://arxiv.org/abs/2512.10224</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated domain generalization, latent space inversion, important weight aggregation, data privacy, model aggregation<br /><br />Summary: Federated domain generalization (FedDG) aims to address distribution shifts among clients within a federated learning setup by creating a global model that generalizes to unseen clients while safeguarding data privacy. Traditional FedDG methods aggregate client model parameters but often compromise privacy by sharing client data statistics. This paper introduces novel techniques to enhance privacy and model performance. First, it proposes latent space inversion during local client training to enforce domain invariance and strengthen privacy protections. Second, it tackles the challenge of non-i.i.d client data by introducing an important weight aggregation strategy, which prioritizes model parameters that are most influential in local predictions, preserving critical local adaptations during aggregation. Extensive experiments demonstrate that the proposed approach outperforms state-of-the-art FedDG methods, achieving better generalization to unseen clients. Additionally, the method reduces communication overhead, making it more efficient in federated environments. Overall, the contributions include improved local training for privacy, a refined aggregation mechanism to handle client heterogeneity, and validated superior results through thorough experimentation. <div>
arXiv:2512.10224v1 Announce Type: new 
Abstract: Federated domain generalization (FedDG) addresses distribution shifts among clients in a federated learning framework. FedDG methods aggregate the parameters of locally trained client models to form a global model that generalizes to unseen clients while preserving data privacy. While improving the generalization capability of the global model, many existing approaches in FedDG jeopardize privacy by sharing statistics of client data between themselves. Our solution addresses this problem by contributing new ways to perform local client training and model aggregation. To improve local client training, we enforce (domain) invariance across local models with the help of a novel technique, \textbf{latent space inversion}, which enables better client privacy. When clients are not \emph{i.i.d}, aggregating their local models may discard certain local adaptations. To overcome this, we propose an \textbf{important weight} aggregation strategy to prioritize parameters that significantly influence predictions of local models during aggregation. Our extensive experiments show that our approach achieves superior results over state-of-the-art methods with less communication overhead.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Information Routing for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.10229</link>
<guid>https://arxiv.org/abs/2512.10229</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal forecasting, time series, text data, Adaptive Information Routing, large language model<br /><br />Summary:  
Time series forecasting plays a crucial role in artificial intelligence with numerous practical applications. Traditional methods rely solely on historical time series data but often face challenges due to limited information. To overcome these challenges, multimodal forecasting approaches incorporating additional data types, particularly text, have been explored. The paper introduces the Adaptive Information Routing (AIR) framework, a novel technique that dynamically guides the time series model by controlling the integration of multivariate time series and textual information, rather than treating text as a simple auxiliary feature. Additionally, the authors propose a text-refinement pipeline utilizing a large language model to transform raw text into a suitable format for multimodal forecasting. A new benchmark is also established to facilitate experiments based on this pipeline. Experiments conducted on real-world market data, including crude oil prices and exchange rates, show that AIR effectively modulates the time series model behavior through textual inputs. This dynamic modulation significantly improves forecasting accuracy across various time series tasks, demonstrating the potential of combining text data and time series data via AIR for better predictive performance. <div>
arXiv:2512.10229v1 Announce Type: new 
Abstract: Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R^2-HGP: A Double-Regularized Gaussian Process for Heterogeneous Transfer Learning</title>
<link>https://arxiv.org/abs/2512.10258</link>
<guid>https://arxiv.org/abs/2512.10258</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-output Gaussian process, Transfer learning, Heterogeneous input alignment, Conditional variational autoencoder, Negative transfer<br /><br />Summary:<br /><br />This paper addresses key challenges in multi-source transfer learning using Multi-output Gaussian Process (MGP) models. First, it tackles the problem of heterogeneous input spaces between source and target domains by introducing a trainable prior probability mapping model that aligns these inputs into a latent space. Second, it incorporates physical insights as a regularization term to ensure the alignment respects known domain-specific knowledge, improving stability and interpretability. Third, to prevent negative transfer caused by irrelevant or harmful information sharing, a sparsity penalty is imposed on transfer coefficients within the multi-source transfer GP model, enabling adaptive selection of the most informative sources. These components are unified within a novel conditional variational autoencoder (CVAE) framework, providing an end-to-end solution for heterogeneous transfer learning. Extensive simulations and engineering case studies demonstrate the proposed Double-Regularized Heterogeneous Gaussian Process framework (R²-HGP) consistently outperforms state-of-the-art models across various metrics, highlighting its effectiveness and robustness in practical applications. <div>
arXiv:2512.10258v1 Announce Type: new 
Abstract: Multi-output Gaussian process (MGP) models have attracted significant attention for their flexibility and uncertainty-quantification capabilities, and have been widely adopted in multi-source transfer learning scenarios due to their ability to capture inter-task correlations. However, they still face several challenges in transfer learning. First, the input spaces of the source and target domains are often heterogeneous, which makes direct knowledge transfer difficult. Second, potential prior knowledge and physical information are typically ignored during heterogeneous transfer, hampering the utilization of domain-specific insights and leading to unstable mappings. Third, inappropriate information sharing among target and sources can easily lead to negative transfer. Traditional models fail to address these issues in a unified way. To overcome these limitations, this paper proposes a Double-Regularized Heterogeneous Gaussian Process framework (R^2-HGP). Specifically, a trainable prior probability mapping model is first proposed to align the heterogeneous input domains. The resulting aligned inputs are treated as latent variables, upon which a multi-source transfer GP model is constructed and the entire structure is integrated into a novel conditional variational autoencoder (CVAE) based framework. Physical insights is further incorporated as a regularization term to ensure that the alignment results adhere to known physical knowledge. Next, within the multi-source transfer GP model, a sparsity penalty is imposed on the transfer coefficients, enabling the model to adaptively select the most informative source outputs and suppress negative transfer. Extensive simulations and real-world engineering case studies validate the effectiveness of our R^2-HGP, demonstrating consistent superiority over state-of-the-art benchmarks across diverse evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Kernel-based Resource-efficient Neural Surrogate for Multi-fidelity Prediction of Aerodynamic Field</title>
<link>https://arxiv.org/abs/2512.10287</link>
<guid>https://arxiv.org/abs/2512.10287</guid>
<content:encoded><![CDATA[
<div> Surrogate models, KHRONOS, multi-fidelity, aerodynamic prediction, neural networks<br /><br />Summary:<br /><br />This study introduces KHRONOS, a kernel-based neural surrogate model designed to efficiently predict aerodynamic fields by combining sparse high-fidelity data with abundant low-fidelity information. Unlike traditional dense neural networks, KHRONOS leverages variational principles, interpolation theory, and tensor decomposition, enabling significant model pruning and computational savings. The research evaluates KHRONOS against three other models: multilayer perceptron (MLP), graph neural network (GNN), and physics-informed neural network (PINN), across different scenarios with 0%, 10%, and 30% high-fidelity data availability and increasing geometric complexity. Using the AirfRANS dataset as high-fidelity reference and NeuralFoil for generating low-fidelity data, the study focuses on predicting the surface pressure coefficient distribution over airfoils. While all models achieve similar accuracy eventually, KHRONOS stands out in resource-constrained environments by requiring orders of magnitude fewer trainable parameters and enabling faster training and inference times. This demonstrates KHRONOS's strong potential to balance predictive accuracy with computational efficiency in multi-fidelity aerodynamic simulations, which is particularly advantageous for design optimization workflows where computational resources are limited. <div>
arXiv:2512.10287v1 Announce Type: new 
Abstract: Surrogate models provide fast alternatives to costly aerodynamic simulations and are extremely useful in design and optimization applications. This study proposes the use of a recent kernel-based neural surrogate, KHRONOS. In this work, we blend sparse high-fidelity (HF) data with low-fidelity (LF) information to predict aerodynamic fields under varying constraints in computational resources. Unlike traditional approaches, KHRONOS is built upon variational principles, interpolation theory, and tensor decomposition. These elements provide a mathematical basis for heavy pruning compared to dense neural networks. Using the AirfRANS dataset as a high-fidelity benchmark and NeuralFoil to generate low-fidelity counterparts, this work compares the performance of KHRONOS with three contemporary model architectures: a multilayer perceptron (MLP), a graph neural network (GNN), and a physics-informed neural network (PINN). We consider varying levels of high-fidelity data availability (0%, 10%, and 30%) and increasingly complex geometry parameterizations. These are used to predict the surface pressure coefficient distribution over the airfoil. Results indicate that, whilst all models eventually achieve comparable predictive accuracy, KHRONOS excels in resource-constrained conditions. In this domain, KHRONOS consistently requires orders of magnitude fewer trainable parameters and delivers much faster training and inference than contemporary dense neural networks at comparable accuracy. These findings highlight the potential of KHRONOS and similar architectures to balance accuracy and efficiency in multi-fidelity aerodynamic field prediction.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Interpretable AI Tool for SAVR vs TAVR in Low to Intermediate Risk Patients with Severe Aortic Stenosis</title>
<link>https://arxiv.org/abs/2512.10308</link>
<guid>https://arxiv.org/abs/2512.10308</guid>
<content:encoded><![CDATA[
<div> Keywords: aortic stenosis, SAVR, TAVR, Optimal Policy Tree, counterfactual modeling<br /><br />Summary:<br /><br />1. The study addresses treatment selection challenges between surgical (SAVR) and transcatheter (TAVR) aortic valve replacement in low to intermediate risk patients with severe aortic stenosis, noting variability in clinical practice due to patient heterogeneity and institutional preferences.  
2. Existing predictive models focus on postprocedural risk but lack interpretable, individualized treatment recommendations aimed at optimizing long-term outcomes.  
3. The authors propose an interpretable prescriptive framework that combines prognostic matching, counterfactual outcome modeling, and an Optimal Policy Tree (OPT) to recommend treatments that minimize expected 5-year mortality.  
4. Data from Hartford Hospital and St. Vincent's Hospital were used; the approach emulated randomization via prognostic matching and sample weighting to estimate counterfactual mortality under both SAVR and TAVR treatments.  
5. The OPT model partitions patients into clinically meaningful subgroups and prescribes the treatment with the lower estimated mortality risk.  
6. Application of OPT-based prescriptions showed an estimated reduction in 5-year mortality of 20.3% at Hartford and 13.8% at St. Vincent's compared to actual treatment decisions, indicating good external generalizability.  
7. The learned decision boundaries corresponded well with clinical observations and real-world outcomes, supporting the model’s interpretability and clinical relevance.  
8. This framework represents the first transparent, data-driven tool to guide SAVR versus TAVR treatment decisions that improve long-term estimated outcomes while supporting precision medicine in structural heart disease. <div>
arXiv:2512.10308v1 Announce Type: new 
Abstract: Background. Treatment selection for low to intermediate risk patients with severe aortic stenosis between surgical (SAVR) and transcatheter (TAVR) aortic valve replacement remains variable in clinical practice, driven by patient heterogeneity and institutional preferences. While existing models predict postprocedural risk, there is a lack of interpretable, individualized treatment recommendations that directly optimize long-term outcomes.
  Methods. We introduce an interpretable prescriptive framework that integrates prognostic matching, counterfactual outcome modeling, and an Optimal Policy Tree (OPT) to recommend the treatment minimizing expected 5-year mortality. Using data from Hartford Hospital and St. Vincent's Hospital, we emulate randomization via prognostic matching and sample weighting and estimate counterfactual mortality under both SAVR and TAVR. The policy model, trained on these counterfactual predictions, partitions patients into clinically coherent subgroups and prescribes the treatment associated with lower estimated risk.
  Findings. If the OPT prescriptions are applied, counterfactual evaluation showed an estimated reduction in 5-year mortality of 20.3\% in Hartford and 13.8\% in St. Vincent's relative to real-life prescriptions, showing promising generalizability to unseen data from a different institution. The learned decision boundaries aligned with real-world outcomes and clinical observations.
  Interpretation. Our interpretable prescriptive framework is, to the best of our knowledge, the first to provide transparent, data-driven recommendations for TAVR versus SAVR that improve estimated long-term outcomes both in an internal and external cohort, while remaining clinically grounded and contributing toward a more systematic and evidence-based approach to precision medicine in structural heart disease.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale</title>
<link>https://arxiv.org/abs/2512.10341</link>
<guid>https://arxiv.org/abs/2512.10341</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, differential privacy, zero-knowledge proofs, multi-cloud, adaptive governance<br /><br />Summary:<br /><br />This paper presents a cloud-native architecture designed for privacy-preserving distributed machine learning across heterogeneous and multi-cloud environments. The system integrates federated learning to enable secure model training and inference without centralizing sensitive data, ensuring data privacy at its core. Differential privacy mechanisms are incorporated to guarantee formal privacy budgets and reduce membership-inference risks during model development. Zero-knowledge compliance proofs are used to cryptographically verify that privacy policies are consistently enforced across different institutions and cloud platforms. Adaptive governance is implemented through reinforcement learning to enable continuous, risk-aware management of privacy and policy enforcement. A full prototype is deployed on hybrid Kubernetes clusters demonstrating practical feasibility and scalability. Experimental evaluations on multi-institution workloads highlight that the framework maintains strong model utility and performance while adding minimal computational overhead. The results confirm stable performance under privacy constraints and effective, verifiable policy enforcement, facilitating trustworthy and compliant distributed learning. Overall, this framework establishes a robust foundation for scalable deployment of privacy-aware distributed machine learning systems with verifiable compliance and adaptive management capabilities across diverse cloud infrastructures. <div>
arXiv:2512.10341v1 Announce Type: new 
Abstract: Distributed machine learning systems require strong privacy guarantees, verifiable compliance, and scalable deploy- ment across heterogeneous and multi-cloud environments. This work introduces a cloud-native privacy-preserving architecture that integrates federated learning, differential privacy, zero- knowledge compliance proofs, and adaptive governance powered by reinforcement learning. The framework supports secure model training and inference without centralizing sensitive data, while enabling cryptographically verifiable policy enforcement across institutions and cloud platforms. A full prototype deployed across hybrid Kubernetes clusters demonstrates reduced membership- inference risk, consistent enforcement of formal privacy budgets, and stable model performance under differential privacy. Ex- perimental evaluation across multi-institution workloads shows that the architecture maintains utility with minimal overhead while providing continuous, risk-aware governance. The pro- posed framework establishes a practical foundation for deploying trustworthy and compliant distributed machine learning systems at scale.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories</title>
<link>https://arxiv.org/abs/2512.10350</link>
<guid>https://arxiv.org/abs/2512.10350</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic systems, large language models, semantic embedding space, dynamical systems, prompt design<br /><br />Summary: This paper investigates the dynamic behavior of agentic systems built on large language models (LLMs), where outputs recursively feed as inputs in loops. The study introduces a geometric framework to analyze these iterative transformations within semantic embedding spaces, differentiating the artifact space (where linguistic manipulation occurs) from the embedding space (used for geometric measurements). Recognizing the bias in cosine similarity caused by embedding anisotropy, the authors propose an isotonic calibration method that removes this bias, aligns similarity metrics with human semantic judgments, and maintains local stability. Using this calibrated framework, they rigorously measure trajectories, clusters, and attractors formed in the embedding space during agentic loops. Controlled experiments reveal two primary dynamical regimes: a contractive rewriting loop that converges to a stable attractor with decreasing dispersion, and an exploratory summarize-and-negate loop that diverges without forming clusters. Each regime displays distinct geometric signatures indicative of contraction or expansion. Crucially, the results demonstrate that prompt design directly influences the dynamical regime of the agentic loop, allowing systematic control over convergence, divergence, and the overall structure of trajectories in iterative LLM transformations. This work offers valuable insights into controlling LLM-based agentic system behaviors through prompt engineering. <div>
arXiv:2512.10350v1 Announce Type: new 
Abstract: Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems. We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors. Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion. Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better Prevent than Tackle: Valuing Defense in Soccer Based on Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.10355</link>
<guid>https://arxiv.org/abs/2512.10355</guid>
<content:encoded><![CDATA[
<div> Defensive performance, soccer analytics, Graph Attention Networks, Expected Possession Value, player evaluation<br /><br />Summary:<br /><br />1. The article addresses the challenge of evaluating defensive performance in soccer, highlighting that effective defending often occurs through preventing dangerous opportunities rather than visible on-ball actions like tackles or interceptions.  
2. It critiques existing approaches for focusing mainly on valuing on-ball defensive actions, which fail to capture defenders' full impact on the game.  
3. The authors propose DEFCON (DEFensive CONtribution evaluator), a novel framework that quantifies defensive contributions at the player level across every attacking situation using advanced machine learning techniques.  
4. DEFCON employs Graph Attention Networks to estimate the success probability and expected value of each attacking option, in addition to assigning responsibility scores to defenders for preventing these options.  
5. By calculating the Expected Possession Value (EPV) before and after actions, DEFCON credits defenders positively or negatively based on their impact on reducing or increasing the opponent's EPV.  
6. The model is trained on 2023-24 and tested on 2024-25 Eredivisie event and tracking data, showing strong positive correlations between aggregated defensive credits and player market valuations.  
7. Practical applications demonstrated include creating defensive contribution timelines during matches, spatial analyses of defensive performance by pitch zones, and summarizing attacker-defender pairwise interactions, offering rich insights for teams, analysts, and scouts. <div>
arXiv:2512.10355v1 Announce Type: new 
Abstract: Evaluating defensive performance in soccer remains challenging, as effective defending is often expressed not through visible on-ball actions such as interceptions and tackles, but through preventing dangerous opportunities before they arise. Existing approaches have largely focused on valuing on-ball actions, leaving much of defenders' true impact unmeasured. To address this gap, we propose DEFCON (DEFensive CONtribution evaluator), a comprehensive framework that quantifies player-level defensive contributions for every attacking situation in soccer. Leveraging Graph Attention Networks, DEFCON estimates the success probability and expected value of each attacking option, along with each defender's responsibility for stopping it. These components yield an Expected Possession Value (EPV) for the attacking team before and after each action, and DEFCON assigns positive or negative credits to defenders according to whether they reduced or increased the opponent's EPV. Trained on 2023-24 and evaluated on 2024-25 Eredivisie event and tracking data, DEFCON's aggregated player credits exhibit strong positive correlations with market valuations. Finally, we showcase several practical applications, including in-game timelines of defensive contributions, spatial analyses across pitch zones, and pairwise summaries of attacker-defender interactions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPG: Generalized Policy Gradient Theorem for Transformer-based Policies</title>
<link>https://arxiv.org/abs/2512.10365</link>
<guid>https://arxiv.org/abs/2512.10365</guid>
<content:encoded><![CDATA[
<div> Generalized Policy Gradient, Transformer-based policies, Policy Gradient Theorem, GRPO, Large Language Models<br /><br />
Summary:<br /><br />
1. The article introduces the Generalized Policy Gradient (GPG) Theorem, a novel theoretical framework tailored specifically for Transformer-based policies. 2. It establishes that both the traditional Policy Gradient Theorem and Generalized Regularized Policy Optimization (GRPO) can be derived as special cases from the broader GPG framework. 3. This unification underscores the flexibility and generality of the GPG approach in encompassing existing reinforcement learning methods. 4. The study further investigates practical applications of GPG in training Large Language Models (LLMs), illustrating how this theorem can lead to more efficient policy optimization strategies. 5. Overall, the work offers new insights and tools to improve reinforcement learning processes in the context of advanced Transformer architectures, potentially enhancing the performance and training efficiency of LLMs. <div>
arXiv:2512.10365v1 Announce Type: new 
Abstract: We present the Generalized Policy Gradient (GPG) Theorem, specifically designed for Transformer-based policies. Notably, we demonstrate that both standard Policy Gradient Theorem and GRPO emerge as special cases within our GPG framework. Furthermore, we explore its practical applications in training Large Language Models (LLMs), offering new insights into efficient policy optimization.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fitting magnetization data using continued fraction of straight lines</title>
<link>https://arxiv.org/abs/2512.10390</link>
<guid>https://arxiv.org/abs/2512.10390</guid>
<content:encoded><![CDATA[
<div> Magnetization, Ferromagnetic domains, Nonlinearity, Continued fraction, Nonlinear regression<br /><br />Summary:<br /><br />This paper discusses the nonlinear relationship between the magnetization of a ferromagnetic material and an externally applied magnetic field. The magnetization increases as the magnetic field strengthens because magnetic moments within microscopic regions or domains align more closely with the applied field, thus reducing the number of misaligned domains. This domain alignment is the physical basis of the nonlinear behavior observed in magnetization curves. To model this nonlinearity, the authors propose approximating the magnetization function as a combination of continued fractions composed of straight lines. This algebraic expression offers a novel way to fit the nonlinear magnetization data. By applying nonlinear regression techniques, the parameters of this continued fraction model can be estimated effectively. Furthermore, the model provides insights into the behavior of both growing and shrinking magnetic domains under varying magnetic field strengths. This approach allows for a better understanding and quantification of the magnetization process, which has implications for magnetic material characterization and the design of related technologies. <div>
arXiv:2512.10390v1 Announce Type: new 
Abstract: Magnetization of a ferromagnetic substance in response to an externally applied magnetic field increases with the strength of the field. This is because at the microscopic level, magnetic moments in certain regions or domains of the substance increasingly align with the applied field, while the amount of misaligned domains decreases. The alignment of such magnetic domains with an applied magnetic field forms the physical basis for the nonlinearity of magnetization. In this paper, the nonlinear function is approximated as a combination of continued fraction of straight lines. The resulting fit is used to interpret the nonlinear behavior in both growing and shrinking magnetic domains. The continued fraction of straight lines used here is an algebraic expression which can be used to estimate parameters using nonlinear regression.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title>
<link>https://arxiv.org/abs/2512.10402</link>
<guid>https://arxiv.org/abs/2512.10402</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor attacks, sparse decision boundaries, influence function, Eminence framework, low poison rate<br /><br />Summary:<br /><br />Deep neural networks (DNNs) are critical yet vulnerable to backdoor attacks, which often rely on heuristic brute-force methods lacking theoretical grounding. This paper provides a rigorous theoretical analysis of backdoor attacks, revealing that sparse decision boundaries enable disproportionate manipulation of models through minimal relabeled samples. The authors derive a closed-form ambiguous boundary region where even negligible poisoning can cause substantial misclassification. Using influence function analysis, they show that these margin poison samples cause significant parameter changes without harming clean accuracy, explaining why extremely low poison rates can still be effective. Building on these insights, the paper introduces Eminence, a black-box backdoor attack framework offering explainability, robustness, and provable theoretical guarantees alongside inherent stealth. Eminence designs a universal, visually subtle trigger that exploits vulnerable decision boundaries to ensure strong misclassification with poison rates below 0.1%, outperforming state-of-the-art methods that often require over 1% poisoning. Extensive experiments confirm the theoretical findings, demonstrating an exponential relationship between margin poisoning and adversarial boundary shifts. Eminence achieves over 90% attack success rates, minimal impact on clean accuracy, and high transferability across diverse models, datasets, and attack settings, marking a significant advancement in both understanding and attacking DNN backdoors. <div>
arXiv:2512.10402v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) underpin critical applications yet remain vulnerable to backdoor attacks, typically reliant on heuristic brute-force methods. Despite significant empirical advancements in backdoor research, the lack of rigorous theoretical analysis limits understanding of underlying mechanisms, constraining attack predictability and adaptability. Therefore, we provide a theoretical analysis targeting backdoor attacks, focusing on how sparse decision boundaries enable disproportionate model manipulation. Based on this finding, we derive a closed-form, ambiguous boundary region, wherein negligible relabeled samples induce substantial misclassification. Influence function analysis further quantifies significant parameter shifts caused by these margin samples, with minimal impact on clean accuracy, formally grounding why such low poison rates suffice for efficacious attacks. Leveraging these insights, we propose Eminence, an explainable and robust black-box backdoor framework with provable theoretical guarantees and inherent stealth properties. Eminence optimizes a universal, visually subtle trigger that strategically exploits vulnerable decision boundaries and effectively achieves robust misclassification with exceptionally low poison rates (< 0.1%, compared to SOTA methods typically requiring > 1%). Comprehensive experiments validate our theoretical discussions and demonstrate the effectiveness of Eminence, confirming an exponential relationship between margin poisoning and adversarial boundary manipulation. Eminence maintains > 90% attack success rate, exhibits negligible clean-accuracy loss, and demonstrates high transferability across diverse models, datasets and scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Operator Origins of Neural Scaling Laws: A Generalized Spectral Transport Dynamics of Deep Learning</title>
<link>https://arxiv.org/abs/2512.10427</link>
<guid>https://arxiv.org/abs/2512.10427</guid>
<content:encoded><![CDATA[
<div> Keywords: neural training dynamics, operator theory, spectral transport PDE, representation drift, scaling laws<br /><br />Summary:<br /><br />This paper develops a unified operator-theoretic framework to describe neural network training dynamics directly from gradient descent in function space. The authors start from the exact evolution equation \(\dot e_t = -M(t)e_t\) and use Kato perturbation theory to derive a coupled system of mode ODEs, which converge after coarse-graining to a spectral transport–dissipation PDE. This PDE characterizes how eigenbasis drift and nonlocal spectral coupling govern training dynamics. The study proves that neural training preserves functional regularity, forcing the eigenbasis drift velocity \(v(\lambda,t)\) to follow an asymptotic power-law form in eigenvalue \(\lambda\). In regimes with weak spectral coupling—naturally arising from spectral locality and SGD noise—the PDE admits self-similar solutions that exhibit a resolution frontier, polynomial amplitude growth, and power-law dissipation. These mathematical structures yield explicit scaling-law exponents and provide a theoretical explanation for phenomena such as double descent in generalization behavior. The concept of effective training time is shown to evolve as \(\tau(t) = t^\alpha L(t)\), with \(L(t)\) a slowly varying function. Finally, the authors demonstrate that the commonly studied Neural Tangent Kernel (NTK) training and feature learning emerge as two limiting cases of the same PDE framework—lazy training corresponds to zero drift, while non-zero drift encodes representation learning—thus unifying these perspectives under a spectral operator formalism. <div>
arXiv:2512.10427v1 Announce Type: new 
Abstract: Modern deep networks operate in a rough, finite-regularity regime where Jacobian-induced operators exhibit heavy-tailed spectra and strong basis drift. In this work, we derive a unified operator-theoretoretic description of neural training dynamics directly from gradient descent. Starting from the exact evolution $\dot e_t = -M(t)e_t$ in function space, we apply Kato perturbation theory to obtain a rigorous system of coupled mode ODEs and show that, after coarse-graining, these dynamics converge to a spectral transport--dissipation PDE \[ \partial_t g + \partial_\lambda (v g) = -\lambda g + S, \] where $v$ captures eigenbasis drift and $S$ encodes nonlocal spectral coupling.
  We prove that neural training preserves functional regularity, forcing the drift to take an asymptotic power-law form $v(\lambda,t)\sim -c(t)\lambda^b$. In the weak-coupling regime -- naturally induced by spectral locality and SGD noise -- the PDE admits self-similar solutions with a resolution frontier, polynomial amplitude growth, and power-law dissipation. This structure yields explicit scaling-law exponents, explains the geometry of double descent, and shows that the effective training time satisfies $\tau(t)=t^\alpha L(t)$ for slowly varying $L$.
  Finally, we show that NTK training and feature learning arise as two limits of the same PDE: $v\equiv 0$ recovers lazy dynamics, while $v\neq 0$ produces representation drift. Our results provide a unified spectral framework connecting operator geometry, optimization dynamics, and the universal scaling behavior of modern deep networks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metacognitive Sensitivity for Test-Time Dynamic Model Selection</title>
<link>https://arxiv.org/abs/2512.10451</link>
<guid>https://arxiv.org/abs/2512.10451</guid>
<content:encoded><![CDATA[
<div> Keywords: metacognition, deep learning, confidence calibration, meta-d', model selection  

<br /><br />Summary:  
This paper addresses the challenge of calibrating confidence in deep learning models, focusing on the concept of metacognition — a model's ability to assess the reliability of its own predictions. First, it introduces meta-d', a measure inspired by human cognitive science, which quantifies how well a model's confidence correlates with its actual accuracy, termed metacognitive sensitivity. Second, the authors propose using this dynamic metacognitive sensitivity as contextual information within a bandit-based arbiter framework for test-time model selection. This arbiter learns to choose among multiple expert models, optimizing trust based on confidence reliability. Third, the approach is evaluated across diverse datasets and model architectures, including convolutional neural networks (CNNs) and vision-language models (VLMs). Fourth, experiments demonstrate that leveraging metacognitive sensitivity enhances joint inference accuracy compared to relying on individual models alone. Finally, this work reframes ensemble selection by incorporating both short-term confidence signals and medium-term metacognitive traits, providing a novel behavioral perspective on AI model evaluation and integration. <div>
arXiv:2512.10451v1 Announce Type: new 
Abstract: A key aspect of human cognition is metacognition - the ability to assess one's own knowledge and judgment reliability. While deep learning models can express confidence in their predictions, they often suffer from poor calibration, a cognitive bias where expressed confidence does not reflect true competence. Do models truly know what they know? Drawing from human cognitive science, we propose a new framework for evaluating and leveraging AI metacognition. We introduce meta-d', a psychologically-grounded measure of metacognitive sensitivity, to characterise how reliably a model's confidence predicts its own accuracy. We then use this dynamic sensitivity score as context for a bandit-based arbiter that performs test-time model selection, learning which of several expert models to trust for a given task. Our experiments across multiple datasets and deep learning model combinations (including CNNs and VLMs) demonstrate that this metacognitive approach improves joint-inference accuracy over constituent models. This work provides a novel behavioural account of AI models, recasting ensemble selection as a problem of evaluating both short-term signals (confidence prediction scores) and medium-term traits (metacognitive sensitivity).
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Physics-ML Model for Forward Osmosis Flux with Complete Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2512.10457</link>
<guid>https://arxiv.org/abs/2512.10457</guid>
<content:encoded><![CDATA[
<div> Forward Osmosis, Gaussian Process Regression, Uncertainty Quantification, Hybrid Physics-ML, Water Flux Prediction<br /><br />Summary:<br /><br />This study addresses the challenge of accurately modeling water flux (Jw) in Forward Osmosis (FO), a promising low-energy membrane separation technology, where traditional mechanistic models and purely data-driven models have limitations. The authors propose a novel Robust Hybrid Physics-ML framework that uses Gaussian Process Regression (GPR) to predict Jw with high accuracy and uncertainty awareness. The GPR is trained specifically on the residual errors between nonlinear physical FO model predictions and experimental data, effectively combining physical insight with data-driven correction. They implement a comprehensive uncertainty quantification (UQ) method by decomposing total predictive variance into epistemic uncertainty (from GPR’s posterior variance) and aleatoric uncertainty (from input noise propagated analytically via the Delta method for correlated inputs). The approach leverages GPR’s strengths in low-data scenarios, achieving state-of-the-art performance with only 120 data points, yielding a remarkably low Mean Absolute Percentage Error (MAPE) of 0.26% and an R² of 0.999 on independent test data. This work validates a robust, reliable surrogate model suitable for advanced FO process optimization and digital twin development, blending physical understanding with machine learning to overcome prior modeling challenges. <div>
arXiv:2512.10457v1 Announce Type: new 
Abstract: Forward Osmosis (FO) is a promising low-energy membrane separation technology, but challenges in accurately modelling its water flux (Jw) persist due to complex internal mass transfer phenomena. Traditional mechanistic models struggle with empirical parameter variability, while purely data-driven models lack physical consistency and rigorous uncertainty quantification (UQ). This study introduces a novel Robust Hybrid Physics-ML framework employing Gaussian Process Regression (GPR) for highly accurate, uncertainty-aware Jw prediction. The core innovation lies in training the GPR on the residual error between the detailed, non-linear FO physical model prediction (Jw_physical) and the experimental water flux (Jw_actual). Crucially, we implement a full UQ methodology by decomposing the total predictive variance (sigma2_total) into model uncertainty (epistemic, from GPR's posterior variance) and input uncertainty (aleatoric, analytically propagated via the Delta method for multi-variate correlated inputs). Leveraging the inherent strength of GPR in low-data regimes, the model, trained on a meagre 120 data points, achieved a state-of-the-art Mean Absolute Percentage Error (MAPE) of 0.26% and an R2 of 0.999 on the independent test data, validating a truly robust and reliable surrogate model for advanced FO process optimization and digital twin development.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method</title>
<link>https://arxiv.org/abs/2512.10461</link>
<guid>https://arxiv.org/abs/2512.10461</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Network, Constraint Satisfaction, Sampling Kaczmarz-Motzkin, Trainable Network, Power System Optimization<br /><br />Summary:  
This paper addresses the challenge of neural network constraint satisfaction, which is important for safety-critical domains like power system optimization, robotic path planning, and autonomous driving. Existing methods for constraint satisfaction struggle with a trade-off between computational efficiency and applicability, especially hard constraint methods that demand high complexity or restrictive assumptions. The authors focus on the Sampling Kaczmarz-Motzkin (SKM) algorithm, a randomized iterative solver for large-scale linear inequality systems, which has favorable convergence but involves non-differentiable argmax operations problematic for neural network training. The key contribution is the Trainable Sampling Kaczmarz-Motzkin Network (T-SKM-Net) framework, which is the first to systematically integrate SKM-type methods into neural network constraints. The framework reformulates mixed constraints into pure inequalities via null space transformation, applies SKM iteratively, and then maps solutions back to the original constraint space, thus efficiently managing both equality and inequality constraints. They provide theoretical guarantees proving post-processing effectiveness in expectation and unbiased gradient estimators allowing end-to-end trainability despite non-differentiabilities. Experimentation on the DC Optimal Power Flow (DCOPF) case118 benchmark shows that T-SKM-Net achieves fast GPU inference (4.27ms/item) with minimal optimality gaps and zero constraint violations, delivering over 25 times speedup compared to the pandapower solver, demonstrating practical effectiveness and efficiency. <div>
arXiv:2512.10461v1 Announce Type: new 
Abstract: Neural network constraint satisfaction is crucial for safety-critical applications such as power system optimization, robotic path planning, and autonomous driving. However, existing constraint satisfaction methods face efficiency-applicability trade-offs, with hard constraint methods suffering from either high computational complexity or restrictive assumptions on constraint structures. The Sampling Kaczmarz-Motzkin (SKM) method is a randomized iterative algorithm for solving large-scale linear inequality systems with favorable convergence properties, but its argmax operations introduce non-differentiability, posing challenges for neural network applications. This work proposes the Trainable Sampling Kaczmarz-Motzkin Network (T-SKM-Net) framework and, for the first time, systematically integrates SKM-type methods into neural network constraint satisfaction. The framework transforms mixed constraint problems into pure inequality problems through null space transformation, employs SKM for iterative solving, and maps solutions back to the original constraint space, efficiently handling both equality and inequality constraints. We provide theoretical proof of post-processing effectiveness in expectation and end-to-end trainability guarantees based on unbiased gradient estimators, demonstrating that despite non-differentiable operations, the framework supports standard backpropagation. On the DCOPF case118 benchmark, our method achieves 4.27ms/item GPU serial forward inference with 0.0025% max optimality gap with post-processing mode and 5.25ms/item with 0.0008% max optimality gap with joint training mode, delivering over 25$\times$ speedup compared to the pandapower solver while maintaining zero constraint violations under given tolerance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10492</link>
<guid>https://arxiv.org/abs/2512.10492</guid>
<content:encoded><![CDATA[
<div> robust adversarial reinforcement learning, critic ensemble, uncertainty, exploration-exploitation trade-off, MuJoCo control<br /><br />Summary:<br /><br />Robust adversarial reinforcement learning addresses challenges in training agents to perform under uncertain disturbances, crucial for applications such as autonomous driving and robotic control. Traditionally, training involves a zero-sum Markov game framework between a protagonist and an adversary, which introduces non-stationarity and causes instability and convergence problems, especially in complex, high-dimensional environments. To tackle these challenges, the paper proposes UACER, a novel method consisting of two key strategies. First, a diversified critic ensemble is used, employing multiple critic networks in parallel to stabilize Q-value estimation, reduce variance, and enhance robustness, in contrast to conventional single-critic approaches. Second, a Time-varying Decay Uncertainty (TDU) mechanism is introduced that leverages epistemic uncertainty derived from variance to dynamically adjust the exploration-exploitation balance, helping stabilize training beyond simple linear aggregation methods. Extensive experiments on various MuJoCo control tasks demonstrate that UACER consistently outperforms state-of-the-art techniques, delivering improvements in overall performance, training stability, and sample efficiency. This work thus presents significant advancements in making adversarial reinforcement learning more reliable and effective for sequential decision-making problems under uncertainty. <div>
arXiv:2512.10492v1 Announce Type: new 
Abstract: Robust adversarial reinforcement learning has emerged as an effective paradigm for training agents to handle uncertain disturbance in real environments, with critical applications in sequential decision-making domains such as autonomous driving and robotic control. Within this paradigm, agent training is typically formulated as a zero-sum Markov game between a protagonist and an adversary to enhance policy robustness. However, the trainable nature of the adversary inevitably induces non-stationarity in the learning dynamics, leading to exacerbated training instability and convergence difficulties, particularly in high-dimensional complex environments. In this paper, we propose a novel approach, Uncertainty-Aware Critic Ensemble for robust adversarial Reinforcement learning (UACER), which consists of two strategies: 1) Diversified critic ensemble: a diverse set of K critic networks is exploited in parallel to stabilize Q-value estimation rather than conventional single-critic architectures for both variance reduction and robustness enhancement. 2) Time-varying Decay Uncertainty (TDU) mechanism: advancing beyond simple linear combinations, we develop a variance-derived Q-value aggregation strategy that explicitly incorporates epistemic uncertainty to dynamically regulate the exploration-exploitation trade-off while simultaneously stabilizing the training process. Comprehensive experiments across several MuJoCo control problems validate the superior effectiveness of UACER, outperforming state-of-the-art methods in terms of overall performance, stability, and efficiency.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10510</link>
<guid>https://arxiv.org/abs/2512.10510</guid>
<content:encoded><![CDATA[
<div> Keywords: Offline-to-Online Reinforcement Learning, Adaptive Replay Buffer, on-policyness, data sampling, D4RL benchmarks<br /><br />Summary:<br /><br />Offline-to-Online Reinforcement Learning (O2O RL) faces the challenge of balancing the use of a fixed offline dataset with new online experiences. Traditional methods often rely on fixed mixing ratios between offline and online data, which can impair early learning stability or asymptotic performance. To address this, the paper introduces the Adaptive Replay Buffer (ARB), a novel and learning-free method that dynamically prioritizes data sampling based on a lightweight metric named 'on-policyness'. ARB measures how well collected trajectories align with the current policy and assigns proportional sampling weights to individual transitions. This approach allows ARB to leverage the offline data early on for stability, then progressively emphasize relevant, high-reward online experiences during later stages of training. The method is simple to implement and can be integrated easily into existing O2O RL algorithms without complex learning procedures or hard-coded ratios. Extensive experiments conducted on D4RL benchmark tasks validate that ARB consistently reduces early performance degradation and significantly enhances the final policy performance. The results emphasize the benefit of adaptive, behavior-aware replay buffers in improving learning efficiency and effectiveness in O2O RL scenarios. <div>
arXiv:2512.10510v1 Announce Type: new 
Abstract: Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees</title>
<link>https://arxiv.org/abs/2512.10522</link>
<guid>https://arxiv.org/abs/2512.10522</guid>
<content:encoded><![CDATA[
<div> Keywords: variational autoencoder, disentangled latent space, out-of-distribution, model compression, knowledge distillation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of reasoning about multi-label out-of-distribution (OOD) test samples using the disentangled latent space of a variational autoencoder (VAE). 2. Disentangled latent space implies that each latent dimension uniquely corresponds to distinct generative factors or important image characteristics, facilitating interpretability and robustness in OOD detection. 3. To enable deployment on resource-constrained devices, the authors propose a Disentangled Distilled Encoder (DDE) framework that compresses the OOD reasoner model while preserving the disentanglement properties. 4. The DDE approach formalizes model compression via student-teacher distillation as a constrained optimization problem that incorporates disentanglement constraints, ensuring the student model maintains the interpretability of the teacher. 5. Theoretical guarantees are provided using Rademacher complexity analysis, which supports the preservation of disentanglement through the distillation process. 6. Empirical evaluation is conducted by deploying the compressed student model on NVIDIA hardware, demonstrating the efficiency gains without compromising OOD reasoning performance or disentanglement quality. This work advances practical applications of disentangled representation learning in real-world, resource-limited environments. <div>
arXiv:2512.10522v1 Announce Type: new 
Abstract: Recently, the disentangled latent space of a variational autoencoder (VAE) has been used to reason about multi-label out-of-distribution (OOD) test samples that are derived from different distributions than training samples. Disentangled latent space means having one-to-many maps between latent dimensions and generative factors or important characteristics of an image. This paper proposes a disentangled distilled encoder (DDE) framework to decrease the OOD reasoner size for deployment on resource-constrained devices while preserving disentanglement. DDE formalizes student-teacher distillation for model compression as a constrained optimization problem while preserving disentanglement with disentanglement constraints. Theoretical guarantees for disentanglement during distillation based on Rademacher complexity are established. The approach is evaluated empirically by deploying the compressed model on an NVIDIA
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mode-Seeking for Inverse Problems with Diffusion Models</title>
<link>https://arxiv.org/abs/2512.10524</link>
<guid>https://arxiv.org/abs/2512.10524</guid>
<content:encoded><![CDATA[
<div> Diffusion models, inverse problems, MAP estimation, variational mode-seeking loss, image restoration<br /><br />Summary:<br /><br />1. The paper addresses solving arbitrary inverse problems using pre-trained unconditional diffusion models combined with posterior sampling or maximum a posteriori (MAP) estimation, eliminating the need for task-specific training or fine-tuning. <br />2. Existing methods for posterior sampling and MAP estimation often involve modeling approximations and significant computational costs that limit their practicality. <br />3. The authors introduce a novel variational mode-seeking loss (VML) that guides the generated sample toward the MAP estimate by minimizing the Kullback-Leibler divergence between the diffusion posterior and the measurement posterior distributions during each reverse diffusion step. <br />4. Notably, for linear inverse problems, VML can be derived exactly without approximation, providing a theoretically grounded and computationally efficient solution. <br />5. Building on these theoretical insights, the VML-MAP algorithm is proposed and empirically validated, demonstrating improvements over existing methods in both accuracy and computational efficiency across various image restoration tasks and multiple datasets. <div>
arXiv:2512.10524v1 Announce Type: new 
Abstract: A pre-trained unconditional diffusion model, combined with posterior sampling or maximum a posteriori (MAP) estimation techniques, can solve arbitrary inverse problems without task-specific training or fine-tuning. However, existing posterior sampling and MAP estimation methods often rely on modeling approximations and can be computationally demanding. In this work, we propose the variational mode-seeking loss (VML), which, when minimized during each reverse diffusion step, guides the generated sample towards the MAP estimate. VML arises from a novel perspective of minimizing the Kullback-Leibler (KL) divergence between the diffusion posterior $p(\mathbf{x}_0|\mathbf{x}_t)$ and the measurement posterior $p(\mathbf{x}_0|\mathbf{y})$, where $\mathbf{y}$ denotes the measurement. Importantly, for linear inverse problems, VML can be analytically derived and need not be approximated. Based on further theoretical insights, we propose VML-MAP, an empirically effective algorithm for solving inverse problems, and validate its efficacy over existing methods in both performance and computational time, through extensive experiments on diverse image-restoration tasks across multiple datasets.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2512.10547</link>
<guid>https://arxiv.org/abs/2512.10547</guid>
<content:encoded><![CDATA[
<div> Top-K Sparse Autoencoders, Key-Value Cache, Attention Mechanism, Semantic Atoms, Dual-Budget Strategy  

<br /><br />Summary:  
The paper addresses the Key-Value (KV) cache bottleneck in long-context Large Language Models (LLMs) by proposing the STA-Attention framework, which leverages Top-K Sparse Autoencoders (SAEs) to break down the KV cache into interpretable semantic components called "semantic atoms." Unlike traditional $L_1$-regularized SAEs, the Top-K approach avoids shrinkage bias, thus maintaining the exact dot-product geometry crucial for accurate attention calculations. A novel discovery in the study is the Key-Value Asymmetry: Key vectors function as sparse routers characterized by a dominant "Semantic Elbow," while Value vectors contain dense information payloads demanding higher representation capacity. To exploit this structure, the authors introduce a Dual-Budget Strategy that prioritizes the preservation of the most meaningful semantic features and filters out representational noise. Experiments conducted on various models such as Yi-6B, Mistral-7B, and Qwen2.5-32B demonstrate that the semantic reconstructions generated by STA-Attention retain model performance, showing similar perplexity and zero-shot capabilities as the original models. Overall, this work bridges the gap between mechanistic interpretability and faithful attention modeling in large language models. <div>
arXiv:2512.10547v1 Announce Type: new 
Abstract: The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant Information Bottleneck Learning</title>
<link>https://arxiv.org/abs/2512.10573</link>
<guid>https://arxiv.org/abs/2512.10573</guid>
<content:encoded><![CDATA[
<div> Keywords: Information Bottleneck, label noise, mutual information, latent disentanglement, robust training<br /><br />Summary:<br /><br />The paper addresses the vulnerability of the Information Bottleneck (IB) principle to noisy labels, a common issue in real-world supervised learning scenarios. It introduces LaT-IB, a Label-Noise ResistanT Information Bottleneck method designed to improve robustness in the presence of label noise. Central to LaT-IB is the "Minimal-Sufficient-Clean" (MSC) criterion, implemented as a mutual information regularizer to preserve task-relevant information while discarding noise, thus overcoming standard IB's limitations. The approach features noise-aware latent disentanglement, separating the latent representation into components corresponding to clean labels and noise, which helps isolate and minimize the effect of noisy labels. The authors theoretically derive mutual information bounds related to prediction, compression, and disentanglement and prove that optimizing the proposed objective leads to noise-invariant representations and clean-noisy label separation. A novel three-phase training framework—Warmup, Knowledge Injection, and Robust Training—is proposed to gradually guide the model toward noise-resistant embeddings. Extensive experiments confirm that LaT-IB significantly enhances robustness and efficiency under various label noise conditions, improving the practical applicability of IB-based methods in noisy real-world environments. <div>
arXiv:2512.10573v1 Announce Type: new 
Abstract: The Information Bottleneck (IB) principle facilitates effective representation learning by preserving label-relevant information while compressing irrelevant information. However, its strong reliance on accurate labels makes it inherently vulnerable to label noise, prevalent in real-world scenarios, resulting in significant performance degradation and overfitting. To address this issue, we propose LaT-IB, a novel Label-Noise ResistanT Information Bottleneck method which introduces a "Minimal-Sufficient-Clean" (MSC) criterion. Instantiated as a mutual information regularizer to retain task-relevant information while discarding noise, MSC addresses standard IB's vulnerability to noisy label supervision. To achieve this, LaT-IB employs a noise-aware latent disentanglement that decomposes the latent representation into components aligned with to the clean label space and the noise space. Theoretically, we first derive mutual information bounds for each component of our objective including prediction, compression, and disentanglement, and moreover prove that optimizing it encourages representations invariant to input noise and separates clean and noisy label information. Furthermore, we design a three-phase training framework: Warmup, Knowledge Injection and Robust Training, to progressively guide the model toward noise-resistant representations. Extensive experiments demonstrate that LaT-IB achieves superior robustness and efficiency under label noise, significantly enhancing robustness and applicability in real-world scenarios with label noise.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>THeGAU: Type-Aware Heterogeneous Graph Autoencoder and Augmentation</title>
<link>https://arxiv.org/abs/2512.10589</link>
<guid>https://arxiv.org/abs/2512.10589</guid>
<content:encoded><![CDATA[
<div> Heterogeneous Graph Neural Networks, Heterogeneous Information Networks, type-aware autoencoder, graph augmentation, node classification  

<br /><br />Summary: Heterogeneous Graph Neural Networks (HGNNs) are powerful tools for modeling Heterogeneous Information Networks (HINs), which contain diverse types of entities and relations. Despite their effectiveness, HGNNs face challenges such as loss of node type information and structural noise, which degrade representational accuracy and generalization capabilities. To address these limitations, the paper proposes THeGAU, a model-agnostic framework that integrates a type-aware graph autoencoder with guided graph augmentation strategies. THeGAU employs an auxiliary task of reconstructing schema-valid edges to maintain semantic consistency of node types. Additionally, it introduces a decoder-driven augmentation mechanism that selectively refines noisy or irrelevant graph structures, leading to improved robustness and precision. This combined design not only boosts model accuracy but also enhances computational efficiency by reducing overhead. Extensive experiments conducted on three benchmark HIN datasets—IMDB, ACM, and DBLP—demonstrate that THeGAU consistently outperforms existing HGNN methods across various backbone architectures, achieving new state-of-the-art results in node classification tasks. The study highlights the benefits of preserving type semantics and selectively enhancing graph structure for better performance in heterogeneous graph learning scenarios. <div>
arXiv:2512.10589v1 Announce Type: new 
Abstract: Heterogeneous Graph Neural Networks (HGNNs) are effective for modeling Heterogeneous Information Networks (HINs), which encode complex multi-typed entities and relations. However, HGNNs often suffer from type information loss and structural noise, limiting their representational fidelity and generalization. We propose THeGAU, a model-agnostic framework that combines a type-aware graph autoencoder with guided graph augmentation to improve node classification. THeGAU reconstructs schema-valid edges as an auxiliary task to preserve node-type semantics and introduces a decoder-driven augmentation mechanism to selectively refine noisy structures. This joint design enhances robustness, accuracy, and efficiency while significantly reducing computational overhead. Extensive experiments on three benchmark HIN datasets (IMDB, ACM, and DBLP) demonstrate that THeGAU consistently outperforms existing HGNN methods, achieving state-of-the-art performance across multiple backbones.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Objective Reward and Preference Optimization: Theory and Algorithms</title>
<link>https://arxiv.org/abs/2512.10601</link>
<guid>https://arxiv.org/abs/2512.10601</guid>
<content:encoded><![CDATA[
<div> constrained reinforcement learning, CMDPs, preference learning, policy optimization, model alignment  

<br /><br />Summary:  
This thesis advances constrained reinforcement learning (RL) by developing new theoretical frameworks and algorithms across diverse settings including control, preference learning, and alignment of large language models. First, it introduces Average-Constrained Policy Optimization (ACPO), the initial method to tackle constrained Markov Decision Processes (CMDPs) under the average-cost criterion, combining sensitivity analysis with trust-region updates to ensure stable constraint satisfaction and providing strong theoretical guarantees alongside state-of-the-art empirical results. Second, constrained RL is extended to finite-horizon episodic scenarios through e-COP, the first policy optimization algorithm for episodic CMDPs, built upon a novel episodic policy difference lemma, which enables provable performance, simplicity, and scalability suitable for safety-critical environments. Third, the thesis addresses RL from human preferences by proposing warmPref-PS, a posterior sampling approach for linear bandits that leverages offline heterogeneous rater data, explicitly modeling rater competence to reduce regret and improve data efficiency in Reinforcement Learning from Human Feedback (RLHF). Further, PSPL jointly samples reward models and transition dynamics from pairwise trajectory comparisons, offering Bayesian simple-regret guarantees and enhanced empirical identification of optimal policies. Lastly, these techniques are applied to large-scale model alignment via MOPO, a multi-objective constrained optimization algorithm with closed-form updates, scalable to multi-billion-parameter language models, ensuring robustness across alignment tasks. Overall, the thesis unifies constrained RL frameworks to provide both theoretical insights and practical tools for safe and aligned decision-making. <div>
arXiv:2512.10601v1 Announce Type: new 
Abstract: This thesis develops theoretical frameworks and algorithms that advance constrained reinforcement learning (RL) across control, preference learning, and alignment of large language models. The first contribution addresses constrained Markov Decision Processes (CMDPs) under the average-cost criterion through the Average-Constrained Policy Optimization (ACPO) algorithm. ACPO integrates sensitivity analysis with trust-region updates to ensure stable constraint handling, achieving state-of-the-art empirical performance with theoretical guarantees. Constrained RL is then extended to finite-horizon settings via e-COP, the first policy optimization method for episodic CMDPs. Built on an episodic policy difference lemma, e-COP offers provable performance, simplicity, and scalability in safety-critical environments. The thesis then investigates reinforcement learning from human preferences. warmPref-PS introduces a posterior sampling strategy for linear bandits that integrates offline preference data from heterogeneous raters into online learning. Explicit modeling of rater competence yields substantial regret reduction and more efficient data collection for RLHF. The PSPL algorithm further advances preference-based RL by jointly sampling reward models and transition dynamics from pairwise trajectory comparisons, providing Bayesian simple-regret guarantees and robust empirical identification of optimal policies. The final contribution applies these methods to large-scale model alignment. A multi-objective constrained optimization view yields MOPO, an iterative algorithm with closed-form updates that scales to multi-billion-parameter language models and remains robust across alignment settings. Collectively, the thesis unifies constrained RL across average-cost, episodic, and preference-driven paradigms, delivering theoretical advances and practical tools for safe and aligned decision-making.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification</title>
<link>https://arxiv.org/abs/2512.10602</link>
<guid>https://arxiv.org/abs/2512.10602</guid>
<content:encoded><![CDATA[
<div> Bayesian Neural Networks, Quantization, Variational Inference, Uncertainty Estimation, Edge Devices  

<br /><br />Summary:  
This paper addresses the computational and memory overhead challenges faced by Bayesian Neural Networks (BNNs) compared to deterministic neural networks. It introduces a multi-level quantization framework tailored for Stochastic Variational Inference-based BNNs, distinguishing three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). A novel logarithmic quantization method for variance parameters is proposed, alongside specialized activation functions designed to preserve the probabilistic distributional structure essential for accurate uncertainty estimation. The research evaluates the quantization techniques on the Dirty-MNIST dataset, demonstrating that BNNs can be compressed to as low as 4-bit precision without sacrificing classification accuracy or the disentanglement of epistemic and aleatoric uncertainties. At 4-bit precision, Joint Quantization yields up to an 8x reduction in memory usage relative to traditional floating-point models while maintaining minimal degradation in uncertainty calibration. The findings highlight the practicality of deploying BNNs on resource-constrained edge devices and offer valuable insights for the development of future low-precision analog hardware, termed "Bayesian Machines," optimized for probabilistic inference tasks. <div>
arXiv:2512.10602v1 Announce Type: new 
Abstract: Bayesian Neural Networks (BNNs) provide principled uncertainty quantification but suffer from substantial computational and memory overhead compared to deterministic networks. While quantization techniques have successfully reduced resource requirements in standard deep learning models, their application to probabilistic models remains largely unexplored. We introduce a systematic multi-level quantization framework for Stochastic Variational Inference based BNNs that distinguishes between three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). Our logarithmic quantization for variance parameters, and specialized activation functions to preserve the distributional structure are essential for calibrated uncertainty estimation. Through comprehensive experiments on Dirty-MNIST, we demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. At 4 bits, Joint Quantization achieves up to 8x memory reduction compared to floating-point implementations with minimal degradation in epistemic and aleatoric uncertainty estimation. These results enable deployment of BNNs on resource-constrained edge devices and provide design guidelines for future analog "Bayesian Machines" operating at inherently low precision.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supporting Migration Policies with Forecasts: Illegal Border Crossings in Europe through a Mixed Approach</title>
<link>https://arxiv.org/abs/2512.10633</link>
<guid>https://arxiv.org/abs/2512.10633</guid>
<content:encoded><![CDATA[
<div> Keywords: illegal border crossings, Europe, machine learning, migration experts, EU migration policy  

<br /><br />Summary:  
This paper introduces a mixed-methodology for forecasting illegal border crossings across five critical migratory routes in Europe, extending the forecast horizon to one year. The approach uniquely combines advanced machine learning techniques with qualitative input from migration experts, incorporating a human-assessed covariate to enhance prediction accuracy. This innovation specifically addresses difficulties associated with abrupt changes in migration flows and data limitations common in traditional forecasting methods. The methodology is developed in direct response to the EU Pact on Migration and Asylum, aligning with the Asylum and Migration Management Regulation (AMMR) to provide forecasts that are relevant for policymaking. Designed as an operational tool, it supports strategic decision-making, early warning systems, and mechanisms of solidarity among EU Member States. By integrating data-driven models with expert judgment, the paper aligns with current academic best practices and addresses practical needs in migration governance. The methodology undergoes testing and validation against historical data to confirm its reliability and applicability within migration policy contexts, demonstrating the potential to improve forecasting accuracy and support EU migration management efforts effectively. <div>
arXiv:2512.10633v1 Announce Type: new 
Abstract: This paper presents a mixed-methodology to forecast illegal border crossings in Europe across five key migratory routes, with a one-year time horizon. The methodology integrates machine learning techniques with qualitative insights from migration experts. This approach aims at improving the predictive capacity of data-driven models through the inclusion of a human-assessed covariate, an innovation that addresses challenges posed by sudden shifts in migration patterns and limitations in traditional datasets. The proposed methodology responds directly to the forecasting needs outlined in the EU Pact on Migration and Asylum, supporting the Asylum and Migration Management Regulation (AMMR). It is designed to provide policy-relevant forecasts that inform strategic decisions, early warning systems, and solidarity mechanisms among EU Member States. By joining data-driven modeling with expert judgment, this work aligns with existing academic recommendations and introduces a novel operational tool tailored for EU migration governance. The methodology is tested and validated with known data to demonstrate its applicability and reliability in migration-related policy context.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token Sample Complexity of Attention</title>
<link>https://arxiv.org/abs/2512.10656</link>
<guid>https://arxiv.org/abs/2512.10656</guid>
<content:encoded><![CDATA[
<div> Keywords: attention convergence, token-sample complexity, large language models, moment convergence, softmax hardmax limit<br /><br />Summary:<br /><br />1. This paper studies how attention mechanisms in large language models behave as the sequence length grows extremely large, focusing on the convergence properties of attention computed on a finite number of tokens towards its limit at infinite tokens.<br /><br />2. The authors introduce the concept of token-sample complexity, which quantifies the rate at which the attention on \(n\) tokens approaches the infinite-token limit.<br /><br />3. Two main results are obtained: first, for compactly supported or sub-Gaussian token distributions, the attention map converges uniformly on a ball of radius \(R\) at a rate proportional to \(C(R)/\sqrt{n}\), where \(C(R)\) grows exponentially with \(R\). Second, to handle large \(R\) where this bound becomes less practical, they study convergence rates of moments of the transformed token distribution showing rates of \(C'(R)/n^{\beta}\) with \(\beta<1/2\), where \(C'(R)\) depends polynomially on the support size.<br /><br />4. The exponent \(\beta\) depends on attention geometry and spectral properties of the tokens, highlighting structural factors influencing convergence speed.<br /><br />5. Additionally, the paper analyzes the limiting regime as the attention parameter goes to infinity, causing softmax to approximate a hardmax, and establishes a logarithmic convergence rate in this setting.<br /><br />6. The theoretical findings are validated through experiments on synthetic Gaussian data and real BERT models applied to Wikipedia text, which confirm the predicted convergence behaviors. <div>
arXiv:2512.10656v1 Announce Type: new 
Abstract: As context windows in large language models continue to expand, it is essential to characterize how attention behaves at extreme sequence lengths. We introduce token-sample complexity: the rate at which attention computed on $n$ tokens converges to its infinite-token limit. We estimate finite-$n$ convergence bounds at two levels: pointwise uniform convergence of the attention map, and convergence of moments for the transformed token distribution. For compactly supported (and more generally sub-Gaussian) distributions, our first result shows that the attention map converges uniformly on a ball of radius $R$ at rate $C(R)/\sqrt{n}$, where $C(R)$ grows exponentially with $R$. For large $R$, this estimate loses practical value, and our second result addresses this issue by establishing convergence rates for the moments of the transformed distribution (the token output of the attention layer). In this case, the rate is $C'(R)/n^{\beta}$ with $\beta<\tfrac{1}{2}$, and $C'(R)$ depends polynomially on the size of the support of the distribution. The exponent $\beta$ depends on the attention geometry and the spectral properties of the tokens distribution. We also examine the regime in which the attention parameter tends to infinity and the softmax approaches a hardmax, and in this setting, we establish a logarithmic rate of convergence. Experiments on synthetic Gaussian data and real BERT models on Wikipedia text confirm our predictions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCFO Additional Material</title>
<link>https://arxiv.org/abs/2512.10659</link>
<guid>https://arxiv.org/abs/2512.10659</guid>
<content:encoded><![CDATA[
<div> Outlier Detection, Counterfactual Explanation, Local Outlier Factor, Density-based Counterfactuals, Gradient-based Optimization

<br /><br />Summary:
This paper addresses the challenge of explaining outliers detected by the Local Outlier Factor (LOF), a widely used unsupervised outlier detection method known for quantifying outlierness through relative local density but lacking interpretability. The authors emphasize the importance of explanations for outliers in understanding underlying factors, validating significance, identifying biases, and enabling actionable insights. They propose Density-based Counterfactuals for Outliers (DCFO), a novel method designed specifically to generate counterfactual explanations tailored for LOF. DCFO works by partitioning the data space into regions where the LOF function behaves smoothly, facilitating the use of efficient gradient-based optimization techniques to find minimal changes that alter outlier predictions. This approach addresses unique challenges in explaining outliers that existing counterfactual explanation methods overlook. The method was extensively evaluated on 50 OpenML datasets, where it consistently outperformed competing methods. DCFO demonstrated superior performance in producing counterfactuals that are both proximal to the original data points and valid in terms of changing the outlier status. Overall, the proposed DCFO framework enhances the interpretability of LOF outlier detection, providing practitioners with an effective tool for understanding and managing outliers better. <div>
arXiv:2512.10659v1 Announce Type: new 
Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning by Analogy: A Causal Framework for Composition Generalization</title>
<link>https://arxiv.org/abs/2512.10669</link>
<guid>https://arxiv.org/abs/2512.10669</guid>
<content:encoded><![CDATA[
<div> Keywords: compositional generalization, causal modularity, hierarchical data-generating process, latent structure identifiability, analogy-based learning  

<br /><br />Summary:  
1. The paper addresses compositional generalization, which is the ability of models to understand and produce novel combinations of learned concepts, thus extending beyond limited prior experiences.  
2. It argues that this capability relies on decomposing complex, high-level concepts into basic, low-level components that can be flexibly recombined, akin to human analogy-making.  
3. To formalize this, the authors introduce a hierarchical data-generating process capturing multiple concept levels and their complex interaction mechanisms, moving beyond previous models that assumed only simple additive interactions.  
4. Theoretical results demonstrate that this hierarchical latent structure is identifiable and can be recovered from observable data such as paired text and images, an essential prerequisite for learning these generative models.  
5. Empirically, the framework inspired by their theory yields significant performance improvements on benchmark datasets, validating the practical benefits of modeling compositionality via causal modularity and minimal changes. <div>
arXiv:2512.10669v1 Announce Type: new 
Abstract: Compositional generalization -- the ability to understand and generate novel combinations of learned concepts -- enables models to extend their capabilities beyond limited experiences. While effective, the data structures and principles that enable this crucial capability remain poorly understood. We propose that compositional generalization fundamentally requires decomposing high-level concepts into basic, low-level concepts that can be recombined across similar contexts, similar to how humans draw analogies between concepts. For example, someone who has never seen a peacock eating rice can envision this scene by relating it to their previous observations of a chicken eating rice.
  In this work, we formalize these intuitive processes using principles of causal modularity and minimal changes. We introduce a hierarchical data-generating process that naturally encodes different levels of concepts and their interaction mechanisms. Theoretically, we demonstrate that this approach enables compositional generalization supporting complex relations between composed concepts, advancing beyond prior work that assumes simpler interactions like additive effects. Critically, we also prove that this latent hierarchical structure is provably recoverable (identifiable) from observable data like text-image pairs, a necessary step for learning such a generative process. To validate our theory, we apply insights from our theoretical framework and achieve significant improvements on benchmark datasets.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated Multimodal Classification</title>
<link>https://arxiv.org/abs/2512.10701</link>
<guid>https://arxiv.org/abs/2512.10701</guid>
<content:encoded><![CDATA[
arXiv:2512.10701v1 Announce Type: new 
Abstract: Vertical Federated Learning (VFL) offers a privacy-preserving paradigm for Edge AI scenarios like mobile health diagnostics, where sensitive multimodal data reside on distributed, resource-constrained devices. Yet, standard VFL systems often suffer performance limitations due to simplistic feature fusion. This paper introduces HybridVFL, a novel framework designed to overcome this bottleneck by employing client-side feature disentanglement paired with a server-side cross-modal transformer for context-aware fusion. Through systematic evaluation on the multimodal HAM10000 skin lesion dataset, we demonstrate that HybridVFL significantly outperforms standard federated baselines, validating the criticality of advanced fusion mechanisms in robust, privacy-preserving systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality</title>
<link>https://arxiv.org/abs/2512.10720</link>
<guid>https://arxiv.org/abs/2512.10720</guid>
<content:encoded><![CDATA[
arXiv:2512.10720v1 Announce Type: new 
Abstract: Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Spherical Neural Operators: Green's Function Formulation</title>
<link>https://arxiv.org/abs/2512.10723</link>
<guid>https://arxiv.org/abs/2512.10723</guid>
<content:encoded><![CDATA[
arXiv:2512.10723v1 Announce Type: new 
Abstract: Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a general operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to anisotropic, constraint-rich systems while retaining spectral efficiency. To exploit GSNO, we develop GSHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and GSHNet consistently outperform state-of-the-art methods. Our results position GSNO as a principled and general framework for spherical operator learning, bridging rigorous theory with real-world complexity.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation</title>
<link>https://arxiv.org/abs/2512.10735</link>
<guid>https://arxiv.org/abs/2512.10735</guid>
<content:encoded><![CDATA[
arXiv:2512.10735v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for graph classification. Specifically, most existing GNNs mainly rely on the message passing strategy between neighbor nodes, where the expressivity is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Although a number of k-WL-based GNNs have been proposed to overcome this limitation, their computational cost increases rapidly with k, significantly restricting the practical applicability. Moreover, since the k-WL models mainly operate on node tuples, these k-WL-based GNNs cannot retain fine-grained node- or edge-level semantics required by attribution methods (e.g., Integrated Gradients), leading to the less interpretable problem. To overcome the above shortcomings, in this paper, we propose a novel Line Graph Aggregation Network (LGAN), that constructs a line graph from the induced subgraph centered at each node to perform the higher-order aggregation. We theoretically prove that the LGAN not only possesses the greater expressive power than the 2-WL under injective aggregation assumptions, but also has lower time complexity. Empirical evaluations on benchmarks demonstrate that the LGAN outperforms state-of-the-art k-WL-based GNNs, while offering better interpretability.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Template-Free Retrosynthesis with Graph-Prior Augmented Transformers</title>
<link>https://arxiv.org/abs/2512.10770</link>
<guid>https://arxiv.org/abs/2512.10770</guid>
<content:encoded><![CDATA[
arXiv:2512.10770v1 Announce Type: new 
Abstract: Retrosynthesis reaction prediction seeks to infer plausible reactant molecules for a given product and is a central problem in computer-aided organic synthesis. Despite recent progress, many existing models still fall short of the accuracy and robustness required for practical deployment. This work studies a template-free, Transformer-based framework that eliminates reliance on handcrafted reaction templates or additional chemical rule engines. The model injects molecular graph information into the attention mechanism to jointly exploit \SMILES\ sequences and structural cues, and further applies a paired data augmentation strategy to enhance training diversity and scale. On the USPTO-50K benchmark, our proposed approach achieves state-of-the-art performance among template-free methods and substantially outperforming a vanilla Transformer baseline.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable and Steerable Concept Bottleneck Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2512.10805</link>
<guid>https://arxiv.org/abs/2512.10805</guid>
<content:encoded><![CDATA[
arXiv:2512.10805v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values</title>
<link>https://arxiv.org/abs/2512.10817</link>
<guid>https://arxiv.org/abs/2512.10817</guid>
<content:encoded><![CDATA[
arXiv:2512.10817v1 Announce Type: new 
Abstract: We report the discovery that binary encoding allows neural networks to extrapolate periodic functions beyond their training bounds. We introduce Normalized Base-2 Encoding (NB2E) as a method for encoding continuous numerical values and demonstrate that, using this input encoding, vanilla multi-layer perceptrons (MLP) successfully extrapolate diverse periodic signals without prior knowledge of their functional form. Internal activation analysis reveals that NB2E induces bit-phase representations, enabling MLPs to learn and extrapolate signal structure independently of position.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments</title>
<link>https://arxiv.org/abs/2512.10835</link>
<guid>https://arxiv.org/abs/2512.10835</guid>
<content:encoded><![CDATA[
arXiv:2512.10835v1 Announce Type: new 
Abstract: This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Symbolic Regression via Posterior Sampling</title>
<link>https://arxiv.org/abs/2512.10849</link>
<guid>https://arxiv.org/abs/2512.10849</guid>
<content:encoded><![CDATA[
arXiv:2512.10849v1 Announce Type: new 
Abstract: Symbolic regression is a powerful tool for discovering governing equations directly from data, but its sensitivity to noise hinders its broader application. This paper introduces a Sequential Monte Carlo (SMC) framework for Bayesian symbolic regression that approximates the posterior distribution over symbolic expressions, enhancing robustness and enabling uncertainty quantification for symbolic regression in the presence of noise. Differing from traditional genetic programming approaches, the SMC-based algorithm combines probabilistic selection, adaptive tempering, and the use of normalized marginal likelihood to efficiently explore the search space of symbolic expressions, yielding parsimonious expressions with improved generalization. When compared to standard genetic programming baselines, the proposed method better deals with challenging, noisy benchmark datasets. The reduced tendency to overfit and enhanced ability to discover accurate and interpretable equations paves the way for more robust symbolic regression in scientific discovery and engineering design applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants</title>
<link>https://arxiv.org/abs/2512.10857</link>
<guid>https://arxiv.org/abs/2512.10857</guid>
<content:encoded><![CDATA[
arXiv:2512.10857v1 Announce Type: new 
Abstract: Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Behavior of Discrete Diffusion Language Models</title>
<link>https://arxiv.org/abs/2512.10858</link>
<guid>https://arxiv.org/abs/2512.10858</guid>
<content:encoded><![CDATA[
arXiv:2512.10858v1 Announce Type: new 
Abstract: Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.
  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting</title>
<link>https://arxiv.org/abs/2512.10866</link>
<guid>https://arxiv.org/abs/2512.10866</guid>
<content:encoded><![CDATA[
arXiv:2512.10866v1 Announce Type: new 
Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Transfer Learning for Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2512.10877</link>
<guid>https://arxiv.org/abs/2512.10877</guid>
<content:encoded><![CDATA[
arXiv:2512.10877v1 Announce Type: new 
Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes</title>
<link>https://arxiv.org/abs/2512.10878</link>
<guid>https://arxiv.org/abs/2512.10878</guid>
<content:encoded><![CDATA[
arXiv:2512.10878v1 Announce Type: new 
Abstract: Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields</title>
<link>https://arxiv.org/abs/2512.10886</link>
<guid>https://arxiv.org/abs/2512.10886</guid>
<content:encoded><![CDATA[
arXiv:2512.10886v1 Announce Type: new 
Abstract: Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.
  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.
  The model accurately reconstructs loop temperatures (RMSE $<2^\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale</title>
<link>https://arxiv.org/abs/2512.10922</link>
<guid>https://arxiv.org/abs/2512.10922</guid>
<content:encoded><![CDATA[
arXiv:2512.10922v1 Announce Type: new 
Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation</title>
<link>https://arxiv.org/abs/2512.10925</link>
<guid>https://arxiv.org/abs/2512.10925</guid>
<content:encoded><![CDATA[
arXiv:2512.10925v1 Announce Type: new 
Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupled Q-Chunking</title>
<link>https://arxiv.org/abs/2512.10926</link>
<guid>https://arxiv.org/abs/2512.10926</guid>
<content:encoded><![CDATA[
arXiv:2512.10926v1 Announce Type: new 
Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asynchronous Reasoning: Training-Free Interactive Thinking LLMs</title>
<link>https://arxiv.org/abs/2512.10931</link>
<guid>https://arxiv.org/abs/2512.10931</guid>
<content:encoded><![CDATA[
arXiv:2512.10931v1 Announce Type: new 
Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks</title>
<link>https://arxiv.org/abs/2512.10936</link>
<guid>https://arxiv.org/abs/2512.10936</guid>
<content:encoded><![CDATA[
arXiv:2512.10936v1 Announce Type: new 
Abstract: The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger Normalization-Free Transformers</title>
<link>https://arxiv.org/abs/2512.10938</link>
<guid>https://arxiv.org/abs/2512.10938</guid>
<content:encoded><![CDATA[
arXiv:2512.10938v1 Announce Type: new 
Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(\alpha x + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Dataset Selection for High-Quality Data Sharing</title>
<link>https://arxiv.org/abs/2512.10952</link>
<guid>https://arxiv.org/abs/2512.10952</guid>
<content:encoded><![CDATA[
arXiv:2512.10952v1 Announce Type: new 
Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bidirectional Normalizing Flow: From Data to Noise and Back</title>
<link>https://arxiv.org/abs/2512.10953</link>
<guid>https://arxiv.org/abs/2512.10953</guid>
<content:encoded><![CDATA[
arXiv:2512.10953v1 Announce Type: new 
Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Acquisition of Discrete Grammatical Categories</title>
<link>https://arxiv.org/abs/2503.18702</link>
<guid>https://arxiv.org/abs/2503.18702</guid>
<content:encoded><![CDATA[
arXiv:2503.18702v1 Announce Type: cross 
Abstract: This article presents experiments performed using a computational laboratory environment for language acquisition experiments. It implements a multi-agent system consisting of two agents: an adult language model and a daughter language model that aims to learn the mother language. Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates. These experiments illustrate how this system can be used to acquire abstract grammatical knowledge. We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules. These rules are subsequently added to the grammatical knowledge of the daughter language model. To this end, hierarchical agglomerative cluster analysis was applied to the utterances consecutively generated by the mother language model. It is argued that this procedure can be used to acquire structures resembling grammatical categories proposed by linguists for natural languages. Thus, it is established that non-trivial grammatical knowledge has been acquired. Moreover, the parameter configuration of this computational laboratory environment determined using training data generated by the mother language model is validated in a second experiment with a test set similarly resulting in the acquisition of non-trivial categories.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Health Misinformation Detection with Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2512.09935</link>
<guid>https://arxiv.org/abs/2512.09935</guid>
<content:encoded><![CDATA[
arXiv:2512.09935v1 Announce Type: cross 
Abstract: Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-stage framework for health misinformation detection: Agreement Score Prediction followed by Multi-Agent Debate. In the first stage, we employ large language models (LLMs) to independently evaluate retrieved articles and compute an aggregated agreement score that reflects the overall evidence stance. When this score indicates insufficient consensus-falling below a predefined threshold-the system proceeds to a second stage. Multiple agents engage in structured debate to synthesize conflicting evidence and generate well-reasoned verdicts with explicit justifications. Experimental results demonstrate that our two-stage approach achieves superior performance compared to baseline methods, highlighting the value of combining automated scoring with collaborative reasoning for complex verification tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QSTAformer: A Quantum-Enhanced Transformer for Robust Short-Term Voltage Stability Assessment against Adversarial Attacks</title>
<link>https://arxiv.org/abs/2512.09936</link>
<guid>https://arxiv.org/abs/2512.09936</guid>
<content:encoded><![CDATA[
arXiv:2512.09936v1 Announce Type: cross 
Abstract: Short-term voltage stability assessment (STVSA) is critical for secure power system operation. While classical machine learning-based methods have demonstrated strong performance, they still face challenges in robustness under adversarial conditions. This paper proposes QSTAformer-a tailored quantum-enhanced Transformer architecture that embeds parameterized quantum circuits (PQCs) into attention mechanisms-for robust and efficient STVSA. A dedicated adversarial training strategy is developed to defend against both white-box and gray-box attacks. Furthermore, diverse PQC architectures are benchmarked to explore trade-offs between expressiveness, convergence, and efficiency. To the best of our knowledge, this is the first work to systematically investigate the adversarial vulnerability of quantum machine learning-based STVSA. Case studies on the IEEE 39-bus system demonstrate that QSTAformer achieves competitive accuracy, reduced complexity, and stronger robustness, underscoring its potential for secure and scalable STVSA under adversarial conditions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Norm-Governed Multi-Agent Decision-Making in Simulator-Coupled Environments:The Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP)</title>
<link>https://arxiv.org/abs/2512.09939</link>
<guid>https://arxiv.org/abs/2512.09939</guid>
<content:encoded><![CDATA[
arXiv:2512.09939v1 Announce Type: cross 
Abstract: Reinsurance decision-making exhibits the core structural properties that motivate multi-agent models: distributed and asymmetric information, partial observability, heterogeneous epistemic responsibilities, simulator-driven environment dynamics, and binding prudential and regulatory constraints. Deterministic workflow automation cannot meet these requirements, as it lacks the epistemic flexibility, cooperative coordination mechanisms, and norm-sensitive behaviour required for institutional risk-transfer.
  We propose the Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP), a formal model that extends stochastic games and Dec-POMDPs by adding three missing elements: (i) simulator-coupled transition dynamics grounded in catastrophe, capital, and portfolio engines; (ii) role-specialized agents with structured observability, belief updates, and typed communication; and (iii) a normative feasibility layer encoding solvency, regulatory, and organizational rules as admissibility constraints on joint actions.
  Using LLM-based agents with tool access and typed message protocols, we show in a domain-calibrated synthetic environment that governed multi-agent coordination yields more stable, coherent, and norm-adherent behaviour than deterministic automation or monolithic LLM baselines--reducing pricing variance, improving capital efficiency, and increasing clause-interpretation accuracy. Embedding prudential norms as admissibility constraints and structuring communication into typed acts measurably enhances equilibrium stability.
  Overall, the results suggest that regulated, simulator-driven decision environments are most naturally modelled as norm-governed, simulator-coupled multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting</title>
<link>https://arxiv.org/abs/2512.09944</link>
<guid>https://arxiv.org/abs/2512.09944</guid>
<content:encoded><![CDATA[
arXiv:2512.09944v1 Announce Type: cross 
Abstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs</title>
<link>https://arxiv.org/abs/2512.09953</link>
<guid>https://arxiv.org/abs/2512.09953</guid>
<content:encoded><![CDATA[
arXiv:2512.09953v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove the influence of specific data points from a trained model to satisfy privacy, copyright, and safety requirements. In real deployments, providers distribute a global model to many edge devices, where each client personalizes the model using private data. When a deletion request is issued, clients may ignore it or falsely claim compliance, and providers cannot check their parameters or data. This makes verification difficult, especially because personalized models must forget the targeted samples while preserving local utility, and verification must remain lightweight on edge devices.
  We introduce ZK APEX, a zero-shot personalized unlearning method that operates directly on the personalized model without retraining. ZK APEX combines sparse masking on the provider side with a small Group OBS compensation step on the client side, using a blockwise empirical Fisher matrix to create a curvature-aware update designed for low overhead. Paired with Halo2 zero-knowledge proofs, it enables the provider to verify that the correct unlearning transformation was applied without revealing any private data or personalized parameters.
  On Vision Transformer classification tasks, ZK APEX recovers nearly all personalization accuracy while effectively removing the targeted information. Applied to the OPT125M generative model trained on code data, it recovers around seventy percent of the original accuracy. Proof generation for the ViT case completes in about two hours, more than ten million times faster than retraining-based checks, with less than one gigabyte of memory use and proof sizes around four hundred megabytes. These results show the first practical framework for verifiable personalized unlearning on edge devices.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0</title>
<link>https://arxiv.org/abs/2512.09961</link>
<guid>https://arxiv.org/abs/2512.09961</guid>
<content:encoded><![CDATA[
arXiv:2512.09961v1 Announce Type: cross 
Abstract: The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Fake-News Detection with Node-Level Topological Features</title>
<link>https://arxiv.org/abs/2512.09974</link>
<guid>https://arxiv.org/abs/2512.09974</guid>
<content:encoded><![CDATA[
arXiv:2512.09974v1 Announce Type: cross 
Abstract: In recent years, the proliferation of misinformation and fake news has posed serious threats to individuals and society, spurring intense research into automated detection methods. Previous work showed that integrating content, user preferences, and propagation structure achieves strong performance, but leaves all graph-level representation learning entirely to the GNN, hiding any explicit topological cues. To close this gap, we introduce a lightweight enhancement: for each node, we append two classical graph-theoretic metrics, degree centrality and local clustering coefficient, to its original BERT and profile embeddings, thus explicitly flagging the roles of hub and community. In the UPFD Politifact subset, this simple modification boosts macro F1 from 0.7753 to 0.8344 over the original baseline. Our study not only demonstrates the practical value of explicit topology features in fake-news detection but also provides an interpretable, easily reproducible template for fusing graph metrics in other information-diffusion tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fuzzy Hierarchical Multiplex</title>
<link>https://arxiv.org/abs/2512.09976</link>
<guid>https://arxiv.org/abs/2512.09976</guid>
<content:encoded><![CDATA[
arXiv:2512.09976v1 Announce Type: cross 
Abstract: A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this is a white-theoretical paper introducing the framework and analyzing the logic and math behind it. Upon this extension the main objectives and the orientation of this framework is expounded and exemplified; this framework is meant for service optimization of information transmission in service process design. Lastly, a thorough analysis of the FHM is included which is done following the logical steps in a simple and elegant manner.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LxCIM: a new rank-based binary classifier performance metric invariant to local exchange of classes</title>
<link>https://arxiv.org/abs/2512.10053</link>
<guid>https://arxiv.org/abs/2512.10053</guid>
<content:encoded><![CDATA[
arXiv:2512.10053v1 Announce Type: cross 
Abstract: Binary classification is one of the oldest, most prevalent, and studied problems in machine learning. However, the metrics used to evaluate model performance have received comparatively little attention. The area under the receiver operating characteristic curve (AUROC) has long been a standard choice for model comparison. Despite its advantages, AUROC is not always ideal, particularly for problems that are invariant to local exchange of classes (LxC), a new form of metric invariance introduced in this work. To address this limitation, we propose LxCIM (LxC-invariant metric), which is not only rank-based and invariant under local exchange of classes, but also intuitive, logically consistent, and always computable, while enabling more detailed analysis through the cumulative accuracy-decision rate curve. Moreover, LxCIM exhibits clear theoretical connections to AUROC, accuracy, and the area under the accuracy-decision rate curve (AUDRC). These relationships allow for multiple complementary interpretations: as a symmetric form of AUROC, a rank-based analogue of accuracy, or a more representative and more interpretable variant of AUDRC. Finally, we demonstrate the direct applicability of LxCIM to the bivariate causal discovery problem (which exhibits invariance to local exchange of classes) and show how it addresses the acknowledged limitations of existing metrics used in this field. All code and implementation details are publicly available at github.com/tiagobrogueira/Causal-Discovery-In-Exchangeable-Data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Independent Density Estimation</title>
<link>https://arxiv.org/abs/2512.10067</link>
<guid>https://arxiv.org/abs/2512.10067</guid>
<content:encoded><![CDATA[
arXiv:2512.10067v1 Announce Type: cross 
Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Neverthe- less, these models still encounter difficulties in achieving human-like composi- tional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connec- tion between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy- based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit</title>
<link>https://arxiv.org/abs/2512.10092</link>
<guid>https://arxiv.org/abs/2512.10092</guid>
<content:encoded><![CDATA[
arXiv:2512.10092v1 Announce Type: cross 
Abstract: Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation</title>
<link>https://arxiv.org/abs/2512.10099</link>
<guid>https://arxiv.org/abs/2512.10099</guid>
<content:encoded><![CDATA[
arXiv:2512.10099v1 Announce Type: cross 
Abstract: Nonprehensile manipulation, such as pushing objects across cluttered environments, presents a challenging control problem due to complex contact dynamics and long-horizon planning requirements. In this work, we propose HeRD, a hierarchical reinforcement learning-diffusion policy that decomposes pushing tasks into two levels: high-level goal selection and low-level trajectory generation. We employ a high-level reinforcement learning (RL) agent to select intermediate spatial goals, and a low-level goal-conditioned diffusion model to generate feasible, efficient trajectories to reach them.
  This architecture combines the long-term reward maximizing behaviour of RL with the generative capabilities of diffusion models. We evaluate our method in a 2D simulation environment and show that it outperforms the state-of-the-art baseline in success rate, path efficiency, and generalization across multiple environment configurations. Our results suggest that hierarchical control with generative low-level planning is a promising direction for scalable, goal-directed nonprehensile manipulation. Code, documentation, and trained models are available: https://github.com/carosteven/HeRD.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Model-Guided Neural Network Method for the Inverse Scattering Problem</title>
<link>https://arxiv.org/abs/2512.10123</link>
<guid>https://arxiv.org/abs/2512.10123</guid>
<content:encoded><![CDATA[
arXiv:2512.10123v1 Announce Type: cross 
Abstract: Inverse medium scattering is an ill-posed, nonlinear wave-based imaging problem arising in medical imaging, remote sensing, and non-destructive testing. Machine learning (ML) methods offer increased inference speed and flexibility in capturing prior knowledge of imaging targets relative to classical optimization-based approaches; however, they perform poorly in regimes where the scattering behavior is highly nonlinear. A key limitation is that ML methods struggle to incorporate the physics governing the scattering process, which are typically inferred implicitly from the training data or loosely enforced via architectural design. In this paper, we present a method that endows a machine learning framework with explicit knowledge of problem physics, in the form of a differentiable solver representing the forward model. The proposed method progressively refines reconstructions of the scattering potential using measurements at increasing wave frequencies, following a classical strategy to stabilize recovery. Empirically, we find that our method provides high-quality reconstructions at a fraction of the computational or sampling costs of competing approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STARS: Semantic Tokens with Augmented Representations for Recommendation at Scale</title>
<link>https://arxiv.org/abs/2512.10149</link>
<guid>https://arxiv.org/abs/2512.10149</guid>
<content:encoded><![CDATA[
arXiv:2512.10149v1 Announce Type: cross 
Abstract: Real-world ecommerce recommender systems must deliver relevant items under strict tens-of-milliseconds latency constraints despite challenges such as cold-start products, rapidly shifting user intent, and dynamic context including seasonality, holidays, and promotions. We introduce STARS, a transformer-based sequential recommendation framework built for large-scale, low-latency ecommerce settings. STARS combines several innovations: dual-memory user embeddings that separate long-term preferences from short-term session intent; semantic item tokens that fuse pretrained text embeddings, learnable deltas, and LLM-derived attribute tags, strengthening content-based matching, long-tail coverage, and cold-start performance; context-aware scoring with learned calendar and event offsets; and a latency-conscious two-stage retrieval pipeline that performs offline embedding generation and online maximum inner-product search with filtering, enabling tens-of-milliseconds response times. In offline evaluations on production-scale data, STARS improves Hit@5 by more than 75 percent relative to our existing LambdaMART system. A large-scale A/B test on 6 million visits shows statistically significant lifts, including Total Orders +0.8%, Add-to-Cart on Home +2.0%, and Visits per User +0.5%. These results demonstrate that combining semantic enrichment, multi-intent modeling, and deployment-oriented design can yield state-of-the-art recommendation quality in real-world environments without sacrificing serving efficiency.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference for Batched Adaptive Experiments</title>
<link>https://arxiv.org/abs/2512.10156</link>
<guid>https://arxiv.org/abs/2512.10156</guid>
<content:encoded><![CDATA[
arXiv:2512.10156v1 Announce Type: cross 
Abstract: The advantages of adaptive experiments have led to their rapid adoption in economics, other fields, as well as among practitioners. However, adaptive experiments pose challenges for causal inference. This note suggests a BOLS (batched ordinary least squares) test statistic for inference of treatment effects in adaptive experiments. The statistic provides a precision-equalizing aggregation of per-period treatment-control differences under heteroskedasticity. The combined test statistic is a normalized average of heteroskedastic per-period z-statistics and can be used to construct asymptotically valid confidence intervals. We provide simulation results comparing rejection rates in the typical case with few treatment periods and few (or many) observations per batch.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 2025 Foundation Model Transparency Index</title>
<link>https://arxiv.org/abs/2512.10169</link>
<guid>https://arxiv.org/abs/2512.10169</guid>
<content:encoded><![CDATA[
arXiv:2512.10169v1 Announce Type: cross 
Abstract: Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Aware Confidence Calibration for Automated Audio Captioning</title>
<link>https://arxiv.org/abs/2512.10170</link>
<guid>https://arxiv.org/abs/2512.10170</guid>
<content:encoded><![CDATA[
arXiv:2512.10170v1 Announce Type: cross 
Abstract: Automated audio captioning models frequently produce overconfident predictions regardless of semantic accuracy, limiting their reliability in deployment. This deficiency stems from two factors: evaluation metrics based on n-gram overlap that fail to capture semantic correctness, and the absence of calibrated confidence estimation. We present a framework that addresses both limitations by integrating confidence prediction into audio captioning and redefining correctness through semantic similarity. Our approach augments a Whisper-based audio captioning model with a learned confidence prediction head that estimates uncertainty from decoder hidden states. We employ CLAP audio-text embeddings and sentence transformer similarities (FENSE) to define semantic correctness, enabling Expected Calibration Error (ECE) computation that reflects true caption quality rather than surface-level text overlap. Experiments on Clotho v2 demonstrate that confidence-guided beam search with semantic evaluation achieves dramatically improved calibration (CLAP-based ECE of 0.071) compared to greedy decoding baselines (ECE of 0.488), while simultaneously improving caption quality across standard metrics. Our results establish that semantic similarity provides a more meaningful foundation for confidence calibration in audio captioning than traditional n-gram metrics.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Interplay of Statistics and Noisy Optimization: Learning Linear Predictors with Random Data Weights</title>
<link>https://arxiv.org/abs/2512.10188</link>
<guid>https://arxiv.org/abs/2512.10188</guid>
<content:encoded><![CDATA[
arXiv:2512.10188v1 Announce Type: cross 
Abstract: We analyze gradient descent with randomly weighted data points in a linear regression model, under a generic weighting distribution. This includes various forms of stochastic gradient descent, importance sampling, but also extends to weighting distributions with arbitrary continuous values, thereby providing a unified framework to analyze the impact of various kinds of noise on the training trajectory. We characterize the implicit regularization induced through the random weighting, connect it with weighted linear regression, and derive non-asymptotic bounds for convergence in first and second moments. Leveraging geometric moment contraction, we also investigate the stationary distribution induced by the added noise. Based on these results, we discuss how specific choices of weighting distribution influence both the underlying optimization problem and statistical properties of the resulting estimator, as well as some examples for which weightings that lead to fast convergence cause bad statistical performance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding</title>
<link>https://arxiv.org/abs/2512.10195</link>
<guid>https://arxiv.org/abs/2512.10195</guid>
<content:encoded><![CDATA[
arXiv:2512.10195v1 Announce Type: cross 
Abstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Learning-Curve Monotonicity for Maximum Likelihood Estimators</title>
<link>https://arxiv.org/abs/2512.10220</link>
<guid>https://arxiv.org/abs/2512.10220</guid>
<content:encoded><![CDATA[
arXiv:2512.10220v1 Announce Type: cross 
Abstract: The property of learning-curve monotonicity, highlighted in a recent series of work by Loog, Mey and Viering, describes algorithms which only improve in average performance given more data, for any underlying data distribution within a given family. We establish the first nontrivial monotonicity guarantees for the maximum likelihood estimator in a variety of well-specified parametric settings. For sequential prediction with log loss, we show monotonicity (in fact complete monotonicity) of the forward KL divergence for Gaussian vectors with unknown covariance and either known or unknown mean, as well as for Gamma variables with unknown scale parameter. The Gaussian setting was explicitly highlighted as open in the aforementioned works, even in dimension 1. Finally we observe that for reverse KL divergence, a folklore trick yields monotonicity for very general exponential families.
  All results in this paper were derived by variants of GPT-5.2 Pro. Humans did not provide any proof strategies or intermediate arguments, but only prompted the model to continue developing additional results, and verified and transcribed its proofs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Galaxy Phase-Space and Field-Level Cosmology: The Strength of Semi-Analytic Models</title>
<link>https://arxiv.org/abs/2512.10222</link>
<guid>https://arxiv.org/abs/2512.10222</guid>
<content:encoded><![CDATA[
arXiv:2512.10222v1 Announce Type: cross 
Abstract: Semi-analytic models are a widely used approach to simulate galaxy properties within a cosmological framework, relying on simplified yet physically motivated prescriptions. They have also proven to be an efficient alternative for generating accurate galaxy catalogs, offering a faster and less computationally expensive option compared to full hydrodynamical simulations. In this paper, we demonstrate that using only galaxy $3$D positions and radial velocities, we can train a graph neural network coupled to a moment neural network to obtain a robust machine learning based model capable of estimating the matter density parameters, $\Omega_{\rm m}$, with a precision of approximately 10%. The network is trained on ($25 h^{-1}$Mpc)$^3$ volumes of galaxy catalogs from L-Galaxies and can successfully extrapolate its predictions to other semi-analytic models (GAEA, SC-SAM, and Shark) and, more remarkably, to hydrodynamical simulations (Astrid, SIMBA, IllustrisTNG, and SWIFT-EAGLE). Our results show that the network is robust to variations in astrophysical and subgrid physics, cosmological and astrophysical parameters, and the different halo-profile treatments used across simulations. This suggests that the physical relationships encoded in the phase-space of semi-analytic models are largely independent of their specific physical prescriptions, reinforcing their potential as tools for the generation of realistic mock catalogs for cosmological parameter inference.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap</title>
<link>https://arxiv.org/abs/2512.10236</link>
<guid>https://arxiv.org/abs/2512.10236</guid>
<content:encoded><![CDATA[
arXiv:2512.10236v1 Announce Type: cross 
Abstract: As both ML training and inference are increasingly distributed, parallelization techniques that shard (divide) ML model across GPUs of a distributed system, are often deployed. With such techniques, there is a high prevalence of data-dependent communication and computation operations where communication is exposed, leaving as high as 1.7x ideal performance on the table. Prior works harness the fact that ML model state and inputs are already sharded, and employ careful overlap of individual computation/communication shards. While such coarse-grain overlap is promising, in this work, we instead make a case for finer-grain compute-communication overlap which we term FiCCO, where we argue for finer-granularity, one-level deeper overlap than at shard-level, to unlock compute/communication overlap for a wider set of network topologies, finer-grain dataflow and more. We show that FiCCO opens up a wider design space of execution schedules than possible at shard-level alone. At the same time, decomposition of ML operations into smaller operations (done in both shard-based and finer-grain techniques) causes operation-level inefficiency losses. To balance the two, we first present a detailed characterization of these inefficiency losses, then present a design space of FiCCO schedules, and finally overlay the schedules with concomitant inefficiency signatures. Doing so helps us design heuristics that frameworks and runtimes can harness to select bespoke FiCCO schedules based on the nature of underlying ML operations. Finally, to further minimize contention inefficiencies inherent with operation overlap, we offload communication to GPU DMA engines. We evaluate several scenarios from realistic ML deployments and demonstrate that our proposed bespoke schedules deliver up to 1.6x speedup and our heuristics provide accurate guidance in 81% of unseen scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective</title>
<link>https://arxiv.org/abs/2512.10244</link>
<guid>https://arxiv.org/abs/2512.10244</guid>
<content:encoded><![CDATA[
arXiv:2512.10244v1 Announce Type: cross 
Abstract: Semi-supervised few-shot learning (SSFSL) formulates real-world applications like ''auto-annotation'', as it aims to learn a model over a few labeled and abundant unlabeled examples to annotate the unlabeled ones. Despite the availability of powerful open-source Vision-Language Models (VLMs) and their pretraining data, the SSFSL literature largely neglects these open-source resources. In contrast, the related area few-shot learning (FSL) has already exploited them to boost performance. Arguably, to achieve auto-annotation in the real world, SSFSL should leverage such open-source resources. To this end, we start by applying established SSL methods to finetune a VLM. Counterintuitively, they significantly underperform FSL baselines. Our in-depth analysis reveals the root cause: VLMs produce rather ''flat'' distributions of softmax probabilities. This results in zero utilization of unlabeled data and weak supervision signals. We address this issue with embarrassingly simple techniques: classifier initialization and temperature tuning. They jointly increase the confidence scores of pseudo-labels, improving the utilization rate of unlabeled data, and strengthening supervision signals. Building on this, we propose: Stage-Wise Finetuning with Temperature Tuning (SWIFT), which enables existing SSL methods to effectively finetune a VLM on limited labeled data, abundant unlabeled data, and task-relevant but noisy data retrieved from the VLM's pretraining set. Extensive experiments on five SSFSL benchmarks show that SWIFT outperforms recent FSL and SSL methods by $\sim$5 accuracy points. SWIFT even rivals supervised learning, which finetunes VLMs with the unlabeled data being labeled with ground truth!
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Analysis of Generalized Langevin Equations with Approximated Memory Kernels</title>
<link>https://arxiv.org/abs/2512.10256</link>
<guid>https://arxiv.org/abs/2512.10256</guid>
<content:encoded><![CDATA[
arXiv:2512.10256v1 Announce Type: cross 
Abstract: We analyze prediction error in stochastic dynamical systems with memory, focusing on generalized Langevin equations (GLEs) formulated as stochastic Volterra equations. We establish that, under a strongly convex potential, trajectory discrepancies decay at a rate determined by the decay of the memory kernel and are quantitatively bounded by the estimation error of the kernel in a weighted norm. Our analysis integrates synchronized noise coupling with a Volterra comparison theorem, encompassing both subexponential and exponential kernel classes. For first-order models, we derive moment and perturbation bounds using resolvent estimates in weighted spaces. For second-order models with confining potentials, we prove contraction and stability under kernel perturbations using a hypocoercive Lyapunov-type distance. This framework accommodates non-translation-invariant kernels and white-noise forcing, explicitly linking improved kernel estimation to enhanced trajectory prediction. Numerical examples validate these theoretical findings.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters</title>
<link>https://arxiv.org/abs/2512.10271</link>
<guid>https://arxiv.org/abs/2512.10271</guid>
<content:encoded><![CDATA[
arXiv:2512.10271v1 Announce Type: cross 
Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neuronal Attention Circuit (NAC) for Representation Learning</title>
<link>https://arxiv.org/abs/2512.10282</link>
<guid>https://arxiv.org/abs/2512.10282</guid>
<content:encoded><![CDATA[
arXiv:2512.10282v1 Announce Type: cross 
Abstract: Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing \textit{C. elegans} Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing \textit{content-target} and \textit{learnable time-constant} gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top-\emph{K} pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning</title>
<link>https://arxiv.org/abs/2512.10296</link>
<guid>https://arxiv.org/abs/2512.10296</guid>
<content:encoded><![CDATA[
arXiv:2512.10296v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative model training across distributed devices while safeguarding data and user privacy. However, FL remains susceptible to privacy threats that can compromise data via direct means. That said, indirectly compromising the confidentiality of the FL model architecture (e.g., a convolutional neural network (CNN) or a recurrent neural network (RNN)) on a client device by an outsider remains unexplored. If leaked, this information can enable next-level attacks tailored to the architecture. This paper proposes a novel side-channel fingerprinting attack, leveraging flow-level and packet-level statistics of encrypted wireless traffic from an FL client to infer its deep learning model architecture. We name it FLARE, a fingerprinting framework based on FL Architecture REconnaissance. Evaluation across various CNN and RNN variants-including pre-trained and custom models trained over IEEE 802.11 Wi-Fi-shows that FLARE achieves over 98% F1-score in closed-world and up to 91% in open-world scenarios. These results reveal that CNN and RNN models leak distinguishable traffic patterns, enabling architecture fingerprinting even under realistic FL settings with hardware, software, and data heterogeneity. To our knowledge, this is the first work to fingerprint FL model architectures by sniffing encrypted wireless traffic, exposing a critical side-channel vulnerability in current FL systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking large chemical reaction networks and rare events by neural networks</title>
<link>https://arxiv.org/abs/2512.10309</link>
<guid>https://arxiv.org/abs/2512.10309</guid>
<content:encoded><![CDATA[
arXiv:2512.10309v1 Announce Type: cross 
Abstract: Chemical reaction networks are widely used to model stochastic dynamics in chemical kinetics, systems biology and epidemiology. Solving the chemical master equation that governs these systems poses a significant challenge due to the large state space exponentially growing with system sizes. The development of autoregressive neural networks offers a flexible framework for this problem; however, its efficiency is limited especially for high-dimensional systems and in scenarios with rare events. Here, we push the frontier of neural-network approach by exploiting faster optimizations such as natural gradient descent and time-dependent variational principle, achieving a 5- to 22-fold speedup, and by leveraging enhanced-sampling strategies to capture rare events. We demonstrate reduced computational cost and higher accuracy over the previous neural-network method in challenging reaction networks, including the mitogen-activated protein kinase (MAPK) cascade network, the hitherto largest biological network handled by the previous approaches of solving the chemical master equation. We further apply the approach to spatially extended reaction-diffusion systems, the Schl\"ogl model with rare events, on two-dimensional lattices, beyond the recent tensor-network approach that handles one-dimensional lattices. The present approach thus enables efficient modeling of chemical reaction networks in general.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual subspace evolution strategies for nonlinear inverse problems</title>
<link>https://arxiv.org/abs/2512.10325</link>
<guid>https://arxiv.org/abs/2512.10325</guid>
<content:encoded><![CDATA[
arXiv:2512.10325v1 Announce Type: cross 
Abstract: Nonlinear inverse problems often feature noisy, non-differentiable, or expensive residual evaluations that make Jacobian-based solvers unreliable. Popular derivative-free optimizers such as natural evolution strategies (NES) or Powell's NEWUOA still assume smoothness or expend many evaluations to maintain stability. Ensemble Kalman inversion (EKI) relies on empirical covariances that require preconditioning and scale poorly with residual dimension.
  We introduce residual subspace evolution strategies (RSES), a derivative-free solver that samples Gaussian probes around the current iterate, builds a residual-only surrogate from their differences, and recombines the probes through a least-squares solve yielding an optimal update without forming Jacobians or covariances. Each iteration costs $k+1$ residual evaluations, where $k \ll n$ for $n$-dimensional problems, with $O(k^3)$ linear algebra overhead.
  Benchmarks on calibration, regression, and deconvolution problems demonstrate consistent misfit reduction in both deterministic and stochastic settings. RSES matches or surpasses xNES and NEWUOA while staying competitive with EKI under matched evaluation budgets, particularly when smoothness or covariance assumptions fail.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D2M: A Decentralized, Privacy-Preserving, Incentive-Compatible Data Marketplace for Collaborative Learning</title>
<link>https://arxiv.org/abs/2512.10372</link>
<guid>https://arxiv.org/abs/2512.10372</guid>
<content:encoded><![CDATA[
arXiv:2512.10372v1 Announce Type: cross 
Abstract: The rising demand for collaborative machine learning and data analytics calls for secure and decentralized data sharing frameworks that balance privacy, trust, and incentives. Existing approaches, including federated learning (FL) and blockchain-based data markets, fall short: FL often depends on trusted aggregators and lacks Byzantine robustness, while blockchain frameworks struggle with computation-intensive training and incentive integration.
  We present \prot, a decentralized data marketplace that unifies federated learning, blockchain arbitration, and economic incentives into a single framework for privacy-preserving data sharing. \prot\ enables data buyers to submit bid-based requests via blockchain smart contracts, which manage auctions, escrow, and dispute resolution. Computationally intensive training is delegated to \cone\ (\uline{Co}mpute \uline{N}etwork for \uline{E}xecution), an off-chain distributed execution layer. To safeguard against adversarial behavior, \prot\ integrates a modified YODA protocol with exponentially growing execution sets for resilient consensus, and introduces Corrected OSMD to mitigate malicious or low-quality contributions from sellers. All protocols are incentive-compatible, and our game-theoretic analysis establishes honesty as the dominant strategy.
  We implement \prot\ on Ethereum and evaluate it over benchmark datasets -- MNIST, Fashion-MNIST, and CIFAR-10 -- under varying adversarial settings. \prot\ achieves up to 99\% accuracy on MNIST and 90\% on Fashion-MNIST, with less than 3\% degradation up to 30\% Byzantine nodes, and 56\% accuracy on CIFAR-10 despite its complexity. Our results show that \prot\ ensures privacy, maintains robustness under adversarial conditions, and scales efficiently with the number of participants, making it a practical foundation for real-world decentralized data sharing.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI</title>
<link>https://arxiv.org/abs/2512.10394</link>
<guid>https://arxiv.org/abs/2512.10394</guid>
<content:encoded><![CDATA[
arXiv:2512.10394v1 Announce Type: cross 
Abstract: Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale</title>
<link>https://arxiv.org/abs/2512.10398</link>
<guid>https://arxiv.org/abs/2512.10398</guid>
<content:encoded><![CDATA[
arXiv:2512.10398v1 Announce Type: cross 
Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion differentiable resampling</title>
<link>https://arxiv.org/abs/2512.10401</link>
<guid>https://arxiv.org/abs/2512.10401</guid>
<content:encoded><![CDATA[
arXiv:2512.10401v1 Announce Type: cross 
Abstract: This paper is concerned with differentiable resampling in the context of sequential Monte Carlo (e.g., particle filtering). We propose a new informative resampling method that is instantly pathwise differentiable, based on an ensemble score diffusion model. We prove that our diffusion resampling method provides a consistent estimate to the resampling distribution, and we show by experiments that it outperforms the state-of-the-art differentiable resampling methods when used for stochastic filtering and parameter estimation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised Learning of Random Neural Architectures Structured by Latent Random Fields on Compact Boundaryless Multiply-Connected Manifolds</title>
<link>https://arxiv.org/abs/2512.10407</link>
<guid>https://arxiv.org/abs/2512.10407</guid>
<content:encoded><![CDATA[
arXiv:2512.10407v1 Announce Type: cross 
Abstract: This paper introduces a new probabilistic framework for supervised learning in neural systems. It is designed to model complex, uncertain systems whose random outputs are strongly non-Gaussian given deterministic inputs. The architecture itself is a random object stochastically generated by a latent anisotropic Gaussian random field defined on a compact, boundaryless, multiply-connected manifold. The goal is to establish a novel conceptual and mathematical framework in which neural architectures are realizations of a geometry-aware, field-driven generative process. Both the neural topology and synaptic weights emerge jointly from a latent random field. A reduced-order parameterization governs the spatial intensity of an inhomogeneous Poisson process on the manifold, from which neuron locations are sampled. Input and output neurons are identified via extremal evaluations of the latent field, while connectivity is established through geodesic proximity and local field affinity. Synaptic weights are conditionally sampled from the field realization, inducing stochastic output responses even for deterministic inputs. To ensure scalability, the architecture is sparsified via percentile-based diffusion masking, yielding geometry-aware sparse connectivity without ad hoc structural assumptions. Supervised learning is formulated as inference on the generative hyperparameters of the latent field, using a negative log-likelihood loss estimated through Monte Carlo sampling from single-observation-per-input datasets. The paper initiates a mathematical analysis of the model, establishing foundational properties such as well-posedness, measurability, and a preliminary analysis of the expressive variability of the induced stochastic mappings, which support its internal coherence and lay the groundwork for a broader theory of geometry-driven stochastic learning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustered Federated Learning with Hierarchical Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.10443</link>
<guid>https://arxiv.org/abs/2512.10443</guid>
<content:encoded><![CDATA[
arXiv:2512.10443v1 Announce Type: cross 
Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maximum Risk Minimization with Random Forests</title>
<link>https://arxiv.org/abs/2512.10445</link>
<guid>https://arxiv.org/abs/2512.10445</guid>
<content:encoded><![CDATA[
arXiv:2512.10445v1 Announce Type: cross 
Abstract: We consider a regression setting where observations are collected in different environments modeled by different data distributions. The field of out-of-distribution (OOD) generalization aims to design methods that generalize better to test environments whose distributions differ from those observed during training. One line of such works has proposed to minimize the maximum risk across environments, a principle that we refer to as MaxRM (Maximum Risk Minimization). In this work, we introduce variants of random forests based on the principle of MaxRM. We provide computationally efficient algorithms and prove statistical consistency for our primary method. Our proposed method can be used with each of the following three risks: the mean squared error, the negative reward (which relates to the explained variance), and the regret (which quantifies the excess risk relative to the best predictor). For MaxRM with regret as the risk, we prove a novel out-of-sample guarantee over unseen test distributions. Finally, we evaluate the proposed methods on both simulated and real-world data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection</title>
<link>https://arxiv.org/abs/2512.10485</link>
<guid>https://arxiv.org/abs/2512.10485</guid>
<content:encoded><![CDATA[
arXiv:2512.10485v1 Announce Type: cross 
Abstract: Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperspectral Image Data Reduction for Endmember Extraction</title>
<link>https://arxiv.org/abs/2512.10506</link>
<guid>https://arxiv.org/abs/2512.10506</guid>
<content:encoded><![CDATA[
arXiv:2512.10506v1 Announce Type: cross 
Abstract: Endmember extraction from hyperspectral images aims to identify the spectral signatures of materials present in a scene. Recent studies have shown that self-dictionary methods can achieve high extraction accuracy; however, their high computational cost limits their applicability to large-scale hyperspectral images. Although several approaches have been proposed to mitigate this issue, it remains a major challenge. Motivated by this situation, this paper pursues a data reduction approach. Assuming that the hyperspectral image follows the linear mixing model with the pure-pixel assumption, we develop a data reduction technique that removes pixels that do not contain endmembers. We analyze the theoretical properties of this reduction step and show that it preserves pixels that lie close to the endmembers. Building on this result, we propose a data-reduced self-dictionary method that integrates the data reduction with a self-dictionary method based on a linear programming formulation. Numerical experiments demonstrate that the proposed method can substantially reduce the computational time of the original self-dictionary method without sacrificing endmember extraction accuracy.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Auction: Generative Auction towards LLM-Native Advertising</title>
<link>https://arxiv.org/abs/2512.10551</link>
<guid>https://arxiv.org/abs/2512.10551</guid>
<content:encoded><![CDATA[
arXiv:2512.10551v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models</title>
<link>https://arxiv.org/abs/2512.10561</link>
<guid>https://arxiv.org/abs/2512.10561</guid>
<content:encoded><![CDATA[
arXiv:2512.10561v1 Announce Type: cross 
Abstract: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flexible Deep Neural Networks for Partially Linear Survival Data</title>
<link>https://arxiv.org/abs/2512.10570</link>
<guid>https://arxiv.org/abs/2512.10570</guid>
<content:encoded><![CDATA[
arXiv:2512.10570v1 Announce Type: cross 
Abstract: We propose a flexible deep neural network (DNN) framework for modeling survival data within a partially linear regression structure. The approach preserves interpretability through a parametric linear component for covariates of primary interest, while a nonparametric DNN component captures complex time-covariate interactions among nuisance variables. We refer to the method as FLEXI-Haz, a flexible hazard model with a partially linear structure. In contrast to existing DNN approaches for partially linear Cox models, FLEXI-Haz does not rely on the proportional hazards assumption. We establish theoretical guarantees: the neural network component attains minimax-optimal convergence rates based on composite Holder classes, and the linear estimator is root-n consistent, asymptotically normal, and semiparametrically efficient. Extensive simulations and real-data analyses demonstrate that FLEXI-Haz provides accurate estimation of the linear effect, offering a principled and interpretable alternative to modern methods based on proportional hazards. Code for implementing FLEXI-Haz, as well as scripts for reproducing data analyses and simulations, is available at: https://github.com/AsafBanana/FLEXI-Haz
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topology-Guided Quantum GANs for Constrained Graph Generation</title>
<link>https://arxiv.org/abs/2512.10582</link>
<guid>https://arxiv.org/abs/2512.10582</guid>
<content:encoded><![CDATA[
arXiv:2512.10582v1 Announce Type: cross 
Abstract: Quantum computing (QC) promises theoretical advantages, benefiting computational problems that would not be efficiently classically simulatable. However, much of this theoretical speedup depends on the quantum circuit design solving the problem. We argue that QC literature has yet to explore more domain specific ansatz-topologies, instead of relying on generic, one-size-fits-all architectures. In this work, we show that incorporating task-specific inductive biases -- specifically geometric priors -- into quantum circuit design can enhance the performance of hybrid Quantum Generative Adversarial Networks (QuGANs) on the task of generating geometrically constrained K4 graphs. We evaluate a portfolio of entanglement topologies and loss-function designs to assess their impact on both statistical fidelity and compliance with geometric constraints, including the Triangle and Ptolemaic inequalities. Our results show that aligning circuit topology with the underlying problem structure yields substantial benefits: the Triangle-topology QuGAN achieves the highest geometric validity among quantum models and matches the performance of classical Generative Adversarial Networks (GAN). Additionally, we showcase how specific architectural choices, such as entangling gate types, variance regularization and output-scaling govern the trade-off between geometric consistency and distributional accuracy, thus emphasizing the value of structured, task-aware quantum ansatz-topologies.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Authority Backdoor: A Certifiable Backdoor Mechanism for Authoring DNNs</title>
<link>https://arxiv.org/abs/2512.10600</link>
<guid>https://arxiv.org/abs/2512.10600</guid>
<content:encoded><![CDATA[
arXiv:2512.10600v1 Announce Type: cross 
Abstract: Deep Neural Networks (DNNs), as valuable intellectual property, face unauthorized use. Existing protections, such as digital watermarking, are largely passive; they provide only post-hoc ownership verification and cannot actively prevent the illicit use of a stolen model. This work proposes a proactive protection scheme, dubbed ``Authority Backdoor," which embeds access constraints directly into the model. In particular, the scheme utilizes a backdoor learning framework to intrinsically lock a model's utility, such that it performs normally only in the presence of a specific trigger (e.g., a hardware fingerprint). But in its absence, the DNN's performance degrades to be useless. To further enhance the security of the proposed authority scheme, the certifiable robustness is integrated to prevent an adaptive attacker from removing the implanted backdoor. The resulting framework establishes a secure authority mechanism for DNNs, combining access control with certifiable robustness against adversarial attacks. Extensive experiments on diverse architectures and datasets validate the effectiveness and certifiable robustness of the proposed framework.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Intrusion Detection System Leveraging Dynamic Neural Models with Adversarial Learning for 5G/6G Networks</title>
<link>https://arxiv.org/abs/2512.10637</link>
<guid>https://arxiv.org/abs/2512.10637</guid>
<content:encoded><![CDATA[
arXiv:2512.10637v1 Announce Type: cross 
Abstract: Intrusion Detection Systems (IDS) are critical components in safeguarding 5G/6G networks from both internal and external cyber threats. While traditional IDS approaches rely heavily on signature-based methods, they struggle to detect novel and evolving attacks. This paper presents an advanced IDS framework that leverages adversarial training and dynamic neural networks in 5G/6G networks to enhance network security by providing robust, real-time threat detection and response capabilities. Unlike conventional models, which require costly retraining to update knowledge, the proposed framework integrates incremental learning algorithms, reducing the need for frequent retraining. Adversarial training is used to fortify the IDS against poisoned data. By using fewer features and incorporating statistical properties, the system can efficiently detect potential threats. Extensive evaluations using the NSL- KDD dataset demonstrate that the proposed approach provides better accuracy of 82.33% for multiclass classification of various network attacks while resisting dataset poisoning. This research highlights the potential of adversarial-trained, dynamic neural networks for building resilient IDS solutions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification</title>
<link>https://arxiv.org/abs/2512.10640</link>
<guid>https://arxiv.org/abs/2512.10640</guid>
<content:encoded><![CDATA[
arXiv:2512.10640v1 Announce Type: cross 
Abstract: Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Virtual camera detection: Catching video injection attacks in remote biometric systems</title>
<link>https://arxiv.org/abs/2512.10653</link>
<guid>https://arxiv.org/abs/2512.10653</guid>
<content:encoded><![CDATA[
arXiv:2512.10653v1 Announce Type: cross 
Abstract: Face anti-spoofing (FAS) is a vital component of remote biometric authentication systems based on facial recognition, increasingly used across web-based applications. Among emerging threats, video injection attacks -- facilitated by technologies such as deepfakes and virtual camera software -- pose significant challenges to system integrity. While virtual camera detection (VCD) has shown potential as a countermeasure, existing literature offers limited insight into its practical implementation and evaluation. This study introduces a machine learning-based approach to VCD, with a focus on its design and validation. The model is trained on metadata collected during sessions with authentic users. Empirical results demonstrate its effectiveness in identifying video injection attempts and reducing the risk of malicious users bypassing FAS systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search</title>
<link>https://arxiv.org/abs/2512.10671</link>
<guid>https://arxiv.org/abs/2512.10671</guid>
<content:encoded><![CDATA[
arXiv:2512.10671v1 Announce Type: cross 
Abstract: Early-exit networks are effective solutions for reducing the overall energy consumption and latency of deep learning models by adjusting computation based on the complexity of input data. By incorporating intermediate exit branches into the architecture, they provide less computation for simpler samples, which is particularly beneficial for resource-constrained devices where energy consumption is crucial. However, designing early-exit networks is a challenging and time-consuming process due to the need to balance efficiency and performance. Recent works have utilized Neural Architecture Search (NAS) to design more efficient early-exit networks, aiming to reduce average latency while improving model accuracy by determining the best positions and number of exit branches in the architecture. Another important factor affecting the efficiency and accuracy of early-exit networks is the depth and types of layers in the exit branches. In this paper, we use hardware-aware NAS to strengthen exit branches, considering both accuracy and efficiency during optimization. Our performance evaluation on the CIFAR-10, CIFAR-100, and SVHN datasets demonstrates that our proposed framework, which considers varying depths and layers for exit branches along with adaptive threshold tuning, designs early-exit networks that achieve higher accuracy with the same or lower average number of MACs compared to the state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Gemini Robotics Policies in a Veo World Simulator</title>
<link>https://arxiv.org/abs/2512.10675</link>
<guid>https://arxiv.org/abs/2512.10675</guid>
<content:encoded><![CDATA[
arXiv:2512.10675v1 Announce Type: cross 
Abstract: Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal transport unlocks end-to-end learning for single-molecule localization</title>
<link>https://arxiv.org/abs/2512.10683</link>
<guid>https://arxiv.org/abs/2512.10683</guid>
<content:encoded><![CDATA[
arXiv:2512.10683v1 Announce Type: cross 
Abstract: Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharp Monocular View Synthesis in Less Than a Second</title>
<link>https://arxiv.org/abs/2512.10685</link>
<guid>https://arxiv.org/abs/2512.10685</guid>
<content:encoded><![CDATA[
arXiv:2512.10685v1 Announce Type: cross 
Abstract: We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PMB-NN: Physiology-Centred Hybrid AI for Personalized Hemodynamic Monitoring from Photoplethysmography</title>
<link>https://arxiv.org/abs/2512.10745</link>
<guid>https://arxiv.org/abs/2512.10745</guid>
<content:encoded><![CDATA[
arXiv:2512.10745v1 Announce Type: cross 
Abstract: Continuous monitoring of blood pressure (BP) and hemodynamic parameters such as peripheral resistance (R) and arterial compliance (C) are critical for early vascular dysfunction detection. While photoplethysmography (PPG) wearables has gained popularity, existing data-driven methods for BP estimation lack interpretability. We advanced our previously proposed physiology-centered hybrid AI method-Physiological Model-Based Neural Network (PMB-NN)-in blood pressure estimation, that unifies deep learning with a 2-element Windkessel based model parameterized by R and C acting as physics constraints. The PMB-NN model was trained in a subject-specific manner using PPG-derived timing features, while demographic information was used to infer an intermediate variable: cardiac output. We validated the model on 10 healthy adults performing static and cycling activities across two days for model's day-to-day robustness, benchmarked against deep learning (DL) models (FCNN, CNN-LSTM, Transformer) and standalone Windkessel based physiological model (PM). Validation was conducted on three perspectives: accuracy, interpretability and plausibility. PMB-NN achieved systolic BP accuracy (MAE: 7.2 mmHg) comparable to DL benchmarks, diastolic performance (MAE: 3.9 mmHg) lower than DL models. However, PMB-NN exhibited higher physiological plausibility than both DL baselines and PM, suggesting that the hybrid architecture unifies and enhances the respective merits of physiological principles and data-driven techniques. Beyond BP, PMB-NN identified R (ME: 0.15 mmHg$\cdot$s/ml) and C (ME: -0.35 ml/mmHg) during training with accuracy similar to PM, demonstrating that the embedded physiological constraints confer interpretability to the hybrid AI framework. These results position PMB-NN as a balanced, physiologically grounded alternative to purely data-driven approaches for daily hemodynamic monitoring.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification</title>
<link>https://arxiv.org/abs/2512.10756</link>
<guid>https://arxiv.org/abs/2512.10756</guid>
<content:encoded><![CDATA[
arXiv:2512.10756v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation</title>
<link>https://arxiv.org/abs/2512.10772</link>
<guid>https://arxiv.org/abs/2512.10772</guid>
<content:encoded><![CDATA[
arXiv:2512.10772v1 Announce Type: cross 
Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting</title>
<link>https://arxiv.org/abs/2512.10780</link>
<guid>https://arxiv.org/abs/2512.10780</guid>
<content:encoded><![CDATA[
arXiv:2512.10780v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What matters for Representation Alignment: Global Information or Spatial Structure?</title>
<link>https://arxiv.org/abs/2512.10794</link>
<guid>https://arxiv.org/abs/2512.10794</guid>
<content:encoded><![CDATA[
arXiv:2512.10794v1 Announce Type: cross 
Abstract: Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \textit{global} \revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Approaches to Urban Logistics: From Core QAOA to Clustered Scalability</title>
<link>https://arxiv.org/abs/2512.10813</link>
<guid>https://arxiv.org/abs/2512.10813</guid>
<content:encoded><![CDATA[
arXiv:2512.10813v1 Announce Type: cross 
Abstract: The Traveling Salesman Problem (TSP) is a fundamental challenge in combinatorial optimization, widely applied in logistics and transportation. As the size of TSP instances grows, traditional algorithms often struggle to produce high-quality solutions within reasonable timeframes. This study investigates the potential of the Quantum Approximate Optimization Algorithm (QAOA), a hybrid quantum-classical method, to solve TSP under realistic constraints. We adopt a QUBO-based formulation of TSP that integrates real-world logistical constraints reflecting operational conditions, such as vehicle capacity, road accessibility, and time windows, while ensuring compatibility with the limitations of current quantum hardware. Our experiments are conducted in a simulated environment using high-performance computing (HPC) resources to assess QAOA's performance across different problem sizes and quantum circuit depths. In order to improve scalability, we propose clustering QAOA (Cl-QAOA), a hybrid approach combining classical machine learning with QAOA. This method decomposes large TSP instances into smaller sub-problems, making quantum optimization feasible even on devices with a limited number of qubits. The results offer a comprehensive evaluation of QAOA's strengths and limitations in solving constrained TSP scenarios. This study advances quantum optimization and lays groundwork for future large-scale applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep sets and event-level maximum-likelihood estimation for fast pile-up jet rejection in ATLAS</title>
<link>https://arxiv.org/abs/2512.10819</link>
<guid>https://arxiv.org/abs/2512.10819</guid>
<content:encoded><![CDATA[
arXiv:2512.10819v1 Announce Type: cross 
Abstract: Multiple proton-proton collisions (pile-up) occur at every bunch crossing at the LHC, with the mean number of interactions expected to reach 80 during Run 3 and up to 200 at the High-Luminosity LHC. As a direct consequence, events with multijet signatures will occur at increasingly high rates. To cope with the increased luminosity, being able to efficiently group jets according to their origin along the beamline is crucial, particularly at the trigger level. In this work, a novel uncertainty-aware jet regression model based on a Deep Sets architecture is introduced, DIPz, to regress on a jet origin position along the beamline. The inputs to the DIPz algorithm are the charged particle tracks associated to each jet. An event-level discriminant, the Maximum Log Product of Likelihoods (MLPL), is constructed by combining the DIPz per-jet predictions. MLPL is cut-optimized to select events compatible with targeted multi-jet signature selection. This combined approach provides a robust and computationally efficient method for pile-up rejection in multi-jet final states, applicable to real-time event selections at the ATLAS High Level Trigger.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agile Deliberation: Concept Deliberation for Subjective Visual Classification</title>
<link>https://arxiv.org/abs/2512.10821</link>
<guid>https://arxiv.org/abs/2512.10821</guid>
<content:encoded><![CDATA[
arXiv:2512.10821v1 Announce Type: cross 
Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Elementary Proof of the Near Optimality of LogSumExp Smoothing</title>
<link>https://arxiv.org/abs/2512.10825</link>
<guid>https://arxiv.org/abs/2512.10825</guid>
<content:encoded><![CDATA[
arXiv:2512.10825v1 Announce Type: cross 
Abstract: We consider the design of smoothings of the (coordinate-wise) max function in $\mathbb{R}^d$ in the infinity norm. The LogSumExp function $f(x)=\ln(\sum^d_i\exp(x_i))$ provides a classical smoothing, differing from the max function in value by at most $\ln(d)$. We provide an elementary construction of a lower bound, establishing that every overestimating smoothing of the max function must differ by at least $\sim 0.8145\ln(d)$. Hence, LogSumExp is optimal up to constant factors. However, in small dimensions, we provide stronger, exactly optimal smoothings attaining our lower bound, showing that the entropy-based LogSumExp approach to smoothing is not exactly optimal.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-informed Polynomial Chaos Expansion with Enhanced Constrained Optimization Solver and D-optimal Sampling</title>
<link>https://arxiv.org/abs/2512.10873</link>
<guid>https://arxiv.org/abs/2512.10873</guid>
<content:encoded><![CDATA[
arXiv:2512.10873v1 Announce Type: cross 
Abstract: Physics-informed polynomial chaos expansions (PC$^2$) provide an efficient physically constrained surrogate modeling framework by embedding governing equations and other physical constraints into the standard data-driven polynomial chaos expansions (PCE) and solving via the Karush-Kuhn-Tucker (KKT) conditions. This approach improves the physical interpretability of surrogate models while achieving high computational efficiency and accuracy. However, the performance and efficiency of PC$^2$ can still be degraded with high-dimensional parameter spaces, limited data availability, or unrepresentative training data. To address this problem, this study explores two complementary enhancements to the PC$^2$ framework. First, a numerically efficient constrained optimization solver, straightforward updating of Lagrange multipliers (SULM), is adopted as an alternative to the conventional KKT solver. The SULM method significantly reduces computational cost when solving physically constrained problems with high-dimensionality and derivative boundary conditions that require a large number of virtual points. Second, a D-optimal sampling strategy is utilized to select informative virtual points to improve the stability and achieve the balance of accuracy and efficiency of the PC$^2$. The proposed methods are integrated into the PC$^2$ framework and evaluated through numerical examples of representative physical systems governed by ordinary or partial differential equations. The results demonstrate that the enhanced PC$^2$ has better comprehensive capability than standard PC$^2$, and is well-suited for high-dimensional uncertainty quantification tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Differentiable Digital Twin of Distributed Link Scheduling for Contention-Aware Networking</title>
<link>https://arxiv.org/abs/2512.10874</link>
<guid>https://arxiv.org/abs/2512.10874</guid>
<content:encoded><![CDATA[
arXiv:2512.10874v1 Announce Type: cross 
Abstract: Many routing and flow optimization problems in wired networks can be solved efficiently using minimum cost flow formulations. However, this approach does not extend to wireless multi-hop networks, where the assumptions of fixed link capacity and linear cost structure collapse due to contention for shared spectrum resources. The key challenge is that the long-term capacity of a wireless link becomes a non-linear function of its network context, including network topology, link quality, and the traffic assigned to neighboring links. In this work, we pursue a new direction of modeling wireless network under randomized medium access control by developing an analytical network digital twin (NDT) that predicts link duty cycles from network context. We generalize randomized contention as finding a Maximal Independent Set (MIS) on the conflict graph using weighted Luby's algorithm, derive an analytical model of link duty cycles, and introduce an iterative procedure that resolves the circular dependency among duty cycle, link capacity, and contention probability. Our numerical experiments show that the proposed NDT accurately predicts link duty cycles and congestion patterns with up to a 5000x speedup over packet-level simulation, and enables us to optimize link scheduling using gradient descent for reduced congestion and radio footprint.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Compositional Data Generation for Robot Control</title>
<link>https://arxiv.org/abs/2512.10891</link>
<guid>https://arxiv.org/abs/2512.10891</guid>
<content:encoded><![CDATA[
arXiv:2512.10891v1 Announce Type: cross 
Abstract: Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Regret Optimal Control Under Moment-Based Ambiguity Sets</title>
<link>https://arxiv.org/abs/2512.10906</link>
<guid>https://arxiv.org/abs/2512.10906</guid>
<content:encoded><![CDATA[
arXiv:2512.10906v1 Announce Type: cross 
Abstract: In this paper, we consider a class of finite-horizon, linear-quadratic stochastic control problems, where the probability distribution governing the noise process is unknown but assumed to belong to an ambiguity set consisting of all distributions whose mean and covariance lie within norm balls centered at given nominal values. To address the distributional ambiguity, we explore the design of causal affine control policies to minimize the worst-case expected regret over all distributions in the given ambiguity set. The resulting minimax optimal control problem is shown to admit an equivalent reformulation as a tractable convex program that corresponds to a regularized version of the nominal linear-quadratic stochastic control problem. While this convex program can be recast as a semidefinite program, semidefinite programs are typically solved using primal-dual interior point methods that scale poorly with the problem size in practice. To address this limitation, we propose a scalable dual projected subgradient method to compute optimal controllers to an arbitrary accuracy. Numerical experiments are presented to benchmark the proposed method against state-of-the-art data-driven and distributionally robust control design approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hermitian Yang--Mills connections on general vector bundles: geometry and physical Yukawa couplings</title>
<link>https://arxiv.org/abs/2512.10907</link>
<guid>https://arxiv.org/abs/2512.10907</guid>
<content:encoded><![CDATA[
arXiv:2512.10907v1 Announce Type: cross 
Abstract: We compute solutions to the Hermitian Yang-Mills equations on holomorphic vector bundles $V$ via an alternating optimisation procedure founded on geometric machine learning. The proposed method is fully general with respect to the rank and structure group of $V$, requiring only the ability to enumerate a basis of global sections for a given bundle. This enables us to compute the physically normalised Yukawa couplings in a broad class of heterotic string compactifications. Using this method, we carry out this computation in full for a heterotic compactification incorporating a gauge bundle with non-Abelian structure group.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noisy Quantum Learning Theory</title>
<link>https://arxiv.org/abs/2512.10929</link>
<guid>https://arxiv.org/abs/2512.10929</guid>
<content:encoded><![CDATA[
arXiv:2512.10929v1 Announce Type: cross 
Abstract: We develop a framework for learning from noisy quantum experiments, focusing on fault-tolerant devices accessing uncharacterized systems through noisy couplings. Our starting point is the complexity class $\textsf{NBQP}$ ("noisy BQP"), modeling noisy fault-tolerant quantum computers that cannot, in general, error-correct the oracle systems they query. Using this class, we show that for natural oracle problems, noise can eliminate exponential quantum learning advantages of ideal noiseless learners while preserving a superpolynomial gap between NISQ and fault-tolerant devices. Beyond oracle separations, we study concrete noisy learning tasks. For purity testing, the exponential two-copy advantage collapses under a single application of local depolarizing noise. Nevertheless, we identify a setting motivated by AdS/CFT in which noise-resilient structure restores a quantum learning advantage in a noisy regime. We then analyze noisy Pauli shadow tomography, deriving lower bounds that characterize how instance size, quantum memory, and noise control sample complexity, and design algorithms with parametrically similar scalings. Together, our results show that the Bell-basis and SWAP-test primitives underlying most exponential quantum learning advantages are fundamentally fragile to noise unless the experimental system has latent noise-robust structure. Thus, realizing meaningful quantum advantages in future experiments will require understanding how noise-robust physical properties interface with available algorithmic techniques.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit</title>
<link>https://arxiv.org/abs/2512.10934</link>
<guid>https://arxiv.org/abs/2512.10934</guid>
<content:encoded><![CDATA[
arXiv:2512.10934v1 Announce Type: cross 
Abstract: Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.
  The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any4D: Unified Feed-Forward Metric 4D Reconstruction</title>
<link>https://arxiv.org/abs/2512.10935</link>
<guid>https://arxiv.org/abs/2512.10935</guid>
<content:encoded><![CDATA[
arXiv:2512.10935v1 Announce Type: cross 
Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning</title>
<link>https://arxiv.org/abs/2512.10946</link>
<guid>https://arxiv.org/abs/2512.10946</guid>
<content:encoded><![CDATA[
arXiv:2512.10946v1 Announce Type: cross 
Abstract: Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noisy Spiking Actor Network for Exploration</title>
<link>https://arxiv.org/abs/2403.04162</link>
<guid>https://arxiv.org/abs/2403.04162</guid>
<content:encoded><![CDATA[
arXiv:2403.04162v2 Announce Type: replace 
Abstract: As a general method for exploration in deep reinforcement learning (RL), NoisyNet can produce problem-specific exploration strategies. Spiking neural networks (SNNs), due to their binary firing mechanism, have strong robustness to noise, making it difficult to realize efficient exploration with local disturbances. To solve this exploration problem, we propose a noisy spiking actor network (NoisySAN) that introduces time-correlated noise during charging and transmission. Moreover, a noise reduction method is proposed to find a stable policy for the agent. Extensive experimental results demonstrate that our method outperforms the state-of-the-art performance on a wide range of continuous control tasks from OpenAI gym.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization</title>
<link>https://arxiv.org/abs/2411.03752</link>
<guid>https://arxiv.org/abs/2411.03752</guid>
<content:encoded><![CDATA[
arXiv:2411.03752v3 Announce Type: replace 
Abstract: Recent studies have shown that deep learning models are very vulnerable to poisoning attacks. Many defense methods have been proposed to address this issue. However, traditional poisoning attacks are not as threatening as commonly believed. This is because they often cause differences in how the model performs on the training set compared to the validation set. Such inconsistency can alert defenders that their data has been poisoned, allowing them to take the necessary defensive actions. In this paper, we introduce a more threatening type of poisoning attack called the Deferred Poisoning Attack. This new attack allows the model to function normally during the training and validation phases but makes it very sensitive to evasion attacks or even natural noise. We achieve this by ensuring the poisoned model's loss function has a similar value as a normally trained model at each input sample but with a large local curvature. A similar model loss ensures that there is no obvious inconsistency between the training and validation accuracy, demonstrating high stealthiness. On the other hand, the large curvature implies that a small perturbation may cause a significant increase in model loss, leading to substantial performance degradation, which reflects a worse robustness. We fulfill this purpose by making the model have singular Hessian information at the optimal point via our proposed Singularization Regularization term. We have conducted both theoretical and empirical analyses of the proposed method and validated its effectiveness through experiments on image classification tasks. Furthermore, we have confirmed the hazards of this form of poisoning attack under more general scenarios using natural noise, offering a new perspective for research in the field of security.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Spatial Clustering of Single-Molecule Localizations with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2412.00173</link>
<guid>https://arxiv.org/abs/2412.00173</guid>
<content:encoded><![CDATA[
arXiv:2412.00173v2 Announce Type: replace 
Abstract: Single-molecule localization microscopy generates point clouds corresponding to fluorophore localizations. Spatial cluster identification and analysis of these point clouds are crucial for extracting insights about molecular organization. However, this task becomes challenging in the presence of localization noise, high point density, or complex biological structures. Here, we introduce MIRO (Multifunctional Integration through Relational Optimization), an algorithm that uses recurrent graph neural networks to transform the point clouds in order to improve clustering efficiency when applying conventional clustering techniques. We show that MIRO supports simultaneous processing of clusters of different shapes and at multiple scales, demonstrating improved performance across varied datasets. Our comprehensive evaluation demonstrates MIRO's transformative potential for single-molecule localization applications, showcasing its capability to revolutionize cluster analysis and provide accurate, reliable details of molecular architecture. In addition, MIRO's robust clustering capabilities hold promise for applications in various fields such as neuroscience, for the analysis of neural connectivity patterns, and environmental science, for studying spatial distributions of ecological data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation</title>
<link>https://arxiv.org/abs/2502.09884</link>
<guid>https://arxiv.org/abs/2502.09884</guid>
<content:encoded><![CDATA[
arXiv:2502.09884v3 Announce Type: replace 
Abstract: We consider linear two-time-scale stochastic approximation algorithms driven by martingale noise. Recent applications in machine learning motivate the need to understand finite-time error rates, but conventional stochastic approximation analysis focus on either asymptotic convergence in distribution or finite-time bounds that are far from optimal. Prior work on asymptotic central limit theorems (CLTs) suggest that two-time-scale algorithms may be able to achieve $1/\sqrt{n}$ error in expectation, with a constant given by the expected norm of the limiting Gaussian vector. However, the best known finite-time rates are much slower. We derive the first nonasymptotic central limit theorem with respect to the Wasserstein-1 distance for two-time-scale stochastic approximation with Polyak-Ruppert averaging. As a corollary, we show that expected error achieved by Polyak-Ruppert averaging decays at rate $1/\sqrt{n}$, which significantly improves on the rates of convergence in prior works.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret in Noise-Free Gaussian Process Bandits</title>
<link>https://arxiv.org/abs/2502.19006</link>
<guid>https://arxiv.org/abs/2502.19006</guid>
<content:encoded><![CDATA[
arXiv:2502.19006v2 Announce Type: replace 
Abstract: We study the noise-free Gaussian Process (GP) bandits problem, in which the learner seeks to minimize regret through noise-free observations of the black-box objective function lying on the known reproducing kernel Hilbert space (RKHS). Gaussian process upper confidence bound (GP-UCB) is the well-known GP-bandits algorithm whose query points are adaptively chosen based on the GP-based upper confidence bound score. Although several existing works have reported the practical success of GP-UCB, the current theoretical results indicate its suboptimal performance. However, GP-UCB tends to perform well empirically compared with other nearly optimal noise-free algorithms that rely on a non-adaptive sampling scheme of query points. This paper resolves this gap between theoretical and empirical performance by showing the nearly optimal regret upper bound of noise-free GP-UCB. Specifically, our analysis shows the first constant cumulative regret in the noise-free settings for the squared exponential kernel and Mat\'ern kernel with some degree of smoothness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Internal Evaluation of Density-Based Clusterings with Noise</title>
<link>https://arxiv.org/abs/2503.00127</link>
<guid>https://arxiv.org/abs/2503.00127</guid>
<content:encoded><![CDATA[
arXiv:2503.00127v2 Announce Type: replace 
Abstract: Being able to evaluate the quality of a clustering result even in the absence of ground truth cluster labels is fundamental for research in data mining. However, most cluster validation indices (CVIs) do not capture noise assignments by density-based clustering methods like DBSCAN or HDBSCAN, even though the ability to correctly determine noise is crucial for successful clustering. In this paper, we propose DISCO, a Density-based Internal Score for Clusterings with nOise, the first CVI to explicitly assess the quality of noise assignments rather than merely counting them. DISCO is based on the established idea of the Silhouette Coefficient, but adopts density-connectivity to evaluate clusters of arbitrary shapes, and proposes explicit noise evaluation: it rewards correctly assigned noise labels and penalizes noise labels where a cluster label would have been more appropriate. The pointwise definition of DISCO allows for the seamless integration of noise evaluation into the final clustering evaluation, while also enabling explainable evaluations of the clustered data. In contrast to most state-of-the-art, DISCO is well-defined and also covers edge cases that regularly appear as output from clustering algorithms, such as singleton clusters or a single cluster plus noise.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM4FS: Leveraging Large Language Models for Feature Selection</title>
<link>https://arxiv.org/abs/2503.24157</link>
<guid>https://arxiv.org/abs/2503.24157</guid>
<content:encoded><![CDATA[
arXiv:2503.24157v4 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have provided new opportunities for decision-making, particularly in the task of automated feature selection. In this paper, we first comprehensively evaluate LLM-based feature selection methods, covering the state-of-the-art DeepSeek-R1, GPT-o3-mini, and GPT-4.5. Then, we propose a new hybrid strategy called LLM4FS that integrates LLMs with traditional data-driven methods. Specifically, input data samples into LLMs, and directly call traditional data-driven techniques such as random forest and forward sequential selection. Notably, our analysis reveals that the hybrid strategy leverages the contextual understanding of LLMs and the high statistical reliability of traditional data-driven methods to achieve excellent feature selection performance, even surpassing LLMs and traditional data-driven methods. Finally, we point out the limitations of its application in decision-making. Our code is available at https://github.com/xianchaoxiu/LLM4FS.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Online Class-Incremental Learning via Dual Classifiers</title>
<link>https://arxiv.org/abs/2504.20566</link>
<guid>https://arxiv.org/abs/2504.20566</guid>
<content:encoded><![CDATA[
arXiv:2504.20566v2 Announce Type: replace 
Abstract: Online class-incremental learning (OCIL) focuses on gradually learning new classes (called plasticity) from a stream of data in a single-pass, while concurrently preserving knowledge of previously learned classes (called stability). The primary challenge in OCIL lies in maintaining a good balance between the knowledge of old and new classes within the continually updated model. Most existing methods rely on explicit knowledge interaction through experience replay, and often employ exclusive training separation to address bias problems. Nevertheless, it still remains a big challenge to achieve a well-balanced learner, as these methods often exhibit either reduced plasticity or limited stability due to difficulties in continually integrating knowledge in the OCIL setting. In this paper, we propose a novel replay-based method, called Balanced Inclusive Separation for Online iNcremental learning (BISON), which can achieve both high plasticity and stability, thus ensuring more balanced performance in OCIL. Our BISON method proposes an inclusive training separation strategy using dual classifiers so that knowledge from both old and new classes can effectively be integrated into the model, while introducing implicit approaches for transferring knowledge across the two classifiers. Extensive experimental evaluations over three widely-used OCIL benchmark datasets demonstrate the superiority of BISON, showing more balanced yet better performance compared to state-of-the-art replay-based OCIL methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning (Approximately) Equivariant Networks via Constrained Optimization</title>
<link>https://arxiv.org/abs/2505.13631</link>
<guid>https://arxiv.org/abs/2505.13631</guid>
<content:encoded><![CDATA[
arXiv:2505.13631v2 Announce Type: replace 
Abstract: Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.17508</link>
<guid>https://arxiv.org/abs/2505.17508</guid>
<content:encoded><![CDATA[
arXiv:2505.17508v3 Announce Type: replace 
Abstract: Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). KL regularization is ubiquitous, yet the design surface, choice of KL direction (forward vs. reverse), normalization (normalized vs. unnormalized), and estimator ($k_1/k_2/k_3$), is scattered across the literature and often intertwined with off-policy estimation. We ask a focused question: under the off-policy setting, what weighting is required for each KL variant so that the surrogate we optimize yields the exact gradient of the intended KL-regularized objective? We answer this with a compact, unified derivation we call the Regularized Policy Gradient (RPG) view. RPG (i) unifies normalized and unnormalized KL variants and shows that the widely-used $k_3$ penalty is exactly the unnormalized KL; (ii) specifies conditions under which REINFORCE-style losses with stop-gradient are gradient-equivalent to fully differentiable surrogates; (iii) identifies and corrects an off-policy importance-weighting mismatch in GRPO's KL term; and (iv) introduces RPG-Style Clip, a clipped-importance-sampling step within RPG-REINFORCE that enables stable, off-policy policy-gradient training at scale. On mathematical reasoning benchmarks (AIME24, AIME25), RPG-REINFORCE with RPG-Style Clip improves accuracy by up to $+6$ absolute percentage points over DAPO. We extend our experiments to 8K context length, and RPG-REINFORCE with RPG-Style Clip achieves 52% accuracy on AIME25, surpassing the official Qwen3-4B-Instruct model (47%). Notably, RPG is a stable and scalable RL algorithm for LLM reasoning, realized via (a) a KL-correct objective, (b) clipped importance sampling, and (c) an iterative reference-policy update scheme.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization</title>
<link>https://arxiv.org/abs/2506.01393</link>
<guid>https://arxiv.org/abs/2506.01393</guid>
<content:encoded><![CDATA[
arXiv:2506.01393v3 Announce Type: replace 
Abstract: This paper addresses the Bayesian optimization problem (also referred to as the Bayesian setting of the Gaussian process bandit), where the learner seeks to minimize the regret under a function drawn from a known Gaussian process (GP). Under a Mat\'ern kernel with a certain degree of smoothness, we show that the Gaussian process upper confidence bound (GP-UCB) algorithm achieves $\tilde{O}(\sqrt{T})$ cumulative regret with high probability. Furthermore, our analysis yields $O(\sqrt{T \ln^2 T})$ regret under a squared exponential kernel. These results fill the gap between the existing regret upper bound for GP-UCB and the best-known bound provided by Scarlett (2018). The key idea in our proof is to capture the concentration behavior of the input sequence realized by GP-UCB, enabling a more refined analysis of the GP's information gain.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.01625</link>
<guid>https://arxiv.org/abs/2506.01625</guid>
<content:encoded><![CDATA[
arXiv:2506.01625v2 Announce Type: replace 
Abstract: We address the problem of Gaussian Process (GP) optimization in the presence of unknown and potentially varying adversarial perturbations. Unlike traditional robust optimization approaches that focus on maximizing performance under worst-case scenarios, we consider a robust satisficing objective, where the goal is to consistently achieve a predefined performance threshold $\tau$, even under adversarial conditions. We propose two novel algorithms based on distinct formulations of robust satisficing, and show that they are instances of a general robust satisficing framework. Further, each algorithm offers different guarantees depending on the nature of the adversary. Specifically, we derive two regret bounds: one that is sublinear over time, assuming certain conditions on the adversary and the satisficing threshold $\tau$, and another that scales with the perturbation magnitude but requires no assumptions on the adversary. Through extensive experiments, we demonstrate that our approach outperforms the established robust optimization methods in achieving the satisficing objective, particularly when the ambiguity set of the robust optimization framework is inaccurately specified.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ENMA: Tokenwise Autoregression for Generative Neural PDE Operators</title>
<link>https://arxiv.org/abs/2506.06158</link>
<guid>https://arxiv.org/abs/2506.06158</guid>
<content:encoded><![CDATA[
arXiv:2506.06158v3 Announce Type: replace 
Abstract: Solving time-dependent parametric partial differential equations (PDEs) remains a fundamental challenge for neural solvers, particularly when generalizing across a wide range of physical parameters and dynamics. When data is uncertain or incomplete-as is often the case-a natural approach is to turn to generative models. We introduce ENMA, a generative neural operator designed to model spatio-temporal dynamics arising from physical phenomena. ENMA predicts future dynamics in a compressed latent space using a generative masked autoregressive transformer trained with flow matching loss, enabling tokenwise generation. Irregularly sampled spatial observations are encoded into uniform latent representations via attention mechanisms and further compressed through a spatio-temporal convolutional encoder. This allows ENMA to perform in-context learning at inference time by conditioning on either past states of the target trajectory or auxiliary context trajectories with similar dynamics. The result is a robust and adaptable framework that generalizes to new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reparameterized LLM Training via Orthogonal Equivalence Transformation</title>
<link>https://arxiv.org/abs/2506.08001</link>
<guid>https://arxiv.org/abs/2506.08001</guid>
<content:encoded><![CDATA[
arXiv:2506.08001v4 Announce Type: replace 
Abstract: While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Regularity in Deterministic Sampling Dynamics of Diffusion-based Generative Models</title>
<link>https://arxiv.org/abs/2506.10177</link>
<guid>https://arxiv.org/abs/2506.10177</guid>
<content:encoded><![CDATA[
arXiv:2506.10177v3 Announce Type: replace 
Abstract: Diffusion-based generative models employ stochastic differential equations (SDEs) and their equivalent probability flow ordinary differential equations (ODEs) to establish a smooth transformation between complex high-dimensional data distributions and tractable prior distributions. In this paper, we reveal a striking geometric regularity in the deterministic sampling dynamics of diffusion generative models: each simulated sampling trajectory along the gradient field lies within an extremely low-dimensional subspace, and all trajectories exhibit an almost identical boomerang shape, regardless of the model architecture, applied conditions, or generated content. We characterize several intriguing properties of these trajectories, particularly under closed-form solutions based on kernel-estimated data modeling. We also demonstrate a practical application of the discovered trajectory regularity by proposing a dynamic programming-based scheme to better align the sampling time schedule with the underlying trajectory structure. This simple strategy requires minimal modification to existing deterministic numerical solvers, incurs negligible computational overhead, and achieves superior image generation performance, especially in regions with only 5 - 10 function evaluations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetry in Neural Network Parameter Spaces</title>
<link>https://arxiv.org/abs/2506.13018</link>
<guid>https://arxiv.org/abs/2506.13018</guid>
<content:encoded><![CDATA[
arXiv:2506.13018v3 Announce Type: replace 
Abstract: Modern deep learning models are highly overparameterized, resulting in large sets of parameter configurations that yield the same outputs. A significant portion of this redundancy is explained by symmetries in the parameter space--transformations that leave the network function unchanged. These symmetries shape the loss landscape and constrain learning dynamics, offering a new lens for understanding optimization, generalization, and model complexity that complements existing theory of deep learning. This survey provides an overview of parameter space symmetry. We summarize existing literature, uncover connections between symmetry and learning theory, and identify gaps and opportunities in this emerging field.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders</title>
<link>https://arxiv.org/abs/2506.15881</link>
<guid>https://arxiv.org/abs/2506.15881</guid>
<content:encoded><![CDATA[
arXiv:2506.15881v3 Announce Type: replace 
Abstract: SHallow REcurrent Decoders (SHRED) are effective for system identification and forecasting from sparse sensor measurements. Such models are light-weight and computationally efficient, allowing them to be trained on consumer laptops. SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding respectively. Despite the relatively simple structure of SHRED, they are able to predict chaotic dynamical systems on different physical, spatial, and temporal scales directly from a sparse set of sensor measurements. In this work, we modify SHRED by leveraging transformers (T-SHRED) embedded with symbolic regression for the temporal encoding, circumventing auto-regressive long-term forecasting for physical data. This is achieved through a new sparse identification of nonlinear dynamics (SINDy) attention mechanism into T-SHRED to impose sparsity regularization on the latent space, which also allows for immediate symbolic interpretation. Symbolic regression improves model interpretability by learning and regularizing the dynamics of the latent space during training. We analyze the performance of T-SHRED on three different dynamical systems ranging from low-data to high-data regimes.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches</title>
<link>https://arxiv.org/abs/2506.16528</link>
<guid>https://arxiv.org/abs/2506.16528</guid>
<content:encoded><![CDATA[
arXiv:2506.16528v2 Announce Type: replace 
Abstract: Traditional ASR metrics like WER and CER fail to capture intelligibility, especially for dysarthric and dysphonic speech, where semantic alignment matters more than exact word matches. ASR systems struggle with these speech types, often producing errors like phoneme repetitions and imprecise consonants, yet the meaning remains clear to human listeners. We identify two key challenges: (1) Existing metrics do not adequately reflect intelligibility, and (2) while LLMs can refine ASR output, their effectiveness in correcting ASR transcripts of dysarthric speech remains underexplored. To address this, we propose a novel metric integrating Natural Language Inference (NLI) scores, semantic similarity, and phonetic similarity. Our ASR evaluation metric achieves a 0.890 correlation with human judgments on Speech Accessibility Project data, surpassing traditional methods and emphasizing the need to prioritize intelligibility over error-based measures.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort</title>
<link>https://arxiv.org/abs/2506.21429</link>
<guid>https://arxiv.org/abs/2506.21429</guid>
<content:encoded><![CDATA[
arXiv:2506.21429v2 Announce Type: replace 
Abstract: This study investigates the efficacy of using multimodal machine learning techniques to detect deception in dyadic interactions, focusing on the integration of data from both the deceiver and the deceived. We compare early and late fusion approaches, utilizing audio and video data - specifically, Action Units and gaze information - across all possible combinations of modalities and participants. Our dataset, newly collected from Swedish native speakers engaged in truth or lie scenarios on emotionally relevant topics, serves as the basis for our analysis. The results demonstrate that incorporating both speech and facial information yields superior performance compared to single-modality approaches. Moreover, including data from both participants significantly enhances deception detection accuracy, with the best performance (71%) achieved using a late fusion strategy applied to both modalities and participants. These findings align with psychological theories suggesting differential control of facial and vocal expressions during initial interactions. As the first study of its kind on a Scandinavian cohort, this research lays the groundwork for future investigations into dyadic interactions, particularly within psychotherapy settings.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proof of a perfect platonic representation hypothesis</title>
<link>https://arxiv.org/abs/2507.01098</link>
<guid>https://arxiv.org/abs/2507.01098</guid>
<content:encoded><![CDATA[
arXiv:2507.01098v2 Announce Type: replace 
Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin et al. (2025) of the ``perfect" Platonic Representation Hypothesis (PRH) for the embedded deep linear network model (EDLN). We show that if trained with the stochastic gradient descent (SGD), two EDLNs with different widths and depths and trained on different data will become Perfectly Platonic, meaning that every possible pair of layers will learn the same representation up to a rotation. Because most of the global minima of the loss function are not Platonic, that SGD only finds the perfectly Platonic solution is rather extraordinary. The proof also suggests at least six ways the PRH can be broken. We also show that in the EDLN model, the emergence of the Platonic representations is due to the same reason as the emergence of progressive sharpening. This implies that these two seemingly unrelated phenomena in deep learning can, surprisingly, have a common cause. Overall, the theory and proof highlight the importance of understanding emergent "entropic forces" due to the irreversibility of SGD training and their role in representation learning. The goal of this note is to be instructive while avoiding jargon and lengthy technical details.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>