<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer</title>
<link>https://arxiv.org/abs/2508.09144</link>
<guid>https://arxiv.org/abs/2508.09144</guid>
<content:encoded><![CDATA[
<div> Transformer model, aircraft ETA prediction, real-time arrival management, feature tokenization, ADS-B data<br>
<br>
Summary: 
This study introduces a feature tokenization-based Transformer model for efficiently predicting aircraft Estimated Time of Arrival (ETA) in real-time, crucial for runway sequencing in aviation. The model utilizes raw input data such as aircraft latitude, longitude, and weather context to update ETA predictions every second, handling requests at a high frequency of 1HZ. Experimental evaluation at Singapore Changi Airport shows the proposed method outperforming XGBoost with improved accuracy by 7% and requiring only 39% of its computing time. With 40 aircraft in the airspace, ETA inference time is a promising 51.7 microseconds, making the model suitable for real-time arrival management systems.<br><br>Summary: <div>
arXiv:2508.09144v1 Announce Type: new 
Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial for arrival management in aviation, particularly for runway sequencing. Given the rapidly changing airspace context, the ETA prediction efficiency is as important as its accuracy in a real-time arrival aircraft management system. In this study, we utilize a feature tokenization-based Transformer model to efficiently predict aircraft ETA. Feature tokenization projects raw inputs to latent spaces, while the multi-head self-attention mechanism in the Transformer captures important aspects of the projections, alleviating the need for complex feature engineering. Moreover, the Transformer's parallel computation capability allows it to handle ETA requests at a high frequency, i.e., 1HZ, which is essential for a real-time arrival management system. The model inputs include raw data, such as aircraft latitude, longitude, ground speed, theta degree for the airport, day and hour from track data, the weather context, and aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA prediction is updated every second. We apply the proposed aircraft ETA prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October 1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers all aircraft within a range of 10NM to 300NM from WSSS. The results show that our proposed method method outperforms the commonly used boosting tree based model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\% of its computing time. Experimental results also indicate that, with 40 aircraft in the airspace at a given timestamp, the ETA inference time is only 51.7 microseconds, making it promising for real-time arrival management systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.09145</link>
<guid>https://arxiv.org/abs/2508.09145</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Sentiment Analysis, MoLAN framework, noise suppression, modality-aware blocking, MoLAN+

Summary: 
The paper introduces the MoLAN framework for multimodal sentiment analysis, which addresses the challenge of irrelevant or misleading visual and auditory information commonly faced by existing approaches. MoLAN performs modality-aware blocking by dividing features into multiple blocks and dynamically assigning denoising strengths based on noise level and relevance. This fine-grained noise suppression process preserves essential multimodal information without losing critical data. MoLAN is a flexible and unified framework that can be easily integrated into various multimodal models. The paper also presents MoLAN+, a novel multimodal sentiment analysis approach built upon the MoLAN framework. Experimental results showcase the effectiveness of MoLAN across different models and datasets, with MoLAN+ achieving state-of-the-art performance. The code for MoLAN is publicly available on GitHub for further exploration and use. <br><br>Summary: <div>
arXiv:2508.09145v1 Announce Type: new 
Abstract: Multimodal Sentiment Analysis aims to integrate information from various modalities, such as audio, visual, and text, to make complementary predictions. However, it often struggles with irrelevant or misleading visual and auditory information. Most existing approaches typically treat the entire modality information (e.g., a whole image, audio segment, or text paragraph) as an independent unit for feature enhancement or denoising. They often suppress the redundant and noise information at the risk of losing critical information. To address this challenge, we propose MoLAN, a unified ModaLity-aware noise dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking by dividing the features of each modality into multiple blocks. Each block is then dynamically assigned a distinct denoising strength based on its noise level and semantic relevance, enabling fine-grained noise suppression while preserving essential multimodal information. Notably, MoLAN is a unified and flexible framework that can be seamlessly integrated into a wide range of multimodal models. Building upon this framework, we further introduce MoLAN+, a new multimodal sentiment analysis approach. Experiments across five models and four datasets demonstrate the broad effectiveness of the MoLAN framework. Extensive evaluations show that MoLAN+ achieves the state-of-the-art performance. The code is publicly available at https://github.com/betterfly123/MoLAN-Framework.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA</title>
<link>https://arxiv.org/abs/2508.09146</link>
<guid>https://arxiv.org/abs/2508.09146</guid>
<content:encoded><![CDATA[
<div> Keywords: binary exponential backoff, WiFi, channel environments, transformer-based in-context learning, throughput performance <br>
Summary: 
- The paper introduces a novel approach called LLM transformer-based In-Context Learning (ICL) theory for optimizing channel access in dynamic environments.
- A transformer-based optimizer is designed to learn patterns and predict contention window thresholds (CWT) for efficient channel access.
- An algorithm is developed to train the transformer for effective ICL, ensuring near-optimal CWT prediction within limited training steps.
- The approach allows for erroneous data input in the prompt, while still maintaining minimal prediction and throughput deviations from optimal values.
- Experimental results on NS-3 show fast convergence and near-optimal throughput compared to existing model-based and Deep Reinforcement Learning (DRL) approaches, especially under unknown node densities. <br><br>Summary: <div>
arXiv:2508.09146v1 Announce Type: new 
Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still incurs poor throughput performance under dynamic channel environments. Recent model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply optimize backoff strategies under a known and fixed node density, still leading to a large throughput loss due to inaccurate node density estimation. This paper is the first to propose LLM transformer-based in-context learning (ICL) theory for optimizing channel access. We design a transformer-based ICL optimizer to pre-collect collision-threshold data examples and a query collision case. They are constructed as a prompt as the input for the transformer to learn the pattern, which then generates a predicted contention window threshold (CWT). To train the transformer for effective ICL, we develop an efficient algorithm and guarantee a near-optimal CWT prediction within limited training steps. As it may be hard to gather perfect data examples for ICL in practice, we further extend to allow erroneous data input in the prompt. We prove that our optimizer maintains minimal prediction and throughput deviations from the optimal values. Experimental results on NS-3 further demonstrate our approach's fast convergence and near-optimal throughput over existing model-based and DRL-based approaches under unknown node densities.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motif 2.6B Technical Report</title>
<link>https://arxiv.org/abs/2508.09148</link>
<guid>https://arxiv.org/abs/2508.09148</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Motif-2.6B, Differential Attention, PolyNorm activation functions, Efficiency

Summary:<br><br>Recent advancements in Large Language Models (LLMs) have led to the development of Motif-2.6B, a 2.6-billion-parameter foundation model aimed at democratizing advanced LLM capabilities. This model incorporates innovative architectural enhancements such as Differential Attention and PolyNorm activation functions, which enhance long-context comprehension and reduce hallucination, improving in-context learning capabilities. Through extensive experimentation and rigorous testing, the optimal architecture for Motif-2.6B was determined. Evaluations show that Motif-2.6B consistently outperforms similarly sized state-of-the-art models across diverse benchmarks, demonstrating its effectiveness, scalability, and real-world applicability. This research significantly advances the landscape of efficient, scalable, and powerful foundational LLMs, providing valuable insights and a robust foundation for future research and deployment. <div>
arXiv:2508.09148v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized artificial intelligence, yet developing an effective foundational LLM that balances high performance with computational efficiency remains challenging, especially for emerging research groups. To address this gap, we introduce Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize advanced LLM capabilities. Motif-2.6B incorporates several innovative architectural enhancements, including Differential Attention and PolyNorm activation functions, which improve long-context comprehension, reduce hallucination, and enhance in-context learning capabilities. We rigorously tested multiple novel architectural components through extensive experimentation to determine the optimal architecture for Motif-2.6B. Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or exceeds the performance of similarly sized state-of-the-art models across diverse benchmarks, showcasing its effectiveness, scalability, and real-world applicability. Through detailed experiments and tailored techniques, Motif-2.6B significantly advances the landscape of efficient, scalable, and powerful foundational LLMs, offering valuable insights and a robust foundation for future research and deployment.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis</title>
<link>https://arxiv.org/abs/2508.09153</link>
<guid>https://arxiv.org/abs/2508.09153</guid>
<content:encoded><![CDATA[
<div> sequence mixers, time series analysis, dense layers, MatrixMixer framework, empirical study  
Summary:  
JustDense, an empirical study, questions the necessity of complex sequence mixers in time series analysis (TSA) and proposes replacing them with dense layers. The study systematically replaces sequence mixers in established TSA models with dense layers, showing comparable or superior performance in 29 benchmarks across five TSA tasks. The MatrixMixer framework treats sequence mixers as mixing matrices, enabling a clear theoretical understanding of their role. The results challenge the assumption that deeper and more complex architectures are inherently better in TSA, suggesting that simpler architectures can achieve similar performance. By isolating the mixing operation, JustDense offers insights into architectural and optimization factors influencing TSA model performance. <div>
arXiv:2508.09153v1 Announce Type: new 
Abstract: Sequence and channel mixers, the core mechanism in sequence models, have become the de facto standard in time series analysis (TSA). However, recent studies have questioned the necessity of complex sequence mixers, such as attention mechanisms, demonstrating that simpler architectures can achieve comparable or even superior performance. This suggests that the benefits attributed to complex sequencemixers might instead emerge from other architectural or optimization factors. Based on this observation, we pose a central question: Are common sequence mixers necessary for time-series analysis? Therefore, we propose JustDense, an empirical study that systematically replaces sequence mixers in various well-established TSA models with dense layers. Grounded in the MatrixMixer framework, JustDense treats any sequence mixer as a mixing matrix and replaces it with a dense layer. This substitution isolates the mixing operation, enabling a clear theoretical foundation for understanding its role. Therefore, we conducted extensive experiments on 29 benchmarks covering five representative TSA tasks using seven state-of-the-art TSA models to address our research question. The results show that replacing sequence mixers with dense layers yields comparable or even superior performance. In the cases where dedicated sequence mixers still offer benefits, JustDense challenges the assumption that "deeper and more complex architectures are inherently better" in TSA.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders</title>
<link>https://arxiv.org/abs/2508.09154</link>
<guid>https://arxiv.org/abs/2508.09154</guid>
<content:encoded><![CDATA[
<div> Deep learning, peer effects, social networks, unobserved confounding, feedback loops

Summary:

DIG2RSI is a novel deep learning framework designed to estimate peer causal effects within complex real-world networks, such as social networks. It addresses the challenges of simultaneous feedback between peers and unobserved confounders. By leveraging I-G transformation and 2SRI techniques, DIG2RSI can capture complex, nonlinear, and high-dimensional relationships. The framework disentangles mutual peer influences and eliminates bias from feedback loops using the I-G transformation. It also deals with unobserved confounding by constructing valid instrumental variables from network data. Through a two-stage process, DIG2RSI uses neural networks to predict peer exposure and extract residuals as proxies for unobserved confounders. The framework's consistency under standard conditions ensures accurate estimation of peer effects. Empirical results on benchmarks and real-world data demonstrate that DIG2RSI outperforms existing approaches, showcasing its effectiveness in peer effect estimation. 

<br><br>Summary: <div>
arXiv:2508.09154v1 Announce Type: new 
Abstract: Estimating peer causal effects within complex real-world networks such as social networks is challenging, primarily due to simultaneous feedback between peers and unobserved confounders. Existing methods either address unobserved confounders while ignoring the simultaneous feedback, or account for feedback but under restrictive linear assumptions, thus failing to obtain accurate peer effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning framework which leverages I-G transformation (matrix operation) and 2SRI (an instrumental variable or IV technique) to address both simultaneous feedback and unobserved confounding, while accommodating complex, nonlinear and high-dimensional relationships. DIG2RSI first applies the I-G transformation to disentangle mutual peer influences and eliminate the bias due to the simultaneous feedback. To deal with unobserved confounding, we first construct valid IVs from network data. In stage 1 of 2RSI, we train a neural network on these IVs to predict peer exposure, and extract residuals as proxies for the unobserved confounders. In the stage 2, we fit a separate neural network augmented by an adversarial discriminator that incorporates these residuals as a control function and enforces the learned representation to contain no residual confounding signal. The expressive power of deep learning models in capturing complex non-linear relationships and adversarial debiasing enhances the effectiveness of DIG2RSI in eliminating bias from both feedback loops and hidden confounders. We prove consistency of our estimator under standard regularity conditions, ensuring asymptotic recovery of the true peer effect. Empirical results on two semi-synthetic benchmarks and a real-world dataset demonstrate that DIG2RSI outperforms existing approaches.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models</title>
<link>https://arxiv.org/abs/2508.09155</link>
<guid>https://arxiv.org/abs/2508.09155</guid>
<content:encoded><![CDATA[
<div> Keywords: Self-evaluation, Large Multimodal Models, Reinforcement Learning, Adaptive Reward Model, Dynamic KL Regularization <br>
Summary: <br>
The paper introduces AdaPO, an online reinforcement learning framework designed to enhance self-evaluation in Large Multimodal Models during multi-turn conversations. Traditional RL methods suffer from reward hacking when optimizing multiple objectives, leading to model collapse. AdaPO addresses this issue by introducing an Adaptive Reward Model (ARM) and a Reward Aware Dynamic KL Regularization mechanism. ARM evaluates the training state based on model-generated trajectories' performance distribution, while the Dynamic KL Regularization adjusts penalties dynamically based on reward gaps. This adaptive approach allows the model to focus on different sub-tasks' training progress without manual intervention. Experimental results across 8 benchmarks demonstrate significant improvements in both direct reasoning and self-evaluation abilities. The code will be released to benefit the research community. <br> <div>
arXiv:2508.09155v1 Announce Type: new 
Abstract: Self-evaluation, a model's ability to assess the correctness of its own output, is crucial for Large Multimodal Models (LMMs) to achieve self-improvement in multi-turn conversations, yet largely absent in foundation models. Recent work has employed reinforcement learning (RL) to enhance self-evaluation; however, its fixed reward mechanism suffers from reward hacking when optimizing multiple training objectives, leading to model collapse. In this paper we propose AdaPO, an online reinforcement learning framework capable of adaptively adjusting training objective in real time according to the current training state for each task. Specifically, to mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's training state from the distribution of model generated multi-turn trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty with dynamic coefficients which is modulated by the reward gap between different multi-turn situations. Notably, our method automatically and smoothly adjusts its learning focus based on sub-tasks' training progress without manual intervention. Extensive experiments over 8 benchmarks and various models show that our method significantly enhances both direct reasoning and self-evaluation capability. We will release our code to contribute to the community.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems</title>
<link>https://arxiv.org/abs/2508.09156</link>
<guid>https://arxiv.org/abs/2508.09156</guid>
<content:encoded><![CDATA[
<div> Framework, Flow-matching generative models, Physical constraints, Inverse problems, Partial differential equations (PDEs)

Summary:
This article introduces a framework for fine-tuning flow-matching generative models to enforce physical constraints and solve inverse problems in scientific systems. The approach involves a post-training procedure that minimizes weak-form residuals of governing PDEs, ensuring physical consistency and adherence to boundary conditions without altering the learned distribution. By incorporating a learnable latent parameter predictor, the model can infer unknown physical inputs such as source terms, material parameters, or boundary data through joint optimization. The model produces physically valid field solutions and accurate estimates of hidden parameters, effectively tackling ill-posed inverse problems while remaining data-driven and physics-aware. Validated on canonical PDE benchmarks, the method showcases improved satisfaction of PDE constraints and precise recovery of latent coefficients. This methodology combines generative modelling and scientific inference, offering opportunities for simulation-driven discovery and efficient modelling of physical systems.<br><br>Summary: <div>
arXiv:2508.09156v1 Announce Type: new 
Abstract: We present a framework for fine-tuning flow-matching generative models to enforce physical constraints and solve inverse problems in scientific systems. Starting from a model trained on low-fidelity or observational data, we apply a differentiable post-training procedure that minimizes weak-form residuals of governing partial differential equations (PDEs), promoting physical consistency and adherence to boundary conditions without distorting the underlying learned distribution. To infer unknown physical inputs, such as source terms, material parameters, or boundary data, we augment the generative process with a learnable latent parameter predictor and propose a joint optimization strategy. The resulting model produces physically valid field solutions alongside plausible estimates of hidden parameters, effectively addressing ill-posed inverse problems in a data-driven yet physicsaware manner. We validate our method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE constraints and accurate recovery of latent coefficients. Our approach bridges generative modelling and scientific inference, opening new avenues for simulation-augmented discovery and data-efficient modelling of physical systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.09158</link>
<guid>https://arxiv.org/abs/2508.09158</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous driving, trajectory generation, multi-objective reinforcement learning, adversarial optimization, iterative decision-making

Summary:
EvaDrive introduces a novel approach to autonomous driving by combining trajectory generation and evaluation through multi-objective reinforcement learning. The framework creates a closed-loop co-evolution process where trajectory proposals are continuously refined and evaluated in an adversarial game setting. This allows for diverse driving styles without the need for external preference data and avoids scalarization bias common in traditional reinforcement learning methods. EvaDrive achieves state-of-the-art performance on benchmark tests, surpassing existing models in terms of Precision-Recall-Dynamic-Move-Scale (PDMS) and Driving Score metrics. By enabling iterative multi-round refinement guided by a Pareto frontier selection mechanism, EvaDrive can escape local optima and maintain trajectory diversity. This innovative framework offers a scalable and effective solution for human-like iterative decision-making in autonomous driving systems. 

<br><br>Summary: <div>
arXiv:2508.09158v1 Announce Type: new 
Abstract: Autonomous driving faces significant challenges in achieving human-like iterative decision-making, which continuously generates, evaluates, and refines trajectory proposals. Current generation-evaluation frameworks isolate trajectory generation from quality assessment, preventing iterative refinement essential for planning, while reinforcement learning methods collapse multi-dimensional preferences into scalar rewards, obscuring critical trade-offs and yielding scalarization bias.To overcome these issues, we present EvaDrive, a novel multi-objective reinforcement learning framework that establishes genuine closed-loop co-evolution between trajectory generation and evaluation via adversarial optimization. EvaDrive frames trajectory planning as a multi-round adversarial game. In this game, a hierarchical generator continuously proposes candidate paths by combining autoregressive intent modeling for temporal causality with diffusion-based refinement for spatial flexibility. These proposals are then rigorously assessed by a trainable multi-objective critic that explicitly preserves diverse preference structures without collapsing them into a single scalarization bias.This adversarial interplay, guided by a Pareto frontier selection mechanism, enables iterative multi-round refinement, effectively escaping local optima while preserving trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic weighting without external preference data, introducing a closed-loop adversarial framework for human-like iterative decision-making, offering a novel scalarization-free trajectory optimization approach.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Presenting DiaData for Research on Type 1 Diabetes</title>
<link>https://arxiv.org/abs/2508.09160</link>
<guid>https://arxiv.org/abs/2508.09160</guid>
<content:encoded><![CDATA[
<div> Keywords: Type 1 diabetes, hypoglycemia, machine learning, dataset integration, data analysis<br>
Summary:<br>
- Type 1 diabetes leads to insulin deficiency, requiring external insulin injections and increasing the risk of hypoglycemia.
- Data analysis plays a crucial role in improving diabetes care by identifying patterns and trends to predict and prevent adverse events.
- Machine learning models can predict glucose levels and provide early warnings for hypoglycemia.
- The integration of 15 datasets results in a database of 2510 subjects with glucose measurements every 5 minutes, including 4% hypoglycemic values.
- Sub-databases for demographics and heart rate data extraction provide valuable additional information.
- Data quality assessment reveals challenges such as imbalance and missing values.
- A correlation study between glucose levels and heart rate data shows a relationship 15 to 55 minutes before hypoglycemia. <div>
arXiv:2508.09160v1 Announce Type: new 
Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction of insulin-producing cells, resulting in insulin deficiency, as to why the affected individuals depend on external insulin injections. However, insulin can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side effects of dizziness, coma, or death. Data analysis can significantly enhance diabetes care by identifying personal patterns and trends leading to adverse events. Especially, machine learning (ML) models can predict glucose levels and provide early alarms. However, diabetes and hypoglycemia research is limited by the unavailability of large datasets. Thus, this work systematically integrates 15 datasets to provide a large database of 2510 subjects with glucose measurements recorded every 5 minutes. In total, 149 million measurements are included, of which 4% represent values in the hypoglycemic range. Moreover, two sub-databases are extracted. Sub-database I includes demographics, and sub-database II includes heart rate data. The integrated dataset provides an equal distribution of sex and different age levels. As a further contribution, data quality is assessed, revealing that data imbalance and missing values present a significant challenge. Moreover, a correlation study on glucose levels and heart rate data is conducted, showing a relation between 15 and 55 minutes before hypoglycemia.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Guided Memory Network for Building Energy Modeling</title>
<link>https://arxiv.org/abs/2508.09161</link>
<guid>https://arxiv.org/abs/2508.09161</guid>
<content:encoded><![CDATA[
<div> deep learning, energy consumption forecasting, building sector, physics-based models, Neural Network

Summary:
The paper introduces a Physics-Guided Memory Network (PgMN) that combines deep learning and physics-based models for accurate energy consumption forecasting in the building sector. PgMN includes components like Parallel Projection Layers, Memory Unit, and Memory Experience Module to address limitations of both types of models. Theoretical evaluation confirms the validity of PgMN components for their respective tasks. Experimental validation demonstrates the PgMN's accuracy and applicability in scenarios with limited historical data, newly constructed buildings, missing data, sparse historical data, and dynamic infrastructure changes. The PgMN offers a promising solution for short-term energy forecasting at an hourly resolution, crucial for operational decision-making in smart grid and smart building systems. <div>
arXiv:2508.09161v1 Announce Type: new 
Abstract: Accurate energy consumption forecasting is essential for efficient resource management and sustainability in the building sector. Deep learning models are highly successful but struggle with limited historical data and become unusable when historical data are unavailable, such as in newly constructed buildings. On the other hand, physics-based models, such as EnergyPlus, simulate energy consumption without relying on historical data but require extensive building parameter specifications and considerable time to model a building. This paper introduces a Physics-Guided Memory Network (PgMN), a neural network that integrates predictions from deep learning and physics-based models to address their limitations. PgMN comprises a Parallel Projection Layers to process incomplete inputs, a Memory Unit to account for persistent biases, and a Memory Experience Module to optimally extend forecasts beyond their input range and produce output. Theoretical evaluation shows that components of PgMN are mathematically valid for performing their respective tasks. The PgMN was evaluated on short-term energy forecasting at an hourly resolution, critical for operational decision-making in smart grid and smart building systems. Experimental validation shows accuracy and applicability of PgMN in diverse scenarios such as newly constructed buildings, missing data, sparse historical data, and dynamic infrastructure changes. This paper provides a promising solution for energy consumption forecasting in dynamic building environments, enhancing model applicability in scenarios where historical data are limited or unavailable or when physics-based models are inadequate.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals</title>
<link>https://arxiv.org/abs/2508.09162</link>
<guid>https://arxiv.org/abs/2508.09162</guid>
<content:encoded><![CDATA[
<div> instrumentation control systems, cyber-physical systems, replay attacks, explainable AI, nuclear reactors
Summary:<br><br>Next generation nuclear reactors will rely on fully digital instrumentation and control systems, generating multivariate time series data. Ensuring data integrity against deception attacks is crucial. Current approaches focus on watermarking or supervised anomaly detection but do not fully characterize the root cause of anomalies. This study proposes an unsupervised explainable AI framework using an autoencoder and customized windowSHAP algorithm to characterize real-time replay attacks in nuclear cyber-physical systems. The framework accurately detects and identifies the source, number, and duration of signals being replayed, achieving 95% accuracy on real-world datasets from Purdue's nuclear reactor. This approach addresses the need for a more comprehensive understanding of replay attacks and improves the safety and reliability of nuclear reactors. <div>
arXiv:2508.09162v1 Announce Type: new 
Abstract: Next generation advanced nuclear reactors are expected to be smaller both in size and power output, relying extensively on fully digital instrumentation and control systems. These reactors will generate a large flow of information in the form of multivariate time series data, conveying simultaneously various non linear cyber physical, process, control, sensor, and operational states. Ensuring data integrity against deception attacks is becoming increasingly important for networked communication and a requirement for safe and reliable operation. Current efforts to address replay attacks, almost universally focus on watermarking or supervised anomaly detection approaches without further identifying and characterizing the root cause of the anomaly. In addition, these approaches rely mostly on synthetic data with uncorrelated Gaussian process and measurement noise and full state feedback or are limited to univariate signals, signal stationarity, linear quadratic regulators, or other linear-time invariant state-space which may fail to capture any unmodeled system dynamics. In the realm of regulated nuclear cyber-physical systems, additional work is needed on characterization of replay attacks and explainability of predictions using real data. Here, we propose an unsupervised explainable AI framework based on a combination of autoencoder and customized windowSHAP algorithm to fully characterize real-time replay attacks, i.e., detection, source identification, timing and type, of increasing complexity during a dynamic time evolving reactor process. The proposed XAI framework was benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1 with up to six signals concurrently being replayed. In all cases, the XAI framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)</title>
<link>https://arxiv.org/abs/2508.09163</link>
<guid>https://arxiv.org/abs/2508.09163</guid>
<content:encoded><![CDATA[
<div> Internet of Things, Stochastic computing, Neural networks, Mixed-precision, Adjustable Sequence Length <br>
<br>
Summary: 
Stochastic computing (SC) is proposed as a low-power option for neural networks (NNs) in IoT environments. A new approach, Adjustable Sequence Length (ASL), is introduced to improve layer-wise mixed-precision implementation in SC NNs. ASL leverages operator-norm-based theoretical models to analyze truncation noise propagation through layers and optimize sequence length configurations. A sensitivity analysis validates the effectiveness of ASL in reducing energy and latency overheads by up to 60% with minimal accuracy loss. Two truncation strategies, coarse-grained and fine-grained, are proposed to cater to different application scenarios. Evaluation on a 32nm pipelined SC MLP confirms the benefits of ASL for IoT applications and highlights the advantages of mixed-precision truncation in SC designs. <div>
arXiv:2508.09163v1 Announce Type: new 
Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative for deploying neural networks (NNs) in resource-limited scenarios, such as the Internet of Things (IoT). By encoding values as serial bitstreams, SC significantly reduces energy dissipation compared to conventional floating-point (FP) designs; however, further improvement of layer-wise mixed-precision implementation for SC remains unexplored. This article introduces Adjustable Sequence Length (ASL), a novel scheme that applies mixed-precision concepts specifically to SC NNs. By introducing an operator-norm-based theoretical model, this article shows that truncation noise can cumulatively propagate through the layers by the estimated amplification factors. An extended sensitivity analysis is presented, using random forest (RF) regression to evaluate multilayer truncation effects and validate the alignment of theoretical predictions with practical network behaviors. To accommodate different application scenarios, this article proposes two truncation strategies (coarse-grained and fine-grained), which apply diverse sequence length configurations at each layer. Evaluations on a pipelined SC MLP synthesized at 32nm demonstrate that ASL can reduce energy and latency overheads by up to over 60% with negligible accuracy loss. It confirms the feasibility of the ASL scheme for IoT applications and highlights the distinct advantages of mixed-precision truncation in SC designs.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Feasible and Diverse Synthetic Populations Using Diffusion Models</title>
<link>https://arxiv.org/abs/2508.09164</link>
<guid>https://arxiv.org/abs/2508.09164</guid>
<content:encoded><![CDATA[
<div> Population synthesis, deep generative models, diffusion model, attribute combinations, feasibility<br>
<br>
Summary:<br>
Population synthesis is a crucial task in agent-based modeling for intelligent transportation systems. With large numbers of attributes, sparsity in survey data can make accurate population modeling challenging. A novel diffusion model-based approach is proposed to estimate the joint distribution of a population, allowing for the recovery of missing sampling zeros while minimizing generated structural zeros. Compared to existing methods like VAE and GAN, this approach strikes a better balance between feasibility and diversity in the synthesized population. The performance is evaluated using metrics such as marginal distribution similarity, feasibility, and diversity, showing superior results in accurately representing the population. <div>
arXiv:2508.09164v1 Announce Type: new 
Abstract: Population synthesis is a critical task that involves generating synthetic yet realistic representations of populations. It is a fundamental problem in agent-based modeling (ABM), which has become the standard to analyze intelligent transportation systems. The synthetic population serves as the primary input for ABM transportation simulation, with traveling agents represented by population members. However, when the number of attributes describing agents becomes large, survey data often cannot densely support the joint distribution of the attributes in the population due to the curse of dimensionality. This sparsity makes it difficult to accurately model and produce the population. Interestingly, deep generative models trained from available sample data can potentially synthesize possible attribute combinations that present in the actual population but do not exist in the sample data(called sampling zeros). Nevertheless, this comes at the cost of falsely generating the infeasible attribute combinations that do not exist in the population (called structural zeros). In this study, a novel diffusion model-based population synthesis method is proposed to estimate the underlying joint distribution of a population. This approach enables the recovery of numerous missing sampling zeros while keeping the generated structural zeros minimal. Our method is compared with other recently proposed approaches such as Variational Autoencoders (VAE) and Generative Adversarial Network (GAN) approaches, which have shown success in high dimensional tabular population synthesis. We assess the performance of the synthesized outputs using a range of metrics, including marginal distribution similarity, feasibility, and diversity. The results demonstrate that our proposed method outperforms previous approaches in achieving a better balance between the feasibility and diversity of the synthesized population.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images</title>
<link>https://arxiv.org/abs/2508.09165</link>
<guid>https://arxiv.org/abs/2508.09165</guid>
<content:encoded><![CDATA[
<div> PatchECG, adaptive variable block count missing representation learning, ECG layouts, arrhythmia diagnosis, AUROC <br>
<br>
Summary: 
The study introduces PatchECG, a framework for addressing challenges in ECG diagnosis caused by differences in ECG layouts. The framework uses an adaptive variable block count missing representation learning approach based on a masking training strategy to focus on key patches with collaborative dependencies between leads. Experimental results on the PTB-XL dataset and generated ECG images show strong robustness under different layouts, achieving an average AUROC of 0.835. External validation on real ECG images from Chaoyang Hospital exhibited AUROC of 0.778 for atrial fibrillation diagnosis and 0.893 on 12 x 1 layout ECGs. The proposed method outperforms classic interpolation and baseline methods and shows improvement over the current optimal large-scale pre-training model ECGFounder. <div>
arXiv:2508.09165v1 Announce Type: new 
Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular diseases such as arrhythmia. Due to the differences in ECG layouts used by different hospitals, the digitized signals exhibit asynchronous lead time and partial blackout loss, which poses a serious challenge to existing models. To address this challenge, the study introduced PatchECG, a framework for adaptive variable block count missing representation learning based on a masking training strategy, which automatically focuses on key patches with collaborative dependencies between leads, thereby achieving key recognition of arrhythmia in ECGs with different layouts. Experiments were conducted on the PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit tool, using the 23 Subclasses as labels. The proposed method demonstrated strong robustness under different layouts, with average Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged with layout changes). In external validation based on 400 real ECG images data from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached 0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to various classic interpolation and baseline methods, and compared to the current optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and 0.19.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVGen: Interpretable Vector Graphics Generation with Large Language Models</title>
<link>https://arxiv.org/abs/2508.09168</link>
<guid>https://arxiv.org/abs/2508.09168</guid>
<content:encoded><![CDATA[
<div> Keywords: Scalable Vector Graphics, SVG, dataset, natural language, SVGen <br>
Summary:

Scalable Vector Graphics (SVG) are essential in front-end development and UI/UX design due to their scalability and efficiency, but creating precise vector graphics can be challenging. To address this, the SVG-1M dataset is introduced, consisting of high-quality SVGs paired with natural language descriptions. The dataset includes well-aligned Text to SVG training pairs, with a subset featuring Chain of Thought annotations for enhanced semantic guidance. An end-to-end model called SVGen is proposed to generate SVG code from natural language inputs, ensuring semantic accuracy and structural completeness through curriculum learning and reinforcement learning optimization. Experiments demonstrate that SVGen outperforms general large models and traditional rendering methods in both effectiveness and efficiency. The code, model, and dataset are available on GitHub.<br><br>Summary: Scalable Vector Graphics are essential in UI/UX design, but creating precise vector graphics is challenging. The SVG-1M dataset, paired with natural language descriptions, addresses this issue by providing well-aligned training pairs. The SVGen model, utilizing curriculum learning and reinforcement learning optimization, outperforms traditional rendering methods and general large models. The code, model, and dataset are accessible on GitHub. <div>
arXiv:2508.09168v1 Announce Type: new 
Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and UI/UX design due to its scalability, editability, and rendering efficiency. However, turning creative ideas into precise vector graphics remains a time-consuming challenge. To address this, we introduce SVG-1M, a large-scale dataset of high-quality SVGs paired with natural language descriptions. Through advanced data augmentation and annotation, we create well-aligned Text to SVG training pairs, including a subset with Chain of Thought annotations for enhanced semantic guidance. Based on this dataset, we propose SVGen, an end-to-end model that generates SVG code from natural language inputs. Our approach ensures semantic accuracy and structural completeness, supported by curriculum learning and reinforcement learning optimization. Experiments show that SVGen outperforms general large models and traditional rendering methods in both effectiveness and efficiency. Code, model, and dataset are available on GitHub.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal RAG Enhanced Visual Description</title>
<link>https://arxiv.org/abs/2508.09170</link>
<guid>https://arxiv.org/abs/2508.09170</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Modality Gap, Multimodal Models, Textual Descriptions, Linear Mapping
Summary:
The article introduces a lightweight training-free approach, Retrieval-Augmented Generation (RAG), to address the modality gap in large multimodal models (LMMs). By using a linear mapping technique, RAG efficiently extends across modalities by retrieving closest textual descriptions from the training set to generate new descriptions. An iterative method is introduced to optimize the mapping by generating synthetic descriptions through a language model. Experimental results on benchmark datasets show significant improvements in image description generation. The proposed approach mitigates the need for costly fine-tuning by leveraging existing textual data for improved multimodal input processing.<br><br>Summary: <div>
arXiv:2508.09170v1 Announce Type: new 
Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of queries to produce relevant output images. Despite efforts to address challenges such as scaling model size and data volume, the cost associated with pre-training and fine-tuning remains substantial. However, pre-trained large multimodal models (LMMs) encounter a modality gap, characterised by a misalignment between textual and visual representations within a common embedding space. Although fine-tuning can potentially mitigate this gap, it is typically expensive and impractical due to the requirement for extensive domain-driven data. To overcome this challenge, we propose a lightweight training-free approach utilising Retrieval-Augmented Generation (RAG) to extend across the modality using a linear mapping, which can be computed efficiently. During inference, this mapping is applied to images embedded by an LMM enabling retrieval of closest textual descriptions from the training set. These textual descriptions, in conjunction with an instruction, cater as an input prompt for the language model to generate new textual descriptions. In addition, we introduce an iterative technique for distilling the mapping by generating synthetic descriptions via the language model facilitating optimisation for standard utilised image description measures. Experimental results on two benchmark multimodal datasets demonstrate significant improvements.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective</title>
<link>https://arxiv.org/abs/2508.09174</link>
<guid>https://arxiv.org/abs/2508.09174</guid>
<content:encoded><![CDATA[
<div> medical imaging, federated learning, non-IID scenarios, feature manifold completion, class-prototypes <br>
Summary:<br>
The article introduces FedMP, a novel method aimed at improving federated learning in non-IID scenarios, particularly in medical imaging applications. FedMP utilizes stochastic feature manifold completion to enhance individual client classifiers and employs class-prototypes to align feature manifolds across clients within semantically consistent subspaces, enabling the creation of distinct decision boundaries. Experimental results demonstrate that FedMP outperforms existing FL algorithms on multiple medical imaging datasets and a multi-domain natural image dataset. The study also analyzes the impact of manifold dimensionality, communication efficiency, and privacy considerations related to feature exposure in FedMP. <div>
arXiv:2508.09174v1 Announce Type: new 
Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which multiple clients collaboratively train a shared model without sharing their local private data. However, real-world applications of FL frequently encounter challenges arising from the non-identically and independently distributed (non-IID) local datasets across participating clients, which is particularly pronounced in the field of medical imaging, where shifts in image feature distributions significantly hinder the global model's convergence and performance. To address this challenge, we propose FedMP, a novel method designed to enhance FL under non-IID scenarios. FedMP employs stochastic feature manifold completion to enrich the training space of individual client classifiers, and leverages class-prototypes to guide the alignment of feature manifolds across clients within semantically consistent subspaces, facilitating the construction of more distinct decision boundaries. We validate the effectiveness of FedMP on multiple medical imaging datasets, including those with real-world multi-center distributions, as well as on a multi-domain natural image dataset. The experimental results demonstrate that FedMP outperforms existing FL algorithms. Additionally, we analyze the impact of manifold dimensionality, communication efficiency, and privacy implications of feature exposure in our method.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic</title>
<link>https://arxiv.org/abs/2508.09176</link>
<guid>https://arxiv.org/abs/2508.09176</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic quantization, mixed-precision, integer-only hardware, bit-shift operation, ResNet.

Summary: 
Dynamic Quantization Training (DQT) is a novel framework for deploying deep neural networks on resource-constrained devices. It introduces instance-based mixed-precision quantization, adapting to varying input complexity for superior accuracy-efficiency trade-off. DQT utilizes a nested integer representation and custom integer-only arithmetic to enable on-the-fly bit-width switching through a low-cost bit-shift operation, eliminating the need for costly dequantization and requantization cycles. This makes DQT the first framework to support efficient dynamic quantization without breaking the integer-only hardware paradigm. DQT achieves state-of-the-art performance on ResNet18 and ResNet50, with a 4-bit dynamic ResNet50 achieving 77.00% top-1 accuracy on ImageNet. Importantly, DQT incurs a bit-width transition cost of only 28.3M bit-shift operations, significantly outperforming previous dynamic approaches in terms of efficiency and accuracy. This breakthrough in adaptive AI represents a new frontier in deep learning model deployment on resource-constrained devices.

<br><br>Summary: <div>
arXiv:2508.09176v1 Announce Type: new 
Abstract: The deployment of deep neural networks on resource-constrained devices relies on quantization. While static, uniform quantization applies a fixed bit-width to all inputs, it fails to adapt to their varying complexity. Dynamic, instance-based mixed-precision quantization promises a superior accuracy-efficiency trade-off by allocating higher precision only when needed. However, a critical bottleneck remains: existing methods require a costly dequantize-to-float and requantize-to-integer cycle to change precision, breaking the integer-only hardware paradigm and compromising performance gains. This paper introduces Dynamic Quantization Training (DQT), a novel framework that removes this bottleneck. At the core of DQT is a nested integer representation where lower-precision values are bit-wise embedded within higher-precision ones. This design, coupled with custom integer-only arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost bit-shift operation. This makes DQT the first quantization framework to enable both dequantization-free static mixed-precision of the backbone network, and truly efficient dynamic, instance-based quantization through a lightweight controller that decides at runtime how to quantize each layer. We demonstrate DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1 accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET, 76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this with a bit-width transition cost of only 28.3M simple bit-shift operations, a drastic improvement over the 56.6M costly Multiply-Accumulate (MAC) floating-point operations required by previous dynamic approaches - unlocking a new frontier in efficient, adaptive AI.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering</title>
<link>https://arxiv.org/abs/2508.09180</link>
<guid>https://arxiv.org/abs/2508.09180</guid>
<content:encoded><![CDATA[
<div> Keywords: single-cell RNA sequencing, cell type annotation, clustering, graph neural networks, contrastive learning<br>
<br>
Summary: <br>
Accurate cell type annotation is crucial in analyzing single-cell RNA sequencing (scRNA-seq) data to understand cellular heterogeneity. Traditional clustering methods struggle with the high dimensionality and zero elements in scRNA-seq data. To address these challenges, scAGC, a novel clustering method, learns adaptive cell graphs with contrastive guidance. It optimizes feature representations and cell graphs simultaneously using a topology-adaptive graph autoencoder and a Gumbel-Softmax sampling strategy to refine graph structures dynamically. The use of a Zero-Inflated Negative Binomial (ZINB) loss helps model the distinct nature of scRNA-seq data, and a contrastive learning objective ensures stability and convergence in graph learning. Experimental results on 9 real datasets show that scAGC outperforms existing methods, achieving the best NMI and ARI scores on most datasets. This method provides a robust and effective approach for cell type annotation in scRNA-seq data analysis.<br> <div>
arXiv:2508.09180v1 Announce Type: new 
Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA sequencing (scRNA-seq) data, which provides valuable insights into cellular heterogeneity. However, due to the high dimensionality and prevalence of zero elements in scRNA-seq data, traditional clustering methods face significant statistical and computational challenges. While some advanced methods use graph neural networks to model cell-cell relationships, they often depend on static graph structures that are sensitive to noise and fail to capture the long-tailed distribution inherent in single-cell populations.To address these limitations, we propose scAGC, a single-cell clustering method that learns adaptive cell graphs with contrastive guidance. Our approach optimizes feature representations and cell graphs simultaneously in an end-to-end manner. Specifically, we introduce a topology-adaptive graph autoencoder that leverages a differentiable Gumbel-Softmax sampling strategy to dynamically refine the graph structure during training. This adaptive mechanism mitigates the problem of a long-tailed degree distribution by promoting a more balanced neighborhood structure. To model the discrete, over-dispersed, and zero-inflated nature of scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for robust feature reconstruction. Furthermore, a contrastive learning objective is incorporated to regularize the graph learning process and prevent abrupt changes in the graph topology, ensuring stability and enhancing convergence. Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC consistently outperforms other state-of-the-art methods, yielding the best NMI and ARI scores on 9 and 7 datasets, respectively.Our code is available at Anonymous Github.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach</title>
<link>https://arxiv.org/abs/2508.09181</link>
<guid>https://arxiv.org/abs/2508.09181</guid>
<content:encoded><![CDATA[
<div> Federated learning; Internet of Vehicles; non-independent and identically distributed data; client selection; truthful auction <br>
Summary:
- Federated learning (FL) in the Internet of Vehicles (IoV) involves decentralized model training on smart vehicles.
- Non-independent and identically distributed (non-IID) data from different vehicles can impact model accuracy.
- Traditional client selection methods face challenges in evaluating data quality and resource wastage.
- A novel Long-term Client-Selection Federated Learning based on Truthful Auction (LCSFLA) is proposed to address these challenges.
- LCSFLA maximizes social welfare, considers long-term data quality, and incorporates an incentive mechanism to ensure information truthfulness. <br> <div>
arXiv:2508.09181v1 Announce Type: new 
Abstract: Federated learning (FL) provides a decentralized framework that enables universal model training through collaborative efforts on mobile nodes, such as smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a mobile client, contributing to the process without uploading local data. This method leverages non-independent and identically distributed (non-IID) training data from different vehicles, influenced by various driving patterns and environmental conditions, which can significantly impact model convergence and accuracy. Although client selection can be a feasible solution for non-IID issues, it faces challenges related to selection metrics. Traditional metrics evaluate client data quality independently per round and require client selection after all clients complete local training, leading to resource wastage from unused training results. In the IoV context, where vehicles have limited connectivity and computational resources, information asymmetry in client selection risks clients submitting false information, potentially making the selection ineffective. To tackle these challenges, we propose a novel Long-term Client-Selection Federated Learning based on Truthful Auction (LCSFLA). This scheme maximizes social welfare with consideration of long-term data quality using a new assessment mechanism and energy costs, and the advised auction mechanism with a deposit requirement incentivizes client participation and ensures information truthfulness. We theoretically prove the incentive compatibility and individual rationality of the advised incentive mechanism. Experimental results on various datasets, including those from IoV scenarios, demonstrate its effectiveness in mitigating performance degradation caused by non-IID data.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring</title>
<link>https://arxiv.org/abs/2508.09187</link>
<guid>https://arxiv.org/abs/2508.09187</guid>
<content:encoded><![CDATA[
<div> Breath analysis, health monitoring, respiratory function, disease detection, machine learning<br>
Summary:<br>
Breath analysis is an essential tool for health monitoring, offering insights into respiratory function and disease detection. This survey explores contact-based and contactless approaches, including Wi-Fi Channel State Information and acoustic sensing, for accurate respiratory monitoring. It covers applications such as respiratory rate detection, user identification, and disease detection. The survey discusses data preprocessing, feature extraction, and classification techniques, highlighting machine learning and deep learning models suited for different approaches. Key challenges like dataset scarcity, multi-user interference, and data privacy are addressed, along with emerging trends like Explainable AI, federated learning, and transfer learning. By providing a comprehensive framework for future innovations in breath analysis, the survey aims to bridge advanced technological capabilities with practical healthcare applications.<br> <div>
arXiv:2508.09187v1 Announce Type: new 
Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering insights into respiratory function, disease detection, and continuous health assessment. While traditional contact-based methods are reliable, they often pose challenges in comfort and practicality, particularly for long-term monitoring. This survey comprehensively examines contact-based and contactless approaches, emphasizing recent advances in machine learning and deep learning techniques applied to breath analysis. Contactless methods, including Wi-Fi Channel State Information and acoustic sensing, are analyzed for their ability to provide accurate, noninvasive respiratory monitoring. We explore a broad range of applications, from single-user respiratory rate detection to multi-user scenarios, user identification, and respiratory disease detection. Furthermore, this survey details essential data preprocessing, feature extraction, and classification techniques, offering comparative insights into machine learning/deep learning models suited to each approach. Key challenges like dataset scarcity, multi-user interference, and data privacy are also discussed, along with emerging trends like Explainable AI, federated learning, transfer learning, and hybrid modeling. By synthesizing current methodologies and identifying open research directions, this survey offers a comprehensive framework to guide future innovations in breath analysis, bridging advanced technological capabilities with practical healthcare applications.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks</title>
<link>https://arxiv.org/abs/2508.09190</link>
<guid>https://arxiv.org/abs/2508.09190</guid>
<content:encoded><![CDATA[
<div> Fine-Grained Safety Neurons, Large Language Models, Fine-tuning, Safety Risks, Defense Strategies 
Summary: 
The article discusses the challenges posed by fine-tuning large language models (LLMs) in injecting domain-specific knowledge while introducing safety risks. Various defense strategies have been proposed for alignment, fine-tuning, and post-fine-tuning phases, but they often lack a comprehensive consideration of safety layers and fine-grained neurons. To address this, the Fine-Grained Safety Neurons (FGSN) with Training-Free Continual Projection method is proposed to reduce fine-tuning safety risks. FGSN integrates multi-scale interactions between safety layers and neurons, localizing more precise fine-grained safety neurons while minimizing interference with task neurons. Safety neuron parameters are then projected onto safety directions to enhance model safety and align with human preferences. Experimental results across multiple fine-tuned LLM models show that the method significantly reduces harmfulness scores and attack success rates while preserving model utility. Additionally, a task-specific, multi-dimensional heterogeneous safety neuron cluster optimization mechanism enhances continual defense and generalization capability against unforeseen safety concerns.<br><br>Summary: <div>
arXiv:2508.09190v1 Announce Type: new 
Abstract: Fine-tuning as service injects domain-specific knowledge into large language models (LLMs), while challenging the original alignment mechanisms and introducing safety risks. A series of defense strategies have been proposed for the alignment, fine-tuning, and post-fine-tuning phases, where most post-fine-tuning defenses rely on coarse-grained safety layer mapping. These methods lack a comprehensive consideration of both safety layers and fine-grained neurons, limiting their ability to efficiently balance safety and utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN) with Training-Free Continual Projection method to reduce the fine-tuning safety risks. FGSN inherently integrates the multi-scale interactions between safety layers and neurons, localizing sparser and more precise fine-grained safety neurons while minimizing interference with downstream task neurons. We then project the safety neuron parameters onto safety directions, improving model safety while aligning more closely with human preferences. Extensive experiments across multiple fine-tuned LLM models demonstrate that our method significantly reduce harmfulness scores and attack success rates with minimal parameter modifications, while preserving the model's utility. Furthermore, by introducing a task-specific, multi-dimensional heterogeneous safety neuron cluster optimization mechanism, we achieve continual defense and generalization capability against unforeseen emerging safety concerns.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization</title>
<link>https://arxiv.org/abs/2508.09191</link>
<guid>https://arxiv.org/abs/2508.09191</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series forecasting, TokenCast, language-based symbolic representations, contextual features, large language model (LLM) 

Summary: 
TokenCast is a framework for time series forecasting that integrates historical numerical sequences with contextual features using language-based symbolic representations. It transforms continuous numerical sequences into temporal tokens and aligns them with contextual tokens through a pre-trained large language model (LLM). This shared representation space is optimized with autoregressive generative objectives to bridge the semantic gap between modalities. The aligned LLM is then fine-tuned in a supervised manner to predict future temporal tokens, which are decoded back into the original numerical space. Experimental results on real-world datasets show the effectiveness and generalizability of TokenCast in improving forecasting accuracy by leveraging contextual information. <br><br>Summary: <div>
arXiv:2508.09191v1 Announce Type: new 
Abstract: Time series forecasting plays a vital role in supporting decision-making across a wide range of critical applications, including energy, healthcare, and finance. Despite recent advances, forecasting accuracy remains limited due to the challenge of integrating historical numerical sequences with contextual features, which often comprise unstructured textual data. To address this challenge, we propose TokenCast, an LLM-driven framework that leverages language-based symbolic representations as a unified intermediary for context-aware time series forecasting. Specifically, TokenCast employs a discrete tokenizer to transform continuous numerical sequences into temporal tokens, enabling structural alignment with language-based inputs. To bridge the semantic gap between modalities, both temporal and contextual tokens are embedded into a shared representation space via a pre-trained large language model (LLM), further optimized with autoregressive generative objectives. Building upon this unified semantic space, the aligned LLM is subsequently fine-tuned in a supervised manner to predict future temporal tokens, which are then decoded back into the original numerical space. Extensive experiments on diverse real-world datasets enriched with contextual features demonstrate the effectiveness and generalizability of TokenCast.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing</title>
<link>https://arxiv.org/abs/2508.09192</link>
<guid>https://arxiv.org/abs/2508.09192</guid>
<content:encoded><![CDATA[
<div> Keywords: dLLMs, diffusion large language models, autoregressive, inference speed, parallel decoding <br>
Summary:<br>
Diffusion Large Language Models (dLLMs) are being explored as a viable option for text generation, offering the potential to decode multiple tokens simultaneously. However, existing dLLMs have not been able to surpass the inference speed of autoregressive (AR) LLMs of similar size. This paper introduces a novel approach called discrete diffusion forcing (D2F) to enhance the efficiency of dLLMs. D2F equips dLLMs with block-wise autoregressive generation and the ability to predict following tokens without completing prior blocks, transforming them into an AR-diffusion hybrid paradigm. Through an asymmetric distillation process and a pipelined parallel decoding algorithm, D2F dLLMs achieve significantly faster inference speeds compared to existing dLLMs like LLaMA3 and Qwen2.5 on GSM8K dataset. The code for D2F is available on GitHub, offering a more than 50x acceleration in inference speed while maintaining comparable output quality. <br> <div>
arXiv:2508.09192v1 Announce Type: new 
Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs for text generation, with the potential to decode multiple tokens in a single iteration. However, none of the existing open-source dLLMs have achieved superior inference speed over AR LLMs of similar size. This paper breaks this barrier based on a simple and effective strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key capabilities: (1) block-wise autoregressive generation to enable KV cache utilization; (2) prediction of following tokens without requiring completion of prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs are refurbished into an AR-diffusion hybrid paradigm for efficient inference. D2F can be implemented with an asymmetric distillation process based on pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm, which enables a trade-off between efficiency and efficacy. Empirically, D2F dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the acceleration can be more than $\mathbf{50\times}$ while maintaining comparable output quality. The code is available at https://github.com/zhijie-group/Discrete-Diffusion-Forcing.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL</title>
<link>https://arxiv.org/abs/2508.09193</link>
<guid>https://arxiv.org/abs/2508.09193</guid>
<content:encoded><![CDATA[
<div> Keywords: generative modeling, natural language, procedural content generation, multi-objective instructions, controllability

Summary: 
Generative modeling has advanced, highlighting the importance of natural language for controlling content generation efficiently. However, current methods struggle with leveraging textual input, especially with complex instructions, limiting controllability. To address this, the proposed method, MIPCGRL, utilizes multi-objective representation learning, incorporating sentence embeddings as conditions. By incorporating multi-label classification and multi-head regression networks, MIPCGRL effectively trains a multi-objective embedding space, resulting in improved controllability with multi-objective instructions, achieving up to a 13.8% enhancement. This capability to process complex instructions allows for more expressive and flexible content generation. 

<br><br>Summary: <div>
arXiv:2508.09193v1 Announce Type: new 
Abstract: Recent advancements in generative modeling emphasize the importance of natural language as a highly expressive and accessible modality for controlling content generation. However, existing instructed reinforcement learning for procedural content generation (IPCGRL) method often struggle to leverage the expressive richness of textual input, especially under complex, multi-objective instructions, leading to limited controllability. To address this problem, we propose \textit{MIPCGRL}, a multi-objective representation learning method for instructed content generators, which incorporates sentence embeddings as conditions. MIPCGRL effectively trains a multi-objective embedding space by incorporating multi-label classification and multi-head regression networks. Experimental results show that the proposed method achieves up to a 13.8\% improvement in controllability with multi-objective instructions. The ability to process complex instructions enables more expressive and flexible content generation.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments</title>
<link>https://arxiv.org/abs/2508.09194</link>
<guid>https://arxiv.org/abs/2508.09194</guid>
<content:encoded><![CDATA[
<div> Meta-learning, decentralized systems, large language models, inference acceleration, efficiency<br>
<br>
Summary: 
In this study, a meta-learning-based framework is introduced to automate the selection of optimal acceleration methods in decentralized systems, particularly for large language models. By learning from historical performance data of various acceleration techniques across different tasks, this framework efficiently identifies the best acceleration strategies based on task-specific characteristics. Compared to traditional methods that rely on random selection or expert intuition, the meta-learning approach consistently outperforms in terms of efficiency and performance. The results showcase the potential of inference acceleration in decentralized AI systems, offering a promising avenue towards more democratic and economically viable artificial intelligence solutions. <div>
arXiv:2508.09194v1 Announce Type: new 
Abstract: The deployment of large-scale models, such as large language models (LLMs), incurs substantial costs due to their computational demands. To mitigate these costs and address challenges related to scalability and data security, there is a growing shift towards decentralized systems for model deployment, where choosing efficient inference acceleration schemes become crucial to manage computational resources effectively and enhance system responsiveness. In this work, we address the challenge of selecting optimal acceleration methods in decentralized systems by introducing a meta-learning-based framework. This framework automates the selection process by learning from historical performance data of various acceleration techniques across different tasks. Unlike traditional methods that rely on random selection or expert intuition, our approach systematically identifies the best acceleration strategies based on the specific characteristics of each task. We demonstrate that our meta-learning framework not only streamlines the decision-making process but also consistently outperforms conventional methods in terms of efficiency and performance. Our results highlight the potential of inference acceleration in decentralized AI systems, offering a path towards more democratic and economically feasible artificial intelligence solutions.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce</title>
<link>https://arxiv.org/abs/2508.09198</link>
<guid>https://arxiv.org/abs/2508.09198</guid>
<content:encoded><![CDATA[
<div> marketing strategy, coupon distribution, online platforms, revenue boosting, user engagement 

Summary: 
The paper discusses the inefficiencies of current coupon distribution strategies on online platforms and proposes a new framework called Aligned Decision Transformer for Coupons (ADT4Coupons) to improve revenue and user engagement. The framework is designed to optimize online decision-making by considering general scenarios, sequential modeling with comprehensive historical data, and efficient iterative updates. Empirical results on real-world industrial datasets, as well as public and synthetic datasets, showcase the effectiveness of the ADT4Coupons framework in enhancing long-term revenue. <div>
arXiv:2508.09198v1 Announce Type: new 
Abstract: Coupon distribution is a critical marketing strategy used by online platforms to boost revenue and enhance user engagement. Regrettably, existing coupon distribution strategies fall far short of effectively leveraging the complex sequential interactions between platforms and users. This critical oversight, despite the abundance of e-commerce log data, has precipitated a performance plateau. In this paper, we focus on the scene that the platforms make sequential coupon distribution decision multiple times for various users, with each user interacting with the platform repeatedly. Based on this marketing scenario, we propose a novel marketing framework, named Aligned Decision Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution policy for long-term revenue boosting. ADT4Coupons enables optimized online decision-making in a variety of real-world marketing scenarios. It achieves this by seamlessly integrating three key characteristics, general scenarios, sequential modeling with more comprehensive historical data, and efficient iterative updates within a unified framework. Furthermore, empirical results on real-world industrial dataset, alongside public and synthetic datasets demonstrate the superiority of our framework.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research</title>
<link>https://arxiv.org/abs/2508.09203</link>
<guid>https://arxiv.org/abs/2508.09203</guid>
<content:encoded><![CDATA[
<div> Keywords: Construction safety, dataset, OSHA, machine learning, inspections <br>
<br>
Summary: 
Construction safety research is crucial for preventing injuries on construction sites. However, the lack of diverse and comprehensive datasets hinders in-depth analysis in this field. To address this gap, the Construction Safety Dataset (CSDataset) is introduced, which includes incidents, inspections, and violations from OSHA. This dataset combines structured attributes with unstructured narratives, enabling a wide range of machine learning and language model-driven approaches. Preliminary benchmarking and cross-level analyses using the dataset reveal insights such as the positive impact of complaint-driven inspections on reducing subsequent incidents by 17.3%. By releasing the dataset and associated code, this research aims to enhance future efforts in construction safety by providing a valuable resource for researchers in the field. <div>
arXiv:2508.09203v1 Announce Type: new 
Abstract: Construction safety research is a critical field in civil engineering, aiming to mitigate risks and prevent injuries through the analysis of site conditions and human factors. However, the limited volume and lack of diversity in existing construction safety datasets pose significant challenges to conducting in-depth analyses. To address this research gap, this paper introduces the Construction Safety Dataset (CSDataset), a well-organized comprehensive multi-level dataset that encompasses incidents, inspections, and violations recorded sourced from the Occupational Safety and Health Administration (OSHA). This dataset uniquely integrates structured attributes with unstructured narratives, facilitating a wide range of approaches driven by machine learning and large language models. We also conduct a preliminary approach benchmarking and various cross-level analyses using our dataset, offering insights to inform and enhance future efforts in construction safety. For example, we found that complaint-driven inspections were associated with a 17.3% reduction in the likelihood of subsequent incidents. Our dataset and code are released at https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoQE: Improve Quantization Model performance via Mixture of Quantization Experts</title>
<link>https://arxiv.org/abs/2508.09204</link>
<guid>https://arxiv.org/abs/2508.09204</guid>
<content:encoded><![CDATA[
<div> quantization, inference framework, MoE architecture, performance improvement, specialization

Summary:<br>
The paper introduces MoQE, a quantization inference framework that utilizes the MoE architecture to enhance model efficiency. MoQE combines multiple quantization variants of a full-precision model as specialized "quantization experts" and dynamically routes input data based on its characteristics. By leveraging specialization quantization expert models, MoQE mitigates the typical performance degradation of single quantization models. Additionally, lightweight, structure-aware router models are designed for computer vision (CV) and natural language processing (NLP) tasks. Experimental evaluations on various model families and benchmark datasets show that MoQE achieves performance levels comparable to state-of-the-art quantization models without significantly increasing inference latency.<br> <div>
arXiv:2508.09204v1 Announce Type: new 
Abstract: Quantization method plays a crucial role in improving model efficiency and reducing deployment costs, enabling the widespread application of deep learning models on resource-constrained devices. However, the quantization process inevitably introduces accuracy degradation. In this paper, we propose Mixture of Quantization Experts( abbr. MoQE), a quantization inference framework based on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the performance of quantization models. MoQE combines multiple quantization variants of one full-precision model as specialized "quantization experts" and dynamically routes input data to the most suitable expert based on its characteristics. MoQE alleviates the performance degradation commonly seen in single quantization models through specialization quantization expert models. We design lightweight, structure-aware router models tailored for both CV and NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText demonstrate that MoQE achieves performance comparable to SOTA quantization model, without incurring significant increases in inference latency.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair</title>
<link>https://arxiv.org/abs/2508.09206</link>
<guid>https://arxiv.org/abs/2508.09206</guid>
<content:encoded><![CDATA[
<div> Keywords: Laser-enabled selective transfer, computational models, microLED fabrication, repair algorithm, gradient-based optimization <br>
Summary: 
The article presents a novel repair algorithm for laser-enabled selective transfer in high-throughput microLED fabrication. This algorithm makes use of a differentiable transfer module that models discrete shifts of transfer platforms and is trainable via gradient-based optimization. Compared to traditional local proximity searching algorithms, this approach outperforms in repair performance and allows for more flexible objective designs, such as minimizing the number of steps. Unlike reinforcement learning-based methods, this algorithm eliminates the need for handcrafted feature extractors and trains significantly faster, enabling scalability to large arrays. Experimental results show a 50% reduction in transfer steps and planning times under two minutes for 2000x2000 arrays. This method offers a practical and adaptable solution for accelerating microLED repair in AR/VR and next-generation display fabrication. <br><br> Summary: <div>
arXiv:2508.09206v1 Announce Type: new 
Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED fabrication, requires computational models that can plan shift sequences to minimize motion of XY stages and adapt to varying optimization objectives across the substrate. We propose the first repair algorithm based on a differentiable transfer module designed to model discrete shifts of transfer platforms, while remaining trainable via gradient-based optimization. Compared to local proximity searching algorithms, our approach achieves superior repair performance and enables more flexible objective designs, such as minimizing the number of steps. Unlike reinforcement learning (RL)-based approaches, our method eliminates the need for handcrafted feature extractors and trains significantly faster, allowing scalability to large arrays. Experiments show a 50% reduction in transfer steps and sub-2-minute planning time on 2000x2000 arrays. This method provides a practical and adaptable solution for accelerating microLED repair in AR/VR and next-generation display fabrication.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2508.09223</link>
<guid>https://arxiv.org/abs/2508.09223</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-time adaptation, Hierarchical Adaptive Networks, Dynamic layer selection, Linear layer agreement, Robustness.

Summary:
Hi-Vec is a novel approach for test-time adaptation that utilizes hierarchical adaptive networks with task vectors. It decomposes the encoder's representation space into multiple layers to handle diverse and complex shifts in incoming data streams. The method includes dynamic layer selection for optimal adaptation, weight merging to ensure all layers receive target information, and linear layer agreement as a gating function to prevent fine-tuning on noisy batches. In rigorous evaluations, Hi-Vec outperformed existing methods in challenging scenarios and on multiple target datasets, demonstrating improved robustness, uncertainty handling, and efficacy in limited batch sizes and increased outlier rates. Hi-Vec represents a significant advancement in test-time adaptation techniques, offering a versatile and effective solution for addressing distribution shifts between source and target domains.<br><br>Summary: <div>
arXiv:2508.09223v1 Announce Type: new 
Abstract: Test-time adaptation allows pretrained models to adjust to incoming data streams, addressing distribution shifts between source and target domains. However, standard methods rely on single-dimensional linear classification layers, which often fail to handle diverse and complex shifts. We propose Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages multiple layers of increasing size for dynamic test-time adaptation. By decomposing the encoder's representation space into such hierarchically organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to adapt to shifts of varying complexity. Our contributions are threefold: First, we propose dynamic layer selection for automatic identification of the optimal layer for adaptation to each test batch. Second, we propose a mechanism that merges weights from the dynamic layer to other layers, ensuring all layers receive target information. Third, we propose linear layer agreement that acts as a gating function, preventing erroneous fine-tuning by adaptation on noisy batches. We rigorously evaluate the performance of Hi-Vec in challenging scenarios and on multiple target datasets, proving its strong capability to advance state-of-the-art methods. Our results show that Hi-Vec improves robustness, addresses uncertainty, and handles limited batch sizes and increased outlier rates.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction</title>
<link>https://arxiv.org/abs/2508.09227</link>
<guid>https://arxiv.org/abs/2508.09227</guid>
<content:encoded><![CDATA[
<div> Graph Attention Network, trajectory prediction, urban transportation, Recurrent Neural Network, intelligent transportation systems
Summary: The article introduces GSMT, a hybrid model that combines a Graph Attention Network (GAT) and a Recurrent Neural Network (RNN) for accurate trajectory prediction of buses in urban environments with limited multimodal data. The model includes a task corrector to extract complex behavioral patterns from trajectory data, clustering historical trajectories to identify motion patterns. GSMT fuses dynamic bus and static station information through embedded networks to predict trajectories and uses the task corrector for refinement. Experiments on real data from Kuala Lumpur show superior performance in short-term and long-term trajectory prediction tasks. <div>
arXiv:2508.09227v1 Announce Type: new 
Abstract: Accurate trajectory prediction for buses is crucial in intelligent transportation systems, particularly within urban environments. In developing regions where access to multimodal data is limited, relying solely on onboard GPS data remains indispensable despite inherent challenges. To address this problem, we propose GSMT, a hybrid model that integrates a Graph Attention Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and incorporates a task corrector capable of extracting complex behavioral patterns from large-scale trajectory data. The task corrector clusters historical trajectories to identify distinct motion patterns and fine-tunes the predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus information and static station information through embedded hybrid networks to perform trajectory prediction, and applies the task corrector for secondary refinement after the initial predictions are generated. This two-stage approach enables multi-node trajectory prediction among buses operating in dense urban traffic environments under complex conditions. Experiments conducted on a real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method significantly outperforms existing approaches, achieving superior performance in both short-term and long-term trajectory prediction tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain Network Analysis using Quantum Inspired Graph Neural Networks &amp; Ensemble Models</title>
<link>https://arxiv.org/abs/2508.09237</link>
<guid>https://arxiv.org/abs/2508.09237</guid>
<content:encoded><![CDATA[
<div> Keywords: financial technology, blockchain networks, Quantum Inspired Graph Neural Networks, Ensemble Model, anti-money laundering

Summary: 
Quantum Inspired Graph Neural Networks (QI-GNN) combined with a choice of QBoost or Random Forrest Classifier in Ensemble Model are proposed for detecting illicit transactions in blockchain networks. A Canonical Polyadic (CP) decomposition layer within the graph neural network framework enhances data processing efficiency. The system achieved an F2 score of 74.8% in detecting fraudulent transactions, surpassing classical machine learning implementations. Quantum-inspired techniques, along with the CP layer, show potential for exceeding traditional methods in complex network analysis for financial security. The study advocates for wider adoption and further exploration of quantum-inspired algorithms in the financial sector to combat fraud effectively. 

<br><br>Summary: <div>
arXiv:2508.09237v1 Announce Type: new 
Abstract: In the rapidly evolving domain of financial technology, the detection of illicit transactions within blockchain networks remains a critical challenge, necessitating robust and innovative solutions. This work proposes a novel approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with flexibility of choice of an Ensemble Model using QBoost or a classic model such as Random Forrest Classifier. This system is tailored specifically for blockchain network analysis in anti-money laundering (AML) efforts. Our methodology to design this system incorporates a novel component, a Canonical Polyadic (CP) decomposition layer within the graph neural network framework, enhancing its capability to process and analyze complex data structures efficiently. Our technical approach has undergone rigorous evaluation against classical machine learning implementations, achieving an F2 score of 74.8% in detecting fraudulent transactions. These results highlight the potential of quantum-inspired techniques, supplemented by the structural advancements of the CP layer, to not only match but potentially exceed traditional methods in complex network analysis for financial security. The findings advocate for a broader adoption and further exploration of quantum-inspired algorithms within the financial sector to effectively combat fraud.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data</title>
<link>https://arxiv.org/abs/2508.09263</link>
<guid>https://arxiv.org/abs/2508.09263</guid>
<content:encoded><![CDATA[
<div> framework, tabular data modeling, large language models, zero-shot learning, few-shot learning
Summary:
This paper introduces a novel framework for tabular data modeling using large language models (LLMs). The framework allows for effective utilization of LLMs in zero-shot and few-shot scenarios by generating feature values based on task and feature descriptions. This approach enables the creation of zero-shot prototypes without the need for training a classifier or finetuning the LLMs. By using an example-free prompt, the framework overcomes constraints associated with example-based prompts, making it scalable and robust. Experimental results demonstrate the effectiveness of the proposed framework in zero and few-shot tabular learning. 
<br><br>Summary: <div>
arXiv:2508.09263v1 Announce Type: new 
Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to in-depth investigation of their potential in tabular data modeling. However, effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is still challenging. To this end, we propose a novel LLM-based prototype estimation framework for tabular learning. Our key idea is to query the LLM to generate feature values based example-free prompt, which solely relies on task and feature descriptions. With the feature values generated by LLM, we can build a zero-shot prototype in a training-free manner, which can be further enhanced by fusing few-shot samples, avoiding training a classifier or finetuning the LLMs. Thanks to the example-free prompt and prototype estimation, ours bypasses the constraints brought by the example-based prompt, providing a scalable and robust framework. Extensive experiments demonstrate the effectiveness of ours in zero and few-shot tabular learning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of Odor Presence via Deep Neural Networks</title>
<link>https://arxiv.org/abs/2508.09264</link>
<guid>https://arxiv.org/abs/2508.09264</guid>
<content:encoded><![CDATA[
<div> Keywords: odor detection, local field potentials, deep learning, ensemble model, olfactory bulb

Summary:
- The study aims to develop a general system for odor detection using local field potentials (LFPs) from the olfactory bulb.
- Two hypotheses were tested: spectral features of LFPs are enough for robust single-trial odor detection, and signals from the olfactory bulb alone are sufficient.
- An ensemble of convolutional networks (ResCNN and AttentionCNN) was used to decode the presence of odor from multichannel olfactory bulb LFPs.
- The model achieved high accuracy (86.6%), F1-score (81.0%), and AUC (0.9247), outperforming previous benchmarks.
- Results demonstrate the feasibility of robust single-trial odor detection from LFPs and the potential of deep learning models in understanding olfactory representations.

<br><br>Summary: <div>
arXiv:2508.09264v1 Announce Type: new 
Abstract: Odor detection underpins food safety, environmental monitoring, medical diagnostics, and many more fields. The current artificial sensors developed for odor detection struggle with complex mixtures while non-invasive recordings lack reliable single-trial fidelity. To develop a general system for odor detection, in this study we present a preliminary work where we aim to test two hypotheses: (i) that spectral features of local field potentials (LFPs) are sufficient for robust single-trial odor detection and (ii) that signals from the olfactory bulb alone are adequate. To test two hypotheses, we propose an ensemble of complementary one-dimensional convolutional networks (ResCNN and AttentionCNN) that decodes the presence of odor from multichannel olfactory bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score of 81.0%, and an AUC of 0.9247, substantially outperforming previous benchmarks. In addition, the t-SNE visualization confirms that our framework captures biologically significant signatures. These findings establish the feasibility of robust single-trial detection of the presence of odor from extracellular LFPs, as well as demonstrate the potential of deep learning models to provide a deeper understanding of olfactory representations.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Over-Squashing in GNNs and Causal Inference of Rewiring Strategies</title>
<link>https://arxiv.org/abs/2508.09265</link>
<guid>https://arxiv.org/abs/2508.09265</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, GNNs, over-squashing, rewiring techniques, topology, empirical analyses.

Summary:
Over-squashing in message-passing GNNs limits expressivity due to exponential compression of long-range information. A new method assesses over-squashing between node pairs by their mutual sensitivity decay rate. This method is extended to graph-level statistics. Empirical analyses on various benchmarks show most graph classification datasets suffer from over-squashing. Rewiring strategies effectively mitigate over-squashing, with varying degrees of success depending on dataset and method. Node classification datasets show less notable over-squashing, and the effects of rewiring are inconsistent. Rewiring is most beneficial for substantially over-squashed graphs when applied with restraint. Aggressive rewiring or applying it to minimally over-squashed graphs may not improve performance and could even harm it. A diagnostic tool allows practitioners to assess the potential benefits of rewiring before training. 

<br><br>Summary: <div>
arXiv:2508.09265v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance across wide-range of domains such as recommender systems, material design, and drug repurposing. Yet message-passing GNNs suffer from over-squashing -- exponential compression of long-range information from distant nodes -- which limits expressivity. Rewiring techniques can ease this bottleneck; but their practical impacts are unclear due to the lack of a direct empirical over-squashing metric. We propose a rigorous, topology-focused method for assessing over-squashing between node pairs using the decay rate of their mutual sensitivity. We then extend these pairwise assessments to four graph-level statistics (prevalence, intensity, variability, extremity). Coupling these metrics with a within-graph causal design, we quantify how rewiring strategies affect over-squashing on diverse graph- and node-classification benchmarks. Our extensive empirical analyses show that most graph classification datasets suffer from over-squashing (but to various extents), and rewiring effectively mitigates it -- though the degree of mitigation, and its translation into performance gains, varies by dataset and method. We also found that over-squashing is less notable in node classification datasets, where rewiring often increases over-squashing, and performance variations are uncorrelated with over-squashing changes. These findings suggest that rewiring is most beneficial when over-squashing is both substantial and corrected with restraint -- while overly aggressive rewiring, or rewiring applied to minimally over-squashed graphs, is unlikely to help and may even harm performance. Our plug-and-play diagnostic tool lets practitioners decide -- before any training -- whether rewiring is likely to pay off.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.09275</link>
<guid>https://arxiv.org/abs/2508.09275</guid>
<content:encoded><![CDATA[
<div> Collaborative multi-agent reinforcement learning, c-MARL, adversarial attacks, vulnerabilities, observations<br>
<br>
Summary: 
This paper explores the vulnerabilities of collaborative multi-agent reinforcement learning (c-MARL) to adversarial attacks, focusing on realistic conditions where adversaries can only collect and perturb the observations of deployed agents. The study introduces effective algorithms for generating adversarial perturbations that deceive victim agents by misaligning their perception of the environment. The proposed approach is tested on three benchmarks and 22 environments, showcasing its robustness across various algorithms and scenarios. Importantly, the algorithm demonstrates high sample efficiency, requiring only 1,000 samples compared to millions in previous methods. This research sheds light on the security implications of c-MARL and provides insights into enhancing the resilience of multi-agent systems against adversarial threats. <div>
arXiv:2508.09275v1 Announce Type: new 
Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly evolved, offering state-of-the-art algorithms for real-world applications, including sensitive domains. However, a key challenge to its widespread adoption is the lack of a thorough investigation into its vulnerabilities to adversarial attacks. Existing work predominantly focuses on training-time attacks or unrealistic scenarios, such as access to policy weights or the ability to train surrogate policies. In this paper, we investigate new vulnerabilities under more realistic and constrained conditions, assuming an adversary can only collect and perturb the observations of deployed agents. We also consider scenarios where the adversary has no access at all. We propose simple yet highly effective algorithms for generating adversarial perturbations designed to misalign how victim agents perceive their environment. Our approach is empirically validated on three benchmarks and 22 environments, demonstrating its effectiveness across diverse algorithms and environments. Furthermore, we show that our algorithm is sample-efficient, requiring only 1,000 samples compared to the millions needed by previous methods.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning</title>
<link>https://arxiv.org/abs/2508.09281</link>
<guid>https://arxiv.org/abs/2508.09281</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized learning, knowledge components, automated KC extraction, explainable framework, code patterns

Summary:
This article introduces a novel framework for automated Knowledge Component (KC) discovery in computer science education. The framework utilizes pattern-based KCs to capture recurring structural patterns within student code, allowing for more accurate knowledge modeling. By training a Variational Autoencoder guided by an explainable, attention-based code representation model, important patterns are extracted from student code and clustered to form pattern-based KCs. Evaluation methods including learning curve analysis and Deep Knowledge Tracing (DKT) show significant improvements in predictive performance over traditional methods. This approach enhances knowledge modeling in CS education by providing an automated, scalable, and explainable way to identify key code patterns and algorithmic constructs necessary for student learning. <div>
arXiv:2508.09281v1 Announce Type: new 
Abstract: Effective personalized learning in computer science education depends on accurately modeling what students know and what they need to learn. While Knowledge Components (KCs) provide a foundation for such modeling, automated KC extraction from student code is inherently challenging due to insufficient explainability of discovered KCs and the open-endedness of programming problems with significant structural variability across student solutions and complex interactions among programming concepts. In this work, we propose a novel, explainable framework for automated KC discovery through pattern-based KCs: recurring structural patterns within student code that capture the specific programming patterns and language constructs that students must master. Toward this, we train a Variational Autoencoder to generate important representative patterns from student code guided by an explainable, attention-based code representation model that identifies important correct and incorrect pattern implementations from student code. These patterns are then clustered to form pattern-based KCs. We evaluate our KCs using two well-established methods informed by Cognitive Science: learning curve analysis and Deep Knowledge Tracing (DKT). Experimental results demonstrate meaningful learning trajectories and significant improvements in DKT predictive performance over traditional KT methods. This work advances knowledge modeling in CS education by providing an automated, scalable, and explainable framework for identifying granular code patterns and algorithmic constructs, essential for student learning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling Reinforcement Learning into Single-Batch Datasets</title>
<link>https://arxiv.org/abs/2508.09283</link>
<guid>https://arxiv.org/abs/2508.09283</guid>
<content:encoded><![CDATA[
<div> Dataset distillation, reinforcement learning environments, supervised learning, proximal policy optimization, meta-learning <br>
Summary: <br>
The article introduces dataset distillation as a method to compress large datasets into smaller synthetic ones, enabling training in just one gradient descent step. It demonstrates the generalizability of distillation across different tasks, showcasing its ability to transform reinforcement learning environments into one-batch supervised learning datasets. By employing a novel extension of proximal policy optimization for meta-learning, the study distills complex RL environments, including MuJoCo environments and Atari games, into one-step supervised learning tasks. It explores the compression of RL environments into the smallest possible synthetic dataset and examines the generalizability of RL distillation across various learner architectures. The research highlights the potential of distillation to condense intricate RL tasks into simpler supervised learning formats, indicating its versatility in transforming learning modalities. <br> <div>
arXiv:2508.09283v1 Announce Type: new 
Abstract: Dataset distillation compresses a large dataset into a small synthetic dataset such that learning on the synthetic dataset approximates learning on the original. Training on the distilled dataset can be performed in as little as one step of gradient descent. We demonstrate that distillation is generalizable to different tasks by distilling reinforcement learning environments into one-batch supervised learning datasets. This demonstrates not only distillation's ability to compress a reinforcement learning task but also its ability to transform one learning modality (reinforcement learning) into another (supervised learning). We present a novel extension of proximal policy optimization for meta-learning and use it in distillation of a multi-dimensional extension of the classic cart-pole problem, all MuJoCo environments, and several Atari games. We demonstrate distillation's ability to compress complex RL environments into one-step supervised learning, explore RL distillation's generalizability across learner architectures, and demonstrate distilling an environment into the smallest-possible synthetic dataset.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation</title>
<link>https://arxiv.org/abs/2508.09299</link>
<guid>https://arxiv.org/abs/2508.09299</guid>
<content:encoded><![CDATA[
<div> Keywords: Weather forecasting, Federated Learning, blockchain technology, security, scalability

Summary: 
Weather forecasting is essential for disaster preparedness and resource management. Current centralized systems face security vulnerabilities and scalability issues. To address these challenges, a decentralized framework combining Federated Learning (FL) and blockchain technology is proposed. FL allows collaborative model training without compromising sensitive data, improving privacy and reducing data transfer overhead. The use of the Ethereum blockchain ensures transparent verification of model updates. A reputation-based voting mechanism assesses the trustworthiness of submitted models, enhancing system security. The Interplanetary File System (IPFS) is utilized for efficient off-chain storage, ensuring system resilience and scalability. Experimental results show that this approach enhances forecasting accuracy and is suitable for deployment in real-world security-critical environments. 

<br><br>Summary: <div>
arXiv:2508.09299v1 Announce Type: new 
Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture, and resource management, yet current centralized forecasting systems are increasingly strained by security vulnerabilities, limited scalability, and susceptibility to single points of failure. To address these challenges, we propose a decentralized weather forecasting framework that integrates Federated Learning (FL) with blockchain technology. FL enables collaborative model training without exposing sensitive local data; this approach enhances privacy and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures transparent and dependable verification of model updates. To further enhance the system's security, we introduce a reputation-based voting mechanism that assesses the trustworthiness of submitted models while utilizing the Interplanetary File System (IPFS) for efficient off-chain storage. Experimental results demonstrate that our approach not only improves forecasting accuracy but also enhances system resilience and scalability, making it a viable candidate for deployment in real-world, security-critical environments.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Verification of Graph Neural Networks with Incremental Constraint Solving</title>
<link>https://arxiv.org/abs/2508.09320</link>
<guid>https://arxiv.org/abs/2508.09320</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, GNNs, adversarial attacks, exact verification method, message-passing neural networks<br>
Summary:
The paper introduces an exact verification method for Graph Neural Networks (GNNs) to provide guarantees against attribute and structural perturbations with budget constraints, including edge additions or deletions. The method utilizes constraint solving with bound tightening and iteratively solves relaxed constraint satisfaction problems. GNNev, a solver developed for message-passing neural networks, supports aggregation functions such as sum, max, and mean. Experimental evaluation on benchmark datasets and real-world fraud datasets shows GNNev's usability and effectiveness, particularly in node classification tasks using max and mean aggregation functions. The method outperforms existing exact verification tools in sum-aggregated tasks. <div>
arXiv:2508.09320v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes applications, such as fraud detection or healthcare, but are susceptible to adversarial attacks. A number of techniques have been proposed to provide adversarial robustness guarantees, but support for commonly used aggregation functions in message-passing GNNs is still lacking. In this paper, we develop an exact (sound and complete) verification method for GNNs to compute guarantees against attribute and structural perturbations that involve edge addition or deletion, subject to budget constraints. Focusing on node classification tasks, our method employs constraint solving with bound tightening, and iteratively solves a sequence of relaxed constraint satisfaction problems while relying on incremental solving capabilities of solvers to improve efficiency. We implement GNNev, a versatile solver for message-passing neural networks, which supports three aggregation functions, sum, max and mean, with the latter two considered here for the first time. Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its usability and effectiveness, as well as superior performance compared to existing {exact verification} tools on sum-aggregated node classification tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization</title>
<link>https://arxiv.org/abs/2508.09330</link>
<guid>https://arxiv.org/abs/2508.09330</guid>
<content:encoded><![CDATA[
<div> synaptic pruning, dropout regularization, magnitude-based, time series forecasting, financial forecasting
<br>
Summary: 
This article introduces a new magnitude-based synaptic pruning method for artificial neural networks that mimics biological synaptic pruning. The method gradually removes low-importance connections during training, improving efficiency. Integrated directly into the training loop, this approach computes weight importance across layers and progressively increases global sparsity using a cubic schedule. By eliminating the need for separate pruning and fine-tuning phases, it simplifies the process. Experimental results on various time series forecasting models demonstrate consistent gains, with the method outperforming traditional dropout techniques significantly. In financial forecasting tasks, it reduced Mean Absolute Error by up to 20% compared to models without or with standard dropout, and up to 52% in certain transformer models. This dynamic pruning mechanism offers a practical alternative for regularization, showcasing its potential in diverse architectures. 
<br> <div>
arXiv:2508.09330v1 Announce Type: new 
Abstract: Synaptic pruning in biological brains removes weak connections to improve efficiency. In contrast, dropout regularization in artificial neural networks randomly deactivates neurons without considering activity-dependent pruning. We propose a magnitude-based synaptic pruning method that better reflects biology by progressively removing low-importance connections during training. Integrated directly into the training loop as a dropout replacement, our approach computes weight importance from absolute magnitudes across layers and applies a cubic schedule to gradually increase global sparsity. At fixed intervals, pruning masks permanently remove low-importance weights while maintaining gradient flow for active ones, eliminating the need for separate pruning and fine-tuning phases. Experiments on multiple time series forecasting models including RNN, LSTM, and Patch Time Series Transformer across four datasets show consistent gains. Our method ranked best overall, with statistically significant improvements confirmed by Friedman tests (p < 0.01). In financial forecasting, it reduced Mean Absolute Error by up to 20% over models with no or standard dropout, and up to 52% in select transformer models. This dynamic pruning mechanism advances regularization by coupling weight elimination with progressive sparsification, offering easy integration into diverse architectures. Its strong performance, especially in financial time series forecasting, highlights its potential as a practical alternative to conventional dropout techniques.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs</title>
<link>https://arxiv.org/abs/2508.09334</link>
<guid>https://arxiv.org/abs/2508.09334</guid>
<content:encoded><![CDATA[
<div> Keywords: RicciFlowRec, recommendation framework, Ricci curvature, financial graphs, risk-aware ranking

Summary:
RicciFlowRec is a novel recommendation framework that utilizes Ricci curvature and flow on dynamic financial graphs to attribute root causes in the financial domain. By analyzing interactions between stocks, macroeconomic indicators, and news, the framework quantifies local stress and traces shock propagation through Ricci flow. The use of discrete Ricci curvature helps identify causal substructures, leading to a structural risk-aware ranking function. Initial results on S&P 500 data combined with sentiment analysis from FinBERT show improved robustness and interpretability in the face of synthetic perturbations. This work lays the foundation for curvature-based attribution and risk-aware ranking, with future plans to incorporate portfolio optimization and return forecasting. RicciFlowRec represents a pioneering effort in applying geometric flow-based reasoning to financial decision-making support. 

<br><br>Summary: Keywords: RicciFlowRec, recommendation framework, Ricci curvature, financial graphs, risk-aware ranking <div>
arXiv:2508.09334v1 Announce Type: new 
Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs root cause attribution via Ricci curvature and flow on dynamic financial graphs. By modelling evolving interactions among stocks, macroeconomic indicators, and news, we quantify local stress using discrete Ricci curvature and trace shock propagation via Ricci flow. Curvature gradients reveal causal substructures, informing a structural risk-aware ranking function. Preliminary results on S\&amp;P~500 data with FinBERT-based sentiment show improved robustness and interpretability under synthetic perturbations. This ongoing work supports curvature-based attribution and early-stage risk-aware ranking, with plans for portfolio optimization and return forecasting. To our knowledge, RicciFlowRec is the first recommender to apply geometric flow-based reasoning in financial decision support.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.09363</link>
<guid>https://arxiv.org/abs/2508.09363</guid>
<content:encoded><![CDATA[
<div> Sparse autoencoders, domain-specific features, medical text, interpretability, Gemma-2 models<br>
Summary:<br>
The study focuses on improving the interpretability of sparse autoencoders (SAEs) by training them on domain-specific data, specifically medical text. By confining the training of SAEs to medical text, the researchers found that the models were able to capture more variance, achieve higher loss recovery, and reduce linear residual error compared to SAEs trained on broad data distributions. The domain-specific SAEs were able to provide a more complete and interpretable latent decomposition, aligning with clinically meaningful concepts. The results suggest that domain-confinement can address key limitations of broad-domain SAEs and highlight the importance of considering domain-specific training for enhanced model performance and interpretability. The findings challenge the notion of scaling foundation models for general-purpose SAEs. <br><br>Summary: <div>
arXiv:2508.09363v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations into latent features that reveal mechanistic structure. Conventional SAEs train on broad data distributions, forcing a fixed latent budget to capture only high-frequency, generic patterns. This often results in significant linear ``dark matter'' in reconstruction error and produces latents that fragment or absorb each other, complicating interpretation. We show that restricting SAE training to a well-defined domain (medical text) reallocates capacity to domain-specific features, improving both reconstruction fidelity and interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2 models using 195k clinical QA examples, we find that domain-confined SAEs explain up to 20\% more variance, achieve higher loss recovery, and reduce linear residual error compared to broad-domain SAEs. Automated and human evaluations confirm that learned features align with clinically meaningful concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather than frequent but uninformative tokens. These domain-specific SAEs capture relevant linear structure, leaving a smaller, more purely nonlinear residual. We conclude that domain-confinement mitigates key limitations of broad-domain SAEs, enabling more complete and interpretable latent decompositions, and suggesting the field may need to question ``foundation-model'' scaling for general-purpose SAEs.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Dementia Speech Alignment with Diffusion-Based Image Generation</title>
<link>https://arxiv.org/abs/2508.09385</link>
<guid>https://arxiv.org/abs/2508.09385</guid>
<content:encoded><![CDATA[
<div> Text-to-image models, dementia detection, alignment, pathological speech, generated images<br>
<br>
Summary: 
In this study, researchers investigated the alignment between pathological speech information related to dementia and generated images using text-to-image models. Surprisingly, they discovered that dementia detection could be achieved solely from the generated images with an impressive 75% accuracy rate on the ADReSS dataset. Additionally, the researchers developed methods to explain the alignment, shedding light on the specific language components that contribute to the detection. This finding demonstrates the potential of utilizing text-to-image models in the context of dementia-related speech analysis and highlights the effectiveness of explainability methods in revealing the underlying mechanisms of alignment between speech information and generated images. <div>
arXiv:2508.09385v1 Announce Type: new 
Abstract: Text-to-image models generate highly realistic images based on natural language descriptions and millions of users use them to create and share images online. While it is expected that such models can align input text and generated image in the same latent space little has been done to understand whether this alignment is possible between pathological speech and generated images. In this work, we examine the ability of such models to align dementia-related speech information with the generated images and develop methods to explain this alignment. Surprisingly, we found that dementia detection is possible from generated images alone achieving 75% accuracy on the ADReSS dataset. We then leverage explainability methods to show which parts of the language contribute to the detection.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment</title>
<link>https://arxiv.org/abs/2508.09399</link>
<guid>https://arxiv.org/abs/2508.09399</guid>
<content:encoded><![CDATA[
<div> Keywords: data privacy, collaborative modeling, federated learning, risk assessment, financial institutions

Summary:<br>
This paper presents a novel risk assessment framework for collaborative financial risk analysis across multiple institutions. The method utilizes federated learning, allowing institutions to jointly model and identify risks without sharing raw data. Using a feature attention mechanism and temporal modeling structure, the model employs distributed optimization to train local sub-models at each institution. Differential privacy and noise injection protect model parameters before aggregation by a central server to generate a global model for systemic risk identification. Experimental results demonstrate the model's superior performance in communication efficiency, model accuracy, systemic risk detection, and cross-market generalization compared to traditional centralized methods and existing federated learning variants. The proposed method enhances risk identification scope and efficiency while safeguarding data sovereignty, providing a secure and efficient solution for intelligent financial risk analysis. 

Summary: <div>
arXiv:2508.09399v1 Announce Type: new 
Abstract: This paper addresses the challenges of data privacy and collaborative modeling in cross-institution financial risk analysis. It proposes a risk assessment framework based on federated learning. Without sharing raw data, the method enables joint modeling and risk identification across multiple institutions. This is achieved by incorporating a feature attention mechanism and temporal modeling structure. Specifically, the model adopts a distributed optimization strategy. Each financial institution trains a local sub-model. The model parameters are protected using differential privacy and noise injection before being uploaded. A central server then aggregates these parameters to generate a global model. This global model is used for systemic risk identification. To validate the effectiveness of the proposed method, multiple experiments are conducted. These evaluate communication efficiency, model accuracy, systemic risk detection, and cross-market generalization. The results show that the proposed model outperforms both traditional centralized methods and existing federated learning variants across all evaluation metrics. It demonstrates strong modeling capabilities and practical value in sensitive financial environments. The method enhances the scope and efficiency of risk identification while preserving data sovereignty. It offers a secure and efficient solution for intelligent financial risk analysis.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery</title>
<link>https://arxiv.org/abs/2508.09401</link>
<guid>https://arxiv.org/abs/2508.09401</guid>
<content:encoded><![CDATA[
<div> Graph convolution, Transformer, anomaly detection, unsupervised learning, distributed systems
<br>
Summary:
This study introduces an unsupervised anomaly detection method for distributed backend service systems. The method utilizes a dynamic graph and graph convolution to extract high-order structural representations. A Transformer model captures the temporal behavior of each node, while a joint embedding mechanism integrates structural and behavioral features into an anomaly vector. Anomaly scores are computed using a nonlinear mapping, enabling end-to-end detection without supervision. Experiments on real-world data highlight the method's performance, showing superior expressiveness and stability in capturing anomaly propagation paths and modeling dynamic behavior sequences. The proposed approach outperforms existing models across different metrics, showcasing its potential for practical deployment. <div>
arXiv:2508.09401v1 Announce Type: new 
Abstract: This study proposes an unsupervised anomaly detection method for distributed backend service systems, addressing practical challenges such as complex structural dependencies, diverse behavioral evolution, and the absence of labeled data. The method constructs a dynamic graph based on service invocation relationships and applies graph convolution to extract high-order structural representations from multi-hop topologies. A Transformer is used to model the temporal behavior of each node, capturing long-term dependencies and local fluctuations. During the feature fusion stage, a learnable joint embedding mechanism integrates structural and behavioral representations into a unified anomaly vector. A nonlinear mapping is then applied to compute anomaly scores, enabling an end-to-end detection process without supervision. Experiments on real-world cloud monitoring data include sensitivity analyses across different graph depths, sequence lengths, and data perturbations. Results show that the proposed method outperforms existing models on several key metrics, demonstrating stronger expressiveness and stability in capturing anomaly propagation paths and modeling dynamic behavior sequences, with high potential for practical deployment.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Generalization to Improve Learning in Meta-Learning Algorithms</title>
<link>https://arxiv.org/abs/2508.09418</link>
<guid>https://arxiv.org/abs/2508.09418</guid>
<content:encoded><![CDATA[
<div> sharpness-aware minimization, meta-learning, generalization, few-shot learning, adaptation

Summary:
The paper introduces DGS-MAML, a novel meta-learning algorithm aimed at generalizing across tasks with limited training data. DGS-MAML combines gradient matching and sharpness-aware minimization in a bi-level optimization framework to enhance model adaptability and robustness. The method is supported by theoretical analysis using PAC-Bayes and convergence guarantees. Experimental results on benchmark datasets demonstrate that DGS-MAML outperforms existing approaches in terms of accuracy and generalization. The proposed algorithm is particularly beneficial for scenarios requiring few-shot learning and quick adaptation. The source code is publicly available on GitHub. This research contributes to advancing meta-learning techniques for improving model generalization and adaptability in scenarios with limited training data. <br><br>Summary: <div>
arXiv:2508.09418v1 Announce Type: new 
Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm designed to generalize across tasks with limited training data. DGS-MAML combines gradient matching with sharpness-aware minimization in a bi-level optimization framework to enhance model adaptability and robustness. We support our method with theoretical analysis using PAC-Bayes and convergence guarantees. Experimental results on benchmark datasets show that DGS-MAML outperforms existing approaches in terms of accuracy and generalization. The proposed method is particularly useful for scenarios requiring few-shot learning and quick adaptation, and the source code is publicly available at GitHub.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees</title>
<link>https://arxiv.org/abs/2508.09427</link>
<guid>https://arxiv.org/abs/2508.09427</guid>
<content:encoded><![CDATA[
<div> Implicit Hypergraph Neural Networks, higher-order relations, global propagation, stable training, efficient <br>
<br>
Summary: 
Implicit Hypergraph Neural Networks (IHGNN) introduce a novel approach to modeling higher-order relations in group-based interactions. Unlike traditional hypergraph neural networks with fixed message-passing layers, IHGNN computes representations through a nonlinear fixed-point equation, allowing for stable and efficient global propagation across hyperedges without deep architectures. A well-posed training scheme with provable convergence, analysis of oversmoothing conditions and model expressivity, as well as a transductive generalization bound on hypergraphs, are developed. The implicit-gradient training procedure and projection-based stabilization strategy enhance training robustness. Experimental results on citation benchmarks demonstrate that IHGNN outperforms traditional graph/hypergraph neural network baselines in accuracy and robustness, showing resilience to random initialization and hyperparameter variation. This highlights the strong generalization and practical value of IHGNN for higher-order relational learning. <br> <div>
arXiv:2508.09427v1 Announce Type: new 
Abstract: Many real-world interactions are group-based rather than pairwise such as papers with multiple co-authors and users jointly engaging with items. Hypergraph neural networks have shown great promise at modeling higher-order relations, but their reliance on a fixed number of explicit message-passing layers limits long-range dependency capture and can destabilize training as depth grows. In this work, we introduce Implicit Hypergraph Neural Networks (IHGNN), which bring the implicit equilibrium formulation to hypergraphs: instead of stacking layers, IHGNN computes representations as the solution to a nonlinear fixed-point equation, enabling stable and efficient global propagation across hyperedges without deep architectures. We develop a well-posed training scheme with provable convergence, analyze the oversmoothing conditions and expressivity of the model, and derive a transductive generalization bound on hypergraphs. We further present an implicit-gradient training procedure coupled with a projection-based stabilization strategy. Extensive experiments on citation benchmarks show that IHGNN consistently outperforms strong traditional graph/hypergraph neural network baselines in both accuracy and robustness. Empirically, IHGNN is resilient to random initialization and hyperparameter variation, highlighting its strong generalization and practical value for higher-order relational learning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)</title>
<link>https://arxiv.org/abs/2508.09447</link>
<guid>https://arxiv.org/abs/2508.09447</guid>
<content:encoded><![CDATA[

arXiv:2508.09447v1 Announce Type: new 
Abstract: Road traffic congestion is a persistent problem. Focusing resources on the causes of congestion is a potentially efficient strategy for reducing slowdowns. We present NEXICA, an algorithm to discover which parts of the highway system tend to cause slowdowns on other parts of the highway. We use time series of road speeds as inputs to our causal discovery algorithm. Finding other algorithms inadequate, we develop a new approach that is novel in three ways. First, it concentrates on just the presence or absence of events in the time series, where an event indicates the temporal beginning of a traffic slowdown. Second, we develop a probabilistic model using maximum likelihood estimation to compute the probabilities of spontaneous and caused slowdowns between two locations on the highway. Third, we train a binary classifier to identify pairs of cause/effect locations trained on pairs of road locations where we are reasonably certain a priori of their causal connections, both positive and negative. We test our approach on six months of road speed data from 195 different highway speed sensors in the Los Angeles area, showing that our approach is superior to state-of-the-art baselines in both accuracy and computation speed.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Contrastive-Generative Framework for Time Series Classification</title>
<link>https://arxiv.org/abs/2508.09451</link>
<guid>https://arxiv.org/abs/2508.09451</guid>
<content:encoded><![CDATA[

arXiv:2508.09451v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes two paradigms: contrastive methods that excel at instance discrimination and generative approaches that model data distributions. While effective individually, their complementary potential remains unexplored. We propose a Contrastive Generative Time series framework (CoGenT), the first framework to unify these paradigms through joint contrastive-generative optimization. CoGenT addresses fundamental limitations of both approaches: it overcomes contrastive learning's sensitivity to high intra-class similarity in temporal data while reducing generative methods' dependence on large datasets. We evaluate CoGenT on six diverse time series datasets. The results show consistent improvements, with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE, respectively. Our analysis reveals that the hybrid objective preserves discriminative power while acquiring generative robustness. These findings establish a foundation for hybrid SSL in temporal domains. We will release the code shortly.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation</title>
<link>https://arxiv.org/abs/2508.09462</link>
<guid>https://arxiv.org/abs/2508.09462</guid>
<content:encoded><![CDATA[

arXiv:2508.09462v1 Announce Type: new 
Abstract: A reliable fault diagnosis system should not only accurately classify known health states but also effectively identify unknown faults. In multimode processes, samples belonging to the same health state often show multiple cluster distributions, making it difficult to construct compact and accurate decision boundaries for that state. To address this challenge, a novel open-set fault diagnosis model named fine-grained clustering and rejection network (FGCRN) is proposed. It combines multiscale depthwise convolution, bidirectional gated recurrent unit and temporal attention mechanism to capture discriminative features. A distance-based loss function is designed to enhance the intra-class compactness. Fine-grained feature representations are constructed through unsupervised learning to uncover the intrinsic structures of each health state. Extreme value theory is employed to model the distance between sample features and their corresponding fine-grained representations, enabling effective identification of unknown faults. Extensive experiments demonstrate the superior performance of the proposed method.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation</title>
<link>https://arxiv.org/abs/2508.09467</link>
<guid>https://arxiv.org/abs/2508.09467</guid>
<content:encoded><![CDATA[

arXiv:2508.09467v1 Announce Type: new 
Abstract: Neural Architecture Search (NAS) automates the design of high-performing neural networks but typically targets a single predefined task, thereby restricting its real-world applicability. To address this, Meta Neural Architecture Search (Meta-NAS) has emerged as a promising paradigm that leverages prior knowledge across tasks to enable rapid adaptation to new ones. Nevertheless, existing Meta-NAS methods often struggle with poor generalization, limited search spaces, or high computational costs. In this paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS first models neural architectures as graphs, and then a hybrid search strategy is developed to find and generate new graphs that lead to promising neural architectures. The search strategy combines global architecture search via Bayesian Optimization in the search space with local exploration for novel neural networks via gradient ascent in the latent space. Such a hybrid search strategy allows GraB-NAS to discover task-aware architectures with strong performance, even beyond the predefined search space. Extensive experiments demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines, achieving better generalization and search effectiveness.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries</title>
<link>https://arxiv.org/abs/2508.09468</link>
<guid>https://arxiv.org/abs/2508.09468</guid>
<content:encoded><![CDATA[

arXiv:2508.09468v1 Announce Type: new 
Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across smart cities, industrial sites, and healthcare systems. They continuously generate time series data that enable advanced analytics and automation in industries. However, challenges such as the loss or ambiguity of sensor metadata, heterogeneity in data sources, varying sampling frequencies, inconsistent units of measurement, and irregular timestamps make raw IoT time series data difficult to interpret, undermining the effectiveness of smart systems. To address these challenges, we propose a novel deep learning model, DeepFeatIoT, which integrates learned local and global features with non-learned randomized convolutional kernel-based features and features from large language models (LLMs). This straightforward yet unique fusion of diverse learned and non-learned features significantly enhances IoT time series sensor data classification, even in scenarios with limited labeled data. Our model's effectiveness is demonstrated through its consistent and generalized performance across multiple real-world IoT sensor datasets from diverse critical application domains, outperforming state-of-the-art benchmark models. These results highlight DeepFeatIoT's potential to drive significant advancements in IoT analytics and support the development of next-generation smart systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models</title>
<link>https://arxiv.org/abs/2508.09471</link>
<guid>https://arxiv.org/abs/2508.09471</guid>
<content:encoded><![CDATA[

arXiv:2508.09471v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in size, the computational and memory challenges involved in deploying these massive foundation models have grown increasingly severe. This underscores the urgent need to develop more efficient model variants. Faced with this challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided Structured Post-training Pruning method. The proposed approach leverages graph theory to guide the design of N:M structured pruning, effectively reducing model size and computational demands. By incorporating concepts from expander graphs, EGGS-PTP ensures information flow within the pruned network, preserving essential model functionality. Extensive numerical experiments demonstrate that EGGS-PTP not only achieves significant acceleration and memory savings due to structured sparsity but also outperforms existing structured pruning techniques in terms of accuracy across various LLMs.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs</title>
<link>https://arxiv.org/abs/2508.09473</link>
<guid>https://arxiv.org/abs/2508.09473</guid>
<content:encoded><![CDATA[

arXiv:2508.09473v1 Announce Type: new 
Abstract: Ensuring robust safety alignment while preserving utility is critical for the reliable deployment of Large Language Models (LLMs). However, current techniques fundamentally suffer from intertwined deficiencies: insufficient robustness against malicious attacks, frequent refusal of benign queries, degradation in generated text quality and general task performance--the former two reflecting deficits in robust safety and the latter constituting utility impairment. We trace these limitations to the coarse-grained layer-wise interventions in existing methods. To resolve this, we propose NeuronTune, a fine-grained framework that dynamically modulates sparse neurons to achieve simultaneous safety-utility optimization. Our approach first identifies safety-critical and utility-preserving neurons across all layers via attribution, then employs meta-learning to adaptively amplify safety-neuron activations and suppress utility-neuron activations. Crucially, NeuronTune enables tunable adjustment of intervention scope via neuron-count thresholds, supporting flexible adaptation to security-critical or utility-priority scenarios. Extensive experimental results demonstrate that our method significantly outperforms existing state-of-the-art technologies, achieving superior model safety while maintaining excellent utility.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Small Model Collaborative Framework for Federated Continual Learning</title>
<link>https://arxiv.org/abs/2508.09489</link>
<guid>https://arxiv.org/abs/2508.09489</guid>
<content:encoded><![CDATA[

arXiv:2508.09489v1 Announce Type: new 
Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet underexplored challenge, especially in Federated Continual Learning (FCL), where each client learns from a private, evolving task stream under strict data and communication constraints. Despite their powerful generalization abilities, FMs often exhibit suboptimal performance on local downstream tasks, as they are unable to utilize private local data. Furthermore, enabling FMs to learn new tasks without forgetting prior knowledge is inherently a challenging problem, primarily due to their immense parameter count and high model complexity. In contrast, small models can be trained locally under resource-constrained conditions and benefit from more mature CL techniques. To bridge the gap between small models and FMs, we propose the first collaborative framework in FCL, where lightweight local models act as a dynamic bridge, continually adapting to new tasks while enhancing the utility of the large model. Two novel components are also included: Small Model Continual Fine-tuning is for preventing small models from temporal forgetting; One-by-One Distillation performs personalized fusion of heterogeneous local knowledge on the server. Experimental results demonstrate its superior performance, even when clients utilize heterogeneous small models.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI</title>
<link>https://arxiv.org/abs/2508.09500</link>
<guid>https://arxiv.org/abs/2508.09500</guid>
<content:encoded><![CDATA[

arXiv:2508.09500v1 Announce Type: new 
Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven promising in efficient storage and computation on edge devices. To further reduce the accuracy drop while increasing speedup, layer-wise mixed-precision quantization (MPQ) becomes a popular solution. However, existing algorithms for exploring MPQ schemes are limited in flexibility and efficiency. Comprehending the complex impacts of different MPQ schemes on post-training quantization and quantization-aware training results is a challenge for conventional methods. Furthermore, an end-to-end framework for the optimization and deployment of MPQ models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and deployment framework for edge AI applications. The framework adopts a novel optimization algorithm to search for optimal quantization schemes with the highest accuracies while meeting latency constraints. Hardware-aware latency models are built for different hardware targets to enable fast explorations. After the exploration, the framework enables direct deployment from PyTorch MPQ models to bare-metal C codes, leading to end-to-end speedup with minimal accuracy drops.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2508.09504</link>
<guid>https://arxiv.org/abs/2508.09504</guid>
<content:encoded><![CDATA[

arXiv:2508.09504v1 Announce Type: new 
Abstract: With the growing complexity of cyberattacks targeting critical infrastructures such as water treatment networks, there is a pressing need for robust anomaly detection strategies that account for both system vulnerabilities and evolving attack patterns. Traditional methods -- statistical, density-based, and graph-based models struggle with distribution shifts and class imbalance in multivariate time series, often leading to high false positive rates. To address these challenges, we propose CGAD, a Causal Graph-based Anomaly Detection framework designed for reliable cyberattack detection in public infrastructure systems. CGAD follows a two-phase supervised framework -- causal profiling and anomaly scoring. First, it learns causal invariant graph structures representing the system's behavior under "Normal" and "Attack" states using Dynamic Bayesian Networks. Second, it employs structural divergence to detect anomalies via causal graph comparison by evaluating topological deviations in causal graphs over time. By leveraging causal structures, CGAD achieves superior adaptability and accuracy in non-stationary and imbalanced time series environments compared to conventional machine learning approaches. By uncovering causal structures beneath volatile sensor data, our framework not only detects cyberattacks with markedly higher precision but also redefines robustness in anomaly detection, proving resilience where traditional models falter under imbalance and drift. Our framework achieves substantial gains in F1 and ROC-AUC scores over best-performing baselines across four industrial datasets, demonstrating robust detection of delayed and structurally complex anomalies.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach</title>
<link>https://arxiv.org/abs/2508.09510</link>
<guid>https://arxiv.org/abs/2508.09510</guid>
<content:encoded><![CDATA[

arXiv:2508.09510v1 Announce Type: new 
Abstract: Despite the significant advancements in Large Language Models (LLMs), catastrophic forgetting remains a substantial challenge, where models lose previously acquired knowledge upon learning new information. Continual learning (CL) strategies have emerged as a potential solution to this problem, with replay-based techniques demonstrating superior performance in preserving learned knowledge. In this context, we introduce Gauss-Tin, a novel approach that integrates the replay strategy with a Gaussian mixture model to enhance the quality of sample selection during training, supplemented by instructional guidance to facilitate the generation of past learning. This method aims to improve LLMs' retention capabilities by strategically reinforcing important past learnings while accommodating new information. Our experimental results indicate a promising 6\% improvement in retention metrics over traditional methods, suggesting that Gauss-Tin is an effective strategy for mitigating catastrophic forgetting in LLMs. This study underscores the potential of hybrid models in enhancing the robustness and adaptability of LLMs in dynamic learning environments.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring</title>
<link>https://arxiv.org/abs/2508.09527</link>
<guid>https://arxiv.org/abs/2508.09527</guid>
<content:encoded><![CDATA[

arXiv:2508.09527v1 Announce Type: new 
Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events in ongoing cases based on historical event logs. While Graph Neural Networks (GNNs) are well suited to capture structural dependencies in process data, existing GNN-based PBPM models remain underdeveloped. Most rely either on short prefix subgraphs or global architectures that overlook temporal relevance and transition semantics. We propose a unified, interpretable GNN framework that advances the state of the art along three key axes. First, we compare prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention Networks(GATs) to quantify the performance gap between localized and global modeling. Second, we introduce a novel time decay attention mechanism that constructs dynamic, prediction-centered windows, emphasizing temporally relevant history and suppressing noise. Third, we embed transition type semantics into edge features to enable fine grained reasoning over structurally ambiguous traces. Our architecture includes multilevel interpretability modules, offering diverse visualizations of attention behavior. Evaluated on five benchmarks, the proposed models achieve competitive Top-k accuracy and DL scores without per-dataset tuning. By addressing architectural, temporal, and semantic gaps, this work presents a robust, generalizable, and explainable solution for next event prediction in PBPM.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks</title>
<link>https://arxiv.org/abs/2508.09532</link>
<guid>https://arxiv.org/abs/2508.09532</guid>
<content:encoded><![CDATA[

arXiv:2508.09532v1 Announce Type: new 
Abstract: Federated fine-tuning has emerged as a promising approach for adapting foundation models (FMs) to diverse downstream tasks in edge environments. In Internet of Vehicles (IoV) systems, enabling efficient and low-latency multi-task adaptation is particularly challenging due to client mobility, heterogeneous resources, and intermittent connectivity. This paper proposes a hierarchical federated fine-tuning framework that coordinates roadside units (RSUs) and vehicles to support resource-aware and mobility-resilient learning across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we introduce a decentralized, energy-aware rank adaptation mechanism formulated as a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is developed to enable adaptive exploration under per-task energy budgets, achieving provable sublinear regret. To evaluate our method, we construct a large-scale IoV simulator based on real-world trajectories, capturing dynamic participation, RSU handoffs, and communication variability. Extensive experiments show that our approach achieves the best accuracy-efficiency trade-off among all baselines, reducing latency by over 24\% and improving average accuracy by more than 2.5\%.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification</title>
<link>https://arxiv.org/abs/2508.09544</link>
<guid>https://arxiv.org/abs/2508.09544</guid>
<content:encoded><![CDATA[

arXiv:2508.09544v1 Announce Type: new 
Abstract: Scarcity of labeled data, especially for rare events, hinders training effective machine learning models. This paper proposes SYNAPSE-G (Synthetic Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline leveraging Large Language Models (LLMs) to generate synthetic training data for rare event classification, addressing the cold-start problem. This synthetic data serve as seeds for semi-supervised label propagation on a similarity graph constructed between the seeds and a large unlabeled dataset. This identifies candidate positive examples, subsequently labeled by an oracle (human or LLM). The expanded dataset then trains/fine-tunes a classifier. We theoretically analyze how the quality (validity and diversity) of the synthetic data impacts the precision and recall of our method. Experiments on the imbalanced SST2 and MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels, outperforming baselines including nearest neighbor search.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges</title>
<link>https://arxiv.org/abs/2508.09561</link>
<guid>https://arxiv.org/abs/2508.09561</guid>
<content:encoded><![CDATA[

arXiv:2508.09561v1 Announce Type: new 
Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge computing, where distributed agents possess the capability to perceive, reason, and act autonomously across diverse, dynamic environments. Central to this vision are world models, which act as proactive internal simulators that not only predict but also actively imagine future trajectories, reason under uncertainty, and plan multi-step actions with foresight. This proactive nature allows agents to anticipate potential outcomes and optimize decisions ahead of real-world interactions. While prior works in robotics and gaming have showcased the potential of world models, their integration into the wireless edge for EGI remains underexplored. This survey bridges this gap by offering a comprehensive analysis of how world models can empower agentic artificial intelligence (AI) systems at the edge. We first examine the architectural foundations of world models, including latent representation learning, dynamics modeling, and imagination-based planning. Building on these core capabilities, we illustrate their proactive applications across EGI scenarios such as vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of Things (IoT) systems, and network functions virtualization, thereby highlighting how they can enhance optimization under latency, energy, and privacy constraints. We then explore their synergy with foundation models and digital twins, positioning world models as the cognitive backbone of EGI. Finally, we highlight open challenges, such as safety guarantees, efficient training, and constrained deployment, and outline future research directions. This survey provides both a conceptual foundation and a practical roadmap for realizing the next generation of intelligent, autonomous edge systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Prediction with Limited Selectivity</title>
<link>https://arxiv.org/abs/2508.09592</link>
<guid>https://arxiv.org/abs/2508.09592</guid>
<content:encoded><![CDATA[

arXiv:2508.09592v1 Announce Type: new 
Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster freely decides on the prediction window that their forecast spans. Many data statistics can be predicted to a non-trivial error rate without any distributional assumptions or expert advice, yet these results rely on that the forecaster may predict at any time. We introduce a model of Prediction with Limited Selectivity (PLS) where the forecaster can start the prediction only on a subset of the time horizon. We study the optimal prediction error both on an instance-by-instance basis and via an average-case analysis. We introduce a complexity measure that gives instance-dependent bounds on the optimal error. For a randomly-generated PLS instance, these bounds match with high probability.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Goal Discovery with Causal Capacity for Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.09624</link>
<guid>https://arxiv.org/abs/2508.09624</guid>
<content:encoded><![CDATA[

arXiv:2508.09624v1 Announce Type: new 
Abstract: Causal inference is crucial for humans to explore the world, which can be modeled to enable an agent to efficiently explore the environment in reinforcement learning. Existing research indicates that establishing the causality between action and state transition will enhance an agent to reason how a policy affects its future trajectory, thereby promoting directed exploration. However, it is challenging to measure the causality due to its intractability in the vast state-action space of complex scenarios. In this paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework for efficient environment exploration. Specifically, we first derive a measurement of causality in state space, \emph{i.e.,} causal capacity, which represents the highest influence of an agent's behavior on future trajectories. After that, we present a Monte Carlo based method to identify critical points in discrete state space and further optimize this method for continuous high-dimensional environments. Those critical points are used to uncover where the agent makes important decisions in the environment, which are then regarded as our subgoals to guide the agent to make exploration more purposefully and efficiently. Empirical results from multi-objective tasks demonstrate that states with high causal capacity align with our expected subgoals, and our GDCC achieves significant success rate improvements compared to baselines.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs</title>
<link>https://arxiv.org/abs/2508.09627</link>
<guid>https://arxiv.org/abs/2508.09627</guid>
<content:encoded><![CDATA[

arXiv:2508.09627v1 Announce Type: new 
Abstract: Solving partial differential equations (PDEs) efficiently and accurately remains a cornerstone challenge in science and engineering, especially for problems involving complex geometries and limited labeled data. We introduce a Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator ($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and time-dependent PDEs. The proposed approach first improves upon the recently developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits the governing physics to learn the underlying solution operator in a simulation-free setup. While the spatio-spectral structure present in the proposed architecture allows multiscale learning, two separate strategies for enabling geometry awareness is introduced in this paper. For time dependent problems, we also introduce a novel hybrid physics informed loss function that combines higher-order time-marching scheme with upscaled theory inspired stochastic projection scheme. This allows accurate integration of the physics-information into the loss function. The performance of the proposed approach is illustrated on number of benchmark examples involving regular and complex domains, variation in geometry during inference, and time-independent and time-dependent problems. The results obtained illustrate the efficacy of the proposed approach as compared to the state-of-the-art physics-informed neural operator algorithms in the literature.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling</title>
<link>https://arxiv.org/abs/2508.09630</link>
<guid>https://arxiv.org/abs/2508.09630</guid>
<content:encoded><![CDATA[

arXiv:2508.09630v1 Announce Type: new 
Abstract: Multivariate time series data typically comprises two distinct modalities: variable semantics and sampled numerical observations. Traditional time series models treat variables as anonymous statistical signals, overlooking the rich semantic information embedded in variable names and data descriptions. However, these textual descriptors often encode critical domain knowledge that is essential for robust and interpretable modeling. Here we present TimeMKG, a multimodal causal reasoning framework that elevates time series modeling from low-level signal processing to knowledge informed inference. TimeMKG employs large language models to interpret variable semantics and constructs structured Multivariate Knowledge Graphs that capture inter-variable relationships. A dual-modality encoder separately models the semantic prompts, generated from knowledge graph triplets, and the statistical patterns from historical time series. Cross-modality attention aligns and fuses these representations at the variable level, injecting causal priors into downstream tasks such as forecasting and classification, providing explicit and interpretable priors to guide model reasoning. The experiment in diverse datasets demonstrates that incorporating variable-level knowledge significantly improves both predictive performance and generalization.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments</title>
<link>https://arxiv.org/abs/2508.09659</link>
<guid>https://arxiv.org/abs/2508.09659</guid>
<content:encoded><![CDATA[

arXiv:2508.09659v1 Announce Type: new 
Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein thermal stability data that overcomes key limitations of existing thermal proteome profiling (TPP) work-flows. Unlike standard approaches that assume sigmoidal melting curves and are constrained by empirical null distributions (limiting significant hits to approximately 5 % of data), Thermal Tracks uses Gaussian Process (GP) models with squared-exponential kernels to flexibly model any melting curve shape while generating unbiased null distributions through kernel priors. This framework is particularly valuable for analyzing proteome-wide perturbations that significantly alter protein thermal stability, such as pathway inhibitions, genetic modifications, or environmental stresses, where conventional TPP methods may miss biologically relevant changes due to their statistical constraints. Furthermore, Thermal Tracks excels at analyzing proteins with un-conventional melting profiles, including phase-separating proteins and membrane proteins, which often exhibit complex, non-sigmoidal thermal stability behaviors. Thermal Tracks is freely available from GitHub and is implemented in Python, providing an accessible and flexible tool for proteome-wide thermal profiling studies.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Convergence Analysis of Vanilla Gradient Descent for Asymmetric Matrix Completion</title>
<link>https://arxiv.org/abs/2508.09685</link>
<guid>https://arxiv.org/abs/2508.09685</guid>
<content:encoded><![CDATA[

arXiv:2508.09685v1 Announce Type: new 
Abstract: This paper investigates the asymmetric low-rank matrix completion problem, which can be formulated as an unconstrained non-convex optimization problem with a nonlinear least-squares objective function, and is solved via gradient descent methods. Previous gradient descent approaches typically incorporate regularization terms into the objective function to guarantee convergence. However, numerical experiments and theoretical analysis of the gradient flow both demonstrate that the elimination of regularization terms in gradient descent algorithms does not adversely affect convergence performance. By introducing the leave-one-out technique, we inductively prove that the vanilla gradient descent with spectral initialization achieves a linear convergence rate with high probability. Besides, we demonstrate that the balancing regularization term exhibits a small norm during iterations, which reveals the implicit regularization property of gradient descent. Empirical results show that our algorithm has a lower computational cost while maintaining comparable completion performance compared to other gradient descent algorithms.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture</title>
<link>https://arxiv.org/abs/2508.09693</link>
<guid>https://arxiv.org/abs/2508.09693</guid>
<content:encoded><![CDATA[

arXiv:2508.09693v1 Announce Type: new 
Abstract: We develop an operator-theoretic framework for temporal anchoring in embedding spaces, modeled as drift maps interleaved with event-indexed blocks culminating in affine projections. We provide complete proofs for a variable-block contraction lemma (products of Lipschitz factors), a drift--projection convergence theorem with explicit uniform-gap envelopes, and ontological convergence under nested affine anchors with a robustness variant. We formalize an internal Manuscript Computer (MC) whose computations are defined purely by these operators and prove a rigorous finite-run equivalence theorem (with perturbation bounds). For attention layers, we give a self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All floats are placed exactly where written; the manuscript uses only in-paper pseudocode and appendix figures.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Noisy Labels via Dynamic Connection Masking</title>
<link>https://arxiv.org/abs/2508.09697</link>
<guid>https://arxiv.org/abs/2508.09697</guid>
<content:encoded><![CDATA[

arXiv:2508.09697v1 Announce Type: new 
Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong capacity of deep neural networks to memorize corrupted labels, these noisy labels can cause significant performance degradation. Existing research on mitigating the negative effects of noisy labels has mainly focused on robust loss functions and sample selection, with comparatively limited exploration of regularization in model architecture. Inspired by the sparsity regularization used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and KANs to enhance the robustness of classifiers against noisy labels. The mechanism can adaptively mask less important edges during training by evaluating their information-carrying capacity. Through theoretical analysis, we demonstrate its efficiency in reducing gradient error. Our approach can be seamlessly integrated into various noise-robust training methods to build more robust deep networks, including robust loss functions, sample selection strategies, and regularization techniques. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our method consistently outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the first to investigate KANs as classifiers against noisy labels, revealing their superior noise robustness over MLPs in real-world noisy scenarios. Our code will soon be publicly available.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation</title>
<link>https://arxiv.org/abs/2508.09710</link>
<guid>https://arxiv.org/abs/2508.09710</guid>
<content:encoded><![CDATA[

arXiv:2508.09710v1 Announce Type: new 
Abstract: Brain connectomes, representing neural connectivity as graphs, are crucial for understanding brain organization but costly and time-consuming to acquire, motivating generative approaches. Recent advances in graph generative modeling offer a data-driven alternative, enabling synthetic connectome generation and reducing dependence on large neuroimaging datasets. However, current models face key limitations: (i) compressing the whole graph into a single latent code (e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node attributes rarely available in connectomes reduces reconstruction quality; (iii) edge-centric models emphasize topology but overlook accurate edge-weight prediction, harming quantitative fidelity; and (iv) computationally expensive designs (e.g., edge-conditioned convolutions) impose high memory demands, limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric generative framework for efficient, accurate connectome synthesis. GTG decomposes each connectome into entropy-guided k-hop trees capturing informative local structure, encoded by a shared GCN. A bipartite message-passing layer fuses subtree embeddings with global node features, while a dual-branch decoder jointly predicts edge existence and weights to reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in self-supervised tasks and remains competitive in supervised settings, delivering higher structural fidelity and more precise weights with far less memory. Its modular design enables extensions to connectome super-resolution and cross-modality synthesis. Code: https://github.com/basiralab/GTG/
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2508.09719</link>
<guid>https://arxiv.org/abs/2508.09719</guid>
<content:encoded><![CDATA[

arXiv:2508.09719v1 Announce Type: new 
Abstract: Large, publicly available clinical datasets have emerged as a novel resource for understanding disease heterogeneity and to explore personalization of therapy. These datasets are derived from data not originally collected for research purposes and, as a result, are often incomplete and lack critical labels. Many AI tools have been developed to retrospectively label these datasets, such as by performing disease classification; however, they often suffer from limited interpretability. Previous work has attempted to explain predictions using Concept Bottleneck Models (CBMs), which learn interpretable concepts that map to higher-level clinical ideas, facilitating human evaluation. However, these models often experience performance limitations when the concepts fail to adequately explain or characterize the task. We use the identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging test case to demonstrate the value of incorporating contextual information from clinical notes to improve CBM performance. Our approach leverages a Large Language Model (LLM) to process clinical notes and generate additional concepts, resulting in a 10% performance gain over existing methods. Additionally, it facilitates the learning of more comprehensive concepts, thereby reducing the risk of information leakage and reliance on spurious shortcuts, thus improving the characterization of ARDS.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization</title>
<link>https://arxiv.org/abs/2508.09730</link>
<guid>https://arxiv.org/abs/2508.09730</guid>
<content:encoded><![CDATA[

arXiv:2508.09730v1 Announce Type: new 
Abstract: In e-commerce advertising, selecting the most compelling combination of creative elements -- such as titles, images, and highlights -- is critical for capturing user attention and driving conversions. However, existing methods often evaluate creative components individually, failing to navigate the exponentially large search space of possible combinations. To address this challenge, we propose a novel framework named GenCO that integrates generative modeling with multi-instance reward learning. Our unified two-stage architecture first employs a generative model to efficiently produce a diverse set of creative combinations. This generative process is optimized with reinforcement learning, enabling the model to effectively explore and refine its selections. Next, to overcome the challenge of sparse user feedback, a multi-instance learning model attributes combination-level rewards, such as clicks, to the individual creative elements. This allows the reward model to provide a more accurate feedback signal, which in turn guides the generative model toward creating more effective combinations. Deployed on a leading e-commerce platform, our approach has significantly increased advertising revenue, demonstrating its practical value. Additionally, we are releasing a large-scale industrial dataset to facilitate further research in this important domain.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks</title>
<link>https://arxiv.org/abs/2508.09743</link>
<guid>https://arxiv.org/abs/2508.09743</guid>
<content:encoded><![CDATA[

arXiv:2508.09743v1 Announce Type: new 
Abstract: A prevailing trend in neural network research suggests that model performance improves with increasing depth and capacity - often at the cost of integrability and efficiency. In this paper, we propose a strategy to optimize small, deployable models by enhancing their capabilities through structured knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a biologically inspired framework for modular and selective transfer of task-relevant features from a larger, pretrained parent network to a smaller child model. Unlike standard knowledge distillation, which enforces uniform imitation of teacher outputs, HKT draws inspiration from biological inheritance mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage process of feature transfer. Neural network blocks are treated as functional carriers, and knowledge is transmitted through three biologically motivated components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention (GA) mechanism governs the integration of inherited and native representations, ensuring both alignment and selectivity. We evaluate HKT across diverse vision tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10), and semantic segmentation (LiTS), demonstrating that it significantly improves child model performance while preserving its compactness. The results show that HKT consistently outperforms conventional distillation approaches, offering a general-purpose, interpretable, and scalable solution for deploying high-performance neural networks in resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers</title>
<link>https://arxiv.org/abs/2508.09747</link>
<guid>https://arxiv.org/abs/2508.09747</guid>
<content:encoded><![CDATA[

arXiv:2508.09747v1 Announce Type: new 
Abstract: Predicting an individual's aging trajectory is a central challenge in preventative medicine and bioinformatics. While machine learning models can predict chronological age from biomarkers, they often fail to capture the dynamic, longitudinal nature of the aging process. In this work, we developed and validated a machine learning pipeline to predict age using a longitudinal cohort with data from two distinct time periods (2019-2020 and 2021-2022). We demonstrate that a model using only static, cross-sectional biomarkers has limited predictive power when generalizing to future time points. However, by engineering novel features that explicitly capture the rate of change (slope) of key biomarkers over time, we significantly improved model performance. Our final LightGBM model, trained on the initial wave of data, successfully predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for males, $R^2 = 0.498$ for females), significantly outperforming both traditional linear models and other tree-based ensembles. SHAP analysis of our successful model revealed that the engineered slope features were among the most important predictors, highlighting that an individual's health trajectory, not just their static health snapshot, is a key determinant of biological age. Our framework paves the way for clinical tools that dynamically track patient health trajectories, enabling early intervention and personalized prevention strategies for age-related diseases.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\mu$-Parametrization for Mixture of Experts</title>
<link>https://arxiv.org/abs/2508.09752</link>
<guid>https://arxiv.org/abs/2508.09752</guid>
<content:encoded><![CDATA[

arXiv:2508.09752v1 Announce Type: new 
Abstract: Recent years have seen a growing interest and adoption of LLMs, with $\mu$Transfer becoming a key technique for tuning hyperparameters in large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a leading architecture in extremely large models. However, the intersection of these two advancements has remained unexplored. In this work, we derive a $\mu$-Parameterization ($\mu$P) for MoE, providing theoretical guarantees for feature learning across model widths in both the router and experts. We empirically validate our parameterization and further investigate how scaling the number of experts and granularity affects the optimal learning rate.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriForecaster: A Mixture of Experts Framework for Multi-Region Electric Load Forecasting with Tri-dimensional Specialization</title>
<link>https://arxiv.org/abs/2508.09753</link>
<guid>https://arxiv.org/abs/2508.09753</guid>
<content:encoded><![CDATA[

arXiv:2508.09753v1 Announce Type: new 
Abstract: Electric load forecasting is pivotal for power system operation, planning and decision-making. The rise of smart grids and meters has provided more detailed and high-quality load data at multiple levels of granularity, from home to bus and cities. Motivated by similar patterns of loads across different cities in a province in eastern China, in this paper we focus on the Multi-Region Electric Load Forecasting (MRELF) problem, targeting accurate short-term load forecasting for multiple sub-regions within a large region. We identify three challenges for MRELF, including regional variation, contextual variation, and temporal variation. To address them, we propose TriForecaster, a new framework leveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning (MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer and Context-Time Specializer (CTSpecializer) layers, enabling dynamic cooperation and specialization of expert models across regional, contextual, and temporal dimensions. Based on evaluation on four real-world MRELF datasets with varied granularity, TriForecaster outperforms state-of-the-art models by achieving an average forecast error reduction of 22.4\%, thereby demonstrating its flexibility and broad applicability. In particular, the deployment of TriForecaster on the eForecaster platform in eastern China exemplifies its practical utility, effectively providing city-level, short-term load forecasts for 17 cities, supporting a population exceeding 110 million and daily electricity usage over 100 gigawatt-hours.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations</title>
<link>https://arxiv.org/abs/2508.09787</link>
<guid>https://arxiv.org/abs/2508.09787</guid>
<content:encoded><![CDATA[

arXiv:2508.09787v1 Announce Type: new 
Abstract: We present Proto-PINV+H, a fast training paradigm that combines closed-form weight computation with gradient-based optimisation of a small set of synthetic inputs, soft labels, and-crucially-hidden activations. At each iteration we recompute all weight matrices in closed form via two (or more) ridge-regularised pseudo-inverse solves, while updating only the prototypes with Adam. The trainable degrees of freedom are thus shifted from weight space to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a multi-layer extension (optimised activations at each hidden stage), learnable ridge parameters, optional PCA/PLS projections, and theory linking the condition number of prototype matrices to generalisation. The approach yields favourable accuracy--speed--size trade-offs against ELM, random-feature ridge, and shallow MLPs trained by back-propagation.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian autoregression to optimize temporal Mat\'ern kernel Gaussian process hyperparameters</title>
<link>https://arxiv.org/abs/2508.09792</link>
<guid>https://arxiv.org/abs/2508.09792</guid>
<content:encoded><![CDATA[

arXiv:2508.09792v1 Announce Type: new 
Abstract: Gaussian processes are important models in the field of probabilistic numerics. We present a procedure for optimizing Mat\'ern kernel temporal Gaussian processes with respect to the kernel covariance function's hyperparameters. It is based on casting the optimization problem as a recursive Bayesian estimation procedure for the parameters of an autoregressive model. We demonstrate that the proposed procedure outperforms maximizing the marginal likelihood as well as Hamiltonian Monte Carlo sampling, both in terms of runtime and ultimate root mean square error in Gaussian process regression.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques</title>
<link>https://arxiv.org/abs/2508.09810</link>
<guid>https://arxiv.org/abs/2508.09810</guid>
<content:encoded><![CDATA[

arXiv:2508.09810v1 Announce Type: new 
Abstract: Biomechanical features have become important indicators for evaluating athletes' techniques. Traditionally, experts propose significant features and evaluate them using physics equations. However, the complexity of the human body and its movements makes it challenging to explicitly analyze the relationships between some features and athletes' final performance. With advancements in modern machine learning and statistics, data analytics methods have gained increasing importance in sports analytics. In this study, we leverage machine learning models to analyze expert-proposed biomechanical features from the finals of long jump competitions in the World Championships. The objectives of the analysis include identifying the most important features contributing to top-performing jumps and exploring the combined effects of these key features. Using quantile regression, we model the relationship between the biomechanical feature set and the target variable (effective distance), with a particular focus on elite-level jumps. To interpret the model, we apply SHapley Additive exPlanations (SHAP) alongside Partial Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The findings reveal that, beyond the well-documented velocity-related features, specific technical aspects also play a pivotal role. For male athletes, the angle of the knee of the supporting leg before take-off is identified as a key factor for achieving top 10% performance in our dataset, with angles greater than 169{\deg}contributing significantly to jump performance. In contrast, for female athletes, the landing pose and approach step technique emerge as the most critical features influencing top 10% performances, alongside velocity. This study establishes a framework for analyzing the impact of various features on athletic performance, with a particular emphasis on top-performing events.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable In-Context Vector Arithmetic via Retrieving Task Concepts</title>
<link>https://arxiv.org/abs/2508.09820</link>
<guid>https://arxiv.org/abs/2508.09820</guid>
<content:encoded><![CDATA[

arXiv:2508.09820v1 Announce Type: new 
Abstract: In-context learning (ICL) has garnered significant attention for its ability to grasp functions/tasks from demonstrations. Recent studies suggest the presence of a latent task/function vector in LLMs during ICL. Merullo et al. (2024) showed that LLMs leverage this vector alongside the residual stream for Word2Vec-like vector arithmetic, solving factual-recall ICL tasks. Additionally, recent work empirically highlighted the key role of Question-Answer data in enhancing factual-recall capabilities. Despite these insights, a theoretical explanation remains elusive. To move one step forward, we propose a theoretical framework building on empirically grounded hierarchical concept modeling. We develop an optimization theory, showing how nonlinear residual transformers trained via gradient descent on cross-entropy loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss convergence and show the strong generalization, including robustness to concept recombination and distribution shifts. These results elucidate the advantages of transformers over static embedding predecessors. Empirical simulations corroborate our theoretical insights.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RankList -- A Listwise Preference Learning Framework for Predicting Subjective Preferences</title>
<link>https://arxiv.org/abs/2508.09826</link>
<guid>https://arxiv.org/abs/2508.09826</guid>
<content:encoded><![CDATA[

arXiv:2508.09826v1 Announce Type: new 
Abstract: Preference learning has gained significant attention in tasks involving subjective human judgments, such as \emph{speech emotion recognition} (SER) and image aesthetic assessment. While pairwise frameworks such as RankNet offer robust modeling of relative preferences, they are inherently limited to local comparisons and struggle to capture global ranking consistency. To address these limitations, we propose RankList, a novel listwise preference learning framework that generalizes RankNet to structured list-level supervision. Our formulation explicitly models local and non-local ranking constraints within a probabilistic framework. The paper introduces a log-sum-exp approximation to improve training efficiency. We further extend RankList with skip-wise comparisons, enabling progressive exposure to complex list structures and enhancing global ranking fidelity. Extensive experiments demonstrate the superiority of our method across diverse modalities. On benchmark SER datasets (MSP-Podcast, IEMOCAP, BIIC Podcast), RankList achieves consistent improvements in Kendall's Tau and ranking accuracy compared to standard listwise baselines. We also validate our approach on aesthetic image ranking using the Artistic Image Aesthetics dataset, highlighting its broad applicability. Through ablation and cross-domain studies, we show that RankList not only improves in-domain ranking but also generalizes better across datasets. Our framework offers a unified, extensible approach for modeling ordered preferences in subjective learning scenarios.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness</title>
<link>https://arxiv.org/abs/2508.09866</link>
<guid>https://arxiv.org/abs/2508.09866</guid>
<content:encoded><![CDATA[

arXiv:2508.09866v1 Announce Type: new 
Abstract: To protect clients' right to be forgotten in federated learning, federated unlearning aims to remove the data contribution of leaving clients from the global learned model. While current studies mainly focused on enhancing unlearning efficiency and effectiveness, the crucial aspects of efficiency fairness and performance fairness among decentralized clients during unlearning have remained largely unexplored. In this study, we introduce FedShard, the first federated unlearning algorithm designed to concurrently guarantee both efficiency fairness and performance fairness. FedShard adaptively addresses the challenges introduced by dilemmas among convergence, unlearning efficiency, and unlearning fairness. Furthermore, we propose two novel metrics to quantitatively assess the fairness of unlearning algorithms, which we prove to satisfy well-known properties in other existing fairness measurements. Our theoretical analysis and numerical evaluation validate FedShard's fairness in terms of both unlearning performance and efficiency. We demonstrate that FedShard mitigates unfairness risks such as cascaded leaving and poisoning attacks and realizes more balanced unlearning costs among clients. Experimental results indicate that FedShard accelerates the data unlearning process 1.3-6.2 times faster than retraining from scratch and 4.9 times faster than the state-of-the-art exact unlearning methods.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning</title>
<link>https://arxiv.org/abs/2508.09883</link>
<guid>https://arxiv.org/abs/2508.09883</guid>
<content:encoded><![CDATA[

arXiv:2508.09883v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities in tasks such as algorithmic coding and mathematical problem-solving. Recent methods have improved reasoning through expanded corpus and multistage training combining reinforcement learning and supervised fine-tuning. Although some methods suggest that small but targeted dataset can incentivize reasoning via only distillation, a reasoning scaling laws is still taking shape, increasing computational costs. To address this, we propose a data-efficient distillation framework (DED) that optimizes the Pareto frontier of reasoning distillation. Inspired by the on-policy learning and diverse roll-out strategies of reinforcement learning, the key idea of our approach is threefold: (1) We identify that benchmark scores alone do not determine an effective teacher model. Through comprehensive comparisons of leading reasoning LLMs, we develop a method to select an optimal teacher model. (2) While scaling distillation can enhance reasoning, it often degrades out-of-domain performance. A carefully curated, smaller corpus achieves a balanced trade-off between in-domain and out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the student model to develop robust reasoning skills. We validate our method through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and code generation (LiveCodeBench), achieving state-of-the-art results with only 0.8k carefully curated examples, bypassing the need for extensive scaling. Our systematic analysis demonstrates that DED outperforms existing methods by considering factors beyond superficial hardness, token length, or teacher model capability. This work offers a practical and efficient pathway to advanced reasoning while preserving general capabilities.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?</title>
<link>https://arxiv.org/abs/2508.09888</link>
<guid>https://arxiv.org/abs/2508.09888</guid>
<content:encoded><![CDATA[

arXiv:2508.09888v1 Announce Type: new 
Abstract: In the field of pedometrics, tabular machine learning is the predominant method for predicting soil properties from remote and proximal soil sensing data, forming a central component of digital soil mapping. At the field-scale, this predictive soil modeling (PSM) task is typically constrained by small training sample sizes and high feature-to-sample ratios in soil spectroscopy. Traditionally, these conditions have proven challenging for conventional deep learning methods. Classical machine learning algorithms, particularly tree-based models like Random Forest and linear models such as Partial Least Squares Regression, have long been the default choice for field-scale PSM. Recent advances in artificial neural networks (ANN) for tabular data challenge this view, yet their suitability for field-scale PSM has not been proven. We introduce a comprehensive benchmark that evaluates state-of-the-art ANN architectures, including the latest multilayer perceptron (MLP)-based models (TabM, RealMLP), attention-based transformer variants (FT-Transformer, ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR, ModernNCA), and an in-context learning foundation model (TabPFN). Our evaluation encompasses 31 field- and farm-scale datasets containing 30 to 460 samples and three critical soil properties: soil organic matter or soil organic carbon, pH, and clay content. Our results reveal that modern ANNs consistently outperform classical methods on the majority of tasks, demonstrating that deep learning has matured sufficiently to overcome the long-standing dominance of classical machine learning for PSM. Notably, TabPFN delivers the strongest overall performance, showing robustness across varying conditions. We therefore recommend the adoption of modern ANNs for field-scale PSM and propose TabPFN as the new default choice in the toolkit of every pedometrician.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rare anomalies require large datasets: About proving the existence of anomalies</title>
<link>https://arxiv.org/abs/2508.09894</link>
<guid>https://arxiv.org/abs/2508.09894</guid>
<content:encoded><![CDATA[

arXiv:2508.09894v1 Announce Type: new 
Abstract: Detecting whether any anomalies exist within a dataset is crucial for effective anomaly detection, yet it remains surprisingly underexplored in anomaly detection literature. This paper presents a comprehensive study that addresses the fundamental question: When can we conclusively determine that anomalies are present? Through extensive experimentation involving over three million statistical tests across various anomaly detection tasks and algorithms, we identify a relationship between the dataset size, contamination rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate $ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents a lower bound on the number of samples required to confirm anomaly existence. This threshold implies a limit to how rare anomalies can be before proving their existence becomes infeasible.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Na\"ive Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs</title>
<link>https://arxiv.org/abs/2508.09904</link>
<guid>https://arxiv.org/abs/2508.09904</guid>
<content:encoded><![CDATA[

arXiv:2508.09904v1 Announce Type: new 
Abstract: Forecasting in real-world settings requires models to integrate not only historical data but also relevant contextual information, often available in textual form. While recent work has shown that large language models (LLMs) can be effective context-aided forecasters via na\"ive direct prompting, their full potential remains underexplored. We address this gap with 4 strategies, providing new insights into the zero-shot capabilities of LLMs in this setting. ReDP improves interpretability by eliciting explicit reasoning traces, allowing us to assess the model's reasoning over the context independently from its forecast accuracy. CorDP leverages LLMs solely to refine existing forecasts with context, enhancing their applicability in real-world forecasting pipelines. IC-DP proposes embedding historical examples of context-aided forecasting tasks in the prompt, substantially improving accuracy even for the largest models. Finally, RouteDP optimizes resource efficiency by using LLMs to estimate task difficulty, and routing the most challenging tasks to larger models. Evaluated on different kinds of context-aided forecasting tasks from the CiK benchmark, our strategies demonstrate distinct benefits over na\"ive prompting across LLMs of different sizes and families. These results open the door to further simple yet effective improvements in LLM-based context-aided forecasting.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prototype-Guided Diffusion: Visual Conditioning without External Memory</title>
<link>https://arxiv.org/abs/2508.09922</link>
<guid>https://arxiv.org/abs/2508.09922</guid>
<content:encoded><![CDATA[

arXiv:2508.09922v1 Announce Type: new 
Abstract: Diffusion models have emerged as a leading framework for high-quality image generation, offering stable training and strong performance across diverse domains. However, they remain computationally intensive, particularly during the iterative denoising process. Latent-space models like Stable Diffusion alleviate some of this cost by operating in compressed representations, though at the expense of fine-grained detail. More recent approaches such as Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning denoising on similar examples retrieved from large external memory banks. While effective, these methods introduce drawbacks: they require costly storage and retrieval infrastructure, depend on static vision-language models like CLIP for similarity, and lack adaptability during training. We propose the Prototype Diffusion Model (PDM), a method that integrates prototype learning directly into the diffusion process for efficient and adaptive visual conditioning - without external memory. Instead of retrieving reference samples, PDM constructs a dynamic set of compact visual prototypes from clean image features using contrastive learning. These prototypes guide the denoising steps by aligning noisy representations with semantically relevant visual patterns, enabling efficient generation with strong semantic grounding. Experiments show that PDM maintains high generation quality while reducing computational and storage overhead, offering a scalable alternative to retrieval-based conditioning in diffusion models.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Residual Reservoir Memory Networks</title>
<link>https://arxiv.org/abs/2508.09925</link>
<guid>https://arxiv.org/abs/2508.09925</guid>
<content:encoded><![CDATA[

arXiv:2508.09925v1 Announce Type: new 
Abstract: We introduce a novel class of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear reservoir, where the latter is based on residual orthogonal connections along the temporal dimension for enhanced long-term propagation of the input. The resulting reservoir state dynamics are studied through the lens of linear stability analysis, and we investigate diverse configurations for the temporal residual connections. The proposed approach is empirically assessed on time-series and pixel-level 1-D classification tasks. Our experimental results highlight the advantages of the proposed approach over other conventional RC models.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models</title>
<link>https://arxiv.org/abs/2508.09968</link>
<guid>https://arxiv.org/abs/2508.09968</guid>
<content:encoded><![CDATA[

arXiv:2508.09968v1 Announce Type: new 
Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost. Code is available at https://github.com/ExplainableML/HyperNoise
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Mixture-of-Experts for Incremental Graph Learning</title>
<link>https://arxiv.org/abs/2508.09974</link>
<guid>https://arxiv.org/abs/2508.09974</guid>
<content:encoded><![CDATA[

arXiv:2508.09974v1 Announce Type: new 
Abstract: Graph incremental learning is a learning paradigm that aims to adapt trained models to continuously incremented graphs and data over time without the need for retraining on the full dataset. However, regular graph machine learning methods suffer from catastrophic forgetting when applied to incremental learning settings, where previously learned knowledge is overridden by new knowledge. Previous approaches have tried to address this by treating the previously trained model as an inseparable unit and using techniques to maintain old behaviors while learning new knowledge. These approaches, however, do not account for the fact that previously acquired knowledge at different timestamps contributes differently to learning new tasks. Some prior patterns can be transferred to help learn new data, while others may deviate from the new data distribution and be detrimental. To address this, we propose a dynamic mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a DyMoE GNN layer adds new expert networks specialized in modeling the incoming data blocks. We design a customized regularization loss that utilizes data sequence information so existing experts can maintain their ability to solve old tasks while helping the new expert learn the new data effectively. As the number of data blocks grows over time, the computational cost of the full mixture-of-experts (MoE) model increases. To address this, we introduce a sparse MoE approach, where only the top-$k$ most relevant experts make predictions, significantly reducing the computation time. Our model achieved 4.92\% relative accuracy increase compared to the best baselines on class incremental learning, showing the model's exceptional power.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet</title>
<link>https://arxiv.org/abs/2508.09140</link>
<guid>https://arxiv.org/abs/2508.09140</guid>
<content:encoded><![CDATA[

arXiv:2508.09140v1 Announce Type: cross 
Abstract: Radio map (RM) has recently attracted much attention since it can provide real-time and accurate spatial channel information for 6G services and applications. However, current deep learning-based methods for RM construction exhibit well known accuracy-efficiency trade-off. In this paper, we introduce RadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the trade-off. Generally, accurate RM construction requires modeling long-range spatial dependencies, reflecting the global nature of wave propagation physics. RadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures these global dependencies with linear complexity, while a parallel convolutional branch extracts local features. This hybrid design generates feature representations that capture both global context and local detail. Experiments show that RadioMamba achieves higher accuracy than existing methods, including diffusion models, while operating nearly 20 times faster and using only 2.9\% of the model parameters. By improving both accuracy and efficiency, RadioMamba presents a viable approach for real-time intelligent optimization in next generation wireless systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic TinyML for Intent-aware Handover in 6G Wireless Networks</title>
<link>https://arxiv.org/abs/2508.09147</link>
<guid>https://arxiv.org/abs/2508.09147</guid>
<content:encoded><![CDATA[

arXiv:2508.09147v1 Announce Type: cross 
Abstract: As 6G networks evolve into increasingly AI-driven, user-centric ecosystems, traditional reactive handover mechanisms demonstrate limitations, especially in mobile edge computing and autonomous agent-based service scenarios. This manuscript introduces WAAN, a cross-layer framework that enables intent-aware and proactive handovers by embedding lightweight TinyML agents as autonomous, negotiation-capable entities across heterogeneous edge nodes that contribute to intent propagation and network adaptation. To ensure continuity across mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points that serve as coordination anchors for context transfer and state preservation. The framework's operational capabilities are demonstrated through a multimodal environmental control case study, highlighting its effectiveness in maintaining user experience under mobility. Finally, the article discusses key challenges and future opportunities associated with the deployment and evolution of WAAN.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI</title>
<link>https://arxiv.org/abs/2508.09152</link>
<guid>https://arxiv.org/abs/2508.09152</guid>
<content:encoded><![CDATA[

arXiv:2508.09152v1 Announce Type: cross 
Abstract: With the advent of 5G networks and technologies, ensuring the integrity and performance of packet core traffic is paramount. During network analysis, test files such as Packet Capture (PCAP) files and log files will contain errors if present in the system that must be resolved for better overall network performance, such as connectivity strength and handover quality. Current methods require numerous person-hours to sort out testing results and find the faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine designed to classify successful and faulty frames in PCAP files, specifically within the 5G packet core. The FA engine analyses network traffic using natural language processing techniques to identify anomalies and inefficiencies, significantly reducing the effort time required and increasing efficiency. The FA Engine also suggests steps to fix the issue using Generative AI via a Large Language Model (LLM) trained on several 5G packet core documents. The engine explains the details of the error from the domain perspective using documents such as the 3GPP standards and user documents regarding the internal conditions of the tests. Test results on the ML models show high classification accuracy on the test dataset when trained with 80-20 splits for the successful and failed PCAP files. Future scopes include extending the AI engine to incorporate 4G network traffic and other forms of network data, such as log text files and multimodal systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery</title>
<link>https://arxiv.org/abs/2508.09183</link>
<guid>https://arxiv.org/abs/2508.09183</guid>
<content:encoded><![CDATA[

arXiv:2508.09183v1 Announce Type: cross 
Abstract: Quantum computation has demonstrated a promising alternative to solving the NP-hard combinatorial problems. Specifically, when it comes to optimization, classical approaches become intractable to account for large-scale solutions. Specifically, we investigate quantum computing to solve the large-scale Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this regard, a Reinforcement Learning (RL) framework augmented with a Parametrized Quantum Circuit (PQC) is designed to minimize the travel time in a realistic last-mile on-demand delivery. A novel problem-specific encoding quantum circuit with an entangling and variational layer is proposed. Moreover, Proximal Policy Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are designed for comparison through numerical experiments, highlighting the superiority of the proposed method in terms of the scale of the solution and training complexity while incorporating the real-world constraints.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2508.09196</link>
<guid>https://arxiv.org/abs/2508.09196</guid>
<content:encoded><![CDATA[

arXiv:2508.09196v1 Announce Type: cross 
Abstract: Different CT segmentation datasets are typically obtained from different scanners under different capture settings and often provide segmentation labels for a limited and often disjoint set of organs. Using these heterogeneous data effectively while preserving patient privacy can be challenging. This work presents a novel federated learning approach to achieve universal segmentation across diverse abdominal CT datasets by utilizing model uncertainty for aggregation and predictive uncertainty for inference. Our approach leverages the inherent noise in stochastic mini-batch gradient descent to estimate a distribution over the model weights to provide an on-the-go uncertainty over the model parameters at the client level. The parameters are then aggregated at the server using the additional uncertainty information using a Bayesian-inspired inverse-variance aggregation scheme. Furthermore, the proposed method quantifies prediction uncertainty by propagating the uncertainty from the model weights, providing confidence measures essential for clinical decision-making. In line with recent work shown, predictive uncertainty is utilized in the inference stage to improve predictive performance. Experimental evaluations demonstrate the effectiveness of this approach in improving both the quality of federated aggregation and uncertainty-weighted inference compared to previously established baselines. The code for this work is made available at: https://github.com/asimukaye/fiva
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning</title>
<link>https://arxiv.org/abs/2508.09207</link>
<guid>https://arxiv.org/abs/2508.09207</guid>
<content:encoded><![CDATA[

arXiv:2508.09207v1 Announce Type: cross 
Abstract: The process of generating fully colorized drawings from sketches is a large, usually costly bottleneck in the manga and anime industry. In this study, we examine multiple models for image-to-image translation between anime characters and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By assessing them qualitatively and quantitatively, we find that C-GAN is the most effective model that is able to produce high-quality and high-resolution images close to those created by humans.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2508.09209</link>
<guid>https://arxiv.org/abs/2508.09209</guid>
<content:encoded><![CDATA[

arXiv:2508.09209v1 Announce Type: cross 
Abstract: Generative adversarial networks (GANs) have emerged as a powerful paradigm for producing high-fidelity data samples, yet their performance is constrained by the quality of latent representations, typically sampled from classical noise distributions. This study investigates hybrid quantum-classical GANs (HQCGANs) in which a quantum generator, implemented via parameterised quantum circuits, produces latent vectors for a classical discriminator. We evaluate a classical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using Qiskit's AerSimulator with realistic noise models to emulate near-term quantum devices. The binary MNIST dataset (digits 0 and 1) is used to align with the low-dimensional latent spaces imposed by current quantum hardware. Models are trained for 150 epochs and assessed with Frechet Inception Distance (FID) and Kernel Inception Distance (KID). Results show that while the classical GAN achieved the best scores, the 7-qubit HQCGAN produced competitive performance, narrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier convergence limitations. Efficiency analysis indicates only moderate training time increases despite quantum sampling overhead. These findings validate the feasibility of noisy quantum circuits as latent priors in GAN architectures, highlighting their potential to enhance generative modelling within the constraints of the noisy intermediate-scale quantum (NISQ) era.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Generative Models for Discrete Genotype Simulation</title>
<link>https://arxiv.org/abs/2508.09212</link>
<guid>https://arxiv.org/abs/2508.09212</guid>
<content:encoded><![CDATA[

arXiv:2508.09212v1 Announce Type: cross 
Abstract: Deep generative models open new avenues for simulating realistic genomic data while preserving privacy and addressing data accessibility constraints. While previous studies have primarily focused on generating gene expression or haplotype data, this study explores generating genotype data in both unconditioned and phenotype-conditioned settings, which is inherently more challenging due to the discrete nature of genotype data. In this work, we developed and evaluated commonly used generative models, including Variational Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks (GANs), and proposed adaptation tailored to discrete genotype data. We conducted extensive experiments on large-scale datasets, including all chromosomes from cow and multiple chromosomes from human. Model performance was assessed using a well-established set of metrics drawn from both deep learning and quantitative genetics literature. Our results show that these models can effectively capture genetic patterns and preserve genotype-phenotype association. Our findings provide a comprehensive comparison of these models and offer practical guidelines for future research in genotype simulation. We have made our code publicly available at https://github.com/SihanXXX/DiscreteGenoGen.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics</title>
<link>https://arxiv.org/abs/2508.09215</link>
<guid>https://arxiv.org/abs/2508.09215</guid>
<content:encoded><![CDATA[

arXiv:2508.09215v1 Announce Type: cross 
Abstract: While analysing rare blood cell aggregates remains challenging in automated haematology, they could markedly advance label-free functional diagnostics. Conventional flow cytometers efficiently perform cell counting with leukocyte differentials but fail to identify aggregates with flagged results, requiring manual reviews. Quantitative phase imaging flow cytometry captures detailed aggregate morphologies, but clinical use is hampered by massive data storage and offline processing. Incorporating hidden biomarkers into routine haematology panels would significantly improve diagnostics without flagged results. We present RT-HAD, an end-to-end deep learning-based image and data processing framework for off-axis digital holographic microscopy (DHM), which combines physics-consistent holographic reconstruction and detection, representing each blood cell in a graph to recognize aggregates. RT-HAD processes >30 GB of image data on-the-fly with turnaround time of <1.5 min and error rate of 8.9% in platelet aggregate detection, which matches acceptable laboratory error rates of haematology biomarkers and solves the big data challenge for point-of-care diagnostics.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Molecular Odor Taxonomies for Structure-based Odor Predictions using Machine Learning</title>
<link>https://arxiv.org/abs/2508.09217</link>
<guid>https://arxiv.org/abs/2508.09217</guid>
<content:encoded><![CDATA[

arXiv:2508.09217v1 Announce Type: cross 
Abstract: One of the key challenges to predict odor from molecular structure is unarguably our limited understanding of the odor space and the complexity of the underlying structure-odor relationships. Here, we show that the predictive performance of machine learning models for structure-based odor predictions can be improved using both, an expert and a data-driven odor taxonomy. The expert taxonomy is based on semantic and perceptual similarities, while the data-driven taxonomy is based on clustering co-occurrence patterns of odor descriptors directly from the prepared dataset. Both taxonomies improve the predictions of different machine learning models and outperform random groupings of descriptors that do not reflect existing relations between odor descriptors. We assess the quality of both taxonomies through their predictive performance across different odor classes and perform an in-depth error analysis highlighting the complexity of odor-structure relationships and identifying potential inconsistencies within the taxonomies by showcasing pear odorants used in perfumery. The data-driven taxonomy allows us to critically evaluate our expert taxonomy and better understand the molecular odor space. Both taxonomies as well as a full dataset are made available to the community, providing a stepping stone for a future community-driven exploration of the molecular basis of smell. In addition, we provide a detailed multi-layer expert taxonomy including a total of 777 different descriptors from the Pyrfume repository.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Objective Soups: Multilingual Multi-Task Modeling for Speech Processing</title>
<link>https://arxiv.org/abs/2508.09228</link>
<guid>https://arxiv.org/abs/2508.09228</guid>
<content:encoded><![CDATA[

arXiv:2508.09228v1 Announce Type: cross 
Abstract: Training a single model for multilingual, multi-task speech processing (MSP) is severely hampered by conflicting objectives between tasks like speech recognition and translation. While multi-objective optimization (MOO) aims to align gradient updates, its effectiveness diminishes as the number of tasks grows, making it difficult to find a common descent direction. This raises a fundamental question: should highly conflicting objectives be optimized jointly or separated into a hierarchical structure? To address this question, this paper investigates three multi-objective MSP formulations, which we refer to as \textbf{objective soup recipes}. These formulations apply multi-objective optimization at different optimization levels to mitigate potential conflicts among all objectives. To ensure efficiency, we introduce a lightweight layer-selection mechanism that computes the conflict-avoiding gradient using only the most problematic layers, minimizing computational and memory overhead. Extensive experiments on CoVoST v2, LibriSpeech, and AISHELL-1 reveal that a bi-level recipe separating recognition and translation tasks consistently outperforms standard flat optimization. Our work demonstrates that hierarchical MOO is a more effective and scalable approach for building state-of-the-art MSP models. Our code has been released at https://github.com/afmsaif/Objective_Soups.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Binary Economic Events in Modern Mercantilism: Traditional methodologies coupled with PCA and K-means Quantitative Analysis of Qualitative Sentimental Data</title>
<link>https://arxiv.org/abs/2508.09243</link>
<guid>https://arxiv.org/abs/2508.09243</guid>
<content:encoded><![CDATA[

arXiv:2508.09243v1 Announce Type: cross 
Abstract: This paper examines Modern Mercantilism, characterized by rising economic nationalism, strategic technological decoupling, and geopolitical fragmentation, as a disruptive shift from the post-1945 globalization paradigm. It applies Principal Component Analysis (PCA) to 768-dimensional SBERT-generated semantic embeddings of curated news articles to extract orthogonal latent factors that discriminate binary event outcomes linked to protectionism, technological sovereignty, and bloc realignments. Analysis of principal component loadings identifies key semantic features driving classification performance, enhancing interpretability and predictive accuracy. This methodology provides a scalable, data-driven framework for quantitatively tracking emergent mercantilist dynamics through high-dimensional text analytics
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Input-Adaptive Inference for Efficient VLN</title>
<link>https://arxiv.org/abs/2508.09262</link>
<guid>https://arxiv.org/abs/2508.09262</guid>
<content:encoded><![CDATA[

arXiv:2508.09262v1 Announce Type: cross 
Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of history-aware multi-modal transformer models. Given a language instruction, these models process observation and navigation history to predict the most appropriate action for an agent. While they have significantly improved performance, the scale of these models can be a bottleneck in practical settings with limited computational resources. In this work, we propose a novel input-adaptive navigation method to enhance VLN model efficiency. We first show that existing input-adaptive mechanisms fail to reduce computations without substantial performance degradation. To address this, we introduce three adaptive algorithms, each deployed at a different level: (1) To improve spatial efficiency, we selectively process panoramic views at each observation of an agent. (2) To improve intra-model efficiency, we propose importance-based adaptive thresholding for the early-exit methods. (3) To improve temporal efficiency, we implement a caching mechanism that prevents reprocessing of views previously seen by the agent. In evaluations on seven VLN benchmarks, we demonstrate over a 2$\times$ reduction in computation across three off-the-shelf agents in both standard and continuous environments. Our code is publicly available at https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Imputation Method for Multimodal Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2508.09271</link>
<guid>https://arxiv.org/abs/2508.09271</guid>
<content:encoded><![CDATA[

arXiv:2508.09271v1 Announce Type: cross 
Abstract: Multimodal data analysis can lead to more accurate diagnoses of brain disorders due to the complementary information that each modality adds. However, a major challenge of using multimodal datasets in the neuroimaging field is incomplete data, where some of the modalities are missing for certain subjects. Hence, effective strategies are needed for completing the data. Traditional methods, such as subsampling or zero-filling, may reduce the accuracy of predictions or introduce unintended biases. In contrast, advanced methods such as generative models have emerged as promising solutions without these limitations. In this study, we proposed a generative adversarial network method designed to reconstruct missing modalities from existing ones while preserving the disease patterns. We used T1-weighted structural magnetic resonance imaging and functional network connectivity as two modalities. Our findings showed a 9% improvement in the classification accuracy for Alzheimer's disease versus cognitive normal groups when using our generative imputation method compared to the traditional approaches.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.09277</link>
<guid>https://arxiv.org/abs/2508.09277</guid>
<content:encoded><![CDATA[

arXiv:2508.09277v1 Announce Type: cross 
Abstract: Value function initialization (VFI) is an effective way to achieve a jumpstart in reinforcement learning (RL) by leveraging value estimates from prior tasks. While this approach is well established in tabular settings, extending it to deep reinforcement learning (DRL) poses challenges due to the continuous nature of the state-action space, the noisy approximations of neural networks, and the impracticality of storing all past models for reuse. In this work, we address these challenges and introduce DQInit, a method that adapts value function initialization to DRL. DQInit reuses compact tabular Q-values extracted from previously solved tasks as a transferable knowledge base. It employs a knownness-based mechanism to softly integrate these transferred values into underexplored regions and gradually shift toward the agent's learned estimates, avoiding the limitations of fixed time decay. Our approach offers a novel perspective on knowledge transfer in DRL by relying solely on value estimates rather than policies or demonstrations, effectively combining the strengths of jumpstart RL and policy distillation while mitigating their drawbacks. Experiments across multiple continuous control tasks demonstrate that DQInit consistently improves early learning efficiency, stability, and overall performance compared to standard initialization and existing transfer techniques.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative</title>
<link>https://arxiv.org/abs/2508.09294</link>
<guid>https://arxiv.org/abs/2508.09294</guid>
<content:encoded><![CDATA[

arXiv:2508.09294v1 Announce Type: cross 
Abstract: Advances in speech synthesis intensify security threats, motivating real-time deepfake detection research. We investigate whether bidirectional Mamba can serve as a competitive alternative to Self-Attention in detecting synthetic speech. Our solution, Fake-Mamba, integrates an XLSR front-end with bidirectional Mamba to capture both local and global artifacts. Our core innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof 21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and 5.85% EER, respectively, representing substantial relative gains over SOTA models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time inference across utterance lengths, demonstrating strong generalization and practical viability. The code is available at https://github.com/xuanxixi/Fake-Mamba.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Code Refactoring Using LLMs</title>
<link>https://arxiv.org/abs/2508.09332</link>
<guid>https://arxiv.org/abs/2508.09332</guid>
<content:encoded><![CDATA[

arXiv:2508.09332v1 Announce Type: cross 
Abstract: This Innovative Practice full paper explores how Large Language Models (LLMs) can enhance the teaching of code refactoring in software engineering courses through real-time, context-aware feedback. Refactoring improves code quality but is difficult to teach, especially with complex, real-world codebases. Traditional methods like code reviews and static analysis tools offer limited, inconsistent feedback. Our approach integrates LLM-assisted refactoring into a course project using structured prompts to help students identify and address code smells such as long methods and low cohesion. Implemented in Spring 2025 in a long-lived OSS project, the intervention is evaluated through student feedback and planned analysis of code quality improvements. Findings suggest that LLMs can bridge theoretical and practical learning, supporting a deeper understanding of maintainability and refactoring principles.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition</title>
<link>https://arxiv.org/abs/2508.09362</link>
<guid>https://arxiv.org/abs/2508.09362</guid>
<content:encoded><![CDATA[

arXiv:2508.09362v1 Announce Type: cross 
Abstract: Accurate recognition of sign language in healthcare communication poses a significant challenge, requiring frameworks that can accurately interpret complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net, a novel attention-based ensemble of spatiotemporal networks that dynamically fuses visual and motion data to enhance recognition accuracy. The proposed approach processes RGB video and range Doppler map radar modalities synchronously through four different spatiotemporal networks. For each network, features from both modalities are continuously fused using an attention-based fusion module before being fed into an ensemble of classifiers. Finally, the outputs of these four different fused channels are combined in an ensemble classification head, thereby enhancing the model's robustness. Experiments demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for Italian Sign Language. Our findings indicate that an ensemble of diverse spatiotemporal networks, unified by attention-based fusion, yields a robust and accurate framework for complex, multimodal isolated gesture recognition tasks. The source code is available at: https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classifying Cool Dwarfs: Comprehensive Spectral Typing of Field and Peculiar Dwarfs Using Machine Learning</title>
<link>https://arxiv.org/abs/2508.09370</link>
<guid>https://arxiv.org/abs/2508.09370</guid>
<content:encoded><![CDATA[

arXiv:2508.09370v1 Announce Type: cross 
Abstract: Low-mass stars and brown dwarfs -- spectral types (SpTs) M0 and later -- play a significant role in studying stellar and substellar processes and demographics, reaching down to planetary-mass objects. Currently, the classification of these sources remains heavily reliant on visual inspection of spectral features, equivalent width measurements, or narrow-/wide-band spectral indices. Recent advances in machine learning (ML) methods offer automated approaches for spectral typing, which are becoming increasingly important as large spectroscopic surveys such as Gaia, SDSS, and SPHEREx generate datasets containing millions of spectra. We investigate the application of ML in spectral type classification on low-resolution (R $\sim$ 120) near-infrared spectra of M0--T9 dwarfs obtained with the SpeX instrument on the NASA Infrared Telescope Facility. We specifically aim to classify the gravity- and metallicity-dependent subclasses for late-type dwarfs. We used binned fluxes as input features and compared the efficacy of spectral type estimators built using Random Forest (RF), Support Vector Machine (SVM), and K-Nearest Neighbor (KNN) models. We tested the influence of different normalizations and analyzed the relative importance of different spectral regions for surface gravity and metallicity subclass classification. Our best-performing model (using KNN) classifies 95.5 $\pm$ 0.6% of sources to within $\pm$1 SpT, and assigns surface gravity and metallicity subclasses with 89.5 $\pm$ 0.9% accuracy. We test the dependence of signal-to-noise ratio on classification accuracy and find sources with SNR $\gtrsim$ 60 have $\gtrsim$ 95% accuracy. We also find that zy-band plays the most prominent role in the RF model, with FeH and TiO having the highest feature importance.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition</title>
<link>https://arxiv.org/abs/2508.09372</link>
<guid>https://arxiv.org/abs/2508.09372</guid>
<content:encoded><![CDATA[

arXiv:2508.09372v1 Announce Type: cross 
Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges, including significant inter-signer variability and poor generalization to novel sentence structures. Traditional solutions frequently fail to handle these issues efficiently. For overcoming these constraints, we propose a dual-architecture framework. For the Signer-Independent (SI) challenge, we propose a Signer-Invariant Conformer that combines convolutions with multi-head self-attention to learn robust, signer-agnostic representations from pose-based skeletal keypoints. For the Unseen-Sentences (US) task, we designed a Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that captures both fine-grained posture dynamics, enabling the model's ability to comprehend novel grammatical compositions. Experiments on the challenging Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US task, the transformer model scores a WER of 47.78%, surpassing previous work. In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th in the SI task, demonstrating the performance of these models. The findings validate our key hypothesis: that developing task-specific networks designed for the particular challenges of CSLR leads to considerable performance improvements and establishes a new baseline for further research. The source code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?</title>
<link>https://arxiv.org/abs/2508.09381</link>
<guid>https://arxiv.org/abs/2508.09381</guid>
<content:encoded><![CDATA[

arXiv:2508.09381v1 Announce Type: cross 
Abstract: Medical image segmentation exhibits intra- and inter-annotator variability due to ambiguous object boundaries, annotator preferences, expertise, and tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated or infiltrative nodules, or irregular borders per the ABCD rule, are particularly prone to disagreement and are often associated with malignancy. In this work, we curate IMA++, the largest multi-annotator skin lesion segmentation dataset, on which we conduct an in-depth study of variability due to annotator, malignancy, tool, and skill factors. We find a statistically significant (p<0.001) association between inter-annotator agreement (IAA), measured using Dice, and the malignancy of skin lesions. We further show that IAA can be accurately predicted directly from dermoscopic images, achieving a mean absolute error of 0.108. Finally, we leverage this association by utilizing IAA as a "soft" clinical feature within a multi-task learning objective, yielding a 4.2% improvement in balanced accuracy averaged across multiple model architectures and across IMA++ and four public dermoscopic datasets. The code is available at https://github.com/sfu-mial/skin-IAV.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs</title>
<link>https://arxiv.org/abs/2508.09389</link>
<guid>https://arxiv.org/abs/2508.09389</guid>
<content:encoded><![CDATA[

arXiv:2508.09389v1 Announce Type: cross 
Abstract: Prosody conveys rich emotional and semantic information of the speech signal as well as individual idiosyncrasies. We propose a stand-alone model that maps text-to-prosodic features such as F0 and energy and can be used in downstream tasks such as TTS. The ProMode encoder takes as input acoustic features and time-aligned textual content, both are partially masked, and obtains a fixed-length latent prosodic embedding. The decoder predicts acoustics in the masked region using both the encoded prosody input and unmasked textual content. Trained on the GigaSpeech dataset, we compare our method with state-of-the-art style encoders. For F0 and energy predictions, we show consistent improvements for our model at different levels of granularity. We also integrate these predicted prosodic features into a TTS system and conduct perceptual tests, which show higher prosody preference compared to the baselines, demonstrating the model's potential in tasks where prosody modeling is important.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A pseudo-inverse of a line graph</title>
<link>https://arxiv.org/abs/2508.09412</link>
<guid>https://arxiv.org/abs/2508.09412</guid>
<content:encoded><![CDATA[

arXiv:2508.09412v1 Announce Type: cross 
Abstract: Line graphs are an alternative representation of graphs where each vertex of the original (root) graph becomes an edge. However not all graphs have a corresponding root graph, hence the transformation from graphs to line graphs is not invertible. We investigate the case when there is a small perturbation in the space of line graphs, and try to recover the corresponding root graph, essentially defining the inverse of the line graph operation. We propose a linear integer program that edits the smallest number of edges in the line graph, that allow a root graph to be found. We use the spectral norm to theoretically prove that such a pseudo-inverse operation is well behaved. Illustrative empirical experiments on Erd\H{o}s-R\'enyi graphs show that our theoretical results work in practice.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss</title>
<link>https://arxiv.org/abs/2508.09453</link>
<guid>https://arxiv.org/abs/2508.09453</guid>
<content:encoded><![CDATA[

arXiv:2508.09453v1 Announce Type: cross 
Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled datasets, has emerged as an effective approach in creating adaptable and reusable architectures that can be leveraged for various downstream tasks using satellite observations. However, their direct application to hyperspectral remote sensing remains challenging due to inherent spectral disparities and the scarcity of available observations. In this work, we present HyperKD, a novel knowledge distillation framework that enables transferring learned representations from a teacher model into a student model for effective development of a foundation model on hyperspectral images. Unlike typical knowledge distillation frameworks, which use a complex teacher to guide a simpler student, HyperKD enables an inverse form of knowledge transfer across different types of spectral data, guided by a simpler teacher model. Building upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi foundational model into a student tailored for EnMAP hyperspectral imagery. HyperKD addresses the inverse domain adaptation problem with spectral gaps by introducing a feature-based strategy that includes spectral range-based channel alignment, spatial feature-guided masking, and an enhanced loss function tailored for hyperspectral images. HyperKD bridges the substantial spectral domain gap, enabling the effective use of pretrained foundation models for geospatial applications. Extensive experiments show that HyperKD significantly improves representation learning in MAEs, leading to enhanced reconstruction fidelity and more robust performance on downstream tasks such as land cover classification, crop type identification, and soil organic carbon prediction, underpinning the potential of knowledge distillation frameworks in remote sensing analytics with hyperspectral imagery.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking</title>
<link>https://arxiv.org/abs/2508.09499</link>
<guid>https://arxiv.org/abs/2508.09499</guid>
<content:encoded><![CDATA[

arXiv:2508.09499v1 Announce Type: cross 
Abstract: Accurately predicting the binding conformation of small-molecule ligands to protein targets is a critical step in rational drug design. Although recent deep learning-based docking surpasses traditional methods in speed and accuracy, many approaches rely on graph representations and language model-inspired encoders while neglecting critical geometric information, resulting in inaccurate pocket localization and unrealistic binding conformations. In this study, we introduce CWFBind, a weighted, fast, and accurate docking method based on local curvature features. Specifically, we integrate local curvature descriptors during the feature extraction phase to enrich the geometric representation of both proteins and ligands, complementing existing chemical, sequence, and structural features. Furthermore, we embed degree-aware weighting mechanisms into the message passing process, enhancing the model's ability to capture spatial structural distinctions and interaction strengths. To address the class imbalance challenge in pocket prediction, CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced loss function, facilitating more precise identification of binding regions and key residues. Comprehensive experimental evaluations demonstrate that CWFBind achieves competitive performance across multiple docking benchmarks, offering a balanced trade-off between accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generation of Indian Sign Language Letters, Numbers, and Words</title>
<link>https://arxiv.org/abs/2508.09522</link>
<guid>https://arxiv.org/abs/2508.09522</guid>
<content:encoded><![CDATA[

arXiv:2508.09522v1 Announce Type: cross 
Abstract: Sign language, which contains hand movements, facial expressions and bodily gestures, is a significant medium for communicating with hard-of-hearing people. A well-trained sign language community communicates easily, but those who don't know sign language face significant challenges. Recognition and generation are basic communication methods between hearing and hard-of-hearing individuals. Despite progress in recognition, sign language generation still needs to be explored. The Progressive Growing of Generative Adversarial Network (ProGAN) excels at producing high-quality images, while the Self-Attention Generative Adversarial Network (SAGAN) generates feature-rich images at medium resolutions. Balancing resolution and detail is crucial for sign language image generation. We are developing a Generative Adversarial Network (GAN) variant that combines both models to generate feature-rich, high-resolution, and class-conditional sign language images. Our modified Attention-based model generates high-quality images of Indian Sign Language letters, numbers, and words, outperforming the traditional ProGAN in Inception Score (IS) and Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12, respectively. Additionally, we are publishing a large dataset incorporating high-quality images of Indian Sign Language alphabets, numbers, and 129 words.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepWKB: Learning WKB Expansions of Invariant Distributions for Stochastic Systems</title>
<link>https://arxiv.org/abs/2508.09529</link>
<guid>https://arxiv.org/abs/2508.09529</guid>
<content:encoded><![CDATA[

arXiv:2508.09529v1 Announce Type: cross 
Abstract: This paper introduces a novel deep learning method, called DeepWKB, for estimating the invariant distribution of randomly perturbed systems via its Wentzel-Kramers-Brillouin (WKB) approximation $u_\epsilon(x) = Q(\epsilon)^{-1} Z_\epsilon(x) \exp\{-V(x)/\epsilon\}$, where $V$ is known as the quasi-potential, $\epsilon$ denotes the noise strength, and $Q(\epsilon)$ is the normalization factor. By utilizing both Monte Carlo data and the partial differential equations satisfied by $V$ and $Z_\epsilon$, the DeepWKB method computes $V$ and $Z_\epsilon$ separately. This enables an approximation of the invariant distribution in the singular regime where $\epsilon$ is sufficiently small, which remains a significant challenge for most existing methods. Moreover, the DeepWKB method is applicable to higher-dimensional stochastic systems whose deterministic counterparts admit non-trivial attractors. In particular, it provides a scalable and flexible alternative for computing the quasi-potential, which plays a key role in the analysis of rare events, metastability, and the stochastic stability of complex systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective</title>
<link>https://arxiv.org/abs/2508.09541</link>
<guid>https://arxiv.org/abs/2508.09541</guid>
<content:encoded><![CDATA[

arXiv:2508.09541v1 Announce Type: cross 
Abstract: Multi-agent self-organizing systems (MASOS) exhibit key characteristics including scalability, adaptability, flexibility, and robustness, which have contributed to their extensive application across various fields. However, the self-organizing nature of MASOS also introduces elements of unpredictability in their emergent behaviors. This paper focuses on the emergence of dependency hierarchies during task execution, aiming to understand how such hierarchies arise from agents' collective pursuit of the joint objective, how they evolve dynamically, and what factors govern their development. To investigate this phenomenon, multi-agent reinforcement learning (MARL) is employed to train MASOS for a collaborative box-pushing task. By calculating the gradients of each agent's actions in relation to the states of other agents, the inter-agent dependencies are quantified, and the emergence of hierarchies is analyzed through the aggregation of these dependencies. Our results demonstrate that hierarchies emerge dynamically as agents work towards a joint objective, with these hierarchies evolving in response to changing task requirements. Notably, these dependency hierarchies emerge organically in response to the shared objective, rather than being a consequence of pre-configured rules or parameters that can be fine-tuned to achieve specific results. Furthermore, the emergence of hierarchies is influenced by the task environment and network initialization conditions. Additionally, hierarchies in MASOS emerge from the dynamic interplay between agents' "Talent" and "Effort" within the "Environment." "Talent" determines an agent's initial influence on collective decision-making, while continuous "Effort" within the "Environment" enables agents to shift their roles and positions within the system.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap</title>
<link>https://arxiv.org/abs/2508.09591</link>
<guid>https://arxiv.org/abs/2508.09591</guid>
<content:encoded><![CDATA[

arXiv:2508.09591v1 Announce Type: cross 
Abstract: The sparsely activated mixture-of-experts (MoE) transformer has become a common architecture for large language models (LLMs) due to its sparsity, which requires fewer computational demands while easily scaling the model size. In MoE models, each MoE layer requires to dynamically choose tokens to activate particular experts for computation while the activated experts may not be located in the same device or GPU as the token. However, this leads to substantial communication and load imbalances across all GPUs, which obstructs the scalability of distributed systems within a GPU cluster. To this end, we introduce HierMoE to accelerate the training of MoE models by two topology-aware techniques: 1) token deduplication to reduce the communication traffic, and 2) expert swap to balance the workloads among all GPUs. To enable the above two proposed approaches to be more general, we build theoretical models aimed at achieving the best token duplication and expert swap strategy under different model configurations and hardware environments. We implement our prototype HierMoE system atop Megatron-LM and conduct experiments on a 32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results show that our HierMoE achieves $1.55\times$ to $3.32\times$ faster communication and delivers $1.18\times$ to $1.27\times$ faster end-to-end training compared to state-of-the-art MoE training systems, Tutel-2DH, SmartMoE, and Megatron-LM.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Learned Cardinality Estimation Model</title>
<link>https://arxiv.org/abs/2508.09602</link>
<guid>https://arxiv.org/abs/2508.09602</guid>
<content:encoded><![CDATA[

arXiv:2508.09602v1 Announce Type: cross 
Abstract: Cardinality estimation is a fundamental task in database management systems, aiming to predict query results accurately without executing the queries. However, existing techniques either achieve low estimation accuracy or incur high inference latency. Simultaneously achieving high speed and accuracy becomes critical for the cardinality estimation problem. In this paper, we propose a novel data-driven approach called CoDe (Covering with Decompositions) to address this problem. CoDe employs the concept of covering design, which divides the table into multiple smaller, overlapping segments. For each segment, CoDe utilizes tensor decomposition to accurately model its data distribution. Moreover, CoDe introduces innovative algorithms to select the best-fitting distributions for each query, combining them to estimate the final result. By employing multiple models to approximate distributions, CoDe excels in effectively modeling discrete distributions and ensuring computational efficiency. Notably, experimental results show that our method represents a significant advancement in cardinality estimation, achieving state-of-the-art levels of both estimation accuracy and inference efficiency. Across various datasets, CoDe achieves absolute accuracy in estimating more than half of the queries.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Robot Control via Structured Behavior Trees and Large Language Models</title>
<link>https://arxiv.org/abs/2508.09621</link>
<guid>https://arxiv.org/abs/2508.09621</guid>
<content:encoded><![CDATA[

arXiv:2508.09621v1 Announce Type: cross 
Abstract: As intelligent robots become more integrated into human environments, there is a growing need for intuitive and reliable Human-Robot Interaction (HRI) interfaces that are adaptable and more natural to interact with. Traditional robot control methods often require users to adapt to interfaces or memorize predefined commands, limiting usability in dynamic, unstructured environments. This paper presents a novel framework that bridges natural language understanding and robotic execution by combining Large Language Models (LLMs) with Behavior Trees. This integration enables robots to interpret natural language instructions given by users and translate them into executable actions by activating domain-specific plugins. The system supports scalable and modular integration, with a primary focus on perception-based functionalities, such as person tracking and hand gesture recognition. To evaluate the system, a series of real-world experiments was conducted across diverse environments. Experimental results demonstrate that the proposed approach is practical in real-world scenarios, with an average cognition-to-execution accuracy of approximately 94%, making a significant contribution to HRI systems and robots. The complete source code of the framework is publicly available at https://github.com/snt-arg/robot_suite.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable h-adaptive probabilistic solver for time-independent and time-dependent systems</title>
<link>https://arxiv.org/abs/2508.09623</link>
<guid>https://arxiv.org/abs/2508.09623</guid>
<content:encoded><![CDATA[

arXiv:2508.09623v1 Announce Type: cross 
Abstract: Solving partial differential equations (PDEs) within the framework of probabilistic numerics offers a principled approach to quantifying epistemic uncertainty arising from discretization. By leveraging Gaussian process regression and imposing the governing PDE as a constraint at a finite set of collocation points, probabilistic numerics delivers mesh-free solutions at arbitrary locations. However, the high computational cost, which scales cubically with the number of collocation points, remains a critical bottleneck, particularly for large-scale or high-dimensional problems. We propose a scalable enhancement to this paradigm through two key innovations. First, we develop a stochastic dual descent algorithm that reduces the per-iteration complexity from cubic to linear in the number of collocation points, enabling tractable inference. Second, we exploit a clustering-based active learning strategy that adaptively selects collocation points to maximize information gain while minimizing computational expense. Together, these contributions result in an $h$-adaptive probabilistic solver that can scale to a large number of collocation points. We demonstrate the efficacy of the proposed solver on benchmark PDEs, including two- and three-dimensional steady-state elliptic problems, as well as a time-dependent parabolic PDE formulated in a space-time setting.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data</title>
<link>https://arxiv.org/abs/2508.09636</link>
<guid>https://arxiv.org/abs/2508.09636</guid>
<content:encoded><![CDATA[

arXiv:2508.09636v1 Announce Type: cross 
Abstract: In this paper, we present a novel model architecture for optimizing personalized product search ranking using a multi-task learning (MTL) framework. Our approach uniquely integrates tabular and non-tabular data, leveraging a pre-trained TinyBERT model for semantic embeddings and a novel sampling technique to capture diverse customer behaviors. We evaluate our model against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2, and MMoE, focusing on their ability to handle mixed data types and optimize personalized ranking. Additionally, we propose a scalable relevance labeling mechanism based on click-through rates, click positions, and semantic similarity, offering an alternative to traditional human-annotated labels. Experimental results show that combining non-tabular data with advanced embedding techniques in multi-task learning paradigm significantly enhances model performance. Ablation studies further underscore the benefits of incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT query-product embedding interactions. These results demonstrate the effectiveness of our approach in achieving improved personalized product search ranking.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Diversity in Language Models: When Temperature Fails, Change the Loss</title>
<link>https://arxiv.org/abs/2508.09654</link>
<guid>https://arxiv.org/abs/2508.09654</guid>
<content:encoded><![CDATA[

arXiv:2508.09654v1 Announce Type: cross 
Abstract: Increasing diversity in language models is a challenging yet essential objective. A common approach is to raise the decoding temperature. In this work, we investigate this approach through a simplistic yet common case to provide insights into why decreasing temperature can improve quality (Precision), while increasing it often fails to boost coverage (Recall). Our analysis reveals that for a model to be effectively tunable through temperature adjustments, it must be trained toward coverage. To address this, we propose rethinking loss functions in language models by leveraging the Precision-Recall framework. Our results demonstrate that this approach achieves a substantially better trade-off between Precision and Recall than merely combining negative log-likelihood training with temperature scaling. These findings offer a pathway toward more versatile and robust language modeling techniques.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection for IoT Global Connectivity</title>
<link>https://arxiv.org/abs/2508.09660</link>
<guid>https://arxiv.org/abs/2508.09660</guid>
<content:encoded><![CDATA[

arXiv:2508.09660v1 Announce Type: cross 
Abstract: Internet of Things (IoT) application providers rely on Mobile Network Operators (MNOs) and roaming infrastructures to deliver their services globally. In this complex ecosystem, where the end-to-end communication path traverses multiple entities, it has become increasingly challenging to guarantee communication availability and reliability. Further, most platform operators use a reactive approach to communication issues, responding to user complaints only after incidents have become severe, compromising service quality. This paper presents our experience in the design and deployment of ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity service of a large global roaming platform. ANCHOR assists engineers by filtering vast amounts of data to identify potential problematic clients (i.e., those with connectivity issues affecting several of their IoT devices), enabling proactive issue resolution before the service is critically impacted. We first describe the IoT service, infrastructure, and network visibility of the IoT connectivity provider we operate. Second, we describe the main challenges and operational requirements for designing an unsupervised anomaly detection solution on this platform. Following these guidelines, we propose different statistical rules, and machine- and deep-learning models for IoT verticals anomaly detection based on passive signaling traffic. We describe the steps we followed working with the operational teams on the design and evaluation of our solution on the operational platform, and report an evaluation on operational IoT customers.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication</title>
<link>https://arxiv.org/abs/2508.09665</link>
<guid>https://arxiv.org/abs/2508.09665</guid>
<content:encoded><![CDATA[

arXiv:2508.09665v1 Announce Type: cross 
Abstract: Recent years have witnessed a rising trend in social-sensor cloud identity cloning incidents. However, existing approaches suffer from unsatisfactory performance, a lack of solutions for detecting duplicated accounts, and a lack of large-scale evaluations on real-world datasets. We introduce a novel method for detecting identity cloning in social-sensor cloud service providers. Our proposed technique consists of two primary components: 1) a similar identity detection method and 2) a cryptography-based authentication protocol. Initially, we developed a weakly supervised deep forest model to identify similar identities using non-privacy-sensitive user profile features provided by the service. Subsequently, we designed a cryptography-based authentication protocol to verify whether similar identities were generated by the same provider. Our extensive experiments on a large real-world dataset demonstrate the feasibility and superior performance of our technique compared to current state-of-the-art identity clone detection methods.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity</title>
<link>https://arxiv.org/abs/2508.09676</link>
<guid>https://arxiv.org/abs/2508.09676</guid>
<content:encoded><![CDATA[

arXiv:2508.09676v1 Announce Type: cross 
Abstract: This study investigates the implementation and efficacy of DeputyDev, an AI-powered code review assistant developed to address inefficiencies in the software development process. The process of code review is highly inefficient for several reasons, such as it being a time-consuming process, inconsistent feedback, and review quality not being at par most of the time. Using our telemetry data, we observed that at TATA 1mg, pull request (PR) processing exhibits significant inefficiencies, with average pick-up and review times of 73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review cycle was marked by prolonged iterative communication between the reviewing and submitting parties. Research from the University of California, Irvine indicates that interruptions can lead to an average of 23 minutes of lost focus, critically affecting code quality and timely delivery. To address these challenges, we developed DeputyDev's PR review capabilities by providing automated, contextual code reviews. We conducted a rigorous double-controlled A/B experiment involving over 200 engineers to evaluate DeputyDev's impact on review times. The results demonstrated a statistically significant reduction in both average per PR (23.09%) and average per-line-of-code (40.13%) review durations. After implementing safeguards to exclude outliers, DeputyDev has been effectively rolled out across the entire organisation. Additionally, it has been made available to external companies as a Software-as-a-Service (SaaS) solution, currently supporting the daily work of numerous engineering professionals. This study explores the implementation and effectiveness of AI-assisted code reviews in improving development workflow timelines and code.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation</title>
<link>https://arxiv.org/abs/2508.09715</link>
<guid>https://arxiv.org/abs/2508.09715</guid>
<content:encoded><![CDATA[

arXiv:2508.09715v1 Announce Type: cross 
Abstract: The rapid growth of multimodal medical imaging data presents significant storage and transmission challenges, particularly in resource-constrained clinical settings. We propose NEURAL, a novel framework that addresses this by using semantics-guided data compression. Our approach repurposes cross-attention scores between the image and its radiological report from a fine-tuned generative vision-language model to structurally prune chest X-rays, preserving only diagnostically critical regions. This process transforms the image into a highly compressed, graph representation. This unified graph-based representation fuses the pruned visual graph with a knowledge graph derived from the clinical report, creating a universal data structure that simplifies downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming other baseline models that use uncompressed data. By creating a persistent, task-agnostic data asset, NEURAL resolves the trade-off between data size and clinical utility, enabling efficient workflows and teleradiology without sacrificing performance. Our NEURAL code is available at https://github.com/basiralab/NEURAL.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction</title>
<link>https://arxiv.org/abs/2508.09717</link>
<guid>https://arxiv.org/abs/2508.09717</guid>
<content:encoded><![CDATA[

arXiv:2508.09717v1 Announce Type: cross 
Abstract: Glioblastoma is a highly invasive brain tumor with rapid progression rates. Recent studies have shown that glioblastoma molecular subtype classification serves as a significant biomarker for effective targeted therapy selection. However, this classification currently requires invasive tissue extraction for comprehensive histopathological analysis. Existing multimodal approaches combining MRI and histopathology images are limited and lack robust mechanisms for preserving shared structural information across modalities. In particular, graph-based models often fail to retain discriminative features within heterogeneous graphs, and structural reconstruction mechanisms for handling missing or incomplete modality data are largely underexplored. To address these limitations, we propose a novel sheaf-based framework for structure-aware and consistent fusion of MRI and histopathology data. Our model outperforms baseline methods and demonstrates robustness in incomplete or missing data scenarios, contributing to the development of virtual biopsy tools for rapid diagnostics. Our source code is available at https://github.com/basiralab/MMSN/.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Kernel Regression VAE: A Computationally Efficient Surrogate for GP-VAEs in ICA</title>
<link>https://arxiv.org/abs/2508.09721</link>
<guid>https://arxiv.org/abs/2508.09721</guid>
<content:encoded><![CDATA[

arXiv:2508.09721v1 Announce Type: cross 
Abstract: The interpretability of generative models is considered a key factor in demonstrating their effectiveness and controllability. The generated data are believed to be determined by latent variables that are not directly observable. Therefore, disentangling, decoupling, decomposing, causal inference, or performing Independent Component Analysis (ICA) in the latent variable space helps uncover the independent factors that influence the attributes or features affecting the generated outputs, thereby enhancing the interpretability of generative models. As a generative model, Variational Autoencoders (VAEs) combine with variational Bayesian inference algorithms. Using VAEs, the inverse process of ICA can be equivalently framed as a variational inference process. In some studies, Gaussian processes (GPs) have been introduced as priors for each dimension of latent variables in VAEs, structuring and separating each dimension from temporal or spatial perspectives, and encouraging different dimensions to control various attributes of the generated data. However, GPs impose a significant computational burden, resulting in substantial resource consumption when handling large datasets. Essentially, GPs model different temporal or spatial structures through various kernel functions. Structuring the priors of latent variables via kernel functions-so that different kernel functions model the correlations among sequence points within different latent dimensions-is at the core of achieving disentanglement in VAEs. The proposed Structured Kernel Regression VAE (SKR-VAE) leverages this core idea in a more efficient way, avoiding the costly kernel matrix inversion required in GPs. This research demonstrates that, while maintaining ICA performance, SKR-VAE achieves greater computational efficiency and significantly reduced computational burden compared to GP-VAE.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning</title>
<link>https://arxiv.org/abs/2508.09726</link>
<guid>https://arxiv.org/abs/2508.09726</guid>
<content:encoded><![CDATA[

arXiv:2508.09726v1 Announce Type: cross 
Abstract: Large language models trained with reinforcement learning with verifiable rewards tend to trade accuracy for length--inflating response lengths to achieve gains in accuracy. While longer answers may be warranted for harder problems, many tokens are merely "filler": repetitive, verbose text that makes no real progress. We introduce GFPO (Group Filtered Policy Optimization), which curbs this length explosion by sampling larger groups per problem during training and filtering responses to train on based on two key metrics: (1) response length and (2) token efficiency: reward per token ratio. By sampling more at training time, we teach models to think less at inference time. On the Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH, LiveCodeBench) while maintaining accuracy. Optimizing for reward per token further increases reductions in length inflation to 71-85%. We also propose Adaptive Difficulty GFPO, which dynamically allocates more training resources to harder problems based on real-time difficulty estimates, improving the balance between computational efficiency and accuracy especially on difficult questions. GFPO demonstrates that increased training-time compute directly translates to reduced test-time compute--a simple yet effective trade-off for efficient reasoning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhance the machine learning algorithm performance in phishing detection with keyword features</title>
<link>https://arxiv.org/abs/2508.09765</link>
<guid>https://arxiv.org/abs/2508.09765</guid>
<content:encoded><![CDATA[

arXiv:2508.09765v1 Announce Type: cross 
Abstract: Recently, we can observe a significant increase of the phishing attacks in the Internet. In a typical phishing attack, the attacker sets up a malicious website that looks similar to the legitimate website in order to obtain the end-users' information. This may cause the leakage of the sensitive information and the financial loss for the end-users. To avoid such attacks, the early detection of these websites' URLs is vital and necessary. Previous researchers have proposed many machine learning algorithms to distinguish the phishing URLs from the legitimate ones. In this paper, we would like to enhance these machine learning algorithms from the perspective of feature selection. We propose a novel method to incorporate the keyword features with the traditional features. This method is applied on multiple traditional machine learning algorithms and the experimental results have shown this method is useful and effective. On average, this method can reduce the classification error by 30% for the large dataset. Moreover, its enhancement is more significant for the small dataset. In addition, this method extracts the information from the URL and does not rely on the additional information provided by the third-part service. The best result for the machine learning algorithm using our proposed method has achieved the accuracy of 99.68%.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Speaker Anonymization Evaluation's Robustness to Target Speakers with Adversarial Learning</title>
<link>https://arxiv.org/abs/2508.09803</link>
<guid>https://arxiv.org/abs/2508.09803</guid>
<content:encoded><![CDATA[

arXiv:2508.09803v1 Announce Type: cross 
Abstract: The current privacy evaluation for speaker anonymization often overestimates privacy when a same-gender target selection algorithm (TSA) is used, although this TSA leaks the speaker's gender and should hence be more vulnerable. We hypothesize that this occurs because the evaluation does not account for the fact that anonymized speech contains information from both the source and target speakers. To address this, we propose to add a target classifier that measures the influence of target speaker information in the evaluation, which can also be removed with adversarial learning. Experiments demonstrate that this approach is effective for multiple anonymizers, particularly when using a same-gender TSA, leading to a more reliable assessment.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</title>
<link>https://arxiv.org/abs/2508.09811</link>
<guid>https://arxiv.org/abs/2508.09811</guid>
<content:encoded><![CDATA[

arXiv:2508.09811v1 Announce Type: cross 
Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical information just from dynamic multi-view videos in the absence of any human labels. By leveraging physics-informed losses as soft constraints or integrating simple physics models into neural nets, existing works often fail to learn complex motion physics, or doing so requires additional labels such as object types or masks. We propose a new framework named TRACE to model the motion physics of complex dynamic 3D scenes. The key novelty of our method is that, by formulating each 3D point as a rigid particle with size and orientation in space, we directly learn a translation rotation dynamics system for each particle, explicitly estimating a complete set of physical parameters to govern the particle's motion over time. Extensive experiments on three existing dynamic datasets and one newly created challenging synthetic datasets demonstrate the extraordinary performance of our method over baselines in the task of future frame extrapolation. A nice property of our framework is that multiple objects or parts can be easily segmented just by clustering the learned physical parameters.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians</title>
<link>https://arxiv.org/abs/2508.09830</link>
<guid>https://arxiv.org/abs/2508.09830</guid>
<content:encoded><![CDATA[

arXiv:2508.09830v1 Announce Type: cross 
Abstract: In this paper, we present a generalizable method for 3D surface reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from RGB images. Unlike existing coordinate-based methods which are often computationally intensive when rendering explicit surfaces, our proposed method, named RayletDF, introduces a new technique called raylet distance field, which aims to directly predict surface points from query rays. Our pipeline consists of three key modules: a raylet feature extractor, a raylet distance field predictor, and a multi-raylet blender. These components work together to extract fine-grained local geometric features, predict raylet distances, and aggregate multiple predictions to reconstruct precise surface points. We extensively evaluate our method on multiple public real-world datasets, demonstrating superior performance in surface reconstruction from point clouds or 3D Gaussians. Most notably, our method achieves exceptional generalization ability, successfully recovering 3D surfaces in a single-forward pass across unseen datasets in testing.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Generalization Limits of Quantum Generative Adversarial Networks with Pure State Generators</title>
<link>https://arxiv.org/abs/2508.09844</link>
<guid>https://arxiv.org/abs/2508.09844</guid>
<content:encoded><![CDATA[

arXiv:2508.09844v1 Announce Type: cross 
Abstract: We investigate the capabilities of Quantum Generative Adversarial Networks (QGANs) in image generations tasks. Our analysis centers on fully quantum implementations of both the generator and discriminator. Through extensive numerical testing of current main architectures, we find that QGANs struggle to generalize across datasets, converging on merely the average representation of the training data. When the output of the generator is a pure-state, we analytically derive a lower bound for the discriminator quality given by the fidelity between the pure-state output of the generator and the target data distribution, thereby providing a theoretical explanation for the limitations observed in current models. Our findings reveal fundamental challenges in the generalization capabilities of existing quantum generative models. While our analysis focuses on QGANs, the results carry broader implications for the performance of related quantum generative models.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Evaluation framework of Alignment Techniques for LLMs</title>
<link>https://arxiv.org/abs/2508.09937</link>
<guid>https://arxiv.org/abs/2508.09937</guid>
<content:encoded><![CDATA[

arXiv:2508.09937v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) become increasingly integrated into real-world applications, ensuring their outputs align with human values and safety standards has become critical. The field has developed diverse alignment approaches including traditional fine-tuning methods (RLHF, instruction tuning), post-hoc correction systems, and inference-time interventions, each with distinct advantages and limitations. However, the lack of unified evaluation frameworks makes it difficult to systematically compare these paradigms and guide deployment decisions. This paper introduces a multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive evaluation framework that provides a systematic comparison across all major alignment paradigms. Our framework assesses methods along four key dimensions: alignment detection, alignment quality, computational efficiency, and robustness. Through experiments across diverse base models and alignment strategies, we demonstrate the utility of our framework in identifying strengths and limitations of current state-of-the-art models, providing valuable insights for future research directions.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable Diffusion Models are Secretly Good at Visual In-Context Learning</title>
<link>https://arxiv.org/abs/2508.09949</link>
<guid>https://arxiv.org/abs/2508.09949</guid>
<content:encoded><![CDATA[

arXiv:2508.09949v1 Announce Type: cross 
Abstract: Large language models (LLM) in natural language processing (NLP) have demonstrated great potential for in-context learning (ICL) -- the ability to leverage a few sets of example prompts to adapt to various tasks without having to explicitly update the model weights. ICL has recently been explored for computer vision tasks with promising early outcomes. These approaches involve specialized training and/or additional data that complicate the process and limit its generalizability. In this work, we show that off-the-shelf Stable Diffusion models can be repurposed for visual in-context learning (V-ICL). Specifically, we formulate an in-place attention re-computation within the self-attention layers of the Stable Diffusion architecture that explicitly incorporates context between the query and example prompts. Without any additional fine-tuning, we show that this repurposed Stable Diffusion model is able to adapt to six different tasks: foreground segmentation, single object detection, semantic segmentation, keypoint detection, edge detection, and colorization. For example, the proposed approach improves the mean intersection over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by 8.9% and 3.2% over recent methods such as Visual Prompting and IMProv, respectively. Additionally, we show that the proposed method is able to effectively leverage multiple prompts through ensembling to infer the task better and further improve the performance.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Specialised or Generic? Tokenization Choices for Radiology Language Models</title>
<link>https://arxiv.org/abs/2508.09952</link>
<guid>https://arxiv.org/abs/2508.09952</guid>
<content:encoded><![CDATA[

arXiv:2508.09952v1 Announce Type: cross 
Abstract: The vocabulary used by language models (LM) - defined by the tokenizer - plays a key role in text generation quality. However, its impact remains under-explored in radiology. In this work, we address this gap by systematically comparing general, medical, and domain-specific tokenizers on the task of radiology report summarisation across three imaging modalities. We also investigate scenarios with and without LM pre-training on PubMed abstracts. Our findings demonstrate that medical and domain-specific vocabularies outperformed widely used natural language alternatives when models are trained from scratch. Pre-training partially mitigates performance differences between tokenizers, whilst the domain-specific tokenizers achieve the most favourable results. Domain-specific tokenizers also reduce memory requirements due to smaller vocabularies and shorter sequences. These results demonstrate that adapting the vocabulary of LMs to the clinical domain provides practical benefits, including improved performance and reduced computational demands, making such models more accessible and effective for both research and real-world healthcare settings.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks</title>
<link>https://arxiv.org/abs/2508.09958</link>
<guid>https://arxiv.org/abs/2508.09958</guid>
<content:encoded><![CDATA[

arXiv:2508.09958v1 Announce Type: cross 
Abstract: With the increasing popularity of large language models (LLMs) for a variety of tasks, there has been a growing interest in strategies that can predict which out of a set of LLMs will yield a successful answer at low cost. This problem promises to become more and more relevant as providers like Microsoft allow users to easily create custom LLM "assistants" specialized to particular types of queries. However, some tasks (i.e., queries) may be too specialized and difficult for a single LLM to handle alone. These applications often benefit from breaking down the task into smaller subtasks, each of which can then be executed by a LLM expected to perform well on that specific subtask. For example, in extracting a diagnosis from medical records, one can first select an LLM to summarize the record, select another to validate the summary, and then select another, possibly different, LLM to extract the diagnosis from the summarized record. Unlike existing LLM selection or routing algorithms, this setting requires that we select a sequence of LLMs, with the output of each LLM feeding into the next and potentially influencing its success. Thus, unlike single LLM selection, the quality of each subtask's output directly affects the inputs, and hence the cost and success rate, of downstream LLMs, creating complex performance dependencies that must be learned and accounted for during selection. We propose a neural contextual bandit-based algorithm that trains neural networks that model LLM success on each subtask in an online manner, thus learning to guide the LLM selections for the different subtasks, even in the absence of historical LLM performance data. Experiments on telecommunications question answering and medical diagnosis prediction datasets illustrate the effectiveness of our proposed approach compared to other LLM selection algorithms.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation</title>
<link>https://arxiv.org/abs/2508.09960</link>
<guid>https://arxiv.org/abs/2508.09960</guid>
<content:encoded><![CDATA[

arXiv:2508.09960v1 Announce Type: cross 
Abstract: The creation of human-like humanoid robots is hindered by a fundamental fragmentation: data processing and learning algorithms are rarely universal across different robot morphologies. This paper introduces the Generalized Behavior Cloning (GBC) framework, a comprehensive and unified solution designed to solve this end-to-end challenge. GBC establishes a complete pathway from human motion to robot action through three synergistic innovations. First, an adaptive data pipeline leverages a differentiable IK network to automatically retarget any human MoCap data to any humanoid. Building on this foundation, our novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust, high-fidelity imitation policies. To complete the ecosystem, the entire framework is delivered as an efficient, open-source platform based on Isaac Lab, empowering the community to deploy the full workflow via simple configuration scripts. We validate the power and generality of GBC by training policies on multiple heterogeneous humanoids, demonstrating excellent performance and transfer to novel motions. This work establishes the first practical and unified pathway for creating truly generalized humanoid controllers.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Story2Board: A Training-Free Approach for Expressive Storyboard Generation</title>
<link>https://arxiv.org/abs/2508.09983</link>
<guid>https://arxiv.org/abs/2508.09983</guid>
<content:encoded><![CDATA[

arXiv:2508.09983v1 Announce Type: cross 
Abstract: We present Story2Board, a training-free framework for expressive storyboard generation from natural language. Existing methods narrowly focus on subject identity, overlooking key aspects of visual storytelling such as spatial composition, background evolution, and narrative pacing. To address this, we introduce a lightweight consistency framework composed of two components: Latent Panel Anchoring, which preserves a shared character reference across panels, and Reciprocal Attention Value Mixing, which softly blends visual features between token pairs with strong reciprocal attention. Together, these mechanisms enhance coherence without architectural changes or fine-tuning, enabling state-of-the-art diffusion models to generate visually diverse yet consistent storyboards. To structure generation, we use an off-the-shelf language model to convert free-form stories into grounded panel-level prompts. To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain narratives designed to assess layout diversity and background-grounded storytelling, in addition to consistency. We also introduce a new Scene Diversity metric that quantifies spatial and pose variation across storyboards. Our qualitative and quantitative results, as well as a user study, show that Story2Board produces more dynamic, coherent, and narratively engaging storyboards than existing baselines.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEAVES: Learning Views for Time-Series Biobehavioral Data in Contrastive Learning</title>
<link>https://arxiv.org/abs/2210.07340</link>
<guid>https://arxiv.org/abs/2210.07340</guid>
<content:encoded><![CDATA[

arXiv:2210.07340v2 Announce Type: replace 
Abstract: Contrastive learning has been utilized as a promising self-supervised learning approach to extract meaningful representations from unlabeled data. The majority of these methods take advantage of data-augmentation techniques to create diverse views from the original input. However, optimizing augmentations and their parameters for generating more effective views in contrastive learning frameworks is often resource-intensive and time-consuming. While several strategies have been proposed for automatically generating new views in computer vision, research in other domains, such as time-series biobehavioral data, remains limited. In this paper, we introduce a simple yet powerful module for automatic view generation in contrastive learning frameworks applied to time-series biobehavioral data, which is essential for modern health care, termed learning views for time-series data (LEAVES). This proposed module employs adversarial training to learn augmentation hyperparameters within contrastive learning frameworks. We assess the efficacy of our method on multiple time-series datasets using two well-known contrastive learning frameworks, namely SimCLR and BYOL. Across four diverse biobehavioral datasets, LEAVES requires only approximately 20 learnable parameters -- dramatically fewer than the about 580k parameters demanded by frameworks like ViewMaker, a previously proposed adversarially trained convolutional module in contrastive learning, while achieving competitive and often superior performance to existing baseline methods. Crucially, these efficiency gains are obtained without extensive manual hyperparameter tuning, which makes LEAVES particularly suitable for large-scale or real-time healthcare applications that demand both accuracy and practicality.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting steam mass flow in power plants using the parallel hybrid network</title>
<link>https://arxiv.org/abs/2307.09483</link>
<guid>https://arxiv.org/abs/2307.09483</guid>
<content:encoded><![CDATA[

arXiv:2307.09483v3 Announce Type: replace 
Abstract: Efficient and sustainable power generation is a crucial concern in the energy sector. In particular, thermal power plants grapple with accurately predicting steam mass flow, which is crucial for operational efficiency and cost reduction. In this study, we use a parallel hybrid neural network architecture that combines a parametrized quantum circuit and a conventional feed-forward neural network specifically designed for time-series prediction in industrial settings to enhance predictions of steam mass flow 15 minutes into the future. Our results show that the parallel hybrid model outperforms standalone classical and quantum models, achieving more than 5.7 and 4.9 times lower mean squared error loss on the test set after training compared to pure classical and pure quantum networks, respectively. Furthermore, the hybrid model demonstrates smaller relative errors between the ground truth and the model predictions on the test set, up to 2 times better than the pure classical model. These findings contribute to the broader scientific understanding of how integrating quantum and classical machine learning techniques can be applied to real-world challenges faced by the energy sector, ultimately leading to optimized power plant operations. To our knowledge, this study constitutes the first parallel hybrid quantum-classical architecture deployed on a real-world power-plant dataset, illustrating how near-term quantum resources can already augment classical analytics in the energy sector.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Bandit Learning for Monotone Stochastic Optimization</title>
<link>https://arxiv.org/abs/2312.15427</link>
<guid>https://arxiv.org/abs/2312.15427</guid>
<content:encoded><![CDATA[

arXiv:2312.15427v2 Announce Type: replace 
Abstract: Stochastic optimization is a widely used approach for optimization under uncertainty, where uncertain input parameters are modeled by random variables. Exact or approximation algorithms have been obtained for several fundamental problems in this area. However, a significant limitation of this approach is that it requires full knowledge of the underlying probability distributions. Can we still get good (approximation) algorithms if these distributions are unknown, and the algorithm needs to learn them through repeated interactions? In this paper, we resolve this question for a large class of ''monotone'' stochastic problems, by providing a generic online learning algorithm with $\sqrt{T\log(T)}$ regret relative to the best approximation algorithm (under known distributions). Importantly, our online algorithm works in a semi-bandit setting, where in each period, the algorithm only observes samples from the random variables that were actually probed. Moreover, our result extends to settings with censored and binary feedback, where the policy only observes truncated or thresholded versions of the probed variables. Our framework applies to several fundamental problems such as prophet inequality, Pandora's box, stochastic knapsack, single-resource revenue management and sequential posted pricing.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Neural Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2402.11628</link>
<guid>https://arxiv.org/abs/2402.11628</guid>
<content:encoded><![CDATA[

arXiv:2402.11628v3 Announce Type: replace 
Abstract: Neural algorithmic reasoning aims to capture computations with neural networks by training models to imitate the execution of classical algorithms. While common architectures are expressive enough to contain the correct model in the weight space, current neural reasoners struggle to generalize well on out-of-distribution data. On the other hand, classical computations are not affected by distributional shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. To achieve this, we separate discrete and continuous data flows and describe the interaction between them. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on multiple algorithmic problems and achieve perfect test scores both in single-task and multitask setups. Moreover, the proposed architectural choice allows us to prove the correctness of the learned algorithms for any test data.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Defer in Congested Systems: The AI-Human Interplay</title>
<link>https://arxiv.org/abs/2402.12237</link>
<guid>https://arxiv.org/abs/2402.12237</guid>
<content:encoded><![CDATA[

arXiv:2402.12237v4 Announce Type: replace 
Abstract: High-stakes applications rely on combining Artificial Intelligence (AI) and humans for responsive and reliable decision making. For example, content moderation in social media platforms often employs an AI-human pipeline to promptly remove policy violations without jeopardizing legitimate content. A typical heuristic estimates the risk of incoming content and uses fixed thresholds to decide whether to auto-delete the content (classification) and whether to send it for human review (admission). This approach can be inefficient as it disregards the uncertainty in AI's estimation, the time-varying element of content arrivals and human review capacity, and the selective sampling in the online dataset (humans only review content filtered by the AI).
  In this paper, we introduce a model to capture such an AI-human interplay. In this model, the AI observes contextual information for incoming jobs, makes classification and admission decisions, and schedules admitted jobs for human review. During these reviews, humans observe a job's true cost and may overturn an erroneous AI classification decision. These reviews also serve as new data to train the AI but are delayed due to congestion in the human review system. The objective is to minimize the costs of eventually misclassified jobs.
  We propose a near-optimal learning algorithm that carefully balances the classification loss from a selectively sampled dataset, the idiosyncratic loss of non-reviewed jobs, and the delay loss of having congestion in the human review system. To the best of our knowledge, this is the first result for online learning in contextual queueing systems. Moreover, numerical experiments based on online comment datasets show that our algorithm can substantially reduce the number of misclassifications compared to existing content moderation practice.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No-Regret M${}^{\natural}$-Concave Function Maximization: Stochastic Bandit Algorithms and Hardness of Adversarial Full-Information Setting</title>
<link>https://arxiv.org/abs/2405.12439</link>
<guid>https://arxiv.org/abs/2405.12439</guid>
<content:encoded><![CDATA[

arXiv:2405.12439v2 Announce Type: replace 
Abstract: M${}^{\natural}$-concave functions, a.k.a. gross substitute valuation functions, play a fundamental role in many fields, including discrete mathematics and economics. In practice, perfect knowledge of M${}^{\natural}$-concave functions is often unavailable a priori, and we can optimize them only interactively based on some feedback. Motivated by such situations, we study online M${}^{\natural}$-concave function maximization problems, which are interactive versions of the problem studied by Murota and Shioura (1999). For the stochastic bandit setting, we present $O(T^{-1/2})$-simple regret and $O(T^{2/3})$-regret algorithms under $T$ times access to unbiased noisy value oracles of M${}^{\natural}$-concave functions. A key to proving these results is the robustness of the greedy algorithm to local errors in M${}^{\natural}$-concave function maximization, which is one of our main technical results. While we obtain those positive results for the stochastic setting, another main result of our work is an impossibility in the adversarial setting. We prove that, even with full-information feedback, no algorithms that run in polynomial time per round can achieve $O(T^{1-c})$ regret for any constant $c > 0$. Our proof is based on a reduction from the matroid intersection problem for three matroids, which would be a novel approach to establishing the hardness in online learning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks</title>
<link>https://arxiv.org/abs/2405.15481</link>
<guid>https://arxiv.org/abs/2405.15481</guid>
<content:encoded><![CDATA[

arXiv:2405.15481v3 Announce Type: replace 
Abstract: The growing demands on GPU memory posed by the increasing number of neural network parameters call for training approaches that are more memory-efficient. Previous memory reduction training techniques, such as Low-Rank Adaptation (LoRA) and ReLoRA, face challenges, with LoRA being constrained by its low-rank structure, particularly during intensive tasks like pre-training, and ReLoRA suffering from saddle point issues. In this paper, we propose Sparse Spectral Training (SST) to optimize memory usage for pre-training. SST updates all singular values and selectively updates singular vectors through a multinomial sampling method weighted by the magnitude of the singular values. Furthermore, SST employs singular value decomposition to initialize and periodically reinitialize low-rank parameters, reducing distortion relative to full-rank training compared to other low-rank methods. Through comprehensive testing on both Euclidean and hyperbolic neural networks across various tasks, SST demonstrates its ability to outperform existing memory reduction training methods and is comparable to full-rank training in various cases. On LLaMA-1.3B, with only 18.7\% of the parameters trainable compared to full-rank training (using a rank equivalent to 6\% of the embedding dimension), SST reduces the perplexity gap between other low-rank methods and full-rank training by 97.4\%. This result highlights SST as an effective parameter-efficient technique for model pre-training.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal Data</title>
<link>https://arxiv.org/abs/2406.09864</link>
<guid>https://arxiv.org/abs/2406.09864</guid>
<content:encoded><![CDATA[

arXiv:2406.09864v3 Announce Type: replace 
Abstract: Multimodal Deep Learning enhances decision-making by integrating diverse information sources, such as texts, images, audio, and videos. To develop trustworthy multimodal approaches, it is essential to understand how uncertainty impacts these models. We propose LUMA, a unique multimodal dataset, featuring audio, image, and textual data from 50 classes, specifically designed for learning from uncertain data. It extends the well-known CIFAR 10/100 dataset with audio samples extracted from three audio corpora, and text data generated using the Gemma-7B Large Language Model (LLM). The LUMA dataset enables the controlled injection of varying types and degrees of uncertainty to achieve and tailor specific experiments and benchmarking initiatives. LUMA is also available as a Python package including the functions for generating multiple variants of the dataset with controlling the diversity of the data, the amount of noise for each modality, and adding out-of-distribution samples. A baseline pre-trained model is also provided alongside three uncertainty quantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable Conflictive Multi-View Learning. This comprehensive dataset and its tools are intended to promote and support the development, evaluation, and benchmarking of trustworthy and robust multimodal deep learning approaches. We anticipate that the LUMA dataset will help the research community to design more trustworthy and robust machine learning approaches for safety critical applications. The code and instructions for downloading and processing the dataset can be found at: https://github.com/bezirganyan/LUMA/ .
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Lag Transformer based on Time-Variable-Aware Learning for Explainable Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2408.16896</link>
<guid>https://arxiv.org/abs/2408.16896</guid>
<content:encoded><![CDATA[

arXiv:2408.16896v2 Announce Type: replace 
Abstract: Time series data is a key element of big data analytics, commonly found in domains such as finance, healthcare, climate forecasting, and transportation. In large scale real world settings, such data is often high dimensional and multivariate, requiring advanced forecasting methods that are both accurate and interpretable. Although Transformer based models perform well in multivariate time series forecasting (MTSF), their lack of explainability limits their use in critical applications. To overcome this, we propose Distributed Lag Transformer (DLFormer), a novel Transformer architecture for explainable and scalable MTSF. DLFormer integrates a distributed lag embedding and a time variable aware learning (TVAL) mechanism to structurally model both local and global temporal dependencies and explicitly capture the influence of past variables on future outcomes. Experiments on ten benchmark and real world datasets show that DLFormer achieves state of the art predictive accuracy while offering robust, interpretable insights into variable wise and temporal dynamics. These results highlight ability of DLFormer to bridge the gap between performance and explainability, making it highly suitable for practical big data forecasting tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning for Smart Grid: A Survey on Applications and Potential Vulnerabilities</title>
<link>https://arxiv.org/abs/2409.10764</link>
<guid>https://arxiv.org/abs/2409.10764</guid>
<content:encoded><![CDATA[

arXiv:2409.10764v3 Announce Type: replace 
Abstract: The Smart Grid (SG) is a critical energy infrastructure that collects real-time electricity usage data to forecast future energy demands using information and communication technologies (ICT). Due to growing concerns about data security and privacy in SGs, federated learning (FL) has emerged as a promising training framework. FL offers a balance between privacy, efficiency, and accuracy in SGs by enabling collaborative model training without sharing private data from IoT devices. In this survey, we thoroughly review recent advancements in designing FL-based SG systems across three stages: generation, transmission and distribution, and consumption. Additionally, we explore potential vulnerabilities that may arise when implementing FL in these stages. Furthermore, we discuss the gap between state-of-the-art (SOTA) FL research and its practical applications in SGs, and we propose future research directions. Unlike traditional surveys addressing security issues in centralized machine learning methods for SG systems, this survey is the first to specifically examine the applications and security concerns unique to FL-based SG systems. We also introduce FedGridShield, an open-source framework featuring implementations of SOTA attack and defense methods. Our aim is to inspire further research into applications and improvements in the robustness of FL-based SG systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Downscaling Extreme Precipitation with Wasserstein Regularized Diffusion</title>
<link>https://arxiv.org/abs/2410.00381</link>
<guid>https://arxiv.org/abs/2410.00381</guid>
<content:encoded><![CDATA[

arXiv:2410.00381v4 Announce Type: replace 
Abstract: Understanding the risks posed by extreme rainfall events requires analysis of precipitation fields with high resolution (to assess localized hazards) and extensive historical coverage (to capture sufficient examples of rare occurrences). Radar and mesonet networks provide precipitation fields at 1 km resolution but with limited historical and geographical coverage, while gauge-based records and reanalysis products cover decades of time on a global scale, but only at 30-50 km resolution. To help provide high-resolution precipitation estimates over long time scales, this study presents Wasserstein Regularized Diffusion (WassDiff), a diffusion framework to downscale (super-resolve) precipitation fields from low-resolution gauge and reanalysis products. Crucially, unlike related deep generative models, WassDiff integrates a Wasserstein distribution-matching regularizer to the denoising process to reduce empirical biases at extreme intensities. Comprehensive evaluations demonstrate that WassDiff quantitatively outperforms existing state-of-the-art generative downscaling methods at recovering extreme weather phenomena such as tropical storms and cold fronts. Case studies further qualitatively demonstrate WassDiff's ability to reproduce realistic fine-scale weather structures and accurate peak intensities. By unlocking decades of high-resolution rainfall information from globally available coarse records, WassDiff offers a practical pathway toward more accurate flood-risk assessments and climate-adaptation planning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiation Through Black-Box Quadratic Programming Solvers</title>
<link>https://arxiv.org/abs/2410.06324</link>
<guid>https://arxiv.org/abs/2410.06324</guid>
<content:encoded><![CDATA[

arXiv:2410.06324v3 Announce Type: replace 
Abstract: Differentiable optimization has attracted significant research interest, particularly for quadratic programming (QP). Existing approaches for differentiating the solution of a QP with respect to its defining parameters often rely on specific integrated solvers. This integration limits their applicability, including their use in neural network architectures and bi-level optimization tasks, restricting users to a narrow selection of solver choices. To address this limitation, we introduce dQP, a modular and solver-agnostic framework for plug-and-play differentiation of virtually any QP solver. Our key theoretical insight is that the solution and its derivative can each be expressed in terms of closely-related and simple linear systems by using the active set at the solution. This insight enables efficient decoupling of the QP's solution, obtained by any solver, from its differentiation. Our open-source, minimal-overhead implementation will be made publicly available and seamlessly integrates with more than 15 state-of-the-art solvers. Comprehensive benchmark experiments demonstrate dQP's robustness and scalability, particularly highlighting its advantages in large-scale sparse problems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Decision Transformer: External Memory for In-context RL</title>
<link>https://arxiv.org/abs/2410.07071</link>
<guid>https://arxiv.org/abs/2410.07071</guid>
<content:encoded><![CDATA[

arXiv:2410.07071v3 Announce Type: replace 
Abstract: In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, we introduce Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, we illuminate the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, we release datasets for four of the considered environments.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning</title>
<link>https://arxiv.org/abs/2411.02199</link>
<guid>https://arxiv.org/abs/2411.02199</guid>
<content:encoded><![CDATA[

arXiv:2411.02199v5 Announce Type: replace 
Abstract: Transformer-based large language models (LLMs) have displayed remarkable creative prowess and emergence capabilities. Existing empirical studies have revealed a strong connection between these LLMs' impressive emergence abilities and their in-context learning (ICL) capacity, allowing them to solve new tasks using only task-specific prompts without further fine-tuning. On the other hand, existing empirical and theoretical studies also show that there is a linear regularity of the multi-concept encoded semantic representation behind transformer-based LLMs. However, existing theoretical work fail to build up an understanding of the connection between this regularity and the innovative power of ICL. Additionally, prior work often focuses on simplified, unrealistic scenarios involving linear transformers or unrealistic loss functions, and they achieve only linear or sub-linear convergence rates. In contrast, this work provides a fine-grained mathematical analysis to show how transformers leverage the multi-concept semantics of words to enable powerful ICL and excellent out-of-distribution ICL abilities, offering insights into how transformers innovate solutions for certain unseen tasks encoded with multiple cross-concept semantics. Inspired by empirical studies on the linear latent geometry of LLMs, the analysis is based on a concept-based low-noise sparse coding prompt model. Leveraging advanced techniques, this work showcases the exponential 0-1 loss convergence over the highly non-convex training dynamics, which pioneeringly incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and cross-entropy loss. Empirical simulations corroborate the theoretical findings.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Feature Training of Thin 2-Layer Networks</title>
<link>https://arxiv.org/abs/2411.06848</link>
<guid>https://arxiv.org/abs/2411.06848</guid>
<content:encoded><![CDATA[

arXiv:2411.06848v2 Announce Type: replace 
Abstract: We consider the approximation of functions by 2-layer neural networks with a small number of hidden weights based on the squared loss and small datasets. Due to the highly non-convex energy landscape, gradient-based training often suffers from local minima. As a remedy, we initialize the hidden weights with samples from a learned proposal distribution, which we parameterize as a deep generative model. To train this model, we exploit the fact that with fixed hidden weights, the optimal output weights solve a linear equation. After learning the generative model, we refine the sampled weights with a gradient-based post-processing in the latent space. Here, we also include a regularization scheme to counteract potential noise. Finally, we demonstrate the effectiveness of our approach by numerical examples.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Out-of-distribution Robustness in the Presence of Unobserved Confounders</title>
<link>https://arxiv.org/abs/2411.19923</link>
<guid>https://arxiv.org/abs/2411.19923</guid>
<content:encoded><![CDATA[

arXiv:2411.19923v2 Announce Type: replace 
Abstract: We consider the task of out-of-distribution (OOD) generalization, where the distribution shift is due to an unobserved confounder ($Z$) affecting both the covariates ($X$) and the labels ($Y$). This confounding introduces heterogeneity in the predictor, i.e., $P(Y | X) = E_{P(Z | X)}[P(Y | X,Z)]$, making traditional covariate and label shift assumptions unsuitable. OOD generalization differs from traditional domain adaptation in that it does not assume access to the covariate distribution ($X^\text{te}$) of the test samples during training. These conditions create a challenging scenario for OOD robustness: (a) $Z^\text{tr}$ is an unobserved confounder during training, (b) $P^\text{te}(Z) \neq P^\text{tr}(Z)$, (c) $X^\text{te}$ is unavailable during training, and (d) the predictive distribution depends on $P^\text{te}(Z)$. While prior work has developed complex predictors requiring multiple additional variables for identifiability of the latent distribution, we explore a set of identifiability assumptions that yield a surprisingly simple predictor using only a single additional variable. Our approach demonstrates superior empirical performance on several benchmark tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Indirect Query Bayesian Optimization with Integrated Feedback</title>
<link>https://arxiv.org/abs/2412.13559</link>
<guid>https://arxiv.org/abs/2412.13559</guid>
<content:encoded><![CDATA[

arXiv:2412.13559v2 Announce Type: replace 
Abstract: We develop the framework of Indirect Query Bayesian Optimization (IQBO), a new class of Bayesian optimization problems where the integrated feedback is given via a conditional expectation of the unknown function $f$ to be optimized. The underlying conditional distribution can be unknown and learned from data. The goal is to find the global optimum of $f$ by adaptively querying and observing in the space transformed by the conditional distribution. This is motivated by real-world applications where one cannot access direct feedback due to privacy, hardware or computational constraints. We propose the Conditional Max-Value Entropy Search (CMES) acquisition function to address this novel setting, and propose a hierarchical search algorithm with multi-resolution feedback to improve computational efficiency. We show regret bounds for our proposed methods and demonstrate the effectiveness of our approaches on simulated optimization tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Bio-Inspired Models under Different Learning Settings For Energy Efficiency in Network Traffic Prediction</title>
<link>https://arxiv.org/abs/2412.17565</link>
<guid>https://arxiv.org/abs/2412.17565</guid>
<content:encoded><![CDATA[

arXiv:2412.17565v2 Announce Type: replace 
Abstract: Cellular traffic forecasting is a critical task that enables network operators to efficiently allocate resources and address anomalies in rapidly evolving environments. The exponential growth of data collected from base stations poses significant challenges to processing and analysis. While machine learning (ML) algorithms have emerged as powerful tools for handling these large datasets and providing accurate predictions, their environmental impact, particularly in terms of energy consumption, is often overlooked in favor of their predictive capabilities. This study investigates the potential of two bio-inspired models: Spiking Neural Networks (SNNs) and Reservoir Computing through Echo State Networks (ESNs) for cellular traffic forecasting. The evaluation focuses on both their predictive performance and energy efficiency. These models are implemented in both centralized and federated settings to analyze their effectiveness and energy consumption in decentralized systems. Additionally, we compare bio-inspired models with traditional architectures, such as Convolutional Neural Networks (CNNs) and Multi-Layer Perceptrons (MLPs), to provide a comprehensive evaluation. Using data collected from three diverse locations in Barcelona, Spain, we examine the trade-offs between predictive accuracy and energy demands across these approaches. The results indicate that bio-inspired models, such as SNNs and ESNs, can achieve significant energy savings while maintaining predictive accuracy comparable to traditional architectures. Furthermore, federated implementations were tested to evaluate their energy efficiency in decentralized settings compared to centralized systems, particularly in combination with bio-inspired models. These findings offer valuable insights into the potential of bio-inspired models for sustainable and privacy-preserving cellular traffic forecasting.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVICAD2: Multi-View Independent Component Analysis with Delays and Dilations</title>
<link>https://arxiv.org/abs/2501.07426</link>
<guid>https://arxiv.org/abs/2501.07426</guid>
<content:encoded><![CDATA[

arXiv:2501.07426v2 Announce Type: replace 
Abstract: Machine learning techniques in multi-view settings face significant challenges, particularly when integrating heterogeneous data, aligning feature spaces, and managing view-specific biases. These issues are prominent in neuroscience, where data from multiple subjects exposed to the same stimuli are analyzed to uncover brain activity dynamics. In magnetoencephalography (MEG), where signals are captured at the scalp level, estimating the brain's underlying sources is crucial, especially in group studies where sources are assumed to be similar for all subjects. Common methods, such as Multi-View Independent Component Analysis (MVICA), assume identical sources across subjects, but this assumption is often too restrictive due to individual variability and age-related changes. Multi-View Independent Component Analysis with Delays (MVICAD) addresses this by allowing sources to differ up to a temporal delay. However, temporal dilation effects, particularly in auditory stimuli, are common in brain dynamics, making the estimation of time delays alone insufficient. To address this, we propose Multi-View Independent Component Analysis with Delays and Dilations (MVICAD2), which allows sources to differ across subjects in both temporal delays and dilations. We present a model with identifiable sources, derive an approximation of its likelihood in closed form, and use regularization and optimization techniques to enhance performance. Through simulations, we demonstrate that MVICAD2 outperforms existing multi-view ICA methods. We further validate its effectiveness using the Cam-CAN dataset, and showing how delays and dilations are related to aging.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction of Classifiers with Many Classes based on Noisy Labels</title>
<link>https://arxiv.org/abs/2501.12749</link>
<guid>https://arxiv.org/abs/2501.12749</guid>
<content:encoded><![CDATA[

arXiv:2501.12749v2 Announce Type: replace 
Abstract: Conformal Prediction (CP) controls the prediction uncertainty of classification systems by producing a small prediction set, ensuring a predetermined probability that the true class lies within this set. This is commonly done by defining a score, based on the model predictions, and setting a threshold on this score using a validation set. In this study, we address the problem of CP calibration when we only have access to a calibration set with noisy labels. We show how we can estimate the noise-free conformal threshold based on the noisy labeled data. We derive a finite sample coverage guarantee for uniform noise that remains effective even in tasks with a large number of classes. We dub our approach Noise-Aware Conformal Prediction (NACP). We illustrate the performance of the proposed results on several standard image classification datasets with a large number of classes.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pivoting Factorization: A Compact Meta Low-Rank Representation of Sparsity for Efficient Inference in Large Language Models</title>
<link>https://arxiv.org/abs/2501.19090</link>
<guid>https://arxiv.org/abs/2501.19090</guid>
<content:encoded><![CDATA[

arXiv:2501.19090v3 Announce Type: replace 
Abstract: The rapid growth of Large Language Models has driven demand for effective model compression techniques to reduce memory and computation costs. Low-rank pruning has gained attention for its GPU compatibility across all densities. However, low-rank pruning struggles to match the performance of semi-structured pruning, often doubling perplexity at similar densities. In this paper, we propose Pivoting Factorization (PIFA), a novel lossless meta low-rank representation that unsupervisedly learns a compact form of any low-rank representation, effectively eliminating redundant information. PIFA identifies pivot rows (linearly independent rows) and expresses non-pivot rows as linear combinations, achieving 24.2% additional memory savings and 24.6% faster inference over low-rank layers at rank = 50% of dimension. To mitigate the performance degradation caused by low-rank pruning, we introduce a novel, retraining-free reconstruction method that minimizes error accumulation (M). MPIFA, combining M and PIFA into an end-to-end framework, significantly outperforms existing low-rank pruning methods, and achieves performance comparable to semi-structured pruning, while surpassing it in GPU efficiency and compatibility. Our code is available at https://github.com/biomedical-cybernetics/pivoting-factorization.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Linear Recurrent Neural Networks for the Edge with Unstructured Sparsity</title>
<link>https://arxiv.org/abs/2502.01330</link>
<guid>https://arxiv.org/abs/2502.01330</guid>
<content:encoded><![CDATA[

arXiv:2502.01330v2 Announce Type: replace 
Abstract: Linear recurrent neural networks enable powerful long-range sequence modeling with constant memory usage and time-per-token during inference. These architectures hold promise for streaming applications at the edge, but deployment in resource-constrained environments requires hardware-aware optimizations to minimize latency and energy consumption. Unstructured sparsity offers a compelling solution, enabling substantial reductions in compute and memory requirements--when accelerated by compatible hardware platforms. In this paper, we conduct a scaling study to investigate the Pareto front of performance and efficiency across inference compute budgets. We find that highly sparse linear RNNs consistently achieve better efficiency-performance trade-offs than dense baselines, with 2x less compute and 36% less memory at iso-accuracy. Our models achieve state-of-the-art results on a real-time streaming task for audio denoising. By quantizing our sparse models to fixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic chip for real-time processing, we translate model compression into tangible gains of 42x lower latency and 149x lower energy consumption compared to a dense model on an edge GPU. Our findings showcase the transformative potential of unstructured sparsity, paving the way for highly efficient recurrent neural networks in real-world, resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEAPS: A discrete neural sampler via locally equivariant networks</title>
<link>https://arxiv.org/abs/2502.10843</link>
<guid>https://arxiv.org/abs/2502.10843</guid>
<content:encoded><![CDATA[

arXiv:2502.10843v2 Announce Type: replace 
Abstract: We propose "LEAPS", an algorithm to sample from discrete distributions known up to normalization by learning a rate matrix of a continuous-time Markov chain (CTMC). LEAPS can be seen as a continuous-time formulation of annealed importance sampling and sequential Monte Carlo methods, extended so that the variance of the importance weights is offset by the inclusion of the CTMC. To derive these importance weights, we introduce a set of Radon-Nikodym derivatives of CTMCs over their path measures. Because the computation of these weights is intractable with standard neural network parameterizations of rate matrices, we devise a new compact representation for rate matrices via what we call "locally equivariant" functions. To parameterize them, we introduce a family of locally equivariant multilayer perceptrons, attention layers, and convolutional networks, and provide an approach to make deep networks that preserve the local equivariance. This property allows us to propose a scalable training algorithm for the rate matrix such that the variance of the importance weights associated to the CTMC are minimal. We demonstrate the efficacy of LEAPS on problems in statistical physics.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast, Accurate Manifold Denoising by Tunneling Riemannian Optimization</title>
<link>https://arxiv.org/abs/2502.16819</link>
<guid>https://arxiv.org/abs/2502.16819</guid>
<content:encoded><![CDATA[

arXiv:2502.16819v2 Announce Type: replace 
Abstract: Learned denoisers play a fundamental role in various signal generation (e.g., diffusion models) and reconstruction (e.g., compressed sensing) architectures, whose success derives from their ability to leverage low-dimensional structure in data. Existing denoising methods, however, either rely on local approximations that require a linear scan of the entire dataset or treat denoising as generic function approximation problems, often sacrificing efficiency and interpretability. We consider the problem of efficiently denoising a new noisy data point sampled from an unknown $d$-dimensional manifold $M \in \mathbb{R}^D$, using only noisy samples. This work proposes a framework for test-time efficient manifold denoising, by framing the concept of "learning-to-denoise" as "learning-to-optimize". We have two technical innovations: (i) online learning methods which learn to optimize over the manifold of clean signals using only noisy data, effectively "growing" an optimizer one sample at a time. (ii) mixed-order methods which guarantee that the learned optimizers achieve global optimality, ensuring both efficiency and near-optimal denoising performance. We corroborate these claims with theoretical analyses of both the complexity and denoising performance of mixed-order traversal. Our experiments on scientific manifolds demonstrate significantly improved complexity-performance tradeoffs compared to nearest neighbor search, which underpins existing provable denoising approaches based on exhaustive search.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-shot Optimized Steering Vectors Mediate Safety-relevant Behaviors in LLMs</title>
<link>https://arxiv.org/abs/2502.18862</link>
<guid>https://arxiv.org/abs/2502.18862</guid>
<content:encoded><![CDATA[

arXiv:2502.18862v2 Announce Type: replace 
Abstract: Steering vectors (SVs) have emerged as a promising approach for interpreting and controlling LLMs, but current methods typically require large contrastive datasets that are often impractical to construct and may capture spurious correlations. We propose directly optimizing SVs through gradient descent on a single training example, and systematically investigate how these SVs generalize. We consider several SV optimization techniques and find that the resulting SVs effectively mediate safety-relevant behaviors in multiple models. Indeed, in experiments on an alignment-faking model, we are able to optimize one-shot SVs that induce harmful behavior on benign examples and whose negations suppress harmful behavior on malign examples. And in experiments on refusal suppression, we demonstrate that one-shot optimized SVs can transfer across inputs, yielding a Harmbench attack success rate of 96.9%. Furthermore, we extend work on "emergent misalignment" and show that SVs optimized to induce a model to write vulnerable code cause the model to respond harmfully on unrelated open-ended prompts. Finally, we use one-shot SV optimization to investigate how an instruction-tuned LLM recovers from outputting false information, and find that this ability is independent of the model's explicit verbalization that the information was false. Overall, our findings suggest that optimizing SVs on a single example can mediate a wide array of misaligned behaviors in LLMs. Code can be found at https://github.com/jacobdunefsky/one-shot-steering-repro and https://github.com/jacobdunefsky/one-shot-steering-misalignment.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.20089</link>
<guid>https://arxiv.org/abs/2502.20089</guid>
<content:encoded><![CDATA[

arXiv:2502.20089v2 Announce Type: replace 
Abstract: We propose a novel Inverse Reinforcement Learning (IRL) method that mitigates the rigidity of fixed reward structures and the limited flexibility of implicit reward regularization. Building on the Maximum Entropy IRL framework, our approach incorporates a squared temporal-difference (TD) regularizer with adaptive targets that evolve dynamically during training, thereby imposing adaptive bounds on recovered rewards and promoting robust decision-making. To capture richer return information, we integrate distributional RL into the learning process. Empirically, our method achieves expert-level performance on complex MuJoCo tasks, surpassing baseline methods on the Humanoid task with 3 demonstrations. Extensive experiments and ablation studies further validate the effectiveness of the approach and provide insights into reward dynamics in imitation learning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underdamped Diffusion Bridges with Applications to Sampling</title>
<link>https://arxiv.org/abs/2503.01006</link>
<guid>https://arxiv.org/abs/2503.01006</guid>
<content:encoded><![CDATA[

arXiv:2503.01006v2 Announce Type: replace 
Abstract: We provide a general framework for learning diffusion bridges that transport prior to target distributions. It includes existing diffusion models for generative modeling, but also underdamped versions with degenerate diffusion matrices, where the noise only acts in certain dimensions. Extending previous findings, our framework allows to rigorously show that score matching in the underdamped case is indeed equivalent to maximizing a lower bound on the likelihood. Motivated by superior convergence properties and compatibility with sophisticated numerical integration schemes of underdamped stochastic processes, we propose \emph{underdamped diffusion bridges}, where a general density evolution is learned rather than prescribed by a fixed noising process. We apply our method to the challenging task of sampling from unnormalized densities without access to samples from the target distribution. Across a diverse range of sampling problems, our approach demonstrates state-of-the-art performance, notably outperforming alternative methods, while requiring significantly fewer discretization steps and no hyperparameter tuning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shifting Perspectives: Steering Vectors for Robust Bias Mitigation in LLMs</title>
<link>https://arxiv.org/abs/2503.05371</link>
<guid>https://arxiv.org/abs/2503.05371</guid>
<content:encoded><![CDATA[

arXiv:2503.05371v2 Announce Type: replace 
Abstract: We present a novel approach to bias mitigation in large language models (LLMs) by applying steering vectors to modify model activations in forward passes. We compute 8 steering vectors, each corresponding to a different social bias axis, such as age, gender, or race, on a training subset of the BBQ dataset and compare the effectiveness of these to 3 additional bias mitigation methods across 4 datasets. When optimized on the BBQ dataset, our individually tuned steering vectors achieve average improvements of 12.8% on BBQ, 8.3% on CLEAR-Bias, and 1% on StereoSet, and show improvements over prompting and Self-Debias in all cases, and improvements over fine-tuning in 12 out of 17 evaluations. In addition, steering vectors showed the lowest impact on MMLU scores of the four bias mitigation methods tested. The work presents the first systematic investigation of steering vectors for bias mitigation, and we demonstrate that they are a powerful and computationally efficient strategy for reducing bias in LLMs, with broader implications for enhancing AI safety.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mosaic: Composite Projection Pruning for Resource-efficient LLMs</title>
<link>https://arxiv.org/abs/2504.06323</link>
<guid>https://arxiv.org/abs/2504.06323</guid>
<content:encoded><![CDATA[

arXiv:2504.06323v2 Announce Type: replace 
Abstract: Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models. Mosaic is available for public use from https://github.com/blessonvar/Mosaic
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedRecon: Missing Modality Reconstruction in Heterogeneous Distributed Environments</title>
<link>https://arxiv.org/abs/2504.09941</link>
<guid>https://arxiv.org/abs/2504.09941</guid>
<content:encoded><![CDATA[

arXiv:2504.09941v3 Announce Type: replace 
Abstract: Multimodal data are often incomplete and exhibit Non-Independent and Identically Distributed (Non-IID) characteristics in real-world scenarios. These inherent limitations lead to both modality heterogeneity through partial modality absence and data heterogeneity from distribution divergence, creating fundamental challenges for effective federated learning (FL). To address these coupled challenges, we propose FedRecon, the first method targeting simultaneous missing modality reconstruction and Non-IID adaptation in multimodal FL. Our approach first employs a lightweight Multimodal Variational Autoencoder (MVAE) to reconstruct missing modalities while preserving cross-modal consistency. Distinct from conventional imputation methods, we achieve sample-level alignment through a novel distribution mapping mechanism that guarantees both data consistency and completeness. Additionally, we introduce a strategy employing global generator freezing to prevent catastrophic forgetting, which in turn mitigates Non-IID fluctuations. Extensive evaluations on multimodal datasets demonstrate FedRecon's superior performance in modality reconstruction under Non-IID conditions, surpassing state-of-the-art methods. The code will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dequantified Diffusion-Schr{\"o}dinger Bridge for Density Ratio Estimation</title>
<link>https://arxiv.org/abs/2505.05034</link>
<guid>https://arxiv.org/abs/2505.05034</guid>
<content:encoded><![CDATA[

arXiv:2505.05034v4 Announce Type: replace 
Abstract: Density ratio estimation is fundamental to tasks involving $f$-divergences, yet existing methods often fail under significantly different distributions or inadequately overlapping supports -- the density-chasm and the support-chasm problems. Additionally, prior approaches yield divergent time scores near boundaries, leading to instability. We design $\textbf{D}^3\textbf{RE}$, a unified framework for \textbf{robust}, \textbf{stable} and \textbf{efficient} density ratio estimation. We propose the dequantified diffusion bridge interpolant (DDBI), which expands support coverage and stabilizes time scores via diffusion bridges and Gaussian dequantization. Building on DDBI, the proposed dequantified Schr{\"o}dinger bridge interpolant (DSBI) incorporates optimal transport to solve the Schr{\"o}dinger bridge problem, enhancing accuracy and efficiency. Our method offers uniform approximation and bounded time scores in theory, and outperforms baselines empirically in mutual information and density estimation tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Halting Recurrent GNNs and the Graded $\mu$-Calculus</title>
<link>https://arxiv.org/abs/2505.11050</link>
<guid>https://arxiv.org/abs/2505.11050</guid>
<content:encoded><![CDATA[

arXiv:2505.11050v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) are a class of machine-learning models that operate on graph-structured data. Their expressive power is intimately related to logics that are invariant under graded bisimilarity. Current proposals for recurrent GNNs either assume that the graph size is given to the model, or suffer from a lack of termination guarantees. In this paper, we propose a halting mechanism for recurrent GNNs. We prove that our halting model can express all node classifiers definable in graded modal mu-calculus, even for the standard GNN variant that is oblivious to the graph size. To prove our main result, we develop a new approximate semantics for graded mu-calculus, which we believe to be of independent interest. We leverage this new semantics into a new model-checking algorithm, called the counting algorithm, which is oblivious to the graph size. In a final step we show that the counting algorithm can be implemented on a halting recurrent GNN.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Nonlinear Implicit Bias via Region Counts in Input Space</title>
<link>https://arxiv.org/abs/2505.11370</link>
<guid>https://arxiv.org/abs/2505.11370</guid>
<content:encoded><![CDATA[

arXiv:2505.11370v3 Announce Type: replace 
Abstract: One explanation for the strong generalization ability of neural networks is implicit bias. Yet, the definition and mechanism of implicit bias in non-linear contexts remains little understood. In this work, we propose to characterize implicit bias by the count of connected regions in the input space with the same predicted label. Compared with parameter-dependent metrics (e.g., norm or normalized margin), region count can be better adapted to nonlinear, overparameterized models, because it is determined by the function mapping and is invariant to reparametrization. Empirically, we found that small region counts align with geometrically simple decision boundaries and correlate well with good generalization performance. We also observe that good hyper-parameter choices such as larger learning rates and smaller batch sizes can induce small region counts. We further establish the theoretical connections and explain how larger learning rate can induce small region counts in neural networks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18433</link>
<guid>https://arxiv.org/abs/2505.18433</guid>
<content:encoded><![CDATA[

arXiv:2505.18433v2 Announce Type: replace 
Abstract: Actor-critic methods for decentralized multi-agent reinforcement learning (MARL) facilitate collaborative optimal decision making without centralized coordination, thus enabling a wide range of applications in practice. To date, however, most theoretical convergence studies for existing actor-critic decentralized MARL methods are limited to the guarantee of a stationary solution under the linear function approximation. This leaves a significant gap between the highly successful use of deep neural actor-critic for decentralized MARL in practice and the current theoretical understanding. To bridge this gap, in this paper, we make the first attempt to develop a deep neural actor-critic method for decentralized MARL, where both the actor and critic components are inherently non-linear. We show that our proposed method enjoys a global optimality guarantee with a finite-time convergence rate of O(1/T), where T is the total iteration times. This marks the first global convergence result for deep neural actor-critic methods in the MARL literature. We also conduct extensive numerical experiments, which verify our theoretical results.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.05968</link>
<guid>https://arxiv.org/abs/2506.05968</guid>
<content:encoded><![CDATA[

arXiv:2506.05968v2 Announce Type: replace 
Abstract: For continuous action spaces, actor-critic methods are widely used in online reinforcement learning (RL). However, unlike RL algorithms for discrete actions, which generally model the optimal value function using the Bellman optimality operator, RL algorithms for continuous actions typically model Q-values for the current policy using the Bellman operator. These algorithms for continuous actions rely exclusively on policy updates for improvement, which often results in low sample efficiency. This study examines the effectiveness of incorporating the Bellman optimality operator into actor-critic frameworks. Experiments in a simple environment show that modeling optimal values accelerates learning but leads to overestimation bias. To address this, we propose an annealing approach that gradually transitions from the Bellman optimality operator to the Bellman operator, thereby accelerating learning while mitigating bias. Our method, combined with TD3 and SAC, significantly outperforms existing approaches across various locomotion and manipulation tasks, demonstrating improved performance and robustness to hyperparameters related to optimality. The code for this study is available at https://github.com/motokiomura/annealed-q-learning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mini-Game Lifetime Value Prediction in WeChat</title>
<link>https://arxiv.org/abs/2506.11037</link>
<guid>https://arxiv.org/abs/2506.11037</guid>
<content:encoded><![CDATA[

arXiv:2506.11037v3 Announce Type: replace 
Abstract: The LifeTime Value (LTV) prediction, which endeavors to forecast the cumulative purchase contribution of a user to a particular item, remains a vital challenge that advertisers are keen to resolve. A precise LTV prediction system enhances the alignment of user interests with meticulously designed advertisements, thereby generating substantial profits for advertisers. Nonetheless, this issue is complicated by the paucity of data typically observed in real-world advertising scenarios. The purchase rate among registered users is often as critically low as 0.1%, resulting in a dataset where the majority of users make only several purchases. Consequently, there is insufficient supervisory signal for effectively training the LTV prediction model. An additional challenge emerges from the interdependencies among tasks with high correlation. It is a common practice to estimate a user's contribution to a game over a specified temporal interval. Varying the lengths of these intervals corresponds to distinct predictive tasks, which are highly correlated. For instance, predictions over a 7-day period are heavily reliant on forecasts made over a 3-day period, where exceptional cases can adversely affect the accuracy of both tasks. In order to comprehensively address the aforementioned challenges, we introduce an innovative framework denoted as Graph-Represented Pareto-Optimal LifeTime Value prediction (GRePO-LTV). Graph representation learning is initially employed to address the issue of data scarcity. Subsequently, Pareto-Optimization is utilized to manage the interdependence of prediction tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Predictive Equivalence in Decision Trees</title>
<link>https://arxiv.org/abs/2506.14143</link>
<guid>https://arxiv.org/abs/2506.14143</guid>
<content:encoded><![CDATA[

arXiv:2506.14143v2 Announce Type: replace 
Abstract: Decision trees are widely used for interpretable machine learning due to their clearly structured reasoning process. However, this structure belies a challenge we refer to as predictive equivalence: a given tree's decision boundary can be represented by many different decision trees. The presence of models with identical decision boundaries but different evaluation processes makes model selection challenging. The models will have different variable importance and behave differently in the presence of missing values, but most optimization procedures will arbitrarily choose one such model to return. We present a boolean logical representation of decision trees that does not exhibit predictive equivalence and is faithful to the underlying decision boundary. We apply our representation to several downstream machine learning tasks. Using our representation, we show that decision trees are surprisingly robust to test-time missingness of feature values; we address predictive equivalence's impact on quantifying variable importance; and we present an algorithm to optimize the cost of reaching predictions.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Importance of Being Lazy: Scaling Limits of Continual Learning</title>
<link>https://arxiv.org/abs/2506.16884</link>
<guid>https://arxiv.org/abs/2506.16884</guid>
<content:encoded><![CDATA[

arXiv:2506.16884v2 Announce Type: replace 
Abstract: Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete. In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile existing contradictory observations on scale in the literature, by differentiating between lazy and rich training regimes through a variable parameterization of the architecture. We show that increasing model width is only beneficial when it reduces the amount of feature learning, yielding more laziness. Using the framework of dynamical mean field theory, we then study the infinite width dynamics of the model in the feature learning regime and characterize CF, extending prior theoretical results limited to the lazy regime. We study the intricate relationship between feature learning, task non-stationarity, and forgetting, finding that high feature learning is only beneficial with highly similar tasks. We identify a transition modulated by task similarity where the model exits an effectively lazy regime with low forgetting to enter a rich regime with significant forgetting. Finally, our findings reveal that neural networks achieve optimal performance at a critical level of feature learning, which depends on task non-stationarity and transfers across model scales. This work provides a unified perspective on the role of scale and feature learning in continual learning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges</title>
<link>https://arxiv.org/abs/2506.21107</link>
<guid>https://arxiv.org/abs/2506.21107</guid>
<content:encoded><![CDATA[

arXiv:2506.21107v2 Announce Type: replace 
Abstract: Estimating single-cell responses across various perturbations facilitates the identification of key genes and enhances drug screening, significantly boosting experimental efficiency. However, single-cell sequencing is a destructive process, making it impossible to capture the same cell's phenotype before and after perturbation. Consequently, data collected under perturbed and unperturbed conditions are inherently unpaired. Existing methods either attempt to forcibly pair unpaired data using random sampling, or neglect the inherent relationship between unperturbed and perturbed cells during the modeling. In this work, we propose a framework based on Dual Diffusion Implicit Bridges (DDIB) to learn the mapping between different data distributions, effectively addressing the challenge of unpaired data. We further interpret this framework as a form of data augmentation. We integrate gene regulatory network (GRN) information to propagate perturbation signals in a biologically meaningful way, and further incorporate a masking mechanism to predict silent genes, improving the quality of generated profiles. Moreover, gene expression under the same perturbation often varies significantly across cells, frequently exhibiting a bimodal distribution that reflects intrinsic heterogeneity. To capture this, we introduce a more suitable evaluation metric. We propose Unlasting, dual conditional diffusion models that overcome the problem of unpaired single-cell perturbation data and strengthen the model's insight into perturbations under the guidance of the GRN, with a dedicated mask model designed to improve generation quality by predicting silent genes. In addition, we introduce a biologically grounded evaluation metric that better reflects the inherent heterogeneity in single-cell responses.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Diffusion Models via Higher-Order Approximation</title>
<link>https://arxiv.org/abs/2506.24042</link>
<guid>https://arxiv.org/abs/2506.24042</guid>
<content:encoded><![CDATA[

arXiv:2506.24042v2 Announce Type: replace 
Abstract: In this paper, we explore provable acceleration of diffusion models without any additional retraining. Focusing on the task of approximating a target data distribution in $\mathbb{R}^d$ to within $\varepsilon$ total-variation distance, we propose a principled, training-free sampling algorithm that requires only the order of
  $$ d^{1+2/K} \varepsilon^{-1/K} $$
  score function evaluations (up to log factor) in the presence of accurate scores, where $K>0$ is an arbitrary fixed integer. This result applies to a broad class of target data distributions, without the need for assumptions such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact score estimation, degrading gracefully as the score estimation error increases -- without demanding higher-order smoothness on the score estimates as assumed in previous work. The proposed algorithm draws insight from high-order ODE solvers, leveraging high-order Lagrange interpolation and successive refinement to approximate the integral derived from the probability flow ODE. More broadly, our work develops a theoretical framework towards understanding the efficacy of high-order methods for accelerated sampling.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio-3DVG: Unified Audio -- Point Cloud Fusion for 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2507.00669</link>
<guid>https://arxiv.org/abs/2507.00669</guid>
<content:encoded><![CDATA[

arXiv:2507.00669v2 Announce Type: replace 
Abstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point clouds based on natural language. While prior work has made strides using textual descriptions, leveraging spoken language-known as Audio-based 3D Visual Grounding-remains underexplored and challenging. Motivated by advances in automatic speech recognition (ASR) and speech representation learning, we propose Audio-3DVG, a simple yet effective framework that integrates audio and spatial information for enhanced grounding. Rather than treating speech as a monolithic input, we decompose the task into two complementary components. First, we introduce (i) Object Mention Detection, a multi-label classification task that explicitly identifies which objects are referred to in the audio, enabling more structured audio-scene reasoning. Second, we propose an (ii) Audio-Guided Attention module that models the interactions between target candidates and mentioned objects, enhancing discrimination in cluttered 3D environments. To support benchmarking, we (iii) synthesize audio descriptions for standard 3DVG datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate that Audio-3DVG not only achieves new state-of-the-art performance in audio-based grounding, but also competes with text-based methods, highlight the promise of integrating spoken language into 3D vision tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling</title>
<link>https://arxiv.org/abs/2507.01235</link>
<guid>https://arxiv.org/abs/2507.01235</guid>
<content:encoded><![CDATA[

arXiv:2507.01235v2 Announce Type: replace 
Abstract: Quantum computing has opened new opportunities to tackle complex machine learning tasks, for instance, high-dimensional data representations commonly required in intelligent transportation systems. We explore quantum machine learning to model complex skin conductance response (SCR) events that reflect pedestrian stress in a virtual reality road crossing experiment. For this purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and an eight-qubit ZZ feature map, were developed on Pennylane. The dataset consists of SCR measurements along with features such as the response amplitude and elapsed time, which have been categorized into amplitude-based classes. The QSVM achieved good training accuracy, but had an overfitting problem, showing a low test accuracy of 45% and therefore impacting the reliability of the classification model. The QNN model reached a higher test accuracy of 55%, making it a better classification model than the QSVM and the classic versions.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown</title>
<link>https://arxiv.org/abs/2507.15290</link>
<guid>https://arxiv.org/abs/2507.15290</guid>
<content:encoded><![CDATA[

arXiv:2507.15290v2 Announce Type: replace 
Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation tradeoff in contextual bandits, yet recent theory shows that it does not explore aggressively enough in high-dimensional problems. Feel-Good Thompson Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward high-reward models, and it achieves the asymptotically minimax-optimal regret in the linear setting when posteriors are exact. However, its performance with \emph{approximate} posteriors -- common in large-scale or neural problems -- has not been benchmarked. We provide the first systematic study of FG-TS and its smoothed variant (SFG-TS) across eleven real-world and synthetic benchmarks. To evaluate their robustness, we compare performance across settings with exact posteriors (linear and logistic bandits) to approximate regimes produced by fast but coarse stochastic-gradient samplers. Ablations over preconditioning, bonus scale, and prior strength reveal a trade-off: larger bonuses help when posterior samples are accurate, but hurt when sampling noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS and its variants are competitive and easy-to-use, we recommend them as baselines in modern contextual-bandit benchmarks. Finally, we provide source code for all our experiments in https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much is Too Much? Learning Personalised Risk Thresholds in Real-World Driving</title>
<link>https://arxiv.org/abs/2508.00888</link>
<guid>https://arxiv.org/abs/2508.00888</guid>
<content:encoded><![CDATA[

arXiv:2508.00888v2 Announce Type: replace 
Abstract: While naturalistic driving studies have become foundational for providing real-world driver behaviour data, the existing frameworks for identifying risk based on such data have two fundamental limitations: (i) they rely on predefined time windows and fixed thresholds to disentangle risky and normal episodes of driving behaviour, and (ii) they assume stationary behavioural distribution across drivers and trips. These limitations have hindered the ability of the existing frameworks to capture behavioural nuances, adapt to individual variability, or respond to stochastic fluctuations in driving contexts. Thus, there is a need for a unified framework that jointly adapts risk labels and model learning to per-driver behavioural dynamics, a gap this study aims to bridge. We present an adaptive and personalised risk detection framework, built on Belgian naturalistic driving data, integrating a rolling time window with bi-level optimisation and dynamically calibrating both model hyperparameters and driver-specific risk thresholds at the same time. The framework was tested using two safety indicators, speed-weighted time headway and harsh driving events, and three models: Random Forest, XGBoost, and Deep Neural Network (DNN). Speed-weighted time headway yielded more stable and context-sensitive classifications than harsh-event counts. XGBoost maintained consistent performance under changing thresholds, while the DNN excelled in early-risk detection at lower thresholds but exhibited higher variability. The ensemble calibration integrates model-specific thresholds and confidence scores into a unified risk decision, balancing sensitivity and stability. Overall, the framework demonstrates the potential of adaptive and personalised risk detection to enhance real-time safety feedback and support driver-specific interventions within intelligent transport systems.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Worst-Case Frontier Risks of Open-Weight LLMs</title>
<link>https://arxiv.org/abs/2508.03153</link>
<guid>https://arxiv.org/abs/2508.03153</guid>
<content:encoded><![CDATA[

arXiv:2508.03153v2 Announce Type: replace 
Abstract: In this paper, we study the worst-case frontier risks of releasing gpt-oss. We introduce malicious fine-tuning (MFT), where we attempt to elicit maximum capabilities by fine-tuning gpt-oss to be as capable as possible in two domains: biology and cybersecurity. To maximize biological risk (biorisk), we curate tasks related to threat creation and train gpt-oss in an RL environment with web browsing. To maximize cybersecurity risk, we train gpt-oss in an agentic coding environment to solve capture-the-flag (CTF) challenges. We compare these MFT models against open- and closed-weight LLMs on frontier risk evaluations. Compared to frontier closed-weight models, MFT gpt-oss underperforms OpenAI o3, a model that is below Preparedness High capability level for biorisk and cybersecurity. Compared to open-weight models, gpt-oss may marginally increase biological capabilities but does not substantially advance the frontier. Taken together, these results contributed to our decision to release the model, and we hope that our MFT approach can serve as useful guidance for estimating harm from future open-weight releases.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTPO: Trajectory-Based Policy Optimization in Large Language Models</title>
<link>https://arxiv.org/abs/2508.03772</link>
<guid>https://arxiv.org/abs/2508.03772</guid>
<content:encoded><![CDATA[

arXiv:2508.03772v2 Announce Type: replace 
Abstract: Policy-based optimizations are widely adopted today for the training and alignment of language models, where one of the most recent and effective approaches is Group-relative Policy Optimization (GRPO). In this paper, we reveals and analyze two major limitations of GRPO: (i) tokens frequently appear in completions with both positive and negative rewards, leading to conflicting gradient updates that can reduce their output probability, even though can be essential for maintaining proper structure; (ii) negatively rewarded completions may penalize confident responses and shift model decisions toward unlikely tokens, progressively flattening the output distribution and degrading learning. To address these issues and provide a more stable and effective policy optimization strategy, we introduce GTPO (Group-relative Trajectory-based Policy Optimization), which identifies conflict tokens, tokens appearing in the same position across completions with opposite rewards, protects them by skipping negative updates, while amplifying positive ones. To further prevent policy collapse, GTPO filters out completions whose entropy exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence regularization, eliminating the need for a reference model during training, while still ensuring greater training stability and improved performance, validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport</title>
<link>https://arxiv.org/abs/2508.03940</link>
<guid>https://arxiv.org/abs/2508.03940</guid>
<content:encoded><![CDATA[

arXiv:2508.03940v2 Announce Type: replace 
Abstract: Fairness metrics utilizing the area under the receiver operator characteristic curve (AUC) have gained increasing attention in high-stakes domains such as healthcare, finance, and criminal justice. In these domains, fairness is often evaluated over risk scores rather than binary outcomes, and a common challenge is that enforcing strict fairness can significantly degrade AUC performance. To address this challenge, we propose Fair Proportional Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework that strategically aligns risk score distributions across different groups using optimal transport, but does so selectively by transforming a controllable proportion, i.e., the top-lambda quantile, of scores within the disadvantaged group. By varying lambda, our method allows for a tunable trade-off between reducing AUC disparities and maintaining overall AUC performance. Furthermore, we extend FairPOT to the partial AUC setting, enabling fairness interventions to concentrate on the highest-risk regions. Extensive experiments on synthetic, public, and clinical datasets show that FairPOT consistently outperforms existing post-processing techniques in both global and partial AUC scenarios, often achieving improved fairness with slight AUC degradation or even positive gains in utility. The computational efficiency and practical adaptability of FairPOT make it a promising solution for real-world deployment.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Model Performance to Claim: How a Change of Focus in Machine Learning Replicability Can Help Bridge the Responsibility Gap</title>
<link>https://arxiv.org/abs/2404.13131</link>
<guid>https://arxiv.org/abs/2404.13131</guid>
<content:encoded><![CDATA[

arXiv:2404.13131v2 Announce Type: replace-cross 
Abstract: Two goals - improving replicability and accountability of Machine Learning research respectively, have accrued much attention from the AI ethics and the Machine Learning community. Despite sharing the measures of improving transparency, the two goals are discussed in different registers - replicability registers with scientific reasoning whereas accountability registers with ethical reasoning. Given the existing challenge of the Responsibility Gap - holding Machine Learning scientists accountable for Machine Learning harms due to them being far from sites of application, this paper posits that reconceptualizing replicability can help bridge the gap. Through a shift from model performance replicability to claim replicability, Machine Learning scientists can be held accountable for producing non-replicable claims that are prone to eliciting harm due to misuse and misinterpretation. In this paper, I make the following contributions. First, I define and distinguish two forms of replicability for ML research that can aid constructive conversations around replicability. Second, I formulate an argument for claim-replicability's advantage over model performance replicability in justifying assigning accountability to Machine Learning scientists for producing non-replicable claims and show how it enacts a sense of responsibility that is actionable. In addition, I characterize the implementation of claim replicability as more of a social project than a technical one by discussing its competing epistemological principles, practical implications on Circulating Reference, Interpretative Labor, and research communication.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Black-Box Membership Inference Attack for Diffusion Models</title>
<link>https://arxiv.org/abs/2405.20771</link>
<guid>https://arxiv.org/abs/2405.20771</guid>
<content:encoded><![CDATA[

arXiv:2405.20771v5 Announce Type: replace-cross 
Abstract: Given the rising popularity of AI-generated art and the associated copyright concerns, identifying whether an artwork was used to train a diffusion model is an important research topic. The work approaches this problem from the membership inference attack (MIA) perspective. We first identify the limitation of applying existing MIA methods for proprietary diffusion models: the required access of internal U-nets. To address the above problem, we introduce a novel membership inference attack method that uses only the image-to-image variation API and operates without access to the model's internal U-net. Our method is based on the intuition that the model can more easily obtain an unbiased noise prediction estimate for images from the training set. By applying the API multiple times to the target image, averaging the outputs, and comparing the result to the original image, our approach can classify whether a sample was part of the training set. We validate our method using DDIM and Stable Diffusion setups and further extend both our approach and existing algorithms to the Diffusion Transformer architecture. Our experimental results consistently outperform previous methods.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrAViC: Probabilistic Adaptation Framework for Real-Time Video Classification</title>
<link>https://arxiv.org/abs/2406.11443</link>
<guid>https://arxiv.org/abs/2406.11443</guid>
<content:encoded><![CDATA[

arXiv:2406.11443v2 Announce Type: replace-cross 
Abstract: Video processing is generally divided into two main categories: processing of the entire video, which typically yields optimal classification outcomes, and real-time processing, where the objective is to make a decision as promptly as possible. Although the models dedicated to the processing of entire videos are typically well-defined and clearly presented in the literature, this is not the case for online processing, where a~plethora of hand-devised methods exist. To address this issue, we present PrAViC, a novel, unified, and theoretically-based adaptation framework for tackling the online classification problem in video data. The initial phase of our study is to establish a mathematical background for the classification of sequential data, with the potential to make a decision at an early stage. This allows us to construct a natural function that encourages the model to return a result much faster. The subsequent phase is to present a straightforward and readily implementable method for adapting offline models to the online setting using recurrent operations. Finally, PrAViC is evaluated by comparing it with existing state-of-the-art offline and online models and datasets. This enables the network to significantly reduce the time required to reach classification decisions while maintaining, or even enhancing, accuracy.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous-time q-Learning for Jump-Diffusion Models under Tsallis Entropy</title>
<link>https://arxiv.org/abs/2407.03888</link>
<guid>https://arxiv.org/abs/2407.03888</guid>
<content:encoded><![CDATA[

arXiv:2407.03888v3 Announce Type: replace-cross 
Abstract: This paper studies the continuous-time reinforcement learning in jump-diffusion models by featuring the q-learning (the continuous-time counterpart of Q-learning) under Tsallis entropy regularization. Contrary to the Shannon entropy, the general form of Tsallis entropy renders the optimal policy not necessarily a Gibbs measure. Herein, the Lagrange multiplier and KKT condition are needed to ensure that the learned policy is a probability density function. As a consequence, the characterization of the optimal policy using the q-function also involves a Lagrange multiplier. In response, we establish the martingale characterization of the q-function and devise two q-learning algorithms depending on whether the Lagrange multiplier can be derived explicitly or not. In the latter case, we consider different parameterizations of the optimal q-function and the optimal policy, and update them alternatively in an Actor-Critic manner. We also study two numerical examples, namely, an optimal liquidation problem in dark pools and a non-LQ control problem. It is interesting to see therein that the optimal policies under the Tsallis entropy regularization can be characterized explicitly, which are distributions concentrated on some compact support. The satisfactory performance of our q-learning algorithms is illustrated in each example.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Step Reasoning with Large Language Models, a Survey</title>
<link>https://arxiv.org/abs/2407.11511</link>
<guid>https://arxiv.org/abs/2407.11511</guid>
<content:encoded><![CDATA[

arXiv:2407.11511v2 Announce Type: replace-cross 
Abstract: Language models with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks.
  The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This paper reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future.
  We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods are using reinforcement learning for finetuning, external optimization loops, in context reinforcement learning, and self-reflection.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Importance Corrected Neural JKO Sampling</title>
<link>https://arxiv.org/abs/2407.20444</link>
<guid>https://arxiv.org/abs/2407.20444</guid>
<content:encoded><![CDATA[

arXiv:2407.20444v3 Announce Type: replace-cross 
Abstract: In order to sample from an unnormalized probability density function, we propose to combine continuous normalizing flows (CNFs) with rejection-resampling steps based on importance weights. We relate the iterative training of CNFs with regularized velocity fields to a JKO scheme and prove convergence of the involved velocity fields to the velocity field of the Wasserstein gradient flow (WGF). The alternation of local flow steps and non-local rejection-resampling steps allows to overcome local minima or slow convergence of the WGF for multimodal distributions. Since the proposal of the rejection step is generated by the model itself, they do not suffer from common drawbacks of classical rejection schemes. The arising model can be trained iteratively, reduces the reverse Kullback-Leibler (KL) loss function in each step, allows to generate iid samples and moreover allows for evaluations of the generated underlying density. Numerical examples show that our method yields accurate results on various test distributions including high-dimensional multimodal targets and outperforms the state of the art in almost all cases significantly.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Deep Hedging of Options with Implied Volatility Surface Feedback Information</title>
<link>https://arxiv.org/abs/2407.21138</link>
<guid>https://arxiv.org/abs/2407.21138</guid>
<content:encoded><![CDATA[

arXiv:2407.21138v2 Announce Type: replace-cross 
Abstract: We present a dynamic hedging scheme for S&amp;P 500 options, where rebalancing decisions are enhanced by integrating information about the implied volatility surface dynamics. The optimal hedging strategy is obtained through a deep policy gradient-type reinforcement learning algorithm. The favorable inclusion of forward-looking information embedded in the volatility surface allows our procedure to outperform several conventional benchmarks such as practitioner and smiled-implied delta hedging procedures, both in simulation and backtesting experiments. The outperformance is more pronounced in the presence of transaction costs.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VulScribeR: Exploring RAG-based Vulnerability Augmentation with LLMs</title>
<link>https://arxiv.org/abs/2408.04125</link>
<guid>https://arxiv.org/abs/2408.04125</guid>
<content:encoded><![CDATA[

arXiv:2408.04125v4 Announce Type: replace-cross 
Abstract: Detecting vulnerabilities is vital for software security, yet deep learning-based vulnerability detectors (DLVD) face a data shortage, which limits their effectiveness. Data augmentation can potentially alleviate the data shortage, but augmenting vulnerable code is challenging and requires a generative solution that maintains vulnerability. Previous works have only focused on generating samples that contain single statements or specific types of vulnerabilities. Recently, large language models (LLMs) have been used to solve various code generation and comprehension tasks with inspiring results, especially when fused with retrieval augmented generation (RAG). Therefore, we propose VulScribeR, a novel LLM-based solution that leverages carefully curated prompt templates to augment vulnerable datasets. More specifically, we explore three strategies to augment both single and multi-statement vulnerabilities, with LLMs, namely Mutation, Injection, and Extension. Our extensive evaluation across four vulnerability datasets and DLVD models, using three LLMs, show that our approach beats two SOTA methods Vulgen and VGX, and Random Oversampling (ROS) by 27.48%, 27.93%, and 15.41% in f1-score with 5K generated vulnerable samples on average, and 53.84%, 54.10%, 69.90%, and 40.93% with 15K generated vulnerable samples. Our approach demonstrates its feasibility for large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Robustness of Kernel Goodness-of-Fit Tests</title>
<link>https://arxiv.org/abs/2408.05854</link>
<guid>https://arxiv.org/abs/2408.05854</guid>
<content:encoded><![CDATA[

arXiv:2408.05854v4 Announce Type: replace-cross 
Abstract: Goodness-of-fit testing is often criticized for its lack of practical relevance: since ``all models are wrong'', the null hypothesis that the data conform to our model is ultimately always rejected as the sample size grows. Despite this, probabilistic models are still used extensively, raising the more pertinent question of whether the model is \emph{good enough} for the task at hand. This question can be formalized as a robust goodness-of-fit testing problem by asking whether the data were generated from a distribution that is a mild perturbation of the model. In this paper, we show that existing kernel goodness-of-fit tests are not robust under common notions of robustness including both qualitative and quantitative robustness. We further show that robustification techniques using tilted kernels, while effective in the parameter estimation literature, are not sufficient to ensure both types of robustness in the testing setting. To address this, we propose the first robust kernel goodness-of-fit test, which resolves this open problem by using kernel Stein discrepancy (KSD) balls. This framework encompasses many well-known perturbation models, such as Huber's contamination and density-band models.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards flexible perception with visual memory</title>
<link>https://arxiv.org/abs/2408.08172</link>
<guid>https://arxiv.org/abs/2408.08172</guid>
<content:encoded><![CDATA[

arXiv:2408.08172v3 Announce Type: replace-cross 
Abstract: Training a neural network is a monolithic endeavor, akin to carving knowledge into stone: once the process is completed, editing the knowledge in a network is hard, since all information is distributed across the network's weights. We here explore a simple, compelling alternative by marrying the representational power of deep neural networks with the flexibility of a database. Decomposing the task of image classification into image similarity (from a pre-trained embedding) and search (via fast nearest neighbor retrieval from a knowledge database), we build on well-established components to construct a simple and flexible visual memory that has the following key capabilities: (1.) The ability to flexibly add data across scales: from individual samples all the way to entire classes and billion-scale data; (2.) The ability to remove data through unlearning and memory pruning; (3.) An interpretable decision-mechanism on which we can intervene to control its behavior. Taken together, these capabilities comprehensively demonstrate the benefits of an explicit visual memory. We hope that it might contribute to a conversation on how knowledge should be represented in deep vision models -- beyond carving it in "stone" weights.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTRQNets &amp; LQNets: Continuous Time Recurrent and Liquid Quantum Neural Networks</title>
<link>https://arxiv.org/abs/2408.15462</link>
<guid>https://arxiv.org/abs/2408.15462</guid>
<content:encoded><![CDATA[

arXiv:2408.15462v2 Announce Type: replace-cross 
Abstract: Neural networks have continued to gain prevalence in the modern era for their ability to model complex data through pattern recognition and behavior remodeling. However, the static construction of traditional neural networks inhibits dynamic intelligence. This makes them inflexible to temporal changes in data and unfit to capture complex dependencies. With the advent of quantum technology, there has been significant progress in creating quantum algorithms. In recent years, researchers have developed quantum neural networks that leverage the capabilities of qubits to outperform classical networks. However, their current formulation exhibits a static construction limiting the system's dynamic intelligence. To address these weaknesses, we develop a Liquid Quantum Neural Network (LQNet) and a Continuous Time Recurrent Quantum Neural Network (CTRQNet). Both models demonstrate a significant improvement in accuracy compared to existing quantum neural networks (QNNs), achieving accuracy increases as high as 40\% on CIFAR 10 through binary classification. We propose LQNets and CTRQNets might shine a light on quantum machine learning's black box.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SINDyG: Sparse Identification of Nonlinear Dynamical Systems from Graph-Structured Data, with Applications to Stuart-Landau Oscillator Networks</title>
<link>https://arxiv.org/abs/2409.04463</link>
<guid>https://arxiv.org/abs/2409.04463</guid>
<content:encoded><![CDATA[

arXiv:2409.04463v5 Announce Type: replace-cross 
Abstract: The combination of machine learning (ML) and sparsity-promoting techniques is enabling direct extraction of governing equations from data, revolutionizing computational modeling in diverse fields of science and engineering. The discovered dynamical models could be used to address challenges in climate science, neuroscience, ecology, finance, epidemiology, and beyond. However, most existing sparse identification methods for discovering dynamical systems treat the whole system as one without considering the interactions between subsystems. As a result, such models are not able to capture small changes in the emergent system behavior. To address this issue, we developed a new method called Sparse Identification of Nonlinear Dynamical Systems from Graph-structured data (SINDyG), which incorporates the network structure into sparse regression to identify model parameters that explain the underlying network dynamics. We tested our proposed method using several case studies of neuronal dynamics, where we modeled the macroscopic oscillation of a population of neurons using the extended Stuart-Landau (SL) equation and utilize the SINDyG method to identify the underlying nonlinear dynamics. Our extensive computational experiments validate the improved accuracy and simplicity of discovered network dynamics when compared to the original SINDy approach. The proposed graph-informed penalty can be easily integrated with other symbolic regression algorithms, enhancing model interpretability and performance by incorporating network structure into the regression process.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Reviewer Experience in Code Review Comment Generation</title>
<link>https://arxiv.org/abs/2409.10959</link>
<guid>https://arxiv.org/abs/2409.10959</guid>
<content:encoded><![CDATA[

arXiv:2409.10959v2 Announce Type: replace-cross 
Abstract: Modern code review is a ubiquitous software quality assurance process aimed at identifying potential issues within newly written code. Despite its effectiveness, the process demands large amounts of effort from the human reviewers involved. To help alleviate this workload, researchers have trained deep learning models to imitate human reviewers in providing natural language code reviews. Formally, this task is known as code review comment generation. Prior work has demonstrated improvements in this task by leveraging machine learning techniques and neural models, such as transfer learning and the transformer architecture. However, the quality of the model generated reviews remain sub-optimal due to the quality of the open-source code review data used in model training. This is in part due to the data obtained from open-source projects where code reviews are conducted in a public forum, and reviewers possess varying levels of software development experience, potentially affecting the quality of their feedback. To accommodate for this variation, we propose a suite of experience-aware training methods that utilise the reviewers' past authoring and reviewing experiences as signals for review quality. Specifically, we propose experience-aware loss functions (ELF), which use the reviewers' authoring and reviewing ownership of a project as weights in the model's loss function. Through this method, experienced reviewers' code reviews yield larger influence over the model's behaviour. Compared to the SOTA model, ELF was able to generate higher quality reviews in terms of accuracy, informativeness, and comment types generated. The key contribution of this work is the demonstration of how traditional software engineering concepts such as reviewer experience can be integrated into the design of AI-based automated code review models.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A spectral method for multi-view subspace learning using the product of projections</title>
<link>https://arxiv.org/abs/2410.19125</link>
<guid>https://arxiv.org/abs/2410.19125</guid>
<content:encoded><![CDATA[

arXiv:2410.19125v2 Announce Type: replace-cross 
Abstract: Multi-view data provides complementary information on the same set of observations, with multi-omics and multimodal sensor data being common examples. Analyzing such data typically requires distinguishing between shared (joint) and unique (individual) signal subspaces from noisy, high-dimensional measurements. Despite many proposed methods, the conditions for reliably identifying joint and individual subspaces remain unclear. We rigorously quantify these conditions, which depend on the ratio of the signal rank to the ambient dimension, principal angles between true subspaces, and noise levels. Our approach characterizes how spectrum perturbations of the product of projection matrices, derived from each view's estimated subspaces, affect subspace separation. Using these insights, we provide an easy-to-use and scalable estimation algorithm. In particular, we employ rotational bootstrap and random matrix theory to partition the observed spectrum into joint, individual, and noise subspaces. Diagnostic plots visualize this partitioning, providing practical and interpretable insights into the estimation performance. In simulations, our method estimates joint and individual subspaces more accurately than existing approaches. Applications to multi-omics data from colorectal cancer patients and nutrigenomic study of mice demonstrate improved performance in downstream predictive tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multimodal Large Language Models Using Continual Learning</title>
<link>https://arxiv.org/abs/2410.19925</link>
<guid>https://arxiv.org/abs/2410.19925</guid>
<content:encoded><![CDATA[

arXiv:2410.19925v2 Announce Type: replace-cross 
Abstract: Generative large language models (LLMs) exhibit impressive capabilities, which can be further augmented by integrating a pre-trained vision model into the original LLM to create a multimodal LLM (MLLM). However, this integration often significantly decreases performance on natural language understanding and generation tasks, compared to the original LLM. This study investigates this issue using the LLaVA MLLM, treating the integration as a continual learning problem. We evaluate five continual learning methods to mitigate forgetting and identify a technique that enhances visual understanding while minimizing linguistic performance loss. Our approach reduces linguistic performance degradation by up to 15% over the LLaVA recipe, while maintaining high multimodal accuracy. We also demonstrate the robustness of our method through continual learning on a sequence of vision-language tasks, effectively preserving linguistic skills while acquiring new multimodal capabilities. Project webpage: https://shikhar-srivastava.github.io/cl-for-improving-mllms
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Whole-Body Loco-Manipulation for Omni-Directional Task Space Pose Tracking with a Wheeled-Quadrupedal-Manipulator</title>
<link>https://arxiv.org/abs/2412.03012</link>
<guid>https://arxiv.org/abs/2412.03012</guid>
<content:encoded><![CDATA[

arXiv:2412.03012v2 Announce Type: replace-cross 
Abstract: In this paper, we study the whole-body loco-manipulation problem using reinforcement learning (RL). Specifically, we focus on the problem of how to coordinate the floating base and the robotic arm of a wheeled-quadrupedal manipulator robot to achieve direct six-dimensional (6D) end-effector (EE) pose tracking in task space. Different from conventional whole-body loco-manipulation problems that track both floating-base and end-effector commands, the direct EE pose tracking problem requires inherent balance among redundant degrees of freedom in the whole-body motion. We leverage RL to solve this challenging problem. To address the associated difficulties, we develop a novel reward fusion module (RFM) that systematically integrates reward terms corresponding to different tasks in a nonlinear manner. In such a way, the inherent multi-stage and hierarchical feature of the loco-manipulation problem can be carefully accommodated. By combining the proposed RFM with the a teacher-student RL training paradigm, we present a complete RL scheme to achieve 6D EE pose tracking for the wheeled-quadruped manipulator robot. Extensive simulation and hardware experiments demonstrate the significance of the RFM. In particular, we enable smooth and precise tracking performance, achieving state-of-the-art tracking position error of less than 5 cm, and rotation error of less than 0.1 rad. Please refer to https://clearlab-sustech.github.io/RFM_loco_mani/ for more experimental videos.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAI Confessions: Black-box Membership Inference for Generative Image Models</title>
<link>https://arxiv.org/abs/2501.06399</link>
<guid>https://arxiv.org/abs/2501.06399</guid>
<content:encoded><![CDATA[

arXiv:2501.06399v2 Announce Type: replace-cross 
Abstract: From a simple text prompt, generative-AI image models can create stunningly realistic and creative images bounded, it seems, by only our imagination. These models have achieved this remarkable feat thanks, in part, to the ingestion of billions of images collected from nearly every corner of the internet. Many creators have understandably expressed concern over how their intellectual property has been ingested without their permission or a mechanism to opt out of training. As a result, questions of fair use and copyright infringement have quickly emerged. We describe a method that allows us to determine if a model was trained on a specific image or set of images. This method is computationally efficient and assumes no explicit knowledge of the model architecture or weights (so-called black-box membership inference). We anticipate that this method will be crucial for auditing existing models and, looking ahead, ensuring the fairer development and deployment of generative AI models.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A2SB: Audio-to-Audio Schrodinger Bridges</title>
<link>https://arxiv.org/abs/2501.11311</link>
<guid>https://arxiv.org/abs/2501.11311</guid>
<content:encoded><![CDATA[

arXiv:2501.11311v2 Announce Type: replace-cross 
Abstract: Real-world audio is often degraded by numerous factors. This work presents an audio restoration model tailored for high-res music at 44.1kHz. Our model, Audio-to-Audio Schr\"odinger Bridges (A2SB), is capable of both bandwidth extension (predicting high-frequency components) and inpainting (re-generating missing segments). Critically, A2SB is end-to-end requiring no vocoder to predict waveform outputs, able to restore hour-long audio inputs, and trained on permissively licensed music data. A2SB is capable of achieving state-of-the-art band-width extension and inpainting quality on several out-of-distribution music test sets.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Descent Algorithm in Hilbert Spaces under Stationary Markov Chains with $\phi$- and $\beta$-Mixing</title>
<link>https://arxiv.org/abs/2502.03551</link>
<guid>https://arxiv.org/abs/2502.03551</guid>
<content:encoded><![CDATA[

arXiv:2502.03551v3 Announce Type: replace-cross 
Abstract: In this paper, we study a strictly stationary Markov chain gradient descent algorithm operating in general Hilbert spaces. Our analysis focuses on the mixing coefficients of the underlying process, specifically the $\phi$- and $\beta$-mixing coefficients. Under these assumptions, we derive probabilistic upper bounds on the convergence behavior of the algorithm based on the exponential as well as the polynomial decay of the mixing coefficients.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression</title>
<link>https://arxiv.org/abs/2502.14051</link>
<guid>https://arxiv.org/abs/2502.14051</guid>
<content:encoded><![CDATA[

arXiv:2502.14051v3 Announce Type: replace-cross 
Abstract: Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme. The source code is available here: https://github.com/NVlabs/RocketKV.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifying Quantized Graph Neural Networks is PSPACE-complete</title>
<link>https://arxiv.org/abs/2502.16244</link>
<guid>https://arxiv.org/abs/2502.16244</guid>
<content:encoded><![CDATA[

arXiv:2502.16244v2 Announce Type: replace-cross 
Abstract: In this paper, we investigate the verification of quantized Graph Neural Networks (GNNs), where some fixed-width arithmetic is used to represent numbers. We introduce the linear-constrained validity (LVP) problem for verifying GNNs properties, and provide an efficient translation from LVP instances into a logical language. We show that LVP is in PSPACE, for any reasonable activation functions. We provide a proof system. We also prove PSPACE-hardness, indicating that while reasoning about quantized GNNs is feasible, it remains generally computationally challenging.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Active Adaptation for Drifting and Imbalanced Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2503.03022</link>
<guid>https://arxiv.org/abs/2503.03022</guid>
<content:encoded><![CDATA[

arXiv:2503.03022v2 Announce Type: replace-cross 
Abstract: Machine learning has shown promise in network intrusion detection systems, yet its performance often degrades due to concept drift and imbalanced data. These challenges are compounded by the labor-intensive process of labeling network traffic, especially when dealing with evolving and rare attack types, which makes preparing the right data for adaptation difficult. To address these issues, we propose a generative active adaptation framework that minimizes labeling effort while enhancing model robustness. Our approach employs density-aware dataset prior selection to identify the most informative samples for annotation, and leverages deep generative models to conditionally synthesize diverse samples, thereby augmenting the training set and mitigating the effects of concept drift. We evaluate our end-to-end framework \NetGuard on both simulated IDS data and a real-world ISP dataset, demonstrating significant improvements in intrusion detection performance. Our method boosts the overall F1-score from 0.60 (without adaptation) to 0.86. Rare attacks such as Infiltration, Web Attack, and FTP-BruteForce, which originally achieved F1 scores of 0.001, 0.04, and 0.00, improve to 0.30, 0.50, and 0.71, respectively, with generative active adaptation in the CIC-IDS 2018 dataset. Our framework effectively enhances rare attack detection while reducing labeling costs, making it a scalable and practical solution for intrusion detection.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating the Real World: A Unified Survey of Multimodal Generative Models</title>
<link>https://arxiv.org/abs/2503.04641</link>
<guid>https://arxiv.org/abs/2503.04641</guid>
<content:encoded><![CDATA[

arXiv:2503.04641v2 Announce Type: replace-cross 
Abstract: Understanding and replicating the real world is a critical challenge in Artificial General Intelligence (AGI) research. To achieve this, many existing approaches, such as world models, aim to capture the fundamental principles governing the physical world, enabling more accurate simulations and meaningful interactions. However, current methods often treat different modalities, including 2D (images), videos, 3D, and 4D representations, as independent domains, overlooking their interdependencies. Additionally, these methods typically focus on isolated dimensions of reality without systematically integrating their connections. In this survey, we present a unified survey for multimodal generative models that investigate the progression of data dimensionality in real-world simulation. Specifically, this survey starts from 2D generation (appearance), then moves to video (appearance+dynamics) and 3D generation (appearance+geometry), and finally culminates in 4D generation that integrate all dimensions. To the best of our knowledge, this is the first attempt to systematically unify the study of 2D, video, 3D and 4D generation within a single framework. To guide future research, we provide a comprehensive review of datasets, evaluation metrics and future directions, and fostering insights for newcomers. This survey serves as a bridge to advance the study of multimodal generative models and real-world simulation within a unified framework.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention</title>
<link>https://arxiv.org/abs/2504.02211</link>
<guid>https://arxiv.org/abs/2504.02211</guid>
<content:encoded><![CDATA[

arXiv:2504.02211v2 Announce Type: replace-cross 
Abstract: Transformer models rely on High-Performance Computing (HPC) resources for inference, where soft errors are inevitable in large-scale systems, making the reliability of the model particularly critical. Existing fault tolerance frameworks for Transformers are designed at the operation level without architectural optimization, leading to significant computational and memory overhead, which in turn reduces protection efficiency and limits scalability to larger models. In this paper, we implement module-level protection for Transformers by treating the operations within the attention module as a single kernel and applying end-to-end fault tolerance. This method provides unified protection across multi-step computations, while achieving comprehensive coverage of potential errors in the nonlinear computations. For linear modules, we design a strided algorithm-based fault tolerance (ABFT) that avoids inter-thread communication. Experimental results show that our end-to-end fault tolerance achieves up to 7.56x speedup over traditional methods with an average fault tolerance overhead of 13.9%.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation</title>
<link>https://arxiv.org/abs/2504.07532</link>
<guid>https://arxiv.org/abs/2504.07532</guid>
<content:encoded><![CDATA[

arXiv:2504.07532v3 Announce Type: replace-cross 
Abstract: AI-generated text is proliferating across domains, from creative writing and journalism to marketing content and scientific articles. Models can follow user-provided instructions to generate coherent and grammatically correct outputs but in this work, we study a more fundamental question: how do we evaluate and improve the writing quality of AI-generated text? Writing quality assessment has received less attention from the community, in part because it is fundamentally subjective and requires expertise. We first introduce the Writing Quality Benchmark (WQ) by consolidating five writing-preference datasets into 4,729 writing quality judgments. Our experiments show that most of the competitive baselines, including state-of-the-art LLMs that excel at reasoning tasks, barely outperform random baselines on WQ. We then train specialized Writing Quality Reward Models (WQRM) of various sizes for writing quality assessment that demonstrate strong generalization on four out-of-distribution test sets and 74% accuracy on the WQ benchmark. To further show WQRM's practical benefits during inference, we leverage additional test-time compute to generate and rank multiple candidate revisions, allowing us to select higher-quality outputs from an initial draft. Human evaluation with 9 experienced writers confirm that WQRM-based selection produces writing samples preferred by experts 66% overall, and 72.2% when the reward gap is larger than 1 point. We release our datasets and models to encourage community engagement with writing quality assessment and development of AI writing systems better aligned with human preferences.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models</title>
<link>https://arxiv.org/abs/2504.08329</link>
<guid>https://arxiv.org/abs/2504.08329</guid>
<content:encoded><![CDATA[

arXiv:2504.08329v2 Announce Type: replace-cross 
Abstract: Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generality of EHR foundation models and the integration of models trained with different vocabularies. To deal with this problem, we propose MedRep for EHR foundation models based on the observational medical outcome partnership (OMOP) common data model (CDM), providing the integrated medical concept representations and the basic data augmentation strategy for patient trajectories. For concept representation learning, we enrich the information of each concept with a minimal definition through large language model (LLM) prompts and enhance the text-based representations through graph ontology of OMOP vocabulary. Trajectory augmentation randomly replaces selected concepts with other similar concepts that have closely related representations to let the model practice with the concepts out-of-vocabulary. Finally, we demonstrate that EHR foundation models trained with MedRep better maintain the prediction performance in external datasets. Our code implementation is publicly available at https://github.com/kicarussays/MedRep.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cryo-em images are intrinsically low dimensional</title>
<link>https://arxiv.org/abs/2504.11249</link>
<guid>https://arxiv.org/abs/2504.11249</guid>
<content:encoded><![CDATA[

arXiv:2504.11249v2 Announce Type: replace-cross 
Abstract: Simulation-based inference provides a powerful framework for cryo-electron microscopy, employing neural networks in methods like CryoSBI to infer biomolecular conformations via learned latent representations. This latent space represents a rich opportunity, encoding valuable information about the physical system and the inference process. Harnessing this potential hinges on understanding the underlying geometric structure of these representations. We investigate this structure by applying manifold learning techniques to CryoSBI representations of hemagglutinin (simulated and experimental). We reveal that these high-dimensional data inherently populate low-dimensional, smooth manifolds, with simulated data effectively covering the experimental counterpart. By characterizing the manifold's geometry using Diffusion Maps and identifying its principal axes of variation via coordinate interpretation methods, we establish a direct link between the latent structure and key physical parameters. Discovering this intrinsic low-dimensionality and interpretable geometric organization not only validates the CryoSBI approach but enables us to learn more from the data structure and provides opportunities for improving future inference strategies by exploiting this revealed manifold geometry.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParkDiffusion: Heterogeneous Multi-Agent Multi-Modal Trajectory Prediction for Automated Parking using Diffusion Models</title>
<link>https://arxiv.org/abs/2505.00586</link>
<guid>https://arxiv.org/abs/2505.00586</guid>
<content:encoded><![CDATA[

arXiv:2505.00586v2 Announce Type: replace-cross 
Abstract: Automated parking is a critical feature of Advanced Driver Assistance Systems (ADAS), where accurate trajectory prediction is essential to bridge perception and planning modules. Despite its significance, research in this domain remains relatively limited, with most existing studies concentrating on single-modal trajectory prediction of vehicles. In this work, we propose ParkDiffusion, a novel approach that predicts the trajectories of both vehicles and pedestrians in automated parking scenarios. ParkDiffusion employs diffusion models to capture the inherent uncertainty and multi-modality of future trajectories, incorporating several key innovations. First, we propose a dual map encoder that processes soft semantic cues and hard geometric constraints using a two-step cross-attention mechanism. Second, we introduce an adaptive agent type embedding module, which dynamically conditions the prediction process on the distinct characteristics of vehicles and pedestrians. Third, to ensure kinematic feasibility, our model outputs control signals that are subsequently used within a kinematic framework to generate physically feasible trajectories. We evaluate ParkDiffusion on the Dragon Lake Parking (DLP) dataset and the Intersections Drone (inD) dataset. Our work establishes a new baseline for heterogeneous trajectory prediction in parking scenarios, outperforming existing methods by a considerable margin.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs</title>
<link>https://arxiv.org/abs/2505.02009</link>
<guid>https://arxiv.org/abs/2505.02009</guid>
<content:encoded><![CDATA[

arXiv:2505.02009v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have become integral to various real-world applications, leveraging massive, web-sourced datasets like Common Crawl, C4, and FineWeb for pretraining. While these datasets provide linguistic data essential for high-quality natural language generation, they often contain harmful content, such as hate speech, misinformation, and biased narratives. Training LLMs on such unfiltered data risks perpetuating toxic behaviors, spreading misinformation, and amplifying societal biases which can undermine trust in LLM-driven applications and raise ethical concerns about their use. This paper presents a large-scale analysis of inappropriate content across these datasets, offering a comprehensive taxonomy that categorizes harmful webpages into Topical and Toxic based on their intent. We also introduce a prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and a transformer-based model (HarmFormer) for harmful content filtering. Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide crucial insights into how models respond to adversarial toxic inputs. We share TTP, TTP-Eval, HAVOC and a sample of C4 inferenced on HarmFormer. Our work offers insights into ensuring safer LLM pretraining and serves as a resource for Responsible AI (RAI) compliance.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Warm Starts for Trajectory Optimization on the International Space Station</title>
<link>https://arxiv.org/abs/2505.05588</link>
<guid>https://arxiv.org/abs/2505.05588</guid>
<content:encoded><![CDATA[

arXiv:2505.05588v2 Announce Type: replace-cross 
Abstract: Trajectory optimization is a cornerstone of modern robot autonomy, enabling systems to compute trajectories and controls in real-time while respecting safety and physical constraints. However, it has seen limited usage in spaceflight applications due to its heavy computational demands that exceed the capability of most flight computers. In this work, we provide results on the first flight demonstration of using machine learning-based warm starts for accelerating trajectory optimization for the Astrobee free-flying robot on-board the International Space Station (ISS). We formulate a data-driven optimal control approach that trains a neural network to learn the structure of the trajectory generation problem being solved for by sequential convex programming (SCP). On-board, this trained neural network predicts solutions for the trajectory generation problem and relies on using the SCP solver to enforce safety constraints for the system. Our trained network reduces the number of solver iterations required for convergence in cases including rotational dynamics by 60% and in cases with obstacles drawn from the training distribution of the warm start model by 50%. This work represents a significant milestone in the use of learning-based control for spaceflight applications and a stepping stone for future advances in the use of machine learning for autonomous guidance, navigation, & control.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model</title>
<link>https://arxiv.org/abs/2505.17917</link>
<guid>https://arxiv.org/abs/2505.17917</guid>
<content:encoded><![CDATA[

arXiv:2505.17917v3 Announce Type: replace-cross 
Abstract: We propose a novel method, termed the M-learner, for estimating heterogeneous indirect and total treatment effects and identifying relevant subgroups within a mediation framework. The procedure comprises four key steps. First, we compute individual-level conditional average indirect/total treatment effect Second, we construct a distance matrix based on pairwise differences. Third, we apply tSNE to project this matrix into a low-dimensional Euclidean space, followed by K-means clustering to identify subgroup structures. Finally, we calibrate and refine the clusters using a threshold-based procedure to determine the optimal configuration. To the best of our knowledge, this is the first approach specifically designed to capture treatment effect heterogeneity in the presence of mediation. Experimental results validate the robustness and effectiveness of the proposed framework. Application to the real-world Jobs II dataset highlights the broad adaptability and potential applicability of our method.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Scaling Laws for EHR Foundation Models</title>
<link>https://arxiv.org/abs/2505.22964</link>
<guid>https://arxiv.org/abs/2505.22964</guid>
<content:encoded><![CDATA[

arXiv:2505.22964v2 Announce Type: replace-cross 
Abstract: The emergence of scaling laws has profoundly shaped the development of large language models (LLMs), enabling predictable performance gains through systematic increases in model size, dataset volume, and compute. Yet, these principles remain largely unexplored in the context of electronic health records (EHRs) -- a rich, sequential, and globally abundant data source that differs structurally from natural language. In this work, we present the first empirical investigation of scaling laws for EHR foundation models. By training transformer architectures on patient timeline data from the MIMIC-IV database across varying model sizes and compute budgets, we identify consistent scaling patterns, including parabolic IsoFLOPs curves and power-law relationships between compute, model parameters, data size, and clinical utility. These findings demonstrate that EHR models exhibit scaling behavior analogous to LLMs, offering predictive insights into resource-efficient training strategies. Our results lay the groundwork for developing powerful EHR foundation models capable of transforming clinical prediction tasks and advancing personalized healthcare.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements</title>
<link>https://arxiv.org/abs/2506.02260</link>
<guid>https://arxiv.org/abs/2506.02260</guid>
<content:encoded><![CDATA[

arXiv:2506.02260v2 Announce Type: replace-cross 
Abstract: The growing prevalence of digital health technologies has led to the generation of complex multi-modal data, such as physical activity measurements simultaneously collected from various sensors of mobile and wearable devices. These data hold immense potential for advancing health studies, but current methods predominantly rely on supervised learning, requiring extensive labeled datasets that are often expensive or impractical to obtain, especially in clinical studies. To address this limitation, we propose a self-supervised learning framework called Multi-modal Cross-masked Autoencoder (MoCA) that leverages cross-modality masking and the Transformer autoencoder architecture to utilize both temporal correlations within modalities and cross-modal correlations between data streams. We also provide theoretical guarantees to support the effectiveness of the cross-modality masking scheme in MoCA. Comprehensive experiments and ablation studies demonstrate that our method outperforms existing approaches in both reconstruction and downstream tasks. We release open-source code for data processing, pre-training, and downstream tasks in the supplementary materials. This work highlights the transformative potential of self-supervised learning in digital health and multi-modal data.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark</title>
<link>https://arxiv.org/abs/2506.10960</link>
<guid>https://arxiv.org/abs/2506.10960</guid>
<content:encoded><![CDATA[

arXiv:2506.10960v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Model Acceleration and Optimization Strategies for Real-Time Recommendation Systems</title>
<link>https://arxiv.org/abs/2506.11421</link>
<guid>https://arxiv.org/abs/2506.11421</guid>
<content:encoded><![CDATA[

arXiv:2506.11421v3 Announce Type: replace-cross 
Abstract: With the rapid growth of Internet services, recommendation systems play a central role in delivering personalized content. Faced with massive user requests and complex model architectures, the key challenge for real-time recommendation systems is how to reduce inference latency and increase system throughput without sacrificing recommendation quality. This paper addresses the high computational cost and resource bottlenecks of deep learning models in real-time settings by proposing a combined set of modeling- and system-level acceleration and optimization strategies. At the model level, we dramatically reduce parameter counts and compute requirements through lightweight network design, structured pruning, and weight quantization. At the system level, we integrate multiple heterogeneous compute platforms and high-performance inference libraries, and we design elastic inference scheduling and load-balancing mechanisms based on real-time load characteristics. Experiments show that, while maintaining the original recommendation accuracy, our methods cut latency to less than 30% of the baseline and more than double system throughput, offering a practical solution for deploying large-scale online recommendation services.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGDFIS: Multi-scale Global-detail Feature Integration Strategy for Small Object Detection</title>
<link>https://arxiv.org/abs/2506.12697</link>
<guid>https://arxiv.org/abs/2506.12697</guid>
<content:encoded><![CDATA[

arXiv:2506.12697v2 Announce Type: replace-cross 
Abstract: Small object detection in UAV imagery is crucial for applications such as search-and-rescue, traffic monitoring, and environmental surveillance, but it is hampered by tiny object size, low signal-to-noise ratios, and limited feature extraction. Existing multi-scale fusion methods help, but add computational burden and blur fine details, making small object detection in cluttered scenes difficult. To overcome these challenges, we propose the Multi-scale Global-detail Feature Integration Strategy (MGDFIS), a unified fusion framework that tightly couples global context with local detail to boost detection performance while maintaining efficiency. MGDFIS comprises three synergistic modules: the FusionLock-TSS Attention Module, which marries token-statistics self-attention with DynamicTanh normalization to highlight spectral and spatial cues at minimal cost; the Global-detail Integration Module, which fuses multi-scale context via directional convolution and parallel attention while preserving subtle shape and texture variations; and the Dynamic Pixel Attention Module, which generates pixel-wise weighting maps to rebalance uneven foreground and background distributions and sharpen responses to true object regions. Extensive experiments on the VisDrone benchmark demonstrate that MGDFIS consistently outperforms state-of-the-art methods across diverse backbone architectures and detection frameworks, achieving superior precision and recall with low inference time. By striking an optimal balance between accuracy and resource usage, MGDFIS provides a practical solution for small-object detection on resource-constrained UAV platforms.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware Learning</title>
<link>https://arxiv.org/abs/2506.13265</link>
<guid>https://arxiv.org/abs/2506.13265</guid>
<content:encoded><![CDATA[

arXiv:2506.13265v2 Announce Type: replace-cross 
Abstract: Autonomous vehicles that navigate in open-world environments may encounter previously unseen object classes. However, most existing LiDAR panoptic segmentation models rely on closed-set assumptions, failing to detect unknown object instances. In this work, we propose ULOPS, an uncertainty-guided open-set panoptic segmentation framework that leverages Dirichlet-based evidential learning to model predictive uncertainty. Our architecture incorporates separate decoders for semantic segmentation with uncertainty estimation, embedding with prototype association, and instance center prediction. During inference, we leverage uncertainty estimates to identify and segment unknown instances. To strengthen the model's ability to differentiate between known and unknown objects, we introduce three uncertainty-driven loss functions. Uniform Evidence Loss to encourage high uncertainty in unknown regions. Adaptive Uncertainty Separation Loss ensures a consistent difference in uncertainty estimates between known and unknown objects at a global scale. Contrastive Uncertainty Loss refines this separation at the fine-grained level. To evaluate open-set performance, we extend benchmark settings on KITTI-360 and introduce a new open-set evaluation for nuScenes. Extensive experiments demonstrate that ULOPS consistently outperforms existing open-set LiDAR panoptic segmentation methods.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AbRank: A Benchmark Dataset and Metric-Learning Framework for Antibody-Antigen Affinity Ranking</title>
<link>https://arxiv.org/abs/2506.17857</link>
<guid>https://arxiv.org/abs/2506.17857</guid>
<content:encoded><![CDATA[

arXiv:2506.17857v2 Announce Type: replace-cross 
Abstract: Accurate prediction of antibody-antigen (Ab-Ag) binding affinity is essential for therapeutic design and vaccine development, yet the performance of current models is limited by noisy experimental labels, heterogeneous assay conditions, and poor generalization across the vast antibody and antigen sequence space. We introduce AbRank, a large-scale benchmark and evaluation framework that reframes affinity prediction as a pairwise ranking problem. AbRank aggregates over 380,000 binding assays from nine heterogeneous sources, spanning diverse antibodies, antigens, and experimental conditions, and introduces standardized data splits that systematically increase distribution shift, from local perturbations such as point mutations to broad generalization across novel antigens and antibodies. To ensure robust supervision, AbRank defines an m-confident ranking framework by filtering out comparisons with marginal affinity differences, focusing training on pairs with at least an m-fold difference in measured binding strength. As a baseline for the benchmark, we introduce WALLE-Affinity, a graph-based approach that integrates protein language model embeddings with structural information to predict pairwise binding preferences. Our benchmarks reveal significant limitations in current methods under realistic generalization settings and demonstrate that ranking-based training improves robustness and transferability. In summary, AbRank offers a robust foundation for machine learning models to generalize across the antibody-antigen space, with direct relevance for scalable, structure-aware antibody therapeutic design.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaCipher: A Time-Persistent and Universal Multi-Agent Framework for Cipher-Based Jailbreak Attacks for LLMs</title>
<link>https://arxiv.org/abs/2506.22557</link>
<guid>https://arxiv.org/abs/2506.22557</guid>
<content:encoded><![CDATA[

arXiv:2506.22557v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) grow more capable, they face growing vulnerability to sophisticated jailbreak attacks. While developers invest heavily in alignment finetuning and safety guardrails, researchers continue publishing novel attacks, driving progress through adversarial iteration. This dynamic mirrors a strategic game of continual evolution. However, two major challenges hinder jailbreak development: the high cost of querying top-tier LLMs and the short lifespan of effective attacks due to frequent safety updates. These factors limit cost-efficiency and practical impact of research in jailbreak attacks. To address this, we propose MetaCipher, a low-cost, multi-agent jailbreak framework that generalizes across LLMs with varying safety measures. Using reinforcement learning, MetaCipher is modular and adaptive, supporting extensibility to future strategies. Within as few as 10 queries, MetaCipher achieves state-of-the-art attack success rates on recent malicious prompt benchmarks, outperforming prior jailbreak methods. We conduct a large-scale empirical evaluation across diverse victim models and benchmarks, demonstrating its robustness and adaptability. Warning: This paper contains model outputs that may be offensive or harmful, shown solely to demonstrate jailbreak efficacy.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLM-4.1V-Thinking and GLM-4.5V: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.01006</link>
<guid>https://arxiv.org/abs/2507.01006</guid>
<content:encoded><![CDATA[

arXiv:2507.01006v3 Announce Type: replace-cross 
Abstract: We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models (VLMs) designed to advance general-purpose multimodal understanding and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document interpretation. In a comprehensive evaluation across 42 public benchmarks, GLM-4.5V achieves state-of-the-art performance on nearly all tasks among open-source models of similar size, and demonstrates competitive or even superior results compared to closed-source models such as Gemini-2.5-Flash on challenging tasks including Coding and GUI Agents. Meanwhile, the smaller GLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to the much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both GLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information are released at https://github.com/zai-org/GLM-V.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoSE: Skill-by-Skill Mixture-of-Experts Learning for Embodied Autonomous Machines</title>
<link>https://arxiv.org/abs/2507.07818</link>
<guid>https://arxiv.org/abs/2507.07818</guid>
<content:encoded><![CDATA[

arXiv:2507.07818v2 Announce Type: replace-cross 
Abstract: To meet the growing demand for smarter, faster, and more efficient embodied AI solutions, we introduce a novel Mixture-of-Expert (MoE) method that significantly boosts reasoning and learning efficiency for embodied autonomous systems. General MoE models demand extensive training data and complex optimization, which limits their applicability in embodied AI such as autonomous driving (AD) and robotic manipulation. In this work, we propose a skill-oriented MoE called MoSE, which mimics the human learning and reasoning process skill-by-skill, step-by-step. We introduce a skill-oriented routing mechanism that begins with defining and annotating specific skills, enabling experts to identify the necessary competencies for various scenarios and reasoning tasks, thereby facilitating skill-by-skill learning. To better align with multi-step planning in human reasoning and in end-to-end driving models, we build a hierarchical skill dataset and pretrain the router to encourage the model to think step-by-step. Unlike other multi-round dialogues, MoSE integrates valuable auxiliary tasks (e.g. perception-prediction-planning for AD, and high-level and low-level planning for robots) in one single forward process without introducing any extra computational cost. With less than 3B sparsely activated parameters, our model effectively grows more diverse expertise and outperforms models on both AD corner-case reasoning tasks and robot reasoning tasks with less than 40% of the parameters.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Visual Appearance Optimization by Learning from Prior Preferences</title>
<link>https://arxiv.org/abs/2507.15355</link>
<guid>https://arxiv.org/abs/2507.15355</guid>
<content:encoded><![CDATA[

arXiv:2507.15355v2 Announce Type: replace-cross 
Abstract: Adjusting visual parameters such as brightness and contrast is common in our everyday experiences. Finding the optimal parameter setting is challenging due to the large search space and the lack of an explicit objective function, leaving users to rely solely on their implicit preferences. Prior work has explored Preferential Bayesian Optimization (PBO) to address this challenge, involving users to iteratively select preferred designs from candidate sets. However, PBO often requires many rounds of preference comparisons, making it more suitable for designers than everyday end-users. We propose Meta-PO, a novel method that integrates PBO with meta-learning to improve sample efficiency. Specifically, Meta-PO infers prior users' preferences and stores them as models, which are leveraged to intelligently suggest design candidates for the new users, enabling faster convergence and more personalized results. An experimental evaluation of our method for appearance design tasks on 2D and 3D content showed that participants achieved satisfactory appearance in 5.86 iterations using Meta-PO when participants shared similar goals with a population (e.g., tuning for a ``warm'' look) and in 8 iterations even generalizes across divergent goals (e.g., from ``vintage'', ``warm'', to ``holiday''). Meta-PO makes personalized visual optimization more applicable to end-users through a generalizable, more efficient optimization conditioned on preferences, with the potential to scale interface personalization more broadly.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRWKV: Focusing on Object Edges for Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2507.18594</link>
<guid>https://arxiv.org/abs/2507.18594</guid>
<content:encoded><![CDATA[

arXiv:2507.18594v2 Announce Type: replace-cross 
Abstract: Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonconvex Optimization Framework for Group-Sparse Feedback Linear-Quadratic Optimal Control: Non-Penalty Approach</title>
<link>https://arxiv.org/abs/2507.19895</link>
<guid>https://arxiv.org/abs/2507.19895</guid>
<content:encoded><![CDATA[

arXiv:2507.19895v3 Announce Type: replace-cross 
Abstract: In [1], the distributed linear-quadratic problem with fixed communication topology (DFT-LQ) and the sparse feedback LQ problem (SF-LQ) are formulated into a nonsmooth and nonconvex optimization problem with affine constraints. Moreover, a penalty approach is considered in [1], and the PALM (proximal alternating linearized minimization) algorithm is studied with convergence and complexity analysis. In this paper, we aim to address the inherent drawbacks of the penalty approach, such as the challenge of tuning the penalty parameter and the risk of introducing spurious stationary points. Specifically, we first reformulate the SF-LQ problem and the DFT-LQ problem from an epi-composition function perspective, aiming to solve constrained problem directly. Then, from a theoretical viewpoint, we revisit the alternating direction method of multipliers (ADMM) and establish its convergence to the set of cluster points under certain assumptions. When these assumptions do not hold, we show that alternative approaches combining subgradient descent with Difference-of-Convex relaxation methods can be effectively utilized. In summary, our results enable the direct design of group-sparse feedback gains with theoretical guarantees, without resorting to convex surrogates, restrictive structural assumptions or penalty formulations that incorporate constraints into the cost function.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoteGCL: Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation</title>
<link>https://arxiv.org/abs/2507.21563</link>
<guid>https://arxiv.org/abs/2507.21563</guid>
<content:encoded><![CDATA[

arXiv:2507.21563v3 Announce Type: replace-cross 
Abstract: Recommendation systems often suffer from data sparsity caused by limited user-item interactions, which degrade their performance and amplify popularity bias in real-world scenarios. This paper proposes a novel data augmentation framework that leverages Large Language Models (LLMs) and item textual descriptions to enrich interaction data. By few-shot prompting LLMs multiple times to rerank items and aggregating the results via majority voting, we generate high-confidence synthetic user-item interactions, supported by theoretical guarantees based on the concentration of measure. To effectively leverage the augmented data in the context of a graph recommendation system, we integrate it into a graph contrastive learning framework to mitigate distributional shift and alleviate popularity bias. Extensive experiments show that our method improves accuracy and reduces popularity bias, outperforming strong baselines.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation biases: will we achieve complete understanding by analyzing representations?</title>
<link>https://arxiv.org/abs/2507.22216</link>
<guid>https://arxiv.org/abs/2507.22216</guid>
<content:encoded><![CDATA[

arXiv:2507.22216v2 Announce Type: replace-cross 
Abstract: A common approach in neuroscience is to study neural representations as a means to understand a system -- increasingly, by relating the neural representations to the internal representations learned by computational models. However, a recent work in machine learning (Lampinen, 2024) shows that learned feature representations may be biased to over-represent certain features, and represent others more weakly and less-consistently. For example, simple (linear) features may be more strongly and more consistently represented than complex (highly nonlinear) features. These biases could pose challenges for achieving full understanding of a system through representational analysis. In this perspective, we illustrate these challenges -- showing how feature representation biases can lead to strongly biased inferences from common analyses like PCA, regression, and RSA. We also present homomorphic encryption as a simple case study of the potential for strong dissociation between patterns of representation and computation. We discuss the implications of these results for representational comparisons between systems, and for neuroscience more generally.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens</title>
<link>https://arxiv.org/abs/2508.01191</link>
<guid>https://arxiv.org/abs/2508.01191</guid>
<content:encoded><![CDATA[

arXiv:2508.01191v3 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience</title>
<link>https://arxiv.org/abs/2508.04700</link>
<guid>https://arxiv.org/abs/2508.04700</guid>
<content:encoded><![CDATA[
<div> LVLMs, computer-use agents, SEAgent, experiential learning, specialist-to-generalist training<br />
Summary:<br />
SEAgent proposes a self-evolving framework for computer-use agents to autonomously master novel software environments through experiential learning. It features a World State Model for trajectory assessment, a Curriculum Generator for task generation, and a policy updated through experiential learning and GRPO. The agent utilizes a specialist-to-generalist training strategy to integrate insights from specialist agents and evolve into a stronger generalist CUA. SEAgent surpasses ensembles of specialist agents on specialized software, achieving a 23.2% improvement in success rate over UI-TARS in the OS-World environment validation. The framework revolutionizes the adaptability of LVLMs as CUAs, enhancing their performance in unfamiliar software scenarios without human annotations.<br /><br />Summary: <div>
arXiv:2508.04700v2 Announce Type: replace-cross 
Abstract: Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants</title>
<link>https://arxiv.org/abs/2508.08266</link>
<guid>https://arxiv.org/abs/2508.08266</guid>
<content:encoded><![CDATA[
<div> land patents, large language models, georeferencing, Virginia, historical

Summary:
Large language models were evaluated for converting historical land patent descriptions into geographically accurate coordinates in Virginia. A corpus of 5,471 patent abstracts from 1695-1732 was analyzed, with a benchmark of 43 test cases. The top-performing LLM achieved a mean error of 23 km, outperforming other LLMs and external baselines. A five-call ensemble reduced errors further to 19 km at low cost. Redacting patentee names increased errors, indicating reliance on textual descriptions. The cost-efficient gpt-4o-2024-08-06 model maintained a 28 km error with a strong cost-accuracy benchmark. External geocoding tools did not offer significant benefits. These findings highlight the potential of LLMs for scalable, accurate, and cost-effective historical georeferencing.<br /><br />Summary: <div>
arXiv:2508.08266v1 Announce Type: new 
Abstract: Virginia's seventeenth- and eighteenth-century land patents survive primarily as narrative metes-and-bounds descriptions, limiting spatial analysis. This study systematically evaluates current-generation large language models (LLMs) in converting these prose abstracts into geographically accurate latitude/longitude coordinates within a focused evaluation context. A digitized corpus of 5,471 Virginia patent abstracts (1695-1732) is released, with 43 rigorously verified test cases serving as an initial, geographically focused benchmark. Six OpenAI models across three architectures (o-series, GPT-4-class, and GPT-3.5) were tested under two paradigms: direct-to-coordinate and tool-augmented chain-of-thought invoking external geocoding APIs. Results were compared with a GIS-analyst baseline, the Stanford NER geoparser, Mordecai-3, and a county-centroid heuristic.
  The top single-call model, o3-2025-04-16, achieved a mean error of 23 km (median 14 km), outperforming the median LLM (37.4 km) by 37.5%, the weakest LLM (50.3 km) by 53.5%, and external baselines by 67% (GIS analyst) and 70% (Stanford NER). A five-call ensemble further reduced errors to 19 km (median 12 km) at minimal additional cost (approx. USD 0.20 per grant), outperforming the median LLM by 48.6%. A patentee-name-redaction ablation increased error by about 9%, indicating reliance on textual landmark and adjacency descriptions rather than memorization. The cost-efficient gpt-4o-2024-08-06 model maintained a 28 km mean error at USD 1.09 per 1,000 grants, establishing a strong cost-accuracy benchmark; external geocoding tools offered no measurable benefit in this evaluation.
  These findings demonstrate the potential of LLMs for scalable, accurate, and cost-effective historical georeferencing.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI</title>
<link>https://arxiv.org/abs/2508.08270</link>
<guid>https://arxiv.org/abs/2508.08270</guid>
<content:encoded><![CDATA[
<div> Keywords: large multimodal models, biomedical data, Doctor Sun, SunMed-VL, medical multimodal research

Summary:
Doctor Sun is a novel large multimodal generative model specifically designed for the medical field. It aims to improve the understanding of complex medical concepts by integrating diverse data modalities such as text and images. The model combines a pre-trained vision encoder with a medical language model and undergoes two-stage training on various medical datasets. By focusing on feature alignment and instruction tuning, Doctor Sun can effectively capture the intricate relationship between texts and images in medical data. The researchers also introduce the SunMed-VL dataset, a bilingual medical multimodal dataset, to support the advancement of biomedical multimodal research. The resources, models, and code associated with Doctor Sun are made freely available to the research community. This contribution holds promise for enhancing pathology analysis, radiology report generation, and other biomedical tasks. 

<br /><br />Summary: sourceMapping: https://arxiv.org/abs/2508.08270v1 <div>
arXiv:2508.08270v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) have demonstrated significant potential in providing innovative solutions for various biomedical tasks, including pathology analysis, radiology report generation, and biomedical assistance. However, the existing multimodal biomedical AI is typically based on foundation LLMs, thus hindering the understanding of intricate medical concepts with limited medical training data. Moreover, recent LLaVA-induced medical LMMs struggle to effectively capture the intricate relationship between the texts and the images. Therefore, we introduce Doctor Sun, a large multimodal generative model specialized in medicine, developed to encode, integrate, and interpret diverse biomedical data modalities such as text and images. In particular, Doctor Sun integrates a pre-trained vision encoder with a medical LLM and conducts two-stage training on various medical datasets, focusing on feature alignment and instruction tuning. Moreover, we release SunMed-VL, a wide-range bilingual medical multimodal dataset, along with all associated models, code, and resources, to freely support the advancement of biomedical multimodal research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Heterogeneity-Aware and Energy-Efficient Topology Optimization for Decentralized Federated Learning in Edge Environment</title>
<link>https://arxiv.org/abs/2508.08278</link>
<guid>https://arxiv.org/abs/2508.08278</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Edge Computing, Decentralized Federated Learning, Energy Efficiency, Data Heterogeneity<br />
Summary:<br />
Hat-DFed is a novel framework for Decentralized Federated Learning (DFL) in Edge Computing (EC) systems. It addresses challenges related to dynamic topology changes, resource heterogeneity, and data heterogeneity in the learning process. The framework formulates topology construction as a dual optimization problem to maximize model performance and minimize energy consumption. A two-phase algorithm is designed to construct optimal communication topologies and estimate their impact on performance and energy cost. Additionally, an importance-aware model aggregation mechanism is integrated to mitigate the effects of data heterogeneity on model performance. Hat-DFed aims to improve the efficiency and effectiveness of DFL in complex edge environments, providing a comprehensive solution for collaborative training of AI models while ensuring data privacy and minimizing communication bottlenecks. <div>
arXiv:2508.08278v1 Announce Type: new 
Abstract: Federated learning (FL) has emerged as a promising paradigm within edge computing (EC) systems, enabling numerous edge devices to collaboratively train artificial intelligence (AI) models while maintaining data privacy. To overcome the communication bottlenecks associated with centralized parameter servers, decentralized federated learning (DFL), which leverages peer-to-peer (P2P) communication, has been extensively explored in the research community. Although researchers design a variety of DFL approach to ensure model convergence, its iterative learning process inevitably incurs considerable cost along with the growth of model complexity and the number of participants. These costs are largely influenced by the dynamic changes of topology in each training round, particularly its sparsity and connectivity conditions. Furthermore, the inherent resources heterogeneity in the edge environments affects energy efficiency of learning process, while data heterogeneity degrades model performance. These factors pose significant challenges to the design of an effective DFL framework for EC systems. To this end, we propose Hat-DFed, a heterogeneity-aware and coset-effective decentralized federated learning (DFL) framework. In Hat-DFed, the topology construction is formulated as a dual optimization problem, which is then proven to be NP-hard, with the goal of maximizing model performance while minimizing cumulative energy consumption in complex edge environments. To solve this problem, we design a two-phase algorithm that dynamically constructs optimal communication topologies while unbiasedly estimating their impact on both model performance and energy cost. Additionally, the algorithm incorporates an importance-aware model aggregation mechanism to mitigate performance degradation caused by data heterogeneity.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XFMNet: Decoding Cross-Site and Nonstationary Water Patterns via Stepwise Multimodal Fusion for Long-Term Water Quality Forecasting</title>
<link>https://arxiv.org/abs/2508.08279</link>
<guid>https://arxiv.org/abs/2508.08279</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-term time-series forecasting, water quality prediction, remote sensing, multimodal fusion network, spatial dynamics <br />
Summary: 
XFMNet is a novel approach for long-term time-series forecasting in water quality prediction, particularly in multi-site scenarios. It integrates remote sensing data to provide spatial and environmental context in river networks. The network aligns temporal resolutions and decomposes components to handle complex periodicity and nonstationarity. A cross-attention gated fusion module dynamically integrates temporal patterns with spatial and ecological cues, enhancing robustness to anomalies. Through progressive and recursive fusion, XFMNet captures both long-term trends and short-term fluctuations effectively. Extensive experiments on real-world datasets demonstrate significant improvements over existing methods, showcasing the efficacy of XFMNet for spatially distributed time series prediction.<br /><br />Summary: <div>
arXiv:2508.08279v1 Announce Type: new 
Abstract: Long-term time-series forecasting is critical for environmental monitoring, yet water quality prediction remains challenging due to complex periodicity, nonstationarity, and abrupt fluctuations induced by ecological factors. These challenges are further amplified in multi-site scenarios that require simultaneous modeling of temporal and spatial dynamics. To tackle this, we introduce XFMNet, a stepwise multimodal fusion network that integrates remote sensing precipitation imagery to provide spatial and environmental context in river networks. XFMNet first aligns temporal resolutions between water quality series and remote sensing inputs via adaptive downsampling, followed by locally adaptive decomposition to disentangle trend and cycle components. A cross-attention gated fusion module dynamically integrates temporal patterns with spatial and ecological cues, enhancing robustness to nonstationarity and site-specific anomalies. Through progressive and recursive fusion, XFMNet captures both long-term trends and short-term fluctuations. Extensive experiments on real-world datasets demonstrate substantial improvements over state-of-the-art baselines, highlighting the effectiveness of XFMNet for spatially distributed time series prediction.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoSSDA: A Semi-Supervised Domain Adaptation Framework for Multivariate Time-Series Classification using Momentum Encoder</title>
<link>https://arxiv.org/abs/2508.08280</link>
<guid>https://arxiv.org/abs/2508.08280</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, semi-supervised domain adaptation, multivariate time-series classification, momentum encoder, MoSSDA<br />
Summary:<br />
Deep learning has shown immense potential in various fields, but suffers from performance degradation when faced with domain shifts. The proposed MoSSDA framework tackles this issue in multivariate time-series classification by incorporating a two-step momentum encoder-utilized approach. This framework utilizes a domain-invariant encoder to extract features from both the source and target domains, followed by a positive contrastive module powered by an online momentum encoder. By leveraging the learned features without data augmentation, MoSSDA achieves state-of-the-art performance on diverse datasets with different backbones and various unlabeled ratios in the target domain data. The efficacy of each module, including two-stage learning, has been validated through an ablation study. The MoSSDA code is available on GitHub for further exploration. <br /> <div>
arXiv:2508.08280v1 Announce Type: new 
Abstract: Deep learning has emerged as the most promising approach in various fields; however, when the distributions of training and test data are different (domain shift), the performance of deep learning models can degrade. Semi-supervised domain adaptation (SSDA) is a major approach for addressing this issue, assuming that a fully labeled training set (source domain) is available, but the test set (target domain) provides labels only for a small subset. In this study, we propose a novel two-step momentum encoder-utilized SSDA framework, MoSSDA, for multivariate time-series classification. Time series data are highly sensitive to noise, and sequential dependencies cause domain shifts resulting in critical performance degradation. To obtain a robust, domain-invariant and class-discriminative representation, MoSSDA employs a domain-invariant encoder to learn features from both source and target domains. Subsequently, the learned features are fed to a mixup-enhanced positive contrastive module consisting of an online momentum encoder. The final classifier is trained with learned features that exhibit consistency and discriminability with limited labeled target domain data, without data augmentation. We applied a two-stage process by separating the gradient flow between the encoders and the classifier to obtain rich and complex representations. Through extensive experiments on six diverse datasets, MoSSDA achieved state-of-the-art performance for three different backbones and various unlabeled ratios in the target domain data. The Ablation study confirms that each module, including two-stage learning, is effective in improving the performance. Our code is available at https://github.com/seonyoungKimm/MoSSDA
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-grained spatial-temporal feature complementarity for accurate online cellular traffic prediction</title>
<link>https://arxiv.org/abs/2508.08281</link>
<guid>https://arxiv.org/abs/2508.08281</guid>
<content:encoded><![CDATA[
<div> Keywords: telecom data, cellular traffic prediction, Multi-Grained Spatial-Temporal feature Complementarity, concept drift, online learning <br />
Summary:<br />
The paper introduces a new method, Multi-Grained Spatial-Temporal feature Complementarity (MGSTC), to improve cellular traffic prediction in the telecom industry. It addresses the challenges of sporadic and bursty traffic patterns by segmenting data into chunks and utilizing coarse-grained temporal attention for trend reference and fine-grained spatial attention for detailed correlations. This approach leverages the complementarity of multi-grained spatial-temporal features to enhance prediction accuracy. Furthermore, an online learning strategy is implemented to detect concept drift in real-time and adjust parameters accordingly. Experimental results on real-world datasets demonstrate the superiority of MGSTC over eleven state-of-the-art baselines consistently.<br />Summary: <div>
arXiv:2508.08281v1 Announce Type: new 
Abstract: Knowledge discovered from telecom data can facilitate proactive understanding of network dynamics and user behaviors, which in turn empowers service providers to optimize cellular traffic scheduling and resource allocation. Nevertheless, the telecom industry still heavily relies on manual expert intervention. Existing studies have been focused on exhaustively explore the spatial-temporal correlations. However, they often overlook the underlying characteristics of cellular traffic, which are shaped by the sporadic and bursty nature of telecom services. Additionally, concept drift creates substantial obstacles to maintaining satisfactory accuracy in continuous cellular forecasting tasks. To resolve these problems, we put forward an online cellular traffic prediction method grounded in Multi-Grained Spatial-Temporal feature Complementarity (MGSTC). The proposed method is devised to achieve high-precision predictions in practical continuous forecasting scenarios. Concretely, MGSTC segments historical data into chunks and employs the coarse-grained temporal attention to offer a trend reference for the prediction horizon. Subsequently, fine-grained spatial attention is utilized to capture detailed correlations among network elements, which enables localized refinement of the established trend. The complementarity of these multi-grained spatial-temporal features facilitates the efficient transmission of valuable information. To accommodate continuous forecasting needs, we implement an online learning strategy that can detect concept drift in real-time and promptly switch to the appropriate parameter update stage. Experiments carried out on four real-world datasets demonstrate that MGSTC outperforms eleven state-of-the-art baselines consistently.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Transformers through the Lens of Pavlovian Conditioning</title>
<link>https://arxiv.org/abs/2508.08289</link>
<guid>https://arxiv.org/abs/2508.08289</guid>
<content:encoded><![CDATA[
<div> attention mechanisms, Pavlovian conditioning, associative memory, capacity theorem, error propagation analysis<br />
<br />
Summary:<br />
The article introduces a new theoretical framework that interprets the core computation of attention in Transformer architectures as Pavlovian conditioning. It presents a mathematical model that simplifies the analysis of attention's associative process by mapping queries, keys, and values to classical conditioning elements. The framework suggests that attention operations form transient associative memories through Hebbian rules, with CS-US pairs creating dynamic associations for later retrieval. The article also presents theoretical insights: a capacity theorem indicating that attention heads can store a certain number of associations before interference occurs, an error propagation analysis highlighting architectural trade-offs for reliability, and a discussion of how biologically plausible learning rules could improve Transformer architectures. This novel framework implies that the success of AI may be attributed to implementing computational principles derived from millions of years of biological evolution. <br /> <div>
arXiv:2508.08289v1 Announce Type: new 
Abstract: Transformer architectures have revolutionized artificial intelligence (AI) through their attention mechanisms, yet the computational principles underlying their success remain opaque. We present a novel theoretical framework that reinterprets the core computation of attention as Pavlovian conditioning. Our model finds a direct mathematical analogue in linear attention, which simplifies the analysis of the underlying associative process. We demonstrate that attention's queries, keys, and values can be mapped to the three elements of classical conditioning: test stimuli that probe associations, conditional stimuli (CS) that serve as retrieval cues, and unconditional stimuli (US) that contain response information. Through this lens, we suggest that each attention operation constructs a transient associative memory via a Hebbian rule, where CS-US pairs form dynamic associations that test stimuli can later retrieve. Our framework yields several theoretical insights grounded in this linearized model: (1) a capacity theorem showing that attention heads can store O($\sqrt{d_k}$) associations before interference degrades retrieval; (2) an error propagation analysis revealing fundamental architectural trade-offs of balancing model depth, width, and head redundancy to maintain reliability; and (3) an understanding of how biologically plausible learning rules could enhance transformer architectures. By establishing this deep connection, we suggest that the success of modern AI may stem not from architectural novelty alone, but from implementing computational principles that biology optimized over millions of years of evolution.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Emissivity Retrieval from Hyperspectral Data via Physics-Guided Variational Inference</title>
<link>https://arxiv.org/abs/2508.08291</link>
<guid>https://arxiv.org/abs/2508.08291</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, hyperspectral imaging, generative model, physics-based, uncertainty quantification

Summary: 
This study introduces a novel approach to hyperspectral imaging (HSI) target identification using a physics-conditioned generative model. The model, based on a probabilistic latent-variable framework, learns the distribution of HSI radiance measurements and predicts the emissivity spectrum. By incorporating information about the HSI scene's atmosphere and background, the model provides context for accurate encoding and decoding processes. To ensure unbiased results and encourage physically consistent mappings, an in-the-loop augmentation scheme and physics-based loss criteria are employed. Monte-Carlo sampling enables uncertainty quantification and produces interpretable emissivity distributions. Furthermore, a material matching scheme returns likely material matches based on inferred emissivity distributions. This approach enables the incorporation of contextual information, captures variations in material spectra, and provides interpretable probability measures for candidate materials based on remotely-sensed radiance measurements. <br /><br />Summary: <div>
arXiv:2508.08291v1 Announce Type: new 
Abstract: Recent research has proven neural networks to be a powerful tool for performing hyperspectral imaging (HSI) target identification. However, many deep learning frameworks deliver a single material class prediction and operate on a per-pixel basis; such approaches are limited in their interpretability and restricted to predicting materials that are accessible in available training libraries. In this work, we present an inverse modeling approach in the form of a physics-conditioned generative model.A probabilistic latent-variable model learns the underlying distribution of HSI radiance measurements and produces the conditional distribution of the emissivity spectrum. Moreover, estimates of the HSI scene's atmosphere and background are used as a physically relevant conditioning mechanism to contextualize a given radiance measurement during the encoding and decoding processes. Furthermore, we employ an in-the-loop augmentation scheme and physics-based loss criteria to avoid bias towards a predefined training material set and to encourage the model to learn physically consistent inverse mappings. Monte-Carlo sampling of the model's conditioned posterior delivers a sought emissivity distribution and allows for interpretable uncertainty quantification. Moreover, a distribution-based material matching scheme is presented to return a set of likely material matches for an inferred emissivity distribution. Hence, we present a strategy to incorporate contextual information about a given HSI scene, capture the possible variation of underlying material spectra, and provide interpretable probability measures of a candidate material accounting for given remotely-sensed radiance measurement.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel-Wise MLPs Improve the Generalization of Recurrent Convolutional Networks</title>
<link>https://arxiv.org/abs/2508.08298</link>
<guid>https://arxiv.org/abs/2508.08298</guid>
<content:encoded><![CDATA[
<div> Keywords: channel mixing, recurrent convolutional networks, multi-layer perceptrons, generalization capabilities, neural program synthesis 

Summary: 
The study compares DARC and DAMP architectures in recurrent convolutional networks. DAMP, incorporating a gated MLP for channel mixing, outperformed DARC in generalization performance on Re-ARC benchmark for both in-distribution and out-of-distribution scenarios. Channel-wise mixing through MLPs in DAMP results in improved learning of robust computational patterns. The findings have implications for neural program synthesis, indicating DAMP as a promising target architecture for hypernetwork approaches. 

Summary: <div>
arXiv:2508.08298v1 Announce Type: new 
Abstract: We investigate the impact of channel-wise mixing via multi-layer perceptrons (MLPs) on the generalization capabilities of recurrent convolutional networks. Specifically, we compare two architectures: DARC (Depth Aware Recurrent Convolution), which employs a simple recurrent convolutional structure, and DAMP (Depth Aware Multi-layer Perceptron), which extends DARC with a gated MLP for channel mixing. Using the Re-ARC benchmark, we find that DAMP significantly outperforms DARC in both in-distribution and out-of-distribution generalization under exact-match grading criteria. These results suggest that explicit channel mixing through MLPs enables recurrent convolutional networks to learn more robust and generalizable computational patterns. Our findings have implications for neural program synthesis and highlight the potential of DAMP as a target architecture for hypernetwork approaches.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative study of machine learning and statistical methods for automatic identification and quantification in {\gamma}-ray spectrometry</title>
<link>https://arxiv.org/abs/2508.08306</link>
<guid>https://arxiv.org/abs/2508.08306</guid>
<content:encoded><![CDATA[
<div> machine learning, statistical approach, gamma-ray spectrometry, simulated datasets, radionuclide identification<br />
Summary:<br />
- Lack of common benchmarks in gamma-ray spectrometry evaluation hinders method comparison.<br />
- An open-source benchmark with simulated datasets, analysis codes, and evaluation metrics is proposed.<br />
- Three scenarios investigated: known spectral signatures, deformed signatures, and shifted signatures.<br />
- Statistical approach outperforms machine learning in radionuclide identification across all scenarios.<br />
- Statistical approach is most effective with well-modeled spectral signatures, while machine learning is useful for uncertain conditions. <br />
- Statistical approach provides accurate radionuclide counting quantifications, while machine learning delivers less satisfactory results. <br /> <div>
arXiv:2508.08306v1 Announce Type: new 
Abstract: During the last decade, a large number of different numerical methods have been proposed to tackle the automatic identification and quantification in {\gamma}-ray spectrometry. However, the lack of common benchmarks, including datasets, code and comparison metrics, makes their evaluation and comparison hard. In that context, we propose an open-source benchmark that comprises simulated datasets of various {\gamma}-spectrometry settings, codes of different analysis approaches and evaluation metrics. This allows us to compare the state-of-the-art end-to-end machine learning with a statistical unmixing approach using the full spectrum. Three scenarios have been investigated: (1) spectral signatures are assumed to be known; (2) spectral signatures are deformed due to physical phenomena such as Compton scattering and attenuation; and (3) spectral signatures are shifted (e.g., due to temperature variation). A large dataset of 200000 simulated spectra containing nine radionuclides with an experimental natural background is used for each scenario with multiple radionuclides present in the spectrum. Regarding identification performance, the statistical approach consistently outperforms the machine learning approaches across all three scenarios for all comparison metrics. However, the performance of the statistical approach can be significantly impacted when spectral signatures are not modeled correctly. Consequently, the full-spectrum statistical approach is most effective with known or well-modeled spectral signatures, while end-to-end machine learning is a good alternative when measurement conditions are uncertain for radionuclide identification. Concerning the quantification task, the statistical approach provides accurate estimates of radionuclide counting, while the machine learning methods deliver less satisfactory results.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weather-Driven Agricultural Decision-Making Using Digital Twins Under Imperfect Conditions</title>
<link>https://arxiv.org/abs/2508.08326</link>
<guid>https://arxiv.org/abs/2508.08326</guid>
<content:encoded><![CDATA[
<div> Keywords: digital twin technology, data-driven decision-making, agricultural weather data, neural network models, Cerealia framework

Summary: 
Digital twin technology offers a real-time virtual representation of physical systems, benefiting data-driven decision-making in digital agriculture. The research demonstrates the utility of digital twins in detecting inconsistencies in agricultural weather data, crucial for agricultural decision-making and automation. The Cerealia framework, utilizing neural network models, allows users to identify data anomalies when perfect weather feeds are not available. It aids users in making informed decisions based on reliable data. A prototype of Cerealia is developed on the NVIDIA Jetson Orin platform and tested with an operational weather network in a commercial orchard and publicly available weather datasets. This research showcases the potential of digital twin technology in enhancing agricultural operations and improving efficiency in data analysis and decision-making processes. 

<br /><br />Summary: <div>
arXiv:2508.08326v1 Announce Type: new 
Abstract: By offering a dynamic, real-time virtual representation of physical systems, digital twin technology can enhance data-driven decision-making in digital agriculture. Our research shows how digital twins are useful for detecting inconsistencies in agricultural weather data measurements, which are key attributes for various agricultural decision-making and automation tasks. We develop a modular framework named Cerealia that allows end-users to check for data inconsistencies when perfect weather feeds are unavailable. Cerealia uses neural network models to check anomalies and aids end-users in informed decision-making. We develop a prototype of Cerealia using the NVIDIA Jetson Orin platform and test it with an operational weather network established in a commercial orchard as well as publicly available weather datasets.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSA-Net: Hierarchical and Structure-Aware Framework for Efficient and Scalable Molecular Language Modeling</title>
<link>https://arxiv.org/abs/2508.08334</link>
<guid>https://arxiv.org/abs/2508.08334</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Molecular Representation Learning, Over-smoothing Problem, Feature Projection, Mamba<br />
<br />
Summary:<br />
The paper introduces a novel framework, HSA-Net, to address the over-smoothing problem in Graph Neural Networks for molecular representation learning. The framework consists of two modules: Hierarchical Adaptive Projector (HAP) and Source-Aware Fusion (SAF). HAP dynamically switches between cross-attention and Graph-Mamba projectors for different graph layers, preserving global topological information and fine-grained details. SAF module then merges these multi-level features, selecting fusion experts based on aggregation features. Experimental results show that HSA-Net outperforms current state-of-the-art methods, providing high-quality, multi-level features for downstream tasks like molecular captioning and property prediction. <div>
arXiv:2508.08334v1 Announce Type: new 
Abstract: Molecular representation learning, a cornerstone for downstream tasks like molecular captioning and molecular property prediction, heavily relies on Graph Neural Networks (GNN). However, GNN suffers from the over-smoothing problem, where node-level features collapse in deep GNN layers. While existing feature projection methods with cross-attention have been introduced to mitigate this issue, they still perform poorly in deep features. This motivated our exploration of using Mamba as an alternative projector for its ability to handle complex sequences. However, we observe that while Mamba excels at preserving global topological information from deep layers, it neglects fine-grained details in shallow layers. The capabilities of Mamba and cross-attention exhibit a global-local trade-off. To resolve this critical global-local trade-off, we propose Hierarchical and Structure-Aware Network (HSA-Net), a novel framework with two modules that enables a hierarchical feature projection and fusion. Firstly, a Hierarchical Adaptive Projector (HAP) module is introduced to process features from different graph layers. It learns to dynamically switch between a cross-attention projector for shallow layers and a structure-aware Graph-Mamba projector for deep layers, producing high-quality, multi-level features. Secondly, to adaptively merge these multi-level features, we design a Source-Aware Fusion (SAF) module, which flexibly selects fusion experts based on the characteristics of the aggregation features, ensuring a precise and effective final representation fusion. Extensive experiments demonstrate that our HSA-Net framework quantitatively and qualitatively outperforms current state-of-the-art (SOTA) methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHeRL-FL: When Representation Learning Meets Split Learning in Hierarchical Federated Learning</title>
<link>https://arxiv.org/abs/2508.08339</link>
<guid>https://arxiv.org/abs/2508.08339</guid>
<content:encoded><![CDATA[
<div> Hierarchical FL, split learning, edge computing, federated learning, image classification

Summary:
In this study, the authors introduce SHeRL-FL, a novel approach that integrates split learning and hierarchical model aggregation for efficient and scalable federated learning. By allowing edge clients and servers to compute training objectives independently, SHeRL-FL reduces coordination complexity and communication overhead significantly. Experimental results using various datasets and models demonstrate that SHeRL-FL outperforms centralized FL, HierFL, and SplitFed in terms of reduced data transmission by over 90% and improved hierarchical split learning methods. The proposed method shows promise in addressing computational heterogeneity issues in large-scale networks and reducing the training burden on resource-limited devices. <div>
arXiv:2508.08339v1 Announce Type: new 
Abstract: Federated learning (FL) is a promising approach for addressing scalability and latency issues in large-scale networks by enabling collaborative model training without requiring the sharing of raw data. However, existing FL frameworks often overlook the computational heterogeneity of edge clients and the growing training burden on resource-limited devices. However, FL suffers from high communication costs and complex model aggregation, especially with large models. Previous works combine split learning (SL) and hierarchical FL (HierFL) to reduce device-side computation and improve scalability, but this introduces training complexity due to coordination across tiers. To address these issues, we propose SHeRL-FL, which integrates SL and hierarchical model aggregation and incorporates representation learning at intermediate layers. By allowing clients and edge servers to compute training objectives independently of the cloud, SHeRL-FL significantly reduces both coordination complexity and communication overhead. To evaluate the effectiveness and efficiency of SHeRL-FL, we performed experiments on image classification tasks using CIFAR-10, CIFAR-100, and HAM10000 with AlexNet, ResNet-18, and ResNet-50 in both IID and non-IID settings. In addition, we evaluate performance on image segmentation tasks using the ISIC-2018 dataset with a ResNet-50-based U-Net. Experimental results demonstrate that SHeRL-FL reduces data transmission by over 90\% compared to centralized FL and HierFL, and by 50\% compared to SplitFed, which is a hybrid of FL and SL, and further improves hierarchical split learning methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy-Pattern Tsetlin Machine</title>
<link>https://arxiv.org/abs/2508.08350</link>
<guid>https://arxiv.org/abs/2508.08350</guid>
<content:encoded><![CDATA[
<div> Keywords: all-or-nothing clause evaluation, fuzzy-pattern Tsetlin Machine, pattern matching, online learning, high accuracy

Summary:
The paper introduces the Fuzzy-Pattern Tsetlin Machine (FPTM), a novel variant that utilizes a fuzzy clause evaluation strategy. This approach allows clauses to contribute to the overall vote even if some literals fail, leading to more flexible and robust pattern matching. FPTM achieves high accuracy with significantly fewer clauses than standard Tsetlin Machines, reducing memory requirements and training time. On the IMDb dataset, FPTM achieves 90.15% accuracy with only one clause per class, while training up to 316x faster compared to existing models. On Fashion-MNIST, FPTM demonstrates accuracy improvement with a reduced number of clauses, outperforming the Composite TM. Moreover, FPTM excels on the Amazon Sales dataset with noise, surpassing the Graph Tsetlin Machine and a Graph Convolutional Neural Network in accuracy. The proposed fuzzy mechanism enables efficient online learning on microcontrollers, with inference throughput reaching 34.5 million predictions/second. 

<br /><br />Summary: <div>
arXiv:2508.08350v1 Announce Type: new 
Abstract: The "all-or-nothing" clause evaluation strategy is a core mechanism in the Tsetlin Machine (TM) family of algorithms. In this approach, each clause - a logical pattern composed of binary literals mapped to input data - is disqualified from voting if even a single literal fails. Due to this strict requirement, standard TMs must employ thousands of clauses to achieve competitive accuracy. This paper introduces the Fuzzy-Pattern Tsetlin Machine (FPTM), a novel variant where clause evaluation is fuzzy rather than strict. If some literals in a clause fail, the remaining ones can still contribute to the overall vote with a proportionally reduced score. As a result, each clause effectively consists of sub-patterns that adapt individually to the input, enabling more flexible, efficient, and robust pattern matching. The proposed fuzzy mechanism significantly reduces the required number of clauses, memory footprint, and training time, while simultaneously improving accuracy. On the IMDb dataset, FPTM achieves 90.15% accuracy with only one clause per class, a 50x reduction in clauses and memory over the Coalesced Tsetlin Machine. FPTM trains up to 316x faster (45 seconds vs. 4 hours) and fits within 50 KB, enabling online learning on microcontrollers. Inference throughput reaches 34.5 million predictions/second (51.4 GB/s). On Fashion-MNIST, accuracy reaches 92.18% (2 clauses), 93.19% (20 clauses) and 94.68% (8000 clauses), a ~400x clause reduction compared to the Composite TM's 93.00% (8000 clauses). On the Amazon Sales dataset with 20% noise, FPTM achieves 85.22% accuracy, significantly outperforming the Graph Tsetlin Machine (78.17%) and a Graph Convolutional Neural Network (66.23%).
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaled-Dot-Product Attention as One-Sided Entropic Optimal Transport</title>
<link>https://arxiv.org/abs/2508.08369</link>
<guid>https://arxiv.org/abs/2508.08369</guid>
<content:encoded><![CDATA[
<div> Entropic Optimal Transport, Attention mechanism, Gradient computation, Information geometry, Fisher Information Matrix <br />
<br />Summary: The article provides a theoretical justification for the scaled-dot-product attention (SDPA) mechanism used in deep learning. It shows that the attention forward pass solves a degenerate, one-sided Entropic Optimal Transport (EOT) problem, maximizing similarity while being maximally entropic. This leads to a specific information geometry on the space of attention distributions, characterized by the Fisher Information Matrix. The backward pass, computed via backpropagation, is mathematically equivalent to an advantage-based policy gradient, a variance-reduced update rule from reinforcement learning. This unified view reveals SDPA as a principled mechanism where the forward pass performs optimal inference and the backward pass implements a rational, manifold-aware learning update. <div>
arXiv:2508.08369v1 Announce Type: new 
Abstract: The scaled-dot-product attention (SDPA) mechanism is a core component of modern deep learning, but its mathematical form is often motivated by heuristics. This work provides a first-principles justification for SDPA. We first show that the attention forward pass is the exact solution to a degenerate, one-sided Entropic Optimal Transport (EOT) problem, which seeks a distribution that maximizes similarity while being maximally entropic. This optimization perspective has a direct consequence for the backward pass. We prove that the standard gradient computed via backpropagation is mathematically identical to an advantage-based policy gradient, a variance-reduced update rule from reinforcement learning. Crucially, we demonstrate that the EOT formulation of the forward pass induces a specific information geometry on the space of attention distributions. It is this geometry, characterized by the Fisher Information Matrix, that dictates the precise form of the learning gradient, revealing the advantage-based update as a natural consequence of the optimization problem being solved. This unified view reveals SDPA as a principled mechanism where the forward pass performs optimal inference and the backward pass implements a rational, manifold-aware learning update.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regret minimization in Linear Bandits with offline data via extended D-optimal exploration</title>
<link>https://arxiv.org/abs/2508.08420</link>
<guid>https://arxiv.org/abs/2508.08420</guid>
<content:encoded><![CDATA[
<div> linear bandits, online regret minimization, offline data, regret bounds, optimal design

Summary:<br />
The article discusses online regret minimization in linear bandits with access to prior observations (offline data). A new algorithm called Offline-Online Phased Elimination (OOPE) is introduced, which incorporates offline data to reduce online regret significantly compared to existing methods. The algorithm utilizes an extended D-optimal design within exploration phases, achieving an online regret of $\tilde{O}(\sqrt{\deff T \log \left(|\mathcal{A}|T\right)}+d^2)$. The effective problem dimension $\deff \leq d$, influenced by the quality of offline data, plays a crucial role in determining regret bounds. Lower bounds on minimax regret explicitly depend on the quality of offline data, demonstrating the optimality of the algorithm in well-explored or poorly explored data scenarios. A Frank-Wolfe approximation further refines the regret term to $O\left(\frac{d^{2}}{\deff} \min \{ \deff,1\} \right)$, particularly beneficial in high-dimensional settings with moderate quality of offline data. <div>
arXiv:2508.08420v1 Announce Type: new 
Abstract: We consider the problem of online regret minimization in linear bandits with access to prior observations (offline data) from the underlying bandit model. There are numerous applications where extensive offline data is often available, such as in recommendation systems, online advertising. Consequently, this problem has been studied intensively in recent literature. Our algorithm, Offline-Online Phased Elimination (OOPE), effectively incorporates the offline data to substantially reduce the online regret compared to prior work. To leverage offline information prudently, OOPE uses an extended D-optimal design within each exploration phase. OOPE achieves an online regret is $\tilde{O}(\sqrt{\deff T \log \left(|\mathcal{A}|T\right)}+d^2)$. $\deff \leq d)$ is the effective problem dimension which measures the number of poorly explored directions in offline data and depends on the eigen-spectrum $(\lambda_k)_{k \in [d]}$ of the Gram matrix of the offline data. The eigen-spectrum $(\lambda_k)_{k \in [d]}$ is a quantitative measure of the \emph{quality} of offline data. If the offline data is poorly explored ($\deff \approx d$), we recover the established regret bounds for purely online setting while, when offline data is abundant ($\Toff >> T$) and well-explored ($\deff = o(1) $), the online regret reduces substantially. Additionally, we provide the first known minimax regret lower bounds in this setting that depend explicitly on the quality of the offline data. These lower bounds establish the optimality of our algorithm in regimes where offline data is either well-explored or poorly explored. Finally, by using a Frank-Wolfe approximation to the extended optimal design we further improve the $O(d^{2})$ term to $O\left(\frac{d^{2}}{\deff} \min \{ \deff,1\} \right)$, which can be substantial in high dimensions with moderate quality of offline data $\deff = \Omega(1)$.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast weight programming and linear transformers: from machine learning to neurobiology</title>
<link>https://arxiv.org/abs/2508.08435</link>
<guid>https://arxiv.org/abs/2508.08435</guid>
<content:encoded><![CDATA[
<div> Fast Weight Programmers, neural networks, machine learning, language modeling, 2D-state RNNs

Summary: 
Fast Weight Programmers (FWPs) are a type of recurrent neural network (RNN) architecture that utilizes 2D matrix-form hidden states instead of conventional vector-form hidden states. These networks have dynamically changing synaptic weights, known as fast weights, that act as short-term memory storage. Another network, called the programmer, controls the modifications of these fast weights based on input observations. FWPs have connections to transformers and state space models in terms of computational characteristics. Furthermore, there are links between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence. This primer provides a technical overview of FWPs, discussing their foundations, computational properties, and connections to various models in the field of machine learning and neural networks. <div>
arXiv:2508.08435v1 Announce Type: new 
Abstract: Recent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Cyclic Causal Discovery Under Unmeasured Confounders</title>
<link>https://arxiv.org/abs/2508.08450</link>
<guid>https://arxiv.org/abs/2508.08450</guid>
<content:encoded><![CDATA[
<div> causal discovery, nonlinear, confounders, interventional data, gene perturbation<br />
<br />
Summary:<br />
The paper introduces DCCD-CONF, a framework for learning nonlinear cyclic causal graphs in the presence of unmeasured confounders using interventional data. It addresses limitations of existing methods by alternating between optimizing the graph structure and estimating the confounder distribution through maximum log-likelihood. The framework outperforms state-of-the-art methods in causal graph recovery and confounder identification, as demonstrated on both synthetic data and real-world gene perturbation datasets. Consistency guarantees are provided to support the theoretical soundness of the approach. <div>
arXiv:2508.08450v1 Announce Type: new 
Abstract: Understanding causal relationships between variables is fundamental across scientific disciplines. Most causal discovery algorithms rely on two key assumptions: (i) all variables are observed, and (ii) the underlying causal graph is acyclic. While these assumptions simplify theoretical analysis, they are often violated in real-world systems, such as biological networks. Existing methods that account for confounders either assume linearity or struggle with scalability. To address these limitations, we propose DCCD-CONF, a novel framework for differentiable learning of nonlinear cyclic causal graphs in the presence of unmeasured confounders using interventional data. Our approach alternates between optimizing the graph structure and estimating the confounder distribution by maximizing the log-likelihood of the data. Through experiments on synthetic data and real-world gene perturbation datasets, we show that DCCD-CONF outperforms state-of-the-art methods in both causal graph recovery and confounder identification. Additionally, we also provide consistency guarantees for our framework, reinforcing its theoretical soundness.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Liver Tumor Detection in CT Images Using 3D U-Net and Bat Algorithm for Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2508.08452</link>
<guid>https://arxiv.org/abs/2508.08452</guid>
<content:encoded><![CDATA[
<div> Keywords: liver cancer, automated segmentation, computed tomography, 3D U-Net, Bat Algorithm

Summary:
Liver cancer is a deadly form of cancer that requires early detection for effective treatment. This paper presents a new method for automating the segmentation of liver tumors in CT images by combining a 3D U-Net architecture with the Bat Algorithm for hyperparameter optimization. By optimizing key parameters such as learning rate and batch size, the model improves segmentation accuracy and reliability. Evaluation on a public dataset shows that our model achieves a high F1-score with a balance of precision and recall, especially at lower prediction thresholds, crucial for clinical diagnostics. This work showcases the power of integrating a robust deep learning architecture with a metaheuristic optimization algorithm for complex segmentation tasks in medical image analysis.<br /><br />Summary: Liver cancer detection is critical, and this paper introduces an automated segmentation method using a 3D U-Net and Bat Algorithm. By optimizing parameters, the model improves accuracy and reliability, achieving a high F1-score for clinical diagnostics. The integration of deep learning and optimization algorithms proves effective for complex medical image analysis tasks. <div>
arXiv:2508.08452v1 Announce Type: new 
Abstract: Liver cancer is one of the most prevalent and lethal forms of cancer, making early detection crucial for effective treatment. This paper introduces a novel approach for automated liver tumor segmentation in computed tomography (CT) images by integrating a 3D U-Net architecture with the Bat Algorithm for hyperparameter optimization. The method enhances segmentation accuracy and robustness by intelligently optimizing key parameters like the learning rate and batch size. Evaluated on a publicly available dataset, our model demonstrates a strong ability to balance precision and recall, with a high F1-score at lower prediction thresholds. This is particularly valuable for clinical diagnostics, where ensuring no potential tumors are missed is paramount. Our work contributes to the field of medical image analysis by demonstrating that the synergy between a robust deep learning architecture and a metaheuristic optimization algorithm can yield a highly effective solution for complex segmentation tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Diffusion-Based Model-Level Explanation of Heterogeneous GNNs with Node Features</title>
<link>https://arxiv.org/abs/2508.08458</link>
<guid>https://arxiv.org/abs/2508.08458</guid>
<content:encoded><![CDATA[
<div> Keywords: heterogeneous graphs, node classification, explanation methods, DiGNNExplainer, diffusion models

Summary:
Nodes in real-world datasets are often part of heterogeneous graphs with diverse features. Node classification on such graphs is crucial for various applications. While Heterogeneous Graph Neural Networks (HGNNs) work well, their predictions lack transparency. Current explanation methods struggle with realistic features beyond node type. In response, DiGNNExplainer is proposed, using discrete denoising diffusion to create realistic features. By generating discrete features in a discrete space, DiGNNExplainer offers better explanations compared to existing methods. Evaluation on multiple datasets demonstrates its effectiveness in producing accurate and faithful explanations aligned with the model's decision-making process. <div>
arXiv:2508.08458v1 Announce Type: new 
Abstract: Many real-world datasets, such as citation networks, social networks, and molecular structures, are naturally represented as heterogeneous graphs, where nodes belong to different types and have additional features. For example, in a citation network, nodes representing "Paper" or "Author" may include attributes like keywords or affiliations. A critical machine learning task on these graphs is node classification, which is useful for applications such as fake news detection, corporate risk assessment, and molecular property prediction. Although Heterogeneous Graph Neural Networks (HGNNs) perform well in these contexts, their predictions remain opaque. Existing post-hoc explanation methods lack support for actual node features beyond one-hot encoding of node type and often fail to generate realistic, faithful explanations. To address these gaps, we propose DiGNNExplainer, a model-level explanation approach that synthesizes heterogeneous graphs with realistic node features via discrete denoising diffusion. In particular, we generate realistic discrete features (e.g., bag-of-words features) using diffusion models within a discrete space, whereas previous approaches are limited to continuous spaces. We evaluate our approach on multiple datasets and show that DiGNNExplainer produces explanations that are realistic and faithful to the model's decision-making, outperforming state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Partial Optimal Transport via Quadratic Regularization</title>
<link>https://arxiv.org/abs/2508.08476</link>
<guid>https://arxiv.org/abs/2508.08476</guid>
<content:encoded><![CDATA[
<div> Partial Optimal Transport, Machine Learning, Sparsity, Quadratic Regularization, Color Transfer
Summary: 
The paper introduces Quadratic Regularized Partial Optimal Transport (QPOT) as an alternative to existing entropic solvers for Partial Optimal Transport (POT). QPOT allows for sparsity in the transport plan, making it more suitable for applications that require sparse solutions. Experiments on synthetic and CIFAR-10 datasets, as well as real-world tasks like color transfer and domain adaptation, show that QPOT outperforms existing methods in terms of sparsity and performance. This novel formulation of POT with quadratic regularization offers greater flexibility by accommodating unbalanced input measures and can be beneficial for various machine learning applications.<br /><br />Summary: <div>
arXiv:2508.08476v1 Announce Type: new 
Abstract: Partial Optimal Transport (POT) has recently emerged as a central tool in various Machine Learning (ML) applications. It lifts the stringent assumption of the conventional Optimal Transport (OT) that input measures are of equal masses, which is often not guaranteed in real-world datasets, and thus offers greater flexibility by permitting transport between unbalanced input measures. Nevertheless, existing major solvers for POT commonly rely on entropic regularization for acceleration and thus return dense transport plans, hindering the adoption of POT in various applications that favor sparsity. In this paper, as an alternative approach to the entropic POT formulation in the literature, we propose a novel formulation of POT with quadratic regularization, hence termed quadratic regularized POT (QPOT), which induces sparsity to the transport plan and consequently facilitates the adoption of POT in many applications with sparsity requirements. Extensive experiments on synthetic and CIFAR-10 datasets, as well as real-world applications such as color transfer and domain adaptations, consistently demonstrate the improved sparsity and favorable performance of our proposed QPOT formulation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biased Local SGD for Efficient Deep Learning on Heterogeneous Systems</title>
<link>https://arxiv.org/abs/2508.08540</link>
<guid>https://arxiv.org/abs/2508.08540</guid>
<content:encoded><![CDATA[
<div> heterogeneous resources, neural network training, local stochastic gradient descent, bias introduction, parallelization strategy <br />
<br />
Summary: 
The article discusses the challenges of utilizing heterogeneous compute resources for neural network training, which is often optimized for homogeneous parallel computing. The authors propose a system-aware local stochastic gradient descent (local SGD) method that allocates workloads based on compute capacity, allowing for efficient use of diverse resources. By intentionally introducing bias into data sampling and model aggregation, the method accelerates training on slower components like CPUs. The study demonstrates that controlled bias in local SGD can achieve comparable or higher accuracy than synchronous SGD with data parallelism in heterogeneous environments. This parallelization strategy is applicable to various setups, including cloud platforms and high-performance computing clusters. <div>
arXiv:2508.08540v1 Announce Type: new 
Abstract: Most large-scale neural network training methods assume homogeneous parallel computing resources. For example, synchronous SGD with data parallelism, the most widely used parallel training strategy, incurs significant synchronization overhead when workers process their assigned data at different speeds. Consequently, in systems with heterogeneous compute resources, users often rely solely on the fastest components, such as GPUs, for training. In this work, we explore how to effectively use heterogeneous resources for neural network training. We propose a system-aware local stochastic gradient descent (local SGD) method that allocates workloads to each compute resource in proportion to its compute capacity. To make better use of slower resources such as CPUs, we intentionally introduce bias into data sampling and model aggregation. Our study shows that well-controlled bias can significantly accelerate local SGD in heterogeneous environments, achieving comparable or even higher accuracy than synchronous SGD with data-parallelism within the same time budget. This fundamental parallelization strategy can be readily extended to diverse heterogeneous environments, including cloud platforms and multi-node high-performance computing clusters.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3-Net: A Cost-Effective Graph-Free MLP-Based Model for Traffic Prediction</title>
<link>https://arxiv.org/abs/2508.08543</link>
<guid>https://arxiv.org/abs/2508.08543</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic prediction, Multilayer Perceptron, M3-Net, spatio-temporal embeddings, MLP-Mixer with MoE

Summary:
The article introduces a cost-effective graph-free model, M3-Net, for accurate traffic prediction in intelligent transportation systems. Traditional deep learning methods face challenges with complex network structures and spatio-temporal dependencies. M3-Net utilizes time series and spatio-temporal embeddings for efficient feature processing and introduces a novel MLP-Mixer architecture with a mixture of experts (MoE) mechanism. Experimental results on real datasets show that M3-Net outperforms existing models in prediction performance and lightweight deployment. The proposed model addresses limitations of existing deep learning approaches, providing a promising solution for achieving accurate traffic prediction efficiently. <div>
arXiv:2508.08543v1 Announce Type: new 
Abstract: Achieving accurate traffic prediction is a fundamental but crucial task in the development of current intelligent transportation systems.Most of the mainstream methods that have made breakthroughs in traffic prediction rely on spatio-temporal graph neural networks, spatio-temporal attention mechanisms, etc. The main challenges of the existing deep learning approaches are that they either depend on a complete traffic network structure or require intricate model designs to capture complex spatio-temporal dependencies. These limitations pose significant challenges for the efficient deployment and operation of deep learning models on large-scale datasets. To address these challenges, we propose a cost-effective graph-free Multilayer Perceptron (MLP) based model M3-Net for traffic prediction. Our proposed model not only employs time series and spatio-temporal embeddings for efficient feature processing but also first introduces a novel MLP-Mixer architecture with a mixture of experts (MoE) mechanism. Extensive experiments conducted on multiple real datasets demonstrate the superiority of the proposed model in terms of prediction performance and lightweight deployment.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UQGNN: Uncertainty Quantification of Graph Neural Networks for Multivariate Spatiotemporal Prediction</title>
<link>https://arxiv.org/abs/2508.08551</link>
<guid>https://arxiv.org/abs/2508.08551</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Uncertainty Quantification, Spatiotemporal Prediction, Multivariate Data, Interaction Patterns <br />
<br />Summary:
The research introduces a novel Graph Neural Network with Uncertainty Quantification (UQGNN) for multivariate spatiotemporal prediction. UQGNN includes an Interaction-aware Spatiotemporal Embedding Module to capture complex spatial and temporal interactions and a multivariate probabilistic prediction module to estimate mean values and uncertainties. The study aims to address the limitations of deterministic models by quantifying uncertainty and considering correlations among urban phenomena. Experiments on real-world datasets from Shenzhen, New York City, and Chicago show that UQGNN outperforms existing models in both prediction accuracy and uncertainty quantification. For instance, in the Shenzhen dataset, UQGNN achieves a 5% improvement in prediction accuracy and uncertainty quantification. The proposed model shows promise in enhancing spatiotemporal prediction accuracy and reliability in various applications such as urban planning, transportation optimization, and disaster response.<br /> <div>
arXiv:2508.08551v1 Announce Type: new 
Abstract: Spatiotemporal prediction plays a critical role in numerous real-world applications such as urban planning, transportation optimization, disaster response, and pandemic control. In recent years, researchers have made significant progress by developing advanced deep learning models for spatiotemporal prediction. However, most existing models are deterministic, i.e., predicting only the expected mean values without quantifying uncertainty, leading to potentially unreliable and inaccurate outcomes. While recent studies have introduced probabilistic models to quantify uncertainty, they typically focus on a single phenomenon (e.g., taxi, bike, crime, or traffic crashes), thereby neglecting the inherent correlations among heterogeneous urban phenomena. To address the research gap, we propose a novel Graph Neural Network with Uncertainty Quantification, termed UQGNN for multivariate spatiotemporal prediction. UQGNN introduces two key innovations: (i) an Interaction-aware Spatiotemporal Embedding Module that integrates a multivariate diffusion graph convolutional network and an interaction-aware temporal convolutional network to effectively capture complex spatial and temporal interaction patterns, and (ii) a multivariate probabilistic prediction module designed to estimate both expected mean values and associated uncertainties. Extensive experiments on four real-world multivariate spatiotemporal datasets from Shenzhen, New York City, and Chicago demonstrate that UQGNN consistently outperforms state-of-the-art baselines in both prediction accuracy and uncertainty quantification. For example, on the Shenzhen dataset, UQGNN achieves a 5% improvement in both prediction accuracy and uncertainty quantification.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHEFL: Resource-Aware Aggregation and Sparsification in Heterogeneous Ensemble Federated Learning</title>
<link>https://arxiv.org/abs/2508.08552</link>
<guid>https://arxiv.org/abs/2508.08552</guid>
<content:encoded><![CDATA[
<div> global ensemble-based federated learning, computational heterogeneity, data bias, communication efficiency, deep ensembles

Summary:
SHEFL is a novel global ensemble-based federated learning framework designed to tackle data and system heterogeneity in distributed training scenarios. By allocating different numbers of global models to clients based on their computational resources, SHEFL effectively addresses computational heterogeneity. A novel aggregation scheme is introduced to mitigate bias between clients with varying computational capacities, improving both fairness and overall performance. SHEFL dynamically adjusts the resource ratio across clients to reduce the computational burden of training deep ensembles and mitigate data bias. Experimental results demonstrate that SHEFL outperforms existing approaches in terms of fairness and overall performance when dealing with clients of diverse computational capacities. <div>
arXiv:2508.08552v1 Announce Type: new 
Abstract: Federated learning enables distributed training with private data of clients, but its convergence is hindered by data and system heterogeneity in realistic communication scenarios. Most existing system heterogeneous FL schemes utilize global pruning or ensemble distillation, yet they often overlook typical constraints required for communication efficiency. Meanwhile, deep ensembles can aggregate predictions from individually trained models to improve performance, but current ensemble-based FL methods fall short in fully capturing the diversity of model predictions. In this work, we propose SHEFL, a global ensemble-based federated learning framework suited for clients with diverse computational capacities. We allocate different numbers of global models to clients based on their available resources. We further introduce a novel aggregation scheme that accounts for bias between clients with different computational capabilities. To reduce the computational burden of training deep ensembles and mitigate data bias, we dynamically adjust the resource ratio across clients - aggressively reducing the influence of underpowered clients in constrained scenarios, while increasing their weight in the opposite case. Extensive experiments demonstrate that our method effectively addresses computational heterogeneity, significantly improving both fairness and overall performance compared to existing approaches.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed optimization: designed for federated learning</title>
<link>https://arxiv.org/abs/2508.08606</link>
<guid>https://arxiv.org/abs/2508.08606</guid>
<content:encoded><![CDATA[
<div> augmented Lagrangian, federated learning, distributed optimization, communication topologies, convergence<br />
Summary:<br />
This paper introduces distributed optimization algorithms based on the augmented Lagrangian technique for federated learning in cross-organizational data collaboration. The algorithms accommodate various communication topologies in centralized and decentralized FL settings, with multiple termination criteria and parameter update mechanisms to improve computational efficiency. The framework extends classical unconstrained optimization methods such as proximal algorithm, gradient descent, and stochastic gradient descent. The convergence properties of these methods are rigorously proven within the proposed theoretical framework. Numerical experiments demonstrate the algorithm's performance in large-scale settings with statistical heterogeneity across clients. <div>
arXiv:2508.08606v1 Announce Type: new 
Abstract: Federated Learning (FL), as a distributed collaborative Machine Learning (ML) framework under privacy-preserving constraints, has garnered increasing research attention in cross-organizational data collaboration scenarios. This paper proposes a class of distributed optimization algorithms based on the augmented Lagrangian technique, designed to accommodate diverse communication topologies in both centralized and decentralized FL settings. Furthermore, we develop multiple termination criteria and parameter update mechanisms to enhance computational efficiency, accompanied by rigorous theoretical guarantees of convergence. By generalizing the augmented Lagrangian relaxation through the incorporation of proximal relaxation and quadratic approximation, our framework systematically recovers a broad of classical unconstrained optimization methods, including proximal algorithm, classic gradient descent, and stochastic gradient descent, among others. Notably, the convergence properties of these methods can be naturally derived within the proposed theoretical framework. Numerical experiments demonstrate that the proposed algorithm exhibits strong performance in large-scale settings with significant statistical heterogeneity across clients.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Rank Adjustment for Accurate and Efficient Neural Network Training</title>
<link>https://arxiv.org/abs/2508.08625</link>
<guid>https://arxiv.org/abs/2508.08625</guid>
<content:encoded><![CDATA[
<div> Keywords: low-rank training, matrix decompositions, neural networks, dynamic-rank training, model accuracy

Summary: 
Low-rank training methods in neural networks reduce trainable parameters by re-parameterizing weights with matrix decompositions like singular value decomposition. However, fixed low-rank structures can limit the model's learning capability of complex patterns. The effective rank of the weights declines during training, which is accelerated in low-rank reparameterization. The study suggests interleaving full-rank training epochs within low-rank epochs to restore the weight matrix rank effectively. A dynamic-rank training framework is proposed to address rank collapse during training, adjusting the weight matrix rank. Empirical results validate the framework, showing comparable accuracy to full-rank training with similar computational cost as SVD-based low-rank training across various benchmarks.<br /><br />Summary: <div>
arXiv:2508.08625v1 Announce Type: new 
Abstract: Low-rank training methods reduce the number of trainable parameters by re-parameterizing the weights with matrix decompositions (e.g., singular value decomposition). However, enforcing a fixed low-rank structure caps the rank of the weight matrices and can hinder the model's ability to learn complex patterns. Furthermore, the effective rank of the model's weights tends to decline during training, and this drop is accelerated when the model is reparameterized into a low-rank structure. In this study, we argue that strategically interleaving full-rank training epochs within low-rank training epochs can effectively restore the rank of the model's weights. Based on our findings, we propose a general dynamic-rank training framework that is readily applicable to a wide range of neural-network tasks. We first describe how to adjust the rank of weight matrix to alleviate the inevitable rank collapse that arises during training, and then present extensive empirical results that validate our claims and demonstrate the efficacy of the proposed framework. Our empirical study shows that the proposed method achieves almost the same computational cost as SVD-based low-rank training while achieving a comparable accuracy to full-rank training across various benchmarks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks</title>
<link>https://arxiv.org/abs/2508.08635</link>
<guid>https://arxiv.org/abs/2508.08635</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic text classification, large language models, token-driven sparse finetuning, specialized classification tasks, small language models

Summary: 
Token-driven sparse finetuning is proposed for adapting small language models to specialized semantic classification tasks. This approach identifies and finetunes a small subset of model parameters based on task-specific token constructs in the finetuning dataset, without introducing additional parameters to the model. The method outperforms end-to-end finetuning, low rank adaptation (LoRA), layer selection, and prefix tuning on various semantic classification tasks. It demonstrates greater stability, reduces training costs by half compared to end-to-end finetuning, and achieves improved performance in tasks requiring the understanding of contextual significance of tokens. This strategy addresses the need for customized classifiers in specialized applications that necessitate high inference throughputs and domain-specific annotation. <div>
arXiv:2508.08635v1 Announce Type: new 
Abstract: Semantic text classification requires the understanding of the contextual significance of specific tokens rather than surface-level patterns or keywords (as in rule-based or statistical text classification), making large language models (LLMs) well-suited for this task. However, semantic classification applications in industry, like customer intent detection or semantic role labeling, tend to be highly specialized. They require annotation by domain experts in contrast to general-purpose corpora for pretraining. Further, they typically require high inference throughputs which limits the model size from latency and cost perspectives. Thus, for a range of specialized classification tasks, the preferred solution is to develop customized classifiers by finetuning smaller language models (e.g., mini-encoders, small language models).
  In this work, we develop a token-driven sparse finetuning strategy to adapt small language models to specialized classification tasks. We identify and finetune a small sensitive subset of model parameters by leveraging task-specific token constructs in the finetuning dataset, while leaving most of the pretrained weights unchanged. Unlike adapter approaches such as low rank adaptation (LoRA), we do not introduce additional parameters to the model. Our approach identifies highly relevant semantic tokens (case study in the Appendix) and outperforms end-to-end finetuning, LoRA, layer selection, and prefix tuning on five diverse semantic classification tasks. We achieve greater stability and half the training costs vs. end-to-end finetuning.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time</title>
<link>https://arxiv.org/abs/2508.08641</link>
<guid>https://arxiv.org/abs/2508.08641</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, black-box optimization, test-time training, synthetic data, MiGrATe 

Summary: 
MiGrATe is a novel method for online Test-Time Training (TTT) that utilizes GRPO as a search algorithm to adapt Large Language Models (LLMs) during inference without the need for external training data. MiGrATe employs a mixed-policy group construction approach that combines on-policy sampling with off-policy data selection techniques, such as greedy sampling and neighborhood sampling, to strike a balance between exploration and exploitation in solution space. By biasing the policy gradient towards promising regions while preserving exploration, MiGrATe outperforms both inference-only and TTT baselines across various challenging domains like word search, molecule optimization, and hypothesis+program induction. This study showcases the potential of online TTT as a viable solution for complex search tasks that do not require external supervision. 

<br /><br />Summary: <div>
arXiv:2508.08641v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being applied to black-box optimization tasks, from program synthesis to molecule design. Prior work typically leverages in-context learning to iteratively guide the model towards better solutions. Such methods, however, often struggle to balance exploration of new solution spaces with exploitation of high-reward ones. Recently, test-time training (TTT) with synthetic data has shown promise in improving solution quality. However, the need for hand-crafted training data tailored to each task limits feasibility and scalability across domains. To address this problem, we introduce MiGrATe-a method for online TTT that uses GRPO as a search algorithm to adapt LLMs at inference without requiring external training data. MiGrATe operates via a mixed-policy group construction procedure that combines on-policy sampling with two off-policy data selection techniques: greedy sampling, which selects top-performing past completions, and neighborhood sampling (NS), which generates completions structurally similar to high-reward ones. Together, these components bias the policy gradient towards exploitation of promising regions in solution space, while preserving exploration through on-policy sampling. We evaluate MiGrATe on three challenging domains-word search, molecule optimization, and hypothesis+program induction on the Abstraction and Reasoning Corpus (ARC)-and find that it consistently outperforms both inference-only and TTT baselines, demonstrating the potential of online TTT as a solution for complex search tasks without external supervision.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\text{M}^{2}$LLM: Multi-view Molecular Representation Learning with Large Language Models</title>
<link>https://arxiv.org/abs/2508.08657</link>
<guid>https://arxiv.org/abs/2508.08657</guid>
<content:encoded><![CDATA[
<div> molecular property prediction, molecular representation methods, large language models, multi-view framework, state-of-the-art performance<br />
<br />
Summary: <br />
Accurate prediction of molecular properties is crucial for various scientific fields. Traditional molecular representation methods like fingerprints and graph neural networks are effective but lack contextual knowledge. A new approach called $\text{M}^{2}$LLM integrates multiple perspectives: the molecular structure view, the molecular task view, and the molecular rules view. This multi-view framework dynamically fuses perspectives to adapt to different tasks, leading to state-of-the-art performance on classification and regression benchmarks. The $\text{M}^{2}$LLM leverages large language models' reasoning abilities to generate rich molecular representations. By encoding molecular embeddings and curating features through advanced reasoning processes, the LLM approach demonstrates exceptional performance. This innovative methodology bridges the gap between molecular structures and advanced reasoning, yielding promising results in various applications. <br /> <div>
arXiv:2508.08657v1 Announce Type: new 
Abstract: Accurate molecular property prediction is a critical challenge with wide-ranging applications in chemistry, materials science, and drug discovery. Molecular representation methods, including fingerprints and graph neural networks (GNNs), achieve state-of-the-art results by effectively deriving features from molecular structures. However, these methods often overlook decades of accumulated semantic and contextual knowledge. Recent advancements in large language models (LLMs) demonstrate remarkable reasoning abilities and prior knowledge across scientific domains, leading us to hypothesize that LLMs can generate rich molecular representations when guided to reason in multiple perspectives. To address these gaps, we propose $\text{M}^{2}$LLM, a multi-view framework that integrates three perspectives: the molecular structure view, the molecular task view, and the molecular rules view. These views are fused dynamically to adapt to task requirements, and experiments demonstrate that $\text{M}^{2}$LLM achieves state-of-the-art performance on multiple benchmarks across classification and regression tasks. Moreover, we demonstrate that representation derived from LLM achieves exceptional performance by leveraging two core functionalities: the generation of molecular embeddings through their encoding capabilities and the curation of molecular features through advanced reasoning processes.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-level Collaborative Distillation Meets Global Workspace Model: A Unified Framework for OCIL</title>
<link>https://arxiv.org/abs/2508.08677</link>
<guid>https://arxiv.org/abs/2508.08677</guid>
<content:encoded><![CDATA[
<div> Keywords: Online Class-Incremental Learning, ensemble learning, Global Workspace Model, memory constraints, adaptability

Summary: 
The article introduces a novel approach for Online Class-Incremental Learning (OCIL) that addresses challenges related to model stability and adaptability. By enhancing traditional ensemble learning with a Global Workspace Model (GWM), which serves as a shared memory for multiple student models, the proposed method aims to maintain stability under memory constraints while ensuring adaptability to new tasks. The GWM captures historical learning trajectories and consolidates knowledge across students, promoting cross-task consistency. Additionally, a multi-level collaborative distillation mechanism enforces peer-to-peer consistency and aligns student models with the GWM to preserve historical knowledge. Experimental results on OCIL benchmarks demonstrate significant performance improvement across various memory budgets, highlighting the effectiveness of the proposed approach in achieving a balance between stability and plasticity.<br /><br />Summary: <div>
arXiv:2508.08677v1 Announce Type: new 
Abstract: Online Class-Incremental Learning (OCIL) enables models to learn continuously from non-i.i.d. data streams and samples of the data streams can be seen only once, making it more suitable for real-world scenarios compared to offline learning. However, OCIL faces two key challenges: maintaining model stability under strict memory constraints and ensuring adaptability to new tasks. Under stricter memory constraints, current replay-based methods are less effective. While ensemble methods improve adaptability (plasticity), they often struggle with stability. To overcome these challenges, we propose a novel approach that enhances ensemble learning through a Global Workspace Model (GWM)-a shared, implicit memory that guides the learning of multiple student models. The GWM is formed by fusing the parameters of all students within each training batch, capturing the historical learning trajectory and serving as a dynamic anchor for knowledge consolidation. This fused model is then redistributed periodically to the students to stabilize learning and promote cross-task consistency. In addition, we introduce a multi-level collaborative distillation mechanism. This approach enforces peer-to-peer consistency among students and preserves historical knowledge by aligning each student with the GWM. As a result, student models remain adaptable to new tasks while maintaining previously learned knowledge, striking a better balance between stability and plasticity. Extensive experiments on three standard OCIL benchmarks show that our method delivers significant performance improvement for several OCIL models across various memory budgets.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expert-Guided Diffusion Planner for Auto-bidding</title>
<link>https://arxiv.org/abs/2508.08687</link>
<guid>https://arxiv.org/abs/2508.08687</guid>
<content:encoded><![CDATA[
<div> Keywords: Auto-bidding, Generative bidding, Conditional diffusion modeling, Expert trajectory guidance, Skip-step sampling strategy

Summary: 
Auto-bidding is widely used in advertising systems, with generative bidding gaining popularity for its planning capabilities. Unlike traditional reinforcement learning-based bidding, generative bidding does not rely on Markov Decision Processes. However, using return as the sole optimality condition lacks personalized structural information. Additionally, diffusion models' autoregressive generation mechanism poses timeliness risks. To address these issues, a novel conditional diffusion modeling method is proposed, combining expert trajectory guidance and skip-step sampling strategy. Offline experiments validated the effectiveness, showing significant improvements in online A/B testing with an increase in conversion and revenue compared to the baseline.<br /><br />Summary: <div>
arXiv:2508.08687v1 Announce Type: new 
Abstract: Auto-bidding is extensively applied in advertising systems, serving a multitude of advertisers. Generative bidding is gradually gaining traction due to its robust planning capabilities and generalizability. In contrast to traditional reinforcement learning-based bidding, generative bidding does not rely on the Markov Decision Process (MDP) exhibiting superior planning capabilities in long-horizon scenarios. Conditional diffusion modeling approaches have demonstrated significant potential in the realm of auto-bidding. However, relying solely on return as the optimality condition is weak to guarantee the generation of genuinely optimal decision sequences, lacking personalized structural information. Moreover, diffusion models' t-step autoregressive generation mechanism inherently carries timeliness risks. To address these issues, we propose a novel conditional diffusion modeling method based on expert trajectory guidance combined with a skip-step sampling strategy to enhance generation efficiency. We have validated the effectiveness of this approach through extensive offline experiments and achieved statistically significant results in online A/B testing, achieving an increase of 11.29% in conversion and a 12.35% in revenue compared with the baseline.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Modeling for Robust Deep Reinforcement Learning on the Traveling Salesman Problem</title>
<link>https://arxiv.org/abs/2508.08718</link>
<guid>https://arxiv.org/abs/2508.08718</guid>
<content:encoded><![CDATA[
<div> Generative Sampling, Combinatorial Optimization, TSP, Neural Networks, Distribution Robustness
Summary: 
The paper discusses the Traveling Salesman Problem (TSP), a challenging combinatorial optimization task with practical applications. Traditional heuristic solvers struggle with larger instances, leading researchers to explore neural network solvers. However, neural networks have difficulty generalizing beyond synthetic data. The authors introduce Combinatorial Optimization with Generative Sampling (COGS), which addresses distribution robustness by training on generative TSP data. They also introduce TSPLib50, a dataset of realistically distributed TSP samples for testing real-world generalization. COGS outperforms existing neural baselines, especially in worst-case scenarios, by providing better data coverage and interpolation in the TSP training distribution space. The method is evaluated on synthetic datasets and TSPLib50, demonstrating improved distribution robustness and performance gains in challenging scenarios. 
Summary: <div>
arXiv:2508.08718v1 Announce Type: new 
Abstract: The Traveling Salesman Problem (TSP) is a classic NP-hard combinatorial optimization task with numerous practical applications. Classic heuristic solvers can attain near-optimal performance for small problem instances, but become computationally intractable for larger problems. Real-world logistics problems such as dynamically re-routing last-mile deliveries demand a solver with fast inference time, which has led researchers to investigate specialized neural network solvers. However, neural networks struggle to generalize beyond the synthetic data they were trained on. In particular, we show that there exist TSP distributions that are realistic in practice, which also consistently lead to poor worst-case performance for existing neural approaches. To address this issue of distribution robustness, we present Combinatorial Optimization with Generative Sampling (COGS), where training data is sampled from a generative TSP model. We show that COGS provides better data coverage and interpolation in the space of TSP training distributions. We also present TSPLib50, a dataset of realistically distributed TSP samples, which tests real-world generalization ability without conflating this issue with instance size. We evaluate our method on various synthetic datasets as well as TSPLib50, and compare to state-of-the-art neural baselines. We demonstrate that COGS improves distribution robustness, with most performance gains coming from worst-case scenarios.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elucidating Rectified Flow with Deterministic Sampler: Polynomial Discretization Complexity for Multi and One-step Models</title>
<link>https://arxiv.org/abs/2508.08735</link>
<guid>https://arxiv.org/abs/2508.08735</guid>
<content:encoded><![CDATA[
<div> RF-based models, rectified flow, polynomial discretization complexity, Langevin process, deterministic sampler 
<br />
<br />
Summary: 
This article discusses the theoretical analysis of the discretization complexity of rectified flow (RF)-based models in both multi-step and one-step generation tasks. The study proves the first polynomial discretization complexity for RF-based models with a deterministic sampler, under the bounded support assumption. Inspired by the predictor-corrector framework of diffusion models, a Langevin process is introduced as a corrector in the multi-step setting, demonstrating better polynomial discretization complexity compared to diffusion models. Detailed analysis of RF-based models is provided, highlighting their superior performance over popular models like variance preserving (VP) and variance exploding (VE) based models. Additionally, the study offers the first polynomial discretization complexity result for one-step RF-based models, surpassing previous results for one-step diffusion-based models. These findings contribute to a deeper understanding of the exceptional empirical performance of RF-based models in both multi-step and one-step generation tasks. <div>
arXiv:2508.08735v1 Announce Type: new 
Abstract: Recently, rectified flow (RF)-based models have achieved state-of-the-art performance in many areas for both the multi-step and one-step generation. However, only a few theoretical works analyze the discretization complexity of RF-based models. Existing works either focus on flow-based models with stochastic samplers or establish complexity results that exhibit exponential dependence on problem parameters. In this work, under the realistic bounded support assumption, we prove the first polynomial discretization complexity for multi-step and one-step RF-based models with a deterministic sampler simultaneously. For the multi-step setting, inspired by the predictor-corrector framework of diffusion models, we introduce a Langevin process as a corrector and show that RF-based models can achieve better polynomial discretization complexity than diffusion models. To achieve this result, we conduct a detailed analysis of the RF-based model and explain why it is better than previous popular models, such as variance preserving (VP) and variance exploding (VE)-based models. Based on the observation of multi-step RF-based models, we further provide the first polynomial discretization complexity result for one-step RF-based models, improving upon prior results for one-step diffusion-based models. These findings mark the first step toward theoretically understanding the impressive empirical performance of RF-based models in both multi-step and one-step generation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Reward Model via Sparse Autoencoder</title>
<link>https://arxiv.org/abs/2508.08746</link>
<guid>https://arxiv.org/abs/2508.08746</guid>
<content:encoded><![CDATA[
<div> Sparse Autoencoder, Reward Model, Interpretability, Feature-level Attribution, Alignment performance

Summary: 
The study introduces the novel Sparse Autoencoder-enhanced Reward Model (SARM) to improve the interpretability and flexibility of reward models used for Reinforcement Learning from Human Feedback (RLHF) with Large Language Models (LLMs). SARM integrates a Sparse Autoencoder (SAE) into the reward model, allowing for direct feature-level attribution of reward assignments and dynamic adjustment to preference shifts. The architecture of SARM enables transparent and conceptually meaningful reward scores by mapping the hidden activations of the LLM-based reward model into a sparse and interpretable feature space. Empirical evaluations show that SARM outperforms conventional reward models in terms of alignment performance, providing a more accurate and reliable way to align LLM behaviors with human values. The code for SARM is available on GitHub for further exploration and experimentation. 

Summary: <br /><br />Keywords: Sparse Autoencoder, Reward Model, Interpretability, Feature-level Attribution, Alignment performance <div>
arXiv:2508.08746v1 Announce Type: new 
Abstract: Large language models (LLMs) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align LLM behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (\textbf{SARM}), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of LLM-based RM into an interpretable, sparse, and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at https://github.com/schrieffer-z/sarm.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs</title>
<link>https://arxiv.org/abs/2508.08769</link>
<guid>https://arxiv.org/abs/2508.08769</guid>
<content:encoded><![CDATA[
<div> semi-supervised learning, graph neural networks, decision factors, consistency, robustness <br />
Summary:<br />
The paper introduces the DiFac framework for semi-supervised learning in graph neural networks to improve performance with unlabeled data. It addresses the challenge of obtaining mutually independent decision factors by deriving and enforcing consistency from a single information source. The model learns to extract these factors during pre-training and removes conflicting samples during training, reducing overconfidence in pseudo-labels. Additional textual knowledge can also be incorporated, with an accountability scoring mechanism to mitigate errors. Experimental results on benchmark datasets show that DiFac consistently enhances robustness and generalization in low-label scenarios, outperforming baseline methods. <div>
arXiv:2508.08769v1 Announce Type: new 
Abstract: In semi-supervised learning (SSL) for enhancing the performance of graph neural networks (GNNs) with unlabeled data, introducing mutually independent decision factors for cross-validation is regarded as an effective strategy to alleviate pseudo-label confirmation bias and training collapse. However, obtaining such factors is challenging in practice: additional and valid information sources are inherently scarce, and even when such sources are available, their independence from the original source cannot be guaranteed. To address this challenge, In this paper we propose a Differentiated Factor Consistency Semi-supervised Framework (DiFac), which derives differentiated factors from a single information source and enforces their consistency. During pre-training, the model learns to extract these factors; in training, it iteratively removes samples with conflicting factors and ranks pseudo-labels based on the shortest stave principle, selecting the top candidate samples to reduce overconfidence commonly observed in confidence-based or ensemble-based methods. Our framework can also incorporate additional information sources. In this work, we leverage the large multimodal language model to introduce latent textual knowledge as auxiliary decision factors, and we design a accountability scoring mechanism to mitigate additional erroneous judgments introduced by these auxiliary factors. Experiments on multiple benchmark datasets demonstrate that DiFac consistently improves robustness and generalization in low-label regimes, outperforming other baseline methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TechOps: Technical Documentation Templates for the AI Act</title>
<link>https://arxiv.org/abs/2508.08804</link>
<guid>https://arxiv.org/abs/2508.08804</guid>
<content:encoded><![CDATA[
<div> Keywords: EU AI Act, technical documentation, traceability, compliance, TechOps

Summary: 
This paper introduces open-source templates and examples for documenting data, models, and applications to ensure compliance with the EU AI Act. The templates cover the entire AI lifecycle, promoting transparency, traceability, and accountability. They enhance discoverability and collaboration, reduce risks, and align with best practices in AI governance. User feedback is used to refine the templates, ensuring usability and implementability. Real-world scenarios, such as documenting a skin tones dataset for fairness evaluations and a neural network for image segmentation, demonstrate the templates' effectiveness. The application template is tested on a system for construction site safety using real-time analytics. This approach, known as TechOps, facilitates regulatory oversight and responsible AI development. <br /><br />Summary: <div>
arXiv:2508.08804v1 Announce Type: new 
Abstract: Operationalizing the EU AI Act requires clear technical documentation to ensure AI systems are transparent, traceable, and accountable. Existing documentation templates for AI systems do not fully cover the entire AI lifecycle while meeting the technical documentation requirements of the AI Act.
  This paper addresses those shortcomings by introducing open-source templates and examples for documenting data, models, and applications to provide sufficient documentation for certifying compliance with the AI Act. These templates track the system status over the entire AI lifecycle, ensuring traceability, reproducibility, and compliance with the AI Act. They also promote discoverability and collaboration, reduce risks, and align with best practices in AI documentation and governance.
  The templates are evaluated and refined based on user feedback to enable insights into their usability and implementability. We then validate the approach on real-world scenarios, providing examples that further guide their implementation: the data template is followed to document a skin tones dataset created to support fairness evaluations of downstream computer vision models and human-centric applications; the model template is followed to document a neural network for segmenting human silhouettes in photos. The application template is tested on a system deployed for construction site safety using real-time video analytics and sensor data. Our results show that TechOps can serve as a practical tool to enable oversight for regulatory compliance and responsible AI development.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TempOpt -- Unsupervised Alarm Relation Learning for Telecommunication Networks</title>
<link>https://arxiv.org/abs/2508.08814</link>
<guid>https://arxiv.org/abs/2508.08814</guid>
<content:encoded><![CDATA[
<div> Keywords: telecommunications network, fault alarms, Network Operations Centre, root alarm identification, unsupervised alarm relation learning<br />
Summary:<br />
In a telecommunications network, fault alarms generated by network nodes are monitored in a Network Operations Centre (NOC) to ensure continuous network operations. With a large number of network nodes possibly from multiple vendors, the volume of alarms can be overwhelming for NOC engineers. The process of active alarms analysis and root alarm identification can be challenging, given the interconnected nature of network nodes. To address this, a novel unsupervised alarm relation learning technique called Temporal Optimization (TempOpt) is proposed. TempOpt aims to improve the accuracy and speed of identifying root alarms by learning relationships among alarms. Experiments conducted on real-world network datasets demonstrate the effectiveness of TempOpt in comparison to existing temporal dependency methods. This approach offers practical solutions for enhancing the quality of alarm relations learned in a network monitoring setting. <br /> <div>
arXiv:2508.08814v1 Announce Type: new 
Abstract: In a telecommunications network, fault alarms generated by network nodes are monitored in a Network Operations Centre (NOC) to ensure network availability and continuous network operations. The monitoring process comprises of tasks such as active alarms analysis, root alarm identification, and resolution of the underlying problem. Each network node potentially can generate alarms of different types, while nodes can be from multiple vendors, a network can have hundreds of nodes thus resulting in an enormous volume of alarms at any time. Since network nodes are inter-connected, a single fault in the network would trigger multiple sequences of alarms across a variety of nodes and from a monitoring point of view, it is a challenging task for a NOC engineer to be aware of relations between the various alarms, when trying to identify, for example, a root alarm on which an action needs to be taken. To effectively identify root alarms, it is essential to learn relation among the alarms for accurate and faster resolution. In this work we propose a novel unsupervised alarm relation learning technique Temporal Optimization (TempOpt) that is practical and overcomes the limitations of an existing class of alarm relational learning method-temporal dependency methods. Experiments have been carried on real-world network datasets, that demonstrate the improved quality of alarm relations learned by TempOpt as compared to temporal dependency method.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wavelet Mixture of Experts for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.08825</link>
<guid>https://arxiv.org/abs/2508.08825</guid>
<content:encoded><![CDATA[
<div> WaveTS-B, WaveTS-M, wavelet transforms, MLP, multi-channel dependencies <br />
Summary: <br />
The paper introduces WaveTS-B, a lightweight time series prediction model that combines wavelet transforms with MLP to capture periodic and non-stationary features. WaveTS-B addresses limitations of traditional Transformers and MLP models by incorporating wavelet transforms to improve data smoothing and capture non-stationary characteristics. The proposed model also introduces a channel clustering strategy based on a Mixture of Experts (MoE) framework to handle multi-channel dependencies efficiently. Additionally, WaveTS-M is introduced as an advanced model specifically designed for multi-channel time series prediction. Empirical evaluation on eight real-world datasets demonstrates that the WaveTS series models achieve state-of-the-art performance with fewer parameters. WaveTS-M shows significant improvements on multi-channel datasets, showcasing its effectiveness in handling complex data dependencies. <br /> <div>
arXiv:2508.08825v1 Announce Type: new 
Abstract: The field of time series forecasting is rapidly advancing, with recent large-scale Transformers and lightweight Multilayer Perceptron (MLP) models showing strong predictive performance. However, conventional Transformer models are often hindered by their large number of parameters and their limited ability to capture non-stationary features in data through smoothing. Similarly, MLP models struggle to manage multi-channel dependencies effectively. To address these limitations, we propose a novel, lightweight time series prediction model, WaveTS-B. This model combines wavelet transforms with MLP to capture both periodic and non-stationary characteristics of data in the wavelet domain. Building on this foundation, we propose a channel clustering strategy that incorporates a Mixture of Experts (MoE) framework, utilizing a gating mechanism and expert network to handle multi-channel dependencies efficiently. We propose WaveTS-M, an advanced model tailored for multi-channel time series prediction. Empirical evaluation across eight real-world time series datasets demonstrates that our WaveTS series models achieve state-of-the-art (SOTA) performance with significantly fewer parameters. Notably, WaveTS-M shows substantial improvements on multi-channel datasets, highlighting its effectiveness.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Battery Manifold Design with Heterogeneous Inputs Through Generative Adversarial Neural Networks</title>
<link>https://arxiv.org/abs/2508.08863</link>
<guid>https://arxiv.org/abs/2508.08863</guid>
<content:encoded><![CDATA[
<div> framework, generative models, design representation, interpretability, Bayesian optimization

Summary: 
This article introduces a systematic framework for constructing training datasets for generative machine learning models to enhance design representation and interpretability. The framework generates internally homogeneous but mutually heterogeneous archetypes to build a diverse training dataset. By integrating generative models with Bayesian optimization, the latent space of admissible designs becomes more interpretable, aiding in identifying optimal features. The effectiveness of this approach is demonstrated through the design of a flow battery manifold, showcasing its ability to capture feasible designs and explore novel configurations efficiently. This work expands the use of generative machine-learning models in system design, improving quality and reliability. 

<br /><br /> <div>
arXiv:2508.08863v1 Announce Type: new 
Abstract: Generative machine learning has emerged as a powerful tool for design representation and exploration. However, its application is often constrained by the need for large datasets of existing designs and the lack of interpretability about what features drive optimality. To address these challenges, we introduce a systematic framework for constructing training datasets tailored to generative models and demonstrate how these models can be leveraged for interpretable design. The novelty of this work is twofold: (i) we present a systematic framework for generating archetypes with internally homogeneous but mutually heterogeneous inputs that can be used to generate a training dataset, and (ii) we show how integrating generative models with Bayesian optimization can enhance the interpretability of the latent space of admissible designs. These findings are validated by using the framework to design a flow battery manifold, demonstrating that it effectively captures the space of feasible designs, including novel configurations while enabling efficient exploration. This work broadens the applicability of generative machine-learning models in system designs by enhancing quality and reliability.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models</title>
<link>https://arxiv.org/abs/2508.08875</link>
<guid>https://arxiv.org/abs/2508.08875</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Large Language Models, Data Privacy, GDPR, Unlearning <br />
<br />
Summary: 
The article introduces Oblivionis, a framework that allows clients to selectively remove private data during federated learning of Large Language Models (LLMs). Current federated LLM frameworks lack mechanisms for regulatory compliance like the right to be forgotten under GDPR. Oblivionis addresses this gap by unifying federated learning and unlearning as dual optimization objectives. It incorporates various federated learning and unlearning algorithms for evaluation, demonstrating superior performance compared to local training. Oblivionis strikes a balance between forgetting efficacy and model utility, enhancing trustworthiness and regulatory compliance in federated LLM training. The framework provides clear directions for future LLM development by enabling selective removal of specific data and ensuring data privacy in collaborative training settings. <div>
arXiv:2508.08875v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly leverage Federated Learning (FL) to utilize private, task-specific datasets for fine-tuning while preserving data privacy. However, while federated LLM frameworks effectively enable collaborative training without raw data sharing, they critically lack built-in mechanisms for regulatory compliance like GDPR's right to be forgotten. Integrating private data heightens concerns over data quality and long-term governance, yet existing distributed training frameworks offer no principled way to selectively remove specific client contributions post-training. Due to distributed data silos, stringent privacy constraints, and the intricacies of interdependent model aggregation, federated LLM unlearning is significantly more complex than centralized LLM unlearning. To address this gap, we introduce Oblivionis, a lightweight learning and unlearning framework that enables clients to selectively remove specific private data during federated LLM training, enhancing trustworthiness and regulatory compliance. By unifying FL and unlearning as a dual optimization objective, we incorporate 6 FL and 5 unlearning algorithms for comprehensive evaluation and comparative analysis, establishing a robust pipeline for federated LLM unlearning. Extensive experiments demonstrate that Oblivionis outperforms local training, achieving a robust balance between forgetting efficacy and model utility, with cross-algorithm comparisons providing clear directions for future LLM development.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable Lottery Ticket Networks using Genetic Algorithms</title>
<link>https://arxiv.org/abs/2508.08877</link>
<guid>https://arxiv.org/abs/2508.08877</guid>
<content:encoded><![CDATA[
<div> Genetic algorithms, subnetwork identification, strong lottery ticket hypothesis, model scalability, binary and multi-class classification
Summary:
Genetic algorithms are utilized to identify strong lottery ticket subnetworks within overparameterized neural networks for efficient and effective model training. The Strong Lottery Ticket Hypothesis suggests that certain subnetworks within these networks can achieve high accuracy without training. The approach achieves superior accuracies and sparsity levels for binary and multi-class classification tasks compared to current methods, without the need for gradient information. Proper evaluation metrics are emphasized for scaling to more complex network architectures and learning tasks. This research challenges traditional paradigms by focusing on minimizing resource consumption from the outset rather than relying on model compression techniques post-training. <div>
arXiv:2508.08877v1 Announce Type: new 
Abstract: Building modern deep learning systems that are not just effective but also efficient requires rethinking established paradigms for model training and neural architecture design. Instead of adapting highly overparameterized networks and subsequently applying model compression techniques to reduce resource consumption, a new class of high-performing networks skips the need for expensive parameter updates, while requiring only a fraction of parameters, making them highly scalable. The Strong Lottery Ticket Hypothesis posits that within randomly initialized, sufficiently overparameterized neural networks, there exist subnetworks that can match the accuracy of the trained original model-without any training. This work explores the usage of genetic algorithms for identifying these strong lottery ticket subnetworks. We find that for instances of binary and multi-class classification tasks, our approach achieves better accuracies and sparsity levels than the current state-of-the-art without requiring any gradient information. In addition, we provide justification for the need for appropriate evaluation metrics when scaling to more complex network architectures and learning tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hi-fi functional priors by learning activations</title>
<link>https://arxiv.org/abs/2508.08880</link>
<guid>https://arxiv.org/abs/2508.08880</guid>
<content:encoded><![CDATA[
<div> activation models, Pade functions, piecewise linear functions, Bayesian Neural Networks, function-space priors 

Summary: 
The article discusses how function-space priors in Bayesian Neural Networks (BNNs) can improve regularization, uncertainty quantification, and decision-making through embedding beliefs directly into the model's output. It explores optimization techniques to handle the challenge of imposing function-space priors on BNNs by using trainable activations that can accommodate higher-complexity priors and match target function distributions. The study investigates flexible activation models such as Pade functions and piecewise linear functions and addresses learning challenges related to identifiability, loss construction, and symmetries. Empirical findings indicate that even a single wide hidden layer in BNNs with flexible trainable activation can effectively achieve desired function-space priors. <div>
arXiv:2508.08880v1 Announce Type: new 
Abstract: Function-space priors in Bayesian Neural Networks (BNNs) provide a more intuitive approach to embedding beliefs directly into the model's output, thereby enhancing regularization, uncertainty quantification, and risk-aware decision-making. However, imposing function-space priors on BNNs is challenging. We address this task through optimization techniques that explore how trainable activations can accommodate higher-complexity priors and match intricate target function distributions. We investigate flexible activation models, including Pade functions and piecewise linear functions, and discuss the learning challenges related to identifiability, loss construction, and symmetries. Our empirical findings indicate that even BNNs with a single wide hidden layer when equipped with flexible trainable activation, can effectively achieve desired function-space priors.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Causal Machine Learning Requires Rigorous Synthetic Experiments for Broader Adoption</title>
<link>https://arxiv.org/abs/2508.08883</link>
<guid>https://arxiv.org/abs/2508.08883</guid>
<content:encoded><![CDATA[
<div> Keywords: causal machine learning, decision-making, predictive power, causal inference, synthetic experiments 

Summary: 
Causal machine learning has the potential to revolutionize decision-making through the combination of machine learning algorithms and causal inference. However, these methods are not widely adopted within the machine learning community due to a lack of reliability and robustness in current empirical evaluations. The overuse of synthetic experiments is a major criticism, but the authors argue that synthetic experiments are crucial for accurately assessing and understanding the capabilities of causal machine learning methods. By critically reviewing current evaluation practices and proposing a set of principles for rigorous empirical analyses with synthetic data, the authors aim to build trust in causal machine learning methods and promote their broader adoption for impactful real-world applications. Adopting these principles will enable comprehensive evaluations that drive the practical utility and credibility of causal machine learning methods. 

<br /><br />Summary: <div>
arXiv:2508.08883v1 Announce Type: new 
Abstract: Causal machine learning has the potential to revolutionize decision-making by combining the predictive power of machine learning algorithms with the theory of causal inference. However, these methods remain underutilized by the broader machine learning community, in part because current empirical evaluations do not permit assessment of their reliability and robustness, undermining their practical utility. Specifically, one of the principal criticisms made by the community is the extensive use of synthetic experiments. We argue, on the contrary, that synthetic experiments are essential and necessary to precisely assess and understand the capabilities of causal machine learning methods. To substantiate our position, we critically review the current evaluation practices, spotlight their shortcomings, and propose a set of principles for conducting rigorous empirical analyses with synthetic data. Adopting the proposed principles will enable comprehensive evaluations that build trust in causal machine learning methods, driving their broader adoption and impactful real-world use.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stationarity Exploration for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.08919</link>
<guid>https://arxiv.org/abs/2508.08919</guid>
<content:encoded><![CDATA[
<div> Frequency domain, time series forecasting, deep learning, amplitude-phase, stationary information

Summary: 
- The paper introduces a new approach called Amplitude-Phase Reconstruct Network (APRNet) for time series forecasting that effectively models the inter-relationships of amplitude and phase.
- APRNet decouples amplitude and phase characteristics, enabling better capture of stationary information from complex frequency components.
- The model represents multivariate time series input in sequence and channel dimensions, emphasizing the correlation between amplitude and phase at various interaction frequencies.
- A Kolmogorov-Arnold-Network-based Local Correlation (KLC) module is proposed to adaptively fit local functions using univariate functions, enhancing the characterization of time-varying patterns.
- Extensive experiments demonstrate the superiority of APRNet over existing state-of-the-art methods in time series forecasting. 

<br /><br /> <div>
arXiv:2508.08919v1 Announce Type: new 
Abstract: Deep learning-based time series forecasting has found widespread applications. Recently, converting time series data into the frequency domain for forecasting has become popular for accurately exploring periodic patterns. However, existing methods often cannot effectively explore stationary information from complex intertwined frequency components. In this paper, we propose a simple yet effective Amplitude-Phase Reconstruct Network (APRNet) that models the inter-relationships of amplitude and phase, which prevents the amplitude and phase from being constrained by different physical quantities, thereby decoupling the distinct characteristics of signals for capturing stationary information. Specifically, we represent the multivariate time series input across sequence and channel dimensions, highlighting the correlation between amplitude and phase at multiple interaction frequencies. We propose a novel Kolmogorov-Arnold-Network-based Local Correlation (KLC) module to adaptively fit local functions using univariate functions, enabling more flexible characterization of stationary features across different amplitudes and phases. This significantly enhances the model's capability to capture time-varying patterns. Extensive experiments demonstrate the superiority of our APRNet against the state-of-the-arts (SOTAs).
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Cross-Stage Adversarial Transferability in Class-Incremental Continual Learning</title>
<link>https://arxiv.org/abs/2508.08920</link>
<guid>https://arxiv.org/abs/2508.08920</guid>
<content:encoded><![CDATA[
<div> Keywords: class-incremental continual learning, adversarial attacks, stage-transferred attacks, model similarity, defense methods <br />
Summary: 
Class-incremental continual learning aims to address catastrophic forgetting by allowing models to retain knowledge of previously learned classes while learning new ones. However, this study finds that models undergoing continual learning are vulnerable to stage-transferred attacks, where an adversarial example generated using a past model stage can compromise the model in a later stage. The susceptibility to these attacks is attributed to model similarity between stages and the gradual degradation of robustness. Moreover, existing adversarial training-based defense mechanisms are found to be inadequate in mitigating stage-transferred attacks. This research highlights a significant security concern in continual learning paradigms and underscores the need for robust defense strategies to protect against adversarial threats. The code for this study is available on GitHub for further exploration and validation. <br /><br />Summary: <div>
arXiv:2508.08920v1 Announce Type: new 
Abstract: Class-incremental continual learning addresses catastrophic forgetting by enabling classification models to preserve knowledge of previously learned classes while acquiring new ones. However, the vulnerability of the models against adversarial attacks during this process has not been investigated sufficiently. In this paper, we present the first exploration of vulnerability to stage-transferred attacks, i.e., an adversarial example generated using the model in an earlier stage is used to attack the model in a later stage. Our findings reveal that continual learning methods are highly susceptible to these attacks, raising a serious security issue. We explain this phenomenon through model similarity between stages and gradual robustness degradation. Additionally, we find that existing adversarial training-based defense methods are not sufficiently effective to stage-transferred attacks. Codes are available at https://github.com/mcml-official/CSAT.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LNN-PINN: A Unified Physics-Only Training Framework with Liquid Residual Blocks</title>
<link>https://arxiv.org/abs/2508.08935</link>
<guid>https://arxiv.org/abs/2508.08935</guid>
<content:encoded><![CDATA[
<div> Keywords: physics-informed neural networks, PINNs, liquid residual gating architecture, predictive accuracy, architectural enhancement<br />
<br />
Summary: <br />
The article introduces LNN-PINN, a new framework that enhances physics-informed neural networks by incorporating a liquid residual gating architecture. This architecture helps improve predictive accuracy without changing the original physics modeling and optimization pipeline. LNN-PINN introduces a lightweight gating mechanism in the hidden-layer mapping, resulting in reduced RMSE and MAE across benchmark problems. The framework maintains adaptability and stability across various dimensions, boundary conditions, and operator characteristics. By solely focusing on architectural refinement, LNN-PINN provides a concise and effective way to enhance the accuracy of physics-informed neural networks in complex scientific and engineering problems. <div>
arXiv:2508.08935v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) have attracted considerable attention for their ability to integrate partial differential equation priors into deep learning frameworks; however, they often exhibit limited predictive accuracy when applied to complex problems. To address this issue, we propose LNN-PINN, a physics-informed neural network framework that incorporates a liquid residual gating architecture while preserving the original physics modeling and optimization pipeline to improve predictive accuracy. The method introduces a lightweight gating mechanism solely within the hidden-layer mapping, keeping the sampling strategy, loss composition, and hyperparameter settings unchanged to ensure that improvements arise purely from architectural refinement. Across four benchmark problems, LNN-PINN consistently reduced RMSE and MAE under identical training conditions, with absolute error plots further confirming its accuracy gains. Moreover, the framework demonstrates strong adaptability and stability across varying dimensions, boundary conditions, and operator characteristics. In summary, LNN-PINN offers a concise and effective architectural enhancement for improving the predictive accuracy of physics-informed neural networks in complex scientific and engineering problems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalising Traffic Forecasting to Regions without Traffic Observations</title>
<link>https://arxiv.org/abs/2508.08947</link>
<guid>https://arxiv.org/abs/2508.08947</guid>
<content:encoded><![CDATA[
<div> forecasting, traffic sensors, GenCast, external signals, physics-informed neural networks

Summary:<br />
The paper introduces GenCast, a model designed to forecast traffic in regions lacking sensors by incorporating external knowledge. It leverages physics-informed neural networks to improve generalization through physical principles. An external signal learning module is employed to analyze correlations between traffic conditions and external factors like weather. A spatial grouping module helps filter out local features that might hinder the model's performance. Experimental results demonstrate GenCast's ability to reduce forecasting errors across multiple real-world datasets. <div>
arXiv:2508.08947v1 Announce Type: new 
Abstract: Traffic forecasting is essential for intelligent transportation systems. Accurate forecasting relies on continuous observations collected by traffic sensors. However, due to high deployment and maintenance costs, not all regions are equipped with such sensors. This paper aims to forecast for regions without traffic sensors, where the lack of historical traffic observations challenges the generalisability of existing models. We propose a model named GenCast, the core idea of which is to exploit external knowledge to compensate for the missing observations and to enhance generalisation. We integrate physics-informed neural networks into GenCast, enabling physical principles to regularise the learning process. We introduce an external signal learning module to explore correlations between traffic states and external signals such as weather conditions, further improving model generalisability. Additionally, we design a spatial grouping module to filter localised features that hinder model generalisability. Extensive experiments show that GenCast consistently reduces forecasting errors on multiple real-world datasets.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAVITY: A Controversial Graph Representation Learning for Vertex Classification</title>
<link>https://arxiv.org/abs/2508.08954</link>
<guid>https://arxiv.org/abs/2508.08954</guid>
<content:encoded><![CDATA[
<div> GRAVITY, vertex classification, graph-based, representation learning, interaction topology <br />
<br />
Summary: <br />
The GRAVITY framework for accurate vertex classification is inspired by physical systems where vertices self-organize based on learned interactions. Vertex influence is shaped by structural proximity and attribute similarity, creating a latent potential field that drives vertices towards energy-efficient positions. Unlike traditional approaches, GRAVITY dynamically modulates the receptive field of each vertex, enabling adaptive aggregation driven by context. This field-driven organization sharpens class boundaries and enhances semantic coherence within latent clusters. Experimental results on real-world datasets demonstrate that GRAVITY produces competitive embeddings and excels in both transductive and inductive vertex classification tasks. <div>
arXiv:2508.08954v1 Announce Type: new 
Abstract: In the quest of accurate vertex classification, we introduce GRAVITY (Graph-based Representation leArning via Vertices Interaction TopologY), a framework inspired by physical systems where objects self-organize under attractive forces. GRAVITY models each vertex as exerting influence through learned interactions shaped by structural proximity and attribute similarity. These interactions induce a latent potential field in which vertices move toward energy efficient positions, coalescing around class-consistent attractors and distancing themselves from unrelated groups. Unlike traditional message-passing schemes with static neighborhoods, GRAVITY adaptively modulates the receptive field of each vertex based on a learned force function, enabling dynamic aggregation driven by context. This field-driven organization sharpens class boundaries and promotes semantic coherence within latent clusters. Experiments on real-world benchmarks show that GRAVITY yields competitive embeddings, excelling in both transductive and inductive vertex classification tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss</title>
<link>https://arxiv.org/abs/2508.08955</link>
<guid>https://arxiv.org/abs/2508.08955</guid>
<content:encoded><![CDATA[
<div> susceptibility, adversarial attacks, time series forecasting, frequency domain features, prediction

Summary:<br />
The article discusses the vulnerability of deep learning models, specifically Transformer-based models, to adversarial attacks in time series forecasting. While research on adversarial robustness in fields like computer vision is extensive, the frequency domain aspects of time series data have not been sufficiently explored in terms of attacks. The paper introduces a novel attack algorithm focused on frequency domain loss for time series prediction. By optimizing adversarial samples using both time-domain and frequency-domain losses, the proposed approach demonstrates significant success on popular time series forecasting datasets. This highlights the need to consider frequency information when assessing the robustness of time series prediction models against adversarial attacks. <div>
arXiv:2508.08955v1 Announce Type: new 
Abstract: Transformer-based models have made significant progress in time series forecasting. However, a key limitation of deep learning models is their susceptibility to adversarial attacks, which has not been studied enough in the context of time series prediction. In contrast to areas such as computer vision, where adversarial robustness has been extensively studied, frequency domain features of time series data play an important role in the prediction task but have not been sufficiently explored in terms of adversarial attacks. This paper proposes a time series prediction attack algorithm based on frequency domain loss. Specifically, we adapt an attack method originally designed for classification tasks to the prediction field and optimize the adversarial samples using both time-domain and frequency-domain losses. To the best of our knowledge, there is no relevant research on using frequency information for time-series adversarial attacks. Our experimental results show that these current time series prediction models are vulnerable to adversarial attacks, and our approach achieves excellent performance on major time series forecasting datasets.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating attention into explanation frameworks for language and vision transformers</title>
<link>https://arxiv.org/abs/2508.08966</link>
<guid>https://arxiv.org/abs/2508.08966</guid>
<content:encoded><![CDATA[
arXiv:2508.08966v1 Announce Type: new 
Abstract: The attention mechanism lies at the core of the transformer architecture, providing an interpretable model-internal signal that has motivated a growing interest in attention-based model explanations. Although attention weights do not directly determine model outputs, they reflect patterns of token influence that can inform and complement established explainability techniques. This work studies the potential of utilising the information encoded in attention weights to provide meaningful model explanations by integrating them into explainable AI (XAI) frameworks that target fundamentally different aspects of model behaviour. To this end, we develop two novel explanation methods applicable to both natural language processing and computer vision tasks. The first integrates attention weights into the Shapley value decomposition by redefining the characteristic function in terms of pairwise token interactions via attention weights, thus adapting this widely used game-theoretic solution concept to provide attention-driven attributions for local explanations. The second incorporates attention weights into token-level directional derivatives defined through concept activation vectors to measure concept sensitivity for global explanations. Our empirical evaluations on standard benchmarks and in a comparison study with widely used explanation methods show that attention weights can be meaningfully incorporated into the studied XAI frameworks, highlighting their value in enriching transformer explainability.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Regret and Low-Complexity Learning for Hierarchical Inference</title>
<link>https://arxiv.org/abs/2508.08985</link>
<guid>https://arxiv.org/abs/2508.08985</guid>
<content:encoded><![CDATA[
arXiv:2508.08985v1 Announce Type: new 
Abstract: This work focuses on Hierarchical Inference (HI) in edge intelligence systems, where a compact Local-ML model on an end-device works in conjunction with a high-accuracy Remote-ML model on an edge-server. HI aims to reduce latency, improve accuracy, and lower bandwidth usage by first using the Local-ML model for inference and offloading to the Remote-ML only when the local inference is likely incorrect. A critical challenge in HI is estimating the likelihood of the local inference being incorrect, especially when data distributions and offloading costs change over time -- a problem we term Hierarchical Inference Learning (HIL). We introduce a novel approach to HIL by modeling the probability of correct inference by the Local-ML as an increasing function of the model's confidence measure, a structure motivated by empirical observations but previously unexploited. We propose two policies, HI-LCB and HI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We demonstrate that both policies achieve order-optimal regret of $O(\log T)$, a significant improvement over existing HIL policies with $O(T^{2/3})$ regret guarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational complexity, making it well-suited for deployment on devices with severe resource limitations. Simulations using real-world datasets confirm that our policies outperform existing state-of-the-art HIL methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation</title>
<link>https://arxiv.org/abs/2508.09005</link>
<guid>https://arxiv.org/abs/2508.09005</guid>
<content:encoded><![CDATA[
arXiv:2508.09005v1 Announce Type: new 
Abstract: Designing mechanical mechanisms to trace specific paths is a classic yet notoriously difficult engineering problem, characterized by a vast and complex search space of discrete topologies and continuous parameters. We introduce MechaFormer, a Transformer-based model that tackles this challenge by treating mechanism design as a conditional sequence generation task. Our model learns to translate a target curve into a domain-specific language (DSL) string, simultaneously determining the mechanism's topology and geometric parameters in a single, unified process. MechaFormer significantly outperforms existing baselines, achieving state-of-the-art path-matching accuracy and generating a wide diversity of novel and valid designs. We demonstrate a suite of sampling strategies that can dramatically improve solution quality and offer designers valuable flexibility. Furthermore, we show that the high-quality outputs from MechaFormer serve as excellent starting points for traditional optimizers, creating a hybrid approach that finds superior solutions with remarkable efficiency.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm</title>
<link>https://arxiv.org/abs/2508.09056</link>
<guid>https://arxiv.org/abs/2508.09056</guid>
<content:encoded><![CDATA[
arXiv:2508.09056v1 Announce Type: new 
Abstract: Intrusion Detection Systems (IDS) have an increasingly important role in preventing exploitation of network vulnerabilities by malicious actors. Recent deep learning based developments have resulted in significant improvements in the performance of IDS systems. In this paper, we present FetFIDS, where we explore the employment of feature embedding instead of positional embedding to improve intrusion detection performance of a transformer based deep learning system. Our model is developed with the aim of deployments in edge learning scenarios, where federated learning over multiple communication rounds can ensure both privacy and localized performance improvements. FetFIDS outperforms multiple state-of-the-art intrusion detection systems in a federated environment and demonstrates a high degree of suitability to federated learning. The code for this work can be found at https://github.com/ghosh64/fetfids.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Machine Learning for Patient-Level Intraoperative Opioid Dose Prediction from Electronic Health Records</title>
<link>https://arxiv.org/abs/2508.09059</link>
<guid>https://arxiv.org/abs/2508.09059</guid>
<content:encoded><![CDATA[
arXiv:2508.09059v1 Announce Type: new 
Abstract: This paper introduces the OPIAID algorithm, a novel approach for predicting and recommending personalized opioid dosages for individual patients. The algorithm optimizes pain management while minimizing opioid related adverse events (ORADE) by employing machine learning models trained on observational electronic health records (EHR) data. It leverages a causal machine learning approach to understand the relationship between opioid dose, case specific patient and intraoperative characteristics, and pain versus ORADE outcomes. The OPIAID algorithm considers patient-specific characteristics and the influence of different opiates, enabling personalized dose recommendations. This paper outlines the algorithm's methodology and architecture, and discusses key assumptions, and approaches to evaluating its performance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-learning optimizes predictions of missing links in real-world networks</title>
<link>https://arxiv.org/abs/2508.09069</link>
<guid>https://arxiv.org/abs/2508.09069</guid>
<content:encoded><![CDATA[
arXiv:2508.09069v1 Announce Type: new 
Abstract: Relational data are ubiquitous in real-world data applications, e.g., in social network analysis or biological modeling, but networks are nearly always incompletely observed. The state-of-the-art for predicting missing links in the hard case of a network without node attributes uses model stacking or neural network techniques. It remains unknown which approach is best, and whether or how the best choice of algorithm depends on the input network's characteristics. We answer these questions systematically using a large, structurally diverse benchmark of 550 real-world networks under two standard accuracy measures (AUC and Top-k), comparing four stacking algorithms with 42 topological link predictors, two of which we introduce here, and two graph neural network algorithms. We show that no algorithm is best across all input networks, all algorithms perform well on most social networks, and few perform well on economic and biological networks. Overall, model stacking with a random forest is both highly scalable and surpasses on AUC or is competitive with graph neural networks on Top-k accuracy. But, algorithm performance depends strongly on network characteristics like the degree distribution, triangle density, and degree assortativity. We introduce a meta-learning algorithm that exploits this variability to optimize link predictions for individual networks by selecting the best algorithm to apply, which we show outperforms all state-of-the-art algorithms and scales to large networks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up Active Testing to Large Language Models</title>
<link>https://arxiv.org/abs/2508.09093</link>
<guid>https://arxiv.org/abs/2508.09093</guid>
<content:encoded><![CDATA[
arXiv:2508.09093v1 Announce Type: new 
Abstract: Active testing enables label-efficient evaluation of models through careful data acquisition. However, its significant computational costs have previously undermined its use for large models. We show how it can be successfully scaled up to the evaluation of large language models (LLMs). In particular we show that the surrogate model used to guide data acquisition can be constructed cheaply using in-context learning, does not require updating within an active-testing loop, and can be smaller than the target model. We even find we can make good data-acquisition decisions without computing predictions with the target model and further introduce a single-run error estimator to asses how well active testing is working on the fly. We find that our approach is able to more effectively evaluate LLM performance with less data than current standard practices.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chi-Geometry: A Library for Benchmarking Chirality Prediction of GNNs</title>
<link>https://arxiv.org/abs/2508.09097</link>
<guid>https://arxiv.org/abs/2508.09097</guid>
<content:encoded><![CDATA[
arXiv:2508.09097v1 Announce Type: new 
Abstract: We introduce Chi-Geometry - a library that generates graph data for testing and benchmarking GNNs' ability to predict chirality. Chi-Geometry generates synthetic graph samples with (i) user-specified geometric and topological traits to isolate certain types of samples and (ii) randomized node positions and species to minimize extraneous correlations. Each generated graph contains exactly one chiral center labeled either R or S, while all other nodes are labeled N/A (non-chiral). The generated samples are then combined into a cohesive dataset that can be used to assess a GNN's ability to predict chirality as a node classification task. Chi-Geometry allows more interpretable and less confounding benchmarking of GNNs for prediction of chirality in the graph samples which can guide the design of new GNN architectures with improved predictive performance. We illustrate Chi-Geometry's efficacy by using it to generate synthetic datasets for benchmarking various state-of-the-art (SOTA) GNN architectures. The conclusions of these benchmarking results guided our design of two new GNN architectures. The first GNN architecture established all-to-all connections in the graph to accurately predict chirality across all challenging configurations where previously tested SOTA models failed, but at a computational cost (both for training and inference) that grows quadratically with the number of graph nodes. The second GNN architecture avoids all-to-all connections by introducing a virtual node in the original graph structure of the data, which restores the linear scaling of training and inference computational cost with respect to the number of nodes in the graph, while still ensuring competitive accuracy in detecting chirality with respect to SOTA GNN architectures.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving</title>
<link>https://arxiv.org/abs/2508.09099</link>
<guid>https://arxiv.org/abs/2508.09099</guid>
<content:encoded><![CDATA[
arXiv:2508.09099v1 Announce Type: new 
Abstract: Large vision language models exhibit notable limitations on Geometry Problem Solving (GPS) because of their unreliable diagram interpretation and pure natural-language reasoning. A recent line of work mitigates this by using symbolic solvers: the model directly generates a formal program that a geometry solver can execute. However, this direct program generation lacks intermediate reasoning, making the decision process opaque and prone to errors. In this work, we explore a new approach that integrates Chain-of-Thought (CoT) with formal language. The model interleaves natural language reasoning with incremental emission of solver-executable code, producing a hybrid reasoning trace in which critical derivations are expressed in formal language. To teach this behavior at scale, we combine (1) supervised fine-tuning on an 11K newly developed synthetic dataset with interleaved natural language reasoning and automatic formalization, and (2) solver-in-the-loop reinforcement learning that jointly optimizes both the CoT narrative and the resulting program through outcome-based rewards. Built on Qwen2.5-VL-7B, our new model, named GF-Reasoner, achieves up to 15% accuracy improvements on standard GPS benchmarks, surpassing both 7B-scale peers and the much larger model Qwen2.5-VL-72B. By exploiting high-order geometric knowledge and offloading symbolic computation to the solver, the generated reasoning traces are noticeably shorter and cleaner. Furthermore, we present a comprehensive analysis of method design choices (e.g., reasoning paradigms, data synthesis, training epochs, etc.), providing actionable insights for future research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Universal Neural Inference</title>
<link>https://arxiv.org/abs/2508.09100</link>
<guid>https://arxiv.org/abs/2508.09100</guid>
<content:encoded><![CDATA[
arXiv:2508.09100v1 Announce Type: new 
Abstract: Real-world data often appears in diverse, disjoint forms -- with varying schemas, inconsistent semantics, and no fixed feature ordering -- making it challenging to build general-purpose models that can leverage information across datasets. We introduce ASPIRE, Arbitrary Set-based Permutation-Invariant Reasoning Engine, a Universal Neural Inference model for semantic reasoning and prediction over heterogeneous structured data. ASPIRE combines a permutation-invariant, set-based Transformer with a semantic grounding module that incorporates natural language descriptions, dataset metadata, and in-context examples to learn cross-dataset feature dependencies. This architecture allows ASPIRE to ingest arbitrary sets of feature--value pairs and support examples, align semantics across disjoint tables, and make predictions for any specified target. Once trained, ASPIRE generalizes to new inference tasks without additional tuning. In addition to delivering strong results across diverse benchmarks, ASPIRE naturally supports cost-aware active feature acquisition in an open-world setting, selecting informative features under test-time budget constraints for an arbitrary unseen dataset. These capabilities position ASPIRE as a step toward truly universal, semantics-aware inference over structured data.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Neural Network Calibration by Reducing Classifier Shift with Stochastic Masking</title>
<link>https://arxiv.org/abs/2508.09116</link>
<guid>https://arxiv.org/abs/2508.09116</guid>
<content:encoded><![CDATA[
arXiv:2508.09116v1 Announce Type: new 
Abstract: In recent years, deep neural networks (DNNs) have shown competitive results in many fields. Despite this success, they often suffer from poor calibration, especially in safety-critical scenarios such as autonomous driving and healthcare, where unreliable confidence estimates can lead to serious consequences. Recent studies have focused on improving calibration by modifying the classifier, yet such efforts remain limited. Moreover, most existing approaches overlook calibration errors caused by underconfidence, which can be equally detrimental. To address these challenges, we propose MaC-Cal, a novel mask-based classifier calibration method that leverages stochastic sparsity to enhance the alignment between confidence and accuracy. MaC-Cal adopts a two-stage training scheme with adaptive sparsity, dynamically adjusting mask retention rates based on the deviation between confidence and accuracy. Extensive experiments show that MaC-Cal achieves superior calibration performance and robustness under data corruption, offering a practical and effective solution for reliable confidence estimation in DNNs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Imputation Techniques for Short-Term Gaps in Heart Rate Data</title>
<link>https://arxiv.org/abs/2508.08268</link>
<guid>https://arxiv.org/abs/2508.08268</guid>
<content:encoded><![CDATA[
arXiv:2508.08268v1 Announce Type: cross 
Abstract: Recent advances in wearable technology have enabled the continuous monitoring of vital physiological signals, essential for predictive modeling and early detection of extreme physiological events. Among these physiological signals, heart rate (HR) plays a central role, as it is widely used in monitoring and managing cardiovascular conditions and detecting extreme physiological events such as hypoglycemia. However, data from wearable devices often suffer from missing values. To address this issue, recent studies have employed various imputation techniques. Traditionally, the effectiveness of these methods has been evaluated using predictive accuracy metrics such as RMSE, MAPE, and MAE, which assess numerical proximity to the original data. While informative, these metrics fail to capture the complex statistical structure inherent in physiological signals. This study bridges this gap by presenting a comprehensive evaluation of four statistical imputation methods, linear interpolation, K Nearest Neighbors (KNN), Piecewise Cubic Hermite Interpolating Polynomial (PCHIP), and B splines, for short term HR data gaps. We assess their performance using both predictive accuracy metrics and statistical distance measures, including the Cohen Distance Test (CDT) and Jensen Shannon Distance (JS Distance), applied to HR data from the D1NAMO dataset and the BIG IDEAs Lab Glycemic Variability and Wearable Device dataset. The analysis reveals limitations in existing imputation approaches and the absence of a robust framework for evaluating imputation quality in physiological signals. Finally, this study proposes a foundational framework to develop a composite evaluation metric to assess imputation performance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning</title>
<link>https://arxiv.org/abs/2508.08273</link>
<guid>https://arxiv.org/abs/2508.08273</guid>
<content:encoded><![CDATA[
arXiv:2508.08273v1 Announce Type: cross 
Abstract: Clinical language models often struggle to provide trustworthy predictions and explanations when applied to lengthy, unstructured electronic health records (EHRs). This work introduces TT-XAI, a lightweight and effective framework that improves both classification performance and interpretability through domain-aware keyword distillation and reasoning with large language models (LLMs). First, we demonstrate that distilling raw discharge notes into concise keyword representations significantly enhances BERT classifier performance and improves local explanation fidelity via a focused variant of LIME. Second, we generate chain-of-thought clinical explanations using keyword-guided prompts to steer LLMs, producing more concise and clinically relevant reasoning. We evaluate explanation quality using deletion-based fidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human study with domain experts. All evaluation modalities consistently favor the keyword-augmented method, confirming that distillation enhances both machine and human interpretability. TT-XAI offers a scalable pathway toward trustworthy, auditable AI in clinical decision support.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling Knowledge from Large Language Models: A Concept Bottleneck Model for Hate and Counter Speech Recognition</title>
<link>https://arxiv.org/abs/2508.08274</link>
<guid>https://arxiv.org/abs/2508.08274</guid>
<content:encoded><![CDATA[
arXiv:2508.08274v1 Announce Type: cross 
Abstract: The rapid increase in hate speech on social media has exposed an unprecedented impact on society, making automated methods for detecting such content important. Unlike prior black-box models, we propose a novel transparent method for automated hate and counter speech recognition, i.e., "Speech Concept Bottleneck Model" (SCBM), using adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to map input texts to an abstract adjective-based representation, which is then sent to a light-weight classifier for downstream tasks. Across five benchmark datasets spanning multiple languages and platforms (e.g., Twitter, Reddit, YouTube), SCBM achieves an average macro-F1 score of 0.69 which outperforms the most recently reported results from the literature on four out of five datasets. Aside from high recognition accuracy, SCBM provides a high level of both local and global interpretability. Furthermore, fusing our adjective-based concept representation with transformer embeddings, leads to a 1.8% performance increase on average across all datasets, showing that the proposed representation captures complementary information. Our results demonstrate that adjective-based concept representations can serve as compact, interpretable, and effective encodings for hate and counter speech recognition. With adapted adjectives, our method can also be applied to other NLP tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Objective Metrics for Evaluating Large Language Models Using External Data Sources</title>
<link>https://arxiv.org/abs/2508.08277</link>
<guid>https://arxiv.org/abs/2508.08277</guid>
<content:encoded><![CDATA[
arXiv:2508.08277v1 Announce Type: cross 
Abstract: Evaluating the performance of Large Language Models (LLMs) is a critical yet challenging task, particularly when aiming to avoid subjective assessments. This paper proposes a framework for leveraging subjective metrics derived from the class textual materials across different semesters to assess LLM outputs across various tasks. By utilizing well-defined benchmarks, factual datasets, and structured evaluation pipelines, the approach ensures consistent, reproducible, and bias-minimized measurements. The framework emphasizes automation and transparency in scoring, reducing reliance on human interpretation while ensuring alignment with real-world applications. This method addresses the limitations of subjective evaluation methods, providing a scalable solution for performance assessment in educational, scientific, and other high-stakes domains.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MinionsLLM: a Task-adaptive Framework For The Training and Control of Multi-Agent Systems Through Natural Language</title>
<link>https://arxiv.org/abs/2508.08283</link>
<guid>https://arxiv.org/abs/2508.08283</guid>
<content:encoded><![CDATA[
arXiv:2508.08283v1 Announce Type: cross 
Abstract: This paper presents MinionsLLM, a novel framework that integrates Large Language Models (LLMs) with Behavior Trees (BTs) and Formal Grammars to enable natural language control of multi-agent systems within arbitrary, user-defined environments. MinionsLLM provides standardized interfaces for defining environments, agents, and behavioral primitives, and introduces two synthetic dataset generation methods (Method A and Method B) to fine-tune LLMs for improved syntactic validity and semantic task relevance. We validate our approach using Google's Gemma 3 model family at three parameter scales (1B, 4B, and 12B) and demonstrate substantial gains: Method B increases syntactic validity to 92.6% and achieves a mean task performance improvement of 33% over baseline. Notably, our experiments show that smaller models benefit most from fine-tuning, suggesting promising directions for deploying compact, locally hosted LLMs in resource-constrained multi-agent control scenarios. The framework and all resources are released open-source to support reproducibility and future research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2508.08285</link>
<guid>https://arxiv.org/abs/2508.08285</guid>
<content:encoded><![CDATA[
arXiv:2508.08285v1 Announce Type: cross 
Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their tendency to hallucinate poses serious challenges for reliable deployment. Despite numerous hallucination detection methods, their evaluations often rely on ROUGE, a metric based on lexical overlap that misaligns with human judgments. Through comprehensive human studies, we demonstrate that while ROUGE exhibits high recall, its extremely low precision leads to misleading performance estimates. In fact, several established detection methods show performance drops of up to 45.9\% when assessed using human-aligned metrics like LLM-as-Judge. Moreover, our analysis reveals that simple heuristics based on response length can rival complex detection techniques, exposing a fundamental flaw in current evaluation practices. We argue that adopting semantically aware and robust evaluation frameworks is essential to accurately gauge the true performance of hallucination detection methods, ultimately ensuring the trustworthiness of LLM outputs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Experiments</title>
<link>https://arxiv.org/abs/2508.08288</link>
<guid>https://arxiv.org/abs/2508.08288</guid>
<content:encoded><![CDATA[
arXiv:2508.08288v1 Announce Type: cross 
Abstract: The scientific process is a means for turning the results of experiments into knowledge about the world in which we live. Much research effort has been directed toward automating this process. To do this, one needs to formulate the scientific process in a precise mathematical language. This paper outlines one such language. What is presented here is hardly new. The material leans much on great thinkers of times past as well as more modern contributions. The novel contributions of this paper are: A new, general data processing inequality, a bias variance decomposition for canonical losses, Streamlined proofs of the Blackwell-Sherman-Stein and Randomization Theorems, and Means to calculate deficiency via linear programming.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Putnam-AXIOM: A Functional and Static Benchmark</title>
<link>https://arxiv.org/abs/2508.08292</link>
<guid>https://arxiv.org/abs/2508.08292</guid>
<content:encoded><![CDATA[
arXiv:2508.08292v1 Announce Type: cross 
Abstract: Current mathematical reasoning benchmarks for large language models (LLMs) are approaching saturation, with some achieving > 90% accuracy, and are increasingly compromised by training-set contamination. We introduce Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn from the prestigious William Lowell Putnam Mathematical Competition, and Putnam-AXIOM Variation, an unseen companion set of 100 functional variants generated by programmatically perturbing variables and constants. The variation protocol produces an unlimited stream of equally difficult, unseen instances -- yielding a contamination-resilient test bed. On the Original set, OpenAI's o1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy drops by 19.6% (46.8% relative decrease) on the paired Variations. The remaining eighteen models show the same downward trend, ten of them with non-overlapping 95% confidence intervals. These gaps suggest memorization and highlight the necessity of dynamic benchmarks. We complement "boxed" accuracy with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores reasoning traces and automates natural language proof evaluations. Putnam-AXIOM therefore provides a rigorous, contamination-resilient evaluation framework for assessing advanced mathematical reasoning of LLMs. Data and evaluation code are publicly available at https://github.com/brando90/putnam-axiom.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational volume reconstruction with the Deep Ritz Method</title>
<link>https://arxiv.org/abs/2508.08309</link>
<guid>https://arxiv.org/abs/2508.08309</guid>
<content:encoded><![CDATA[
arXiv:2508.08309v1 Announce Type: cross 
Abstract: We present a novel approach to variational volume reconstruction from sparse, noisy slice data using the Deep Ritz method. Motivated by biomedical imaging applications such as MRI-based slice-to-volume reconstruction (SVR), our approach addresses three key challenges: (i) the reliance on image segmentation to extract boundaries from noisy grayscale slice images, (ii) the need to reconstruct volumes from a limited number of slice planes, and (iii) the computational expense of traditional mesh-based methods. We formulate a variational objective that combines a regression loss designed to avoid image segmentation by operating on noisy slice data directly with a modified Cahn-Hilliard energy incorporating anisotropic diffusion to regularize the reconstructed geometry. We discretize the phase field with a neural network, approximate the objective at each optimization step with Monte Carlo integration, and use ADAM to find the minimum of the approximated variational objective. While the stochastic integration may not yield the true solution to the variational problem, we demonstrate that our method reliably produces high-quality reconstructed volumes in a matter of seconds, even when the slice data is sparse and noisy.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFM-GP: Unified Conditional Flow Matching to Learn Gene Perturbation Across Cell Types</title>
<link>https://arxiv.org/abs/2508.08312</link>
<guid>https://arxiv.org/abs/2508.08312</guid>
<content:encoded><![CDATA[
arXiv:2508.08312v1 Announce Type: cross 
Abstract: Understanding gene perturbation effects across diverse cellular contexts is a central challenge in functional genomics, with important implications for therapeutic discovery and precision medicine. Single-cell technologies enable high-resolution measurement of transcriptional responses, but collecting such data is costly and time-consuming, especially when repeated for each cell type. Existing computational methods often require separate models per cell type, limiting scalability and generalization. We present CFM-GP, a method for cell type-agnostic gene perturbation prediction. CFM-GP learns a continuous, time-dependent transformation between unperturbed and perturbed gene expression distributions, conditioned on cell type, allowing a single model to predict across all cell types. Unlike prior approaches that use discrete modeling, CFM-GP employs a flow matching objective to capture perturbation dynamics in a scalable manner. We evaluate on five datasets: SARS-CoV-2 infection, IFN-beta stimulated PBMCs, glioblastoma treated with Panobinostat, lupus under IFN-beta stimulation, and Statefate progenitor fate mapping. CFM-GP consistently outperforms state-of-the-art baselines in R-squared and Spearman correlation, and pathway enrichment analysis confirms recovery of key biological pathways. These results demonstrate the robustness and biological fidelity of CFM-GP as a scalable solution for cross-cell type gene perturbation prediction.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Collusion of Pricing and Advertising on E-commerce Platforms</title>
<link>https://arxiv.org/abs/2508.08325</link>
<guid>https://arxiv.org/abs/2508.08325</guid>
<content:encoded><![CDATA[
arXiv:2508.08325v1 Announce Type: cross 
Abstract: Online sellers have been adopting AI learning algorithms to automatically make product pricing and advertising decisions on e-commerce platforms. When sellers compete using such algorithms, one concern is that of tacit collusion - the algorithms learn to coordinate on higher than competitive. We empirically investigate whether these concerns are valid when sellers make pricing and advertising decisions together, i.e., two-dimensional decisions. Our empirical strategy is to analyze competition with multi-agent reinforcement learning, which we calibrate to a large-scale dataset collected from Amazon.com products. Our first contribution is to find conditions under which learning algorithms can facilitate win-win-win outcomes that are beneficial for consumers, sellers, and even the platform, when consumers have high search costs. In these cases the algorithms learn to coordinate on prices that are lower than competitive prices. The intuition is that the algorithms learn to coordinate on lower advertising bids, which lower advertising costs, leading to lower prices. Our second contribution is an analysis of a large-scale, high-frequency keyword-product dataset for more than 2 million products on Amazon.com. Our estimates of consumer search costs show a wide range of costs for different product keywords. We generate an algorithm usage and find a negative interaction between the estimated consumer search costs and the algorithm usage index, providing empirical evidence of beneficial collusion. Finally, we analyze the platform's strategic response. We find that reserve price adjustments will not increase profits for the platform, but commission adjustments will. Our analyses help alleviate some worries about the potentially harmful effects of competing learning algorithms, and can help sellers, platforms and policymakers to decide on whether to adopt or regulate such algorithms.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesize, Retrieve, and Propagate: A Unified Predictive Modeling Framework for Relational Databases</title>
<link>https://arxiv.org/abs/2508.08327</link>
<guid>https://arxiv.org/abs/2508.08327</guid>
<content:encoded><![CDATA[
arXiv:2508.08327v1 Announce Type: cross 
Abstract: Relational databases (RDBs) have become the industry standard for storing massive and heterogeneous data. However, despite the widespread use of RDBs across various fields, the inherent structure of relational databases hinders their ability to benefit from flourishing deep learning methods. Previous research has primarily focused on exploiting the unary dependency among multiple tables in a relational database using the primary key - foreign key relationships, either joining multiple tables into a single table or constructing a graph among them, which leaves the implicit composite relations among different tables and a substantial potential of improvement for predictive modeling unexplored. In this paper, we propose SRP, a unified predictive modeling framework that synthesizes features using the unary dependency, retrieves related information to capture the composite dependency, and propagates messages across a constructed graph to learn adjacent patterns for prediction on relation databases. By introducing a new retrieval mechanism into RDB, SRP is designed to fully capture both the unary and the composite dependencies within a relational database, thereby enhancing the receptive field of tabular data prediction. In addition, we conduct a comprehensive analysis on the components of SRP, offering a nuanced understanding of model behaviors and practical guidelines for future applications. Extensive experiments on five real-world datasets demonstrate the effectiveness of SRP and its potential applicability in industrial scenarios. The code is released at https://github.com/NingLi670/SRP.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Fairness amid Social Determinants: Reflection, Characterization, and Approach</title>
<link>https://arxiv.org/abs/2508.08337</link>
<guid>https://arxiv.org/abs/2508.08337</guid>
<content:encoded><![CDATA[
arXiv:2508.08337v1 Announce Type: cross 
Abstract: Social determinants are variables that, while not directly pertaining to any specific individual, capture key aspects of contexts and environments that have direct causal influences on certain attributes of an individual. Previous algorithmic fairness literature has primarily focused on sensitive attributes, often overlooking the role of social determinants. Our paper addresses this gap by introducing formal and quantitative rigor into a space that has been shaped largely by qualitative proposals regarding the use of social determinants. To demonstrate theoretical perspectives and practical applicability, we examine a concrete setting of college admissions, using region as a proxy for social determinants. Our approach leverages a region-based analysis with Gamma distribution parameterization to model how social determinants impact individual outcomes. Despite its simplicity, our method quantitatively recovers findings that resonate with nuanced insights in previous qualitative debates, that are often missed by existing algorithmic fairness approaches. Our findings suggest that mitigation strategies centering solely around sensitive attributes may introduce new structural injustice when addressing existing discrimination. Considering both sensitive attributes and social determinants facilitates a more comprehensive explication of benefits and burdens experienced by individuals from diverse demographic backgrounds as well as contextual environments, which is essential for understanding and achieving fairness effectively and transparently.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImageDDI: Image-enhanced Molecular Motif Sequence Representation for Drug-Drug Interaction Prediction</title>
<link>https://arxiv.org/abs/2508.08338</link>
<guid>https://arxiv.org/abs/2508.08338</guid>
<content:encoded><![CDATA[
arXiv:2508.08338v1 Announce Type: cross 
Abstract: To mitigate the potential adverse health effects of simultaneous multi-drug use, including unexpected side effects and interactions, accurately identifying and predicting drug-drug interactions (DDIs) is considered a crucial task in the field of deep learning. Although existing methods have demonstrated promising performance, they suffer from the bottleneck of limited functional motif-based representation learning, as DDIs are fundamentally caused by motif interactions rather than the overall drug structures. In this paper, we propose an Image-enhanced molecular motif sequence representation framework for \textbf{DDI} prediction, called ImageDDI, which represents a pair of drugs from both global and local structures. Specifically, ImageDDI tokenizes molecules into functional motifs. To effectively represent a drug pair, their motifs are combined into a single sequence and embedded using a transformer-based encoder, starting from the local structure representation. By leveraging the associations between drug pairs, ImageDDI further enhances the spatial representation of molecules using global molecular image information (e.g. texture, shadow, color, and planar spatial relationships). To integrate molecular visual information into functional motif sequence, ImageDDI employs Adaptive Feature Fusion, enhancing the generalization of ImageDDI by dynamically adapting the fusion process of feature representations. Experimental results on widely used datasets demonstrate that ImageDDI outperforms state-of-the-art methods. Moreover, extensive experiments show that ImageDDI achieved competitive performance in both 2D and 3D image-enhanced scenarios compared to other models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The DNA of nuclear models: How AI predicts nuclear masses</title>
<link>https://arxiv.org/abs/2508.08370</link>
<guid>https://arxiv.org/abs/2508.08370</guid>
<content:encoded><![CDATA[
arXiv:2508.08370v1 Announce Type: cross 
Abstract: Obtaining high-precision predictions of nuclear masses, or equivalently nuclear binding energies, $E_b$, remains an important goal in nuclear-physics research. Recently, many AI-based tools have shown promising results on this task, some achieving precision that surpasses the best physics models. However, the utility of these AI models remains in question given that predictions are only useful where measurements do not exist, which inherently requires extrapolation away from the training (and testing) samples. Since AI models are largely black boxes, the reliability of such an extrapolation is difficult to assess. We present an AI model that not only achieves cutting-edge precision for $E_b$, but does so in an interpretable manner. For example, we find (and explain why) that the most important dimensions of its internal representation form a double helix, where the analog of the hydrogen bonds in DNA here link the number of protons and neutrons found in the most stable nucleus of each isotopic chain. Furthermore, we show that the AI prediction of $E_b$ can be factorized and ordered hierarchically, with the most important terms corresponding to well-known symbolic models (such as the famous liquid drop). Remarkably, the improvement of the AI model over symbolic ones can almost entirely be attributed to an observation made by Jaffe in 1969. The end result is a fully interpretable data-driven model of nuclear masses.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Query-Relevant Document Summaries via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.08404</link>
<guid>https://arxiv.org/abs/2508.08404</guid>
<content:encoded><![CDATA[
arXiv:2508.08404v1 Announce Type: cross 
Abstract: E-commerce search engines often rely solely on product titles as input for ranking models with latency constraints. However, this approach can result in suboptimal relevance predictions, as product titles often lack sufficient detail to capture query intent. While product descriptions provide richer information, their verbosity and length make them unsuitable for real-time ranking, particularly for computationally expensive architectures like cross-encoder ranking models. To address this challenge, we propose ReLSum, a novel reinforcement learning framework designed to generate concise, query-relevant summaries of product descriptions optimized for search relevance. ReLSum leverages relevance scores as rewards to align the objectives of summarization and ranking, effectively overcoming limitations of prior methods, such as misaligned learning targets. The framework employs a trainable large language model (LLM) to produce summaries, which are then used as input for a cross-encoder ranking model. Experimental results demonstrate significant improvements in offline metrics, including recall and NDCG, as well as online user engagement metrics. ReLSum provides a scalable and efficient solution for enhancing search relevance in large-scale e-commerce systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Tangent Knowledge Distillation for Optical Convolutional Networks</title>
<link>https://arxiv.org/abs/2508.08421</link>
<guid>https://arxiv.org/abs/2508.08421</guid>
<content:encoded><![CDATA[
arXiv:2508.08421v1 Announce Type: cross 
Abstract: Hybrid Optical Neural Networks (ONNs, typically consisting of an optical frontend and a digital backend) offer an energy-efficient alternative to fully digital deep networks for real-time, power-constrained systems. However, their adoption is limited by two main challenges: the accuracy gap compared to large-scale networks during training, and discrepancies between simulated and fabricated systems that further degrade accuracy. While previous work has proposed end-to-end optimizations for specific datasets (e.g., MNIST) and optical systems, these approaches typically lack generalization across tasks and hardware designs. To address these limitations, we propose a task-agnostic and hardware-agnostic pipeline that supports image classification and segmentation across diverse optical systems. To assist optical system design before training, we estimate achievable model accuracy based on user-specified constraints such as physical size and the dataset. For training, we introduce Neural Tangent Knowledge Distillation (NTKD), which aligns optical models with electronic teacher networks, thereby narrowing the accuracy gap. After fabrication, NTKD also guides fine-tuning of the digital backend to compensate for implementation errors. Experiments on multiple datasets (e.g., MNIST, CIFAR, Carvana Masking) and hardware configurations show that our pipeline consistently improves ONN performance and enables practical deployment in both pre-fabrication simulations and physical implementations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference</title>
<link>https://arxiv.org/abs/2508.08438</link>
<guid>https://arxiv.org/abs/2508.08438</guid>
<content:encoded><![CDATA[
arXiv:2508.08438v1 Announce Type: cross 
Abstract: Global KV-cache sharing has emerged as a key optimization for accelerating large language model (LLM) inference. However, it exposes a new class of timing side-channel attacks, enabling adversaries to infer sensitive user inputs via shared cache entries. Existing defenses, such as per-user isolation, eliminate leakage but degrade performance by up to 38.9% in time-to-first-token (TTFT), making them impractical for high-throughput deployment. To address this gap, we introduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware KV-cache management framework that selectively shares non-sensitive entries while confining sensitive content to private caches. SafeKV comprises three components: (i) a hybrid, multi-tier detection pipeline that integrates rule-based pattern matching, a general-purpose privacy detector, and context-aware validation; (ii) a unified radix-tree index that manages public and private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and (iii) entropy-based access monitoring to detect and mitigate residual information leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of timing-based side-channel attacks. Compared to per-user isolation method, SafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across diverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from 50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with high cache reuse efficiency, SafeKV reclaims the performance advantages of global sharing while providing robust runtime privacy guarantees for LLM inference.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Can Understand Spectra: A Multimodal Model for Molecular Structure Elucidation</title>
<link>https://arxiv.org/abs/2508.08441</link>
<guid>https://arxiv.org/abs/2508.08441</guid>
<content:encoded><![CDATA[
arXiv:2508.08441v1 Announce Type: cross 
Abstract: Structure elucidation is a fundamental technique for understanding the microscopic composition of matter and is widely applied across various disciplines in the natural sciences and engineering. However, existing methods often rely heavily on prior databases or known structural information, making it difficult to resolve unknown structures. In addition, complex structures typically require the joint analysis of multiple spectroscopic modalities. This process heavily depends on expert domain knowledge and is often accompanied by high costs in terms of both time and instrumentation. To address these challenges, we propose SpectraLLM, the first large language model designed to support multi-modal spectroscopic joint reasoning. SpectraLLM is capable of processing either single or multiple spectroscopic inputs and performing end-to-end structure elucidation. By integrating continuous and discrete spectroscopic modalities into a shared semantic space, SpectraLLM learns to uncover substructural patterns that are consistent and complementary across spectra, enabling precise molecular structure elucidation. We pretrain and fine-tune SpectraLLM in the domain of small molecules, and evaluate it on six standardized, publicly available chemical datasets. The model achieves state-of-the-art performance, significantly outperforming existing approaches trained on single modalities. Notably, SpectraLLM demonstrates strong robustness and generalization even for single-spectrum inference, while its multi-modal reasoning capability further improves the accuracy of structural prediction.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Federated Learning for Throughput Prediction in 5G Live Streaming Applications</title>
<link>https://arxiv.org/abs/2508.08479</link>
<guid>https://arxiv.org/abs/2508.08479</guid>
<content:encoded><![CDATA[
arXiv:2508.08479v1 Announce Type: cross 
Abstract: Accurate and adaptive network throughput prediction is essential for latency-sensitive and bandwidth-intensive applications in 5G and emerging 6G networks. However, most existing methods rely on centralized training with uniformly collected data, limiting their applicability in heterogeneous mobile environments with non-IID data distributions. This paper presents the first comprehensive benchmarking of federated learning (FL) strategies for throughput prediction in realistic 5G edge scenarios. We evaluate three aggregation algorithms - FedAvg, FedProx, and FedBN - across four time-series architectures: LSTM, CNN, CNN+LSTM, and Transformer, using five diverse real-world datasets. We systematically analyze the effects of client heterogeneity, cohort size, and history window length on prediction performance. Our results reveal key trade-offs among model complexities, convergence rates, and generalization. It is found that FedBN consistently delivers robust performance under non-IID conditions. On the other hand, LSTM and Transformer models outperform CNN-based baselines by up to 80% in R2 scores. Moreover, although Transformers converge in half the rounds of LSTM, they require longer history windows to achieve a high R2, indicating higher context dependence. LSTM is, therefore, found to achieve a favorable balance between accuracy, rounds, and temporal footprint. To validate the end-to-end applicability of the framework, we have integrated our FL-based predictors into a live adaptive streaming pipeline. It is seen that FedBN-based LSTM and Transformer models improve mean QoE scores by 11.7% and 11.4%, respectively, over FedAvg, while also reducing the variance. These findings offer actionable insights for building scalable, privacy-preserving, and edge-aware throughput prediction systems in next-generation wireless networks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Momentum Point-Perplexity Mechanics in Large Language Models</title>
<link>https://arxiv.org/abs/2508.08492</link>
<guid>https://arxiv.org/abs/2508.08492</guid>
<content:encoded><![CDATA[
arXiv:2508.08492v1 Announce Type: cross 
Abstract: We take a physics-based approach to studying how the internal hidden states of large language models change from token to token during inference. Across 20 open-source transformer models (135M-3B parameters), we find that a quantity combining the rate of change in hidden states and the model's next-token certainty, analogous to energy in physics, remains nearly constant. Random-weight models conserve this "energy" more tightly than pre-trained ones, while training shifts models into a faster, more decisive regime with greater variability. Using this "log-Lagrangian" view, we derive a control method called Jacobian steering, which perturbs hidden states in the minimal way needed to favor a target token. This approach maintained near-constant energy in two tested models and produced continuations rated higher in semantic quality than the models' natural outputs. Viewing transformers through this mechanics lens offers a principled basis for interpretability, anomaly detection, and low-risk steering. This could help make powerful models more predictable and aligned with human intent.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When the Domain Expert Has No Time and the LLM Developer Has No Clinical Expertise: Real-World Lessons from LLM Co-Design in a Safety-Net Hospital</title>
<link>https://arxiv.org/abs/2508.08504</link>
<guid>https://arxiv.org/abs/2508.08504</guid>
<content:encoded><![CDATA[
arXiv:2508.08504v1 Announce Type: cross 
Abstract: Large language models (LLMs) have the potential to address social and behavioral determinants of health by transforming labor intensive workflows in resource-constrained settings. Creating LLM-based applications that serve the needs of underserved communities requires a deep understanding of their local context, but it is often the case that neither LLMs nor their developers possess this local expertise, and the experts in these communities often face severe time/resource constraints. This creates a disconnect: how can one engage in meaningful co-design of an LLM-based application for an under-resourced community when the communication channel between the LLM developer and domain expert is constrained? We explored this question through a real-world case study, in which our data science team sought to partner with social workers at a safety net hospital to build an LLM application that summarizes patients' social needs. Whereas prior works focus on the challenge of prompt tuning, we found that the most critical challenge in this setting is the careful and precise specification of \what information to surface to providers so that the LLM application is accurate, comprehensive, and verifiable. Here we present a novel co-design framework for settings with limited access to domain experts, in which the summary generation task is first decomposed into individually-optimizable attributes and then each attribute is efficiently refined and validated through a multi-tier cascading approach.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCAL Tokenwise Compression</title>
<link>https://arxiv.org/abs/2508.08514</link>
<guid>https://arxiv.org/abs/2508.08514</guid>
<content:encoded><![CDATA[
arXiv:2508.08514v1 Announce Type: cross 
Abstract: This paper introduces DeCAL, a new method for tokenwise compression. DeCAL uses an encoder-decoder language model pretrained with denoising to learn to produce high-quality, general-purpose compressed representations by the encoder. DeCAL applies small modifications to the encoder, with the emphasis on maximizing compression quality, even at the expense of compute. We show that DeCAL at 2x compression can match uncompressed on many downstream tasks, with usually only minor dropoff in metrics up to 8x compression, among question-answering, summarization, and multi-vector retrieval tasks. DeCAL offers significant savings where pre-computed dense representations can be utilized, and we believe the approach can be further developed to be more broadly applicable.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection-based multifidelity linear regression for data-scarce applications</title>
<link>https://arxiv.org/abs/2508.08517</link>
<guid>https://arxiv.org/abs/2508.08517</guid>
<content:encoded><![CDATA[
arXiv:2508.08517v1 Announce Type: cross 
Abstract: Surrogate modeling for systems with high-dimensional quantities of interest remains challenging, particularly when training data are costly to acquire. This work develops multifidelity methods for multiple-input multiple-output linear regression targeting data-limited applications with high-dimensional outputs. Multifidelity methods integrate many inexpensive low-fidelity model evaluations with limited, costly high-fidelity evaluations. We introduce two projection-based multifidelity linear regression approaches that leverage principal component basis vectors for dimensionality reduction and combine multifidelity data through: (i) a direct data augmentation using low-fidelity data, and (ii) a data augmentation incorporating explicit linear corrections between low-fidelity and high-fidelity data. The data augmentation approaches combine high-fidelity and low-fidelity data into a unified training set and train the linear regression model through weighted least squares with fidelity-specific weights. Various weighting schemes and their impact on regression accuracy are explored. The proposed multifidelity linear regression methods are demonstrated on approximating the surface pressure field of a hypersonic vehicle in flight. In a low-data regime of no more than ten high-fidelity samples, multifidelity linear regression achieves approximately 3% - 12% improvement in median accuracy compared to single-fidelity methods with comparable computational cost.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Target Backdoor Attacks Against Speaker Recognition</title>
<link>https://arxiv.org/abs/2508.08559</link>
<guid>https://arxiv.org/abs/2508.08559</guid>
<content:encoded><![CDATA[
arXiv:2508.08559v1 Announce Type: cross 
Abstract: In this work, we propose a multi-target backdoor attack against speaker identification using position-independent clicking sounds as triggers. Unlike previous single-target approaches, our method targets up to 50 speakers simultaneously, achieving success rates of up to 95.04%. To simulate more realistic attack conditions, we vary the signal-to-noise ratio between speech and trigger, demonstrating a trade-off between stealth and effectiveness. We further extend the attack to the speaker verification task by selecting the most similar training speaker - based on cosine similarity - as the target. The attack is most effective when target and enrolled speaker pairs are highly similar, reaching success rates of up to 90% in such cases.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superclass-Guided Representation Disentanglement for Spurious Correlation Mitigation</title>
<link>https://arxiv.org/abs/2508.08570</link>
<guid>https://arxiv.org/abs/2508.08570</guid>
<content:encoded><![CDATA[
arXiv:2508.08570v1 Announce Type: cross 
Abstract: To enhance group robustness to spurious correlations, prior work often relies on auxiliary annotations for groups or spurious features and assumes identical sets of groups across source and target domains. These two requirements are both unnatural and impractical in real-world settings. To overcome these limitations, we propose a method that leverages the semantic structure inherent in class labels--specifically, superclass information--to naturally reduce reliance on spurious features. Our model employs gradient-based attention guided by a pre-trained vision-language model to disentangle superclass-relevant and irrelevant features. Then, by promoting the use of all superclass-relevant features for prediction, our approach achieves robustness to more complex spurious correlations without the need to annotate any source samples. Experiments across diverse datasets demonstrate that our method significantly outperforms baselines in domain generalization tasks, with clear improvements in both quantitative metrics and qualitative visualizations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable Model-agnostic Vision-Language Model Adaptation for Efficient Weak-to-Strong Generalization</title>
<link>https://arxiv.org/abs/2508.08604</link>
<guid>https://arxiv.org/abs/2508.08604</guid>
<content:encoded><![CDATA[
arXiv:2508.08604v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have been widely used in various visual recognition tasks due to their remarkable generalization capabilities. As these models grow in size and complexity, fine-tuning becomes costly, emphasizing the need to reuse adaptation knowledge from 'weaker' models to efficiently enhance 'stronger' ones. However, existing adaptation transfer methods exhibit limited transferability across models due to their model-specific design and high computational demands. To tackle this, we propose Transferable Model-agnostic adapter (TransMiter), a light-weight adapter that improves vision-language models 'without backpropagation'. TransMiter captures the knowledge gap between pre-trained and fine-tuned VLMs, in an 'unsupervised' manner. Once trained, this knowledge can be seamlessly transferred across different models without the need for backpropagation. Moreover, TransMiter consists of only a few layers, inducing a negligible additional inference cost. Notably, supplementing the process with a few labeled data further yields additional performance gain, often surpassing a fine-tuned stronger model, with a marginal training cost. Experimental results and analyses demonstrate that TransMiter effectively and efficiently transfers adaptation knowledge while preserving generalization abilities across VLMs of different sizes and architectures in visual recognition tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Artistic Style and Color Transfer Using Deep Learning</title>
<link>https://arxiv.org/abs/2508.08608</link>
<guid>https://arxiv.org/abs/2508.08608</guid>
<content:encoded><![CDATA[
arXiv:2508.08608v1 Announce Type: cross 
Abstract: Neural artistic style transfers and blends the content and style representation of one image with the style of another. This enables artists to create unique innovative visuals and enhances artistic expression in various fields including art, design, and film. Color transfer algorithms are an important in digital image processing by adjusting the color information in a target image based on the colors in the source image. Color transfer enhances images and videos in film and photography, and can aid in image correction. We introduce a methodology that combines neural artistic style with color transfer. The method uses the Kullback-Leibler (KL) divergence to quantitatively evaluate color and luminance histogram matching algorithms including Reinhard global color transfer, iteration distribution transfer (IDT), IDT with regrain, Cholesky, and PCA between the original and neural artistic style transferred image using deep learning. We estimate the color channel kernel densities. Various experiments are performed to evaluate the KL of these algorithms and their color histograms for style to content transfer.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Learning as Nonparametric Conditional Probability Estimation: Risk Bounds and Optimality</title>
<link>https://arxiv.org/abs/2508.08673</link>
<guid>https://arxiv.org/abs/2508.08673</guid>
<content:encoded><![CDATA[
arXiv:2508.08673v1 Announce Type: cross 
Abstract: This paper investigates the expected excess risk of In-Context Learning (ICL) for multiclass classification. We model each task as a sequence of labeled prompt samples and a query input, where a pre-trained model estimates the conditional class probabilities of the query. The expected excess risk is defined as the average truncated Kullback-Leibler (KL) divergence between the predicted and ground-truth conditional class distributions, averaged over a specified family of tasks. We establish a new oracle inequality for the expected excess risk based on KL divergence in multiclass classification. This allows us to derive tight upper and lower bounds for the expected excess risk in transformer-based models, demonstrating that the ICL estimator achieves the minimax optimal rate - up to a logarithmic factor - for conditional probability estimation. From a technical standpoint, our results introduce a novel method for controlling generalization error using the uniform empirical covering entropy of the log-likelihood function class. Furthermore, we show that multilayer perceptrons (MLPs) can also perform ICL and achieve this optimal rate under specific assumptions, suggesting that transformers may not be the exclusive architecture capable of effective ICL.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffVolume: Diffusion Models for Volume Generation in Limit Order Books</title>
<link>https://arxiv.org/abs/2508.08698</link>
<guid>https://arxiv.org/abs/2508.08698</guid>
<content:encoded><![CDATA[
arXiv:2508.08698v1 Announce Type: cross 
Abstract: Modeling limit order books (LOBs) dynamics is a fundamental problem in market microstructure research. In particular, generating high-dimensional volume snapshots with strong temporal and liquidity-dependent patterns remains a challenging task, despite recent work exploring the application of Generative Adversarial Networks to LOBs. In this work, we propose a conditional \textbf{Diff}usion model for the generation of future LOB \textbf{Volume} snapshots (\textbf{DiffVolume}). We evaluate our model across three axes: (1) \textit{Realism}, where we show that DiffVolume, conditioned on past volume history and time of day, better reproduces statistical properties such as marginal distribution, spatial correlation, and autocorrelation decay; (2) \textit{Counterfactual generation}, allowing for controllable generation under hypothetical liquidity scenarios by additionally conditioning on a target future liquidity profile; and (3) \textit{Downstream prediction}, where we show that the synthetic counterfactual data from our model improves the performance of future liquidity forecasting models. Together, these results suggest that DiffVolume provides a powerful and flexible framework for realistic and controllable LOB volume generation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeFix: Targeted Model Repair via Controlled Image Generation</title>
<link>https://arxiv.org/abs/2508.08701</link>
<guid>https://arxiv.org/abs/2508.08701</guid>
<content:encoded><![CDATA[
arXiv:2508.08701v1 Announce Type: cross 
Abstract: Deep learning models for visual recognition often exhibit systematic errors due to underrepresented semantic subpopulations. Although existing debugging frameworks can pinpoint these failures by identifying key failure attributes, repairing the model effectively remains difficult. Current solutions often rely on manually designed prompts to generate synthetic training images -- an approach prone to distribution shift and semantic errors. To overcome these challenges, we introduce a model repair module that builds on an interpretable failure attribution pipeline. Our approach uses a conditional text-to-image model to generate semantically faithful and targeted images for failure cases. To preserve the quality and relevance of the generated samples, we further employ a large vision-language model (LVLM) to filter the outputs, enforcing alignment with the original data distribution and maintaining semantic consistency. By retraining vision models with this rare-case-augmented synthetic dataset, we significantly reduce errors associated with rare cases. Our experiments demonstrate that this targeted repair strategy improves model robustness without introducing new bugs. Code is available at https://github.com/oxu2/SafeFix
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRADLE: Conversational RTL Design Space Exploration with LLM-based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2508.08709</link>
<guid>https://arxiv.org/abs/2508.08709</guid>
<content:encoded><![CDATA[
arXiv:2508.08709v1 Announce Type: cross 
Abstract: This paper presents CRADLE, a conversational framework for design space exploration of RTL designs using LLM-based multi-agent systems. Unlike existing rigid approaches, CRADLE enables user-guided flows with internal self-verification, correction, and optimization. We demonstrate the framework with a generator-critic agent system targeting FPGA resource minimization using state-of-the-art LLMs. Experimental results on the RTLLM benchmark show that CRADLE achieves significant reductions in resource usage with averages of 48% and 40% in LUTs and FFs across all benchmark designs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Variable Importance with Statistical Control for Medical Data-Based Prediction</title>
<link>https://arxiv.org/abs/2508.08724</link>
<guid>https://arxiv.org/abs/2508.08724</guid>
<content:encoded><![CDATA[
arXiv:2508.08724v1 Announce Type: cross 
Abstract: Recent advances in machine learning have greatly expanded the repertoire of predictive methods for medical imaging. However, the interpretability of complex models remains a challenge, which limits their utility in medical applications. Recently, model-agnostic methods have been proposed to measure conditional variable importance and accommodate complex non-linear models. However, they often lack power when dealing with highly correlated data, a common problem in medical imaging. We introduce Hierarchical-CPI, a model-agnostic variable importance measure that frames the inference problem as the discovery of groups of variables that are jointly predictive of the outcome. By exploring subgroups along a hierarchical tree, it remains computationally tractable, yet also enjoys explicit family-wise error rate control. Moreover, we address the issue of vanishing conditional importance under high correlation with a tree-based importance allocation mechanism. We benchmarked Hierarchical-CPI against state-of-the-art variable importance methods. Its effectiveness is demonstrated in two neuroimaging datasets: classifying dementia diagnoses from MRI data (ADNI dataset) and analyzing the Berger effect on EEG data (TDBRAIN dataset), identifying biologically plausible variables.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity Analysis to Unobserved Confounding with Copula-based Normalizing Flows</title>
<link>https://arxiv.org/abs/2508.08752</link>
<guid>https://arxiv.org/abs/2508.08752</guid>
<content:encoded><![CDATA[
arXiv:2508.08752v1 Announce Type: cross 
Abstract: We propose a novel method for sensitivity analysis to unobserved confounding in causal inference. The method builds on a copula-based causal graphical normalizing flow that we term $\rho$-GNF, where $\rho \in [-1,+1]$ is the sensitivity parameter. The parameter represents the non-causal association between exposure and outcome due to unobserved confounding, which is modeled as a Gaussian copula. In other words, the $\rho$-GNF enables scholars to estimate the average causal effect (ACE) as a function of $\rho$, accounting for various confounding strengths. The output of the $\rho$-GNF is what we term the $\rho_{curve}$, which provides the bounds for the ACE given an interval of assumed $\rho$ values. The $\rho_{curve}$ also enables scholars to identify the confounding strength required to nullify the ACE. We also propose a Bayesian version of our sensitivity analysis method. Assuming a prior over the sensitivity parameter $\rho$ enables us to derive the posterior distribution over the ACE, which enables us to derive credible intervals. Finally, leveraging on experiments from simulated and real-world data, we show the benefits of our sensitivity analysis method.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bio-Inspired Artificial Neural Networks based on Predictive Coding</title>
<link>https://arxiv.org/abs/2508.08762</link>
<guid>https://arxiv.org/abs/2508.08762</guid>
<content:encoded><![CDATA[
arXiv:2508.08762v1 Announce Type: cross 
Abstract: Backpropagation (BP) of errors is the backbone training algorithm for artificial neural networks (ANNs). It updates network weights through gradient descent to minimize a loss function representing the mismatch between predictions and desired outputs. BP uses the chain rule to propagate the loss gradient backward through the network hierarchy, allowing efficient weight updates. However, this process requires weight updates at every layer to rely on a global error signal generated at the network's output.
  In contrast, the Hebbian model of synaptic plasticity states that weight updates are local, depending only on the activity of pre- and post-synaptic neurons. This suggests biological brains likely do not implement BP directly. Recently, Predictive Coding (PC) has gained interest as a biologically plausible alternative that updates weights using only local information. Originating from 1950s work on signal compression, PC was later proposed as a model of the visual cortex and formalized under the free energy principle, linking it to Bayesian inference and dynamical systems. PC weight updates rely solely on local information and provide theoretical advantages such as automatic scaling of gradients based on uncertainty.
  This lecture notes column offers a novel, tutorial-style introduction to PC, focusing on its formulation, derivation, and connections to well-known optimization and signal processing algorithms such as BP and the Kalman Filter (KF). It aims to support existing literature by guiding readers from the mathematical foundations of PC to practical implementation, including Python examples using PyTorch.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2508.08777</link>
<guid>https://arxiv.org/abs/2508.08777</guid>
<content:encoded><![CDATA[
arXiv:2508.08777v1 Announce Type: cross 
Abstract: Evaluating personalized recommendations remains a central challenge, especially in long-form audio domains like podcasts, where traditional offline metrics suffer from exposure bias and online methods such as A/B testing are costly and operationally constrained. In this paper, we propose a novel framework that leverages Large Language Models (LLMs) as offline judges to assess the quality of podcast recommendations in a scalable and interpretable manner. Our two-stage profile-aware approach first constructs natural-language user profiles distilled from 90 days of listening history. These profiles summarize both topical interests and behavioral patterns, serving as compact, interpretable representations of user preferences. Rather than prompting the LLM with raw data, we use these profiles to provide high-level, semantically rich context-enabling the LLM to reason more effectively about alignment between a user's interests and recommended episodes. This reduces input complexity and improves interpretability. The LLM is then prompted to deliver fine-grained pointwise and pairwise judgments based on the profile-episode match. In a controlled study with 47 participants, our profile-aware judge matched human judgments with high fidelity and outperformed or matched a variant using raw listening histories. The framework enables efficient, profile-aware evaluation for iterative testing and model selection in recommender systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subsampling Factorization Machine Annealing</title>
<link>https://arxiv.org/abs/2508.08778</link>
<guid>https://arxiv.org/abs/2508.08778</guid>
<content:encoded><![CDATA[
arXiv:2508.08778v1 Announce Type: cross 
Abstract: Quantum computing and machine learning are state-of-the-art technologies which have been investigated intensively in both academia and industry. The hybrid technology of these two ingredients is expected to be a powerful tool to solve complex problems in many branches of science and engineering such as combinatorial optimization problems and accelerate the creation of next-generation technologies. In this work, we develop an algorithm to solve a black-box optimization problem by improving Factorization Machine Annealing (FMA) such that the training of a machine learning model called Factorization Machine is performed not by a full dataset but by a subdataset which is sampled from a full dataset: Subsampling Factorization Machine Annealing (SFMA). According to such a probabilistic training process, the performance of FMA on exploring a solution space gets enhanced. As a result, SFMA exhibits balanced performance of exploration and exploitation which we call exploitation-exploration functionality. We conduct numerical benchmarking tests to compare the performance of SFMA with that of FMA. Consequently, SFMA certainly exhibits the exploration-exploitation functionality and outperforms FMA in speed and accuracy. In addition, the performance of SFMA can be further improved by sequentially using two subsampling datasets with different sizes such that the size of the latter dataset is substantially smaller than the former. Such a substantial reduction not only enhances the exploration performance of SFMA but also enables us to run it with correspondingly low computational cost even for a large-scale problem. These results indicate the effectiveness of SFMA in a certain class of black-box optimization problems of significant size: the potential scalability of SFMA in solving large-scale problems with correspondingly low computational cost.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image selective encryption analysis using mutual information in CNN based embedding space</title>
<link>https://arxiv.org/abs/2508.08832</link>
<guid>https://arxiv.org/abs/2508.08832</guid>
<content:encoded><![CDATA[
arXiv:2508.08832v1 Announce Type: cross 
Abstract: As digital data transmission continues to scale, concerns about privacy grow increasingly urgent - yet privacy remains a socially constructed and ambiguously defined concept, lacking a universally accepted quantitative measure. This work examines information leakage in image data, a domain where information-theoretic guarantees are still underexplored. At the intersection of deep learning, information theory, and cryptography, we investigate the use of mutual information (MI) estimators - in particular, the empirical estimator and the MINE framework - to detect leakage from selectively encrypted images. Motivated by the intuition that a robust estimator would require a probabilistic frameworks that can capture spatial dependencies and residual structures, even within encrypted representations - our work represent a promising direction for image information leakage estimation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems</title>
<link>https://arxiv.org/abs/2508.08833</link>
<guid>https://arxiv.org/abs/2508.08833</guid>
<content:encoded><![CDATA[
arXiv:2508.08833v1 Announce Type: cross 
Abstract: In this paper, we introduce a systematic framework beyond conventional method to assess LLMs' mathematical-reasoning robustness by stress-testing them on advanced math problems that are mathematically equivalent but with linguistic and parametric variation. These transformations allow us to measure the sensitivity of LLMs to non-mathematical perturbations, thereby enabling a more accurate evaluation of their mathematical reasoning capabilities. Using this new evaluation methodology, we created PutnamGAP, a new benchmark dataset with multiple mathematically-equivalent variations of competition-level math problems. With the new dataset, we evaluate multiple families of representative LLMs and examine their robustness. Across 18 commercial and open-source models we observe sharp performance degradation on the variants. OpenAI's flagship reasoning model, O3, scores 49 % on the originals but drops by 4 percentage points on surface variants, and by 10.5 percentage points on core-step-based variants, while smaller models fare far worse. Overall, the results show that the proposed new evaluation methodology is effective for deepening our understanding of the robustness of LLMs and generating new insights for further improving their mathematical reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiasGym: Fantastic Biases and How to Find (and Remove) Them</title>
<link>https://arxiv.org/abs/2508.08855</link>
<guid>https://arxiv.org/abs/2508.08855</guid>
<content:encoded><![CDATA[
arXiv:2508.08855v1 Announce Type: cross 
Abstract: Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. Biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasing particularly challenging. To address this, we introduce BiasGym, a simple, cost-effective, and generalizable framework for reliably injecting, analyzing, and mitigating conceptual associations within LLMs. BiasGym consists of two components: BiasInject, which injects specific biases into the model via token-based fine-tuning while keeping the model frozen, and BiasScope, which leverages these injected signals to identify and steer the components responsible for biased behavior. Our method enables consistent bias elicitation for mechanistic analysis, supports targeted debiasing without degrading performance on downstream tasks, and generalizes to biases unseen during training. We demonstrate the effectiveness of BiasGym in reducing real-world stereotypes (e.g., people from a country being `reckless drivers') and in probing fictional associations (e.g., people from a country having `blue skin'), showing its utility for both safety interventions and interpretability research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sound Signal Synthesis with Auxiliary Classifier GAN, COVID-19 cough as an example</title>
<link>https://arxiv.org/abs/2508.08892</link>
<guid>https://arxiv.org/abs/2508.08892</guid>
<content:encoded><![CDATA[
arXiv:2508.08892v1 Announce Type: cross 
Abstract: One of the fastest-growing domains in AI is healthcare. Given its importance, it has been the interest of many researchers to deploy ML models into the ever-demanding healthcare domain to aid doctors and increase accessibility. Delivering reliable models, however, demands a sizable amount of data, and the recent COVID-19 pandemic served as a reminder of the rampant and scary nature of healthcare that makes training models difficult. To alleviate such scarcity, many published works attempted to synthesize radiological cough data to train better COVID-19 detection models on the respective radiological data. To accommodate the time sensitivity expected during a pandemic, this work focuses on detecting COVID-19 through coughs using synthetic data to improve the accuracy of the classifier. The work begins by training a CNN on a balanced subset of the Coughvid dataset, establishing a baseline classification test accuracy of 72%. The paper demonstrates how an Auxiliary Classification GAN (ACGAN) may be trained to conditionally generate novel synthetic Mel Spectrograms of both healthy and COVID-19 coughs. These coughs are used to augment the training dataset of the CNN classifier, allowing it to reach a new test accuracy of 75%. The work highlights the expected messiness and inconsistency in training and offers insights into detecting and handling such shortcomings.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic and standardized surgical reporting for central nervous system tumors</title>
<link>https://arxiv.org/abs/2508.08916</link>
<guid>https://arxiv.org/abs/2508.08916</guid>
<content:encoded><![CDATA[
arXiv:2508.08916v1 Announce Type: cross 
Abstract: Magnetic resonance (MR) imaging is essential for evaluating central nervous system (CNS) tumors, guiding surgical planning, treatment decisions, and assessing postoperative outcomes and complication risks. While recent work has advanced automated tumor segmentation and report generation, most efforts have focused on preoperative data, with limited attention to postoperative imaging analysis. This study introduces a comprehensive pipeline for standardized postsurtical reporting in CNS tumors. Using the Attention U-Net architecture, segmentation models were trained for the preoperative (non-enhancing) tumor core, postoperative contrast-enhancing residual tumor, and resection cavity. Additionally, MR sequence classification and tumor type identification for contrast-enhancing lesions were explored using the DenseNet architecture. The models were integrated into a reporting pipeline, following the RANO 2.0 guidelines. Training was conducted on multicentric datasets comprising 2000 to 7000 patients, using a 5-fold cross-validation. Evaluation included patient-, voxel-, and object-wise metrics, with benchmarking against the latest BraTS challenge results. The segmentation models achieved average voxel-wise Dice scores of 87%, 66%, 70%, and 77% for the tumor core, non-enhancing tumor core, contrast-enhancing residual tumor, and resection cavity, respectively. Classification models reached 99.5% balanced accuracy in MR sequence classification and 80% in tumor type classification. The pipeline presented in this study enables robust, automated segmentation, MR sequence classification, and standardized report generation aligned with RANO 2.0 guidelines, enhancing postoperative evaluation and clinical decision-making. The proposed models and methods were integrated into Raidionics, open-source software platform for CNS tumor analysis, now including a dedicated module for postsurgical analysis.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated Volumetric Compression without Hierarchies: A Fourier Feature Based Implicit Neural Representation Approach</title>
<link>https://arxiv.org/abs/2508.08937</link>
<guid>https://arxiv.org/abs/2508.08937</guid>
<content:encoded><![CDATA[
arXiv:2508.08937v1 Announce Type: cross 
Abstract: Volumetric data compression is critical in fields like medical imaging, scientific simulation, and entertainment. We introduce a structure-free neural compression method combining Fourierfeature encoding with selective voxel sampling, yielding compact volumetric representations and faster convergence. Our dynamic voxel selection uses morphological dilation to prioritize active regions, reducing redundant computation without any hierarchical metadata. In the experiment, sparse training reduced training time by 63.7 % (from 30 to 11 minutes) with only minor quality loss: PSNR dropped 0.59 dB (from 32.60 to 32.01) and SSIM by 0.008 (from 0.948 to 0.940). The resulting neural representation, stored solely as network weights, achieves a compression rate of 14 and eliminates traditional data-loading overhead. This connects coordinate-based neural representation with efficient volumetric compression, offering a scalable, structure-free solution for practical applications.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train Long, Think Short: Curriculum Learning for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2508.08940</link>
<guid>https://arxiv.org/abs/2508.08940</guid>
<content:encoded><![CDATA[
arXiv:2508.08940v1 Announce Type: cross 
Abstract: Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QAMRO: Quality-aware Adaptive Margin Ranking Optimization for Human-aligned Assessment of Audio Generation Systems</title>
<link>https://arxiv.org/abs/2508.08957</link>
<guid>https://arxiv.org/abs/2508.08957</guid>
<content:encoded><![CDATA[
arXiv:2508.08957v1 Announce Type: cross 
Abstract: Evaluating audio generation systems, including text-to-music (TTM), text-to-speech (TTS), and text-to-audio (TTA), remains challenging due to the subjective and multi-dimensional nature of human perception. Existing methods treat mean opinion score (MOS) prediction as a regression problem, but standard regression losses overlook the relativity of perceptual judgments. To address this limitation, we introduce QAMRO, a novel Quality-aware Adaptive Margin Ranking Optimization framework that seamlessly integrates regression objectives from different perspectives, aiming to highlight perceptual differences and prioritize accurate ratings. Our framework leverages pre-trained audio-text models such as CLAP and Audiobox-Aesthetics, and is trained exclusively on the official AudioMOS Challenge 2025 dataset. It demonstrates superior alignment with human evaluations across all dimensions, significantly outperforming robust baseline models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion</title>
<link>https://arxiv.org/abs/2508.08982</link>
<guid>https://arxiv.org/abs/2508.08982</guid>
<content:encoded><![CDATA[
arXiv:2508.08982v1 Announce Type: cross 
Abstract: Exploration is crucial for enabling legged robots to learn agile locomotion behaviors that can overcome diverse obstacles. However, such exploration is inherently challenging, and we often rely on extensive reward engineering, expert demonstrations, or curriculum learning - all of which limit generalizability. In this work, we propose Skill Discovery as Exploration (SDAX), a novel learning framework that significantly reduces human engineering effort. SDAX leverages unsupervised skill discovery to autonomously acquire a diverse repertoire of skills for overcoming obstacles. To dynamically regulate the level of exploration during training, SDAX employs a bi-level optimization process that autonomously adjusts the degree of exploration. We demonstrate that SDAX enables quadrupedal robots to acquire highly agile behaviors including crawling, climbing, leaping, and executing complex maneuvers such as jumping off vertical walls. Finally, we deploy the learned policy on real hardware, validating its successful transfer to the real world.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrospective Sparse Attention for Efficient Long-Context Generation</title>
<link>https://arxiv.org/abs/2508.09001</link>
<guid>https://arxiv.org/abs/2508.09001</guid>
<content:encoded><![CDATA[
arXiv:2508.09001v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value (KV) cache, whose memory footprint grows linearly with sequence length and dominates latency at each decoding step. While recent KV cache compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long decoding. In this paper, we introduce RetroAttention, a novel KV cache update technique that retrospectively revises past attention outputs using newly arrived KV entries from subsequent decoding steps. By maintaining a lightweight output cache, RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) KV compression methods, increasing effective KV exposure by up to 1.6$\times$ and accuracy by up to 21.9\%.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA</title>
<link>https://arxiv.org/abs/2508.09012</link>
<guid>https://arxiv.org/abs/2508.09012</guid>
<content:encoded><![CDATA[
arXiv:2508.09012v1 Announce Type: cross 
Abstract: This paper describes our participation in SemEval 2025 Task 8, focused on Tabular Question Answering. We developed a zero-shot pipeline that leverages an Large Language Model to generate functional code capable of extracting the relevant information from tabular data based on an input question. Our approach consists of a modular pipeline where the main code generator module is supported by additional components that identify the most relevant columns and analyze their data types to improve extraction accuracy. In the event that the generated code fails, an iterative refinement process is triggered, incorporating the error feedback into a new generation prompt to enhance robustness. Our results show that zero-shot code generation is a valid approach for Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of task-specific fine-tuning.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Training-free Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2508.09016</link>
<guid>https://arxiv.org/abs/2508.09016</guid>
<content:encoded><![CDATA[
arXiv:2508.09016v1 Announce Type: cross 
Abstract: The alignment of large language models (LLMs) aims to ensure their outputs adhere to human values, ethical standards, and legal norms. Traditional alignment methods often rely on resource-intensive fine-tuning (FT), which may suffer from knowledge degradation and face challenges in scenarios where the model accessibility or computational resources are constrained. In contrast, training-free (TF) alignment techniques--leveraging in-context learning, decoding-time adjustments, and post-generation corrections--offer a promising alternative by enabling alignment without heavily retraining LLMs, making them adaptable to both open-source and closed-source environments. This paper presents the first systematic review of TF alignment methods, categorizing them by stages of pre-decoding, in-decoding, and post-decoding. For each stage, we provide a detailed examination from the viewpoint of LLMs and multimodal LLMs (MLLMs), highlighting their mechanisms and limitations. Furthermore, we identify key challenges and future directions, paving the way for more inclusive and effective TF alignment techniques. By synthesizing and organizing the rapidly growing body of research, this survey offers a guidance for practitioners and advances the development of safer and more reliable LLMs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attacks and Defenses Against LLM Fingerprinting</title>
<link>https://arxiv.org/abs/2508.09021</link>
<guid>https://arxiv.org/abs/2508.09021</guid>
<content:encoded><![CDATA[
arXiv:2508.09021v1 Announce Type: cross 
Abstract: As large language models are increasingly deployed in sensitive environments, fingerprinting attacks pose significant privacy and security risks. We present a study of LLM fingerprinting from both offensive and defensive perspectives. Our attack methodology uses reinforcement learning to automatically optimize query selection, achieving better fingerprinting accuracy with only 3 queries compared to randomly selecting 3 queries from the same pool. Our defensive approach employs semantic-preserving output filtering through a secondary LLM to obfuscate model identity while maintaining semantic integrity. The defensive method reduces fingerprinting accuracy across tested models while preserving output quality. These contributions show the potential to improve fingerprinting tools capabilities while providing practical mitigation strategies against fingerprinting attacks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P/D-Device: Disaggregated Large Language Model between Cloud and Devices</title>
<link>https://arxiv.org/abs/2508.09035</link>
<guid>https://arxiv.org/abs/2508.09035</guid>
<content:encoded><![CDATA[
arXiv:2508.09035v1 Announce Type: cross 
Abstract: Serving disaggregated large language models has been widely adopted in industrial practice for enhanced performance. However, too many tokens generated in decoding phase, i.e., occupying the resources for a long time, essentially hamper the cloud from achieving a higher throughput. Meanwhile, due to limited on-device resources, the time to first token (TTFT), i.e., the latency of prefill phase, increases dramatically with the growth on prompt length. In order to concur with such a bottleneck on resources, i.e., long occupation in cloud and limited on-device computing capacity, we propose to separate large language model between cloud and devices. That is, the cloud helps a portion of the content for each device, only in its prefill phase. Specifically, after receiving the first token from the cloud, decoupling with its own prefill, the device responds to the user immediately for a lower TTFT. Then, the following tokens from cloud are presented via a speed controller for smoothed TPOT (the time per output token), until the device catches up with the progress. On-device prefill is then amortized using received tokens while the resource usage in cloud is controlled. Moreover, during cloud prefill, the prompt can be refined, using those intermediate data already generated, to further speed up on-device inference. We implement such a scheme P/D-Device, and confirm its superiority over other alternatives. We further propose an algorithm to decide the best settings. Real-trace experiments show that TTFT decreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud throughput increases by up to 15x.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive Maintenance Using Deep Neural Networks</title>
<link>https://arxiv.org/abs/2508.09054</link>
<guid>https://arxiv.org/abs/2508.09054</guid>
<content:encoded><![CDATA[
arXiv:2508.09054v1 Announce Type: cross 
Abstract: Track circuits are critical for railway operations, acting as the main signalling sub-system to locate trains. Continuous Variable Current Modulation (CVCM) is one such technology. Like any field-deployed, safety-critical asset, it can fail, triggering cascading disruptions. Many failures originate as subtle anomalies that evolve over time, often not visually apparent in monitored signals. Conventional approaches, which rely on clear signal changes, struggle to detect them early. Early identification of failure types is essential to improve maintenance planning, minimising downtime and revenue loss. Leveraging deep neural networks, we propose a predictive maintenance framework that classifies anomalies well before they escalate into failures. Validated on 10 CVCM failure cases across different installations, the method is ISO-17359 compliant and outperforms conventional techniques, achieving 99.31% overall accuracy with detection within 1% of anomaly onset. Through conformal prediction, we provide uncertainty estimates, reaching 99% confidence with consistent coverage across classes. Given CVCMs global deployment, the approach is scalable and adaptable to other track circuits and railway systems, enhancing operational reliability.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chartwin: a Case Study on Channel Charting-aided Localization in Dynamic Digital Network Twins</title>
<link>https://arxiv.org/abs/2508.09055</link>
<guid>https://arxiv.org/abs/2508.09055</guid>
<content:encoded><![CDATA[
arXiv:2508.09055v1 Announce Type: cross 
Abstract: Wireless communication systems can significantly benefit from the availability of spatially consistent representations of the wireless channel to efficiently perform a wide range of communication tasks. Towards this purpose, channel charting has been introduced as an effective unsupervised learning technique to achieve both locally and globally consistent radio maps. In this letter, we propose Chartwin, a case study on the integration of localization-oriented channel charting with dynamic Digital Network Twins (DNTs). Numerical results showcase the significant performance of semi-supervised channel charting in constructing a spatially consistent chart of the considered extended urban environment. The considered method results in $\approx$ 4.5 m localization error for the static DNT and $\approx$ 6 m in the dynamic DNT, fostering DNT-aided channel charting and localization.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing a Transferable Federated Network Intrusion Detection System</title>
<link>https://arxiv.org/abs/2508.09060</link>
<guid>https://arxiv.org/abs/2508.09060</guid>
<content:encoded><![CDATA[
arXiv:2508.09060v1 Announce Type: cross 
Abstract: Intrusion Detection Systems (IDS) are a vital part of a network-connected device. In this paper, we develop a deep learning based intrusion detection system that is deployed in a distributed setup across devices connected to a network. Our aim is to better equip deep learning models against unknown attacks using knowledge from known attacks. To this end, we develop algorithms to maximize the number of transferability relationships. We propose a Convolutional Neural Network (CNN) model, along with two algorithms that maximize the number of relationships observed. One is a two step data pre-processing stage, and the other is a Block-Based Smart Aggregation (BBSA) algorithm. The proposed system succeeds in achieving superior transferability performance while maintaining impressive local detection rates. We also show that our method is generalizable, exhibiting transferability potential across datasets and even with different backbones. The code for this work can be found at https://github.com/ghosh64/tabfidsv2.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VertexRegen: Mesh Generation with Continuous Level of Detail</title>
<link>https://arxiv.org/abs/2508.09062</link>
<guid>https://arxiv.org/abs/2508.09062</guid>
<content:encoded><![CDATA[
arXiv:2508.09062v1 Announce Type: cross 
Abstract: We introduce VertexRegen, a novel mesh generation framework that enables generation at a continuous level of detail. Existing autoregressive methods generate meshes in a partial-to-complete manner and thus intermediate steps of generation represent incomplete structures. VertexRegen takes inspiration from progressive meshes and reformulates the process as the reversal of edge collapse, i.e. vertex split, learned through a generative model. Experimental results demonstrate that VertexRegen produces meshes of comparable quality to state-of-the-art methods while uniquely offering anytime generation with the flexibility to halt at any step to yield valid meshes with varying levels of detail.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring</title>
<link>https://arxiv.org/abs/2508.09085</link>
<guid>https://arxiv.org/abs/2508.09085</guid>
<content:encoded><![CDATA[
arXiv:2508.09085v1 Announce Type: cross 
Abstract: Outdoor health monitoring is essential to detect early abnormal health status for safeguarding human health and safety. Conventional outdoor monitoring relies on static multimodal deep learning frameworks, which requires extensive data training from scratch and fails to capture subtle health status changes. Multimodal large language models (MLLMs) emerge as a promising alternative, utilizing only small datasets to fine-tune pre-trained information-rich models for enabling powerful health status monitoring. Unfortunately, MLLM-based outdoor health monitoring also faces significant challenges: I) sensor data contains input noise stemming from sensor data acquisition and fluctuation noise caused by sudden changes in physiological signals due to dynamic outdoor environments, thus degrading the training performance; ii) current transformer based MLLMs struggle to achieve robust multimodal fusion, as they lack a design for fusing the noisy modality; iii) modalities with varying noise levels hinder accurate recovery of missing data from fluctuating distributions. To combat these challenges, we propose an uncertainty-aware multimodal fusion framework, named DUAL-Health, for outdoor health monitoring in dynamic and noisy environments. First, to assess the impact of noise, we accurately quantify modality uncertainty caused by input and fluctuation noise with current and temporal features. Second, to empower efficient muitimodal fusion with low-quality modalities,we customize the fusion weight for each modality based on quantified and calibrated uncertainty. Third, to enhance data recovery from fluctuating noisy modalities, we align modality distributions within a common semantic space. Extensive experiments demonstrate that our DUAL-Health outperforms state-of-the-art baselines in detection accuracy and robustness.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained free energy minimization for the design of thermal states and stabilizer thermodynamic systems</title>
<link>https://arxiv.org/abs/2508.09103</link>
<guid>https://arxiv.org/abs/2508.09103</guid>
<content:encoded><![CDATA[
arXiv:2508.09103v1 Announce Type: cross 
Abstract: A quantum thermodynamic system is described by a Hamiltonian and a list of conserved, non-commuting charges, and a fundamental goal is to determine the minimum energy of the system subject to constraints on the charges. Recently, [Liu et al., arXiv:2505.04514] proposed first- and second-order classical and hybrid quantum-classical algorithms for solving a dual chemical potential maximization problem, and they proved that these algorithms converge to global optima by means of gradient-ascent approaches. In this paper, we benchmark these algorithms on several problems of interest in thermodynamics, including one- and two-dimensional quantum Heisenberg models with nearest and next-to-nearest neighbor interactions and with the charges set to the total $x$, $y$, and $z$ magnetizations. We also offer an alternative compelling interpretation of these algorithms as methods for designing ground and thermal states of controllable Hamiltonians, with potential applications in molecular and material design. Furthermore, we introduce stabilizer thermodynamic systems as thermodynamic systems based on stabilizer codes, with the Hamiltonian constructed from a given code's stabilizer operators and the charges constructed from the code's logical operators. We benchmark the aforementioned algorithms on several examples of stabilizer thermodynamic systems, including those constructed from the one-to-three-qubit repetition code, the perfect one-to-five-qubit code, and the two-to-four-qubit error-detecting code. Finally, we observe that the aforementioned hybrid quantum-classical algorithms, when applied to stabilizer thermodynamic systems, can serve as alternative methods for encoding qubits into stabilizer codes at a fixed temperature, and we provide an effective method for warm-starting these encoding algorithms whenever a single qubit is encoded into multiple physical qubits.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complex Logical Instruction Generation</title>
<link>https://arxiv.org/abs/2508.09125</link>
<guid>https://arxiv.org/abs/2508.09125</guid>
<content:encoded><![CDATA[
arXiv:2508.09125v1 Announce Type: cross 
Abstract: Instruction following has catalyzed the recent era of Large Language Models (LLMs) and is the foundational skill underpinning more advanced capabilities such as reasoning and agentic behaviors. As tasks grow more challenging, the logic structures embedded in natural language instructions becomes increasingly intricate. However, how well LLMs perform on such logic-rich instructions remains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a scalable, automated framework for generating verifiable instructions from code functions, which can naturally express rich logic such as conditionals, nesting, recursion, and function calls. We further curate a collection of complex code functions and use LogicIFGen to construct LogicIFEval, a benchmark comprising 426 verifiable logic-rich instructions. Our experiments demonstrate that current state-of-the-art LLMs still struggle to correctly follow the instructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the instructions, revealing significant deficiencies in the instruction-following ability. Code and Benchmark: https://github.com/mianzhang/LogicIF
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tame Riemannian Stochastic Approximation</title>
<link>https://arxiv.org/abs/2302.00709</link>
<guid>https://arxiv.org/abs/2302.00709</guid>
<content:encoded><![CDATA[
arXiv:2302.00709v5 Announce Type: replace 
Abstract: We study the properties of stochastic approximation applied to a tame nondifferentiable function subject to constraints defined by a Riemannian manifold. The objective landscape of tame functions, arising in o-minimal topology extended to a geometric category when generalized to manifolds, exhibits some structure that enables theoretical guarantees of expected function decrease and asymptotic convergence for generic stochastic sub-gradient descent. Recent work has shown that this class of functions faithfully model the loss landscape of deep neural network training objectives, and the autograd operation used in deep learning packages implements a variant of subgradient descent with the correct properties for convergence. Riemannian optimization uses geometric properties of a constraint set to perform a minimization procedure while enforcing adherence to the the optimization variable lying on a Riemannian manifold. This paper presents the first study of tame optimization on Riemannian manifolds, highlighting the rich geometric structure of the problem and confirming the appropriateness of the canonical "SGD" for such a problem with the analysis and numerical reports of a simple Retracted SGD algorithm.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BELLA: Black box model Explanations by Local Linear Approximations</title>
<link>https://arxiv.org/abs/2305.11311</link>
<guid>https://arxiv.org/abs/2305.11311</guid>
<content:encoded><![CDATA[
arXiv:2305.11311v3 Announce Type: replace 
Abstract: Understanding the decision-making process of black-box models has become not just a legal requirement, but also an additional way to assess their performance. However, the state of the art post-hoc explanation approaches for regression models rely on synthetic data generation, which introduces uncertainty and can hurt the reliability of the explanations. Furthermore, they tend to produce explanations that apply to only very few data points. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. BELLA maximizes the size of the neighborhood to which the linear model applies so that the explanations are accurate, simple, general, and robust.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Operator Variational Inference based on Regularized Stein Discrepancy for Deep Gaussian Processes</title>
<link>https://arxiv.org/abs/2309.12658</link>
<guid>https://arxiv.org/abs/2309.12658</guid>
<content:encoded><![CDATA[
arXiv:2309.12658v2 Announce Type: replace 
Abstract: Deep Gaussian Process (DGP) models offer a powerful nonparametric approach for Bayesian inference, but exact inference is typically intractable, motivating the use of various approximations. However, existing approaches, such as mean-field Gaussian assumptions, limit the expressiveness and efficacy of DGP models, while stochastic approximation can be computationally expensive. To tackle these challenges, we introduce Neural Operator Variational Inference (NOVI) for Deep Gaussian Processes. NOVI uses a neural generator to obtain a sampler and minimizes the Regularized Stein Discrepancy in L2 space between the generated distribution and true posterior. We solve the minimax problem using Monte Carlo estimation and subsampling stochastic optimization techniques. We demonstrate that the bias introduced by our method can be controlled by multiplying the Fisher divergence with a constant, which leads to robust error control and ensures the stability and precision of the algorithm. Our experiments on datasets ranging from hundreds to tens of thousands demonstrate the effectiveness and the faster convergence rate of the proposed method. We achieve a classification accuracy of 93.56 on the CIFAR10 dataset, outperforming SOTA Gaussian process methods. Furthermore, our method guarantees theoretically controlled prediction error for DGP models and demonstrates remarkable performance on various datasets. We are optimistic that NOVI has the potential to enhance the performance of deep Bayesian nonparametric models and could have significant implications for various practical applications
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control</title>
<link>https://arxiv.org/abs/2310.10948</link>
<guid>https://arxiv.org/abs/2310.10948</guid>
<content:encoded><![CDATA[
arXiv:2310.10948v3 Announce Type: replace 
Abstract: Over the years, reinforcement learning has emerged as a popular approach to develop signal control and vehicle platooning strategies either independently or in a hierarchical way. However, jointly controlling both in real-time to alleviate traffic congestion presents new challenges, such as the inherent physical and behavioral heterogeneity between signal control and platooning, as well as coordination between them. This paper proposes an innovative solution to tackle these challenges based on heterogeneous graph multi-agent reinforcement learning and traffic theories. Our approach involves: 1) designing platoon and signal control as distinct reinforcement learning agents with their own set of observations, actions, and reward functions to optimize traffic flow; 2) designing coordination by incorporating graph neural networks within multi-agent reinforcement learning to facilitate seamless information exchange among agents on a regional scale; 3) applying alternating optimization for training, allowing agents to update their own policies and adapt to other agents' policies. We evaluate our approach through SUMO simulations, which show convergent results in terms of both travel time and fuel consumption, and superior performance compared to other adaptive signal control methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multidimensional Adaptive Coefficient for Inference Trajectory Optimization in Flow and Diffusion</title>
<link>https://arxiv.org/abs/2404.14161</link>
<guid>https://arxiv.org/abs/2404.14161</guid>
<content:encoded><![CDATA[
arXiv:2404.14161v4 Announce Type: replace 
Abstract: Flow and diffusion models have demonstrated strong performance and training stability across various tasks but lack two critical properties of simulation-based methods: freedom of dimensionality and adaptability to different inference trajectories. To address this limitation, we propose the Multidimensional Adaptive Coefficient (MAC), a plug-in module for flow and diffusion models that extends conventional unidimensional coefficients to multidimensional ones and enables inference trajectory-wise adaptation. MAC is trained via simulation-based feedback through adversarial refinement. Empirical results across diverse frameworks and datasets demonstrate that MAC enhances generative quality with high training efficiency. Consequently, our work offers a new perspective on inference trajectory optimality, encouraging future research to move beyond vector field design and to leverage training-efficient, simulation-based optimization.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A DNN Biophysics Model with Topological and Electrostatic Features</title>
<link>https://arxiv.org/abs/2409.03658</link>
<guid>https://arxiv.org/abs/2409.03658</guid>
<content:encoded><![CDATA[
arXiv:2409.03658v2 Announce Type: replace 
Abstract: In this project, we provide a deep-learning neural network (DNN) based biophysics model to predict protein properties. The model uses multi-scale and uniform topological and electrostatic features generated with protein structural information and force field, which governs the molecular mechanics. The topological features are generated using the element specified persistent homology (ESPH) while the electrostatic features are fast computed using a Cartesian treecode. These features are uniform in number for proteins with various sizes thus the broadly available protein structure database can be used in training the network. These features are also multi-scale thus the resolution and computational cost can be balanced by the users. The machine learning simulation on over 4000 protein structures shows the efficiency and fidelity of these features in representing the protein structure and force field for the predication of their biophysical properties such as electrostatic solvation energy. Tests on topological or electrostatic features alone and the combination of both showed the optimal performance when both features are used. This model shows its potential as a general tool in assisting biophysical properties and function prediction for the broad biomolecules using data from both theoretical computing and experiments.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Diversity Shortens the ICL Plateau</title>
<link>https://arxiv.org/abs/2410.05448</link>
<guid>https://arxiv.org/abs/2410.05448</guid>
<content:encoded><![CDATA[
arXiv:2410.05448v3 Announce Type: replace 
Abstract: In-context learning (ICL) describes a language model's ability to generate outputs based on a set of input demonstrations and a subsequent query. To understand this remarkable capability, researchers have studied simplified, stylized models. These studies have consistently observed long loss plateaus, during which models exhibit minimal improvement, followed by a sudden, rapid surge of learning. In this work, we reveal that training on multiple diverse ICL tasks simultaneously shortens the loss plateaus, making each task easier to learn. This finding is surprising as it contradicts the natural intuition that the combined complexity of multiple ICL tasks would lengthen the learning process, not shorten it. Our result suggests that the recent success in large-scale training of language models may be attributed not only to the richness of the data at scale but also to the easier optimization (training) induced by the diversity of natural language training data.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Generalization of Vision-Based RL Without Data Augmentation</title>
<link>https://arxiv.org/abs/2410.07441</link>
<guid>https://arxiv.org/abs/2410.07441</guid>
<content:encoded><![CDATA[
arXiv:2410.07441v2 Announce Type: replace 
Abstract: Generalizing vision-based reinforcement learning (RL) agents to novel environments remains a difficult and open challenge. Current trends are to collect large-scale datasets or use data augmentation techniques to prevent overfitting and improve downstream generalization. However, the computational and data collection costs increase exponentially with the number of task variations and can destabilize the already difficult task of training RL agents. In this work, we take inspiration from recent advances in computational neuroscience and propose a model, Associative Latent DisentAnglement (ALDA), that builds on standard off-policy RL towards zero-shot generalization. Specifically, we revisit the role of latent disentanglement in RL and show how combining it with a model of associative memory achieves zero-shot generalization on difficult task variations without relying on data augmentation. Finally, we formally show that data augmentation techniques are a form of weak disentanglement and discuss the implications of this insight.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Policies with Physics-informed Representations in Complex Fluid Environments</title>
<link>https://arxiv.org/abs/2410.15250</link>
<guid>https://arxiv.org/abs/2410.15250</guid>
<content:encoded><![CDATA[
arXiv:2410.15250v2 Announce Type: replace 
Abstract: Control in fluid environments is an important research area with numerous applications across various domains, including underwater robotics, aerospace engineering, and biomedical systems. However, in practice, control methods often face challenges due to sparse or missing observations, stemming from sensor limitations and faults. These issues result in observations that are not only sparse but also inconsistent in their number and modalities (e.g., velocity and pressure sensors). In this work, we propose a Physics-Informed Representation (PIR) algorithm for multi-modal policies of control to leverage the sparse and random observations in complex fluid environments. PIR integrates sparse observational data with the Partial Differential Equation (PDE) information to distill a unified representation of fluid systems. The main idea is that PDE solutions are determined by three elements: the equation, initial conditions, and boundary conditions. Given the equation, we only need to learn the representation of the initial and boundary conditions, which define a trajectory of a specific fluid system. Specifically, it leverages PDE loss to fit the neural network and data loss calculated on the observations with random quantities and multi-modalities to propagate the information with initial and boundary conditions into the representations. The representations are the learnable parameters or the output of the encoder. In the experiments, the PIR illustrates the superior consistency with the features of the ground truth compared with baselines, even when there are missing modalities. Furthermore, PIR combined with Reinforcement Learning has been successfully applied in control tasks where the robot leverages the learned state by PIR faster and more accurately, passing through the complex vortex street from a random starting location to reach a random target.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Aggregations of Proper Learners in Multiclass Classification</title>
<link>https://arxiv.org/abs/2410.22749</link>
<guid>https://arxiv.org/abs/2410.22749</guid>
<content:encoded><![CDATA[
arXiv:2410.22749v2 Announce Type: replace 
Abstract: Multiclass learnability is known to exhibit a properness barrier: there are learnable classes which cannot be learned by any proper learner. Binary classification faces no such barrier for learnability, but a similar one for optimal learning, which can in general only be achieved by improper learners. Fortunately, recent advances in binary classification have demonstrated that this requirement can be satisfied using aggregations of proper learners, some of which are strikingly simple. This raises a natural question: to what extent can simple aggregations of proper learners overcome the properness barrier in multiclass classification?
  We give a positive answer to this question for classes which have finite Graph dimension, $d_G$. Namely, we demonstrate that the optimal binary learners of Hanneke, Larsen, and Aden-Ali et al. (appropriately generalized to the multiclass setting) achieve sample complexity $O\left(\frac{d_G + \ln(1 / \delta)}{\epsilon}\right)$. This forms a strict improvement upon the sample complexity of ERM. We complement this with a lower bound demonstrating that for certain classes of Graph dimension $d_G$, majorities of ERM learners require $\Omega \left( \frac{d_G + \ln(1 / \delta)}{\epsilon}\right)$ samples. Furthermore, we show that a single ERM requires $\Omega \left(\frac{d_G \ln(1 / \epsilon) + \ln(1 / \delta)}{\epsilon}\right)$ samples on such classes, exceeding the lower bound of Daniely et al. (2015) by a factor of $\ln(1 / \epsilon)$. For multiclass learning in full generality -- i.e., for classes of finite DS dimension but possibly infinite Graph dimension -- we give a strong refutation to these learning strategies, by exhibiting a learnable class which cannot be learned to constant error by any aggregation of a finite number of proper learners.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chemist-aligned retrosynthesis by ensembling diverse inductive bias models</title>
<link>https://arxiv.org/abs/2412.05269</link>
<guid>https://arxiv.org/abs/2412.05269</guid>
<content:encoded><![CDATA[
arXiv:2412.05269v2 Announce Type: replace 
Abstract: Chemical synthesis remains a critical bottleneck in the discovery and manufacture of functional small molecules. AI-based synthesis planning models could be a potential remedy to find effective syntheses, and have made progress in recent years. However, they still struggle with less frequent, yet critical reactions for synthetic strategy, as well as hallucinated, incorrect predictions. This hampers multi-step search algorithms that rely on models, and leads to misalignment with chemists' expectations. Here we propose RetroChimera: a frontier retrosynthesis model, built upon two newly developed components with complementary inductive biases, which we fuse together using a new framework for integrating predictions from multiple sources via a learning-based ensembling strategy. Through experiments across several orders of magnitude in data scale and splitting strategy, we show RetroChimera outperforms all major models by a large margin, demonstrating robustness outside the training data, as well as for the first time the ability to learn from even a very small number of examples per reaction class. Moreover, industrial organic chemists prefer predictions from RetroChimera over the reactions it was trained on in terms of quality, revealing high levels of alignment. Finally, we demonstrate zero-shot transfer to an internal dataset from a major pharmaceutical company, showing robust generalization under distribution shift. With the new dimension that our ensembling framework unlocks, we anticipate further acceleration in the development of even more accurate models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding-based Regression</title>
<link>https://arxiv.org/abs/2501.19383</link>
<guid>https://arxiv.org/abs/2501.19383</guid>
<content:encoded><![CDATA[
arXiv:2501.19383v2 Announce Type: replace 
Abstract: Language models have recently been shown capable of performing regression wherein numeric predictions are represented as decoded strings. In this work, we provide theoretical grounds for this capability and furthermore investigate the utility of causal sequence decoding models as numeric regression heads given any feature representation. We find that, despite being trained in the usual way - for next-token prediction via cross-entropy loss - decoder-based heads are as performant as standard pointwise heads when benchmarked over standard regression tasks, while being flexible enough to capture smooth numeric distributions, such as in the task of density estimation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning</title>
<link>https://arxiv.org/abs/2502.08577</link>
<guid>https://arxiv.org/abs/2502.08577</guid>
<content:encoded><![CDATA[
arXiv:2502.08577v2 Announce Type: replace 
Abstract: In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns. However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID). The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling. Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments. To close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion. This paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm. Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions. Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forget the Data and Fine-Tuning! Just Fold the Network to Compress</title>
<link>https://arxiv.org/abs/2502.10216</link>
<guid>https://arxiv.org/abs/2502.10216</guid>
<content:encoded><![CDATA[
arXiv:2502.10216v2 Announce Type: replace 
Abstract: We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging k-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-driven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization</title>
<link>https://arxiv.org/abs/2502.10648</link>
<guid>https://arxiv.org/abs/2502.10648</guid>
<content:encoded><![CDATA[
arXiv:2502.10648v3 Announce Type: replace 
Abstract: We introduce LLM-Lasso, a novel framework that leverages large language models (LLMs) to guide feature selection in Lasso $\ell_1$ regression. Unlike traditional methods that rely solely on numerical data, LLM-Lasso incorporates domain-specific knowledge extracted from natural language, enhanced through a retrieval-augmented generation (RAG) pipeline, to seamlessly integrate data-driven modeling with contextual insights. Specifically, the LLM generates penalty factors for each feature, which are converted into weights for the Lasso penalty using a simple, tunable model. Features identified as more relevant by the LLM receive lower penalties, increasing their likelihood of being retained in the final model, while less relevant features are assigned higher penalties, reducing their influence. Importantly, LLM-Lasso has an internal validation step that determines how much to trust the contextual knowledge in our prediction pipeline. Hence it addresses key challenges in robustness, making it suitable for mitigating potential inaccuracies or hallucinations from the LLM. In various biomedical case studies, LLM-Lasso outperforms standard Lasso and existing feature selection baselines, all while ensuring the LLM operates without prior access to the datasets. To our knowledge, this is the first approach to effectively integrate conventional feature selection techniques directly with LLM-based domain-specific reasoning.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN</title>
<link>https://arxiv.org/abs/2502.12207</link>
<guid>https://arxiv.org/abs/2502.12207</guid>
<content:encoded><![CDATA[
arXiv:2502.12207v3 Announce Type: replace 
Abstract: Deep neural networks have demonstrated remarkable performance across various domains. However, they are vulnerable to adversarial examples, which can lead to erroneous predictions. Generative Adversarial Networks (GANs) can leverage the generators and discriminators model to quickly produce high-quality adversarial examples. Since both modules train in a competitive and simultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial examples with better transferability compared to traditional methods. However, the generation of perturbations is usually limited to a single iteration, preventing these examples from fully exploiting the potential of the methods. To tackle this issue, we introduce a novel approach named Progressive Auto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive iteration mechanism within a progressive generation network to craft adversarial examples with enhanced attack capability. We thoroughly evaluate our PAR-AdvGAN method with a large-scale experiment, demonstrating its superior performance over various state-of-the-art black-box adversarial attacks, as well as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the adversarial example generation, i.e., achieving the speeds of up to 335.5 frames per second on Inception-v3 model, outperforming the gradient-based transferable attack algorithms. Our code is available at: https://github.com/LMBTough/PAR
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions</title>
<link>https://arxiv.org/abs/2502.13135</link>
<guid>https://arxiv.org/abs/2502.13135</guid>
<content:encoded><![CDATA[
arXiv:2502.13135v3 Announce Type: replace 
Abstract: We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent's understanding of the synthetic users' needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Computation Pruning for the Forgetting Transformer</title>
<link>https://arxiv.org/abs/2504.06949</link>
<guid>https://arxiv.org/abs/2504.06949</guid>
<content:encoded><![CDATA[
arXiv:2504.06949v2 Announce Type: replace 
Abstract: The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. In particular, our method performs provably safe pruning via a dynamically set pruning threshold that guarantees the pruned attention weights are negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs and memory accesses in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 50% to 70% reduction in attention runtime (or a 2-3$\times$ speedup) and a roughly 10% to 40% increase in end-to-end training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning</title>
<link>https://arxiv.org/abs/2504.08713</link>
<guid>https://arxiv.org/abs/2504.08713</guid>
<content:encoded><![CDATA[
arXiv:2504.08713v5 Announce Type: replace 
Abstract: Deep learning-based electrocardiogram (ECG) classification has shown impressive performance but clinical adoption has been slowed by the lack of transparent and faithful explanations. Post hoc methods such as saliency maps may fail to reflect a model's true decision process. Prototype-based reasoning offers a more transparent alternative by grounding decisions in similarity to learned representations of real ECG segments, enabling faithful, case-based explanations. We introduce ProtoECGNet, a prototype-based deep learning model for interpretable, multi-label ECG classification. ProtoECGNet employs a structured, multi-branch architecture that reflects clinical interpretation workflows: it integrates a 1D CNN with global prototypes for rhythm classification, a 2D CNN with time-localized prototypes for morphology-based reasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each branch is trained with a prototype loss designed for multi-label learning, combining clustering, separation, diversity, and a novel contrastive loss that encourages appropriate separation between prototypes of unrelated classes while allowing clustering for frequently co-occurring diagnoses. We evaluate ProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating competitive performance relative to state-of-the-art black-box models while providing structured, case-based explanations. To assess prototype quality, we conduct a structured clinician review of the final model's projected prototypes, finding that they are rated as representative and clear. ProtoECGNet shows that prototype learning can be effectively scaled to complex, multi-label time-series classification, offering a practical path toward transparent and trustworthy deep learning models for clinical decision support.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal Temporal Fusion for Financial Market Forecasting</title>
<link>https://arxiv.org/abs/2504.13522</link>
<guid>https://arxiv.org/abs/2504.13522</guid>
<content:encoded><![CDATA[
arXiv:2504.13522v2 Announce Type: replace 
Abstract: Accurate forecasting in financial markets requires integrating diverse data sources, from historical prices to macroeconomic indicators and financial news. However, existing models often fail to align these modalities effectively, limiting their practical use. In this paper, we introduce a transformer-based deep learning framework, Cross-Modal Temporal Fusion (CMTF), that fuses structured and unstructured financial data for improved market prediction. The model incorporates a tensor interpretation module for feature selection and an auto-training pipeline for efficient hyperparameter tuning. Experimental results using FTSE 100 stock data demonstrate that CMTF achieves superior performance in price direction classification compared to classical and deep learning baselines. These findings suggest that our framework is an effective and scalable solution for real-world cross-modal financial forecasting tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU</title>
<link>https://arxiv.org/abs/2504.17028</link>
<guid>https://arxiv.org/abs/2504.17028</guid>
<content:encoded><![CDATA[
arXiv:2504.17028v3 Announce Type: replace 
Abstract: This paper demonstrates the feasibility of democratizing AI-driven global weather forecasting models among university research groups by leveraging Graphics Processing Units (GPUs) and freely available AI models, such as NVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network for weather prediction and is trained on a 73-channel subset of the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset at single levels and different pressure levels. Although the training specifications for FourCastNetv2 are not released to the public, the training documentation of the model's first generation, FourCastNet, is available to all users. The training had 64 A100 GPUs and took 16 hours to complete. Although NVIDIA's models offer significant reductions in both time and cost compared to traditional Numerical Weather Prediction (NWP), reproducing published forecasting results presents ongoing challenges for resource-constrained university research groups with limited GPU availability. We demonstrate both (i) leveraging FourCastNetv2 to create predictions through the designated application programming interface (API) and (ii) utilizing NVIDIA hardware to train the original FourCastNet model. Further, this paper demonstrates the capabilities and limitations of NVIDIA A100's for resource-limited research groups in universities. We also explore data management, training efficiency, and model validation, highlighting the advantages and challenges of using limited high-performance computing resources. Consequently, this paper and its corresponding GitHub materials may serve as an initial guide for other university research groups and courses related to machine learning, climate science, and data science to develop research and education programs on AI weather forecasting, and hence help democratize the AI NWP in the digital economy.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence</title>
<link>https://arxiv.org/abs/2504.17703</link>
<guid>https://arxiv.org/abs/2504.17703</guid>
<content:encoded><![CDATA[
arXiv:2504.17703v3 Announce Type: replace 
Abstract: Federated Learning (FL) has emerged as a transformative paradigm in the field of distributed machine learning, enabling multiple clients such as mobile devices, edge nodes, or organizations to collaboratively train a shared global model without the need to centralize sensitive data. This decentralized approach addresses growing concerns around data privacy, security, and regulatory compliance, making it particularly attractive in domains such as healthcare, finance, and smart IoT systems. This survey provides a concise yet comprehensive overview of Federated Learning, beginning with its core architecture and communication protocol. We discuss the standard FL lifecycle, including local training, model aggregation, and global updates. A particular emphasis is placed on key technical challenges such as handling non-IID (non-independent and identically distributed) data, mitigating system and hardware heterogeneity, reducing communication overhead, and ensuring privacy through mechanisms like differential privacy and secure aggregation. Furthermore, we examine emerging trends in FL research, including personalized FL, cross-device versus cross-silo settings, and integration with other paradigms such as reinforcement learning and quantum computing. We also highlight real-world applications and summarize benchmark datasets and evaluation metrics commonly used in FL research. Finally, we outline open research problems and future directions to guide the development of scalable, efficient, and trustworthy FL systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mj\"olnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density</title>
<link>https://arxiv.org/abs/2504.19822</link>
<guid>https://arxiv.org/abs/2504.19822</guid>
<content:encoded><![CDATA[
arXiv:2504.19822v3 Announce Type: replace 
Abstract: Recent advances in AI-based weather forecasting models, such as FourCastNet, Pangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep learning to emulate complex atmospheric dynamics. Building on this momentum, we propose Mj\"olnir, a novel deep learning-based framework for global lightning flash density parameterization. Trained on ERA5 atmospheric predictors and World Wide Lightning Location Network (WWLLN) observations at a daily temporal resolution and 1 degree spatial resolution, Mj\"olnir captures the nonlinear mapping between large-scale environmental conditions and lightning activity. The model architecture is based on the InceptionNeXt backbone with SENet, and a multi-task learning strategy to simultaneously predict lightning occurrence and magnitude. Extensive evaluations yield that Mollnir accurately reproduces the global distribution, seasonal variability, and regional characteristics of lightning activity, achieving a global Pearson correlation coefficient of 0.96 for annual mean fields. These results suggest that Mj\"olnir serves not only as an effective data-driven global lightning parameterization but also as a promising AI-based scheme for next-generation Earth system models (AI-ESMs).
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Fuzzy C-Means with Adaptive Weight-based Filtering for Efficient Clustering</title>
<link>https://arxiv.org/abs/2505.04335</link>
<guid>https://arxiv.org/abs/2505.04335</guid>
<content:encoded><![CDATA[
arXiv:2505.04335v2 Announce Type: replace 
Abstract: Clustering algorithms play a pivotal role in unsupervised learning by identifying and grouping similar objects based on shared characteristics. Although traditional clustering techniques, such as hard and fuzzy center-based clustering, have been widely used, they struggle with complex, high-dimensional, and non-Euclidean datasets. In particular, the fuzzy $C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits notable limitations in non-Euclidean spaces. Euclidean spaces assume linear separability and uniform distance scaling, limiting their effectiveness in capturing complex, hierarchical, or non-Euclidean structures in fuzzy clustering. To overcome these challenges, we introduce Filtration-based Hyperbolic Fuzzy C-Means (HypeFCM), a novel clustering algorithm tailored for better representation of data relationships in non-Euclidean spaces. HypeFCM integrates the principles of fuzzy clustering with hyperbolic geometry and employs a weight-based filtering mechanism to improve performance. The algorithm initializes weights using a Dirichlet distribution and iteratively refines cluster centroids and membership assignments based on a hyperbolic metric in the Poincar\'e Disc model. Extensive experimental evaluations on $6$ synthetic and $12$ real-world datasets demonstrate that HypeFCM significantly outperforms conventional fuzzy clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.15130</link>
<guid>https://arxiv.org/abs/2505.15130</guid>
<content:encoded><![CDATA[
arXiv:2505.15130v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) such as CLIP have shown remarkable performance in cross-modal tasks through large-scale contrastive pre-training. To adapt these large transformer-based models efficiently for downstream tasks, Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as scalable alternatives to full fine-tuning, especially in few-shot scenarios. However, like traditional deep neural networks, VLMs are highly vulnerable to adversarial attacks, where imperceptible perturbations can significantly degrade model performance. Adversarial training remains the most effective strategy for improving model robustness in PEFT. In this work, we propose AdvCLIP-LoRA, the first algorithm designed to enhance the adversarial robustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method formulates adversarial fine-tuning as a minimax optimization problem and provides theoretical guarantees for convergence under smoothness and nonconvex-strong-concavity assumptions. Empirical results across eight datasets using ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly improves robustness against common adversarial attacks (e.g., FGSM, PGD), without sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA as a practical and theoretically grounded approach for robust adaptation of VLMs in resource-constrained settings.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.03850</link>
<guid>https://arxiv.org/abs/2506.03850</guid>
<content:encoded><![CDATA[
arXiv:2506.03850v2 Announce Type: replace 
Abstract: Harmful fine-tuning (HFT), performed directly on open-source LLMs or through Fine-tuning-as-a-Service, breaks safety alignment and poses significant threats. Existing methods aim to mitigate HFT risks by learning robust representation on alignment data or making harmful data unlearnable, but they treat each data sample equally, leaving data vulnerability patterns understudied. In this work, we reveal that certain subsets of alignment data are consistently more prone to forgetting during HFT across different fine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware Alignment (VAA), which estimates data vulnerability, partitions data into "vulnerable" and "invulnerable" groups, and encourages balanced learning using a group distributionally robust optimization (Group DRO) framework. Specifically, VAA learns an adversarial sampler that samples examples from the currently underperforming group and then applies group-dependent adversarial perturbations to the data during training, aiming to encourage a balanced learning process across groups. Experiments across four fine-tuning tasks demonstrate that VAA significantly reduces harmful scores while preserving downstream task performance, outperforming state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saturation Self-Organizing Map</title>
<link>https://arxiv.org/abs/2506.10680</link>
<guid>https://arxiv.org/abs/2506.10680</guid>
<content:encoded><![CDATA[
arXiv:2506.10680v3 Announce Type: replace 
Abstract: Continual learning poses a fundamental challenge for neural systems, which often suffer from catastrophic forgetting when exposed to sequential tasks. Self-Organizing Maps (SOMs), despite their interpretability and efficiency, are not immune to this issue. In this paper, we introduce Saturation Self-Organizing Maps (SatSOM)-an extension of SOMs designed to improve knowledge retention in continual learning scenarios. SatSOM incorporates a novel saturation mechanism that gradually reduces the learning rate and neighborhood radius of neurons as they accumulate information. This effectively freezes well-trained neurons and redirects learning to underutilized areas of the map.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models</title>
<link>https://arxiv.org/abs/2506.14291</link>
<guid>https://arxiv.org/abs/2506.14291</guid>
<content:encoded><![CDATA[
arXiv:2506.14291v3 Announce Type: replace 
Abstract: Graph machine learning architectures are typically tailored to specific tasks on specific datasets, which hinders their broader applicability. This has led to a new quest in graph machine learning: how to build graph foundation models capable of generalizing across arbitrary graphs and features? In this work, we present a recipe for designing graph foundation models for node-level tasks from first principles. The key ingredient underpinning our study is a systematic investigation of the symmetries that a graph foundation model must respect. In a nutshell, we argue that label permutation-equivariance alongside feature permutation-invariance are necessary in addition to the common node permutation-equivariance on each local neighborhood of the graph. To this end, we first characterize the space of linear transformations that are equivariant to permutations of nodes and labels, and invariant to permutations of features. We then prove that the resulting network is a universal approximator on multisets that respect the aforementioned symmetries. Our recipe uses such layers on the multiset of features induced by the local neighborhood of the graph to obtain a class of graph foundation models for node property prediction. We validate our approach through extensive experiments on 29 real-world node classification datasets, demonstrating both strong zero-shot empirical performance and consistent improvement as the number of training graphs increases.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies</title>
<link>https://arxiv.org/abs/2506.15716</link>
<guid>https://arxiv.org/abs/2506.15716</guid>
<content:encoded><![CDATA[
arXiv:2506.15716v2 Announce Type: replace 
Abstract: Citizens' assemblies are an increasingly influential form of deliberative democracy, where randomly selected people discuss policy questions. The legitimacy of these assemblies hinges on their representation of the broader population, but participant dropout often leads to an unbalanced composition. In practice, dropouts are replaced by preselected alternates, but existing methods do not address how to choose these alternates. To address this gap, we introduce an optimization framework for alternate selection. Our algorithmic approach, which leverages learning-theoretic machinery, estimates dropout probabilities using historical data and selects alternates to minimize expected misrepresentation. Our theoretical bounds provide guarantees on sample complexity (with implications for computational efficiency) and on loss due to dropout probability mis-estimation. Empirical evaluation using real-world data demonstrates that, compared to the status quo, our method significantly improves representation while requiring fewer alternates.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System</title>
<link>https://arxiv.org/abs/2507.23261</link>
<guid>https://arxiv.org/abs/2507.23261</guid>
<content:encoded><![CDATA[
arXiv:2507.23261v2 Announce Type: replace 
Abstract: Current multi-agent systems (MAS) frameworks often rely on manually designed and static collaboration graph structures, limiting adaptability and performance. To address these limitations, we propose DynaSwarm, a dynamic framework that enhances LLM-based MAS through two key innovations: (1) an actor-critic reinforcement learning (A2C) mechanism to optimize graph structures with improved stability over prior RL methods, and (2) a dynamic graph selector that adaptively chooses the optimal graph structure for each input sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the need for rigid, one-fits-all graph architectures, instead leveraging sample-specific idiosyncrasies to dynamically route queries through specialized agent networks. (c) We propose to fine-tune the demonstration retriever to fully exploit the power of in-context learning (ICL). Extensive experiments on question answering, mathematical reasoning, and coding tasks demonstrate that DynaSwarm consistently outperforms state-of-the-art single-agent and MAS baselines across multiple LLM backbones. Our findings highlight the importance of sample-aware structural flexibility in LLM MAS designs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Time Series Classifiers with PHAR: Rule Extraction and Fusion from Post-hoc Attributions</title>
<link>https://arxiv.org/abs/2508.01687</link>
<guid>https://arxiv.org/abs/2508.01687</guid>
<content:encoded><![CDATA[
arXiv:2508.01687v2 Announce Type: replace 
Abstract: Explaining machine learning (ML) models for time series (TS) classification remains challenging due to the difficulty of interpreting raw time series and the high dimensionality of the input space. We introduce PHAR-Post-hoc Attribution Rules-a unified framework that transforms numeric feature attributions from post-hoc, instance-wise explainers (e.g., LIME, SHAP) into structured, human-readable rules. These rules define interpretable intervals that indicate where and when key decision boundaries occur, enhancing model transparency. PHAR performs comparably to native rule-based methods, such as Anchor, while scaling more efficiently to long TS sequences and achieving broader instance coverage. A dedicated rule fusion step consolidates rule sets using strategies like weighted selection and lasso-based refinement, balancing key quality metrics: coverage, confidence, and simplicity. This fusion ensures each instance receives a concise and unambiguous rule, improving both explanation fidelity and consistency. We further introduce visualization techniques to illustrate specificity-generalization trade-offs in the derived rules. PHAR resolves conflicting and overlapping explanations-a common effect of the Rashomon phenomenon-into coherent, domain-adaptable insights. Comprehensive experiments on UCR/UEA Time Series Classification Archive demonstrate that PHAR improves interpretability, decision transparency, and practical applicability for TS classification tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence Software Structured to Simulate Human Working Memory, Mental Imagery, and Mental Continuity</title>
<link>https://arxiv.org/abs/2204.05138</link>
<guid>https://arxiv.org/abs/2204.05138</guid>
<content:encoded><![CDATA[
arXiv:2204.05138v2 Announce Type: replace-cross 
Abstract: This article presents an artificial intelligence (AI) architecture intended to simulate the iterative updating of the human working memory system. It features several interconnected neural networks designed to emulate the specialized modules of the cerebral cortex. These are structured hierarchically and integrated into a global workspace. They are capable of temporarily maintaining high-level representational patterns akin to the psychological items maintained in working memory. This maintenance is made possible by persistent neural activity in the form of two modalities: sustained neural firing (resulting in a focus of attention) and synaptic potentiation (resulting in a short-term store). Representations held in persistent activity are recursively replaced resulting in incremental changes to the content of the working memory system. As this content gradually evolves, successive processing states overlap and are continuous with one another. The present article will explore how this architecture can lead to iterative shift in the distribution of coactive representations, ultimately leading to mental continuity between processing states, and thus to human-like thought and cognition. Like the human brain, this AI working memory store will be linked to multiple imagery (topographic map) generation systems corresponding to various sensory modalities. As working memory is iteratively updated, the maps created in response will construct sequences of related mental imagery. Thus, neural networks emulating the prefrontal cortex and its reciprocal interactions with early sensory and motor cortex capture the imagery guidance functions of the human brain. This sensory and motor imagery creation, coupled with an iteratively updated working memory store may provide an AI system with the cognitive assets needed to achieve synthetic consciousness or artificial sentience.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Generative Models for Climbing Aircraft from Radar Data</title>
<link>https://arxiv.org/abs/2309.14941</link>
<guid>https://arxiv.org/abs/2309.14941</guid>
<content:encoded><![CDATA[
arXiv:2309.14941v2 Announce Type: replace-cross 
Abstract: Accurate trajectory prediction (TP) for climbing aircraft is hampered by the presence of epistemic uncertainties concerning aircraft operation, which can lead to significant misspecification between predicted and observed trajectories. This paper proposes a generative model for climbing aircraft in which the standard Base of Aircraft Data (BADA) model is enriched by a functional correction to the thrust that is learned from data. The method offers three features: predictions of the arrival time with 26.7% less error when compared to BADA; generated trajectories that are realistic when compared to test data; and a means of computing confidence bounds for minimal computational cost.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Optimal and Fair Policies for Online Allocation of Scarce Societal Resources from Data Collected in Deployment</title>
<link>https://arxiv.org/abs/2311.13765</link>
<guid>https://arxiv.org/abs/2311.13765</guid>
<content:encoded><![CDATA[
arXiv:2311.13765v2 Announce Type: replace-cross 
Abstract: We study the problem of allocating scarce societal resources of different types (e.g., permanent housing, deceased donor kidneys for transplantation, ventilators) to heterogeneous allocatees on a waitlist (e.g., people experiencing homelessness, individuals suffering from end-stage renal disease, Covid-19 patients) based on their observed covariates. We leverage administrative data collected in deployment to design an online policy that maximizes expected outcomes while satisfying budget constraints, in the long run. Our proposed policy waitlists each individual for the resource maximizing the difference between their estimated mean treatment outcome and the estimated resource dual-price or, roughly, the opportunity cost of using the resource. Resources are then allocated as they arrive, in a first-come first-serve fashion. We demonstrate that our data-driven policy almost surely asymptotically achieves the expected outcome of the optimal out-of-sample policy under mild technical assumptions. We extend our framework to incorporate various fairness constraints. We evaluate the performance of our approach on the problem of designing policies for allocating scarce housing resources to people experiencing homelessness in Los Angeles based on data from the homeless management information system. In particular, we show that using our policies improves rates of exit from homelessness by 5.16% and that policies that are fair in either allocation or outcomes by race come at a very low price of fairness.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Lab to Field: Real-World Evaluation of an AI-Driven Smart Video Solution to Enhance Community Safety</title>
<link>https://arxiv.org/abs/2312.02078</link>
<guid>https://arxiv.org/abs/2312.02078</guid>
<content:encoded><![CDATA[
arXiv:2312.02078v3 Announce Type: replace-cross 
Abstract: This article adopts and evaluates an AI-enabled Smart Video Solution (SVS) designed to enhance safety in the real world. The system integrates with existing infrastructure camera networks, leveraging recent advancements in AI for easy adoption. Prioritizing privacy and ethical standards, pose based data is used for downstream AI tasks such as anomaly detection. Cloud-based infrastructure and mobile app are deployed, enabling real-time alerts within communities. The SVS employs innovative data representation and visualization techniques, such as the Occupancy Indicator, Statistical Anomaly Detection, Bird's Eye View, and Heatmaps, to understand pedestrian behaviors and enhance public safety. Evaluation of the SVS demonstrates its capacity to convert complex computer vision outputs into actionable insights for stakeholders, community partners, law enforcement, urban planners, and social scientists. This article presents a comprehensive real-world deployment and evaluation of the SVS, implemented in a community college environment across 16 cameras. The system integrates AI-driven visual processing, supported by statistical analysis, database management, cloud communication, and user notifications. Additionally, the article evaluates the end-to-end latency from the moment an AI algorithm detects anomalous behavior in real-time at the camera level to the time stakeholders receive a notification. The results demonstrate the system's robustness, effectively managing 16 CCTV cameras with a consistent throughput of 16.5 frames per second (FPS) over a 21-hour period and an average end-to-end latency of 26.76 seconds between anomaly detection and alert issuance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whispers in the Machine: Confidentiality in Agentic Systems</title>
<link>https://arxiv.org/abs/2402.06922</link>
<guid>https://arxiv.org/abs/2402.06922</guid>
<content:encoded><![CDATA[
arXiv:2402.06922v4 Announce Type: replace-cross 
Abstract: The interaction between users and applications is increasingly shifted toward natural language by deploying Large Language Models (LLMs) as the core interface. The capabilities of these so-called agents become more capable the more tools and services they serve as an interface for, ultimately leading to agentic systems. Agentic systems use LLM-based agents as interfaces for most user interactions and various integrations with external tools and services. While these interfaces can significantly enhance the capabilities of the agentic system, they also introduce a new attack surface. Manipulated integrations, for example, can exploit the internal LLM and compromise sensitive data accessed through other interfaces. While previous work primarily focused on attacks targeting a model's alignment or the leakage of training data, the security of data that is only available during inference has escaped scrutiny so far. In this work, we demonstrate how the integration of LLMs into systems with external tool integration poses a risk similar to established prompt-based attacks, able to compromise the confidentiality of the entire system. Introducing a systematic approach to evaluate these confidentiality risks, we identify two specific attack scenarios unique to these agentic systems and formalize these into a tool-robustness framework designed to measure a model's ability to protect sensitive information. Our analysis reveals significant vulnerabilities across all tested models, highlighting an increased risk when models are combined with external tools.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Large Language Models for Information Extraction from Real Estate Transactions</title>
<link>https://arxiv.org/abs/2404.18043</link>
<guid>https://arxiv.org/abs/2404.18043</guid>
<content:encoded><![CDATA[
arXiv:2404.18043v3 Announce Type: replace-cross 
Abstract: Real estate sales contracts contain crucial information for property transactions, but manual data extraction can be time-consuming and error-prone. This paper explores the application of large language models, specifically transformer-based architectures, for automated information extraction from real estate contracts. We discuss challenges, techniques, and future directions in leveraging these models to improve efficiency and accuracy in real estate contract analysis. We generated synthetic contracts using the real-world transaction dataset, thereby fine-tuning the large-language model and achieving significant metrics improvements and qualitative improvements in information retrieval and reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating lightweight unsupervised online IDS for masquerade attacks in CAN</title>
<link>https://arxiv.org/abs/2406.13778</link>
<guid>https://arxiv.org/abs/2406.13778</guid>
<content:encoded><![CDATA[
arXiv:2406.13778v3 Announce Type: replace-cross 
Abstract: Vehicular controller area networks (CANs) are susceptible to masquerade attacks by malicious adversaries. In masquerade attacks, adversaries silence a targeted ID and then send malicious frames with forged content at the expected timing of benign frames. As masquerade attacks could seriously harm vehicle functionality and are the stealthiest attacks to detect in CAN, recent work has devoted attention to compare frameworks for detecting masquerade attacks in CAN. However, most existing works report offline evaluations using CAN logs already collected using simulations that do not comply with the domain's real-time constraints. Here we contribute to advance the state of the art by presenting a comparative evaluation of four different non-deep learning (DL)-based unsupervised online intrusion detection systems (IDS) for masquerade attacks in CAN. Our approach differs from existing comparative evaluations in that we analyze the effect of controlling streaming data conditions in a sliding window setting. In doing so, we use realistic masquerade attacks being replayed from the ROAD dataset. We show that although evaluated IDS are not effective at detecting every attack type, the method that relies on detecting changes in the hierarchical structure of clusters of time series produces the best results at the expense of higher computational overhead. We discuss limitations, open challenges, and how the evaluated methods can be used for practical unsupervised online CAN IDS for masquerade attacks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention</title>
<link>https://arxiv.org/abs/2406.16258</link>
<guid>https://arxiv.org/abs/2406.16258</guid>
<content:encoded><![CDATA[
arXiv:2406.16258v3 Announce Type: replace-cross 
Abstract: Aligning robot behavior with human preferences is crucial for deploying embodied AI agents in human-centered environments. A promising solution is interactive imitation learning from human intervention, where a human expert observes the policy's execution and provides interventions as feedback. However, existing methods often fail to utilize the prior policy efficiently to facilitate learning, thus hindering sample efficiency. In this work, we introduce MEReQ (Maximum-Entropy Residual-Q Inverse Reinforcement Learning), designed for sample-efficient alignment from human intervention. Instead of inferring the complete human behavior characteristics, MEReQ infers a residual reward function that captures the discrepancy between the human expert's and the prior policy's underlying reward functions. It then employs Residual Q-Learning (RQL) to align the policy with human preferences using this residual reward function. Extensive evaluations on simulated and real-world tasks demonstrate that MEReQ achieves sample-efficient policy alignment from human intervention.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>fastkqr: A Fast Algorithm for Kernel Quantile Regression</title>
<link>https://arxiv.org/abs/2408.05393</link>
<guid>https://arxiv.org/abs/2408.05393</guid>
<content:encoded><![CDATA[
arXiv:2408.05393v2 Announce Type: replace-cross 
Abstract: Quantile regression is a powerful tool for robust and heterogeneous learning that has seen applications in a diverse range of applied areas. However, its broader application is often hindered by the substantial computational demands arising from the non-smooth quantile loss function. In this paper, we introduce a novel algorithm named fastkqr, which significantly advances the computation of quantile regression in reproducing kernel Hilbert spaces. The core of fastkqr is a finite smoothing algorithm that magically produces exact regression quantiles, rather than approximations. To further accelerate the algorithm, we equip fastkqr with a novel spectral technique that carefully reutilizes matrix computations. In addition, we extend fastkqr to accommodate a flexible kernel quantile regression with a data-driven crossing penalty, addressing the interpretability challenges of crossing quantile curves at multiple levels. We have implemented fastkqr in a publicly available R package. Extensive simulations and real applications show that fastkqr matches the accuracy of state-of-the-art algorithms but can operate up to an order of magnitude faster.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DFacePolicy: Audio-Driven 3D Facial Animation Based on Action Control</title>
<link>https://arxiv.org/abs/2409.10848</link>
<guid>https://arxiv.org/abs/2409.10848</guid>
<content:encoded><![CDATA[
arXiv:2409.10848v2 Announce Type: replace-cross 
Abstract: Audio-driven 3D facial animation has achieved significant progress in both research and applications. While recent baselines struggle to generate natural and continuous facial movements due to their frame-by-frame vertex generation approach, we propose 3DFacePolicy, a pioneer work that introduces a novel definition of vertex trajectory changes across consecutive frames through the concept of "action". By predicting action sequences for each vertex that encode frame-to-frame movements, we reformulate vertex generation approach into an action-based control paradigm. Specifically, we leverage a robotic control mechanism, diffusion policy, to predict action sequences conditioned on both audio and vertex states. Extensive experiments on VOCASET and BIWI datasets demonstrate that our approach significantly outperforms state-of-the-art methods and is particularly expert in dynamic, expressive and naturally smooth facial animations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph-based Motion Generation with Multi-modal Interaction Relational Reasoning</title>
<link>https://arxiv.org/abs/2409.11676</link>
<guid>https://arxiv.org/abs/2409.11676</guid>
<content:encoded><![CDATA[
arXiv:2409.11676v2 Announce Type: replace-cross 
Abstract: The intricate nature of real-world driving environments, characterized by dynamic and diverse interactions among multiple vehicles and their possible future states, presents considerable challenges in accurately predicting the motion states of vehicles and handling the uncertainty inherent in the predictions. Addressing these challenges requires comprehensive modeling and reasoning to capture the implicit relations among vehicles and the corresponding diverse behaviors. This research introduces an integrated framework for autonomous vehicles (AVs) motion prediction to address these complexities, utilizing a novel Relational Hypergraph Interaction-informed Neural mOtion generator (RHINO). RHINO leverages hypergraph-based relational reasoning by integrating a multi-scale hypergraph neural network to model group-wise interactions among multiple vehicles and their multi-modal driving behaviors, thereby enhancing motion prediction accuracy and reliability. Experimental validation using real-world datasets demonstrates the superior performance of this framework in improving predictive accuracy and fostering socially aware automated driving in dynamic traffic scenarios. The source code is publicly available at https://github.com/keshuw95/RHINO-Hypergraph-Motion-Generation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Touch and Tell: Multimodal Decoding of Human Emotions and Social Gestures for Robots</title>
<link>https://arxiv.org/abs/2412.03300</link>
<guid>https://arxiv.org/abs/2412.03300</guid>
<content:encoded><![CDATA[
arXiv:2412.03300v2 Announce Type: replace-cross 
Abstract: Human emotions are complex and can be conveyed through nuanced touch gestures. Previous research has primarily focused on how humans recognize emotions through touch or on identifying key features of emotional expression for robots. However, there is a gap in understanding how reliably these emotions and gestures can be communicated to robots via touch and interpreted using data driven methods. This study investigates the consistency and distinguishability of emotional and gestural expressions through touch and sound. To this end, we integrated a custom piezoresistive pressure sensor as well as a microphone on a social robot. Twenty-eight participants first conveyed ten different emotions to the robot using spontaneous touch gestures, then they performed six predefined social touch gestures. Our findings reveal statistically significant consistency in both emotion and gesture expression among participants. However, some emotions exhibited low intraclass correlation values, and certain emotions with similar levels of arousal or valence did not show significant differences in their conveyance. To investigate emotion and social gesture decoding within affective human-robot tactile interaction, we developed single-modality models and multimodal models integrating tactile and auditory features. A support vector machine (SVM) model trained on multimodal features achieved the highest accuracy for classifying ten emotions, reaching 40 %.For gesture classification, a Convolutional Neural Network- Long Short-Term Memory Network (CNN-LSTM) achieved 90.74 % accuracy. Our results demonstrate that even though the unimodal models have the potential to decode emotions and touch gestures, the multimodal integration of touch and sound significantly outperforms unimodal approaches, enhancing the decoding of both emotions and gestures.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos</title>
<link>https://arxiv.org/abs/2501.12254</link>
<guid>https://arxiv.org/abs/2501.12254</guid>
<content:encoded><![CDATA[
arXiv:2501.12254v3 Announce Type: replace-cross 
Abstract: Self-supervised learning holds the promise of learning good representations from real-world continuous uncurated data streams. However, most existing works in visual self-supervised learning focus on static images or artificial data streams. Towards exploring a more realistic learning substrate, we investigate streaming self-supervised learning from long-form real-world egocentric video streams. Inspired by the event segmentation mechanism in human perception and memory, we propose "Memory Storyboard" that groups recent past frames into temporal segments for more effective summarization of the past visual streams for memory replay. To accommodate efficient temporal segmentation, we propose a two-tier memory hierarchy: the recent past is stored in a short-term memory, and the storyboard temporal segments are then transferred to a long-term memory. Experiments on real-world egocentric video datasets including SAYCam and KrishnaCam show that contrastive learning objectives on top of storyboard frames result in semantically meaningful representations that outperform those produced by state-of-the-art unsupervised continual learning methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Covariance Estimation in Nonsmooth Stochastic Approximation</title>
<link>https://arxiv.org/abs/2502.05305</link>
<guid>https://arxiv.org/abs/2502.05305</guid>
<content:encoded><![CDATA[
arXiv:2502.05305v2 Announce Type: replace-cross 
Abstract: We consider applying stochastic approximation (SA) methods to solve nonsmooth variational inclusion problems. Existing studies have shown that the averaged iterates of SA methods exhibit asymptotic normality, with an optimal limiting covariance matrix in the local minimax sense of H\'ajek and Le Cam. However, no methods have been proposed to estimate this covariance matrix in a nonsmooth and potentially non-monotone (nonconvex) setting. In this paper, we study an online batch-means covariance matrix estimator introduced in Zhu et al.(2023). The estimator groups the SA iterates appropriately and computes the sample covariance among batches as an estimate of the limiting covariance. Its construction does not require prior knowledge of the total sample size, and updates can be performed recursively as new data arrives. We establish that, as long as the batch size sequence is properly specified (depending on the stepsize sequence), the estimator achieves a convergence rate of order $O(\sqrt{d}n^{-1/8+\varepsilon})$ for any $\varepsilon>0$, where $d$ and $n$ denote the problem dimensionality and the number of iterations (or samples) used. Although the problem is nonsmooth and potentially non-monotone (nonconvex), our convergence rate matches the best-known rate for covariance estimation methods using only first-order information in smooth and strongly-convex settings. The consistency of this covariance estimator enables asymptotically valid statistical inference, including constructing confidence intervals and performing hypothesis testing.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Tensor Completion via Approximate Richardson Iteration</title>
<link>https://arxiv.org/abs/2502.09534</link>
<guid>https://arxiv.org/abs/2502.09534</guid>
<content:encoded><![CDATA[
arXiv:2502.09534v2 Announce Type: replace-cross 
Abstract: We study tensor completion (TC) through the lens of low-rank tensor decomposition (TD). Many TD algorithms use fast alternating minimization methods to solve highly structured linear regression problems at each step (e.g., for CP, Tucker, and tensor-train decompositions). However, such algebraic structure is often lost in TC regression problems, making direct extensions unclear. This work proposes a novel lifting method for approximately solving TC regression problems using structured TD regression algorithms as blackbox subroutines, enabling sublinear-time methods. We analyze the convergence rate of our approximate Richardson iteration-based algorithm, and our empirical study shows that it can be 100x faster than direct methods for CP completion on real-world tensors.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Emotion Annotation in Facial Images Using Large Multimodal Models: Benchmarking and Prospects for Multi-Class, Multi-Frame Approaches</title>
<link>https://arxiv.org/abs/2502.12454</link>
<guid>https://arxiv.org/abs/2502.12454</guid>
<content:encoded><![CDATA[
arXiv:2502.12454v2 Announce Type: replace-cross 
Abstract: This study investigates the feasibility and performance of using large multimodal models (LMMs) to automatically annotate human emotions in everyday scenarios. We conducted experiments on the DailyLife subset of the publicly available FERV39k dataset, employing the GPT-4o-mini model for rapid, zero-shot labeling of key frames extracted from video segments. Under a seven-class emotion taxonomy ("Angry," "Disgust," "Fear," "Happy," "Neutral," "Sad," "Surprise"), the LMM achieved an average precision of approximately 50%. In contrast, when limited to ternary emotion classification (negative/neutral/positive), the average precision increased to approximately 64%. Additionally, we explored a strategy that integrates multiple frames within 1-2 second video clips to enhance labeling performance and reduce costs. The results indicate that this approach can slightly improve annotation accuracy. Overall, our preliminary findings highlight the potential application of zero-shot LMMs in human facial emotion annotation tasks, offering new avenues for reducing labeling costs and broadening the applicability of LMMs in complex multimodal environments.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Linguistic Calibration: Trading-off between Factuality and Specificity</title>
<link>https://arxiv.org/abs/2502.19110</link>
<guid>https://arxiv.org/abs/2502.19110</guid>
<content:encoded><![CDATA[
arXiv:2502.19110v4 Announce Type: replace-cross 
Abstract: Language model outputs are not always reliable, thus prompting research into how to adapt model responses based on uncertainty. Common approaches include: \emph{abstention}, where models refrain from generating responses when uncertain; and \emph{linguistic calibration}, where models hedge their statements using uncertainty quantifiers. However, abstention can withhold valuable information, while linguistically calibrated responses are often challenging to leverage in downstream tasks. We propose a unified view, Conformal Linguistic Calibration (CLC), which reinterprets linguistic calibration as \emph{answer set prediction}. First we present a framework connecting abstention and linguistic calibration through the lens of linguistic pragmatics. We then describe an implementation of CLC that allows for controlling the level of imprecision in model responses. Results demonstrate our method produces calibrated outputs with conformal guarantees on factual accuracy. Further, our approach enables fine-tuning models to perform uncertainty-aware adaptive claim rewriting, offering a controllable balance between factuality and specificity.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Prefrontal Control over Hippocampal Episodic Memory for Goal-Directed Generalization</title>
<link>https://arxiv.org/abs/2503.02303</link>
<guid>https://arxiv.org/abs/2503.02303</guid>
<content:encoded><![CDATA[
arXiv:2503.02303v3 Announce Type: replace-cross 
Abstract: Many tasks require flexibly modifying perception and behavior based on current goals. Humans can retrieve episodic memories from days to years ago, using them to contextualize and generalize behaviors across novel but structurally related situations. The brain's ability to control episodic memories based on task demands is often attributed to interactions between the prefrontal cortex (PFC) and hippocampus (HPC). We propose a reinforcement learning model that incorporates a PFC-HPC interaction mechanism for goal-directed generalization. In our model, the PFC learns to generate query-key representations to encode and retrieve goal-relevant episodic memories, modulating HPC memories top-down based on current task demands. Moreover, the PFC adapts its encoding and retrieval strategies dynamically when faced with multiple goals presented in a blocked, rather than interleaved, manner. Our results show that: (1) combining working memory with selectively retrieved episodic memory allows transfer of decisions among similar environments or situations, (2) top-down control from PFC over HPC improves learning of arbitrary structural associations between events for generalization to novel environments compared to a bottom-up sensory-driven approach, and (3) the PFC encodes generalizable representations during both encoding and retrieval of goal-relevant memories, whereas the HPC exhibits event-specific representations. Together, these findings highlight the importance of goal-directed prefrontal control over hippocampal episodic memory for decision-making in novel situations and suggest a computational mechanism by which PFC-HPC interactions enable flexible behavior.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gait in Eight: Efficient On-Robot Learning for Omnidirectional Quadruped Locomotion</title>
<link>https://arxiv.org/abs/2503.08375</link>
<guid>https://arxiv.org/abs/2503.08375</guid>
<content:encoded><![CDATA[
arXiv:2503.08375v2 Announce Type: replace-cross 
Abstract: On-robot Reinforcement Learning is a promising approach to train embodiment-aware policies for legged robots. However, the computational constraints of real-time learning on robots pose a significant challenge. We present a framework for efficiently learning quadruped locomotion in just 8 minutes of raw real-time training utilizing the sample efficiency and minimal computational overhead of the new off-policy algorithm CrossQ. We investigate two control architectures: Predicting joint target positions for agile, high-speed locomotion and Central Pattern Generators for stable, natural gaits. While prior work focused on learning simple forward gaits, our framework extends on-robot learning to omnidirectional locomotion. We demonstrate the robustness of our approach in different indoor and outdoor environments.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomised Postiterations for Calibrated BayesCG</title>
<link>https://arxiv.org/abs/2504.04247</link>
<guid>https://arxiv.org/abs/2504.04247</guid>
<content:encoded><![CDATA[
arXiv:2504.04247v2 Announce Type: replace-cross 
Abstract: The Bayesian conjugate gradient method offers probabilistic solutions to linear systems but suffers from poor calibration, limiting its utility in uncertainty quantification tasks. Recent approaches leveraging postiterations to construct priors have improved computational properties but failed to correct calibration issues. In this work, we propose a novel randomised postiteration strategy that enhances the calibration of the BayesCG posterior while preserving its favourable convergence characteristics. We present theoretical guarantees for the improved calibration, supported by results on the distribution of posterior errors. Numerical experiments demonstrate the efficacy of the method in both synthetic and inverse problem settings, showing enhanced uncertainty quantification and better propagation of uncertainties through computational pipelines.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIE: Semantic and Structural Post-Training of Image Editing Diffusion Models with AI feedback</title>
<link>https://arxiv.org/abs/2504.12833</link>
<guid>https://arxiv.org/abs/2504.12833</guid>
<content:encoded><![CDATA[
arXiv:2504.12833v2 Announce Type: replace-cross 
Abstract: This paper presents SPIE: a novel approach for semantic and structural post-training of instruction-based image editing diffusion models, addressing key challenges in alignment with user prompts and consistency with input images. We introduce an online reinforcement learning framework that aligns the diffusion model with human preferences without relying on extensive human annotations or curating a large dataset. Our method significantly improves the alignment with instructions and realism in two ways. First, SPIE captures fine nuances in the desired edit by leveraging a visual prompt, enabling detailed control over visual edits without lengthy textual prompts. Second, it achieves precise and structurally coherent modifications in complex scenes while maintaining high fidelity in instruction-irrelevant areas. This approach simplifies users' efforts to achieve highly specific edits, requiring only 5 reference images depicting a certain concept for training. Experimental results demonstrate that SPIE can perform intricate edits in complex scenes, after just 10 training steps. Finally, we showcase the versatility of our method by applying it to robotics, where targeted image edits enhance the visual realism of simulated environments, which improves their utility as proxy for real-world settings.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Learning on Large Graphs using a Densifying Regularity Lemma</title>
<link>https://arxiv.org/abs/2504.18273</link>
<guid>https://arxiv.org/abs/2504.18273</guid>
<content:encoded><![CDATA[
arXiv:2504.18273v2 Announce Type: replace-cross 
Abstract: Learning on large graphs presents significant challenges, with traditional Message Passing Neural Networks suffering from computational and memory costs scaling linearly with the number of edges. We introduce the Intersecting Block Graph (IBG), a low-rank factorization of large directed graphs based on combinations of intersecting bipartite components, each consisting of a pair of communities, for source and target nodes. By giving less weight to non-edges, we show how to efficiently approximate any graph, sparse or dense, by a dense IBG. Specifically, we prove a constructive version of the weak regularity lemma, showing that for any chosen accuracy, every graph, regardless of its size or sparsity, can be approximated by a dense IBG whose rank depends only on the accuracy. This dependence of the rank solely on the accuracy, and not on the sparsity level, is in contrast to previous forms of the weak regularity lemma. We present a graph neural network architecture operating on the IBG representation of the graph and demonstrating competitive performance on node classification, spatio-temporal graph analysis, and knowledge graph completion, while having memory and computational complexity linear in the number of nodes rather than edges.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey</title>
<link>https://arxiv.org/abs/2505.01821</link>
<guid>https://arxiv.org/abs/2505.01821</guid>
<content:encoded><![CDATA[
arXiv:2505.01821v3 Announce Type: replace-cross 
Abstract: Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay</title>
<link>https://arxiv.org/abs/2505.04209</link>
<guid>https://arxiv.org/abs/2505.04209</guid>
<content:encoded><![CDATA[
arXiv:2505.04209v3 Announce Type: replace-cross 
Abstract: E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). The relevance of advertiser keyphrases plays an important role in preventing the inundation of search systems with numerous irrelevant items that compete for attention in auctions, in addition to maintaining a healthy seller perception. In this work, we describe the shortcomings of training Advertiser keyphrase relevance filter models on click/sales/search relevance signals and the importance of aligning with human judgment, as sellers have the power to adopt or reject said keyphrase recommendations. In this study, we frame Advertiser keyphrase relevance as a complex interaction between 3 dynamical systems -- seller judgment, which influences seller adoption of our product, Advertising, which provides the keyphrases to bid on, and Search, who holds the auctions for the same keyphrases. This study discusses the practicalities of using human judgment via a case study at eBay Advertising and demonstrate that using LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our relevance models achieves a better harmony across the three systems -- provided that they are bound by a meticulous evaluation framework grounded in business metrics.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations</title>
<link>https://arxiv.org/abs/2505.06502</link>
<guid>https://arxiv.org/abs/2505.06502</guid>
<content:encoded><![CDATA[
arXiv:2505.06502v3 Announce Type: replace-cross 
Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super-Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional SR methods, even with limited training data (e.g., only 13% of training data is required to achieve performance similar to SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning by improving accuracy and efficiency, enhancing process understanding, and broadening applications to scientific research. We publicly release the complete source code of PC-SRGAN and all experiments at https://github.com/hasan-rakibul/PC-SRGAN.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics</title>
<link>https://arxiv.org/abs/2505.19317</link>
<guid>https://arxiv.org/abs/2505.19317</guid>
<content:encoded><![CDATA[
arXiv:2505.19317v2 Announce Type: replace-cross 
Abstract: Although popularized AI fairness metrics, e.g., demographic parity, have uncovered bias in AI-assisted decision-making outcomes, they do not consider how much effort one has spent to get to where one is today in the input feature space. However, the notion of effort is important in how Philosophy and humans understand fairness. We propose a philosophy-informed approach to conceptualize and evaluate Effort-aware Fairness (EaF), grounded in the concept of Force, which represents the temporal trajectory of predictive features coupled with inertia. Besides theoretical formulation, our empirical contributions include: (1) a pre-registered human subjects experiment, which shows that for both stages of the (individual) fairness evaluation process, people consider the temporal trajectory of a predictive feature more than its aggregate value; (2) pipelines to compute Effort-aware Individual/Group Fairness in the criminal justice and personal finance contexts. Our work may enable AI model auditors to uncover and potentially correct unfair decisions against individuals who have spent significant efforts to improve but are still stuck with systemic disadvantages outside their control.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness</title>
<link>https://arxiv.org/abs/2506.05735</link>
<guid>https://arxiv.org/abs/2506.05735</guid>
<content:encoded><![CDATA[
arXiv:2506.05735v2 Announce Type: replace-cross 
Abstract: Machine unlearning techniques aim to mitigate unintended memorization in large language models (LLMs). However, existing approaches predominantly focus on the explicit removal of isolated facts, often overlooking latent inferential dependencies and the non-deterministic nature of knowledge within LLMs. Consequently, facts presumed forgotten may persist implicitly through correlated information. To address these challenges, we propose a knowledge unlearning evaluation framework that more accurately captures the implicit structure of real-world knowledge by representing relevant factual contexts as knowledge graphs with associated confidence scores. We further develop an inference-based evaluation protocol leveraging powerful LLMs as judges; these judges reason over the extracted knowledge subgraph to determine unlearning success. Our LLM judges utilize carefully designed prompts and are calibrated against human evaluations to ensure their trustworthiness and stability. Extensive experiments on our newly constructed benchmark demonstrate that our framework provides a more realistic and rigorous assessment of unlearning performance. Moreover, our findings reveal that current evaluation strategies tend to overestimate unlearning effectiveness. Our code is publicly available at https://github.com/Graph-COM/Knowledge_Unlearning.git.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete and Continuous Difference of Submodular Minimization</title>
<link>https://arxiv.org/abs/2506.07952</link>
<guid>https://arxiv.org/abs/2506.07952</guid>
<content:encoded><![CDATA[
arXiv:2506.07952v2 Announce Type: replace-cross 
Abstract: Submodular functions, defined on continuous or discrete domains, arise in numerous applications. We study the minimization of the difference of two submodular (DS) functions, over both domains, extending prior work restricted to set functions. We show that all functions on discrete domains and all smooth functions on continuous domains are DS. For discrete domains, we observe that DS minimization is equivalent to minimizing the difference of two convex (DC) functions, as in the set function case. We propose a novel variant of the DC Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable theoretical guarantees as in the set function case. The algorithm can be applied to continuous domains via discretization. Experiments demonstrate that our method outperforms baselines in integer compressive sensing and integer least squares.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patient-Specific Deep Reinforcement Learning for Automatic Replanning in Head-and-Neck Cancer Proton Therapy</title>
<link>https://arxiv.org/abs/2506.10073</link>
<guid>https://arxiv.org/abs/2506.10073</guid>
<content:encoded><![CDATA[
arXiv:2506.10073v2 Announce Type: replace-cross 
Abstract: Anatomical changes during intensity-modulated proton therapy (IMPT) for head-and-neck cancer (HNC) can shift Bragg peaks, risking tumor underdosing and organ-at-risk overdosing. Treatment replanning is often required to maintain clinically acceptable treatment quality. However, current manual replanning processes are resource-intensive and time-consuming. We propose a patient-specific deep reinforcement learning (DRL) framework for automated IMPT replanning, with a reward-shaping mechanism based on a $150$-point plan quality score addressing competing clinical objectives. We formulate the planning process as a reinforcement learning problem where agents learn control policies to adjust optimization priorities, maximizing plan quality. Unlike population-based approaches, our framework trains agents for each patient using their planning Computed Tomography (CT) and augmented anatomies simulating anatomical changes (tumor progression and regression). This patient-specific approach leverages anatomical similarities along the treatment course, enabling effective plan adaptation. We implemented two DRL algorithms, Deep Q-Network and Proximal Policy Optimization, using dose-volume histograms (DVHs) as state representations and a $22$-dimensional action space of priority adjustments. Evaluation on eight HNC patients using actual replanning CT data showed that both agents improved initial plan scores from $120.78 \pm 17.18$ to $139.59 \pm 5.50$ (DQN) and $141.50 \pm 4.69$ (PPO), surpassing the replans manually generated by a human planner ($136.32 \pm 4.79$). Clinical validation confirms that improvements translate to better tumor coverage and OAR sparing across diverse anatomical changes. This work highlights DRL's potential in addressing geometric and dosimetric complexities of adaptive proton therapy, offering efficient offline adaptation solutions and advancing online adaptive proton therapy.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition</title>
<link>https://arxiv.org/abs/2506.14412</link>
<guid>https://arxiv.org/abs/2506.14412</guid>
<content:encoded><![CDATA[
arXiv:2506.14412v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55 in the non-human evaluation, placing it overall in third place in the SIGIR 2025 LiveRAG Challenge.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?</title>
<link>https://arxiv.org/abs/2506.14805</link>
<guid>https://arxiv.org/abs/2506.14805</guid>
<content:encoded><![CDATA[
arXiv:2506.14805v2 Announce Type: replace-cross 
Abstract: As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Effective Query Context-Aware Learning-to-Rank Model for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2507.03789</link>
<guid>https://arxiv.org/abs/2507.03789</guid>
<content:encoded><![CDATA[
arXiv:2507.03789v2 Announce Type: replace-cross 
Abstract: Modern sequential recommender systems commonly use transformer-based models for next-item prediction. While these models demonstrate a strong balance between efficiency and quality, integrating interleaving features - such as the query context (e.g., browse category) under which next-item interactions occur - poses challenges. Effectively capturing query context is crucial for refining ranking relevance and enhancing user engagement, as it provides valuable signals about user intent within a session. Unlike item features, historical query context is typically not aligned with item sequences and may be unavailable at inference due to privacy constraints or feature store limitations - making its integration into transformers both challenging and error-prone. This paper analyzes different strategies for incorporating query context into transformers trained with a causal language modeling procedure as a case study. We propose a new method that effectively fuses the item sequence with query context within the attention mechanism. Through extensive offline and online experiments on a large-scale online platform and open datasets, we present evidence that our proposed method is an effective approach for integrating query context to improve model ranking quality in terms of relevance and diversity.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Pedagogy: Dialogic Social Learning for Artificial Agents</title>
<link>https://arxiv.org/abs/2507.21065</link>
<guid>https://arxiv.org/abs/2507.21065</guid>
<content:encoded><![CDATA[
arXiv:2507.21065v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uni-Mol3: A Multi-Molecular Foundation Model for Advancing Organic Reaction Modeling</title>
<link>https://arxiv.org/abs/2508.00920</link>
<guid>https://arxiv.org/abs/2508.00920</guid>
<content:encoded><![CDATA[
arXiv:2508.00920v2 Announce Type: replace-cross 
Abstract: Organic reaction, the foundation of modern chemical industry, is crucial for new material development and drug discovery. However, deciphering reaction mechanisms and modeling multi-molecular relationships remain formidable challenges due to the complexity of molecular dynamics. While several state-of-the-art models like Uni-Mol2 have revolutionized single-molecular representation learning, their extension to multi-molecular systems, where chemical reactions inherently occur, has been underexplored. This paper introduces Uni-Mol3, a novel deep learning framework that employs a hierarchical pipeline for multi-molecular reaction modeling. At its core, Uni-Mol3 adopts a multi-scale molecular tokenizer (Mol-Tokenizer) that encodes 3D structures of molecules and other features into discrete tokens, creating a 3D-aware molecular language. The framework innovatively combines two pre-training stages: molecular pre-training to learn the molecular grammars and reaction pre-training to capture fundamental reaction principles, forming a progressive learning paradigm from single- to multi-molecular systems. With prompt-aware downstream fine-tuning, Uni-Mol3 demonstrates exceptional performance in diverse organic reaction tasks and supports multi-task prediction with strong generalizability. Experimental results across 10 datasets spanning 4 downstream tasks show that Uni-Mol3 outperforms existing methods, validating its effectiveness in modeling complex organic reactions. This work not only ushers in an alternative paradigm for multi-molecular computational modeling but also charts a course for intelligent organic reaction by bridging molecular representation with reaction mechanism understanding.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trainable Dynamic Mask Sparse Attention</title>
<link>https://arxiv.org/abs/2508.02124</link>
<guid>https://arxiv.org/abs/2508.02124</guid>
<content:encoded><![CDATA[
arXiv:2508.02124v2 Announce Type: replace-cross 
Abstract: In large language models, the demand for modeling long contexts is constantly increasing, but the quadratic complexity of the standard self-attention mechanism often becomes a bottleneck. Although existing sparse attention mechanisms have improved efficiency, they may still encounter issues such as static patterns or information loss. We introduce a trainable dynamic mask sparse attention mechanism, Dynamic Mask Attention, which effectively utilizes content-aware and position-aware sparsity. DMA achieves this through two key innovations: First, it dynamically generates content-aware sparse masks from value representations, enabling the model to identify and focus on critical information adaptively. Second, it implements position-aware sparse attention computation that effectively skips unnecessary calculation regions. This dual-sparsity design allows the model to significantly reduce the computational complexity of important information while retaining complete information, achieving an excellent balance between information fidelity and computational efficiency. We have verified the performance of DMA through comprehensive experiments. Comparative studies show that DMA outperforms multi-head attention, sliding window attention, multi-head latent attention, and native sparse attention in terms of perplexity under Chinchilla Scaling Law settings. Moreover, in challenging multi-query associative recall tasks, DMA also demonstrates superior performance and efficiency compared to these methods. Crucially, in the evaluation of a 1.7B parameter model, DMA significantly outperforms multi-head attention in both standard benchmark performance and the challenging needle-in-a-haystack task. These experimental results highlight its capability to balance model efficiency and long-context modeling ability effectively.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BoostTransformer: Enhancing Transformer Models with Subgrid Selection and Importance Sampling</title>
<link>https://arxiv.org/abs/2508.02924</link>
<guid>https://arxiv.org/abs/2508.02924</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer, NLP, BoostTransformer, text classification, boosting

Summary:
BoostTransformer is a novel framework that enhances transformer architectures in NLP by incorporating boosting principles. Through subgrid token selection and importance-weighted sampling, BoostTransformer integrates a least square boosting objective directly into the transformer pipeline. This approach enables more efficient training and improved performance, with faster convergence and higher accuracy on multiple fine-grained text classification benchmarks compared to standard transformers. BoostTransformer achieves these advancements without the need for extensive computational resources or intricate hyperparameter tuning, minimizing architectural search overhead. This method proves to be a promising solution for enhancing transformer architectures in NLP tasks. <br /><br />Summary: <div>
arXiv:2508.02924v2 Announce Type: replace 
Abstract: Transformer architectures dominate modern NLP but often demand heavy computational resources and intricate hyperparameter tuning. To mitigate these challenges, we propose a novel framework, BoostTransformer, that augments transformers with boosting principles through subgrid token selection and importance-weighted sampling. Our method incorporates a least square boosting objective directly into the transformer pipeline, enabling more efficient training and improved performance. Across multiple fine-grained text classification benchmarks, BoostTransformer demonstrates both faster convergence and higher accuracy, surpassing standard transformers while minimizing architectural search overhead.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation</title>
<link>https://arxiv.org/abs/2508.03104</link>
<guid>https://arxiv.org/abs/2508.03104</guid>
<content:encoded><![CDATA[
<div> Keywords: Contrastive learning, Hypergraph, Text-attributed, Semantic-aware augmentation, Self-supervised learning 

Summary:
HiTeC is a new framework for self-supervised learning on text-attributed hypergraphs (TAHGs), addressing limitations of existing methods. It introduces a two-stage hierarchical contrastive learning approach with semantic-aware data augmentation to improve representation quality. The first stage pre-trains the text encoder with a structure-aware contrastive objective to capture correlations between text and hypergraph topology. The second stage introduces semantic-aware augmentation strategies to enhance informative view generation. A multi-scale contrastive loss is proposed to capture long-range dependencies by extending existing objectives. By decoupling text encoder pretraining from hypergraph contrastive learning, HiTeC improves scalability without compromising representation quality. Experimental results validate the effectiveness of the proposed framework. <div>
arXiv:2508.03104v2 Announce Type: replace 
Abstract: Contrastive learning (CL) has become a dominant paradigm for self-supervised hypergraph learning, enabling effective training without costly labels. However, node entities in real-world hypergraphs are often associated with rich textual information, which is overlooked in prior works. Directly applying existing CL-based methods to such text-attributed hypergraphs (TAHGs) leads to three key limitations: (1) The common use of graph-agnostic text encoders overlooks the correlations between textual content and hypergraph topology, resulting in suboptimal representations. (2) Their reliance on random data augmentations introduces noise and weakens the contrastive objective. (3) The primary focus on node- and hyperedge-level contrastive signals limits the ability to capture long-range dependencies, which is essential for expressive representation learning. Although HyperBERT pioneers CL on TAHGs, its co-training paradigm suffers from poor scalability. To fill the research gap, we introduce HiTeC, a two-stage hierarchical contrastive learning framework with semantic-aware augmentation for scalable and effective self-supervised learning on TAHGs. In the first stage, we pre-train the text encoder with a structure-aware contrastive objective to overcome the graph-agnostic nature of conventional methods. In the second stage, we introduce two semantic-aware augmentation strategies, including prompt-enhanced text augmentation and semantic-aware hyperedge drop, to facilitate informative view generation. Furthermore, we propose a multi-scale contrastive loss that extends existing objectives with an $s$-walk-based subgraph-level contrast to better capture long-range dependencies. By decoupling text encoder pretraining from hypergraph contrastive learning, this two-stage design enhances scalability without compromising representation quality. Extensive experiments confirm the effectiveness of HiTeC.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thompson Exploration with Best Challenger Rule in Best Arm Identification</title>
<link>https://arxiv.org/abs/2310.00539</link>
<guid>https://arxiv.org/abs/2310.00539</guid>
<content:encoded><![CDATA[
<div> Thompson sampling, best arm identification, bandit framework, exponential models, computationally efficient<br />
<br />
Summary:<br />
This article addresses the fixed-confidence best arm identification (BAI) problem in bandit frameworks, focusing on single-parameter exponential models. Traditional BAI policies often involve significant computational overhead or exploration requirements, complicating real-world implementation. The proposed policy combines Thompson sampling with the best challenger rule, offering a novel approach that balances exploration and exploitation efficiently. It demonstrates asymptotic optimality for two-armed bandit problems and near optimality for general cases with three or more arms. Empirical evaluations showcase its competitive performance in terms of sample complexity and computational cost, underscoring its practical advantages. The concept of $\beta$-optimality is highlighted as a relaxed notion of asymptotic optimality, providing additional insights into policy comparisons and analysis. <div>
arXiv:2310.00539v4 Announce Type: replace-cross 
Abstract: This paper studies the fixed-confidence best arm identification (BAI) problem in the bandit framework in the canonical single-parameter exponential models. For this problem, many policies have been proposed, but most of them require solving an optimization problem at every round and/or are forced to explore an arm at least a certain number of times except those restricted to the Gaussian model. To address these limitations, we propose a novel policy that combines Thompson sampling with a computationally efficient approach known as the best challenger rule. While Thompson sampling was originally considered for maximizing the cumulative reward, we demonstrate that it can be used to naturally explore arms in BAI without forcing it. We show that our policy is asymptotically optimal for any two-armed bandit problems and achieves near optimality for general $K$-armed bandit problems for $K\geq 3$. Nevertheless, in numerical experiments, our policy shows competitive performance compared to asymptotically optimal policies in terms of sample complexity while requiring less computation cost. In addition, we highlight the advantages of our policy by comparing it to the concept of $\beta$-optimality, a relaxed notion of asymptotic optimality commonly considered in the analysis of a class of policies including the proposed one.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpectrumFM: Redefining Spectrum Cognition via Foundation Modeling</title>
<link>https://arxiv.org/abs/2508.02742</link>
<guid>https://arxiv.org/abs/2508.02742</guid>
<content:encoded><![CDATA[
<div> Keywords: spectrum efficiency, secure spectrum utilization, spectrum cognition, SpectrumFM, convolutional neural networks<br />
<br />
Summary: SpectrumFM is proposed as a spectrum foundation model to improve spectrum cognition. It incorporates a spectrum encoder utilizing convolutional neural networks and multi-head self attention mechanisms to capture fine-grained local structures and global dependencies in spectrum data. Self-supervised learning tasks such as masked reconstruction and next-slot signal prediction are used for pre-training, enhancing the model's adaptability. Low-rank adaptation (LoRA) parameter-efficient fine-tuning allows SpectrumFM to seamlessly adapt to different spectrum cognition tasks like spectrum sensing, anomaly detection, and wireless technology classification. Experimental results show significant improvements over existing methods, with a 30% increase in detection probability for spectrum sensing, over 10% boost in area under the curve for anomaly detection, and a 9.6% enhancement in wireless technology classification accuracy.<br /><br />Summary: <div>
arXiv:2508.02742v2 Announce Type: replace-cross 
Abstract: The enhancement of spectrum efficiency and the realization of secure spectrum utilization are critically dependent on spectrum cognition. However, existing spectrum cognition methods often exhibit limited generalization and suboptimal accuracy when deployed across diverse spectrum environments and tasks. To overcome these challenges, we propose a spectrum foundation model, termed SpectrumFM, which provides a new paradigm for spectrum cognition. An innovative spectrum encoder that exploits the convolutional neural networks and the multi-head self attention mechanisms is proposed to effectively capture both fine-grained local signal structures and high-level global dependencies in the spectrum data. To enhance its adaptability, two novel self-supervised learning tasks, namely masked reconstruction and next-slot signal prediction, are developed for pre-training SpectrumFM, enabling the model to learn rich and transferable representations. Furthermore, low-rank adaptation (LoRA) parameter-efficient fine-tuning is exploited to enable SpectrumFM to seamlessly adapt to various downstream spectrum cognition tasks, including spectrum sensing (SS), anomaly detection (AD), and wireless technology classification (WTC). Extensive experiments demonstrate the superiority of SpectrumFM over state-of-the-art methods. Specifically, it improves detection probability in the SS task by 30% at -4 dB signal-to-noise ratio (SNR), boosts the area under the curve (AUC) in the AD task by over 10%, and enhances WTC accuracy by 9.6%.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exponential convergence rate for Iterative Markovian Fitting</title>
<link>https://arxiv.org/abs/2508.02770</link>
<guid>https://arxiv.org/abs/2508.02770</guid>
<content:encoded><![CDATA[
<div> algorithm, Schrdinger bridge, discrete-time, convergence, exponential<br />
<br />
Summary: 
The article discusses the discrete-time Schrdinger bridge problem on a finite state space and focuses on the Iterative Markovian Fitting (IMF) algorithm's convergence. While previous knowledge confirmed the convergence of IMF to the ground truth solution, this study quantifies the speed of convergence for the first time. The research reveals that IMF demonstrates exponential convergence with a specific contraction factor, shedding light on the efficiency and reliability of the algorithm in solving the Schrdinger bridge problem in discrete time. <div>
arXiv:2508.02770v2 Announce Type: replace-cross 
Abstract: We consider the discrete-time Schr\"odinger bridge problem on a finite state space. Although it has been known that the Iterative Markovian Fitting (IMF) algorithm converges in Kullback-Leibler divergence to the ground truth solution, the speed of that convergence remained unquantified. In this work, we establish for the first time that IMF exhibits exponential convergence with an explicit contraction factor.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for Advertiser Keyphrase Recommendations at eBay</title>
<link>https://arxiv.org/abs/2508.03628</link>
<guid>https://arxiv.org/abs/2508.03628</guid>
<content:encoded><![CDATA[
<div> LLM, keyphrase recommendations, eBay, bias, multi-task training<br />
<br />
Summary: 
This study focuses on the importance of providing sellers on eBay with relevant keyphrase recommendations to improve advertising campaign performance. To address biases in click data and enhance the accuracy of keyphrase suggestions, a two-step LLM distillation process is introduced. By training a bi-encoder model using a multi-task approach, knowledge is distilled from an LLM teacher through a cross-encoder assistant to improve the retrieval of relevant keyphrases. The study demonstrates that integrating LLM distillation in a multi-task training setup can enhance the performance of the bi-encoder model in recommending keyphrases for advertisers on eBay. By aligning seller and search judgments, this approach aims to mitigate the overcrowding of search systems with irrelevant items and maintain a positive perception of sellers on eBay. <div>
arXiv:2508.03628v2 Announce Type: replace-cross 
Abstract: Sellers at eBay are recommended keyphrases to bid on to enhance the performance of their advertising campaigns. The relevance of these keyphrases is crucial in avoiding the overcrowding of search systems with irrelevant items and maintaining a positive seller perception. It is essential that keyphrase recommendations align with both seller and Search judgments regarding auctions. Due to the difficulty in procuring negative human judgment at scale, employing LLM-as-a-judge to mimic seller judgment has been established as the norm in several studies. This study introduces a novel two-step LLM distillation process from a LLM-judge used to debias our Embedding Based Retrieval (EBR) model from the various biases that exist in click-data. We distill from an LLM teacher via a cross-encoder assistant into a bi-encoder student using a multi-task training approach, ultimately employing the student bi-encoder to retrieve relevant advertiser keyphrases. We show that integrating a knowledge distillation process from LLMs in a multi-task training setup enhances bi-encoder performance in retrieving relevant advertiser keyphrases at eBay.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems</title>
<link>https://arxiv.org/abs/2508.06539</link>
<guid>https://arxiv.org/abs/2508.06539</guid>
<content:encoded><![CDATA[
<div> Keywords: survival, self-organizing survival manifolds, geodesic flows, biological constraints, geometric phase transitions<br />
Summary:<br />
This paper introduces a new approach to modeling survival that challenges traditional supervised learning methods. The authors propose that survival is not an externally annotated target but rather an emergent property of biological state space. They develop a theory of Self-Organizing Survival Manifolds (SOSM), where survival dynamics arise from low-curvature geodesic flows on latent manifolds shaped by internal biological constraints. A survival energy functional based on geodesic curvature minimization is introduced to induce structures aligning prognosis with geometric flow stability. The framework connects survival modeling to physical laws, including thermodynamics, entropy flow, Ricci curvature, and optimal transport. Health, disease, aging, and death are reframed as geometric phase transitions within the manifold's structure. This theory offers a universal, label-free foundation for survival modeling, bridging machine learning, biophysics, and the geometry of life itself.<br /> <div>
arXiv:2508.06539v1 Announce Type: new 
Abstract: Survival is traditionally modeled as a supervised learning task, reliant on curated outcome labels and fixed covariates. This work rejects that premise. It proposes that survival is not an externally annotated target but a geometric consequence: an emergent property of the curvature and flow inherent in biological state space. We develop a theory of Self-Organizing Survival Manifolds (SOSM), in which survival-relevant dynamics arise from low-curvature geodesic flows on latent manifolds shaped by internal biological constraints. A survival energy functional based on geodesic curvature minimization is introduced and shown to induce structures where prognosis aligns with geometric flow stability. We derive discrete and continuous formulations of the objective and prove theoretical results demonstrating the emergence and convergence of survival-aligned trajectories under biologically plausible conditions. The framework draws connections to thermodynamic efficiency, entropy flow, Ricci curvature, and optimal transport, grounding survival modeling in physical law. Health, disease, aging, and death are reframed as geometric phase transitions in the manifold's structure. This theory offers a universal, label-free foundation for modeling survival as a property of form, not annotation-bridging machine learning, biophysics, and the geometry of life itself.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering</title>
<link>https://arxiv.org/abs/2508.06574</link>
<guid>https://arxiv.org/abs/2508.06574</guid>
<content:encoded><![CDATA[
<div> Keywords: fraud detection, supply chains, semi-supervised learning, anomaly detection, Isolation Forest 

Summary: 
The paper presents a novel two-phase learning framework for fraud detection in supply chains, addressing challenges such as class imbalance and limited supervision. In the first phase, the Isolation Forest algorithm is used for unsupervised anomaly detection to identify potential fraud cases and reduce data volume. The second phase employs a self-training Support Vector Machine (SVM) to refine predictions using labeled and high-confidence pseudo-labeled samples, enabling robust semi-supervised learning. Evaluation on the DataCo Smart Supply Chain Dataset shows an F1-score of 0.817 with a low false positive rate. The study highlights the effectiveness of combining unsupervised pre-filtering and semi-supervised refinement in real-world supply chain fraud detection. However, it acknowledges limitations such as concept drift and the necessity of comparing with deep learning approaches.<br /><br />Summary: <div>
arXiv:2508.06574v1 Announce Type: new 
Abstract: Detecting fraud in modern supply chains is a growing challenge, driven by the complexity of global networks and the scarcity of labeled data. Traditional detection methods often struggle with class imbalance and limited supervision, reducing their effectiveness in real-world applications. This paper proposes a novel two-phase learning framework to address these challenges. In the first phase, the Isolation Forest algorithm performs unsupervised anomaly detection to identify potential fraud cases and reduce the volume of data requiring further analysis. In the second phase, a self-training Support Vector Machine (SVM) refines the predictions using both labeled and high-confidence pseudo-labeled samples, enabling robust semi-supervised learning. The proposed method is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive real-world supply chain dataset with fraud indicators. It achieves an F1-score of 0.817 while maintaining a false positive rate below 3.0%. These results demonstrate the effectiveness and efficiency of combining unsupervised pre-filtering with semi-supervised refinement for supply chain fraud detection under real-world constraints, though we acknowledge limitations regarding concept drift and the need for comparison with deep learning approaches.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GFlowNets for Learning Better Drug-Drug Interaction Representations</title>
<link>https://arxiv.org/abs/2508.06576</link>
<guid>https://arxiv.org/abs/2508.06576</guid>
<content:encoded><![CDATA[
<div> Keywords: Drug-drug interactions, class imbalance, predictive models, Generative Flow Networks, Variational Graph Autoencoders 

Summary: 
Drug-drug interactions (DDIs) are a significant challenge in clinical pharmacology due to class imbalance among interaction types. Existing predictive models struggle with rare but critical interaction types, leading to poor performance on infrequent cases. This study proposes a novel framework that combines Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE) to address this issue. By generating synthetic samples for rare classes, the model achieves better balance, improves predictive performance across interaction types, and ensures better clinical reliability. This approach enhances the effectiveness and novelty of DDI predictions by considering class-specific nuances, thereby reducing bias towards frequent interactions. Overall, the integration of GFlowNet and VGAE offers a promising solution to enhance DDI prediction accuracy and clinical relevance. 

<br /><br />Summary: <div>
arXiv:2508.06576v1 Announce Type: new 
Abstract: Drug-drug interactions pose a significant challenge in clinical pharmacology, with severe class imbalance among interaction types limiting the effectiveness of predictive models. Common interactions dominate datasets, while rare but critical interactions remain underrepresented, leading to poor model performance on infrequent cases. Existing methods often treat DDI prediction as a binary problem, ignoring class-specific nuances and exacerbating bias toward frequent interactions. To address this, we propose a framework combining Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE) to generate synthetic samples for rare classes, improving model balance and generate effective and novel DDI pairs. Our approach enhances predictive performance across interaction types, ensuring better clinical reliability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph Neural Network with State Space Models for Node Classification</title>
<link>https://arxiv.org/abs/2508.06587</link>
<guid>https://arxiv.org/abs/2508.06587</guid>
<content:encoded><![CDATA[
<div> role-based features, graph neural networks, hypergraph neural network, node classification, state space model

Summary:
HGMN is introduced as a novel hypergraph neural network that integrates role-aware representations into graph neural networks and a state space model. It addresses the limitations of existing methods by leveraging hypergraph construction techniques, combining role-based and adjacency-based representations through a learnable transformer mechanism. By utilizing two hypergraph construction methods based on node degree and neighborhood levels, HGMN strengthens connections among nodes with similar roles, enhancing representational power. Incorporating hypergraph convolution layers allows the model to capture complex dependencies within hypergraph structures. To address over-smoothing, a residual network is included for improved stability and feature propagation across layers. Extensive experiments demonstrate HGMN's superiority in node classification tasks, outperforming state-of-the-art GNN methods and highlighting its versatility and power for various graph-based learning applications. 

<br /><br />Summary: <div>
arXiv:2508.06587v1 Announce Type: new 
Abstract: In recent years, graph neural networks (GNNs) have gained significant attention for node classification tasks on graph-structured data. However, traditional GNNs primarily focus on adjacency relationships between nodes, often overlooking the rich role-based characteristics that are crucial for learning more expressive node representations. Existing methods for capturing role-based features are largely unsupervised and fail to achieve optimal performance in downstream tasks. To address these limitations, we propose a novel hypergraph neural network with state space model (HGMN) that effectively integrates role-aware representations into GNNs and the state space model. HGMN utilizes hypergraph construction techniques to model higher-order relationships and combines role-based and adjacency-based representations through a learnable mamba transformer mechanism. By leveraging two distinct hypergraph construction methods-based on node degree and neighborhood levels, it strengthens the connections among nodes with similar roles, enhancing the model's representational power. Additionally, the inclusion of hypergraph convolution layers enables the model to capture complex dependencies within hypergraph structures. To mitigate the over-smoothing problem inherent in deep GNNs, we incorporate a residual network, ensuring improved stability and better feature propagation across layers. Extensive experiments conducted on one newly introduced dataset and four benchmark datasets demonstrate the superiority of HGMN. The model achieves significant performance improvements on node classification tasks compared to state-of-the-art GNN methods. These results highlight HGMN's ability to provide enriched node representations by effectively embedding role-based features alongside adjacency information, making it a versatile and powerful tool for a variety of graph-based learning applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning</title>
<link>https://arxiv.org/abs/2508.06588</link>
<guid>https://arxiv.org/abs/2508.06588</guid>
<content:encoded><![CDATA[
<div> Vector Quantization, graph data, codebook collapse, RGVQ, graph token representations <br />
Summary: 
This paper addresses the issue of codebook collapse in Vector Quantization (VQ) applied to graph data. The study shows that codebook collapse is consistent in graph VQ due to early assignment imbalances and optimization loops. To tackle this, the authors propose RGVQ, a framework that incorporates graph topology and feature similarity to enhance codebook utilization. RGVQ introduces soft assignments using Gumbel-Softmax reparameterization and includes structure-aware contrastive regularization to penalize token co-assignments. Experimental results demonstrate that RGVQ significantly improves codebook utilization and enhances the performance of graph VQ backbones across various tasks, leading to more expressive and transferable graph token representations.<br /><br />Summary: <div>
arXiv:2508.06588v1 Announce Type: new 
Abstract: Vector Quantization (VQ) has recently emerged as a promising approach for learning discrete representations of graph-structured data. However, a fundamental challenge, i.e., codebook collapse, remains underexplored in the graph domain, significantly limiting the expressiveness and generalization of graph tokens.In this paper, we present the first empirical study showing that codebook collapse consistently occurs when applying VQ to graph data, even with mitigation strategies proposed in vision or language domains. To understand why graph VQ is particularly vulnerable to collapse, we provide a theoretical analysis and identify two key factors: early assignment imbalances caused by redundancy in graph features and structural patterns, and self-reinforcing optimization loops in deterministic VQ. To address these issues, we propose RGVQ, a novel framework that integrates graph topology and feature similarity as explicit regularization signals to enhance codebook utilization and promote token diversity. RGVQ introduces soft assignments via Gumbel-Softmax reparameterization, ensuring that all codewords receive gradient updates. In addition, RGVQ incorporates a structure-aware contrastive regularization to penalize the token co-assignments among similar node pairs. Extensive experiments demonstrate that RGVQ substantially improves codebook utilization and consistently boosts the performance of state-of-the-art graph VQ backbones across multiple downstream tasks, enabling more expressive and transferable graph token representations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis</title>
<link>https://arxiv.org/abs/2508.06589</link>
<guid>https://arxiv.org/abs/2508.06589</guid>
<content:encoded><![CDATA[
<div> framework, neuroimaging, CAD systems, federated learning, subtype heterogeneity<br />
Summary:<br />
The article introduces a novel federated learning framework for neuroimaging CAD systems to address challenges in small-sample studies and data heterogeneity. The framework includes a dynamic navigation module to route samples to suitable local models based on subtype representations and a meta-integration module to combine predictions from local models. Evaluation on fMRI data from MDD patients and healthy controls shows improved diagnostic accuracy and robustness compared to traditional methods. The framework achieves an average accuracy of 74.06% across all sites, demonstrating effectiveness in handling subtype heterogeneity and enhancing model generalizability. Ablation studies confirm the importance of dynamic navigation and meta-integration modules in performance improvement. By addressing data heterogeneity and subtype confounding, the framework advances reliable neuroimaging CAD systems for personalized medicine and clinical decision-making in neurology and psychiatry.<br /> <div>
arXiv:2508.06589v1 Announce Type: new 
Abstract: Computer-aided diagnosis (CAD) systems play a crucial role in analyzing neuroimaging data for neurological and psychiatric disorders. However, small-sample studies suffer from low reproducibility, while large-scale datasets introduce confounding heterogeneity due to multiple disease subtypes being labeled under a single category. To address these challenges, we propose a novel federated learning framework tailored for neuroimaging CAD systems. Our approach includes a dynamic navigation module that routes samples to the most suitable local models based on latent subtype representations, and a meta-integration module that combines predictions from heterogeneous local models into a unified diagnostic output. We evaluated our framework using a comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100 healthy controls across multiple study cohorts. Experimental results demonstrate significant improvements in diagnostic accuracy and robustness compared to traditional methods. Specifically, our framework achieved an average accuracy of 74.06\% across all tested sites, showcasing its effectiveness in handling subtype heterogeneity and enhancing model generalizability. Ablation studies further confirmed the importance of both the dynamic navigation and meta-integration modules in improving performance. By addressing data heterogeneity and subtype confounding, our framework advances reliable and reproducible neuroimaging CAD systems, offering significant potential for personalized medicine and clinical decision-making in neurology and psychiatry.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials</title>
<link>https://arxiv.org/abs/2508.06591</link>
<guid>https://arxiv.org/abs/2508.06591</guid>
<content:encoded><![CDATA[
<div> framework, generative AI, materials science, bioinspired materials, human-AI collaboration
Summary:
The article introduces a framework that integrates generative AI with literature from various fields to extract insights and design experiments for materials science. The focus is on humidity-responsive systems like pollen-based materials and Rhapis excelsa leaves, which exhibit self-actuation. By using AI tools such as BioinspiredLLM and Retrieval-Augmented Generation, the framework extracts structure-property relationships to create new classes of bioinspired materials. Through structured inference protocols, novel and experimentally feasible ideas are generated and evaluated. Real-world implementation of LLM-generated procedures resulted in the fabrication of a novel pollen-based adhesive with tunable morphology and shear strength. This work showcases the potential of AI-assisted ideation in driving materials design and facilitating effective human-AI collaboration.
<br /><br />Summary: <div>
arXiv:2508.06591v1 Announce Type: new 
Abstract: Large language models (LLMs) have reshaped the research landscape by enabling new approaches to knowledge retrieval and creative ideation. Yet their application in discipline-specific experimental science, particularly in highly multi-disciplinary domains like materials science, remains limited. We present a first-of-its-kind framework that integrates generative AI with literature from hitherto-unconnected fields such as plant science, biomimetics, and materials engineering to extract insights and design experiments for materials. We focus on humidity-responsive systems such as pollen-based materials and Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and adaptive performance. Using a suite of AI tools, including a fine-tuned model (BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a Hierarchical Sampling strategy, we extract structure-property relationships and translate them into new classes of bioinspired materials. Structured inference protocols generate and evaluate hundreds of hypotheses from a single query, surfacing novel and experimentally tractable ideas. We validate our approach through real-world implementation: LLM-generated procedures, materials designs, and mechanical predictions were tested in the laboratory, culminating in the fabrication of a novel pollen-based adhesive with tunable morphology and measured shear strength, establishing a foundation for future plant-derived adhesive design. This work demonstrates how AI-assisted ideation can drive real-world materials design and enable effective human-AI collaboration.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</title>
<link>https://arxiv.org/abs/2508.06601</link>
<guid>https://arxiv.org/abs/2508.06601</guid>
<content:encoded><![CDATA[
<div> open-weight AI systems, tampering attacks, data filtering, biothreat proxy knowledge, pretraining data curation<br />
Summary:<br /> 
- Open-weight AI systems offer benefits of transparency and decentralized access but are vulnerable to tampering attacks.
- Existing methods struggle to make models resistant to adversarial fine-tuning.
- Filtering text about dual-use topics can help prevent unwanted capabilities in AI models.
- Pretraining data curation can minimize biothreat proxy knowledge in language models.
- Filtered models show resistance to attacks on biothreat-related text but can leverage dangerous knowledge when provided in context, suggesting a need for a defense-in-depth approach. <br /> 
Summary: <div>
arXiv:2508.06601v1 Announce Type: new 
Abstract: Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. In this paper, we investigate whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. We pretrain multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, we find that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Diffusion Models and Phases of Data Distributions</title>
<link>https://arxiv.org/abs/2508.06614</link>
<guid>https://arxiv.org/abs/2508.06614</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, generative artificial intelligence, data distribution phases, local denoisers, neural networks

Summary: 
The article introduces a new perspective on the phases of data distributions in generative artificial intelligence frameworks inspired by statistical physics. It emphasizes the importance of considering the spatial structure of real-life data, such as images, and proposes a method to construct local denoisers with reduced computational costs. By defining data distribution phases based on mutual connectivity via local operations, the study reveals an early trivial phase, a late data phase, and a rapid phase transition where local denoisers may fail. An information-theoretic bound on the fidelity of local denoisers is derived, providing insights for designing more efficient architectures for diffusion models. The findings suggest using small local neural networks away from phase transition points and global networks only during phase transitions. This work paves the way for further research on data distribution phases and the design of neural networks inspired by physics concepts.<br /><br />Summary: <div>
arXiv:2508.06614v1 Announce Type: new 
Abstract: As a class of generative artificial intelligence frameworks inspired by statistical physics, diffusion models have shown extraordinary performance in synthesizing complicated data distributions through a denoising process gradually guided by score functions. Real-life data, like images, is often spatially structured in low-dimensional spaces. However, ordinary diffusion models ignore this local structure and learn spatially global score functions, which are often computationally expensive. In this work, we introduce a new perspective on the phases of data distributions, which provides insight into constructing local denoisers with reduced computational costs. We define two distributions as belonging to the same data distribution phase if they can be mutually connected via spatially local operations such as local denoisers. Then, we show that the reverse denoising process consists of an early trivial phase and a late data phase, sandwiching a rapid phase transition where local denoisers must fail. To diagnose such phase transitions, we prove an information-theoretic bound on the fidelity of local denoisers based on conditional mutual information, and conduct numerical experiments in a real-world dataset. This work suggests simpler and more efficient architectures of diffusion models: far from the phase transition point, we can use small local neural networks to compute the score function; global neural networks are only necessary around the narrow time interval of phase transitions. This result also opens up new directions for studying phases of data distributions, the broader science of generative artificial intelligence, and guiding the design of neural networks inspired by physics concepts.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing Scaling Laws for Dense and Sparse Large Language Models</title>
<link>https://arxiv.org/abs/2508.06617</link>
<guid>https://arxiv.org/abs/2508.06617</guid>
<content:encoded><![CDATA[
<div> scaling laws, language models, efficiency, training process, resources 

Summary: 
This article introduces a new scaling law for large language models that can be applied to both dense and sparse architectures. With the exponential growth in model size and training costs, the need for efficient training techniques has become paramount. Existing scaling laws have primarily been specific to certain architectures, but this proposed generalized scaling law aims to provide a unified framework. By evaluating and comparing this new scaling law with existing ones, the authors showcase its effectiveness in optimizing the prediction of model size and resource allocation. This research contributes to the ongoing efforts to enhance the efficiency of training large language models. <br /><br /> <div>
arXiv:2508.06617v1 Announce Type: new 
Abstract: Over the past few years, the size of language models has grown exponentially, as has the computational cost to train these large models. This rapid growth has motivated researchers to develop new techniques aimed at enhancing the efficiency of the training process. Despite these advancements, optimally predicting the model size or allocating optimal resources remains a challenge. Several efforts have addressed the challenge by proposing different scaling laws, but almost all of them are architecture-specific (dense or sparse). In this work we revisit existing scaling laws and propose a generalized scaling law to provide a unified framework that is applicable to both dense and sparse large language models. We evaluate and compare our proposed scaling law with existing scaling laws to demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels</title>
<link>https://arxiv.org/abs/2508.06622</link>
<guid>https://arxiv.org/abs/2508.06622</guid>
<content:encoded><![CDATA[
<div> Keywords: ANTIDOTE, learning under noisy labels, adversarial training, label noise, time complexity 

Summary:
ANTIDOTE is a new approach for learning under noisy labels, using a relaxation over an information-divergence neighborhood. It is formulated as an adversarial training method with similar computational cost to standard cross-entropy loss. ANTIDOTE adaptively reduces the influence of samples with noisy labels, behaving like forgetting those samples during learning. Effective in practical environments with label noise or adversarial label alterations, ANTIDOTE outperforms leading losses in the field on various types of label noise. Empirical evaluations show its superiority and time complexity close to standard cross entropy loss. Overall, ANTIDOTE offers a promising solution for learning under challenging conditions of noisy labels, demonstrating improved performance and efficient training. 

Summary: <br /><br />ANTIDOTE is a novel approach for learning under noisy labels, utilizing adversarial training and a relaxation over an information-divergence neighborhood. It adaptively reduces the influence of noisy-label samples, outperforming leading losses in the field and exhibiting efficient time complexity. ANTIDOTE proves effective in practical scenarios with label noise or adversarial label modifications, showcasing superior performance and promising potential for improved learning outcomes. <div>
arXiv:2508.06622v1 Announce Type: new 
Abstract: We introduce ANTIDOTE, a new class of objectives for learning under noisy labels which are defined in terms of a relaxation over an information-divergence neighborhood. Using convex duality, we provide a reformulation as an adversarial training method that has similar computational cost to training with standard cross-entropy loss. We show that our approach adaptively reduces the influence of the samples with noisy labels during learning, exhibiting a behavior that is analogous to forgetting those samples. ANTIDOTE is effective in practical environments where label noise is inherent in the training data or where an adversary can alter the training labels. Extensive empirical evaluations on different levels of symmetric, asymmetric, human annotation, and real-world label noise show that ANTIDOTE outperforms leading comparable losses in the field and enjoys a time complexity that is very close to that of the standard cross entropy loss.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record</title>
<link>https://arxiv.org/abs/2508.06627</link>
<guid>https://arxiv.org/abs/2508.06627</guid>
<content:encoded><![CDATA[
<div> Keywords: pancreatic ductal adenocarcinoma, early detection, multimodal approach, electronic health records, neural networks

Summary:
This study introduces a novel multimodal approach for early detection of pancreatic ductal adenocarcinoma (PDAC) using longitudinal diagnosis code histories and laboratory measurements from electronic health records. The proposed method combines neural controlled differential equations, pretrained language models, and recurrent networks to model irregular lab time series and learn diagnosis code trajectory representations. By incorporating cross-attention mechanisms, interactions between the two modalities are captured. The approach is evaluated on a dataset of nearly 4,700 patients and demonstrates significant improvements in AUC compared to state-of-the-art methods, ranging from 6.5% to 15.5%. Additionally, the model identifies specific diagnosis codes and laboratory panels associated with elevated PDAC risk, including established and new biomarkers. The code for the model is accessible on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2508.06627v1 Announce Type: new 
Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and early detection remains a major clinical challenge due to the absence of specific symptoms and reliable biomarkers. In this work, we propose a new multimodal approach that integrates longitudinal diagnosis code histories and routinely collected laboratory measurements from electronic health records to detect PDAC up to one year prior to clinical diagnosis. Our method combines neural controlled differential equations to model irregular lab time series, pretrained language models and recurrent networks to learn diagnosis code trajectory representations, and cross-attention mechanisms to capture interactions between the two modalities. We develop and evaluate our approach on a real-world dataset of nearly 4,700 patients and achieve significant improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods. Furthermore, our model identifies diagnosis codes and laboratory panels associated with elevated PDAC risk, including both established and new biomarkers. Our code is available at https://github.com/MosbahAouad/EarlyPDAC-MML.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Imperfect Synthetic Data in Downstream Inference Tasks</title>
<link>https://arxiv.org/abs/2508.06635</link>
<guid>https://arxiv.org/abs/2508.06635</guid>
<content:encoded><![CDATA[
<div> Estimator, Generalized method of moments, Synthetic data, Computational social science, Regression tasks
Summary: 
This new study explores the use of large language models for generating synthetic data in computational social science research. By introducing a new estimator based on the generalized method of moments, the researchers provide a hyperparameter-free solution with strong theoretical guarantees. Surprisingly, they find that interactions between the moment residuals of synthetic and real data can improve estimates of the target parameter. Empirical validation across various regression tasks in computational social science applications shows significant gains in finite-sample performance. This innovative approach offers a promising way to combine synthetic and real data to draw statistically valid conclusions, opening new possibilities for research in limited data regimes. <div>
arXiv:2508.06635v1 Announce Type: new 
Abstract: Predictions and generations from large language models are increasingly being explored as an aid to computational social science and human subject research in limited data regimes. While previous technical work has explored the potential to use model-predicted labels for unlabeled data in a principled manner, there is increasing interest in using large language models to generate entirely new synthetic samples (also termed as synthetic simulations), such as in responses to surveys. However, it is not immediately clear by what means practitioners can combine such data with real data and yet produce statistically valid conclusions upon them. In this work, we introduce a new estimator based on generalized method of moments, providing a hyperparameter-free solution with strong theoretical guarantees to address the challenge at hand. Surprisingly, we find that interactions between the moment residuals of synthetic data and those of real data can improve estimates of the target parameter. We empirically validate the finite-sample performance of our estimator across different regression tasks in computational social science applications, demonstrating large empirical gains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series</title>
<link>https://arxiv.org/abs/2508.06638</link>
<guid>https://arxiv.org/abs/2508.06638</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series data, Anomaly detection, Adaptive thresholding, Nonstationary environments, Statistical properties

Summary: 
The article introduces two novel adaptive thresholding frameworks, Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence Segments (MACS), to address the challenges of anomaly detection in nonstationary environments where statistical properties shift over time. Traditional static thresholds are not effective in such scenarios where regime shifts, concept drift, or multi-scale changes can occur. The new frameworks leverage statistical online learning and segmentation principles for contextually sensitive adaptation, ensuring false alarm rates are maintained even under evolving distributions. Experimental results on Wafer Manufacturing benchmark datasets show significant improvement in F1-score compared to traditional approaches like percentiles and rolling quantiles. The study illustrates that robust, statistically principled adaptive thresholds enable reliable, interpretable, and timely detection of various real-world anomalies. 

<br /><br />Summary: <div>
arXiv:2508.06638v1 Announce Type: new 
Abstract: As time series data become increasingly prevalent in domains such as manufacturing, IT, and infrastructure monitoring, anomaly detection must adapt to nonstationary environments where statistical properties shift over time. Traditional static thresholds are easily rendered obsolete by regime shifts, concept drift, or multi-scale changes. To address these challenges, we introduce and empirically evaluate two novel adaptive thresholding frameworks: Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence Segments (MACS). Both leverage statistical online learning and segmentation principles for local, contextually sensitive adaptation, maintaining guarantees on false alarm rates even under evolving distributions. Our experiments across Wafer Manufacturing benchmark datasets show significant F1-score improvement compared to traditional percentile and rolling quantile approaches. This work demonstrates that robust, statistically principled adaptive thresholds enable reliable, interpretable, and timely detection of diverse real-world anomalies.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractal Language Modelling by Universal Sequence Maps (USM)</title>
<link>https://arxiv.org/abs/2508.06641</link>
<guid>https://arxiv.org/abs/2508.06641</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Transformers, Encoding, Universal Sequence Maps, Genomic Sequences

Summary:<br />
- The article discusses the encoding procedures that represent symbolic sequences at multiple scales using Language Models and Transformers, specifically focusing on Universal Sequence Maps (USM).
- USM is a bijective fractal encoding method that encodes symbolic sequences onto numerical spaces, allowing for the computation of Chebyshev distance metric and k-mer frequencies.
- The report addresses seeding biases in the USM process, leading to the reconciliation of numeric positioning with sequence identity and revealing USM as an efficient numeric process converging towards a steady state sequence embedding solution.
- The study primarily examines genomic sequences due to the ease of representation with a 4-token alphabet (nucleotides), but the application to sequences with varying alphabets is also feasible.
- The results highlight the significance of USM in capturing contextual information in symbolic sequences and provide insights into its iterative nature and convergence properties.

<br /><br />Summary: <div>
arXiv:2508.06641v1 Announce Type: new 
Abstract: Motivation: With the advent of Language Models using Transformers, popularized by ChatGPT, there is a renewed interest in exploring encoding procedures that numerically represent symbolic sequences at multiple scales and embedding dimensions. The challenge that encoding addresses is the need for mechanisms that uniquely retain contextual information about the succession of individual symbols, which can then be modeled by nonlinear formulations such as neural networks.
  Context: Universal Sequence Maps(USM) are iterated functions that bijectively encode symbolic sequences onto embedded numerical spaces. USM is composed of two Chaos Game Representations (CGR), iterated forwardly and backwardly, that can be projected into the frequency domain (FCGR). The corresponding USM coordinates can be used to compute a Chebyshev distance metric as well as k-mer frequencies, without having to recompute the embedded numeric coordinates, and, paradoxically, allowing for non-integers values of k.
  Results: This report advances the bijective fractal encoding by Universal Sequence Maps (USM) by resolving seeding biases affecting the iterated process. The resolution had two results, the first expected, the second an intriguing outcome: 1) full reconciliation of numeric positioning with sequence identity; and 2) uncovering the nature of USM as an efficient numeric process converging towards a steady state sequence embedding solution. We illustrate these results for genomic sequences because of the convenience of a planar representation defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless, the application to alphabet of arbitrary cardinality was found to be straightforward.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN</title>
<link>https://arxiv.org/abs/2508.06647</link>
<guid>https://arxiv.org/abs/2508.06647</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthetic data generation, Privacy preservation, Tabular Auto-Regressive Generative Network, Statistical similarity, Membership-inference attacks

Summary: 
The article introduces the Tabular Auto-Regressive Generative Network (TabularARGN), a neural network architecture designed for generating synthetic tabular data while preserving privacy. TabularARGN utilizes a discretization-based auto-regressive approach to achieve high data fidelity and computational efficiency. Comparative evaluations against existing methods demonstrate TabularARGN's competitive performance in statistical similarity, machine learning utility, and detection robustness. A thorough privacy evaluation using membership-inference attacks showcases the robustness and effective privacy-utility balance of TabularARGN. This research addresses the increasing need for secure sharing and analysis of sensitive data sets, offering a promising solution for privacy-preserving synthetic data generation. <br /><br />Summary: <div>
arXiv:2508.06647v1 Announce Type: new 
Abstract: Synthetic data generation has become essential for securely sharing and analyzing sensitive data sets. Traditional anonymization techniques, however, often fail to adequately preserve privacy. We introduce the Tabular Auto-Regressive Generative Network (TabularARGN), a neural network architecture specifically designed for generating high-quality synthetic tabular data. Using a discretization-based auto-regressive approach, TabularARGN achieves high data fidelity while remaining computationally efficient. We evaluate TabularARGN against existing synthetic data generation methods, showing competitive results in statistical similarity, machine learning utility, and detection robustness. We further perform an in-depth privacy evaluation using systematic membership-inference attacks, highlighting the robustness and effective privacy-utility balance of our approach.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Reinforcement Learning via Communicative World Models</title>
<link>https://arxiv.org/abs/2508.06659</link>
<guid>https://arxiv.org/abs/2508.06659</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, ICRL, CORAL framework, transferable communicative context, emergent communication<br />
<br />
Summary:<br />
This work introduces the CORAL framework for improving agents' in-context RL abilities by decoupling latent representation learning from control. It formulates ICRL as a two-agent communication problem, with an Information Agent (IA) pre-trained on diverse tasks to build a world model and distill understanding into concise messages. A Causal Influence Loss shapes the communication protocol by measuring the message's effect on the next action. The IA then serves as a contextualizer for a Control Agent (CA) during deployment, enabling the CA to interpret the communicative context and achieve significant gains in sample efficiency. The CA can successfully perform zero-shot adaptation in entirely unseen sparse-reward environments, demonstrating the efficacy of learning a transferable communicative representation.<br /> <div>
arXiv:2508.06659v1 Announce Type: new 
Abstract: Reinforcement learning (RL) agents often struggle to generalize to new tasks and contexts without updating their parameters, mainly because their learned representations and policies are overfit to the specifics of their training environments. To boost agents' in-context RL (ICRL) ability, this work formulates ICRL as a two-agent emergent communication problem and introduces CORAL (Communicative Representation for Adaptive RL), a framework that learns a transferable communicative context by decoupling latent representation learning from control. In CORAL, an Information Agent (IA) is pre-trained as a world model on a diverse distribution of tasks. Its objective is not to maximize task reward, but to build a world model and distill its understanding into concise messages. The emergent communication protocol is shaped by a novel Causal Influence Loss, which measures the effect that the message has on the next action. During deployment, the previously trained IA serves as a fixed contextualizer for a new Control Agent (CA), which learns to solve tasks by interpreting the provided communicative context. Our experiments demonstrate that this approach enables the CA to achieve significant gains in sample efficiency and successfully perform zero-shot adaptation with the help of pre-trained IA in entirely unseen sparse-reward environments, validating the efficacy of learning a transferable communicative representation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2508.06663</link>
<guid>https://arxiv.org/abs/2508.06663</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Kolmogorov-Arnold Networks, scalability, efficiency, knowledge amalgamation

Summary:
Graph Neural Networks (GNNs) have limitations in scalability and efficiency due to their reliance on graph connectivity. A recent architecture, Kolmogorov-Arnold Networks (KANs), offers strong nonlinear expressiveness and efficient inference. This work integrates KANs into popular GNN architectures (such as GAT, SGC, and APPNP) to create new models (KGAT, KSGC, and KAPPNP). A multi-teacher knowledge amalgamation framework is also utilized, where knowledge from multiple KAN-based GNNs is distilled into a graph-independent KAN student model. Experiments on benchmark datasets demonstrate that the proposed models enhance node classification accuracy, and the knowledge amalgamation approach notably improves the student model's performance. These findings underscore the potential of KANs in enhancing GNN expressiveness and enabling efficient, graph-free inference. 

<br /><br />Summary: <div>
arXiv:2508.06663v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have shown strong performance on graph-structured data, but their reliance on graph connectivity often limits scalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent architecture with learnable univariate functions, offer strong nonlinear expressiveness and efficient inference. In this work, we integrate KANs into three popular GNN architectures-GAT, SGC, and APPNP-resulting in three new models: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge amalgamation framework, where knowledge from multiple KAN-based GNNs is distilled into a graph-independent KAN student model. Experiments on benchmark datasets show that the proposed models improve node classification accuracy, and the knowledge amalgamation approach significantly boosts student model performance. Our findings highlight the potential of KANs for enhancing GNN expressiveness and for enabling efficient, graph-free inference.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation</title>
<link>https://arxiv.org/abs/2508.06676</link>
<guid>https://arxiv.org/abs/2508.06676</guid>
<content:encoded><![CDATA[
<div> watermarking, machine learning, Kolmogorov-Arnold Networks, Discrete Cosine Transform, robustness

Summary:
Discrete Cosine Transform-based Activation Watermarking (DCT-AW) is introduced as a novel watermarking method tailored for Kolmogorov-Arnold Networks (KAN), which have learnable activation functions. This method perturbs activation outputs using discrete cosine transform to embed watermarks, ensuring compatibility with various tasks and achieving task independence. Experimental results show that DCT-AW has minimal impact on model performance and provides superior robustness against watermark removal attacks, such as fine-tuning, pruning, and retraining after pruning. The increasing importance of protecting intellectual property in machine learning has led to significant attention on watermarking techniques. While existing methods are effective for traditional deep neural networks, they often struggle to adapt to the unique architecture of KAN. Therefore, DCT-AW fills this gap by offering a solution specifically designed for KAN, demonstrating its effectiveness and robustness in model protection. 

<br /><br />Summary: <div>
arXiv:2508.06676v1 Announce Type: new 
Abstract: With the increasing importance of protecting intellectual property in machine learning, watermarking techniques have gained significant attention. As advanced models are increasingly deployed in domains such as social network analysis, the need for robust model protection becomes even more critical. While existing watermarking methods have demonstrated effectiveness for conventional deep neural networks, they often fail to adapt to the novel architecture, Kolmogorov-Arnold Networks (KAN), which feature learnable activation functions. KAN holds strong potential for modeling complex relationships in network-structured data. However, their unique design also introduces new challenges for watermarking. Therefore, we propose a novel watermarking method, Discrete Cosine Transform-based Activation Watermarking (DCT-AW), tailored for KAN. Leveraging the learnable activation functions of KAN, our method embeds watermarks by perturbing activation outputs using discrete cosine transform, ensuring compatibility with diverse tasks and achieving task independence. Experimental results demonstrate that DCT-AW has a small impact on model performance and provides superior robustness against various watermark removal attacks, including fine-tuning, pruning, and retraining after pruning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select</title>
<link>https://arxiv.org/abs/2508.06692</link>
<guid>https://arxiv.org/abs/2508.06692</guid>
<content:encoded><![CDATA[
<div> client data, Federated Learning, HeteRo-Select, training stability, accuracy<br />
<br />
Summary: 
The article introduces the HeteRo-Select framework for improving training stability in Federated Learning (FL). By selecting a smart subset of clients based on factors like usefulness, fairness, update speed, and data variety, HeteRo-Select aims to maintain high performance throughout the training process. The theoretical analysis demonstrates that selecting a subset of clients can reduce communication more effectively in cases of high heterogeneity in client data. Experimental results on the CIFAR-10 dataset with significant label skew show that HeteRo-Select outperforms existing methods in terms of peak accuracy, final accuracy, and training stability. With a peak accuracy of 74.75%, final accuracy of 72.76%, and minimal stability drop of 1.99%, HeteRo-Select proves to be a reliable solution for real-world heterogeneous FL problems. <div>
arXiv:2508.06692v1 Announce Type: new 
Abstract: Federated Learning (FL) is a machine learning technique that often suffers from training instability due to the diverse nature of client data. Although utility-based client selection methods like Oort are used to converge by prioritizing high-loss clients, they frequently experience significant drops in accuracy during later stages of training. We propose a theoretical HeteRo-Select framework designed to maintain high performance and ensure long-term training stability. We provide a theoretical analysis showing that when client data is very different (high heterogeneity), choosing a smart subset of client participation can reduce communication more effectively compared to full participation. Our HeteRo-Select method uses a clear, step-by-step scoring system that considers client usefulness, fairness, update speed, and data variety. It also shows convergence guarantees under strong regularization. Our experimental results on the CIFAR-10 dataset under significant label skew ($\alpha=0.1$) support the theoretical findings. The HeteRo-Select method performs better than existing approaches in terms of peak accuracy, final accuracy, and training stability. Specifically, HeteRo-Select achieves a peak accuracy of $74.75\%$, a final accuracy of $72.76\%$, and a minimal stability drop of $1.99\%$. In contrast, Oort records a lower peak accuracy of $73.98\%$, a final accuracy of $71.25\%$, and a larger stability drop of $2.73\%$. The theoretical foundations and empirical performance in our study make HeteRo-Select a reliable solution for real-world heterogeneous FL problems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations</title>
<link>https://arxiv.org/abs/2508.06704</link>
<guid>https://arxiv.org/abs/2508.06704</guid>
<content:encoded><![CDATA[
<div> deep learning, species distribution models, biotic interactions, incomplete species observations, ecological research

Summary:
CISO is a deep learning-based method for species distribution modeling that addresses the challenge of incorporating incomplete biotic information. It allows for predictions to be conditioned on a flexible number of species observations alongside environmental variables. The method improves predictive performance on spatially separate test sets by including partial biotic information. CISO outperforms alternative methods when conditioned on a subset of species within the same dataset. Additionally, combining observations from multiple datasets can further enhance performance. This approach is a promising ecological tool that can identify potential interactions between species from different taxa, making it valuable for conservation planning and ecological research.<br /><br />Summary: <div>
arXiv:2508.06704v1 Announce Type: new 
Abstract: Species distribution models (SDMs) are widely used to predict species' geographic distributions, serving as critical tools for ecological research and conservation planning. Typically, SDMs relate species occurrences to environmental variables representing abiotic factors, such as temperature, precipitation, and soil properties. However, species distributions are also strongly influenced by biotic interactions with other species, which are often overlooked. While some methods partially address this limitation by incorporating biotic interactions, they often assume symmetrical pairwise relationships between species and require consistent co-occurrence data. In practice, species observations are sparse, and the availability of information about the presence or absence of other species varies significantly across locations. To address these challenges, we propose CISO, a deep learning-based method for species distribution modeling Conditioned on Incomplete Species Observations. CISO enables predictions to be conditioned on a flexible number of species observations alongside environmental variables, accommodating the variability and incompleteness of available biotic data. We demonstrate our approach using three datasets representing different species groups: sPlotOpen for plants, SatBird for birds, and a new dataset, SatButterfly, for butterflies. Our results show that including partial biotic information improves predictive performance on spatially separate test sets. When conditioned on a subset of species within the same dataset, CISO outperforms alternative methods in predicting the distribution of the remaining species. Furthermore, we show that combining observations from multiple datasets can improve performance. CISO is a promising ecological tool, capable of incorporating incomplete biotic information and identifying potential interactions between species from disparate taxa.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Schedule-Free Nonconvex Optimization</title>
<link>https://arxiv.org/abs/2508.06743</link>
<guid>https://arxiv.org/abs/2508.06743</guid>
<content:encoded><![CDATA[
<div> Keywords: First-order methods, Schedule-Free, nonconvex optimization, Lyapunov framework, Performance Estimation Problem (PEP) <br />
Summary: 
The article introduces a Schedule-Free (SF) method for first-order methods in large-scale learning algorithms with hyperparameters that are independent of the total horizon $T$. A robust Lyapunov framework is presented for analyzing SF in nonconvex settings, providing horizon-agnostic bounds. The analysis yields rates of $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for polynomial averaging. The authors validate these rates through Performance Estimation Problem (PEP) experiments. The study extends SF's horizon-free guarantees to smooth nonconvex optimization, suggesting a potential improvement in the $O(1/\log T)$ bound to $O(1/T). The work paves the way for optimal nonconvex rates in the future. <br /><br /> <div>
arXiv:2508.06743v1 Announce Type: new 
Abstract: First-order methods underpin most large-scale learning algorithms, yet their classical convergence guarantees hinge on carefully scheduled step-sizes that depend on the total horizon $T$, which is rarely known in advance. The Schedule-Free (SF) method promises optimal performance with hyperparameters that are independent of $T$ by interpolating between Polyak--Ruppert averaging and momentum, but nonconvex analysis of SF has been limited or reliant on strong global assumptions. We introduce a robust Lyapunov framework that, under only $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step descent inequality. This yields horizon-agnostic bounds in the nonconvex setting: $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for a linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for polynomial averaging. We complement these proofs with Performance Estimation Problem (PEP) experiments that numerically validate our rates and suggest that our $O(1/\log T)$ bound on the original nonconvex SF algorithm may tighten to $O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex optimization and charts future directions for optimal nonconvex rates.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning</title>
<link>https://arxiv.org/abs/2508.06765</link>
<guid>https://arxiv.org/abs/2508.06765</guid>
<content:encoded><![CDATA[
<div> Keywords: Collaborative fine-tuning, large language models, federated learning, mobile devices, model heterogeneity

Summary:
Fed MobiLLM introduces a novel approach for efficient federated fine-tuning of large language models on heterogeneous mobile devices. It utilizes a server-assisted federated side-tuning paradigm where mobile devices perform lightweight computations on local data using pre-scaled backbone models and upload selected activations for server training. This eliminates client-side backpropagation and allows for asynchronous updates. An adaptive layer-wise feature alignment method ensures consistent representations for collaboratively tuning a shared side network across different devices. Experimental results show that Fed MobiLLM achieves robust fine-tuning performance with significantly reduced computation overhead, communication costs, and faster convergence compared to existing methods, making it suitable for practical adaptation of large language models on mobile devices. 

<br /><br />Summary: <div>
arXiv:2508.06765v1 Announce Type: new 
Abstract: Collaboratively fine-tuning (FT) large language models (LLMs) over heterogeneous mobile devices fosters immense potential applications of personalized intelligence. However, such a vision faces critical system challenges. Conventional federated LLM FT approaches place prohibitive computational and memory burdens on mobile hardware, and their synchronous model aggregation protocols stall for slower devices. In this paper, we propose Fed MobiLLM, a novel design to facilitate efficient federated LLM FT across mobile devices with diverse computing/communication speeds and local model architectures. In particular, Fed MobiLLM implements a pioneering server-assisted federated side-tuning paradigm. Briefly, mobile devices perform lightweight forward propagation computations on local data using their frozen pre-scaled backbone LLMs, and then upload selected intermediate activations. The server trains a shared side-network independently, eliminating client-side backpropagation and enabling asynchronous updates. To bridge model heterogeneity across different devices, we introduce an adaptive layer-wise feature alignment method, which ensures consistent representations for collaboratively tuning a shared side network. Extensive experimental results demonstrate that Fed MobiLLM can maintain robust fine-tuning performance while achieving extremely low on-device memory, with at least 95.2% reduction in computation overhead, 93.2% reduction in communication costs and 5.1x faster convergence compared to existing methods, validating its efficacy for practical LLM adaptation over heterogeneous mobile devices.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems</title>
<link>https://arxiv.org/abs/2508.06767</link>
<guid>https://arxiv.org/abs/2508.06767</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital Twins, Embodied AI, Data Handling, Multi-agent Path Finding, Autonomous Task Execution<br />
Summary: <br />
The article introduces PANAMA, a new algorithm for Multi-agent Reinforcement Learning in Digital Twin ecosystems. It focuses on the importance of efficient data handling in next-gen networks, emphasizing the relationship between application and network providers. PANAMA utilizes a Centralized Training with Decentralized Execution framework and asynchronous actor-learner architectures to accelerate training and enable autonomous task execution by embodied AI. Through simulations, PANAMA demonstrates superior performance in pathfinding accuracy, speed, and scalability compared to existing benchmarks. The algorithm's optimized data-sharing strategies ensure resilience in complex real-world environments, bridging the gap between network-aware decision-making and multi-agent coordination. This advancement in technology enhances the synergy between Digital Twins, wireless networks, and AI-driven automation. <br /> <div>
arXiv:2508.06767v1 Announce Type: new 
Abstract: Digital Twins (DTs) are transforming industries through advanced data processing and analysis, positioning the world of DTs, Digital World, as a cornerstone of nextgeneration technologies including embodied AI. As robotics and automated systems scale, efficient data-sharing frameworks and robust algorithms become critical. We explore the pivotal role of data handling in next-gen networks, focusing on dynamics between application and network providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL) based multi-agent path finding (MAPF). By adopting a Centralized Training with Decentralized Execution (CTDE) framework and asynchronous actor-learner architectures, PANAMA accelerates training while enabling autonomous task execution by embodied AI. Our approach demonstrates superior pathfinding performance in accuracy, speed, and scalability compared to existing benchmarks. Through simulations, we highlight optimized data-sharing strategies for scalable, automated systems, ensuring resilience in complex, real-world environments. PANAMA bridges the gap between network-aware decision-making and robust multi-agent coordination, advancing the synergy between DTs, wireless networks, and AI-driven automation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift</title>
<link>https://arxiv.org/abs/2508.06776</link>
<guid>https://arxiv.org/abs/2508.06776</guid>
<content:encoded><![CDATA[
<div> framework, model drift, transformer activations, null directions, theory-only <br />
Summary: 
The Zero-Direction Probing (ZDP) framework introduces a novel approach for detecting model drift without the need for task labels or output evaluations. By examining the null directions of transformer activations, ZDP offers concrete guarantees on representational change. Key results include the Variance-Leak Theorem, Fisher Null-Conservation, and a Rank-Leak bound for low-rank updates. Additionally, ZDP provides a logarithmic-regret guarantee for online null-space trackers. The framework introduces a Spectral Null-Leakage (SNL) metric with non-asymptotic tail bounds and concentration inequalities, allowing for a-priori thresholds for drift detection under a Gaussian null model. By monitoring the right and left null spaces of layer activations and their Fisher geometry, ZDP offers a reliable method for assessing model drift and representational changes. <br /><br />Summary: <div>
arXiv:2508.06776v1 Announce Type: new 
Abstract: We present Zero-Direction Probing (ZDP), a theory-only framework for detecting model drift from null directions of transformer activations without task labels or output evaluations. Under assumptions A1--A6, we prove: (i) the Variance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound for low-rank updates, and (iv) a logarithmic-regret guarantee for online null-space trackers. We derive a Spectral Null-Leakage (SNL) metric with non-asymptotic tail bounds and a concentration inequality, yielding a-priori thresholds for drift under a Gaussian null model. These results show that monitoring right/left null spaces of layer activations and their Fisher geometry provides concrete, testable guarantees on representational change.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROPS: Progressively Private Self-alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2508.06783</link>
<guid>https://arxiv.org/abs/2508.06783</guid>
<content:encoded><![CDATA[
<div> Alignment, Large Language Models, Privacy, PROPS, Preference-level Privacy<br />
<br />
Summary: <br />
Alignment in Large Language Models (LLMs) is crucial to ensure adherence to human values, but the use of human feedback raises privacy concerns. This work introduces PROPS, a privacy-preserving alignment framework that focuses on preserving the privacy of preference labels provided by humans. PROPS progresses through multiple stages of alignment, where privately aligned models in previous stages serve as labelers for supplementing training data in the subsequent stages. The framework provides theoretical guarantees and is validated using various models and datasets, demonstrating higher utility compared to existing methods while maintaining high privacy levels. For the same privacy budget, PROPS achieves significantly higher win-rates in alignment compared to Differentially Private SGD (DP-SGD) and Randomized Response (RR) based alignment methods. <div>
arXiv:2508.06783v1 Announce Type: new 
Abstract: Alignment is a key step in developing Large Language Models (LLMs) using human feedback to ensure adherence to human values and societal norms. Dependence on human feedback raises privacy concerns about how much a labeler's preferences may reveal about their personal values, beliefs, and personality traits. Existing approaches, such as Differentially Private SGD (DP-SGD), provide rigorous privacy guarantees by privatizing gradients during fine-tuning and alignment but can provide more privacy than necessary as human preferences are tied only to labels of (prompt, response) pairs and can degrade model utility. This work focuses on LLM alignment with preference-level privacy, which preserves the privacy of preference labels provided by humans. We propose PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving alignment framework where privately aligned models in previous stages can serve as labelers for supplementing training data in the subsequent stages of alignment. We present theoretical guarantees for PROPS as well as comprehensive validation using multiple models (Pythia and GPT) and datasets (AlpacaEval, Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over existing methods while still providing high privacy. For the same privacy budget, alignment via PROPS can achieve up to 3x higher win-rates compared to DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based alignment.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning</title>
<link>https://arxiv.org/abs/2508.06784</link>
<guid>https://arxiv.org/abs/2508.06784</guid>
<content:encoded><![CDATA[
<div> Keywords: high-dimensional data, self-supervised learning, tensor networks, Tucker decomposition, non-linear framework

Summary:
The article introduces the Mode-Aware Non-linear Tucker Autoencoder (MA-NTAE) to address challenges in self-supervised learning with high-dimensional data. Traditional autoencoders face issues with large model sizes and computational overhead, while existing tensor networks struggle with learning non-linear relationships. MA-NTAE extends Tucker decomposition to a non-linear framework and employs a Pick-and-Unfold strategy for encoding high-order tensors in a flexible manner. It integrates tensor structural priors effectively and exhibits computational complexity growth proportional to mode dimensions. Experimental results show MA-NTAE outperforms standard autoencoders and current tensor networks in compression and clustering tasks, especially for higher-order tensors. This innovation offers an efficient solution for deep structural feature capture in high-dimensional data. 

<br /><br />Summary: <div>
arXiv:2508.06784v1 Announce Type: new 
Abstract: High-dimensional data, particularly in the form of high-order tensors, presents a major challenge in self-supervised learning. While MLP-based autoencoders (AE) are commonly employed, their dependence on flattening operations exacerbates the curse of dimensionality, leading to excessively large model sizes, high computational overhead, and challenging optimization for deep structural feature capture. Although existing tensor networks alleviate computational burdens through tensor decomposition techniques, most exhibit limited capability in learning non-linear relationships. To overcome these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder (MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear framework and employs a Pick-and-Unfold strategy, facilitating flexible per-mode encoding of high-order tensors via recursive unfold-encode-fold operations, effectively integrating tensor structural priors. Notably, MA-NTAE exhibits linear growth in computational complexity with tensor order and proportional growth with mode dimensions. Extensive experiments demonstrate MA-NTAE's performance advantages over standard AE and current tensor networks in compression and clustering tasks, which become increasingly pronounced for higher-order, higher-dimensional tensors.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities</title>
<link>https://arxiv.org/abs/2508.06800</link>
<guid>https://arxiv.org/abs/2508.06800</guid>
<content:encoded><![CDATA[
<div> Keywords: missing modalities, multimodal emotion recognition, hardness-aware dynamic curriculum learning, reconstruction difficulty, benchmark datasets

Summary: 
The article introduces HARDY-MER, a novel framework for multimodal emotion recognition that addresses missing modalities by incorporating hardness-aware dynamic curriculum learning. This framework operates in two stages: estimating the hardness level of each sample and strategically emphasizing hard samples during training. The Multi-view Hardness Evaluation mechanism quantifies difficulty using both Direct Hardness (modality reconstruction errors) and Indirect Hardness (cross-modal mutual information). The Retrieval-based Dynamic Curriculum Learning strategy adjusts the training curriculum by retrieving similar samples and balancing focus between easy and hard instances. Extensive experiments show HARDY-MER outperforms existing methods in missing-modality scenarios, with code available on GitHub. <div>
arXiv:2508.06800v1 Announce Type: new 
Abstract: Missing modalities have recently emerged as a critical research direction in multimodal emotion recognition (MER). Conventional approaches typically address this issue through missing modality reconstruction. However, these methods fail to account for variations in reconstruction difficulty across different samples, consequently limiting the model's ability to handle hard samples effectively. To overcome this limitation, we propose a novel Hardness-Aware Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates in two key stages: first, it estimates the hardness level of each sample, and second, it strategically emphasizes hard samples during training to enhance model performance on these challenging instances. Specifically, we first introduce a Multi-view Hardness Evaluation mechanism that quantifies reconstruction difficulty by considering both Direct Hardness (modality reconstruction errors) and Indirect Hardness (cross-modal mutual information). Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy that dynamically adjusts the training curriculum by retrieving samples with similar semantic information and balancing the learning focus between easy and hard instances. Extensive experiments on benchmark datasets demonstrate that HARDY-MER consistently outperforms existing methods in missing-modality scenarios. Our code will be made publicly available at https://github.com/HARDY-MER/HARDY-MER.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation</title>
<link>https://arxiv.org/abs/2508.06806</link>
<guid>https://arxiv.org/abs/2508.06806</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Data Augmentation, Offline-to-online, Classifier-Free Diffusion Generation, Performance Improvement <br />
Summary: 
Offline-to-online Reinforcement Learning (O2O RL) involves fine-tuning a pre-trained policy online while minimizing online interactions. Existing methods use offline datasets to augment data, but there is still a gap between generated and online data. To address this, a new approach called Classifier-Free Diffusion Generation (CFDG) is proposed. CFDG utilizes guidance diffusion without additional classifier training overhead to enhance data generation quality across different distributions. It employs reweighting to align more generated data with online data, improving performance and agent stability. Experimental results demonstrate CFDG outperforming other data generation methods. Integration of CFDG with popular RL algorithms results in a significant 15% average performance improvement on D4RL benchmarks like MuJoCo and AntMaze. <br /> <div>
arXiv:2508.06806v1 Announce Type: new 
Abstract: Offline-to-online Reinforcement Learning (O2O RL) aims to perform online fine-tuning on an offline pre-trained policy to minimize costly online interactions. Existing work used offline datasets to generate data that conform to the online data distribution for data augmentation. However, generated data still exhibits a gap with the online data, limiting overall performance. To address this, we propose a new data augmentation approach, Classifier-Free Diffusion Generation (CFDG). Without introducing additional classifier training overhead, CFDG leverages classifier-free guidance diffusion to significantly enhance the generation quality of offline and online data with different distributions. Additionally, it employs a reweighting method to enable more generated data to align with the online data, enhancing performance while maintaining the agent's stability. Experimental results show that CFDG outperforms replaying the two data types or using a standard diffusion model to generate new data. Our method is versatile and can be integrated with existing offline-to-online RL algorithms. By implementing CFDG to popular methods IQL, PEX and APL, we achieve a notable 15% average improvement in empirical performance on the D4RL benchmark such as MuJoCo and AntMaze.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report: Full-Stack Fine-Tuning for the Q Programming Language</title>
<link>https://arxiv.org/abs/2508.06813</link>
<guid>https://arxiv.org/abs/2508.06813</guid>
<content:encoded><![CDATA[
<div> Comprehensive, open-source approach, Q programming language, quantitative finance, Leetcode style evaluation dataset, modeling techniques
<br />
Summary:
This work addresses the challenge of adapting large language models (LLMs) to niche programming languages like Q, commonly used in quantitative finance but underrepresented online. The researchers introduce a new evaluation dataset for the Q language and train a suite of reasoning and non-reasoning models based on the Qwen-2.5 series with varying parameter sizes. Their best model achieves a pass@1 accuracy of 59%, outperforming existing models like Claude Opus-4 and GPT-4.1 on the Q benchmark task. The methodology, encompassing dataset construction, model pretraining, supervised fine-tuning, and reinforcement learning, is released alongside the models, code, and data for reproducibility. The study demonstrates the potential of leveraging LLMs for specialized applications and provides insights on how these techniques can be extended to other tasks beyond programming languages with objective signals. 
<br /><br />Summary: <div>
arXiv:2508.06813v1 Announce Type: new 
Abstract: Even though large language models are becoming increasingly capable, it is still unreasonable to expect them to excel at tasks that are under-represented on the Internet. Leveraging LLMs for specialized applications, particularly in niche programming languages and private domains, remains challenging and largely unsolved. In this work, we address this gap by presenting a comprehensive, open-source approach for adapting LLMs to the Q programming language, a popular tool in quantitative finance that is much less present on the Internet compared to Python, C, Java, and other ``mainstream" languages and is therefore not a strong suit of general-purpose AI models. We introduce a new Leetcode style evaluation dataset for Q, benchmark major frontier models on the dataset, then do pretraining, supervised fine tuning, and reinforcement learning to train a suite of reasoning and non-reasoning models based on the Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our best model achieves a pass@1 accuracy of 59 percent on our Q benchmark, surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent. Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task. In addition to releasing models, code, and data, we provide a detailed blueprint for dataset construction, model pretraining, supervised fine-tuning, and reinforcement learning. Our methodology is broadly applicable, and we discuss how these techniques can be extended to other tasks, including those where evaluation may rely on soft or subjective signals.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who's the Evil Twin? Differential Auditing for Undesired Behavior</title>
<link>https://arxiv.org/abs/2508.06827</link>
<guid>https://arxiv.org/abs/2508.06827</guid>
<content:encoded><![CDATA[
<div> adversarial game, neural networks, hidden behaviors, detection, auditing<br />
<br />
Summary: 
The study investigates the challenge of detecting hidden harmful behavior in neural networks. It frames the problem as an adversarial game between two teams  the red team training models on benign and harmful data, and the blue team identifying the compromised model. Various strategies are explored, including Gaussian noise analysis, model diffing, integrated gradients, and adversarial attacks. Adversarial-attack-based methods show promising results in accurately identifying the compromised model with hints. Limited to no information makes effective auditing in Large Language Models (LLMs) challenging, requiring hints about the undesired distribution. LLM auditing methods rely on a combination of black-box and open-weight methods to probe models and reveal misalignments. The research findings, along with open-sourced auditing games, aim to contribute to the development of better auditing techniques for neural networks. <div>
arXiv:2508.06827v1 Announce Type: new 
Abstract: Detecting hidden behaviors in neural networks poses a significant challenge due to minimal prior knowledge and potential adversarial obfuscation. We explore this problem by framing detection as an adversarial game between two teams: the red team trains two similar models, one trained solely on benign data and the other trained on data containing hidden harmful behavior, with the performance of both being nearly indistinguishable on the benign dataset. The blue team, with limited to no information about the harmful behaviour, tries to identify the compromised model. We experiment using CNNs and try various blue team strategies, including Gaussian noise analysis, model diffing, integrated gradients, and adversarial attacks under different levels of hints provided by the red team. Results show high accuracy for adversarial-attack-based methods (100\% correct prediction, using hints), which is very promising, whilst the other techniques yield more varied performance. During our LLM-focused rounds, we find that there are not many parallel methods that we could apply from our study with CNNs. Instead, we find that effective LLM auditing methods require some hints about the undesired distribution, which can then used in standard black-box and open-weight methods to probe the models further and reveal their misalignment. We open-source our auditing games (with the model and data) and hope that our findings contribute to designing better audits.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.06871</link>
<guid>https://arxiv.org/abs/2508.06871</guid>
<content:encoded><![CDATA[
<div> Keywords: plasticity loss, deep reinforcement learning, multi-task reinforcement learning, sparsification methods, Gradual Magnitude Pruning (GMP), Sparse Evolutionary Training (SET)

Summary:
Plasticity loss poses a challenge in deep reinforcement learning, especially in multi-task settings where adaptability is crucial. This study investigates the use of sparsification methods such as GMP and SET to improve plasticity in MTRL agents. Various MTRL architectures are evaluated on standardized benchmarks, showing that sparsity can enhance performance by mitigating indicators of plasticity degradation like neuron dormancy. Sparse agents often outperform dense ones and achieve competitive results without explicit plasticity interventions. The results highlight the effectiveness of dynamic sparsification in enhancing adaptability in MTRL systems, offering insights into the relationship between plasticity, network sparsity, and MTRL designs. <br /><br />Summary: Plasticity loss is a challenge in deep reinforcement learning, especially in multi-task settings. Sparsification methods like GMP and SET can improve plasticity in MTRL agents. Sparsity enhances performance by mitigating indicators of plasticity degradation, with sparse agents often outperforming dense counterparts. Dynamic sparsification proves to be an effective tool for enhancing adaptability in MTRL systems. <div>
arXiv:2508.06871v1 Announce Type: new 
Abstract: Plasticity loss, a diminishing capacity to adapt as training progresses, is a critical challenge in deep reinforcement learning. We examine this issue in multi-task reinforcement learning (MTRL), where higher representational flexibility is crucial for managing diverse and potentially conflicting task demands. We systematically explore how sparsification methods, particularly Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance plasticity and consequently improve performance in MTRL agents. We evaluate these approaches across distinct MTRL architectures (shared backbone, Mixture of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks, comparing against dense baselines, and a comprehensive range of alternative plasticity-inducing or regularization methods. Our results demonstrate that both GMP and SET effectively mitigate key indicators of plasticity degradation, such as neuron dormancy and representational collapse. These plasticity improvements often correlate with enhanced multi-task performance, with sparse agents frequently outperforming dense counterparts and achieving competitive results against explicit plasticity interventions. Our findings offer insights into the interplay between plasticity, network sparsity, and MTRL designs, highlighting dynamic sparsification as a robust but context-sensitive tool for developing more adaptable MTRL systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction and Trustworthy AI</title>
<link>https://arxiv.org/abs/2508.06885</link>
<guid>https://arxiv.org/abs/2508.06885</guid>
<content:encoded><![CDATA[
<div> Keywords: Conformal predictors, uncertainty quantification, trustworthy AI, generalization risk, bias identification.

Summary: Conformal predictors, developed in the 1990s, provide set predictions with guaranteed confidence levels, making them popular for uncertainty quantification in machine learning. They offer reliable machine learning with well-calibrated uncertainty quantification, essential for developing trustworthy AI. This article explores how conformal prediction can contribute to trustworthy AI by addressing issues such as generalization risk and AI governance. Experiments and examples demonstrate its use as a well-calibrated predictor and for bias identification and mitigation. By leveraging conformal prediction, AI systems can enhance their trustworthiness, ensuring reliable and accountable decision-making processes. <br /><br />Summary: <div>
arXiv:2508.06885v1 Announce Type: new 
Abstract: Conformal predictors are machine learning algorithms developed in the 1990's by Gammerman, Vovk, and their research team, to provide set predictions with guaranteed confidence level. Over recent years, they have grown in popularity and have become a mainstream methodology for uncertainty quantification in the machine learning community. From its beginning, there was an understanding that they enable reliable machine learning with well-calibrated uncertainty quantification. This makes them extremely beneficial for developing trustworthy AI, a topic that has also risen in interest over the past few years, in both the AI community and society more widely. In this article, we review the potential for conformal prediction to contribute to trustworthy AI beyond its marginal validity property, addressing problems such as generalization risk and AI governance. Experiments and examples are also provided to demonstrate its use as a well-calibrated predictor and for bias identification and mitigation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting</title>
<link>https://arxiv.org/abs/2508.06915</link>
<guid>https://arxiv.org/abs/2508.06915</guid>
<content:encoded><![CDATA[
<div> keywords: Time series forecasting, Zero-shot, TSPMs, RAG, QuiZSF

Summary:
QuiZSF introduces a novel approach to enhance zero-shot time series forecasting by integrating retrieval-augmented generation (RAG) with time series pre-trained models (TSPMs). The framework includes a hierarchical tree-structured ChronoRAG Base (CRB) for scalable time-series storage and domain-aware retrieval. It incorporates a Multi-grained Series Interaction Learner (MSIL) to extract relational features and a Model Cooperation Coherer (MCC) to align retrieved knowledge with TSPMs. QuiZSF outperforms contemporary baselines in 75% and 87.5% of prediction settings using Non-LLM based and LLM based TSPMs, respectively, while maintaining efficiency in memory and inference time. This innovative approach offers a lightweight and modular solution for zero-shot time series forecasting, demonstrating the potential of combining efficient retrieval with representation learning for improved accuracy and adaptability in diverse forecasting scenarios.<br /><br />Summary: <div>
arXiv:2508.06915v1 Announce Type: new 
Abstract: Time series forecasting has become increasingly important to empower diverse applications with streaming data. Zero-shot time-series forecasting (ZSF), particularly valuable in data-scarce scenarios, such as domain transfer or forecasting under extreme conditions, is difficult for traditional models to deal with. While time series pre-trained models (TSPMs) have demonstrated strong performance in ZSF, they often lack mechanisms to dynamically incorporate external knowledge. Fortunately, emerging retrieval-augmented generation (RAG) offers a promising path for injecting such knowledge on demand, yet they are rarely integrated with TSPMs. To leverage the strengths of both worlds, we introduce RAG into TSPMs to enhance zero-shot time series forecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series Forecaster), a lightweight and modular framework that couples efficient retrieval with representation learning and model adaptation for ZSF. Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB) for scalable time-series storage and domain-aware retrieval, introduce a Multi-grained Series Interaction Learner (MSIL) to extract fine- and coarse-grained relational features, and develop a dual-branch Model Cooperation Coherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM based and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM based and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and 87.5% of prediction settings, while maintaining high efficiency in memory and inference time.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Class Unbiasing for Generalization in Medical Diagnosis</title>
<link>https://arxiv.org/abs/2508.06943</link>
<guid>https://arxiv.org/abs/2508.06943</guid>
<content:encoded><![CDATA[
<div> Bias, Medical diagnosis, Class-feature bias, Cls-unbias, Generalization

Summary: 
This work addresses the issue of bias in medical diagnosis models, specifically focusing on class-feature bias. The study identifies that models may rely on features that are strongly correlated with only a subset of classes, leading to biased performance and poor generalization on other classes. To combat this, the researchers propose a class-unbiased model called Cls-unbias, which aims to mitigate class imbalance and class-feature bias simultaneously. They introduce a class-wise inequality loss to promote equal contributions of classification loss from positive and negative class samples. Moreover, they optimize a class-wise group distributionally robust optimization objective to enhance the effectiveness of the inequality loss under class imbalance. Through experiments on synthetic and real-world datasets, the study demonstrates the negative impact of class-feature bias on model performance and shows that their proposed method effectively mitigates both class-feature bias and class imbalance, leading to improved generalization ability. 

<br /><br />Summary: <div>
arXiv:2508.06943v1 Announce Type: new 
Abstract: Medical diagnosis might fail due to bias. In this work, we identified class-feature bias, which refers to models' potential reliance on features that are strongly correlated with only a subset of classes, leading to biased performance and poor generalization on other classes. We aim to train a class-unbiased model (Cls-unbias) that mitigates both class imbalance and class-feature bias simultaneously. Specifically, we propose a class-wise inequality loss which promotes equal contributions of classification loss from positive-class and negative-class samples. We propose to optimize a class-wise group distributionally robust optimization objective-a class-weighted training objective that upweights underperforming classes-to enhance the effectiveness of the inequality loss under class imbalance. Through synthetic and real-world datasets, we empirically demonstrate that class-feature bias can negatively impact model performance. Our proposed method effectively mitigates both class-feature bias and class imbalance, thereby improving the model's generalization ability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</title>
<link>https://arxiv.org/abs/2508.06944</link>
<guid>https://arxiv.org/abs/2508.06944</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Adaptive Meta Fine-Tuning, implicit rewards, reinforcement learning, state-of-the-art 

Summary: 
Adaptive Meta Fine-Tuning (AMFT) is a novel single-stage algorithm that optimally balances Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) through a learnable parameter. By treating SFT and RL as complementary reward signals, AMFT dynamically adjusts the balance to maximize long-term task performance. Regularized by policy entropy for stability, AMFT autonomously discovers an effective training curriculum, leading to state-of-the-art results on challenging benchmarks in mathematical reasoning, abstract visual reasoning, and vision-language navigation. The meta-learning controller in AMFT plays a critical role in stability, sample efficiency, and performance, offering a more principled and effective paradigm for Large Language Model alignment. The open-sourced codes for AMFT are available at https://github.com/hlxtsyj/AMFT.

<br /><br />Summary: <div>
arXiv:2508.06944v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of \textbf{implicit rewards}, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity</title>
<link>https://arxiv.org/abs/2508.06953</link>
<guid>https://arxiv.org/abs/2508.06953</guid>
<content:encoded><![CDATA[
<div> Low-rank adaptation, parameter-efficient fine-tuning, large language models, BoRA, block diversified, block matrix multiplication <br />
Summary: <br />
Low-rank adaptation (LoRA) is a technique used in large language models to fine-tune pretrained weight matrices efficiently. The Block Diversified Low-Rank Adaptation (BoRA) method proposed in this paper enhances the rank of LoRA weights with minimal additional parameters. By treating the weight product as a block matrix multiplication and introducing unique diagonal matrices for each block, BoRA increases the rank of LoRA weights by a factor of the number of blocks used, while only requiring a small number of additional parameters. Experimental results across various datasets and models showcase the effectiveness of BoRA, and ablation studies confirm its scalability. <div>
arXiv:2508.06953v1 Announce Type: new 
Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs). It approximates the update of a pretrained weight matrix $W\in\mathbb{R}^{m\times n}$ by the product of two low-rank matrices, $BA$, where $A \in\mathbb{R}^{r\times n}$ and $B\in\mathbb{R}^{m\times r} (r\ll\min\{m,n\})$. Increasing the dimension $r$ can raise the rank of LoRA weights (i.e., $BA$), which typically improves fine-tuning performance but also significantly increases the number of trainable parameters. In this paper, we propose Block Diversified Low-Rank Adaptation (BoRA), which improves the rank of LoRA weights with a small number of additional parameters. Specifically, BoRA treats the product $BA$ as a block matrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along the columns and rows, respectively (i.e., $A=[A_1,\dots,A_b]$ and $B=[B_1,\dots,B_b]^\top$). Consequently, the product $BA$ becomes the concatenation of the block products $B_iA_j$ for $i,j\in[b]$. To enhance the diversity of different block products, BoRA introduces a unique diagonal matrix $\Sigma_{i,j} \in \mathbb{R}^{r\times r}$ for each block multiplication, resulting in $B_i \Sigma_{i,j} A_j$. By leveraging these block-wise diagonal matrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only requiring $b^2r$ additional parameters. Extensive experiments across multiple datasets and models demonstrate the superiority of BoRA, and ablation studies further validate its scalability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Multitask Learning Enhance Model Explainability?</title>
<link>https://arxiv.org/abs/2508.06966</link>
<guid>https://arxiv.org/abs/2508.06966</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, multimodal learning networks, multitask learning, interpretability, satellite data

Summary:
Multimodal learning networks leverage the diversity of satellite data for improved performance but lack interpretability. This study explores using multitask learning to explain model behavior by predicting certain modalities as additional targets along with the main task. The approach benefits from the information richness of satellite data, requiring no additional data collection at deployment and maintaining performance comparable to multimodal baselines. In some cases, it even outperforms them. Additionally, errors in the main task can be explained through the model's behavior in the auxiliary tasks. The efficiency of this approach is demonstrated across three datasets involving segmentation, classification, and regression tasks. The code for this approach is available at git.opendfki.de/hiba.najjar/mtl_explainability/. 

<br /><br />Summary: <div>
arXiv:2508.06966v1 Announce Type: new 
Abstract: Remote sensing provides satellite data in diverse types and formats. The usage of multimodal learning networks exploits this diversity to improve model performance, except that the complexity of such networks comes at the expense of their interpretability. In this study, we explore how modalities can be leveraged through multitask learning to intrinsically explain model behavior. In particular, instead of additional inputs, we use certain modalities as additional targets to be predicted along with the main task. The success of this approach relies on the rich information content of satellite data, which remains as input modalities. We show how this modeling context provides numerous benefits: (1) in case of data scarcity, the additional modalities do not need to be collected for model inference at deployment, (2) the model performance remains comparable to the multimodal baseline performance, and in some cases achieves better scores, (3) prediction errors in the main task can be explained via the model behavior in the auxiliary task(s). We demonstrate the efficiency of our approach on three datasets, including segmentation, classification, and regression tasks. Code available at git.opendfki.de/hiba.najjar/mtl_explainability/.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Preserving Digital Twins via Conditional Neural Whitney Forms</title>
<link>https://arxiv.org/abs/2508.06981</link>
<guid>https://arxiv.org/abs/2508.06981</guid>
<content:encoded><![CDATA[
<div> framework, real-time digital twins, reduced finite element models, conditional attention mechanisms, finite element exterior calculus (FEEC)

Summary:
The article introduces a framework for constructing real-time digital twins using reduced finite element models conditioned on a latent variable Z. This framework incorporates conditional attention mechanisms to learn a reduced finite element basis and a nonlinear conservation law within the context of finite element exterior calculus (FEEC), ensuring numerical well-posedness and exact conservation of quantities. The method allows for real-time calibration to parametric variables, enabling the development of digital twins that support closed-loop inference and calibration to sensor data. It seamlessly integrates with traditional finite element techniques, facilitating the treatment of complex geometries. Benchmarks demonstrate the method's accuracy in predicting phenomena such as advection diffusion, shock hydrodynamics, electrostatics, and battery thermal runaway. The approach achieves precise predictions on intricate geometries with sparse data, including capturing the transition to turbulence, and enables real-time inference with a significant speedup compared to traditional simulations. An open-source implementation of the framework is available on GitHub. 

<br /><br />Summary: <div>
arXiv:2508.06981v1 Announce Type: new 
Abstract: We present a framework for constructing real-time digital twins based on structure-preserving reduced finite element models conditioned on a latent variable Z. The approach uses conditional attention mechanisms to learn both a reduced finite element basis and a nonlinear conservation law within the framework of finite element exterior calculus (FEEC). This guarantees numerical well-posedness and exact preservation of conserved quantities, regardless of data sparsity or optimization error. The conditioning mechanism supports real-time calibration to parametric variables, allowing the construction of digital twins which support closed loop inference and calibration to sensor data. The framework interfaces with conventional finite element machinery in a non-invasive manner, allowing treatment of complex geometries and integration of learned models with conventional finite element techniques.
  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics, and a complex battery thermal runaway problem. The method achieves accurate predictions on complex geometries with sparse data (25 LES simulations), including capturing the transition to turbulence and achieving real-time inference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source implementation is available on GitHub.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovery Learning accelerates battery design evaluation</title>
<link>https://arxiv.org/abs/2508.06985</link>
<guid>https://arxiv.org/abs/2508.06985</guid>
<content:encoded><![CDATA[
<div> Discovery Learning, battery research, machine learning, active learning, battery lifetime prediction <br />
<br />
Summary: <br />
A new scientific machine-learning paradigm called Discovery Learning (DL) is introduced to accelerate battery design evaluation. DL integrates active learning, physics-guided learning, and zero-shot learning to reduce prototyping needs and improve efficiency in predicting battery lifetime. Testing DL on industrial lithium-ion pouch cells shows a 7.2% test error in predicting cycle life without additional data labeling. This approach saves 98% in time and 95% in energy compared to traditional practices. By learning from historical designs, DL enables rapid evaluation of novel material-design combinations, aiding in the development of next-generation battery technologies. This work demonstrates the potential of leveraging machine learning for efficient data-driven modeling in scientific discovery and engineering innovation. <br /> <div>
arXiv:2508.06985v1 Announce Type: new 
Abstract: Fast and reliable validation of novel designs in complex physical systems such as batteries is critical to accelerating technological innovation. However, battery research and development remain bottlenecked by the prohibitively high time and energy costs required to evaluate numerous new design candidates, particularly in battery prototyping and life testing. Despite recent progress in data-driven battery lifetime prediction, existing methods require labeled data of target designs to improve accuracy and cannot make reliable predictions until after prototyping, thus falling far short of the efficiency needed to enable rapid feedback for battery design. Here, we introduce Discovery Learning (DL), a scientific machine-learning paradigm that integrates active learning, physics-guided learning, and zero-shot learning into a human-like reasoning loop, drawing inspiration from learning theories in educational psychology. DL can learn from historical battery designs and actively reduce the need for prototyping, thus enabling rapid lifetime evaluation for unobserved material-design combinations without requiring additional data labeling. To test DL, we present 123 industrial-grade large-format lithium-ion pouch cells, spanning eight material-design combinations and diverse cycling protocols. Trained solely on public datasets of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting the average cycle life under unknown device variability. This results in savings of 98% in time and 95% in energy compared to industrial practices. This work highlights the potential of uncovering insights from historical designs to inform and accelerate the development of next-generation battery technologies. DL represents a key advance toward efficient data-driven modeling and helps realize the promise of machine learning for accelerating scientific discovery and engineering innovation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniMove: A Unified Model for Multi-city Human Mobility Prediction</title>
<link>https://arxiv.org/abs/2508.06986</link>
<guid>https://arxiv.org/abs/2508.06986</guid>
<content:encoded><![CDATA[
<div> Keywords: Human mobility prediction, UniMove, multi-city, spatial representations, MoE Transformer

Summary:
UniMove is a unified model designed for multi-city human mobility prediction to address challenges such as randomness, non-uniform time intervals, and complex patterns in human mobility. The model incorporates a trajectory-location dual-tower architecture, utilizing a location tower for universal spatial encoding and a trajectory tower for sequential mobility modeling. Additionally, MoE Transformer blocks are used to adaptively select experts to handle diverse movement patterns. By enabling joint training on multi-city data and mutual data enhancement, UniMove significantly improves mobility prediction accuracy by over 10.2%. The model demonstrates the capability to capture heterogeneous mobility patterns across different cities through the use of universal spatial representations. This advancement represents a foundational model with a unified architecture for human mobility prediction. The implementation of UniMove is publicly available on GitHub for further research and development. 

<br /><br />Summary: <div>
arXiv:2508.06986v1 Announce Type: new 
Abstract: Human mobility prediction is vital for urban planning, transportation optimization, and personalized services. However, the inherent randomness, non-uniform time intervals, and complex patterns of human mobility, compounded by the heterogeneity introduced by varying city structures, infrastructure, and population densities, present significant challenges in modeling. Existing solutions often require training separate models for each city due to distinct spatial representations and geographic coverage. In this paper, we propose UniMove, a unified model for multi-city human mobility prediction, addressing two challenges: (1) constructing universal spatial representations for effective token sharing across cities, and (2) modeling heterogeneous mobility patterns from varying city characteristics. We propose a trajectory-location dual-tower architecture, with a location tower for universal spatial encoding and a trajectory tower for sequential mobility modeling. We also design MoE Transformer blocks to adaptively select experts to handle diverse movement patterns. Extensive experiments across multiple datasets from diverse cities demonstrate that UniMove truly embodies the essence of a unified model. By enabling joint training on multi-city data with mutual data enhancement, it significantly improves mobility prediction accuracy by over 10.2\%. UniMove represents a key advancement toward realizing a true foundational model with a unified architecture for human mobility. We release the implementation at https://github.com/tsinghua-fib-lab/UniMove/.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Feature Selection in Tsetlin Machines</title>
<link>https://arxiv.org/abs/2508.06991</link>
<guid>https://arxiv.org/abs/2508.06991</guid>
<content:encoded><![CDATA[
<div> Keywords: Feature Selection, Tsetlin Machine, Interpretable Learning, Model Interpretability, Embedded Methods

Summary: 
Feature Selection (FS) plays a crucial role in enhancing model interpretability, reducing complexity, and improving accuracy. This paper focuses on adapting and evaluating various FS techniques for the Tsetlin machine (TM), a model offering interpretable clause-based learning. The study includes classical filter and embedded methods, as well as post-hoc explanation methods like SHAP and LIME. Novel TM-specific scorers derived from TM clause weights and Tsetlin automaton (TA) states are also introduced. Benchmarking on 12 datasets using the ROAR and ROAD strategies shows that TM-internal scorers perform competitively, revealing interacting feature patterns through the interpretability of clauses. Additionally, simpler TM-specific scorers demonstrate similar accuracy retention at lower computational costs. This study establishes a comprehensive baseline for FS in TM and lays the groundwork for developing specialized TM-specific interpretability techniques. 

<br /><br />Summary: <div>
arXiv:2508.06991v1 Announce Type: new 
Abstract: Feature Selection (FS) is crucial for improving model interpretability, reducing complexity, and sometimes for enhancing accuracy. The recently introduced Tsetlin machine (TM) offers interpretable clause-based learning, but lacks established tools for estimating feature importance. In this paper, we adapt and evaluate a range of FS techniques for TMs, including classical filter and embedded methods as well as post-hoc explanation methods originally developed for neural networks (e.g., SHAP and LIME) and a novel family of embedded scorers derived from TM clause weights and Tsetlin automaton (TA) states. We benchmark all methods across 12 datasets, using evaluation protocols, like Remove and Retrain (ROAR) strategy and Remove and Debias (ROAD), to assess causal impact. Our results show that TM-internal scorers not only perform competitively but also exploit the interpretability of clauses to reveal interacting feature patterns. Simpler TM-specific scorers achieve similar accuracy retention at a fraction of the computational cost. This study establishes the first comprehensive baseline for FS in TM and paves the way for developing specialized TM-specific interpretability techniques.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Set-based Human-AI Complementarity with Multiple Experts</title>
<link>https://arxiv.org/abs/2508.06997</link>
<guid>https://arxiv.org/abs/2508.06997</guid>
<content:encoded><![CDATA[
<div> Conformal prediction, decision support systems, human-AI collaboration, expert selection, subset selection <br />
Summary: <br />
Decision support systems assist human experts in classification tasks by providing conformal prediction sets from a pre-trained model. This study focuses on selecting instance-specific experts from a pool of multiple human experts, improving classification performance. The research contrasts with single-expert scenarios and examines conditions where multiple experts benefit from conformal sets. A greedy algorithm utilizing conformal sets is introduced for subset selection, outperforming naive methods. Real expert predictions from CIFAR-10H and ImageNet-16H datasets show the algorithm achieves near-optimal subsets, enhancing classification performance among multiple experts. <div>
arXiv:2508.06997v1 Announce Type: new 
Abstract: Decision support systems are designed to assist human experts in classification tasks by providing conformal prediction sets derived from a pre-trained model. This human-AI collaboration has demonstrated enhanced classification performance compared to using either the model or the expert independently. In this study, we focus on the selection of instance-specific experts from a pool of multiple human experts, contrasting it with existing research that typically focuses on single-expert scenarios. We characterize the conditions under which multiple experts can benefit from the conformal sets. With the insight that only certain experts may be relevant for each instance, we explore the problem of subset selection and introduce a greedy algorithm that utilizes conformal sets to identify the subset of expert predictions that will be used in classifying an instance. This approach is shown to yield better performance compared to naive methods for human subset selection. Based on real expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation study indicates that our proposed greedy algorithm achieves near-optimal subsets, resulting in improved classification performance among multiple experts.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations</title>
<link>https://arxiv.org/abs/2508.07016</link>
<guid>https://arxiv.org/abs/2508.07016</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series forecasting, deep learning, cross-correlations, Sequence Shifted Dynamic Time Warping (SSDTW), contrastive learning

Summary: 
The paper introduces the Time-Lagged Cross-Correlations-based Sequence Prediction framework (TLCCSP) for time series forecasting, aiming to capture time-lagged cross-correlations often overlooked by existing deep learning models. The framework incorporates the Sequence Shifted Dynamic Time Warping (SSDTW) algorithm to capture lagged correlations and a contrastive learning-based encoder to improve accuracy. Experimental results on weather, finance, and real estate datasets demonstrate the effectiveness of TLCCSP, with significant reductions in mean squared error (MSE). For instance, on the weather dataset, SSDTW and contrastive learning encoder (CLE) achieve MSE reductions of 16.01% and 17.88% respectively. Additionally, the contrastive learning approach reduces computational time by 99%, ensuring scalability and real-time applicability in various time series forecasting tasks. This framework presents a promising approach for enhancing forecasting accuracy by effectively incorporating time-lagged cross-correlated sequences. 

<br /><br />Summary: <div>
arXiv:2508.07016v1 Announce Type: new 
Abstract: Time series forecasting is critical across various domains, such as weather, finance and real estate forecasting, as accurate forecasts support informed decision-making and risk mitigation. While recent deep learning models have improved predictive capabilities, they often overlook time-lagged cross-correlations between related sequences, which are crucial for capturing complex temporal relationships. To address this, we propose the Time-Lagged Cross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances forecasting accuracy by effectively integrating time-lagged cross-correlated sequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW) algorithm to capture lagged correlations and a contrastive learning-based encoder to efficiently approximate SSDTW distances.
  Experimental results on weather, finance and real estate time series datasets demonstrate the effectiveness of our framework. On the weather dataset, SSDTW reduces mean squared error (MSE) by 16.01% compared with single-sequence methods, while the contrastive learning encoder (CLE) further decreases MSE by 17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE reduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by 21.29% and 8.62%, respectively. Additionally, the contrastive learning approach decreases SSDTW computational time by approximately 99%, ensuring scalability and real-time applicability across multiple time series forecasting tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.07029</link>
<guid>https://arxiv.org/abs/2508.07029</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, Behavioral Cloning, Offline Reinforcement Learning, Conservative Q-Learning, autonomous driving

Summary:
The study addresses the challenge of learning robust driving policies in autonomous vehicles using large-scale, real-world datasets. While Behavioral Cloning (BC) provides a simple imitation learning approach, policies trained with BC are prone to errors. The researchers developed a series of BC baselines, including a Transformer-based model, which still struggled in long-horizon simulations. By applying the Offline Reinforcement Learning algorithm, Conservative Q-Learning (CQL), to the same data and architecture, a more robust policy was learned. The CQL agent, with a carefully engineered reward function, achieved significantly higher success and lower collision rates than the BC baseline in a large-scale evaluation on unseen scenarios. This demonstrates the importance of an offline RL approach in learning robust driving policies from static expert data.<br /><br />Summary: <div>
arXiv:2508.07029v1 Announce Type: new 
Abstract: Learning robust driving policies from large-scale, real-world datasets is a central challenge in autonomous driving, as online data collection is often unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward approach to imitation learning, policies trained with BC are notoriously brittle and suffer from compounding errors in closed-loop execution. This work presents a comprehensive pipeline and a comparative study to address this limitation. We first develop a series of increasingly sophisticated BC baselines, culminating in a Transformer-based model that operates on a structured, entity-centric state representation. While this model achieves low imitation loss, we show that it still fails in long-horizon simulations. We then demonstrate that by applying a state-of-the-art Offline Reinforcement Learning algorithm, Conservative Q-Learning (CQL), to the same data and architecture, we can learn a significantly more robust policy. Using a carefully engineered reward function, the CQL agent learns a conservative value function that enables it to recover from minor errors and avoid out-of-distribution states. In a large-scale evaluation on 1,000 unseen scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a 3.2x higher success rate and a 7.4x lower collision rate than the strongest BC baseline, proving that an offline RL approach is critical for learning robust, long-horizon driving policies from static expert data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling</title>
<link>https://arxiv.org/abs/2508.07032</link>
<guid>https://arxiv.org/abs/2508.07032</guid>
<content:encoded><![CDATA[
<div> diffusion process, neurodegenerative diseases, brain connectome, Mixture of Experts (MoE), graph neural diffusion model (IGND)

Summary:
The article introduces a novel stage-aware Mixture of Experts (MoE) framework to model the progression of neurodegenerative diseases. It addresses challenges in longitudinal data scarcity and the complex interplay of pathological mechanisms by incorporating time-dependent expert weighting. The framework utilizes an iterative dual optimization method to estimate the temporal position of observations and constructs a cohort-level progression trajectory from irregular snapshots. It enhances the spatial component with an inhomogeneous graph neural diffusion model (IGND) that varies diffusivity based on node states and time. Additionally, a localized neural reaction module captures complex dynamics beyond standard processes. The resulting model dynamically integrates components across temporal states, providing insights into stage-specific pathological mechanisms. The study suggests that graph-related processes are more influential at early stages, while other physical processes dominate later on. <div>
arXiv:2508.07032v1 Announce Type: new 
Abstract: The long-term progression of neurodegenerative diseases is commonly conceptualized as a spatiotemporal diffusion process that consists of a graph diffusion process across the structural brain connectome and a localized reaction process within brain regions. However, modeling this progression remains challenging due to 1) the scarcity of longitudinal data obtained through irregular and infrequent subject visits and 2) the complex interplay of pathological mechanisms across brain regions and disease stages, where traditional models assume fixed mechanisms throughout disease progression. To address these limitations, we propose a novel stage-aware Mixture of Experts (MoE) framework that explicitly models how different contributing mechanisms dominate at different disease stages through time-dependent expert weighting.Data-wise, we utilize an iterative dual optimization method to properly estimate the temporal position of individual observations, constructing a co hort-level progression trajectory from irregular snapshots. Model-wise, we enhance the spatial component with an inhomogeneous graph neural diffusion model (IGND) that allows diffusivity to vary based on node states and time, providing more flexible representations of brain networks. We also introduce a localized neural reaction module to capture complex dynamics beyond standard processes.The resulting IGND-MoE model dynamically integrates these components across temporal states, offering a principled way to understand how stage-specific pathological mechanisms contribute to progression. The stage-wise weights yield novel clinical insights that align with literature, suggesting that graph-related processes are more influential at early stages, while other unknown physical processes become dominant later on.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Adaptive Kalman Filtering via Optimal Transport</title>
<link>https://arxiv.org/abs/2508.07037</link>
<guid>https://arxiv.org/abs/2508.07037</guid>
<content:encoded><![CDATA[
<div> Keywords: Learning-based filtering, noise-statistics drift, adaptive Kalman filtering, optimal transport, online adaptation

Summary:
OTAKNet proposes an online solution for handling noise-statistics drift in learning-based adaptive Kalman filtering. Unlike existing methods, OTAKNet utilizes optimal transport to address drift in noise statistics, allowing for online adaptation without the need for ground truth labels or retraining. The approach leverages optimal transport's geometry-aware cost and stable gradients to maintain performance in dynamic environments. Compared to classical model-based adaptive Kalman filtering and offline learning-based methods, OTAKNet demonstrates superior performance on synthetic and real-world datasets from the NCLT dataset, especially in situations with limited training data. This innovative approach shows promise for applications in non-linear dynamical systems where noise statistics can change due to environmental factors, ensuring robust and reliable performance over time.
<br /><br />Summary: <div>
arXiv:2508.07037v1 Announce Type: new 
Abstract: Learning-based filtering has demonstrated strong performance in non-linear dynamical systems, particularly when the statistics of noise are unknown. However, in real-world deployments, environmental factors, such as changing wind conditions or electromagnetic interference, can induce unobserved noise-statistics drift, leading to substantial degradation of learning-based methods. To address this challenge, we propose OTAKNet, the first online solution to noise-statistics drift within learning-based adaptive Kalman filtering. Unlike existing learning-based methods that perform offline fine-tuning using batch pointwise matching over entire trajectories, OTAKNet establishes a connection between the state estimate and the drift via one-step predictive measurement likelihood, and addresses it using optimal transport. This leverages OT's geometry - aware cost and stable gradients to enable fully online adaptation without ground truth labels or retraining. We compare OTAKNet against classical model-based adaptive Kalman filtering and offline learning-based filtering. The performance is demonstrated on both synthetic and real-world NCLT datasets, particularly under limited training data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership and Memorization in LLM Knowledge Distillation</title>
<link>https://arxiv.org/abs/2508.07054</link>
<guid>https://arxiv.org/abs/2508.07054</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Distillation, Large Language Models, Privacy Risks, NLP Tasks, Teacher-Student Model Relationship

Summary: 
This study explores the privacy risks associated with transferring knowledge from large language models (LLMs) to smaller student models using Knowledge Distillation (KD) techniques. By analyzing the impact of different KD components and model configurations, the researchers demonstrate that all existing LLM KD approaches pose membership and memorization privacy risks from the teacher to the student models. The extent of these risks varies across different KD techniques, highlighting the need for careful consideration when implementing LLM KD. The study also reveals significant disparities between memorization and membership privacy risks in LLM KD techniques. Furthermore, the researchers find that privacy risks can differ significantly across different model blocks. This research provides valuable insights into the privacy implications of using KD to train LLMs and emphasizes the importance of enhancing privacy protections in the development and deployment of such models.

<br /><br />Summary: <div>
arXiv:2508.07054v1 Announce Type: new 
Abstract: Recent advances in Knowledge Distillation (KD) aim to mitigate the high computational demands of Large Language Models (LLMs) by transferring knowledge from a large ''teacher'' to a smaller ''student'' model. However, students may inherit the teacher's privacy when the teacher is trained on private data. In this work, we systematically characterize and investigate membership and memorization privacy risks inherent in six LLM KD techniques. Using instruction-tuning settings that span seven NLP tasks, together with three teacher model families (GPT-2, LLAMA-2, and OPT), and various size student models, we demonstrate that all existing LLM KD approaches carry membership and memorization privacy risks from the teacher to its students. However, the extent of privacy risks varies across different KD techniques. We systematically analyse how key LLM KD components (KD objective functions, student training data and NLP tasks) impact such privacy risks. We also demonstrate a significant disagreement between memorization and membership privacy risks of LLM KD techniques. Finally, we characterize per-block privacy risk and demonstrate that the privacy risk varies across different blocks by a large margin.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation</title>
<link>https://arxiv.org/abs/2508.07075</link>
<guid>https://arxiv.org/abs/2508.07075</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, knowledge updates, catastrophic forgetting, fine-tuning, interpretability-guided intervention <br />
Summary: 
This paper introduces a novel "unlearn-then-learn" strategy for precise knowledge editing in Large Language Models (LLMs). The strategy leverages the parameter-efficient fine-tuning technique, Infused Adapter by Inhibiting and Amplifying Inner Activations ($IA^3). It involves a two-stage approach with an initial circuit localization phase to target specific internal components responsible for conflicting facts. Experimental results on microsoft/Phi-3-mini-4k-instruct demonstrate near-perfect accuracy for new facts while effectively suppressing conflicting ones. The strategy shows unprecedented localization and mitigates catastrophic forgetting observed in direct fine-tuning approaches. Qualitative analysis reveals a mechanism of "soft forgetting" where original knowledge is suppressed from default retrieval but remains conditionally accessible, enhancing model safety. This advancement offers precise, localized, and safe knowledge management in compact LLMs.<br /><br />Summary: <div>
arXiv:2508.07075v1 Announce Type: new 
Abstract: Large Language Models (LLMs) struggle with dynamic knowledge updates, especially when new information conflicts with deeply embedded facts. Such conflicting factual edits often lead to two critical issues: resistance to adopting the new fact and severe catastrophic forgetting of unrelated knowledge. This paper introduces and evaluates a novel "unlearn-then-learn" strategy for precise knowledge editing in LLMs, leveraging the parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting and Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach is powered by an initial circuit localization phase that identifies and targets the specific internal components responsible for encoding the conflicting fact. Through a rigorous experimental methodology on microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically informed two-stage approach achieves near-perfect accuracy (98.50%) for the new, modulated fact while simultaneously effectively suppressing the original conflicting fact (96.00% forget rate). Critically, our strategy exhibits unprecedented localization (72.00% F_control accuracy), dramatically mitigating catastrophic forgetting observed in direct fine-tuning approaches (which showed as low as ~20% F_control accuracy), a direct benefit of our targeted interpretability-guided intervention. Furthermore, qualitative analysis reveals a nuanced mechanism of "soft forgetting," where original knowledge is suppressed from default retrieval but remains latent and conditionally accessible, enhancing model safety and control. These findings represent a significant advancement towards precise, localized, and safe knowledge management in compact LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework</title>
<link>https://arxiv.org/abs/2508.07085</link>
<guid>https://arxiv.org/abs/2508.07085</guid>
<content:encoded><![CDATA[
<div> Transformers, Autoencoders, concept drift detection, Trust Score methodology, time sequenced data <br />
Summary: <br />
This study introduces a hybrid framework using Transformers and Autoencoders for detecting concept drift in applied machine learning. The proposed framework combines statistical and reconstruction-based drift metrics, prediction uncertainty, rules violations, and trend of classifier error to calculate a Trust Score for better drift detection. Using a time-sequenced airline passenger dataset with synthetic drift, the model showed early and sensitive detection compared to baseline methods. The Transformer-Autoencoder model outperformed traditional autoencoders in detecting drift, providing improved modeling with lower error rates and logical violations. Overall, the framework offers a robust solution for monitoring concept drift in real-time applications. <div>
arXiv:2508.07085v1 Announce Type: new 
Abstract: In applied machine learning, concept drift, which is either gradual or abrupt changes in data distribution, can significantly reduce model performance. Typical detection methods,such as statistical tests or reconstruction-based models,are generally reactive and not very sensitive to early detection. Our study proposes a hybrid framework consisting of Transformers and Autoencoders to model complex temporal dynamics and provide online drift detection. We create a distinct Trust Score methodology, which includes signals on (1) statistical and reconstruction-based drift metrics, more specifically, PSI, JSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations, and (4) trend of classifier error aligned with the combined metrics defined by the Trust Score. Using a time sequenced airline passenger data set with synthetic drift, our proposed model allows for a better detection of drift using as a whole and at different detection thresholds for both sensitivity and interpretability compared to baseline methods and provides a strong pipeline for drift detection in real time for applied machine learning. We evaluated performance using a time-sequenced airline passenger dataset having the gradually injected stimulus of drift in expectations,e.g. permuted ticket prices in later batches, broken into 10 time segments [1].In the data, our results support that the Transformation-Autoencoder detected drift earlier and with more sensitivity than the autoencoders commonly used in the literature, and provided improved modeling over more error rates and logical violations. Therefore, a robust framework was developed to reliably monitor concept drift.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria</title>
<link>https://arxiv.org/abs/2508.07102</link>
<guid>https://arxiv.org/abs/2508.07102</guid>
<content:encoded><![CDATA[
arXiv:2508.07102v1 Announce Type: new 
Abstract: Generative modelling has seen significant advances through simulation-free paradigms such as Flow Matching, and in particular, the MeanFlow framework, which replaces instantaneous velocity fields with average velocities to enable efficient single-step sampling. In this work, we introduce a theoretical study on Second-Order MeanFlow, a novel extension that incorporates average acceleration fields into the MeanFlow objective. We first establish the feasibility of our approach by proving that the average acceleration satisfies a generalized consistency condition analogous to first-order MeanFlow, thereby supporting stable, one-step sampling and tractable loss functions. We then characterize its expressivity via circuit complexity analysis, showing that under mild assumptions, the Second-Order MeanFlow sampling process can be implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class. Finally, we derive provably efficient criteria for scalable implementation by leveraging fast approximate attention computations: we prove that attention operations within the Second-Order MeanFlow architecture can be approximated to within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results lay the theoretical foundation for high-order flow matching models that combine rich dynamics with practical sampling efficiency.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation</title>
<link>https://arxiv.org/abs/2508.07106</link>
<guid>https://arxiv.org/abs/2508.07106</guid>
<content:encoded><![CDATA[
arXiv:2508.07106v1 Announce Type: new 
Abstract: Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely used to study human brain activity. fMRI signals in areas across the brain transiently synchronise and desynchronise their activity in a highly structured manner, even when an individual is at rest. These functional connectivity dynamics may be related to behaviour and neuropsychiatric disease. To model these dynamics, temporal brain connectivity representations are essential, as they reflect evolving interactions between brain regions and provide insight into transient neural states and network reconfigurations. However, conventional graph neural networks (GNNs) often struggle to capture long-range temporal dependencies in dynamic fMRI data. To address this challenge, we propose BrainATCL, an unsupervised, nonparametric framework for adaptive temporal brain connectivity learning, enabling functional link prediction and age estimation. Our method dynamically adjusts the lookback window for each snapshot based on the rate of newly added edges. Graph sequences are subsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal representations of dynamic functional connectivity in resting-state fMRI data of 1,000 participants from the Human Connectome Project. To further improve spatial modeling, we incorporate brain structure and function-informed edge attributes, i.e., the left/right hemispheric identity and subnetwork membership of brain regions, enabling the model to capture biologically meaningful topological patterns. We evaluate our BrainATCL on two tasks: functional link prediction and age estimation. The experimental results demonstrate superior performance and strong generalization, including in cross-session prediction scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning</title>
<link>https://arxiv.org/abs/2508.07114</link>
<guid>https://arxiv.org/abs/2508.07114</guid>
<content:encoded><![CDATA[
arXiv:2508.07114v1 Announce Type: new 
Abstract: In this work, we propose a new machine learning (ML) methodology to obtain more precise predictions for some parameters of interest in a given hypotheses testing problem. Our proposed method also allows ML models to have more discriminative power in cases where it is extremely challenging for state-of-the-art classifiers to have any level of accurate predictions. This method can also allow us to systematically decrease the error from ML models in their predictions. In this paper, we provide a mathematical motivation why Multiple Instance Learning (MIL) would have more predictive power over their single-instance counterparts. We support our theoretical claims by analyzing the behavior of the MIL models through their scaling behaviors with respect to the number of instances on which the model makes predictions. As a concrete application, we constrain Wilson coefficients of the Standard Model Effective Field Theory (SMEFT) using kinematic information from subatomic particle collision events at the Large Hadron Collider (LHC). We show that under certain circumstances, it might be possible to extract the theoretical maximum Fisher Information latent in a dataset.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context</title>
<link>https://arxiv.org/abs/2508.07117</link>
<guid>https://arxiv.org/abs/2508.07117</guid>
<content:encoded><![CDATA[
arXiv:2508.07117v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over structured data, including text-attributed graphs, which are common in domains such as citation networks, social platforms, and knowledge graphs. GNNs are not inherently interpretable and thus, many explanation methods have been proposed. However, existing explanation methods often struggle to generate interpretable, fine-grained rationales, especially when node attributes include rich natural language. In this work, we introduce LOGIC, a lightweight, post-hoc framework that uses large language models (LLMs) to generate faithful and interpretable explanations for GNN predictions. LOGIC projects GNN node embeddings into the LLM embedding space and constructs hybrid prompts that interleave soft prompts with textual inputs from the graph structure. This enables the LLM to reason about GNN internal representations and produce natural language explanations along with concise explanation subgraphs. Our experiments across four real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off between fidelity and sparsity, while significantly improving human-centric metrics such as insightfulness. LOGIC sets a new direction for LLM-based explainability in graph learning by aligning GNN internals with human reasoning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks</title>
<link>https://arxiv.org/abs/2508.07122</link>
<guid>https://arxiv.org/abs/2508.07122</guid>
<content:encoded><![CDATA[
arXiv:2508.07122v1 Announce Type: new 
Abstract: This paper proposes a spatiotemporal graph neural network-based performance prediction algorithm to address the challenge of forecasting performance fluctuations in distributed backend systems with multi-level service call structures. The method abstracts system states at different time slices into a sequence of graph structures. It integrates the runtime features of service nodes with the invocation relationships among services to construct a unified spatiotemporal modeling framework. The model first applies a graph convolutional network to extract high-order dependency information from the service topology. Then it uses a gated recurrent network to capture the dynamic evolution of performance metrics over time. A time encoding mechanism is also introduced to enhance the model's ability to represent non-stationary temporal sequences. The architecture is trained in an end-to-end manner, optimizing the multi-layer nested structure to achieve high-precision regression of future service performance metrics. To validate the effectiveness of the proposed method, a large-scale public cluster dataset is used. A series of multi-dimensional experiments are designed, including variations in time windows and concurrent load levels. These experiments comprehensively evaluate the model's predictive performance and stability. The experimental results show that the proposed model outperforms existing representative methods across key metrics such as MAE, RMSE, and R2. It maintains strong robustness under varying load intensities and structural complexities. These results demonstrate the model's practical potential for backend service performance management tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning</title>
<link>https://arxiv.org/abs/2508.07126</link>
<guid>https://arxiv.org/abs/2508.07126</guid>
<content:encoded><![CDATA[
arXiv:2508.07126v1 Announce Type: new 
Abstract: Training reinforcement learning agents with human feedback is crucial when task objectives are difficult to specify through dense reward functions. While prior methods rely on offline trajectory comparisons to elicit human preferences, such data is unavailable in online learning scenarios where agents must adapt on the fly. Recent approaches address this by collecting real-time scalar feedback to guide agent behavior and train reward models for continued learning after human feedback becomes unavailable. However, scalar feedback is often noisy and inconsistent, limiting the accuracy and generalization of learned rewards. We propose Pref-GUIDE, a framework that transforms real-time scalar feedback into preference-based data to improve reward model learning for continual policy training. Pref-GUIDE Individual mitigates temporal inconsistency by comparing agent behaviors within short windows and filtering ambiguous feedback. Pref-GUIDE Voting further enhances robustness by aggregating reward models across a population of users to form consensus preferences. Across three challenging environments, Pref-GUIDE significantly outperforms scalar-feedback baselines, with the voting variant exceeding even expert-designed dense rewards. By reframing scalar feedback as structured preferences with population feedback, Pref-GUIDE offers a scalable and principled approach for harnessing human input in online reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?</title>
<link>https://arxiv.org/abs/2508.07127</link>
<guid>https://arxiv.org/abs/2508.07127</guid>
<content:encoded><![CDATA[
arXiv:2508.07127v1 Announce Type: new 
Abstract: Cardiovascular disease (CVD) prediction remains a tremendous challenge due to its multifactorial etiology and global burden of morbidity and mortality. Despite the growing availability of genomic and electrophysiological data, extracting biologically meaningful insights from such high-dimensional, noisy, and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has been applied effectively to predict structural variations in biological sequences. In this work, we explore the potential of fine-tuned LLMs to predict cardiac diseases and SNPs potentially leading to CVD risk using genetic markers derived from high-throughput genomic profiling. We investigate the effect of genetic patterns associated with cardiac conditions and evaluate how LLMs can learn latent biological relationships from structured and semi-structured genomic data obtained by mapping genetic aspects that are inherited from the family tree. By framing the problem as a Chain of Thought (CoT) reasoning task, the models are prompted to generate disease labels and articulate informed clinical deductions across diverse patient profiles and phenotypes. The findings highlight the promise of LLMs in contributing to early detection, risk assessment, and ultimately, the advancement of personalized medicine in cardiac care.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Globally Optimal Analytic Solution for Semi-Nonnegative Matrix Factorization with Nonnegative or Mixed Inputs</title>
<link>https://arxiv.org/abs/2508.07134</link>
<guid>https://arxiv.org/abs/2508.07134</guid>
<content:encoded><![CDATA[
arXiv:2508.07134v1 Announce Type: new 
Abstract: Semi-Nonnegative Matrix Factorization (semi-NMF) extends classical Nonnegative Matrix Factorization (NMF) by allowing the basis matrix to contain both positive and negative entries, making it suitable for decomposing data with mixed signs. However, most existing semi-NMF algorithms are iterative, non-convex, and prone to local minima. In this paper, we propose a novel method that yields a globally optimal solution to the semi-NMF problem under the Frobenius norm, through an orthogonal decomposition derived from the scatter matrix of the input data. We rigorously prove that our solution attains the global minimum of the reconstruction error. Furthermore, we demonstrate that when the input matrix is nonnegative, our method often achieves lower reconstruction error than standard NMF algorithms, although unfortunately the basis matrix may not satisfy nonnegativity. In particular, in low-rank cases such as rank 1 or 2, our solution reduces exactly to a nonnegative factorization, recovering the NMF structure. We validate our approach through experiments on both synthetic data and the UCI Wine dataset, showing that our method consistently outperforms existing NMF and semi-NMF methods in terms of reconstruction accuracy. These results confirm that our globally optimal, non-iterative formulation offers both theoretical guarantees and empirical advantages, providing a new perspective on matrix factorization in optimization and data analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stable and Principled Loss Function for Direct Language Model Alignment</title>
<link>https://arxiv.org/abs/2508.07137</link>
<guid>https://arxiv.org/abs/2508.07137</guid>
<content:encoded><![CDATA[
arXiv:2508.07137v1 Announce Type: new 
Abstract: The alignment of large language models (LLMs) with human preferences is commonly achieved through Reinforcement Learning from Human Feedback (RLHF). Direct Preference Optimization (DPO) simplified this paradigm by establishing a direct mapping between the optimal policy and a reward function, eliminating the need for an explicit reward model. However, we argue that the DPO loss function is theoretically misaligned with its own derivation, as it promotes the indefinite maximization of a logits difference, which can lead to training instability and reward hacking. In this paper, we propose a novel loss function derived directly from the RLHF optimality condition. Our proposed loss targets a specific, finite value for the logits difference, which is dictated by the underlying reward, rather than its maximization. We provide a theoretical analysis, including a gradient-based comparison, to demonstrate that our method avoids the large gradients that plague DPO when the probability of dispreferred responses approaches zero. This inherent stability prevents reward hacking and leads to more effective alignment. We validate our approach by fine-tuning a Qwen2.5-7B model, showing significant win-rate improvements over a standard DPO baseline and achieving competitive performance against larger models like Llama-3.1-8B.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Incentivization for Locally Differentially Private Federated Learning</title>
<link>https://arxiv.org/abs/2508.07138</link>
<guid>https://arxiv.org/abs/2508.07138</guid>
<content:encoded><![CDATA[
arXiv:2508.07138v1 Announce Type: new 
Abstract: In Federated Learning (FL), multiple clients jointly train a machine learning model by sharing gradient information, instead of raw data, with a server over multiple rounds. To address the possibility of information leakage in spite of sharing only the gradients, Local Differential Privacy (LDP) is often used. In LDP, clients add a selective amount of noise to the gradients before sending the same to the server. Although such noise addition protects the privacy of clients, it leads to a degradation in global model accuracy. In this paper, we model this privacy-accuracy trade-off as a game, where the sever incentivizes the clients to add a lower degree of noise for achieving higher accuracy, while the clients attempt to preserve their privacy at the cost of a potential loss in accuracy. A token based incentivization mechanism is introduced in which the quantum of tokens credited to a client in an FL round is a function of the degree of perturbation of its gradients. The client can later access a newly updated global model only after acquiring enough tokens, which are to be deducted from its balance. We identify the players, their actions and payoff, and perform a strategic analysis of the game. Extensive experiments were carried out to study the impact of different parameters.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGD Convergence under Stepsize Shrinkage in Low-Precision Training</title>
<link>https://arxiv.org/abs/2508.07142</link>
<guid>https://arxiv.org/abs/2508.07142</guid>
<content:encoded><![CDATA[
arXiv:2508.07142v1 Announce Type: new 
Abstract: Low-precision training has become essential for reducing the computational and memory costs of large-scale deep learning. However, quantization of gradients introduces both magnitude shrinkage and additive noise, which can alter the convergence behavior of stochastic gradient descent (SGD). In this work, we study the convergence of SGD under a gradient shrinkage model, where each stochastic gradient is scaled by a factor $q_k \in (0,1]$ and perturbed by zero-mean quantization noise. We show that this shrinkage is equivalent to replacing the nominal stepsize $\mu_k$ with an effective stepsize $\mu_k q_k$, which slows convergence when $q_{\min} < 1$. Under standard smoothness and bounded-variance assumptions, we prove that low-precision SGD still converges, but at a reduced rate determined by $q_{\min}$, and with an increased asymptotic error floor due to quantization noise. We theoretically analyze how reduced numerical precision slows down training by modeling it as gradient shrinkage in the standard SGD convergence framework.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains</title>
<link>https://arxiv.org/abs/2508.07208</link>
<guid>https://arxiv.org/abs/2508.07208</guid>
<content:encoded><![CDATA[
arXiv:2508.07208v1 Announce Type: new 
Abstract: In-context learning (ICL) is a hallmark capability of transformers, through which trained models learn to adapt to new tasks by leveraging information from the input context. Prior work has shown that ICL emerges in transformers due to the presence of special circuits called induction heads. Given the equivalence between induction heads and conditional k-grams, a recent line of work modeling sequential inputs as Markov processes has revealed the fundamental impact of model depth on its ICL capabilities: while a two-layer transformer can efficiently represent a conditional 1-gram model, its single-layer counterpart cannot solve the task unless it is exponentially large. However, for higher order Markov sources, the best known constructions require at least three layers (each with a single attention head) - leaving open the question: can a two-layer single-head transformer represent any kth-order Markov process? In this paper, we precisely address this and theoretically show that a two-layer transformer with one head per layer can indeed represent any conditional k-gram. Thus, our result provides the tightest known characterization of the interplay between transformer depth and Markov order for ICL. Building on this, we further analyze the learning dynamics of our two-layer construction, focusing on a simplified variant for first-order Markov chains, illustrating how effective in-context representations emerge during training. Together, these results deepen our current understanding of transformer-based ICL and illustrate how even shallow architectures can surprisingly exhibit strong ICL capabilities on structured sequence modeling tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Bridge Processes</title>
<link>https://arxiv.org/abs/2508.07220</link>
<guid>https://arxiv.org/abs/2508.07220</guid>
<content:encoded><![CDATA[
arXiv:2508.07220v1 Announce Type: new 
Abstract: Learning stochastic functions from partially observed context-target pairs is a fundamental problem in probabilistic modeling. Traditional models like Gaussian Processes (GPs) face scalability issues with large datasets and assume Gaussianity, limiting their applicability. While Neural Processes (NPs) offer more flexibility, they struggle with capturing complex, multi-modal target distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a learned diffusion process but rely solely on conditional signals in the denoising network, resulting in weak input coupling from an unconditional forward process and semantic mismatch at the diffusion endpoint. In this work, we propose Neural Bridge Processes (NBPs), a novel method for modeling stochastic functions where inputs x act as dynamic anchors for the entire diffusion trajectory. By reformulating the forward kernel to explicitly depend on x, NBP enforces a constrained path that strictly terminates at the supervised target. This approach not only provides stronger gradient signals but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG signal regression and image regression tasks, achieving substantial improvements over baselines. These results underscore the effectiveness of DDPM-style bridge sampling in enhancing both performance and theoretical consistency for structured prediction tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference</title>
<link>https://arxiv.org/abs/2508.07221</link>
<guid>https://arxiv.org/abs/2508.07221</guid>
<content:encoded><![CDATA[
arXiv:2508.07221v1 Announce Type: new 
Abstract: Estimating individualized treatment effects from observational data presents a persistent challenge due to unmeasured confounding and structural bias. Causal Machine Learning (causal ML) methods, such as causal trees and doubly robust estimators, provide tools for estimating conditional average treatment effects. These methods have limited effectiveness in complex real-world environments due to the presence of latent confounders or those described in unstructured formats. Moreover, reliance on domain experts for confounder identification and rule interpretation introduces high annotation cost and scalability concerns. In this work, we proposed Large Language Model-based agents for automated confounder discovery and subgroup analysis that integrate agents into the causal ML pipeline to simulate domain expertise. Our framework systematically performs subgroup identification and confounding structure discovery by leveraging the reasoning capabilities of LLM-based agents, which reduces human dependency while preserving interpretability. Experiments on real-world medical datasets show that our proposed approach enhances treatment effect estimation robustness by narrowing confidence intervals and uncovering unrecognized confounding biases. Our findings suggest that LLM-based agents offer a promising path toward scalable, trustworthy, and semantically aware causal inference.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning</title>
<link>https://arxiv.org/abs/2508.07224</link>
<guid>https://arxiv.org/abs/2508.07224</guid>
<content:encoded><![CDATA[
arXiv:2508.07224v1 Announce Type: new 
Abstract: We present EDGE, a general-purpose, misconception-aware adaptive learning framework composed of four stages: Evaluate (ability and state estimation), Diagnose (posterior infer-ence of misconceptions), Generate (counterfactual item synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies psychometrics (IRT/Bayesian state space models), cog-nitive diagnostics (misconception discovery from distractor patterns and response latencies), contrastive item generation (minimal perturbations that invalidate learner shortcuts while pre-serving psychometric validity), and principled scheduling (a restless bandit approximation to spaced retrieval). We formalize a composite readiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity, and derive an index policy that is near-optimal under mild assumptions on forgetting and learning gains. We further establish conditions under which counterfactual items provably reduce the posterior probability of a targeted misconception faster than standard practice. The paper focuses on theory and implementable pseudocode; empirical study is left to future work.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation</title>
<link>https://arxiv.org/abs/2508.07243</link>
<guid>https://arxiv.org/abs/2508.07243</guid>
<content:encoded><![CDATA[
arXiv:2508.07243v1 Announce Type: new 
Abstract: Heuristic negative sampling enhances recommendation performance by selecting negative samples of varying hardness levels from predefined candidate pools to guide the model toward learning more accurate decision boundaries. However, our empirical and theoretical analyses reveal that unobserved environmental confounders (e.g., exposure or popularity biases) in candidate pools may cause heuristic sampling methods to introduce false hard negatives (FHNS). These misleading samples can encourage the model to learn spurious correlations induced by such confounders, ultimately compromising its generalization ability under distribution shifts. To address this issue, we propose a novel method named Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing negative samples in the latent space via a conditional diffusion process, CNSDiff avoids the bias introduced by predefined candidate pools and thus reduces the likelihood of generating FHNS. Moreover, it incorporates a causal regularization term to explicitly mitigate the influence of environmental confounders during the negative sampling process, leading to robust negatives that promote out-of-distribution (OOD) generalization. Comprehensive experiments under four representative distribution shift scenarios demonstrate that CNSDiff achieves an average improvement of 13.96% across all evaluation metrics compared to state-of-the-art baselines, verifying its effectiveness and robustness in OOD recommendation tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Newton methods for Distortion Riskmetrics</title>
<link>https://arxiv.org/abs/2508.07249</link>
<guid>https://arxiv.org/abs/2508.07249</guid>
<content:encoded><![CDATA[
arXiv:2508.07249v1 Announce Type: new 
Abstract: We consider the problem of risk-sensitive control in a reinforcement learning (RL) framework. In particular, we aim to find a risk-optimal policy by maximizing the distortion riskmetric (DRM) of the discounted reward in a finite horizon Markov decision process (MDP). DRMs are a rich class of risk measures that include several well-known risk measures as special cases. We derive a policy Hessian theorem for the DRM objective using the likelihood ratio method. Using this result, we propose a natural DRM Hessian estimator from sample trajectories of the underlying MDP. Next, we present a cubic-regularized policy Newton algorithm for solving this problem in an on-policy RL setting using estimates of the DRM gradient and Hessian. Our proposed algorithm is shown to converge to an $\epsilon$-second-order stationary point ($\epsilon$-SOSP) of the DRM objective, and this guarantee ensures the escaping of saddle points. The sample complexity of our algorithms to find an $ \epsilon$-SOSP is $\mathcal{O}(\epsilon^{-3.5})$. Our experiments validate the theoretical findings. To the best of our knowledge, our is the first work to present convergence to an $\epsilon$-SOSP of a risk-sensitive objective, while existing works in the literature have either shown convergence to a first-order stationary point of a risk-sensitive objective, or a SOSP of a risk-neutral one.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets</title>
<link>https://arxiv.org/abs/2508.07253</link>
<guid>https://arxiv.org/abs/2508.07253</guid>
<content:encoded><![CDATA[
arXiv:2508.07253v1 Announce Type: new 
Abstract: Reliable seizure detection is critical for diagnosing and managing epilepsy, yet clinical workflows remain dependent on time-consuming manual EEG interpretation. While machine learning has shown promise, existing approaches often rely on dataset-specific optimisations, limiting their real-world applicability and reproducibility. Here, we introduce an innovative, open-source machine-learning framework that enables robust and generalisable seizure detection across varied clinical datasets. We evaluate our approach on two publicly available EEG datasets that differ in patient populations and electrode configurations. To enhance robustness, the framework incorporates an automated pre-processing pipeline to standardise data and a majority voting mechanism, in which multiple models independently assess each second of EEG before reaching a final decision. We train, tune, and evaluate models within each dataset, assessing their cross-dataset transferability. Our models achieve high within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and 0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets despite differences in EEG setups and populations (AUC 0.615+/-0.039 for models trained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case) without any post-processing. Furthermore, a mild post-processing improved the within-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset results to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the potential of, and essential considerations for, deploying our framework in diverse clinical settings. By making our methodology fully reproducible, we provide a foundation for advancing clinically viable, dataset-agnostic seizure detection systems. This approach has the potential for widespread adoption, complementing rather than replacing expert interpretation, and accelerating clinical integration.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Data Attribution for Influence Functions</title>
<link>https://arxiv.org/abs/2508.07297</link>
<guid>https://arxiv.org/abs/2508.07297</guid>
<content:encoded><![CDATA[
arXiv:2508.07297v1 Announce Type: new 
Abstract: The goal of data attribution is to trace the model's predictions through the learning algorithm and back to its training data. thereby identifying the most influential training samples and understanding how the model's behavior leads to particular predictions. Understanding how individual training examples influence a model's predictions is fundamental for machine learning interpretability, data debugging, and model accountability. Influence functions, originating from robust statistics, offer an efficient, first-order approximation to estimate the impact of marginally upweighting or removing a data point on a model's learned parameters and its subsequent predictions, without the need for expensive retraining. This paper comprehensively reviews the data attribution capability of influence functions in deep learning. We discuss their theoretical foundations, recent algorithmic advances for efficient inverse-Hessian-vector product estimation, and evaluate their effectiveness for data attribution and mislabel detection. Finally, highlighting current challenges and promising directions for unleashing the huge potential of influence functions in large-scale, real-world deep learning scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective</title>
<link>https://arxiv.org/abs/2508.07299</link>
<guid>https://arxiv.org/abs/2508.07299</guid>
<content:encoded><![CDATA[
arXiv:2508.07299v1 Announce Type: new 
Abstract: Neuro-symbolic (Nesy) learning improves the target task performance of models by enabling them to satisfy knowledge, while semi/self-supervised learning (SSL) improves the target task performance by designing unsupervised pretext tasks for unlabeled data to make models satisfy corresponding assumptions. We extend the Nesy theory based on reliable knowledge to the scenario of unreliable knowledge (i.e., assumptions), thereby unifying the theoretical frameworks of SSL and Nesy. Through rigorous theoretical analysis, we demonstrate that, in theory, the impact of pretext tasks on target performance hinges on three factors: knowledge learnability with respect to the model, knowledge reliability with respect to the data, and knowledge completeness with respect to the target. We further propose schemes to operationalize these theoretical metrics, and thereby develop a method that can predict the effectiveness of pretext tasks in advance. This will change the current status quo in practical applications, where the selections of unsupervised tasks are heuristic-based rather than theory-based, and it is difficult to evaluate the rationality of unsupervised pretext task selection before testing the model on the target task. In experiments, we verify a high correlation between the predicted performance-estimated using minimal data-and the actual performance achieved after large-scale semi-supervised or self-supervised learning, thus confirming the validity of the theory and the effectiveness of the evaluation method.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative</title>
<link>https://arxiv.org/abs/2508.07329</link>
<guid>https://arxiv.org/abs/2508.07329</guid>
<content:encoded><![CDATA[
arXiv:2508.07329v1 Announce Type: new 
Abstract: With the breakthrough progress of large language models (LLMs) in natural language processing and multimodal tasks, efficiently deploying them on resource-constrained edge devices has become a critical challenge. The Mixture of Experts (MoE) architecture enhances model capacity through sparse activation, but faces two major difficulties in practical deployment: (1) The presence of numerous outliers in activation distributions leads to severe degradation in quantization accuracy for both activations and weights, significantly impairing inference performance; (2) Under limited memory, efficient offloading and collaborative inference of expert modules struggle to balance latency and throughput. To address these issues, this paper proposes an efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ) and CPU-GPU collaborative inference. First, by introducing smoothed Hessian matrix quantization, we achieve joint 8-bit quantization of activations and weights, which significantly alleviates the accuracy loss caused by outliers while ensuring efficient implementation on mainstream hardware. Second, we design an expert-level collaborative offloading and inference mechanism, which, combined with expert activation path statistics, enables efficient deployment and scheduling of expert modules between CPU and GPU, greatly reducing memory footprint and inference latency. Extensive experiments validate the effectiveness of our method on mainstream large models such as the OPT series and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of the low-bit quantized model approaches that of the full-precision model, while GPU memory usage is reduced by about 60%, and inference latency is significantly improved.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants</title>
<link>https://arxiv.org/abs/2508.07333</link>
<guid>https://arxiv.org/abs/2508.07333</guid>
<content:encoded><![CDATA[
arXiv:2508.07333v1 Announce Type: new 
Abstract: Stochastic interpolants offer a robust framework for continuously transforming samples between arbitrary data distributions, holding significant promise for generative modeling. Despite their potential, rigorous finite-time convergence guarantees for practical numerical schemes remain largely unexplored. In this work, we address the finite-time convergence analysis of numerical implementations for ordinary differential equations (ODEs) derived from stochastic interpolants. Specifically, we establish novel finite-time error bounds in total variation distance for two widely used numerical integrators: the first-order forward Euler method and the second-order Heun's method. Furthermore, our analysis on the iteration complexity of specific stochastic interpolant constructions provides optimized schedules to enhance computational efficiency. Our theoretical findings are corroborated by numerical experiments, which validate the derived error bounds and complexity analyses.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis</title>
<link>https://arxiv.org/abs/2508.07345</link>
<guid>https://arxiv.org/abs/2508.07345</guid>
<content:encoded><![CDATA[
arXiv:2508.07345v1 Announce Type: new 
Abstract: \textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is essential for genomic studies due to their crucial role as structural elements in bacteriophages. Computational tools, particularly machine learning, have emerged for annotating phage protein sequences from high-throughput sequencing. However, effective annotation requires specialized sequence encodings. Our paper introduces ProteoKnight, a new image-based encoding method that addresses spatial constraints in existing techniques, yielding competitive performance in PVP classification using pre-trained convolutional neural networks. Additionally, our study evaluates prediction uncertainty in binary PVP classification through Monte Carlo Dropout (MCD). \textbf{Methods:} ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences, incorporating pixel colors and adjusting walk distances to capture intricate protein features. Encoded sequences were classified using multiple pre-trained CNNs. Variance and entropy measures assessed prediction uncertainty across proteins of various classes and lengths. \textbf{Results:} Our experiments achieved 90.8% accuracy in binary classification, comparable to state-of-the-art methods. Multi-class classification accuracy remains suboptimal. Our uncertainty analysis unveils variability in prediction confidence influenced by protein class and sequence length. \textbf{Conclusions:} Our study surpasses frequency chaos game representation (FCGR) by introducing novel image encoding that mitigates spatial information loss limitations. Our classification technique yields accurate and robust PVP predictions while identifying low-confidence predictions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic training dynamics of deep neural networks</title>
<link>https://arxiv.org/abs/2508.07370</link>
<guid>https://arxiv.org/abs/2508.07370</guid>
<content:encoded><![CDATA[
arXiv:2508.07370v1 Announce Type: new 
Abstract: A fundamental challenge in the theory of deep learning is to understand whether gradient-based training in high-dimensional parameter spaces can be captured by simpler, lower-dimensional structures, leading to so-called implicit bias. As a stepping stone, we study when a gradient flow on a high-dimensional variable $\theta$ implies an intrinsic gradient flow on a lower-dimensional variable $z = \phi(\theta)$, for an architecture-related function $\phi$. We express a so-called intrinsic dynamic property and show how it is related to the study of conservation laws associated with the factorization $\phi$. This leads to a simple criterion based on the inclusion of kernels of linear maps which yields a necessary condition for this property to hold. We then apply our theory to general ReLU networks of arbitrary depth and show that, for any initialization, it is possible to rewrite the flow as an intrinsic dynamic in a lower dimension that depends only on $z$ and the initialization, when $\phi$ is the so-called path-lifting. In the case of linear networks with $\phi$ the product of weight matrices, so-called balanced initializations are also known to enable such a dimensionality reduction; we generalize this result to a broader class of {\em relaxed balanced} initializations, showing that, in certain configurations, these are the \emph{only} initializations that ensure the intrinsic dynamic property. Finally, for the linear neural ODE associated with the limit of infinitely deep linear networks, with relaxed balanced initialization, we explicitly express the corresponding intrinsic dynamics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tight Bounds for Schr\"odinger Potential Estimation in Unpaired Image-to-Image Translation Problems</title>
<link>https://arxiv.org/abs/2508.07392</link>
<guid>https://arxiv.org/abs/2508.07392</guid>
<content:encoded><![CDATA[
arXiv:2508.07392v1 Announce Type: new 
Abstract: Modern methods of generative modelling and unpaired image-to-image translation based on Schr\"odinger bridges and stochastic optimal control theory aim to transform an initial density to a target one in an optimal way. In the present paper, we assume that we only have access to i.i.d. samples from initial and final distributions. This makes our setup suitable for both generative modelling and unpaired image-to-image translation. Relying on the stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as the reference one and estimate the corresponding Schr\"odinger potential. Introducing a risk function as the Kullback-Leibler divergence between couplings, we derive tight bounds on generalization ability of an empirical risk minimizer in a class of Schr\"odinger potentials including Gaussian mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we almost achieve fast rates of convergence up to some logarithmic factors in favourable scenarios. We also illustrate performance of the suggested approach with numerical experiments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs</title>
<link>https://arxiv.org/abs/2508.07395</link>
<guid>https://arxiv.org/abs/2508.07395</guid>
<content:encoded><![CDATA[
arXiv:2508.07395v1 Announce Type: new 
Abstract: Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack state-tracking capability due to either time-invariant transition matrices or restricted eigenvalue ranges. To address this, input-dependent transition matrices, particularly those that are complex or non-triangular, have been proposed to enhance SSM performance on such tasks. While existing theorems demonstrate that both input-independent and non-negative SSMs are incapable of solving simple state-tracking tasks, such as parity, regardless of depth, they do not explore whether combining these two types in a multilayer SSM could help. We investigate this question for efficient SSMs with diagonal transition matrices and show that such combinations still fail to solve parity. This implies that a recurrence layer must both be input-dependent and include negative eigenvalues. Our experiments support this conclusion by analyzing an SSM model that combines S4D and Mamba layers.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors</title>
<link>https://arxiv.org/abs/2508.07400</link>
<guid>https://arxiv.org/abs/2508.07400</guid>
<content:encoded><![CDATA[
arXiv:2508.07400v1 Announce Type: new 
Abstract: In this paper, we consider the problem of recovering time-varying reward functions from either optimal policies or demonstrations coming from a max entropy reinforcement learning problem. This problem is highly ill-posed without additional assumptions on the underlying rewards. However, in many applications, the rewards are indeed parsimonious, and some prior information is available. We consider two such priors on the rewards: 1) rewards are mostly constant and they change infrequently, 2) rewards can be represented by a linear combination of a small number of feature functions. We first show that the reward identification problem with the former prior can be recast as a sparsification problem subject to linear constraints. Moreover, we give a polynomial-time algorithm that solves this sparsification problem exactly. Then, we show that identifying rewards representable with the minimum number of features can be recast as a rank minimization problem subject to linear constraints, for which convex relaxations of rank can be invoked. In both cases, these observations lead to efficient optimization-based reward identification algorithms. Several examples are given to demonstrate the accuracy of the recovered rewards as well as their generalizability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightning Prediction under Uncertainty: DeepLight with Hazy Loss</title>
<link>https://arxiv.org/abs/2508.07428</link>
<guid>https://arxiv.org/abs/2508.07428</guid>
<content:encoded><![CDATA[
arXiv:2508.07428v1 Announce Type: new 
Abstract: Lightning, a common feature of severe meteorological conditions, poses significant risks, from direct human injuries to substantial economic losses. These risks are further exacerbated by climate change. Early and accurate prediction of lightning would enable preventive measures to safeguard people, protect property, and minimize economic losses. In this paper, we present DeepLight, a novel deep learning architecture for predicting lightning occurrences. Existing prediction models face several critical limitations: they often struggle to capture the dynamic spatial context and inherent uncertainty of lightning events, underutilize key observational data, such as radar reflectivity and cloud properties, and rely heavily on Numerical Weather Prediction (NWP) systems, which are both computationally expensive and highly sensitive to parameter settings. To overcome these challenges, DeepLight leverages multi-source meteorological data, including radar reflectivity, cloud properties, and historical lightning occurrences through a dual-encoder architecture. By employing multi-branch convolution techniques, it dynamically captures spatial correlations across varying extents. Furthermore, its novel Hazy Loss function explicitly addresses the spatio-temporal uncertainty of lightning by penalizing deviations based on proximity to true events, enabling the model to better learn patterns amidst randomness. Extensive experiments show that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over state-of-the-art methods, establishing it as a robust solution for lightning prediction.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised operator learning approach for dissipative equations via Onsager principle</title>
<link>https://arxiv.org/abs/2508.07440</link>
<guid>https://arxiv.org/abs/2508.07440</guid>
<content:encoded><![CDATA[
arXiv:2508.07440v1 Announce Type: new 
Abstract: Existing operator learning methods rely on supervised training with high-fidelity simulation data, introducing significant computational cost. In this work, we propose the deep Onsager operator learning (DOOL) method, a novel unsupervised framework for solving dissipative equations. Rooted in the Onsager variational principle (OVP), DOOL trains a deep operator network by directly minimizing the OVP-defined Rayleighian functional, requiring no labeled data, and then proceeds in time explicitly through conservation/change laws for the solution. Another key innovation here lies in the spatiotemporal decoupling strategy: the operator's trunk network processes spatial coordinates exclusively, thereby enhancing training efficiency, while integrated external time stepping enables temporal extrapolation. Numerical experiments on typical dissipative equations validate the effectiveness of the DOOL method, and systematic comparisons with supervised DeepONet and MIONet demonstrate its enhanced performance. Extensions are made to cover the second-order wave models with dissipation that do not directly follow OVP.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.07452</link>
<guid>https://arxiv.org/abs/2508.07452</guid>
<content:encoded><![CDATA[
arXiv:2508.07452v1 Announce Type: new 
Abstract: Integrated, end-to-end learning of representations and policies remains a cornerstone of deep reinforcement learning (RL). However, to address the challenge of learning effective features from a sparse reward signal, recent trends have shifted towards adding complex auxiliary objectives or fully decoupling the two processes, often at the cost of increased design complexity. This work proposes an alternative to both decoupling and naive end-to-end learning, arguing that performance can be significantly improved by structuring the interaction between distinct perception and control networks with a principled, game-theoretic dynamic. We formalize this dynamic by introducing the Stackelberg Coupled Representation and Reinforcement Learning (SCORER) framework, which models the interaction between perception and control as a Stackelberg game. The perception network (leader) strategically learns features to benefit the control network (follower), whose own objective is to minimize its Bellman error. We approximate the game's equilibrium with a practical two-timescale algorithm. Applied to standard DQN variants on benchmark tasks, SCORER improves sample efficiency and final performance. Our results show that performance gains can be achieved through principled algorithmic design of the perception-control dynamic, without requiring complex auxiliary objectives or architectures.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unveiling Predictive Uncertainty Vulnerabilities in the Context of the Right to Be Forgotten</title>
<link>https://arxiv.org/abs/2508.07458</link>
<guid>https://arxiv.org/abs/2508.07458</guid>
<content:encoded><![CDATA[
arXiv:2508.07458v1 Announce Type: new 
Abstract: Currently, various uncertainty quantification methods have been proposed to provide certainty and probability estimates for deep learning models' label predictions. Meanwhile, with the growing demand for the right to be forgotten, machine unlearning has been extensively studied as a means to remove the impact of requested sensitive data from a pre-trained model without retraining the model from scratch. However, the vulnerabilities of such generated predictive uncertainties with regard to dedicated malicious unlearning attacks remain unexplored. To bridge this gap, for the first time, we propose a new class of malicious unlearning attacks against predictive uncertainties, where the adversary aims to cause the desired manipulations of specific predictive uncertainty results. We also design novel optimization frameworks for our attacks and conduct extensive experiments, including black-box scenarios. Notably, our extensive experiments show that our attacks are more effective in manipulating predictive uncertainties than traditional attacks that focus on label misclassifications, and existing defenses against conventional attacks are ineffective against our attacks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification</title>
<link>https://arxiv.org/abs/2508.07465</link>
<guid>https://arxiv.org/abs/2508.07465</guid>
<content:encoded><![CDATA[
arXiv:2508.07465v1 Announce Type: new 
Abstract: Integrating multi-omics data, such as DNA methylation, mRNA expression, and microRNA (miRNA) expression, offers a comprehensive view of the biological mechanisms underlying disease. However, the high dimensionality and complex interactions among omics layers present major challenges for predictive modeling. We propose Multi-Omics integration with Tree-generated Graph Neural Network (MOTGNN), a novel and interpretable framework for binary disease classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform omics-specific supervised graph construction, followed by modality-specific Graph Neural Networks (GNNs) for hierarchical representation learning, and a deep feedforward network for cross-omics integration. On three real-world disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance (e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains computational efficiency through sparse graphs (2.1-2.8 edges per node) and provides built-in interpretability, revealing both top-ranked biomarkers and the relative contributions of each omics modality. These results highlight MOTGNN's potential to improve both predictive accuracy and interpretability in multi-omics disease modeling.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications</title>
<link>https://arxiv.org/abs/2508.07473</link>
<guid>https://arxiv.org/abs/2508.07473</guid>
<content:encoded><![CDATA[
arXiv:2508.07473v1 Announce Type: new 
Abstract: In Online Convex Optimization (OCO), when the stochastic gradient has a finite variance, many algorithms provably work and guarantee a sublinear regret. However, limited results are known if the gradient estimate has a heavy tail, i.e., the stochastic gradient only admits a finite $\mathsf{p}$-th central moment for some $\mathsf{p}\in\left(1,2\right]$. Motivated by it, this work examines different old algorithms for OCO (e.g., Online Gradient Descent) in the more challenging heavy-tailed setting. Under the standard bounded domain assumption, we establish new regrets for these classical methods without any algorithmic modification. Remarkably, these regret bounds are fully optimal in all parameters (can be achieved even without knowing $\mathsf{p}$), suggesting that OCO with heavy tails can be solved effectively without any extra operation (e.g., gradient clipping). Our new results have several applications. A particularly interesting one is the first provable convergence result for nonsmooth nonconvex optimization under heavy-tailed noise without gradient clipping. Furthermore, we explore broader settings (e.g., smooth OCO) and extend our ideas to optimistic algorithms to handle different cases simultaneously.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.07490</link>
<guid>https://arxiv.org/abs/2508.07490</guid>
<content:encoded><![CDATA[
arXiv:2508.07490v1 Announce Type: new 
Abstract: Deep learning approaches are increasingly relevant for time series forecasting tasks. Methods such as N-BEATS, which is built on stacks of multilayer perceptrons (MLPs) blocks, have achieved state-of-the-art results on benchmark datasets and competitions. N-BEATS is also more interpretable relative to other deep learning approaches, as it decomposes forecasts into different time series components, such as trend and seasonality. In this work, we present N-BEATS-MOE, an extension of N-BEATS based on a Mixture-of-Experts (MoE) layer. N-BEATS-MOE employs a dynamic block weighting strategy based on a gating network which allows the model to better adapt to the characteristics of each time series. We also hypothesize that the gating mechanism provides additional interpretability by identifying which expert is most relevant for each series. We evaluate our method across 12 benchmark datasets against several approaches, achieving consistent improvements on several datasets, especially those composed of heterogeneous time series.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach</title>
<link>https://arxiv.org/abs/2508.07505</link>
<guid>https://arxiv.org/abs/2508.07505</guid>
<content:encoded><![CDATA[
arXiv:2508.07505v1 Announce Type: new 
Abstract: Decentralized min-max optimization allows multi-agent systems to collaboratively solve global min-max optimization problems by facilitating the exchange of model updates among neighboring agents, eliminating the need for a central server. However, sharing model updates in such systems carry a risk of exposing sensitive data to inference attacks, raising significant privacy concerns. To mitigate these privacy risks, differential privacy (DP) has become a widely adopted technique for safeguarding individual data. Despite its advantages, implementing DP in decentralized min-max optimization poses challenges, as the added noise can hinder convergence, particularly in non-convex scenarios with complex agent interactions in min-max optimization problems. In this work, we propose an algorithm called DPMixSGD (Differential Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving algorithm specifically designed for non-convex decentralized min-max optimization. Our method builds on the state-of-the-art STORM-based algorithm, one of the fastest decentralized min-max solutions. We rigorously prove that the noise added to local gradients does not significantly compromise convergence performance, and we provide theoretical bounds to ensure privacy guarantees. To validate our theoretical findings, we conduct extensive experiments across various tasks and models, demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction</title>
<link>https://arxiv.org/abs/2508.07518</link>
<guid>https://arxiv.org/abs/2508.07518</guid>
<content:encoded><![CDATA[
arXiv:2508.07518v1 Announce Type: new 
Abstract: As deep spatio-temporal neural networks are increasingly utilised in urban computing contexts, the deployment of such methods can have a direct impact on users of critical urban infrastructure, such as public transport, emergency services, and traffic management systems. While many spatio-temporal methods focus on improving accuracy, fairness has recently gained attention due to growing evidence that biased predictions in spatio-temporal applications can disproportionately disadvantage certain demographic or geographic groups, thereby reinforcing existing socioeconomic inequalities and undermining the ethical deployment of AI in public services. In this paper, we propose a novel framework, FairDRL-ST, based on disentangled representation learning, to address fairness concerns in spatio-temporal prediction, with a particular focus on mobility demand forecasting. By leveraging adversarial learning and disentangled representation learning, our framework learns to separate attributes that contain sensitive information. Unlike existing methods that enforce fairness through supervised learning, which may lead to overcompensation and degraded performance, our framework achieves fairness in an unsupervised manner with minimal performance loss. We apply our framework to real-world urban mobility datasets and demonstrate its ability to close fairness gaps while delivering competitive predictive performance compared to state-of-the-art fairness-aware methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning</title>
<link>https://arxiv.org/abs/2508.07536</link>
<guid>https://arxiv.org/abs/2508.07536</guid>
<content:encoded><![CDATA[
arXiv:2508.07536v1 Announce Type: new 
Abstract: Accurate and interpretable bearing fault classification is critical for ensuring the reliability of rotating machinery, particularly under variable operating conditions where domain shifts can significantly degrade model performance. This study proposes a physics-informed multimodal convolutional neural network (CNN) with a late fusion architecture, integrating vibration and motor current signals alongside a dedicated physics-based feature extraction branch. The model incorporates a novel physics-informed loss function that penalizes physically implausible predictions based on characteristic bearing fault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass Frequency Inner (BPFI) - derived from bearing geometry and shaft speed. Comprehensive experiments on the Paderborn University dataset demonstrate that the proposed physics-informed approach consistently outperforms a non-physics-informed baseline, achieving higher accuracy, reduced false classifications, and improved robustness across multiple data splits. To address performance degradation under unseen operating conditions, three transfer learning (TL) strategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy (LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LAS yields the best generalization, with additional performance gains when combined with physics-informed modeling. Validation on the KAIST bearing dataset confirms the framework's cross-dataset applicability, achieving up to 98 percent accuracy. Statistical hypothesis testing further verifies significant improvements (p < 0.01) in classification performance. The proposed framework demonstrates the potential of integrating domain knowledge with data-driven learning to achieve robust, interpretable, and generalizable fault diagnosis for real-world industrial applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Remote Inference</title>
<link>https://arxiv.org/abs/2508.07555</link>
<guid>https://arxiv.org/abs/2508.07555</guid>
<content:encoded><![CDATA[
arXiv:2508.07555v1 Announce Type: new 
Abstract: We consider a remote inference system with multiple modalities, where a multimodal machine learning (ML) model performs real-time inference using features collected from remote sensors. As sensor observations may change dynamically over time, fresh features are critical for inference tasks. However, timely delivering features from all modalities is often infeasible due to limited network resources. To this end, we study a two-modality scheduling problem to minimize the ML model's inference error, which is expressed as a penalty function of AoI for both modalities. We develop an index-based threshold policy and prove its optimality. Specifically, the scheduler switches modalities when the current modality's index function exceeds a threshold. We show that the two modalities share the same threshold, and both the index functions and the threshold can be computed efficiently. The optimality of our policy holds for (i) general AoI functions that are \emph{non-monotonic} and \emph{non-additive} and (ii) \emph{heterogeneous} transmission times. Numerical results show that our policy reduces inference error by up to 55% compared to round-robin and uniform random policies, which are oblivious to the AoI-based inference error function. Our results shed light on how to improve remote inference accuracy by optimizing task-oriented AoI functions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning</title>
<link>https://arxiv.org/abs/2508.07556</link>
<guid>https://arxiv.org/abs/2508.07556</guid>
<content:encoded><![CDATA[
arXiv:2508.07556v1 Announce Type: new 
Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. This thesis investigates how uncertainty estimation can enhance the safety and trustworthiness of ML, focusing on selective prediction -- where models abstain when confidence is low.
  We first show that a model's training trajectory contains rich uncertainty signals that can be exploited without altering its architecture or loss. By ensembling predictions from intermediate checkpoints, we propose a lightweight, post-hoc abstention method that works across tasks, avoids the cost of deep ensembles, and achieves state-of-the-art selective prediction performance. Crucially, this approach is fully compatible with differential privacy (DP), allowing us to study how privacy noise affects uncertainty quality. We find that while many methods degrade under DP, our trajectory-based approach remains robust, and we introduce a framework for isolating the privacy-uncertainty trade-off. Next, we then develop a finite-sample decomposition of the selective classification gap -- the deviation from the oracle accuracy-coverage curve -- identifying five interpretable error sources and clarifying which interventions can close the gap. This explains why calibration alone cannot fix ranking errors, motivating methods that improve uncertainty ordering. Finally, we show that uncertainty signals can be adversarially manipulated to hide errors or deny service while maintaining high accuracy, and we design defenses combining calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating, and safeguarding uncertainty estimation, enabling models that not only make accurate predictions -- but also know when to say "I do not know".
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression</title>
<link>https://arxiv.org/abs/2508.07571</link>
<guid>https://arxiv.org/abs/2508.07571</guid>
<content:encoded><![CDATA[
arXiv:2508.07571v1 Announce Type: new 
Abstract: Using more test-time computation during language model inference, such as generating more intermediate thoughts or sampling multiple candidate answers, has proven effective in significantly improving model performance. This paper takes an initial step toward bridging the gap between practical language model inference and theoretical transformer analysis by incorporating randomness and sampling. We focus on in-context linear regression with continuous/binary coefficients, where our framework simulates language model decoding through noise injection and binary coefficient sampling. Through this framework, we provide detailed analyses of widely adopted inference techniques. Supported by empirical results, our theoretical framework and analysis demonstrate the potential for offering new insights into understanding inference behaviors in real-world language models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When and how can inexact generative models still sample from the data manifold?</title>
<link>https://arxiv.org/abs/2508.07581</link>
<guid>https://arxiv.org/abs/2508.07581</guid>
<content:encoded><![CDATA[
arXiv:2508.07581v1 Announce Type: new 
Abstract: A curious phenomenon observed in some dynamical generative models is the following: despite learning errors in the score function or the drift vector field, the generated samples appear to shift \emph{along} the support of the data distribution but not \emph{away} from it. In this work, we investigate this phenomenon of \emph{robustness of the support} by taking a dynamical systems approach on the generating stochastic/deterministic process. Our perturbation analysis of the probability flow reveals that infinitesimal learning errors cause the predicted density to be different from the target density only on the data manifold for a wide class of generative models. Further, what is the dynamical mechanism that leads to the robustness of the support? We show that the alignment of the top Lyapunov vectors (most sensitive infinitesimal perturbation directions) with the tangent spaces along the boundary of the data manifold leads to robustness and prove a sufficient condition on the dynamics of the generating process to achieve this alignment. Moreover, the alignment condition is efficient to compute and, in practice, for robust generative models, automatically leads to accurate estimates of the tangent bundle of the data manifold. Using a finite-time linear perturbation analysis on samples paths as well as probability flows, our work complements and extends existing works on obtaining theoretical guarantees for generative models from a stochastic analysis, statistical learning and uncertainty quantification points of view. Our results apply across different dynamical generative models, such as conditional flow-matching and score-based generative models, and for different target distributions that may or may not satisfy the manifold hypothesis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization</title>
<link>https://arxiv.org/abs/2508.07629</link>
<guid>https://arxiv.org/abs/2508.07629</guid>
<content:encoded><![CDATA[
arXiv:2508.07629v1 Announce Type: new 
Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the model's exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\% on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo</title>
<link>https://arxiv.org/abs/2508.07631</link>
<guid>https://arxiv.org/abs/2508.07631</guid>
<content:encoded><![CDATA[
arXiv:2508.07631v1 Announce Type: new 
Abstract: We study the problem of posterior sampling in the context of score based generative models. We have a trained score network for a prior $p(x)$, a measurement model $p(y|x)$, and are tasked with sampling from the posterior $p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case) under well-accepted computational hardness assumptions. Despite this, popular algorithms for tasks such as image super-resolution, stylization, and reconstruction enjoy empirical success. Rather than establishing distributional assumptions or restricted settings under which exact posterior sampling is tractable, we view this as a more general "tilting" problem of biasing a distribution towards a measurement. Under minimal assumptions, we show that one can tractably sample from a distribution that is simultaneously close to the posterior of a noised prior in KL divergence and the true posterior in Fisher divergence. Intuitively, this combination ensures that the resulting sample is consistent with both the measurement and the prior. To the best of our knowledge these are the first formal results for (approximate) posterior sampling in polynomial time.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attribution Explanations for Deep Neural Networks: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2508.07636</link>
<guid>https://arxiv.org/abs/2508.07636</guid>
<content:encoded><![CDATA[
arXiv:2508.07636v1 Announce Type: new 
Abstract: Attribution explanation is a typical approach for explaining deep neural networks (DNNs), inferring an importance or contribution score for each input variable to the final output. In recent years, numerous attribution methods have been developed to explain DNNs. However, a persistent concern remains unresolved, i.e., whether and which attribution methods faithfully reflect the actual contribution of input variables to the decision-making process. The faithfulness issue undermines the reliability and practical utility of attribution explanations. We argue that these concerns stem from three core challenges. First, difficulties arise in comparing attribution methods due to their unstructured heterogeneity, differences in heuristics, formulations, and implementations that lack a unified organization. Second, most methods lack solid theoretical underpinnings, with their rationales remaining absent, ambiguous, or unverified. Third, empirically evaluating faithfulness is challenging without ground truth. Recent theoretical advances provide a promising way to tackle these challenges, attracting increasing attention. We summarize these developments, with emphasis on three key directions: (i) Theoretical unification, which uncovers commonalities and differences among methods, enabling systematic comparisons; (ii) Theoretical rationale, clarifying the foundations of existing methods; (iii) Theoretical evaluation, rigorously proving whether methods satisfy faithfulness principles. Beyond a comprehensive review, we provide insights into how these studies help deepen theoretical understanding, inform method selection, and inspire new attribution methods. We conclude with a discussion of promising open problems for further work.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Complex Topology from Multivariate Functional Approximation: Contours, Jacobi Sets, and Ridge-Valley Graphs</title>
<link>https://arxiv.org/abs/2508.07637</link>
<guid>https://arxiv.org/abs/2508.07637</guid>
<content:encoded><![CDATA[
arXiv:2508.07637v1 Announce Type: new 
Abstract: Implicit continuous models, such as functional models and implicit neural networks, are an increasingly popular method for replacing discrete data representations with continuous, high-order, and differentiable surrogates. These models offer new perspectives on the storage, transfer, and analysis of scientific data. In this paper, we introduce the first framework to directly extract complex topological features -- contours, Jacobi sets, and ridge-valley graphs -- from a type of continuous implicit model known as multivariate functional approximation (MFA). MFA replaces discrete data with continuous piecewise smooth functions. Given an MFA model as the input, our approach enables direct extraction of complex topological features from the model, without reverting to a discrete representation of the model. Our work is easily generalizable to any continuous implicit model that supports the queries of function values and high-order derivatives. Our work establishes the building blocks for performing topological data analysis and visualization on implicit continuous models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals</title>
<link>https://arxiv.org/abs/2508.07638</link>
<guid>https://arxiv.org/abs/2508.07638</guid>
<content:encoded><![CDATA[
arXiv:2508.07638v1 Announce Type: new 
Abstract: Aligning Large Language Models (LLMs) with diverse human values requires moving beyond a single holistic "better-than" preference criterion. While collecting fine-grained, aspect-specific preference data is more reliable and scalable, existing methods like Direct Preference Optimization (DPO) struggle with the severe noise and conflicts inherent in such aggregated datasets. In this paper, we tackle this challenge from a data-centric perspective. We first derive the Direct Multi-Preference Optimization (DMPO) objective, and uncover a key Preference Divergence (PD) term that quantifies inter-aspect preference conflicts. Instead of using this term for direct optimization, we leverage it to formulate a novel, theoretically-grounded data selection principle. Our principle advocates for selecting a subset of high-consensus data-identified by the most negative PD values-for efficient DPO training. We prove the optimality of this strategy by analyzing the loss bounds of the DMPO objective in the selection problem. To operationalize our approach, we introduce practical methods of PD term estimation and length bias mitigation, thereby proposing our PD selection method. Evaluation on the UltraFeedback dataset with three varying conflict levels shows that our simple yet effective strategy achieves over 10% relative improvement against both the standard holistic preference and a stronger oracle using aggregated preference signals, all while boosting training efficiency and obviating the need for intractable holistic preference annotating, unlocking the potential of robust LLM alignment via fine-grained preference signals.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Turn Jailbreaks Are Simpler Than They Seem</title>
<link>https://arxiv.org/abs/2508.07646</link>
<guid>https://arxiv.org/abs/2508.07646</guid>
<content:encoded><![CDATA[
arXiv:2508.07646v1 Announce Type: new 
Abstract: While defenses against single-turn jailbreak attacks on Large Language Models (LLMs) have improved significantly, multi-turn jailbreaks remain a persistent vulnerability, often achieving success rates exceeding 70% against models optimized for single-turn protection. This work presents an empirical analysis of automated multi-turn jailbreak attacks across state-of-the-art models including GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark. Our findings challenge the perceived sophistication of multi-turn attacks: when accounting for the attacker's ability to learn from how models refuse harmful requests, multi-turn jailbreaking approaches are approximately equivalent to simply resampling single-turn attacks multiple times. Moreover, attack success is correlated among similar models, making it easier to jailbreak newly released ones. Additionally, for reasoning models, we find surprisingly that higher reasoning effort often leads to higher attack success rates. Our results have important implications for AI safety evaluation and the design of jailbreak-resistant systems. We release the source code at https://github.com/diogo-cruz/multi_turn_simpler
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning</title>
<link>https://arxiv.org/abs/2508.07659</link>
<guid>https://arxiv.org/abs/2508.07659</guid>
<content:encoded><![CDATA[
arXiv:2508.07659v1 Announce Type: new 
Abstract: This study aims to discover spatial correlations between Earth observations and atmospheric states to improve the forecasting accuracy of global atmospheric state estimation, which are usually conducted using conventional numerical weather prediction (NWP) systems and is the beginning of weather forecasting. NWP systems predict future atmospheric states at fixed locations, which are called NWP grid points, by analyzing previous atmospheric states and newly acquired Earth observations without fixed locations. Thus, surrounding meteorological context and the changing locations of the observations make spatial correlations between atmospheric states and observations over time. To handle complicated spatial correlations, which change dynamically, we employ spatiotemporal graph neural networks (STGNNs) with structure learning. However, structure learning has an inherent limitation that this can cause structural information loss and over-smoothing problem by generating excessive edges. To solve this problem, we regulate edge sampling by adaptively determining node degrees and considering the spatial distances between NWP grid points and observations. We validated the effectiveness of the proposed method by using real-world atmospheric state and observation data from East Asia. Even in areas with high atmospheric variability, the proposed method outperformed existing STGNN models with and without structure learning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLiClass: Generalist Lightweight Model for Sequence Classification Tasks</title>
<link>https://arxiv.org/abs/2508.07662</link>
<guid>https://arxiv.org/abs/2508.07662</guid>
<content:encoded><![CDATA[
arXiv:2508.07662v1 Announce Type: new 
Abstract: Classification is one of the most widespread tasks in AI applications, serving often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative LLMs have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data-sparse conditions or from human feedback.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting</title>
<link>https://arxiv.org/abs/2508.07668</link>
<guid>https://arxiv.org/abs/2508.07668</guid>
<content:encoded><![CDATA[
arXiv:2508.07668v1 Announce Type: new 
Abstract: With the increase in maritime traffic and the mandatory implementation of the Automatic Identification System (AIS), the importance and diversity of maritime traffic analysis tasks based on AIS data, such as vessel trajectory prediction, anomaly detection, and collision risk assessment, is rapidly growing. However, existing approaches tend to address these tasks individually, making it difficult to holistically consider complex maritime situations. To address this limitation, we propose a novel framework, AIS-LLM, which integrates time-series AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a Cross-Modality Alignment Module for semantic alignment between time-series data and textual prompts, and an LLM-based Multi-Task Decoder. This architecture enables the simultaneous execution of three key tasks: trajectory prediction, anomaly detection, and risk assessment of vessel collisions within a single end-to-end system. Experimental results demonstrate that AIS-LLM outperforms existing methods across individual tasks, validating its effectiveness. Furthermore, by integratively analyzing task outputs to generate situation summaries and briefings, AIS-LLM presents the potential for more intelligent and efficient maritime traffic management.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation</title>
<link>https://arxiv.org/abs/2508.07675</link>
<guid>https://arxiv.org/abs/2508.07675</guid>
<content:encoded><![CDATA[
arXiv:2508.07675v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are revolutionizing how users interact with information systems, yet their high inference cost poses serious scalability and sustainability challenges. Caching inference responses, allowing them to be retrieved without another forward pass through the LLM, has emerged as one possible solution. Traditional exact-match caching, however, overlooks the semantic similarity between queries, leading to unnecessary recomputation. Semantic caching addresses this by retrieving responses based on semantic similarity, but introduces a fundamentally different cache eviction problem: one must account for mismatch costs between incoming queries and cached responses. Moreover, key system parameters, such as query arrival probabilities and serving costs, are often unknown and must be learned over time. Existing semantic caching methods are largely ad-hoc, lacking theoretical foundations and unable to adapt to real-world uncertainty. In this paper, we present a principled, learning-based framework for semantic cache eviction under unknown query and cost distributions. We formulate both offline optimization and online learning variants of the problem, and develop provably efficient algorithms with state-of-the-art guarantees. We also evaluate our framework on a synthetic dataset, showing that our proposed algorithms perform matching or superior performance compared with baselines.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks</title>
<link>https://arxiv.org/abs/2508.07676</link>
<guid>https://arxiv.org/abs/2508.07676</guid>
<content:encoded><![CDATA[
arXiv:2508.07676v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across decentralized clients without sharing local data, thereby enhancing privacy and facilitating collaboration among clients connected via social networks. However, these social connections introduce privacy externalities: a client's privacy loss depends not only on its privacy protection strategy but also on the privacy decisions of others, propagated through the network via multi-hop interactions. In this work, we propose a socially-aware privacy-preserving FL mechanism that systematically quantifies indirect privacy leakage through a multi-hop propagation model. We formulate the server-client interaction as a two-stage Stackelberg game, where the server, as the leader, optimizes incentive policies, and clients, as followers, strategically select their privacy budgets, which determine their privacy-preserving levels by controlling the magnitude of added noise. To mitigate information asymmetry in networked privacy estimation, we introduce a mean-field estimator to approximate the average external privacy risk. We theoretically prove the existence and convergence of the fixed point of the mean-field estimator and derive closed-form expressions for the Stackelberg Nash Equilibrium. Despite being designed from a client-centric incentive perspective, our mechanism achieves approximately-optimal social welfare, as revealed by Price of Anarchy (PoA) analysis. Experiments on diverse datasets demonstrate that our approach significantly improves client utilities and reduces server costs while maintaining model performance, outperforming both Social-Agnostic (SA) baselines and methods that account for social externalities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation</title>
<link>https://arxiv.org/abs/2508.07681</link>
<guid>https://arxiv.org/abs/2508.07681</guid>
<content:encoded><![CDATA[
arXiv:2508.07681v1 Announce Type: new 
Abstract: Sepsis, a life-threatening inflammatory response to infection, causes organ dysfunction, making early detection and optimal management critical. Previous reinforcement learning (RL) approaches to sepsis management rely primarily on structured data, such as lab results or vital signs, and on a dearth of a comprehensive understanding of the patient's condition. In this work, we propose a Multimodal Offline REinforcement learning for Clinical notes Leveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis control in intensive care units. MORE-CLEAR employs pre-trained large-scale language models (LLMs) to facilitate the extraction of rich semantic representations from clinical notes, preserving clinical context and improving patient state representation. Gated fusion and cross-modal attention allow dynamic weight adjustment in the context of time and the effective integration of multimodal data. Extensive cross-validation using two public (MIMIC-III and MIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly improves estimated survival rate and policy performance compared to single-modal RL approaches. To our knowledge, this is the first to leverage LLM capabilities within a multimodal offline RL for better state representation in medical applications. This approach can potentially expedite the treatment and management of sepsis by enabling reinforcement learning models to propose enhanced actions based on a more comprehensive understanding of patient conditions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Enhanced Time-Series Forecasting via Large Language Models</title>
<link>https://arxiv.org/abs/2508.07697</link>
<guid>https://arxiv.org/abs/2508.07697</guid>
<content:encoded><![CDATA[
arXiv:2508.07697v1 Announce Type: new 
Abstract: Time series forecasting plays a significant role in finance, energy, meteorology, and IoT applications. Recent studies have leveraged the generalization capabilities of large language models (LLMs) to adapt to time series forecasting, achieving promising performance. However, existing studies focus on token-level modal alignment, instead of bridging the intrinsic modality gap between linguistic knowledge structures and time series data patterns, greatly limiting the semantic representation. To address this issue, we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent periodicity and anomalous characteristics of time series to embed into the semantic space to enhance the token embedding. This process enhances the interpretability of tokens for LLMs, thereby activating the potential of LLMs for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel at capturing long-range dependencies but are weak at modeling short-term anomalies in time-series data. Hence, we propose a plugin module embedded within self-attention that models long-term and short-term dependencies to effectively adapt LLMs to time-series analysis. Our approach freezes the LLM and reduces the sequence dimensionality of tokens, greatly reducing computational consumption. Experiments demonstrate the superiority performance of our SE-LLM against the state-of-the-art (SOTA) methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Consumption in Parallel Neural Network Training</title>
<link>https://arxiv.org/abs/2508.07706</link>
<guid>https://arxiv.org/abs/2508.07706</guid>
<content:encoded><![CDATA[
arXiv:2508.07706v1 Announce Type: new 
Abstract: The increasing demand for computational resources of training neural networks leads to a concerning growth in energy consumption. While parallelization has enabled upscaling model and dataset sizes and accelerated training, its impact on energy consumption is often overlooked. To close this research gap, we conducted scaling experiments for data-parallel training of two models, ResNet50 and FourCastNet, and evaluated the impact of parallelization parameters, i.e., GPU count, global batch size, and local batch size, on predictive performance, training time, and energy consumption. We show that energy consumption scales approximately linearly with the consumed resources, i.e., GPU hours; however, the respective scaling factor differs substantially between distinct model trainings and hardware, and is systematically influenced by the number of samples and gradient updates per GPU hour. Our results shed light on the complex interplay of scaling up neural network training and can inform future developments towards more sustainable AI research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer</title>
<link>https://arxiv.org/abs/2508.07710</link>
<guid>https://arxiv.org/abs/2508.07710</guid>
<content:encoded><![CDATA[
arXiv:2508.07710v1 Announce Type: new 
Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a promising approach for constructing energy-efficient Transformer architectures. Compared to directly trained Spiking Transformers, ANN-to-SNN conversion methods bypass the high training costs. However, existing methods still suffer from notable limitations, failing to effectively handle nonlinear operations in Transformer architectures and requiring additional fine-tuning processes for pre-trained ANNs. To address these issues, we propose a high-performance and training-free ANN-to-SNN conversion framework tailored for Transformer architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE) neuron, which employs an exponential decay strategy and multi-basis encoding method to efficiently approximate various nonlinear operations. It removes the requirement for weight modifications in pre-trained ANNs. Extensive experiments across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures (ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless conversion accuracy with significantly lower latency. This provides a promising pathway for the efficient and scalable deployment of Spiking Transformers in real-world applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information</title>
<link>https://arxiv.org/abs/2508.07713</link>
<guid>https://arxiv.org/abs/2508.07713</guid>
<content:encoded><![CDATA[
arXiv:2508.07713v1 Announce Type: new 
Abstract: Deep neural networks can memorize corrupted labels, making data quality critical for model performance, yet real-world datasets are frequently compromised by both label noise and input noise. This paper proposes a mutual information-based framework for data selection under hybrid noise scenarios that quantifies statistical dependencies between inputs and labels. We compute each sample's pointwise contribution to the overall mutual information and find that lower contributions indicate noisy or mislabeled instances. Empirical validation on MNIST with different synthetic noise settings demonstrates that the method effectively filters low-quality samples. Under label corruption, training on high-MI samples improves classification accuracy by up to 15\% compared to random sampling. Furthermore, the method exhibits robustness to benign input modifications, preserving semantically valid data while filtering truly corrupted samples.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations</title>
<link>https://arxiv.org/abs/2508.07722</link>
<guid>https://arxiv.org/abs/2508.07722</guid>
<content:encoded><![CDATA[
arXiv:2508.07722v1 Announce Type: new 
Abstract: In this work, we address the problem of training Reinforcement Learning (RL) agents over communication networks. The RL paradigm requires the agent to instantaneously perceive the state evolution to infer the effects of its actions on the environment. This is impossible if the agent receives state updates over lossy or delayed wireless systems and thus operates with partial and intermittent information. In recent years, numerous frameworks have been proposed to manage RL with imperfect feedback; however, they often offer specific solutions with a substantial computational burden. To address these limits, we propose a novel architecture, named Homomorphic Robust Remote Reinforcement Learning (HR3L), that enables the training of remote RL agents exchanging observations across a non-ideal wireless channel. HR3L considers two units: the transmitter, which encodes meaningful representations of the environment, and the receiver, which decodes these messages and performs actions to maximize a reward signal. Importantly, HR3L does not require the exchange of gradient information across the wireless channel, allowing for quicker training and a lower communication overhead than state-of-the-art solutions. Experimental results demonstrate that HR3L significantly outperforms baseline methods in terms of sample efficiency and adapts to different communication scenarios, including packet losses, delayed transmissions, and capacity limitations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning</title>
<link>https://arxiv.org/abs/2508.07738</link>
<guid>https://arxiv.org/abs/2508.07738</guid>
<content:encoded><![CDATA[
arXiv:2508.07738v1 Announce Type: new 
Abstract: Multi-Domain Continual Learning (MDCL) acquires knowledge from sequential tasks with shifting class sets and distribution. Despite the Parameter-Efficient Fine-Tuning (PEFT) methods can adapt for this dual heterogeneity, they still suffer from catastrophic forgetting and forward forgetting. To address these challenges, we propose a Two-Level Routing Grouped Mixture-of-Experts (TRGE) method. Firstly, TRGE dynamically expands the pre-trained CLIP model, assigning specific expert group for each task to mitigate catastrophic forgetting. With the number of experts continually grows in this process, TRGE maintains the static experts count within the group and introduces the intra-group router to alleviate routing overfitting caused by the increasing routing complexity. Meanwhile, we design an inter-group routing policy based on task identifiers and task prototype distance, which dynamically selects relevant expert groups and combines their outputs to enhance inter-task collaboration. Secondly, to get the correct task identifiers, we leverage Multimodal Large Language Models (MLLMs) which own powerful multimodal comprehension capabilities to generate semantic task descriptions and recognize the correct task identifier. Finally, to mitigate forward forgetting, we dynamically fuse outputs for unseen samples from the frozen CLIP model and TRGE adapter based on training progress, leveraging both pre-trained and learned knowledge. Through extensive experiments across various settings, our method outperforms other advanced methods with fewer trainable parameters.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory</title>
<link>https://arxiv.org/abs/2508.07746</link>
<guid>https://arxiv.org/abs/2508.07746</guid>
<content:encoded><![CDATA[
arXiv:2508.07746v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) aims to optimize the return given a fixed dataset of agent trajectories without additional interactions with the environment. While algorithm development has progressed rapidly, significant theoretical advances have also been made in understanding the fundamental challenges of offline RL. However, bridging these theoretical insights with practical algorithm design remains an ongoing challenge. In this survey, we explore key intuitions derived from theoretical work and their implications for offline RL algorithms.
  We begin by listing the conditions needed for the proofs, including function representation and data coverage assumptions. Function representation conditions tell us what to expect for generalization, and data coverage assumptions describe the quality requirement of the data. We then examine counterexamples, where offline RL is not solvable without an impractically large amount of data. These cases highlight what cannot be achieved for all algorithms and the inherent hardness of offline RL. Building on techniques to mitigate these challenges, we discuss the conditions that are sufficient for offline RL. These conditions are not merely assumptions for theoretical proofs, but they also reveal the limitations of these algorithms and remind us to search for novel solutions when the conditions cannot be satisfied.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment</title>
<link>https://arxiv.org/abs/2508.07750</link>
<guid>https://arxiv.org/abs/2508.07750</guid>
<content:encoded><![CDATA[
arXiv:2508.07750v1 Announce Type: new 
Abstract: Alignment methodologies have emerged as a critical pathway for enhancing language model alignment capabilities. While SFT (supervised fine-tuning) accelerates convergence through direct token-level loss intervention, its efficacy is constrained by offline policy trajectory. In contrast, RL(reinforcement learning) facilitates exploratory policy optimization, but suffers from low sample efficiency and stringent dependency on high-quality base models. To address these dual challenges, we propose GRAO (Group Relative Alignment Optimization), a unified framework that synergizes the respective strengths of SFT and RL through three key innovations: 1) A multi-sample generation strategy enabling comparative quality assessment via reward feedback; 2) A novel Group Direct Alignment Loss formulation leveraging intra-group relative advantage weighting; 3) Reference-aware parameter updates guided by pairwise preference dynamics. Our theoretical analysis establishes GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. Comprehensive evaluations across complex human alignment tasks demonstrate GRAO's superior performance, achieving 57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and GRPO baselines respectively. This work provides both a theoretically grounded alignment framework and empirical evidence for efficient capability evolution in language models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Probabilistic Graph Circuits</title>
<link>https://arxiv.org/abs/2508.07763</link>
<guid>https://arxiv.org/abs/2508.07763</guid>
<content:encoded><![CDATA[
arXiv:2508.07763v1 Announce Type: new 
Abstract: Deep generative models (DGMs) for graphs achieve impressively high expressive power thanks to very efficient and scalable neural networks. However, these networks contain non-linearities that prevent analytical computation of many standard probabilistic inference queries, i.e., these DGMs are considered \emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs) address this issue by enabling \emph{tractable} probabilistic inference, they operate on dense graph representations with $\mathcal{O}(n^2)$ complexity for graphs with $n$ nodes and \emph{$m$ edges}. To address this scalability issue, we introduce Sparse PGCs, a new class of tractable generative models that operate directly on sparse graph representation, reducing the complexity to $\mathcal{O}(n + m)$, which is particularly beneficial for $m \ll n^2$. In the context of de novo drug design, we empirically demonstrate that SPGCs retain exact inference capabilities, improve memory efficiency and inference speed, and match the performance of intractable DGMs in key metrics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pareto Multi-Objective Alignment for Language Models</title>
<link>https://arxiv.org/abs/2508.07768</link>
<guid>https://arxiv.org/abs/2508.07768</guid>
<content:encoded><![CDATA[
arXiv:2508.07768v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in real-world applications that require careful balancing of multiple, often conflicting, objectives, such as informativeness versus conciseness, or helpfulness versus creativity. However, current alignment methods, primarily based on RLHF, optimize LLMs toward a single reward function, resulting in rigid behavior that fails to capture the complexity and diversity of human preferences. This limitation hinders the adaptability of LLMs to practical scenarios, making multi-objective alignment (MOA) a critical yet underexplored area. To bridge this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and computationally efficient algorithm designed explicitly for MOA in LLMs. In contrast to computationally prohibitive multi-objective optimization (MOO) methods, PAMA transforms multi-objective RLHF into a convex optimization with a closed-form solution, significantly enhancing scalability. Traditional MOO approaches suffer from prohibitive O(n^2*d) complexity, where d represents the number of model parameters, typically in the billions for LLMs, rendering direct optimization infeasible. PAMA reduces this complexity to O(n) where n is the number of objectives, enabling optimization to be completed within milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto stationary point, where no objective can be improved without degrading at least one other. Extensive experiments across language models ranging from 125M to 7B parameters demonstrate PAMA's robust and effective MOA capabilities, aligning with its theoretical advantages. PAMA provides a highly efficient solution to the MOA problem that was previously considered intractable, offering a practical and theoretically grounded approach to aligning LLMs with diverse human values, paving the way for versatile and adaptable real-world AI deployments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Feature Compression for Molecular Graph Neural Networks</title>
<link>https://arxiv.org/abs/2508.07807</link>
<guid>https://arxiv.org/abs/2508.07807</guid>
<content:encoded><![CDATA[
arXiv:2508.07807v1 Announce Type: new 
Abstract: Recent advances in molecular representation learning have produced highly effective encodings of molecules for numerous cheminformatics and bioinformatics tasks. However, extracting general chemical insight while balancing predictive accuracy, interpretability, and computational efficiency remains a major challenge. In this work, we introduce a novel Graph Neural Network (GNN) architecture that combines compressed higher-order topological signals with standard molecular features. Our approach captures global geometric information while preserving computational tractability and human-interpretable structure. We evaluate our model across a range of benchmarks, from small-molecule datasets to complex material datasets, and demonstrate superior performance using a parameter-efficient architecture. We achieve the best performing results in both accuracy and robustness across almost all benchmarks. We open source all code \footnote{All code and results can be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.07809</link>
<guid>https://arxiv.org/abs/2508.07809</guid>
<content:encoded><![CDATA[
arXiv:2508.07809v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable reward (RLVR) has become a promising paradigm for post-training large language models (LLMs) to improve their reasoning capability. However, when the rollout accuracy is low on hard problems, the reward becomes sparse, limiting learning efficiency and causing exploration bottlenecks. Existing approaches either rely on stronger LLMs for distillation or filter out difficult problems, which limits scalability or restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the exploration space by self-generating and verifying CoT trajectories, then gradually shortens them to expand the space in a controlled way. This enables LLMs to stably learn from initially unsolved hard problems under sparse rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek, and Llama. Experiments show that EvoCoT enables LLMs to solve previously unsolved problems, improves reasoning capability without external CoT supervision, and is compatible with various RL fine-tuning methods. We release the source code to support future research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow</title>
<link>https://arxiv.org/abs/2508.07841</link>
<guid>https://arxiv.org/abs/2508.07841</guid>
<content:encoded><![CDATA[
arXiv:2508.07841v1 Announce Type: new 
Abstract: Attitude control is a fundamental aspect of spacecraft operations. Model Predictive Control (MPC) has emerged as a powerful strategy for these tasks, relying on accurate models of the system dynamics to optimize control actions over a prediction horizon. In scenarios where physics models are incomplete, difficult to derive, or computationally expensive, machine learning offers a flexible alternative by learning the system behavior directly from data. However, purely data-driven models often struggle with generalization and stability, especially when applied to inputs outside their training domain. To address these limitations, we investigate the benefits of incorporating Physics-Informed Neural Networks (PINNs) into the learning of spacecraft attitude dynamics, comparing their performance with that of purely data-driven approaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network architecture with a self-attention mechanism, we trained several models on simulated data generated with the Basilisk simulator. Two training strategies were considered: a purely data-driven baseline and a physics-informed variant to improve robustness and stability. Our results demonstrate that the inclusion of physics-based information significantly enhances the performance in terms of the mean relative error of the best architectures found by 27.08%. These advantages are particularly evident when the learned models are integrated into an MPC framework, where PINN-based models consistently outperform their purely data-driven counterparts in terms of control accuracy and robustness, yielding improvements of up to 42.86% in performance stability error and increased robustness-to-noise.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant</title>
<link>https://arxiv.org/abs/2508.07887</link>
<guid>https://arxiv.org/abs/2508.07887</guid>
<content:encoded><![CDATA[
arXiv:2508.07887v1 Announce Type: new 
Abstract: Simulators have revolutionized scientific practice across the natural sciences. By generating data that reliably approximate real-world phenomena, they enable scientists to accelerate hypothesis testing and optimize experimental designs. This is perhaps best illustrated by AlphaFold, a Nobel-prize winning simulator in chemistry that predicts protein structures from amino acid sequences, enabling rapid prototyping of molecular interactions, drug targets, and protein functions. In the behavioral sciences, a reliable participant simulator - a system capable of producing human-like behavior across cognitive tasks - would represent a similarly transformative advance. Recently, Binz et al. introduced Centaur, a large language model (LLM) fine-tuned on human data from 160 experiments, proposing its use not only as a model of cognition but also as a participant simulator for "in silico prototyping of experimental studies", e.g., to advance automated cognitive science. Here, we review the core criteria for a participant simulator and assess how well Centaur meets them. Although Centaur demonstrates strong predictive accuracy, its generative behavior - a critical criterion for a participant simulator - systematically diverges from human data. This suggests that, while Centaur is a significant step toward predicting human behavior, it does not yet meet the standards of a reliable participant simulator or an accurate model of cognition.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score Augmentation for Diffusion Models</title>
<link>https://arxiv.org/abs/2508.07926</link>
<guid>https://arxiv.org/abs/2508.07926</guid>
<content:encoded><![CDATA[
arXiv:2508.07926v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success in generative modeling. However, this study confirms the existence of overfitting in diffusion model training, particularly in data-limited regimes. To address this challenge, we propose Score Augmentation (ScoreAug), a novel data augmentation framework specifically designed for diffusion models. Unlike conventional augmentation approaches that operate on clean data, ScoreAug applies transformations to noisy data, aligning with the inherent denoising mechanism of diffusion. Crucially, ScoreAug further requires the denoiser to predict the augmentation of the original target. This design establishes an equivariant learning objective, enabling the denoiser to learn scores across varied denoising spaces, thereby realizing what we term score augmentation. We also theoretically analyze the relationship between scores in different spaces under general transformations. In experiments, we extensively validate ScoreAug on multiple benchmarks including CIFAR-10, FFHQ, AFHQv2, and ImageNet, with results demonstrating significant performance improvements over baselines. Notably, ScoreAug effectively mitigates overfitting across diverse scenarios, such as varying data scales and model capacities, while exhibiting stable convergence properties. Another advantage of ScoreAug over standard data augmentation lies in its ability to circumvent data leakage issues under certain conditions. Furthermore, we show that ScoreAug can be synergistically combined with traditional data augmentation techniques to achieve additional performance gains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.07927</link>
<guid>https://arxiv.org/abs/2508.07927</guid>
<content:encoded><![CDATA[
arXiv:2508.07927v1 Announce Type: new 
Abstract: Time series forecasting poses significant challenges in non-stationary environments where underlying patterns evolve over time. In this work, we propose a novel framework that enhances deep neural network (DNN) performance by leveraging specialized model adaptation and selection. Initially, a base DNN is trained offline on historical time series data. A reserved validation subset is then segmented to extract and cluster the most dominant patterns within the series, thereby identifying distinct regimes. For each identified cluster, the base DNN is fine-tuned to produce a specialized version that captures unique pattern characteristics. At inference, the most recent input is matched against the cluster centroids, and the corresponding fine-tuned version is deployed based on the closest similarity measure. Additionally, our approach integrates a concept drift detection mechanism to identify and adapt to emerging patterns caused by non-stationary behavior. The proposed framework is generalizable across various DNN architectures and has demonstrated significant performance gains on both traditional DNNs and recent advanced architectures implemented in the GluonTS library.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters</title>
<link>https://arxiv.org/abs/2508.07952</link>
<guid>https://arxiv.org/abs/2508.07952</guid>
<content:encoded><![CDATA[
arXiv:2508.07952v1 Announce Type: new 
Abstract: Clustering algorithms often assume all features contribute equally to the data structure, an assumption that usually fails in high-dimensional or noisy settings. Feature weighting methods can address this, but most require additional parameter tuning. We propose SHARK (Shapley Reweighted $k$-means), a feature-weighted clustering algorithm motivated by the use of Shapley values from cooperative game theory to quantify feature relevance, which requires no additional parameters beyond those in $k$-means. We prove that the $k$-means objective can be decomposed into a sum of per-feature Shapley values, providing an axiomatic foundation for unsupervised feature relevance and reducing Shapley computation from exponential to polynomial time. SHARK iteratively re-weights features by the inverse of their Shapley contribution, emphasising informative dimensions and down-weighting irrelevant ones. Experiments on synthetic and real-world data sets show that SHARK consistently matches or outperforms existing methods, achieving superior robustness and accuracy, particularly in scenarios where noise may be present. Software: https://github.com/rickfawley/shark.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer</title>
<link>https://arxiv.org/abs/2508.07970</link>
<guid>https://arxiv.org/abs/2508.07970</guid>
<content:encoded><![CDATA[
arXiv:2508.07970v1 Announce Type: new 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent paradigm for training large language models and multimodal systems. Despite notable advances enabled by existing RLHF training frameworks, significant challenges remain in scaling to complex multimodal workflows and adapting to dynamic workloads. In particular, current systems often encounter limitations related to controller scalability when managing large models, as well as inefficiencies in orchestrating intricate RLHF pipelines, especially in scenarios that require dynamic sampling and resource allocation. In this paper, we introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple, scalable, and balanced RLHF training framework specifically designed to address these challenges. WeChat-YATT features a parallel controller programming model that enables flexible and efficient orchestration of complex RLHF workflows, effectively mitigating the bottlenecks associated with centralized controller architectures and facilitating scalability in large-scale data scenarios. In addition, we propose a dynamic placement schema that adaptively partitions computational resources and schedules workloads, thereby significantly reducing hardware idle time and improving GPU utilization under variable training conditions. We evaluate WeChat-YATT across a range of experimental scenarios, demonstrating that it achieves substantial improvements in throughput compared to state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been successfully deployed to train models supporting WeChat product features for a large-scale user base, underscoring its effectiveness and robustness in real-world applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-informed Deep Operator for Real-Time Freeway Traffic State Estimation</title>
<link>https://arxiv.org/abs/2508.08002</link>
<guid>https://arxiv.org/abs/2508.08002</guid>
<content:encoded><![CDATA[
arXiv:2508.08002v1 Announce Type: new 
Abstract: Traffic state estimation (TSE) falls methodologically into three categories: model-driven, data-driven, and model-data dual-driven. Model-driven TSE relies on macroscopic traffic flow models originated from hydrodynamics. Data-driven TSE leverages historical sensing data and employs statistical models or machine learning methods to infer traffic state. Model-data dual-driven traffic state estimation attempts to harness the strengths of both aspects to achieve more accurate TSE. From the perspective of mathematical operator theory, TSE can be viewed as a type of operator that maps available measurements of inerested traffic state into unmeasured traffic state variables in real time. For the first time this paper proposes to study real-time freeway TSE in the idea of physics-informed deep operator network (PI-DeepONet), which is an operator-oriented architecture embedding traffic flow models based on deep neural networks. The paper has developed an extended architecture from the original PI-DeepONet. The extended architecture is featured with: (1) the acceptance of 2-D data input so as to support CNN-based computations; (2) the introduction of a nonlinear expansion layer, an attention mechanism, and a MIMO mechanism; (3) dedicated neural network design for adaptive identification of traffic flow model parameters. A traffic state estimator built on the basis of this extended PI-DeepONet architecture was evaluated with respect to a short freeway stretch of NGSIM and a large-scale urban expressway in China, along with other four baseline TSE methods. The evaluation results demonstrated that this novel TSE method outperformed the baseline methods with high-precision estimation results of flow and mean speed.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</title>
<link>https://arxiv.org/abs/2508.08005</link>
<guid>https://arxiv.org/abs/2508.08005</guid>
<content:encoded><![CDATA[
arXiv:2508.08005v1 Announce Type: new 
Abstract: Extensive experiments and prior studies show that no single maximum clique algorithm consistently performs best across all instances, highlighting the importance of selecting suitable algorithms based on instance features. Through an extensive analysis of relevant studies, it is found that there is a lack of research work concerning algorithm selection oriented toward the Maximum Clique Problem (MCP). In this work, we propose a learning-based framework that integrates both traditional machine learning and graph neural networks to address this gap. We construct a labeled dataset by running four exact MCP algorithms on a diverse collection of graph instances, accompanied by structural and global statistical features extracted from each graph. We first evaluate four conventional classifiers: Support Vector Machine (SVM), Random Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple dataset variants. Experimental results show that RF consistently shows strong performance across metrics and dataset variants, making it a reliable baseline. In addition, feature importance analysis indicates that connectivity and topological structure are strong predictors of algorithm performance. Building on these findings, we develop a dual-channel model named GAT-MLP, which combines a Graph Attention Network (GAT) for local structural encoding with a Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model shows strong and consistent performance across all metrics. Our results highlight the effectiveness of dual-channel architectures and the promise of graph neural networks in combinatorial algorithm selection.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Zero-Order and First-Order Federated Learning Methods over Wireless Networks</title>
<link>https://arxiv.org/abs/2508.08013</link>
<guid>https://arxiv.org/abs/2508.08013</guid>
<content:encoded><![CDATA[
arXiv:2508.08013v1 Announce Type: new 
Abstract: Federated Learning (FL) is an emerging learning framework that enables edge devices to collaboratively train ML models without sharing their local data. FL faces, however, a significant challenge due to the high amount of information that must be exchanged between the devices and the aggregator in the training phase, which can exceed the limited capacity of wireless systems. In this paper, two communication-efficient FL methods are considered where communication overhead is reduced by communicating scalar values instead of long vectors and by allowing high number of users to send information simultaneously. The first approach employs a zero-order optimization technique with two-point gradient estimator, while the second involves a first-order gradient computation strategy. The novelty lies in leveraging channel information in the learning algorithms, eliminating hence the need for additional resources to acquire channel state information (CSI) and to remove its impact, as well as in considering asynchronous devices. We provide a rigorous analytical framework for the two methods, deriving convergence guarantees and establishing appropriate performance bounds.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles</title>
<link>https://arxiv.org/abs/2508.08034</link>
<guid>https://arxiv.org/abs/2508.08034</guid>
<content:encoded><![CDATA[
arXiv:2508.08034v1 Announce Type: new 
Abstract: Accurate power consumption prediction is crucial for improving efficiency and reducing environmental impact, yet traditional methods relying on specialized instruments or rigid physical models are impractical for large-scale, real-world deployment. This study introduces a scalable data-driven method using powertrain dynamic feature sets and both traditional machine learning and deep neural networks to estimate instantaneous and cumulative power consumption in internal combustion engine (ICE), electric vehicle (EV), and hybrid electric vehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with mean absolute error and root mean squared error on the order of $10^{-3}$, and cumulative errors under 3%. Transformer and long short-term memory models performed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%, respectively. Results confirm the approach's effectiveness across vehicles and models. Uncertainty analysis revealed greater variability in EV and HEV datasets than ICE, due to complex power management, emphasizing the need for robust models for advanced powertrains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</title>
<link>https://arxiv.org/abs/2508.08040</link>
<guid>https://arxiv.org/abs/2508.08040</guid>
<content:encoded><![CDATA[
arXiv:2508.08040v1 Announce Type: new 
Abstract: Prompt-based tuning has emerged as a lightweight alternative to full fine-tuning in large vision-language models, enabling efficient adaptation via learned contextual prompts. This paradigm has recently been extended to federated learning settings (e.g., PromptFL), where clients collaboratively train prompts under data privacy constraints. However, the security implications of prompt-based aggregation in federated multimodal learning remain largely unexplored, leaving a critical attack surface unaddressed. In this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack targeting prompt-based federated learning in multimodal contrastive models. In BadPromptFL, compromised clients jointly optimize local backdoor triggers and prompt embeddings, injecting poisoned prompts into the global aggregation process. These prompts are then propagated to benign clients, enabling universal backdoor activation at inference without modifying model parameters. Leveraging the contextual learning behavior of CLIP-style architectures, BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal visibility and limited client participation. Extensive experiments across multiple datasets and aggregation protocols validate the effectiveness, stealth, and generalizability of our attack, raising critical concerns about the robustness of prompt-based federated learning in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Understanding of the Dynamics of Model Capacity in Continual Learning</title>
<link>https://arxiv.org/abs/2508.08052</link>
<guid>https://arxiv.org/abs/2508.08052</guid>
<content:encoded><![CDATA[
arXiv:2508.08052v1 Announce Type: new 
Abstract: The stability-plasticity dilemma, closely related to a neural network's (NN) capacity-its ability to represent tasks-is a fundamental challenge in continual learning (CL). Within this context, we introduce CL's effective model capacity (CLEMC) that characterizes the dynamic behavior of the stability-plasticity balance point. We develop a difference equation to model the evolution of the interplay between the NN, task data, and optimization procedure. We then leverage CLEMC to demonstrate that the effective capacity-and, by extension, the stability-plasticity balance point is inherently non-stationary. We show that regardless of the NN architecture or optimization method, a NN's ability to represent new tasks diminishes when incoming task distributions differ from previous ones. We conduct extensive experiments to support our theoretical findings, spanning a range of architectures-from small feedforward network and convolutional networks to medium-sized graph neural networks and transformer-based large language models with millions of parameters.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations</title>
<link>https://arxiv.org/abs/2508.08061</link>
<guid>https://arxiv.org/abs/2508.08061</guid>
<content:encoded><![CDATA[
arXiv:2508.08061v1 Announce Type: new 
Abstract: Event logs reflect the behavior of business processes that are mapped in organizational information systems. Predictive process monitoring (PPM) transforms these data into value by creating process-related predictions that provide the insights required for proactive interventions at process runtime. Existing PPM techniques require sufficient amounts of event data or other relevant resources that might not be readily available, preventing some organizations from utilizing PPM. The transfer learning-based PPM technique presented in this paper allows organizations without suitable event data or other relevant resources to implement PPM for effective decision support. The technique is instantiated in two real-life use cases, based on which numerical experiments are performed using event logs for IT service management processes in an intra- and inter-organizational setting. The results of the experiments suggest that knowledge of one business process can be transferred to a similar business process in the same or a different organization to enable effective PPM in the target context. With the proposed technique, organizations can benefit from transfer learning in an intra- and inter-organizational setting, where resources like pre-trained models are transferred within and across organizational boundaries.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction</title>
<link>https://arxiv.org/abs/2508.08071</link>
<guid>https://arxiv.org/abs/2508.08071</guid>
<content:encoded><![CDATA[
arXiv:2508.08071v1 Announce Type: new 
Abstract: Connecting an ever-expanding catalogue of products with suitable manufacturers and suppliers is critical for resilient, efficient global supply chains, yet traditional methods struggle to capture complex capabilities, certifications, geographic constraints, and rich multimodal data of real-world manufacturer profiles. To address these gaps, we introduce PMGraph, a public benchmark of bipartite and heterogeneous multimodal supply-chain graphs linking 8,888 manufacturers, over 70k products, more than 110k manufacturer-product edges, and over 29k product images. Building on this benchmark, we propose the Cascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first aligns and aggregates textual and visual attributes into intermediate group embeddings, then propagates them through a manufacturer-product hetero-graph via multiscale message passing to enhance link prediction accuracy. C-MAG also provides practical guidelines for modality-aware fusion, preserving predictive performance in noisy, real-world settings.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring</title>
<link>https://arxiv.org/abs/2508.08073</link>
<guid>https://arxiv.org/abs/2508.08073</guid>
<content:encoded><![CDATA[
arXiv:2508.08073v1 Announce Type: new 
Abstract: In electronic design automation, logic optimization operators play a crucial role in minimizing the gate count of logic circuits. However, their computation demands are high. Operators such as refactor conventionally form iterative cuts for each node, striving for a more compact representation - a task which often fails 98% on average. Prior research has sought to mitigate computational cost through parallelization. In contrast, our approach leverages a classifier to prune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis operations. Experiments on the refactor operator using the EPFL benchmark suite and 10 large industrial designs demonstrate that this technique can speedup logic optimization by 3.9x on average compared with the state-of-the-art ABC implementation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles</title>
<link>https://arxiv.org/abs/2508.08080</link>
<guid>https://arxiv.org/abs/2508.08080</guid>
<content:encoded><![CDATA[
arXiv:2508.08080v1 Announce Type: new 
Abstract: Symbolic Regression (SR) is a well-established framework for generating interpretable or white-box predictive models. Although SR has been successfully applied to create interpretable estimates of the average of the outcome, it is currently not well understood how it can be used to estimate the relationship between variables at other points in the distribution of the target variable. Such estimates of e.g. the median or an extreme value provide a fuller picture of how predictive variables affect the outcome and are necessary in high-stakes, safety-critical application domains. This study introduces Symbolic Quantile Regression (SQR), an approach to predict conditional quantiles with SR. In an extensive evaluation, we find that SQR outperforms transparent models and performs comparably to a strong black-box baseline without compromising transparency. We also show how SQR can be used to explain differences in the target distribution by comparing models that predict extreme and central outcomes in an airline fuel usage case study. We conclude that SQR is suitable for predicting conditional quantiles and understanding interesting feature influences at varying quantiles.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation</title>
<link>https://arxiv.org/abs/2508.08087</link>
<guid>https://arxiv.org/abs/2508.08087</guid>
<content:encoded><![CDATA[
arXiv:2508.08087v1 Announce Type: new 
Abstract: Reliable digital twins of lithium-ion batteries must achieve high physical fidelity with sub-millisecond speed. In this work, we benchmark three operator-learning surrogates for the Single Particle Model (SPM): Deep Operator Networks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed parameter-embedded Fourier Neural Operator (PE-FNO), which conditions each spectral layer on particle radius and solid-phase diffusivity. Models are trained on simulated trajectories spanning four current families (constant, triangular, pulse-train, and Gaussian-random-field) and a full range of State-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates constant-current behaviour but struggles with more dynamic loads. The basic FNO maintains mesh invariance and keeps concentration errors below 1 %, with voltage mean-absolute errors under 1.7 mV across all load types. Introducing parameter embedding marginally increases error, but enables generalisation to varying radii and diffusivities. PE-FNO executes approximately 200 times faster than a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse tasks are explored in a parameter estimation task with Bayesian optimisation, recovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute percentage error, respectively, and 0.5918 percentage points higher error in comparison with classical methods. These results pave the way for neural operators to meet the accuracy, speed and parametric flexibility demands of real-time battery management, design-of-experiments and large-scale inference. PE-FNO outperforms conventional neural surrogates, offering a practical path towards high-speed and high-fidelity electrochemical digital twins.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid2Guide: A* Enabled Small Language Model for Indoor Navigation</title>
<link>https://arxiv.org/abs/2508.08100</link>
<guid>https://arxiv.org/abs/2508.08100</guid>
<content:encoded><![CDATA[
arXiv:2508.08100v1 Announce Type: new 
Abstract: Reliable indoor navigation remains a significant challenge in complex environments, particularly where external positioning signals and dedicated infrastructures are unavailable. This research presents Grid2Guide, a hybrid navigation framework that combines the A* search algorithm with a Small Language Model (SLM) to generate clear, human-readable route instructions. The framework first conducts a binary occupancy matrix from a given indoor map. Using this matrix, the A* algorithm computes the optimal path between origin and destination, producing concise textual navigation steps. These steps are then transformed into natural language instructions by the SLM, enhancing interpretability for end users. Experimental evaluations across various indoor scenarios demonstrate the method's effectiveness in producing accurate and timely navigation guidance. The results validate the proposed approach as a lightweight, infrastructure-free solution for real-time indoor navigation support.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Based Localization and LLM-based Navigation for Indoor Environments</title>
<link>https://arxiv.org/abs/2508.08120</link>
<guid>https://arxiv.org/abs/2508.08120</guid>
<content:encoded><![CDATA[
arXiv:2508.08120v1 Announce Type: new 
Abstract: Indoor navigation remains a complex challenge due to the absence of reliable GPS signals and the architectural intricacies of large enclosed environments. This study presents an indoor localization and navigation approach that integrates vision-based localization with large language model (LLM)-based navigation. The localization system utilizes a ResNet-50 convolutional neural network fine-tuned through a two-stage process to identify the user's position using smartphone camera input. To complement localization, the navigation module employs an LLM, guided by a carefully crafted system prompt, to interpret preprocessed floor plan images and generate step-by-step directions. Experimental evaluation was conducted in a realistic office corridor with repetitive features and limited visibility to test localization robustness. The model achieved high confidence and an accuracy of 96% across all tested waypoints, even under constrained viewing conditions and short-duration queries. Navigation tests using ChatGPT on real building floor maps yielded an average instruction accuracy of 75%, with observed limitations in zero-shot reasoning and inference time. This research demonstrates the potential for scalable, infrastructure-free indoor navigation using off-the-shelf cameras and publicly available floor plans, particularly in resource-constrained settings like hospitals, airports, and educational institutions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing</title>
<link>https://arxiv.org/abs/2508.08122</link>
<guid>https://arxiv.org/abs/2508.08122</guid>
<content:encoded><![CDATA[
arXiv:2508.08122v1 Announce Type: new 
Abstract: Knowledge Tracing (KT) is committed to capturing students' knowledge mastery from their historical interactions. Simulating students' memory states is a promising approach to enhance both the performance and interpretability of knowledge tracing models. Memory consists of three fundamental processes: encoding, storage, and retrieval. Although forgetting primarily manifests during the storage stage, most existing studies rely on a single, undifferentiated forgetting mechanism, overlooking other memory processes as well as personalized forgetting patterns. To address this, this paper proposes memoryKT, a knowledge tracing model based on a novel temporal variational autoencoder. The model simulates memory dynamics through a three-stage process: (i) Learning the distribution of students' knowledge memory features, (ii) Reconstructing their exercise feedback, while (iii) Embedding a personalized forgetting module within the temporal workflow to dynamically modulate memory storage strength. This jointly models the complete encoding-storage-retrieval cycle, significantly enhancing the model's perception capability for individual differences. Extensive experiments on four public datasets demonstrate that our proposed approach significantly outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection</title>
<link>https://arxiv.org/abs/2508.08124</link>
<guid>https://arxiv.org/abs/2508.08124</guid>
<content:encoded><![CDATA[
arXiv:2508.08124v1 Announce Type: new 
Abstract: Large-scale models pre-trained on Electroencephalography (EEG) have shown promise in clinical applications such as neurological disorder detection. However, the practical deployment of EEG-based large-scale models faces critical challenges such as limited labeled EEG data and suboptimal performance in clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel large-scale model specifically designed for detecting EEG-based neurological disorders. Our key contributions include (i) a Selective Temporal-Frequency Embedding mechanism that adaptively captures complex temporal and spectral patterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy that refines feature representation in a two-stage process. In the first stage, our model learns the fundamental discriminative features of EEG activities; in the second stage, the model further extracts more specialized fine-grained features for accurate diagnostic performance. We evaluated NeuroDx-LM on the CHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in EEG-based seizure and schizophrenia detection, respectively. These results demonstrate the great potential of EEG-based large-scale models to advance clinical applicability. Our code is available at https://github.com/LetItBe12345/NeuroDx-LM.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OFAL: An Oracle-Free Active Learning Framework</title>
<link>https://arxiv.org/abs/2508.08126</link>
<guid>https://arxiv.org/abs/2508.08126</guid>
<content:encoded><![CDATA[
arXiv:2508.08126v1 Announce Type: new 
Abstract: In the active learning paradigm, using an oracle to label data has always been a complex and expensive task, and with the emersion of large unlabeled data pools, it would be highly beneficial If we could achieve better results without relying on an oracle. This research introduces OFAL, an oracle-free active learning scheme that utilizes neural network uncertainty. OFAL uses the model's own uncertainty to transform highly confident unlabeled samples into informative uncertain samples. First, we start with separating and quantifying different parts of uncertainty and introduce Monte Carlo Dropouts as an approximation of the Bayesian Neural Network model. Secondly, by adding a variational autoencoder, we go on to generate new uncertain samples by stepping toward the uncertain part of latent space starting from a confidence seed sample. By generating these new informative samples, we can perform active learning and enhance the model's accuracy. Lastly, we try to compare and integrate our method with other widely used active learning sampling methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.08137</link>
<guid>https://arxiv.org/abs/2508.08137</guid>
<content:encoded><![CDATA[
arXiv:2508.08137v1 Announce Type: new 
Abstract: Conducting a comprehensive literature review is crucial for advancing circuit design methodologies. However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging. In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason + Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step information retrieval. It functions as a question-answering design assistant, capable of interpreting complex queries and providing reasoned responses grounded in circuit literature. Its multimodal capabilities enable processing of both textual and visual data, facilitating more efficient and comprehensive analysis. The system dynamically adapts using intelligent search tools, automated document retrieval from the internet, and real-time database updates. Unlike conventional approaches constrained by model context limits, MuaLLM decouples retrieval from inference, enabling scalable reasoning over arbitrarily large corpora. At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy. This allows rapid, no-human-in-the-loop database generation, overcoming the bottleneck of simulation-based dataset creation for circuits. To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8% accuracy on Reas-100.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks</title>
<link>https://arxiv.org/abs/2508.08151</link>
<guid>https://arxiv.org/abs/2508.08151</guid>
<content:encoded><![CDATA[
arXiv:2508.08151v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) are being utilized in various aspects of our daily lives, including high-stakes decision-making applications that impact individuals. However, these systems reflect and amplify bias from the data used during training and testing, potentially resulting in biased behavior and inaccurate decisions. For instance, having different misclassification rates between white and black sub-populations. However, effectively and efficiently identifying and correcting biased behavior in DNNs is a challenge. This paper introduces FairFLRep, an automated fairness-aware fault localization and repair technique that identifies and corrects potentially bias-inducing neurons in DNN classifiers. FairFLRep focuses on adjusting neuron weights associated with sensitive attributes, such as race or gender, that contribute to unfair decisions. By analyzing the input-output relationships within the network, FairFLRep corrects neurons responsible for disparities in predictive quality parity. We evaluate FairFLRep on four image classification datasets using two DNN classifiers, and four tabular datasets with a DNN model. The results show that FairFLRep consistently outperforms existing methods in improving fairness while preserving accuracy. An ablation study confirms the importance of considering fairness during both fault localization and repair stages. Our findings also show that FairFLRep is more efficient than the baseline approaches in repairing the network.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets</title>
<link>https://arxiv.org/abs/2508.08159</link>
<guid>https://arxiv.org/abs/2508.08159</guid>
<content:encoded><![CDATA[
arXiv:2508.08159v1 Announce Type: new 
Abstract: Developing accurate and generalizable epileptic seizure prediction models from electroencephalography (EEG) data across multiple clinical sites is hindered by patient privacy regulations and significant data heterogeneity (non-IID characteristics). Federated Learning (FL) offers a privacy-preserving framework for collaborative training, but standard aggregation methods like Federated Averaging (FedAvg) can be biased by dominant datasets in heterogeneous settings. This paper investigates FL for seizure prediction using a single EEG channel across four diverse public datasets (Siena, CHB-MIT, Helsinki, NCH), representing distinct patient populations (adult, pediatric, neonate) and recording conditions. We implement privacy-preserving global normalization and propose a Random Subset Aggregation strategy, where each client trains on a fixed-size random subset of its data per round, ensuring equal contribution during aggregation. Our results show that locally trained models fail to generalize across sites, and standard weighted FedAvg yields highly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on Helsinki and 50.6% on NCH). In contrast, Random Subset Aggregation significantly improves performance on under-represented clients (accuracy increases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior macro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites, demonstrating a more robust and fair global model. This work highlights the potential of balanced FL approaches for building effective and generalizable seizure prediction systems in realistic, heterogeneous multi-hospital environments while respecting data privacy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Logic Networks for Interpretable Classification</title>
<link>https://arxiv.org/abs/2508.08172</link>
<guid>https://arxiv.org/abs/2508.08172</guid>
<content:encoded><![CDATA[
arXiv:2508.08172v1 Announce Type: new 
Abstract: Traditional neural networks have an impressive classification performance, but what they learn cannot be inspected, verified or extracted. Neural Logic Networks on the other hand have an interpretable structure that enables them to learn a logical mechanism relating the inputs and outputs with AND and OR operations. We generalize these networks with NOT operations and biases that take into account unobserved data and develop a rigorous logical and probabilistic modeling in terms of concept combinations to motivate their use. We also propose a novel factorized IF-THEN rule structure for the model as well as a modified learning algorithm. Our method improves the state-of-the-art in Boolean networks discovery and is able to learn relevant, interpretable rules in tabular classification, notably on an example from the medical field where interpretability has tangible value.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion</title>
<link>https://arxiv.org/abs/2508.08216</link>
<guid>https://arxiv.org/abs/2508.08216</guid>
<content:encoded><![CDATA[
arXiv:2508.08216v1 Announce Type: new 
Abstract: Personalised music-based interventions offer a powerful means of supporting motor rehabilitation by dynamically tailoring auditory stimuli to provide external timekeeping cues, modulate affective states, and stabilise gait patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for adapting these interventions across individuals. However, inter-subject variability in EEG signals, further compounded by movement-induced artefacts and motor planning differences, hinders the generalisability of BCIs and results in lengthy calibration processes. We propose Individual Tangent Space Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific recentering, distribution matching, and supervised rotational alignment to enhance cross-subject generalisation. Our hybrid architecture fuses Regularised Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and sequential configurations, improving class separability while maintaining the geometric structure of covariance matrices for robust statistical computation. Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant performance improvements across subjects and conditions. The parallel fusion approach shows the greatest enhancement over its sequential counterpart, with robust performance maintained across varying data conditions and electrode configurations. The code will be made publicly available at the time of publication.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning</title>
<link>https://arxiv.org/abs/2508.08221</link>
<guid>https://arxiv.org/abs/2508.08221</guid>
<content:encoded><![CDATA[
arXiv:2508.08221v1 Announce Type: new 
Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent</title>
<link>https://arxiv.org/abs/2508.08222</link>
<guid>https://arxiv.org/abs/2508.08222</guid>
<content:encoded><![CDATA[
arXiv:2508.08222v1 Announce Type: new 
Abstract: Transformers have demonstrated remarkable capabilities in multi-step reasoning tasks. However, understandings of the underlying mechanisms by which they acquire these abilities through training remain limited, particularly from a theoretical standpoint. This work investigates how transformers learn to solve symbolic multi-step reasoning problems through chain-of-thought processes, focusing on path-finding in trees. We analyze two intertwined tasks: a backward reasoning task, where the model outputs a path from a goal node to the root, and a more complex forward reasoning task, where the model implements two-stage reasoning by first identifying the goal-to-root path and then reversing it to produce the root-to-goal path. Our theoretical analysis, grounded in the dynamics of gradient descent, shows that trained one-layer transformers can provably solve both tasks with generalization guarantees to unseen trees. In particular, our multi-phase training dynamics for forward reasoning elucidate how different attention heads learn to specialize and coordinate autonomously to solve the two subtasks in a single autoregressive path. These results provide a mechanistic explanation of how trained transformers can implement sequential algorithmic procedures. Moreover, they offer insights into the emergence of reasoning abilities, suggesting that when tasks are structured to take intermediate chain-of-thought steps, even shallow multi-head transformers can effectively solve problems that would otherwise require deeper architectures.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers</title>
<link>https://arxiv.org/abs/2508.05691</link>
<guid>https://arxiv.org/abs/2508.05691</guid>
<content:encoded><![CDATA[
arXiv:2508.05691v1 Announce Type: cross 
Abstract: Generative models are increasingly adopted in high-stakes domains, yet current deployments offer no mechanisms to verify the origin of model outputs. We address this gap by extending model fingerprinting techniques beyond the traditional collaborative setting to one where the model provider may act adversarially. To our knowledge, this is the first work to evaluate fingerprinting for provenance attribution under such a threat model. The methods rely on a trusted verifier that extracts secret fingerprints from the model's output space, unknown to the provider, and trains a model to predict and verify them. Our empirical evaluation shows that our methods achieve near-zero FPR@95%TPR for instances of GAN and diffusion models, even when tested on small modifications to the original architecture and training data. Moreover, the methods remain robust against adversarial attacks that actively modify the outputs to bypass detection. Source codes are available at https://github.com/PSMLab/authprint.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Commodity Price Shocks Using Temporal and Semantic Fusion of Prices Signals and Agentic Generative AI Extracted Economic News</title>
<link>https://arxiv.org/abs/2508.06497</link>
<guid>https://arxiv.org/abs/2508.06497</guid>
<content:encoded><![CDATA[
arXiv:2508.06497v1 Announce Type: cross 
Abstract: Accurate forecasting of commodity price spikes is vital for countries with limited economic buffers, where sudden increases can strain national budgets, disrupt import-reliant sectors, and undermine food and energy security. This paper introduces a hybrid forecasting framework that combines historical commodity price data with semantic signals derived from global economic news, using an agentic generative AI pipeline. The architecture integrates dual-stream Long Short-Term Memory (LSTM) networks with attention mechanisms to fuse structured time-series inputs with semantically embedded, fact-checked news summaries collected from 1960 to 2023. The model is evaluated on a 64-year dataset comprising normalized commodity price series and temporally aligned news embeddings. Results show that the proposed approach achieves a mean AUC of 0.94 and an overall accuracy of 0.91 substantially outperforming traditional baselines such as logistic regression (AUC = 0.34), random forest (AUC = 0.57), and support vector machines (AUC = 0.47). Additional ablation studies reveal that the removal of attention or dimensionality reduction leads to moderate declines in performance, while eliminating the news component causes a steep drop in AUC to 0.46, underscoring the critical value of incorporating real-world context through unstructured text. These findings demonstrate that integrating agentic generative AI with deep learning can meaningfully improve early detection of commodity price shocks, offering a practical tool for economic planning and risk mitigation in volatile market environments while saving the very high costs of operating a full generative AI agents pipeline.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Streetscapes Still Matter for Customer Ratings of Eating and Drinking Establishments in Car-Dependent Cities?</title>
<link>https://arxiv.org/abs/2508.06513</link>
<guid>https://arxiv.org/abs/2508.06513</guid>
<content:encoded><![CDATA[
arXiv:2508.06513v1 Announce Type: cross 
Abstract: This study examines how indoor and outdoor aesthetics, streetscapes, and neighborhood features shape customer satisfaction at eating and dining establishments (EDEs) across different urban contexts, varying in car dependency, in Washington, DC. Using review photos and street view images, computer vision models quantified perceived safety and visual appeal. Ordinal logistic regression analyzed their effects on Yelp ratings. Findings reveal that both indoor and outdoor environments significantly impact EDE ratings, while streetscape quality's influence diminishes in car-dependent areas. The study highlights the need for context-sensitive planning that integrates indoor and outdoor factors to enhance customer experiences in diverse settings.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models</title>
<link>https://arxiv.org/abs/2508.06524</link>
<guid>https://arxiv.org/abs/2508.06524</guid>
<content:encoded><![CDATA[
arXiv:2508.06524v1 Announce Type: cross 
Abstract: Neural scaling laws have driven the development of increasingly large language models (LLMs) by linking accuracy improvements to growth in parameter count, dataset size, and compute. However, these laws overlook the carbon emissions that scale exponentially with LLM size. This paper presents \textit{CarbonScaling}, an analytical framework that extends neural scaling laws to incorporate both operational and embodied carbon in LLM training. By integrating models for neural scaling, GPU hardware evolution, parallelism optimization, and carbon estimation, \textit{CarbonScaling} quantitatively connects model accuracy to carbon footprint. Results show that while a power-law relationship between accuracy and carbon holds, real-world inefficiencies significantly increase the scaling factor. Hardware technology scaling reduces carbon emissions for small to mid-sized models, but offers diminishing returns for extremely large LLMs due to communication overhead and underutilized GPUs. Training optimizations-especially aggressive critical batch size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers key insights for training more sustainable and carbon-efficient LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.06529</link>
<guid>https://arxiv.org/abs/2508.06529</guid>
<content:encoded><![CDATA[
arXiv:2508.06529v1 Announce Type: cross 
Abstract: Autonomous driving systems rely on panoptic driving perception that requires both precision and real-time performance. In this work, we propose RMT-PPAD, a real-time, transformer-based multi-task model that jointly performs object detection, drivable area segmentation, and lane line segmentation. We introduce a lightweight module, a gate control with an adapter to adaptively fuse shared and task-specific features, effectively alleviating negative transfer between tasks. Additionally, we design an adaptive segmentation decoder to learn the weights over multi-scale features automatically during the training stage. This avoids the manual design of task-specific structures for different segmentation tasks. We also identify and resolve the inconsistency between training and testing labels in lane line segmentation. This allows fairer evaluation. Experiments on the BDD100K dataset demonstrate that RMT-PPAD achieves state-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object detection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and accuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6 FPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD performance in practice. The results show that RMT-PPAD consistently delivers stable performance. The source codes and pre-trained models are released at https://github.com/JiayuanWang-JW/RMT-PPAD.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Makes "Good" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?</title>
<link>https://arxiv.org/abs/2508.06530</link>
<guid>https://arxiv.org/abs/2508.06530</guid>
<content:encoded><![CDATA[
arXiv:2508.06530v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs), empowered by the success of Large Language Models (LLMs), have achieved impressive performance across domains. Despite the great advances in LVLMs, they still suffer from the unavailable object hallucination issue, which tends to generate objects inconsistent with the image content. The most commonly used Polling-based Object Probing Evaluation (POPE) benchmark evaluates this issue by sampling negative categories according to category-level statistics, \textit{e.g.}, category frequencies and co-occurrence. However, with the continuous advancement of LVLMs, the POPE benchmark has shown diminishing effectiveness in assessing object hallucination, as it employs a simplistic sampling strategy that overlooks image-specific information and restricts distractors to negative object categories only. In this paper, we introduce the Hallucination searching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate the most misleading distractors (\textit{i.e.}, non-existent objects or incorrect image descriptions) that can trigger hallucination in LVLMs, which serves as a means to more rigorously assess their immunity to hallucination. To explore the image-specific information, the content-aware hallucination searching leverages Contrastive Language-Image Pre-Training (CLIP) to approximate the predictive behavior of LVLMs by selecting negative objects with the highest predicted likelihood as distractors. To expand the scope of hallucination assessment, the description-based hallucination searching constructs highly misleading distractors by pairing true objects with false descriptions. Experimental results show that HOPE leads to a precision drop of at least 9\% and up to 23\% across various state-of-the-art LVLMs, significantly outperforming POPE in exposing hallucination vulnerabilities. The code is available at https://github.com/xiemk/HOPE.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification</title>
<link>https://arxiv.org/abs/2508.06535</link>
<guid>https://arxiv.org/abs/2508.06535</guid>
<content:encoded><![CDATA[
arXiv:2508.06535v1 Announce Type: cross 
Abstract: Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral blood smear images is essential for early diagnosis and effective treatment planning. This study investigates the use of transfer learning with pretrained convolutional neural networks (CNNs) to improve diagnostic performance. To address the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL images, we applied extensive data augmentation techniques to create a balanced training set of 10,000 images per class. We evaluated several models, including ResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3 achieved the best results, with an F1-score of 94.30%, accuracy of 92.02%, andAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge. Thesefindings demonstrate the effectiveness of combining data augmentation with advanced transfer learning models, particularly EfficientNet-B3, in developing accurate and robust diagnostic tools for hematologic malignancy detection.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factor Augmented Supervised Learning with Text Embeddings</title>
<link>https://arxiv.org/abs/2508.06548</link>
<guid>https://arxiv.org/abs/2508.06548</guid>
<content:encoded><![CDATA[
arXiv:2508.06548v1 Announce Type: cross 
Abstract: Large language models (LLMs) generate text embeddings from text data, producing vector representations that capture the semantic meaning and contextual relationships of words. However, the high dimensionality of these embeddings often impedes efficiency and drives up computational cost in downstream tasks. To address this, we propose AutoEncoder-Augmented Learning with Text (AEALT), a supervised, factor-augmented framework that incorporates dimension reduction directly into pre-trained LLM workflows. First, we extract embeddings from text documents; next, we pass them through a supervised augmented autoencoder to learn low-dimensional, task-relevant latent factors. By modeling the nonlinear structure of complex embeddings, AEALT outperforms conventional deep-learning approaches that rely on raw embeddings. We validate its broad applicability with extensive experiments on classification, anomaly detection, and prediction tasks using multiple real-world public datasets. Numerical results demonstrate that AEALT yields substantial gains over both vanilla embeddings and several standard dimension reduction methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Bid Shading in Real-Time Bidding Advertising</title>
<link>https://arxiv.org/abs/2508.06550</link>
<guid>https://arxiv.org/abs/2508.06550</guid>
<content:encoded><![CDATA[
arXiv:2508.06550v1 Announce Type: cross 
Abstract: Bid shading plays a crucial role in Real-Time Bidding~(RTB) by adaptively adjusting the bid to avoid advertisers overspending. Existing mainstream two-stage methods, which first model bid landscapes and then optimize surplus using operations research techniques, are constrained by unimodal assumptions that fail to adapt for non-convex surplus curves and are vulnerable to cascading errors in sequential workflows. Additionally, existing discretization models of continuous values ignore the dependence between discrete intervals, reducing the model's error correction ability, while sample selection bias in bidding scenarios presents further challenges for prediction. To address these issues, this paper introduces Generative Bid Shading~(GBS), which comprises two primary components: (1) an end-to-end generative model that utilizes an autoregressive approach to generate shading ratios by stepwise residuals, capturing complex value dependencies without relying on predefined priors; and (2) a reward preference alignment system, which incorporates a channel-aware hierarchical dynamic network~(CHNet) as the reward model to extract fine-grained features, along with modules for surplus optimization and exploration utility reward alignment, ultimately optimizing both short-term and long-term surplus using group relative policy optimization~(GRPO). Extensive experiments on both offline and online A/B tests validate GBS's effectiveness. Moreover, GBS has been deployed on the Meituan DSP platform, serving billions of bid requests daily.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection</title>
<link>https://arxiv.org/abs/2508.06552</link>
<guid>https://arxiv.org/abs/2508.06552</guid>
<content:encoded><![CDATA[
arXiv:2508.06552v1 Announce Type: cross 
Abstract: The challenges associated with deepfake detection are increasing significantly with the latest advancements in technology and the growing popularity of deepfake videos and images. Despite the presence of numerous detection models, demographic bias in the deepfake dataset remains largely unaddressed. This paper focuses on the mitigation of age-specific bias in the deepfake dataset by introducing an age-diverse deepfake dataset that will improve fairness across age groups. The dataset is constructed through a modular pipeline incorporating the existing deepfake datasets Celeb-DF, FaceForensics++, and UTKFace datasets, and the creation of synthetic data to fill the age distribution gaps. The effectiveness and generalizability of this dataset are evaluated using three deepfake detection models: XceptionNet, EfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and EER, revealed that models trained on the age-diverse dataset demonstrated fairer performance across age groups, improved overall accuracy, and higher generalization across datasets. This study contributes a reproducible, fairness-aware deepfake dataset and model pipeline that can serve as a foundation for future research in fairer deepfake detection. The complete dataset and implementation code are available at https://github.com/unishajoshi/age-diverse-deepfake-detection.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets</title>
<link>https://arxiv.org/abs/2508.06556</link>
<guid>https://arxiv.org/abs/2508.06556</guid>
<content:encoded><![CDATA[
arXiv:2508.06556v1 Announce Type: cross 
Abstract: Object detection has advanced rapidly in recent years, driven by increasingly large and diverse datasets. However, label errors, defined as missing labels, incorrect classification or inaccurate localization, often compromise the quality of these datasets. This can have a significant impact on the outcomes of training and benchmark evaluations. Although several methods now exist for detecting label errors in object detection datasets, they are typically validated only on synthetic benchmarks or limited manual inspection. How to correct such errors systemically and at scale therefore remains an open problem. We introduce a semi-automated framework for label-error correction called REC$\checkmark$D (Rechecked). Building on existing detectors, the framework pairs their error proposals with lightweight, crowd-sourced microtasks. These tasks enable multiple annotators to independently verify each candidate bounding box, and their responses are aggregated to estimate ambiguity and improve label quality. To demonstrate the effectiveness of REC$\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our crowdsourced review yields high-quality corrected annotations, which indicate a rate of at least 24% of missing and inaccurate annotations in original annotations. This validated set will be released as a new real-world benchmark for label error detection and correction. We show that current label error detection methods, when combined with our correction framework, can recover hundreds of errors in the time it would take a human to annotate bounding boxes from scratch. However, even the best methods still miss up to 66% of the true errors and with low quality labels introduce more errors than they find. This highlights the urgent need for further research, now enabled by our released benchmark.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Learning Co-Design for Differentially Private Over-the-Air Federated Distillation</title>
<link>https://arxiv.org/abs/2508.06557</link>
<guid>https://arxiv.org/abs/2508.06557</guid>
<content:encoded><![CDATA[
arXiv:2508.06557v1 Announce Type: cross 
Abstract: The ever-growing learning model size nowadays challenges the communication efficiency and privacy preservation of the traditional federated learning (FL). In this paper, we propose a novel differentially private (DP) over-the-air federated distillation (FD) framework, where wireless devices (WDs) periodically share noise-perturbed model outputs with the parameter server by harnessing the superposition property of multi-access channels. Accordingly, over-the-air FD enables the shared responsibility of the DP preservation on the low-dimensional disclosed signals among WDs. We study the communication-learning co-design problem in differentially private over-the-air FD, aiming to maximize the learning convergence rate while meeting the transmit power and DP requirements of WDs. The main challenge is rooted in the intractable learning and privacy analysis in over-the-air FD, together with the strong coupling among the decision variables spanning two timescales. To tackle this problem, we first derive the analytical learning convergence rate and privacy losses of WDs, based on which the optimal transceiver design per FD round and long-term training rounds decision are obtained in the closed forms. Numerical results demonstrate that the proposed differentially private over-the-air FD approach achieves a better learning-privacy trade-off with largely-reduced communication overhead than the conventional FL benchmarks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications</title>
<link>https://arxiv.org/abs/2508.06558</link>
<guid>https://arxiv.org/abs/2508.06558</guid>
<content:encoded><![CDATA[
arXiv:2508.06558v1 Announce Type: cross 
Abstract: Deploying deep learning models in clinical practice often requires leveraging multiple data modalities, such as images, text, and structured data, to achieve robust and trustworthy decisions. However, not all modalities are always available at inference time. In this work, we propose multimodal privileged knowledge distillation (MMPKD), a training strategy that utilizes additional modalities available solely during training to guide a unimodal vision model. Specifically, we used a text-based teacher model for chest radiographs (MIMIC-CXR) and a tabular metadata-based teacher model for mammography (CBIS-DDSM) to distill knowledge into a vision transformer student model. We show that MMPKD can improve the resulting attention maps' zero-shot capabilities of localizing ROI in input images, while this effect does not generalize across domains, as contrarily suggested by prior research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization</title>
<link>https://arxiv.org/abs/2508.06559</link>
<guid>https://arxiv.org/abs/2508.06559</guid>
<content:encoded><![CDATA[
arXiv:2508.06559v1 Announce Type: cross 
Abstract: Pasur is a fishing card game played over six rounds and is played similarly to games such as Cassino and Scopa, and Bastra. This paper introduces a CUDA-accelerated computational framework for simulating Pasur, emphasizing efficient memory management. We use our framework to compute near-Nash equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the large size of its game tree. We handle rule complexity using PyTorch CUDA tensors and to address the memory-intensive nature of the game, we decompose the game tree into two key components: (1) actual game states, and (2) inherited scores from previous rounds. We construct the Full Game Tree by pairing card states with accumulated scores in the Unfolding Process. This design reduces memory overhead by storing only essential strategy values and node connections. To further manage computational complexity, we apply a round-by-round backward training strategy, starting from the final round and recursively propagating average utilities to earlier stages. Our approach constructs the complete game tree, which on average consists of over $10^9$ nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model to predict these strategies for use during gameplay. We then estimate the fair value of each deck through large-scale self-play between equilibrium strategies by simulating, for instance, 10,000 games per matchup, executed in parallel using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms where the action tree naturally decomposes into multiple rounds such as turn-based strategy games or sequential trading decisions in financial markets.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2508.06565</link>
<guid>https://arxiv.org/abs/2508.06565</guid>
<content:encoded><![CDATA[
arXiv:2508.06565v1 Announce Type: cross 
Abstract: Integrating brain imaging data with clinical reports offers a valuable opportunity to leverage complementary multimodal information for more effective and timely diagnosis in practical clinical settings. This approach has gained significant attention in brain disorder research, yet a key challenge remains: how to effectively link objective imaging data with subjective text-based reports, such as doctors' notes. In this work, we propose a novel framework that aligns brain connectomes with clinical reports in a shared cross-modal latent space at both the subject and connectome levels, thereby enhancing representation learning. The key innovation of our approach is that we treat brain subnetworks as tokens of imaging data, rather than raw image patches, to align with word tokens in clinical reports. This enables a more efficient identification of system-level associations between neuroimaging findings and clinical observations, which is critical since brain disorders often manifest as network-level abnormalities rather than isolated regional alterations. We applied our method to mild cognitive impairment (MCI) using the ADNI dataset. Our approach not only achieves state-of-the-art predictive performance but also identifies clinically meaningful connectome-text pairs, offering new insights into the early mechanisms of Alzheimer's disease and supporting the development of clinically useful multimodal biomarkers.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos</title>
<link>https://arxiv.org/abs/2508.06570</link>
<guid>https://arxiv.org/abs/2508.06570</guid>
<content:encoded><![CDATA[
arXiv:2508.06570v1 Announce Type: cross 
Abstract: The existing research has primarily focused on text and image-based hate speech detection, video-based approaches remain underexplored. In this work, we introduce a novel dataset, ImpliHateVid, specifically curated for implicit hate speech detection in videos. ImpliHateVid consists of 2,009 videos comprising 509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos, making it one of the first large-scale video datasets dedicated to implicit hate detection. We also propose a novel two-stage contrastive learning framework for hate speech detection in videos. In the first stage, we train modality-specific encoders for audio, text, and image using contrastive loss by concatenating features from the three encoders. In the second stage, we train cross-encoders using contrastive learning to refine multimodal representations. Additionally, we incorporate sentiment, emotion, and caption-based features to enhance implicit hate detection. We evaluate our method on two datasets, ImpliHateVid for implicit hate speech detection and another dataset for general hate speech detection in videos, HateMM dataset, demonstrating the effectiveness of the proposed multimodal contrastive learning for hateful content detection in videos and the significance of our dataset.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Unlearning Without an Expert Curated Dataset</title>
<link>https://arxiv.org/abs/2508.06595</link>
<guid>https://arxiv.org/abs/2508.06595</guid>
<content:encoded><![CDATA[
arXiv:2508.06595v1 Announce Type: cross 
Abstract: Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Self-Driving Labs</title>
<link>https://arxiv.org/abs/2508.06642</link>
<guid>https://arxiv.org/abs/2508.06642</guid>
<content:encoded><![CDATA[
arXiv:2508.06642v1 Announce Type: cross 
Abstract: A key goal of modern materials science is accelerating the pace of materials discovery. Self-driving labs, or systems that select experiments using machine learning and then execute them using automation, are designed to fulfil this promise by performing experiments faster, more intelligently, more reliably, and with richer metadata than conventional means. This review summarizes progress in understanding the degree to which SDLs accelerate learning by quantifying how much they reduce the number of experiments required for a given goal. The review begins by summarizing the theory underlying two key metrics, namely acceleration factor AF and enhancement factor EF, which quantify how much faster and better an algorithm is relative to a reference strategy. Next, we provide a comprehensive review of the literature, which reveals a wide range of AFs with a median of 6, and that tends to increase with the dimensionality of the space, reflecting an interesting blessing of dimensionality. In contrast, reported EF values vary by over two orders of magnitude, although they consistently peak at 10-20 experiments per dimension. To understand these results, we perform a series of simulated Bayesian optimization campaigns that reveal how EF depends upon the statistical properties of the parameter space while AF depends on its complexity. Collectively, these results reinforce the motivation for using SDLs by revealing their value across a wide range of material parameter spaces and provide a common language for quantifying and understanding this acceleration.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Online Learning for Heterogeneous Multisource Streaming Data</title>
<link>https://arxiv.org/abs/2508.06652</link>
<guid>https://arxiv.org/abs/2508.06652</guid>
<content:encoded><![CDATA[
arXiv:2508.06652v1 Announce Type: cross 
Abstract: Federated learning has emerged as an essential paradigm for distributed multi-source data analysis under privacy concerns. Most existing federated learning methods focus on the ``static" datasets. However, in many real-world applications, data arrive continuously over time, forming streaming datasets. This introduces additional challenges for data storage and algorithm design, particularly under high-dimensional settings. In this paper, we propose a federated online learning (FOL) method for distributed multi-source streaming data analysis. To account for heterogeneity, a personalized model is constructed for each data source, and a novel ``subgroup" assumption is employed to capture potential similarities, thereby enhancing model performance. We adopt the penalized renewable estimation method and the efficient proximal gradient descent for model training. The proposed method aligns with both federated and online learning frameworks: raw data are not exchanged among sources, ensuring data privacy, and only summary statistics of previous data batches are required for model updates, significantly reducing storage demands. Theoretically, we establish the consistency properties for model estimation, variable selection, and subgroup structure recovery, demonstrating optimal statistical efficiency. Simulations illustrate the effectiveness of the proposed method. Furthermore, when applied to the financial lending data and the web log data, the proposed method also exhibits advantageous prediction performance. Results of the analysis also provide some practical insights.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machines Learn Number Fields, But How? The Case of Galois Groups</title>
<link>https://arxiv.org/abs/2508.06670</link>
<guid>https://arxiv.org/abs/2508.06670</guid>
<content:encoded><![CDATA[
arXiv:2508.06670v1 Announce Type: cross 
Abstract: By applying interpretable machine learning methods such as decision trees, we study how simple models can classify the Galois groups of Galois extensions over $\mathbb{Q}$ of degrees 4, 6, 8, 9, and 10, using Dedekind zeta coefficients. Our interpretation of the machine learning results allows us to understand how the distribution of zeta coefficients depends on the Galois group, and to prove new criteria for classifying the Galois groups of these extensions. Combined with previous results, this work provides another example of a new paradigm in mathematical research driven by machine learning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role of Large Language Models and Retrieval-Augmented Generation for Accelerating Crystalline Material Discovery: A Systematic Review</title>
<link>https://arxiv.org/abs/2508.06691</link>
<guid>https://arxiv.org/abs/2508.06691</guid>
<content:encoded><![CDATA[
arXiv:2508.06691v1 Announce Type: cross 
Abstract: Large language models (LLMs) have emerged as powerful tools for knowledge-intensive tasks across domains. In materials science, to find novel materials for various energy efficient devices for various real-world applications, requires several time and cost expensive simulations and experiments. In order to tune down the uncharted material search space, minimizing the experimental cost, LLMs can play a bigger role to first provide an accelerated search of promising known material candidates. Furthermore, the integration of LLMs with domain-specific information via retrieval-augmented generation (RAG) is poised to revolutionize how researchers predict materials structures, analyze defects, discover novel compounds, and extract knowledge from literature and databases. In motivation to the potentials of LLMs and RAG in accelerating material discovery, this paper presents a broad and systematic review to examine the recent advancements in applying LLMs and RAG to key materials science problems. We survey state-of-the-art developments in crystal structure prediction, defect analysis, materials discovery, literature mining, database integration, and multi-modal retrieval, highlighting how combining LLMs with external knowledge sources enables new capabilities. We discuss the performance, limitations, and implications of these approaches, and outline future directions for leveraging LLMs to accelerate materials research and discovery for advancement in technologies in the area of electronics, optics, biomedical, and energy storage.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tight Lower Bound for the Approximation Guarantee of Higher-Order Singular Value Decomposition</title>
<link>https://arxiv.org/abs/2508.06693</link>
<guid>https://arxiv.org/abs/2508.06693</guid>
<content:encoded><![CDATA[
arXiv:2508.06693v1 Announce Type: cross 
Abstract: We prove that the classic approximation guarantee for the higher-order singular value decomposition (HOSVD) is tight by constructing a tensor for which HOSVD achieves an approximation ratio of $N/(1+\varepsilon)$, for any $\varepsilon > 0$. This matches the upper bound of De Lathauwer et al. (2000a) and shows that the approximation ratio of HOSVD cannot be improved. Using a more advanced construction, we also prove that the approximation guarantees for the ST-HOSVD algorithm of Vannieuwenhoven et al. (2012) and higher-order orthogonal iteration (HOOI) of De Lathauwer et al. (2000b) are tight by showing that they can achieve their worst-case approximation ratio of $N / (1 + \varepsilon)$, for any $\varepsilon > 0$.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMFformer: Multimodal Fusion Transformer Network for Depression Detection</title>
<link>https://arxiv.org/abs/2508.06701</link>
<guid>https://arxiv.org/abs/2508.06701</guid>
<content:encoded><![CDATA[
arXiv:2508.06701v1 Announce Type: cross 
Abstract: Depression is a serious mental health illness that significantly affects an individual's well-being and quality of life, making early detection crucial for adequate care and treatment. Detecting depression is often difficult, as it is based primarily on subjective evaluations during clinical interviews. Hence, the early diagnosis of depression, thanks to the content of social networks, has become a prominent research area. The extensive and diverse nature of user-generated information poses a significant challenge, limiting the accurate extraction of relevant temporal information and the effective fusion of data across multiple modalities. This paper introduces MMFformer, a multimodal depression detection network designed to retrieve depressive spatio-temporal high-level patterns from multimodal social media information. The transformer network with residual connections captures spatial features from videos, and a transformer encoder is exploited to design important temporal dynamics in audio. Moreover, the fusion architecture fused the extracted features through late and intermediate fusion strategies to find out the most relevant intermodal correlations among them. Finally, the proposed network is assessed on two large-scale depression detection datasets, and the results clearly reveal that it surpasses existing state-of-the-art approaches, improving the F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is made available publicly at https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning</title>
<link>https://arxiv.org/abs/2508.06716</link>
<guid>https://arxiv.org/abs/2508.06716</guid>
<content:encoded><![CDATA[
arXiv:2508.06716v1 Announce Type: cross 
Abstract: Differentiable inductive logic programming (ILP) techniques have proven effective at finding approximate rule-based solutions to link prediction and node classification problems on knowledge graphs; however, the common assumption of chain-like rule structure can hamper the performance and interpretability of existing approaches. We introduce GLIDR, a differentiable rule learning method that models the inference of logic rules with more expressive syntax than previous methods. GLIDR uses a differentiable message passing inference algorithm that generalizes previous chain-like rule learning methods to allow rules with features like branches and cycles. GLIDR has a simple and expressive rule search space which is parameterized by a limit on the maximum number of free variables that may be included in a rule. Explicit logic rules can be extracted from the weights of a GLIDR model for use with symbolic solvers. We demonstrate that GLIDR can significantly outperform existing rule learning methods on knowledge graph completion tasks and even compete with embedding methods despite the inherent disadvantage of being a structure-only prediction method. We show that rules extracted from GLIDR retain significant predictive performance, and that GLIDR is highly robust to training data noise. Finally, we demonstrate that GLIDR can be chained with deep neural networks and optimized end-to-end for rule learning on arbitrary data modalities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets</title>
<link>https://arxiv.org/abs/2508.06732</link>
<guid>https://arxiv.org/abs/2508.06732</guid>
<content:encoded><![CDATA[
arXiv:2508.06732v1 Announce Type: cross 
Abstract: Ensemble datasets are ever more prevalent in various scientific domains. In climate science, ensemble datasets are used to capture variability in projections under plausible future conditions including greenhouse and aerosol emissions. Each ensemble model run produces projections that are fundamentally similar yet meaningfully distinct. Understanding this variability among ensemble model runs and analyzing its magnitude and patterns is a vital task for climate scientists. In this paper, we present ClimateSOM, a visual analysis workflow that leverages a self-organizing map (SOM) and Large Language Models (LLMs) to support interactive exploration and interpretation of climate ensemble datasets. The workflow abstracts climate ensemble model runs - spatiotemporal time series - into a distribution over a 2D space that captures the variability among the ensemble model runs using a SOM. LLMs are integrated to assist in sensemaking of this SOM-defined 2D space, the basis for the visual analysis tasks. In all, ClimateSOM enables users to explore the variability among ensemble model runs, identify patterns, compare and cluster the ensemble model runs. To demonstrate the utility of ClimateSOM, we apply the workflow to an ensemble dataset of precipitation projections over California and the Northwestern United States. Furthermore, we conduct a short evaluation of our LLM integration, and conduct an expert review of the visual workflow and the insights from the case studies with six domain experts to evaluate our approach and its utility.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Distribution Shift in Graph-Based Android Malware Classification via Function Metadata and LLM Embeddings</title>
<link>https://arxiv.org/abs/2508.06734</link>
<guid>https://arxiv.org/abs/2508.06734</guid>
<content:encoded><![CDATA[
arXiv:2508.06734v1 Announce Type: cross 
Abstract: Graph-based malware classifiers can achieve over 94% accuracy on standard Android datasets, yet we find they suffer accuracy drops of up to 45% when evaluated on previously unseen malware variants from the same family - a scenario where strong generalization would typically be expected. This highlights a key limitation in existing approaches: both the model architectures and their structure-only representations often fail to capture deeper semantic patterns. In this work, we propose a robust semantic enrichment framework that enhances function call graphs with contextual features, including function-level metadata and, when available, code embeddings derived from large language models. The framework is designed to operate under real-world constraints where feature availability is inconsistent, and supports flexible integration of semantic signals. To evaluate generalization under realistic domain and temporal shifts, we introduce two new benchmarks: MalNet-Tiny-Common and MalNet-Tiny-Distinct, constructed using malware family partitioning to simulate cross-family generalization and evolving threat behavior. Experiments across multiple graph neural network backbones show that our method improves classification performance by up to 8% under distribution shift and consistently enhances robustness when integrated with adaptation-based methods. These results offer a practical path toward building resilient malware detection systems in evolving threat environments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search</title>
<link>https://arxiv.org/abs/2508.06736</link>
<guid>https://arxiv.org/abs/2508.06736</guid>
<content:encoded><![CDATA[
arXiv:2508.06736v1 Announce Type: cross 
Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial computational resources due to their combinatorial nature. Parallelization has emerged as a critical strategy to accelerate solution times and enhance scalability to tackle large, complex instances. This paper investigates the parallelization capabilities of Balans, a recently proposed multi-armed bandits-based adaptive large neighborhood search for MIPs. While Balans's modular architecture inherently supports parallel exploration of diverse parameter configurations, this potential has not been thoroughly examined. To address this gap, we introduce ParBalans, an extension that leverages both solver-level and algorithmic-level parallelism to improve performance on challenging MIP instances. Our experimental results demonstrate that ParBalans exhibits competitive performance compared to the state-of-the-art commercial solver Gurobi, particularly on hard optimization benchmarks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Causal Structure Distributions for Robust Planning</title>
<link>https://arxiv.org/abs/2508.06742</link>
<guid>https://arxiv.org/abs/2508.06742</guid>
<content:encoded><![CDATA[
arXiv:2508.06742v1 Announce Type: cross 
Abstract: Structural causal models describe how the components of a robotic system interact. They provide both structural and functional information about the relationships that are present in the system. The structural information outlines the variables among which there is interaction. The functional information describes how such interactions work, via equations or learned models. In this paper we find that learning the functional relationships while accounting for the uncertainty about the structural information leads to more robust dynamics models which improves downstream planning, while using significantly lower computational resources. This in contrast with common model-learning methods that ignore the causal structure and fail to leverage the sparsity of interactions in robotic systems. We achieve this by estimating a causal structure distribution that is used to sample causal graphs that inform the latent-space representations in an encoder-multidecoder probabilistic model. We show that our model can be used to learn the dynamics of a robot, which together with a sampling-based planner can be used to perform new tasks in novel environments, provided an objective function for the new requirement is available. We validate our method using manipulators and mobile robots in both simulation and the real-world. Additionally, we validate the learned dynamics' adaptability and increased robustness to corrupted inputs and changes in the environment, which is highly desirable in challenging real-world robotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Envelope of LLM Inference on AI-PC</title>
<link>https://arxiv.org/abs/2508.06753</link>
<guid>https://arxiv.org/abs/2508.06753</guid>
<content:encoded><![CDATA[
arXiv:2508.06753v1 Announce Type: cross 
Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the perplexity and end-task performance of their full-precision counterparts using the same model size, is ushering in a new era of LLM inference for resource-constrained environments such as edge devices and AI PCs. While these quantization advances promise models that are more cost-effective in terms of latency, memory, throughput, and energy consumption, the computational efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp) used to deploy them remains underexplored. In this work, we take a bottom-up approach: we first design and implement 1-bit and 2-bit microkernels optimized for modern CPUs, achieving peak computational efficiency across a variety of CPU platforms. We integrate these microkernels into a state-of-the-art LLM inference framework, namely PyTorch-TPP, and present end-to-end inference results with 2-bit models that outperform the current SOTA runtime bitnet.cpp by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model inference. Our optimized runtime advances the state of LLM inference on AI PCs and edge devices, paving the way for efficient deployment of ultra-low-bit LLM models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Story Ribbons: Reimagining Storyline Visualizations with Large Language Models</title>
<link>https://arxiv.org/abs/2508.06772</link>
<guid>https://arxiv.org/abs/2508.06772</guid>
<content:encoded><![CDATA[
arXiv:2508.06772v1 Announce Type: cross 
Abstract: Analyzing literature involves tracking interactions between characters, locations, and themes. Visualization has the potential to facilitate the mapping and analysis of these complex relationships, but capturing structured information from unstructured story data remains a challenge. As large language models (LLMs) continue to advance, we see an opportunity to use their text processing and analysis capabilities to augment and reimagine existing storyline visualization techniques. Toward this goal, we introduce an LLM-driven data parsing pipeline that automatically extracts relevant narrative information from novels and scripts. We then apply this pipeline to create Story Ribbons, an interactive visualization system that helps novice and expert literary analysts explore detailed character and theme trajectories at multiple narrative levels. Through pipeline evaluations and user studies with Story Ribbons on 36 literary works, we demonstrate the potential of LLMs to streamline narrative visualization creation and reveal new insights about familiar stories. We also describe current limitations of AI-based systems, and interaction motifs designed to address these issues.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation</title>
<link>https://arxiv.org/abs/2508.06781</link>
<guid>https://arxiv.org/abs/2508.06781</guid>
<content:encoded><![CDATA[
arXiv:2508.06781v1 Announce Type: cross 
Abstract: Neural sentence embedding models for dense retrieval typically rely on binary relevance labels, treating query-document pairs as either relevant or irrelevant. However, real-world relevance often exists on a continuum, and recent advances in large language models (LLMs) have made it feasible to scale the generation of fine-grained graded relevance labels. In this work, we propose BiXSE, a simple and effective pointwise training method that optimizes binary cross-entropy (BCE) over LLM-generated graded relevance scores. BiXSE interprets these scores as probabilistic targets, enabling granular supervision from a single labeled query-document pair per query. Unlike pairwise or listwise losses that require multiple annotated comparisons per query, BiXSE achieves strong performance with reduced annotation and compute costs by leveraging in-batch negatives. Extensive experiments across sentence embedding (MMTEB) and retrieval benchmarks (BEIR, TREC-DL) show that BiXSE consistently outperforms softmax-based contrastive learning (InfoNCE), and matches or exceeds strong pairwise ranking baselines when trained on LLM-supervised data. BiXSE offers a robust, scalable alternative for training dense retrieval models as graded relevance supervision becomes increasingly accessible.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Aware Spiking Graph Neural Network</title>
<link>https://arxiv.org/abs/2508.06793</link>
<guid>https://arxiv.org/abs/2508.06793</guid>
<content:encoded><![CDATA[
arXiv:2508.06793v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have demonstrated impressive capabilities in modeling graph-structured data, while Spiking Neural Networks (SNNs) offer high energy efficiency through sparse, event-driven computation. However, existing spiking GNNs predominantly operate in Euclidean space and rely on fixed geometric assumptions, limiting their capacity to model complex graph structures such as hierarchies and cycles. To overcome these limitations, we propose \method{}, a novel Geometry-Aware Spiking Graph Neural Network that unifies spike-based neural dynamics with adaptive representation learning on Riemannian manifolds. \method{} features three key components: a Riemannian Embedding Layer that projects node features into a pool of constant-curvature manifolds, capturing non-Euclidean structures; a Manifold Spiking Layer that models membrane potential evolution and spiking behavior in curved spaces via geometry-consistent neighbor aggregation and curvature-based attention; and a Manifold Learning Objective that enables instance-wise geometry adaptation through jointly optimized classification and link prediction losses defined over geodesic distances. All modules are trained using Riemannian SGD, eliminating the need for backpropagation through time. Extensive experiments on multiple benchmarks show that GSG achieves superior accuracy, robustness, and energy efficiency compared to both Euclidean SNNs and manifold-based GNNs, establishing a new paradigm for curvature-aware, energy-efficient graph learning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anatomy of a Machine Learning Ecosystem: 2 Million Models on Hugging Face</title>
<link>https://arxiv.org/abs/2508.06811</link>
<guid>https://arxiv.org/abs/2508.06811</guid>
<content:encoded><![CDATA[
arXiv:2508.06811v1 Announce Type: cross 
Abstract: Many have observed that the development and deployment of generative machine learning (ML) and artificial intelligence (AI) models follow a distinctive pattern in which pre-trained models are adapted and fine-tuned for specific downstream tasks. However, there is limited empirical work that examines the structure of these interactions. This paper analyzes 1.86 million models on Hugging Face, a leading peer production platform for model development. Our study of model family trees -- networks that connect fine-tuned models to their base or parent -- reveals sprawling fine-tuning lineages that vary widely in size and structure. Using an evolutionary biology lens to study ML models, we use model metadata and model cards to measure the genetic similarity and mutation of traits over model families. We find that models tend to exhibit a family resemblance, meaning their genetic markers and traits exhibit more overlap when they belong to the same model family. However, these similarities depart in certain ways from standard models of asexual reproduction, because mutations are fast and directed, such that two `sibling' models tend to exhibit more similarity than parent/child pairs. Further analysis of the directional drifts of these mutations reveals qualitative insights about the open machine learning ecosystem: Licenses counter-intuitively drift from restrictive, commercial licenses towards permissive or copyleft licenses, often in violation of upstream license's terms; models evolve from multi-lingual compatibility towards english-only compatibility; and model cards reduce in length and standardize by turning, more often, to templates and automatically generated text. Overall, this work takes a step toward an empirically grounded understanding of model fine-tuning and suggests that ecological models and methods can yield novel scientific insights.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions</title>
<link>https://arxiv.org/abs/2508.06834</link>
<guid>https://arxiv.org/abs/2508.06834</guid>
<content:encoded><![CDATA[
arXiv:2508.06834v1 Announce Type: cross 
Abstract: We propose a novel framework for adaptively learning the time-evolving solutions of stochastic partial differential equations (SPDEs) using score-based diffusion models within a recursive Bayesian inference setting. SPDEs play a central role in modeling complex physical systems under uncertainty, but their numerical solutions often suffer from model errors and reduced accuracy due to incomplete physical knowledge and environmental variability. To address these challenges, we encode the governing physics into the score function of a diffusion model using simulation data and incorporate observational information via a likelihood-based correction in a reverse-time stochastic differential equation. This enables adaptive learning through iterative refinement of the solution as new data becomes available. To improve computational efficiency in high-dimensional settings, we introduce the ensemble score filter, a training-free approximation of the score function designed for real-time inference. Numerical experiments on benchmark SPDEs demonstrate the accuracy and robustness of the proposed method under sparse and noisy observations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOCA-HESP: Meta High-dimensional Bayesian Optimization for Combinatorial and Mixed Spaces via Hyper-ellipsoid Partitioning</title>
<link>https://arxiv.org/abs/2508.06847</link>
<guid>https://arxiv.org/abs/2508.06847</guid>
<content:encoded><![CDATA[
arXiv:2508.06847v1 Announce Type: cross 
Abstract: High-dimensional Bayesian Optimization (BO) has attracted significant attention in recent research. However, existing methods have mainly focused on optimizing in continuous domains, while combinatorial (ordinal and categorical) and mixed domains still remain challenging. In this paper, we first propose MOCA-HESP, a novel high-dimensional BO method for combinatorial and mixed variables. The key idea is to leverage the hyper-ellipsoid space partitioning (HESP) technique with different categorical encoders to work with high-dimensional, combinatorial and mixed spaces, while adaptively selecting the optimal encoders for HESP using a multi-armed bandit technique. Our method, MOCA-HESP, is designed as a \textit{meta-algorithm} such that it can incorporate other combinatorial and mixed BO optimizers to further enhance the optimizers' performance. Finally, we develop three practical BO methods by integrating MOCA-HESP with state-of-the-art BO optimizers for combinatorial and mixed variables: standard BO, CASMOPOLITAN, and Bounce. Our experimental results on various synthetic and real-world benchmarks show that our methods outperform existing baselines. Our code implementation can be found at https://github.com/LamNgo1/moca-hesp
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2508.06863</link>
<guid>https://arxiv.org/abs/2508.06863</guid>
<content:encoded><![CDATA[
arXiv:2508.06863v1 Announce Type: cross 
Abstract: Unmanned aerial vehicles (UAVs) have been recently utilized in multi-access edge computing (MEC) as edge servers. It is desirable to design UAVs' trajectories and user to UAV assignments to ensure satisfactory service to the users and energy efficient operation simultaneously. The posed optimization problem is challenging to solve because: (i) The formulated problem is non-convex, (ii) Due to the mobility of ground users, their future positions and channel gains are not known in advance, (iii) Local UAVs' observations should be communicated to a central entity that solves the optimization problem. The (semi-) centralized processing leads to communication overhead, communication/processing bottlenecks, lack of flexibility and scalability, and loss of robustness to system failures. To simultaneously address all these limitations, we advocate a fully decentralized setup with no centralized entity. Each UAV obtains its local observation and then communicates with its immediate neighbors only. After sharing information with neighbors, each UAV determines its next position via a locally run deep reinforcement learning (DRL) algorithm. None of the UAVs need to know the global communication graph. Two main components of our proposed solution are (i) Graph attention layers (GAT), and (ii) Experience and parameter sharing proximal policy optimization (EPS-PPO). Our proposed approach eliminates all the limitations of semi-centralized MADRL methods such as MAPPO and MA deep deterministic policy gradient (MADDPG), while guaranteeing a better performance than independent local DRLs such as in IPPO. Numerical results reveal notable performance gains in several different criteria compared to the existing MADDPG algorithm, demonstrating the potential for offering a better performance, while utilizing local communications only.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text to Speech System for Meitei Mayek Script</title>
<link>https://arxiv.org/abs/2508.06870</link>
<guid>https://arxiv.org/abs/2508.06870</guid>
<content:encoded><![CDATA[
arXiv:2508.06870v1 Announce Type: cross 
Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the Manipuri language
  using the Meitei Mayek script. Leveraging Tacotron 2 and HiFi-GAN, we introduce a neural TTS
  architecture adapted to support tonal phonology and under-resourced linguistic environments. We
  develop a phoneme mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and
  demonstrate intelligible and natural speech synthesis, validated through subjective and objective
  metrics. This system lays the groundwork for linguistic preservation and technological inclusion of
  Manipuri.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Optimal Convergence of Accelerated Gradient Methods under Generalized and $(L_0, L_1)$-Smoothness</title>
<link>https://arxiv.org/abs/2508.06884</link>
<guid>https://arxiv.org/abs/2508.06884</guid>
<content:encoded><![CDATA[
arXiv:2508.06884v1 Announce Type: cross 
Abstract: We study first-order methods for convex optimization problems with functions $f$ satisfying the recently proposed $\ell$-smoothness condition $||\nabla^{2}f(x)|| \le \ell\left(||\nabla f(x)||\right),$ which generalizes the $L$-smoothness and $(L_{0},L_{1})$-smoothness. While accelerated gradient descent AGD is known to reach the optimal complexity $O(\sqrt{L} R / \sqrt{\varepsilon})$ under $L$-smoothness, where $\varepsilon$ is an error tolerance and $R$ is the distance between a starting and an optimal point, existing extensions to $\ell$-smoothness either incur extra dependence on the initial gradient, suffer exponential factors in $L_{1} R$, or require costly auxiliary sub-routines, leaving open whether an AGD-type $O(\sqrt{\ell(0)} R / \sqrt{\varepsilon})$ rate is possible for small-$\varepsilon$, even in the $(L_{0},L_{1})$-smoothness case.
  We resolve this open question. Leveraging a new Lyapunov function and designing new algorithms, we achieve $O(\sqrt{\ell(0)} R / \sqrt{\varepsilon})$ oracle complexity for small-$\varepsilon$ and virtually any $\ell$. For instance, for $(L_{0},L_{1})$-smoothness, our bound $O(\sqrt{L_0} R / \sqrt{\varepsilon})$ is provably optimal in the small-$\varepsilon$ regime and removes all non-constant multiplicative factors present in prior accelerated algorithms.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushdown Reward Machines for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.06894</link>
<guid>https://arxiv.org/abs/2508.06894</guid>
<content:encoded><![CDATA[
arXiv:2508.06894v1 Announce Type: cross 
Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian) reward functions for reinforcement learning (RL). RMs can reward any behaviour representable in regular languages and, when paired with RL algorithms that exploit RM structure, have been shown to significantly improve sample efficiency in many domains. In this work, we present pushdown reward machines (pdRMs), an extension of reward machines based on deterministic pushdown automata. pdRMs can recognize and reward temporally extended behaviours representable in deterministic context-free languages, making them more expressive than reward machines. We introduce two variants of pdRM-based policies, one which has access to the entire stack of the pdRM, and one which can only access the top $k$ symbols (for a given constant $k$) of the stack. We propose a procedure to check when the two kinds of policies (for a given environment, pdRM, and constant $k$) achieve the same optimal expected reward. We then provide theoretical results establishing the expressive power of pdRMs, and space complexity results about the proposed learning problems. Finally, we provide experimental results showing how agents can be trained to perform tasks representable in deterministic context-free languages using pdRMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization</title>
<link>https://arxiv.org/abs/2508.06906</link>
<guid>https://arxiv.org/abs/2508.06906</guid>
<content:encoded><![CDATA[
arXiv:2508.06906v1 Announce Type: cross 
Abstract: Integer and mixed-integer nonlinear programming (INLP, MINLP) are central to logistics, energy, and scheduling, but remain computationally challenging. This survey examines how machine learning and reinforcement learning can enhance exact optimization methods - particularly branch-and-bound (BB), without compromising global optimality. We cover discrete, continuous, and mixed-integer formulations, and highlight applications such as crew scheduling, vehicle routing, and hydropower planning. We introduce a unified BB framework that embeds learning-based strategies into branching, cut selection, node ordering, and parameter control. Classical algorithms are augmented using supervised, imitation, and reinforcement learning models to accelerate convergence while maintaining correctness. We conclude with a taxonomy of learning methods by solver class and learning paradigm, and outline open challenges in generalization, hybridization, and scaling intelligent solvers.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection</title>
<link>https://arxiv.org/abs/2508.06913</link>
<guid>https://arxiv.org/abs/2508.06913</guid>
<content:encoded><![CDATA[
arXiv:2508.06913v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) has resulted in increasingly sophisticated AI-generated content, posing significant challenges in distinguishing LLM-generated text from human-written language. Existing detection methods, primarily based on lexical heuristics or fine-tuned classifiers, often suffer from limited generalizability and are vulnerable to paraphrasing, adversarial perturbations, and cross-domain shifts. In this work, we propose SentiDetect, a model-agnostic framework for detecting LLM-generated text by analyzing the divergence in sentiment distribution stability. Our method is motivated by the empirical observation that LLM outputs tend to exhibit emotionally consistent patterns, whereas human-written texts display greater emotional variability. To capture this phenomenon, we define two complementary metrics: sentiment distribution consistency and sentiment distribution preservation, which quantify stability under sentiment-altering and semantic-preserving transformations. We evaluate SentiDetect on five diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro, Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its superiority over state-of-the-art baselines, with over 16% and 11% F1 score improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover, SentiDetect also shows greater robustness to paraphrasing, adversarial attacks, and text length variations, outperforming existing detectors in challenging scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Formalization via Conceptual Retrieval-Augmented LLMs</title>
<link>https://arxiv.org/abs/2508.06931</link>
<guid>https://arxiv.org/abs/2508.06931</guid>
<content:encoded><![CDATA[
arXiv:2508.06931v1 Announce Type: cross 
Abstract: Interactive theorem provers (ITPs) require manual formalization, which is labor-intensive and demands expert knowledge. While automated formalization offers a potential solution, it faces two major challenges: model hallucination (e.g., undefined predicates, symbol misuse, and version incompatibility) and the semantic gap caused by ambiguous or missing premises in natural language descriptions. To address these issues, we propose CRAMF, a Concept-driven Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances LLM-based autoformalization by retrieving formal definitions of core mathematical concepts, providing contextual grounding during code generation. However, applying retrieval-augmented generation (RAG) in this setting is non-trivial due to the lack of structured knowledge bases, the polymorphic nature of mathematical concepts, and the high precision required in formal retrieval. We introduce a framework for automatically constructing a concept-definition knowledge base from Mathlib4, the standard mathematical library for the Lean 4 theorem prover, indexing over 26,000 formal definitions and 1,000+ core mathematical concepts. To address conceptual polymorphism, we propose contextual query augmentation with domain- and application-level signals. In addition, we design a dual-channel hybrid retrieval strategy with reranking to ensure accurate and relevant definition retrieval. Experiments on miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding consistent improvements in translation accuracy, achieving up to 62.1% and an average of 29.9% relative improvement.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction</title>
<link>https://arxiv.org/abs/2508.06939</link>
<guid>https://arxiv.org/abs/2508.06939</guid>
<content:encoded><![CDATA[
arXiv:2508.06939v1 Announce Type: cross 
Abstract: Multimodal learning enables various machine learning tasks to benefit from diverse data sources, effectively mimicking the interplay of different factors in real-world applications, particularly in agriculture. While the heterogeneous nature of involved data modalities may necessitate the design of complex architectures, the model interpretability is often overlooked. In this study, we leverage the intrinsic explainability of Transformer-based models to explain multimodal learning networks, focusing on the task of crop yield prediction at the subfield level. The large datasets used cover various crops, regions, and years, and include four different input modalities: multispectral satellite and weather time series, terrain elevation maps and soil properties. Based on the self-attention mechanism, we estimate feature attributions using two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and evaluate their performance against Shapley-based model-agnostic estimations, Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality Activation (WMA) method to assess modality attributions and compare it with SVS attributions. Our findings indicate that Transformer-based models outperform other architectures, specifically convolutional and recurrent networks, achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field levels, respectively. AR is shown to provide more robust and reliable temporal attributions, as confirmed through qualitative and quantitative evaluation, compared to GA and SVS values. Information about crop phenology stages was leveraged to interpret the explanation results in the light of established agronomic knowledge. Furthermore, modality attributions revealed varying patterns across the two methods compared.[...]
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Beam Field for Spatial Beam RSRP Prediction</title>
<link>https://arxiv.org/abs/2508.06956</link>
<guid>https://arxiv.org/abs/2508.06956</guid>
<content:encoded><![CDATA[
arXiv:2508.06956v1 Announce Type: cross 
Abstract: Accurately predicting beam-level reference signal received power (RSRP) is essential for beam management in dense multi-user wireless networks, yet challenging due to high measurement overhead and fast channel variations. This paper proposes Neural Beam Field (NBF), a hybrid neural-physical framework for efficient and interpretable spatial beam RSRP prediction. Central to our approach is the introduction of the Multi-path Conditional Power Profile (MCPP), which bridges site-specific multipath propagation with antenna/beam configurations via closed-form analytical modeling. We adopt a decoupled ``blackbox-whitebox" design: a Transformer-based deep neural network (DNN) learns the MCPP from sparse user measurements and positions, while a physics-inspired module analytically infers beam RSRP statistics. To improve convergence and adaptivity, we further introduce a Pretrain-and-Calibrate (PaC) strategy that leverages ray-tracing priors and on-site calibration using RSRP data. Extensive simulations results demonstrate that NBF significantly outperforms conventional table-based channel knowledge maps (CKMs) and pure blackbox DNNs in prediction accuracy, training efficiency, and generalization, while maintaining a compact model size. The proposed framework offers a scalable and physically grounded solution for intelligent beam management in next-generation dense wireless networks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair</title>
<link>https://arxiv.org/abs/2508.06963</link>
<guid>https://arxiv.org/abs/2508.06963</guid>
<content:encoded><![CDATA[
arXiv:2508.06963v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness issues, motivating developers to seek automated and flexible repair methods that enable convenient deployment across diverse scenarios. Existing repair methods like supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) are costly and slow, while prompt engineering lacks robustness and scalability. Representation engineering, which steers model behavior by injecting targeted concept vectors during inference, offers a lightweight, training-free alternative. However, current approaches depend on manually crafted samples and fixed steering strategies, limiting automation and adaptability. To overcome these challenges, we propose MASteer, the first end-to-end framework for trustworthiness repair in LLMs based on representation engineering. MASteer integrates two core components: AutoTester, a multi-agent system that generates diverse, high-quality steer samples tailored to developer needs; and AutoRepairer, which constructs adaptive steering strategies with anchor vectors for automated, context-aware strategy selection during inference. Experiments on standard and customized trustworthiness tasks show MASteer consistently outperforms baselines, improving metrics by 15.36% on LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model capabilities. MASteer demonstrates strong robustness, generalization, and practical value for scalable, efficient trustworthiness repair.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning</title>
<link>https://arxiv.org/abs/2508.06972</link>
<guid>https://arxiv.org/abs/2508.06972</guid>
<content:encoded><![CDATA[
arXiv:2508.06972v1 Announce Type: cross 
Abstract: DSperse is a modular framework for distributed machine learning inference with strategic cryptographic verification. Operating within the emerging paradigm of distributed zero-knowledge machine learning, DSperse avoids the high cost and rigidity of full-model circuitization by enabling targeted verification of strategically chosen subcomputations. These verifiable segments, or "slices", may cover part or all of the inference pipeline, with global consistency enforced through audit, replication, or economic incentives. This architecture supports a pragmatic form of trust minimization, localizing zero-knowledge proofs to the components where they provide the greatest value. We evaluate DSperse using multiple proving systems and report empirical results on memory usage, runtime, and circuit behavior under sliced and unsliced configurations. By allowing proof boundaries to align flexibly with the model's logical structure, DSperse supports scalable, targeted verification strategies suited to diverse deployment needs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI for Curie Temperature Prediction in Magnetic Materials</title>
<link>https://arxiv.org/abs/2508.06996</link>
<guid>https://arxiv.org/abs/2508.06996</guid>
<content:encoded><![CDATA[
arXiv:2508.06996v1 Announce Type: cross 
Abstract: We explore machine learning techniques for predicting Curie temperatures of magnetic materials using the NEMAD database. By augmenting the dataset with composition-based and domain-aware descriptors, we evaluate the performance of several machine learning models. We find that the Extra Trees Regressor delivers the best performance reaching an R^2 score of up to 0.85 $\pm$ 0.01 (cross-validated) for a balanced dataset. We employ the k-means clustering algorithm to gain insights into the performance of chemically distinct material groups. Furthermore, we perform the SHAP analysis to identify key physicochemical drivers of Curie behavior, such as average atomic number and magnetic moment. By employing explainable AI techniques, this analysis offers insights into the model's predictive behavior, thereby advancing scientific interpretability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization</title>
<link>https://arxiv.org/abs/2508.07001</link>
<guid>https://arxiv.org/abs/2508.07001</guid>
<content:encoded><![CDATA[
arXiv:2508.07001v1 Announce Type: cross 
Abstract: With wireless devices increasingly forming a unified smart network for seamless, user-friendly operations, random access (RA) medium access control (MAC) design is considered a key solution for handling unpredictable data traffic from multiple terminals. However, it remains challenging to design an effective RA-based MAC protocol to minimize collisions and ensure transmission fairness across the devices. While existing multi-agent reinforcement learning (MARL) approaches with centralized training and decentralized execution (CTDE) have been proposed to optimize RA performance, their reliance on centralized training and the significant overhead required for information collection can make real-world applications unrealistic. In this work, we adopt a fully decentralized MARL architecture, where policy learning does not rely on centralized tasks but leverages consensus-based information exchanges across devices. We design our MARL algorithm over an actor-critic (AC) network and propose exchanging only local rewards to minimize communication overhead. Furthermore, we provide a theoretical proof of global convergence for our approach. Numerical experiments show that our proposed MARL algorithm can significantly improve RA network performance compared to other baselines.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems</title>
<link>https://arxiv.org/abs/2508.07009</link>
<guid>https://arxiv.org/abs/2508.07009</guid>
<content:encoded><![CDATA[
arXiv:2508.07009v1 Announce Type: cross 
Abstract: Intelligent Reflecting Surfaces (IRSs) have potential for significant performance gains in next-generation wireless networks but face key challenges, notably severe double-pathloss and complex multi-user scheduling due to hardware constraints. Active IRSs partially address pathloss but still require efficient scheduling in cell-level multi-IRS multi-user systems, whereby the overhead/delay of channel state acquisition and the scheduling complexity both rise dramatically as the user density and channel dimensions increase. Motivated by these challenges, this paper proposes a novel scheduling framework based on neural Channel Knowledge Map (CKM), designing Transformer-based deep neural networks (DNNs) to predict ergodic spectral efficiency (SE) from historical channel/throughput measurements tagged with user positions. Specifically, two cascaded networks, LPS-Net and SE-Net, are designed to predict link power statistics (LPS) and ergodic SE accurately. We further propose a low-complexity Stable Matching-Iterative Balancing (SM-IB) scheduling algorithm. Numerical evaluations verify that the proposed neural CKM significantly enhances prediction accuracy and computational efficiency, while the SM-IB algorithm effectively achieves near-optimal max-min throughput with greatly reduced complexity.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders</title>
<link>https://arxiv.org/abs/2508.07020</link>
<guid>https://arxiv.org/abs/2508.07020</guid>
<content:encoded><![CDATA[
arXiv:2508.07020v1 Announce Type: cross 
Abstract: Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of contiguous spectral bands, enabling fine-grained mapping of soils, crops, and land cover. While self-supervised Masked Autoencoders excel on RGB and low-band multispectral data, they struggle to exploit the intricate spatial-spectral correlations in 200+ band hyperspectral images. We introduce TerraMAE, a novel HSI encoding framework specifically designed to learn highly representative spatial-spectral embeddings for diverse geospatial analyses. TerraMAE features an adaptive channel grouping strategy, based on statistical reflectance properties to capture spectral similarities, and an enhanced reconstruction loss function that incorporates spatial and spectral quality metrics. We demonstrate TerraMAE's effectiveness through superior spatial-spectral information preservation in high-fidelity image reconstruction. Furthermore, we validate its practical utility and the quality of its learned representations through strong performance on three key downstream geospatial tasks: crop identification, land cover classification, and soil texture prediction.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA</title>
<link>https://arxiv.org/abs/2508.07022</link>
<guid>https://arxiv.org/abs/2508.07022</guid>
<content:encoded><![CDATA[
arXiv:2508.07022v1 Announce Type: cross 
Abstract: Knowledge editing (KE) provides a scalable approach for updating factual knowledge in large language models without full retraining. While previous studies have demonstrated effectiveness in general domains and medical QA tasks, little attention has been paid to KE in multimodal medical scenarios. Unlike text-only settings, medical KE demands integrating updated knowledge with visual reasoning to support safe and interpretable clinical decisions. To address this gap, we propose MultiMedEdit, the first benchmark tailored to evaluating KE in clinical multimodal tasks. Our framework spans both understanding and reasoning task types, defines a three-dimensional metric suite (reliability, generality, and locality), and supports cross-paradigm comparisons across general and domain-specific models. We conduct extensive experiments under single-editing and lifelong-editing settings. Results suggest that current methods struggle with generalization and long-tail reasoning, particularly in complex clinical workflows. We further present an efficiency analysis (e.g., edit latency, memory footprint), revealing practical trade-offs in real-world deployment across KE paradigms. Overall, MultiMedEdit not only reveals the limitations of current approaches but also provides a solid foundation for developing clinically robust knowledge editing techniques in the future.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whisfusion: Parallel ASR Decoding via a Diffusion Transformer</title>
<link>https://arxiv.org/abs/2508.07048</link>
<guid>https://arxiv.org/abs/2508.07048</guid>
<content:encoded><![CDATA[
arXiv:2508.07048v1 Announce Type: cross 
Abstract: Fast Automatic Speech Recognition (ASR) is critical for latency-sensitive applications such as real-time captioning and meeting transcription. However, truly parallel ASR decoding remains challenging due to the sequential nature of autoregressive (AR) decoders and the context limitations of non-autoregressive (NAR) methods. While modern ASR encoders can process up to 30 seconds of audio at once, AR decoders still generate tokens sequentially, creating a latency bottleneck. We propose Whisfusion, the first framework to fuse a pre-trained Whisper encoder with a text diffusion decoder. This NAR architecture resolves the AR latency bottleneck by processing the entire acoustic context in parallel at every decoding step. A lightweight cross-attention adapter trained via parameter-efficient fine-tuning (PEFT) bridges the two modalities. We also introduce a batch-parallel, multi-step decoding strategy that improves accuracy by increasing the number of candidates with minimal impact on speed. Fine-tuned solely on LibriSpeech (960h), Whisfusion achieves a lower WER than Whisper-tiny (8.3% vs. 9.7%), and offers comparable latency on short audio. For longer utterances (>20s), it is up to 2.6x faster than the AR baseline, establishing a new, efficient operating point for long-form ASR. The implementation and training scripts are available at https://github.com/taeyoun811/Whisfusion.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference for Autoencoder-based Anomaly Detection after Representation Learning-based Domain Adaptation</title>
<link>https://arxiv.org/abs/2508.07049</link>
<guid>https://arxiv.org/abs/2508.07049</guid>
<content:encoded><![CDATA[
arXiv:2508.07049v1 Announce Type: cross 
Abstract: Anomaly detection (AD) plays a vital role across a wide range of domains, but its performance might deteriorate when applied to target domains with limited data. Domain Adaptation (DA) offers a solution by transferring knowledge from a related source domain with abundant data. However, this adaptation process can introduce additional uncertainty, making it difficult to draw statistically valid conclusions from AD results. In this paper, we propose STAND-DA -- a novel framework for statistically rigorous Autoencoder-based AD after Representation Learning-based DA. Built on the Selective Inference (SI) framework, STAND-DA computes valid $p$-values for detected anomalies and rigorously controls the false positive rate below a pre-specified level $\alpha$ (e.g., 0.05). To address the computational challenges of applying SI to deep learning models, we develop the GPU-accelerated SI implementation, significantly enhancing both scalability and runtime performance. This advancement makes SI practically feasible for modern, large-scale deep architectures. Extensive experiments on synthetic and real-world datasets validate the theoretical results and computational efficiency of the proposed STAND-DA method.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</title>
<link>https://arxiv.org/abs/2508.07050</link>
<guid>https://arxiv.org/abs/2508.07050</guid>
<content:encoded><![CDATA[
arXiv:2508.07050v1 Announce Type: cross 
Abstract: Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are available at https://github.com/8421BCD/ReasonRank.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taking the Garbage Out of Data-Driven Prediction Across Climate Timescales</title>
<link>https://arxiv.org/abs/2508.07062</link>
<guid>https://arxiv.org/abs/2508.07062</guid>
<content:encoded><![CDATA[
arXiv:2508.07062v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) -- and specifically machine learning (ML) -- applications for climate prediction across timescales are proliferating quickly. The emergence of these methods prompts a revisit to the impact of data preprocessing, a topic familiar to the climate community, as more traditional statistical models work with relatively small sample sizes. Indeed, the skill and confidence in the forecasts produced by data-driven models are directly influenced by the quality of the datasets and how they are treated during model development, thus yielding the colloquialism "garbage in, garbage out." As such, this article establishes protocols for the proper preprocessing of input data for AI/ML models designed for climate prediction (i.e., subseasonal to decadal and longer). The three aims are to: (1) educate researchers, developers, and end users on the effects that preprocessing has on climate predictions; (2) provide recommended practices for data preprocessing for such applications; and (3) empower end users to decipher whether the models they are using are properly designed for their objectives. Specific topics covered in this article include the creation of (standardized) anomalies, dealing with non-stationarity and the spatiotemporally correlated nature of climate data, and handling of extreme values and variables with potentially complex distributions. Case studies will illustrate how using different preprocessing techniques can produce different predictions from the same model, which can create confusion and decrease confidence in the overall process. Ultimately, implementing the recommended practices set forth in this article will enhance the robustness and transparency of AI/ML in climate prediction studies.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction of Solar EUV Irradiance Using CaII K Images and SOHO/SEM Data with Bayesian Deep Learning and Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2508.07065</link>
<guid>https://arxiv.org/abs/2508.07065</guid>
<content:encoded><![CDATA[
arXiv:2508.07065v1 Announce Type: cross 
Abstract: Solar extreme ultraviolet (EUV) irradiance plays a crucial role in heating the Earth's ionosphere, thermosphere, and mesosphere, affecting atmospheric dynamics over varying time scales. Although significant effort has been spent studying short-term EUV variations from solar transient events, there is little work to explore the long-term evolution of the EUV flux over multiple solar cycles. Continuous EUV flux measurements have only been available since 1995, leaving significant gaps in earlier data. In this study, we propose a Bayesian deep learning model, named SEMNet, to fill the gaps. We validate our approach by applying SEMNet to construct SOHO/SEM EUV flux measurements in the period between 1998 and 2014 using CaII K images from the Precision Solar Photometric Telescope. We then extend SEMNet through transfer learning to reconstruct solar EUV irradiance in the period between 1950 and 1960 using CaII K images from the Kodaikanal Solar Observatory. Experimental results show that SEMNet provides reliable predictions along with uncertainty bounds, demonstrating the feasibility of CaII K images as a robust proxy for long-term EUV fluxes. These findings contribute to a better understanding of solar influences on Earth's climate over extended periods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attacks with False Discovery Rate Control</title>
<link>https://arxiv.org/abs/2508.07066</link>
<guid>https://arxiv.org/abs/2508.07066</guid>
<content:encoded><![CDATA[
arXiv:2508.07066v1 Announce Type: cross 
Abstract: Recent studies have shown that deep learning models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. To analyze and study these vulnerabilities, various MIA methods have been proposed. Despite the significance and popularity of MIAs, existing works on MIAs are limited in providing guarantees on the false discovery rate (FDR), which refers to the expected proportion of false discoveries among the identified positive discoveries. However, it is very challenging to ensure the false discovery rate guarantees, because the underlying distribution is usually unknown, and the estimated non-member probabilities often exhibit interdependence. To tackle the above challenges, in this paper, we design a novel membership inference attack method, which can provide the guarantees on the false discovery rate. Additionally, we show that our method can also provide the marginal probability guarantee on labeling true non-member data as member data. Notably, our method can work as a wrapper that can be seamlessly integrated with existing MIA methods in a post-hoc manner, while also providing the FDR control. We perform the theoretical analysis for our method. Extensive experiments in various settings (e.g., the black-box setting and the lifelong learning setting) are also conducted to verify the desirable performance of our method.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization</title>
<link>https://arxiv.org/abs/2508.07086</link>
<guid>https://arxiv.org/abs/2508.07086</guid>
<content:encoded><![CDATA[
arXiv:2508.07086v1 Announce Type: cross 
Abstract: Voice anonymization protects speaker privacy by concealing identity while preserving linguistic and paralinguistic content. Self-supervised learning (SSL) representations encode linguistic features but preserve speaker traits. We propose a novel speaker-embedding-free framework called SEF-MK. Instead of using a single k-means model trained on the entire dataset, SEF-MK anonymizes SSL representations for each utterance by randomly selecting one of multiple k-means models, each trained on a different subset of speakers. We explore this approach from both attacker and user perspectives. Extensive experiments show that, compared to a single k-means model, SEF-MK with multiple k-means models better preserves linguistic and emotional content from the user's viewpoint. However, from the attacker's perspective, utilizing multiple k-means models boosts the effectiveness of privacy attacks. These insights can aid users in designing voice anonymization systems to mitigate attacker threats.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search</title>
<link>https://arxiv.org/abs/2508.07104</link>
<guid>https://arxiv.org/abs/2508.07104</guid>
<content:encoded><![CDATA[
arXiv:2508.07104v1 Announce Type: cross 
Abstract: The quest for effective quantum feature maps for data encoding presents significant challenges, particularly due to the flat training landscapes and lengthy training processes associated with parameterised quantum circuits. To address these issues, we propose an evolutionary training-free quantum architecture search (QAS) framework that employs circuit-based heuristics focused on trainability, hardware robustness, generalisation ability, expressivity, complexity, and kernel-target alignment. By ranking circuit architectures with various proxies, we reduce evaluation costs and incorporate hardware-aware circuits to enhance robustness against noise. We evaluate our approach on classification tasks (using quantum support vector machine) across diverse datasets using both artificial and quantum-generated datasets. Our approach demonstrates competitive accuracy on both simulators and real quantum hardware, surpassing state-of-the-art QAS methods in terms of sampling efficiency and achieving up to a 2x speedup in architecture search runtime.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2508.07112</link>
<guid>https://arxiv.org/abs/2508.07112</guid>
<content:encoded><![CDATA[
arXiv:2508.07112v1 Announce Type: cross 
Abstract: Lifting-based methods for 3D Human Pose Estimation (HPE), which predict 3D poses from detected 2D keypoints, often generalize poorly to new datasets and real-world settings. To address this, we propose \emph{AugLift}, a simple yet effective reformulation of the standard lifting pipeline that significantly improves generalization performance without requiring additional data collection or sensors. AugLift sparsely enriches the standard input -- the 2D keypoint coordinates $(x, y)$ -- by augmenting it with a keypoint detection confidence score $c$ and a corresponding depth estimate $d$. These additional signals are computed from the image using off-the-shelf, pre-trained models (e.g., for monocular depth estimation), thereby inheriting their strong generalization capabilities. Importantly, AugLift serves as a modular add-on and can be readily integrated into existing lifting architectures.
  Our extensive experiments across four datasets demonstrate that AugLift boosts cross-dataset performance on unseen datasets by an average of $10.1\%$, while also improving in-distribution performance by $4.0\%$. These gains are consistent across various lifting architectures, highlighting the robustness of our method. Our analysis suggests that these sparse, keypoint-aligned cues provide robust frame-level context, offering a practical way to significantly improve the generalization of any lifting-based pose estimation model. Code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensory robustness through top-down feedback and neural stochasticity in recurrent vision models</title>
<link>https://arxiv.org/abs/2508.07115</link>
<guid>https://arxiv.org/abs/2508.07115</guid>
<content:encoded><![CDATA[
arXiv:2508.07115v1 Announce Type: cross 
Abstract: Biological systems leverage top-down feedback for visual processing, yet most artificial vision models succeed in image classification using purely feedforward or recurrent architectures, calling into question the functional significance of descending cortical pathways. Here, we trained convolutional recurrent neural networks (ConvRNN) on image classification in the presence or absence of top-down feedback projections to elucidate the specific computational contributions of those feedback pathways. We found that ConvRNNs with top-down feedback exhibited remarkable speed-accuracy trade-off and robustness to noise perturbations and adversarial attacks, but only when they were trained with stochastic neural variability, simulated by randomly silencing single units via dropout. By performing detailed analyses to identify the reasons for such benefits, we observed that feedback information substantially shaped the representational geometry of the post-integration layer, combining the bottom-up and top-down streams, and this effect was amplified by dropout. Moreover, feedback signals coupled with dropout optimally constrained network activity onto a low-dimensional manifold and encoded object information more efficiently in out-of-distribution regimes, with top-down information stabilizing the representational dynamics at the population level. Together, these findings uncover a dual mechanism for resilient sensory coding. On the one hand, neural stochasticity prevents unit-level co-adaptation albeit at the cost of more chaotic dynamics. On the other hand, top-down feedback harnesses high-level information to stabilize network activity on compact low-dimensional manifolds.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI</title>
<link>https://arxiv.org/abs/2508.07183</link>
<guid>https://arxiv.org/abs/2508.07183</guid>
<content:encoded><![CDATA[
arXiv:2508.07183v1 Announce Type: cross 
Abstract: Explainable AI (XAI) in creative contexts can go beyond transparency to support artistic engagement, modifiability, and sustained practice. While curated datasets and training human-scale models can offer artists greater agency and control, large-scale generative models like text-to-image diffusion systems often obscure these possibilities. We suggest that even large models can be treated as creative materials if their internal structure is exposed and manipulable. We propose a craft-based approach to explainability rooted in long-term, hands-on engagement akin to Sch\"on's "reflection-in-action" and demonstrate its application through a model-bending and inspection plugin integrated into the node-based interface of ComfyUI. We demonstrate that by interactively manipulating different parts of a generative model, artists can develop an intuition about how each component influences the output.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</title>
<link>https://arxiv.org/abs/2508.07185</link>
<guid>https://arxiv.org/abs/2508.07185</guid>
<content:encoded><![CDATA[
arXiv:2508.07185v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a sparse knowledge attention mechanism, which allows the LLM to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building LLMs that can stay current with the ever-changing world.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Does a Deep Neural Network Look at Lexical Stress?</title>
<link>https://arxiv.org/abs/2508.07229</link>
<guid>https://arxiv.org/abs/2508.07229</guid>
<content:encoded><![CDATA[
arXiv:2508.07229v1 Announce Type: cross 
Abstract: Despite their success in speech processing, neural networks often operate as black boxes, prompting the question: what informs their decisions, and how can we interpret them? This work examines this issue in the context of lexical stress. A dataset of English disyllabic words was automatically constructed from read and spontaneous speech. Several Convolutional Neural Network (CNN) architectures were trained to predict stress position from a spectrographic representation of disyllabic words lacking minimal stress pairs (e.g., initial stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out test data. Layerwise Relevance Propagation (LRP), a technique for CNN interpretability analysis, revealed that predictions for held-out minimal pairs (PROtest vs. proTEST ) were most strongly influenced by information in stressed versus unstressed syllables, particularly the spectral properties of stressed vowels. However, the classifiers also attended to information throughout the word. A feature-specific relevance analysis is proposed, and its results suggest that our best-performing classifier is strongly influenced by the stressed vowel's first and second formants, with some evidence that its pitch and third formant also contribute. These results reveal deep learning's ability to acquire distributed cues to stress from naturally occurring data, extending traditional phonetic work based around highly controlled stimuli.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIGBOY1.2: Generating Realistic Synthetic Data for Disease Outbreak Modelling and Analytics</title>
<link>https://arxiv.org/abs/2508.07239</link>
<guid>https://arxiv.org/abs/2508.07239</guid>
<content:encoded><![CDATA[
arXiv:2508.07239v1 Announce Type: cross 
Abstract: Modelling disease outbreak models remains challenging due to incomplete surveillance data, noise, and limited access to standardized datasets. We have created BIGBOY1.2, an open synthetic dataset generator that creates configurable epidemic time series and population-level trajectories suitable for benchmarking modelling, forecasting, and visualisation. The framework supports SEIR and SIR-like compartmental logic, custom seasonality, and noise injection to mimic real reporting artifacts. BIGBOY1.2 can produce datasets with diverse characteristics, making it suitable for comparing traditional epidemiological models (e.g., SIR, SEIR) with modern machine learning approaches (e.g., SVM, neural networks).
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenHAIV: A Framework Towards Practical Open-World Learning</title>
<link>https://arxiv.org/abs/2508.07270</link>
<guid>https://arxiv.org/abs/2508.07270</guid>
<content:encoded><![CDATA[
arXiv:2508.07270v1 Announce Type: cross 
Abstract: Substantial progress has been made in various techniques for open-world recognition. Out-of-distribution (OOD) detection methods can effectively distinguish between known and unknown classes in the data, while incremental learning enables continuous model knowledge updates. However, in open-world scenarios, these approaches still face limitations. Relying solely on OOD detection does not facilitate knowledge updates in the model, and incremental fine-tuning typically requires supervised conditions, which significantly deviate from open-world settings. To address these challenges, this paper proposes OpenHAIV, a novel framework that integrates OOD detection, new class discovery, and incremental continual fine-tuning into a unified pipeline. This framework allows models to autonomously acquire and update knowledge in open-world environments. The proposed framework is available at https://haiv-lab.github.io/openhaiv .
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel Charting in Smart Radio Environments</title>
<link>https://arxiv.org/abs/2508.07305</link>
<guid>https://arxiv.org/abs/2508.07305</guid>
<content:encoded><![CDATA[
arXiv:2508.07305v1 Announce Type: cross 
Abstract: This paper introduces the use of static electromagnetic skins (EMSs) to enable robust device localization via channel charting (CC) in realistic urban environments. We develop a rigorous optimization framework that leverages EMS to enhance channel dissimilarity and spatial fingerprinting, formulating EMS phase profile design as a codebook-based problem targeting the upper quantiles of key embedding metric, localization error, trustworthiness, and continuity. Through 3D ray-traced simulations of a representative city scenario, we demonstrate that optimized EMS configurations, in addition to significant improvement of the average positioning error, reduce the 90th-percentile localization error from over 60 m (no EMS) to less than 25 m, while drastically improving trustworthiness and continuity. To the best of our knowledge, this is the first work to exploit Smart Radio Environment (SRE) with static EMS for enhancing CC, achieving substantial gains in localization performance under challenging None-Line-of-Sight (NLoS) conditions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways</title>
<link>https://arxiv.org/abs/2508.07308</link>
<guid>https://arxiv.org/abs/2508.07308</guid>
<content:encoded><![CDATA[
arXiv:2508.07308v1 Announce Type: cross 
Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering (Q&amp;A), specifically designed to evaluate complex reasoning in Large Language Models (LLMs). This dataset is generated through a semi-automated pipeline that transforms explicit decision pathways from medical source into realistic patient cases with associated questions and answers. Covering 4,063 case studies across 17 healthcare topics, each data point is based on clinically validated reasoning chains. HealthBranches supports both open-ended and multiple-choice question formats and uniquely includes the full reasoning path for each Q&amp;A. Its structured design enables robust evaluation of LLMs' multi-step inference capabilities, including their performance in structured Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a foundation for the development of more trustworthy, interpretable, and clinically reliable LLMs in high-stakes domains while also serving as a valuable resource for educational purposes.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities</title>
<link>https://arxiv.org/abs/2508.07315</link>
<guid>https://arxiv.org/abs/2508.07315</guid>
<content:encoded><![CDATA[
arXiv:2508.07315v1 Announce Type: cross 
Abstract: While beam search improves speech recognition quality over greedy decoding, standard implementations are slow, often sequential, and CPU-bound. To fully leverage modern hardware capabilities, we present a novel open-source FlexCTC toolkit for fully GPU-based beam decoding, designed for Connectionist Temporal Classification (CTC) models. Developed entirely in Python and PyTorch, it offers a fast, user-friendly, and extensible alternative to traditional C++, CUDA, or WFST-based decoders. The toolkit features a high-performance, fully batched GPU implementation with eliminated CPU-GPU synchronization and minimized kernel launch overhead via CUDA Graphs. It also supports advanced contextualization techniques, including GPU-powered N-gram language model fusion and phrase-level boosting. These features enable accurate and efficient decoding, making them suitable for both research and production use.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering</title>
<link>https://arxiv.org/abs/2508.07321</link>
<guid>https://arxiv.org/abs/2508.07321</guid>
<content:encoded><![CDATA[
arXiv:2508.07321v1 Announce Type: cross 
Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly contributed to the development of equitable AI systems capable of factual question-answering (QA). However, no known study tests the LLMs' robustness when presented with obfuscated versions of questions. To systematically evaluate these limitations, we propose a novel technique, ObfusQAte and, leveraging the same, introduce ObfusQA, a comprehensive, first of its kind, framework with multi-tiered obfuscation levels designed to examine LLM capabilities across three distinct dimensions: (i) Named-Entity Indirection, (ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these fine-grained distinctions in language, ObfusQA provides a comprehensive benchmark for evaluating LLM robustness and adaptability. Our study observes that LLMs exhibit a tendency to fail or generate hallucinated responses when confronted with these increasingly nuanced variations. To foster research in this direction, we make ObfusQAte publicly available.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonparametric Reaction Coordinate Optimization with Histories: A Framework for Rare Event Dynamics</title>
<link>https://arxiv.org/abs/2508.07326</link>
<guid>https://arxiv.org/abs/2508.07326</guid>
<content:encoded><![CDATA[
arXiv:2508.07326v1 Announce Type: cross 
Abstract: Rare but critical events in complex systems, such as protein folding, chemical reactions, disease progression, and extreme weather or climate phenomena, are governed by complex, high-dimensional, stochastic dynamics. Identifying an optimal reaction coordinate (RC) that accurately captures the progress of these dynamics is crucial for understanding and simulating such processes. This work introduces a nonparametric RC optimization framework that incorporates trajectory histories, enabling robust analysis even for irregular or incomplete data. The power of the method is demonstrated through increasingly challenging analyses of protein folding dynamics, where it provides accurate committor estimates that pass a stringent validation test and yield high-resolution free energy profiles. Its generality is further illustrated through applications to dynamics in phase space, a conceptual ocean circulation model, and a longitudinal clinical dataset. These results demonstrate that rare event dynamics can be accurately characterized without exhaustive sampling of the configuration space, establishing a general, flexible, and robust framework for analyzing complex dynamical systems and longitudinal datasets.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach</title>
<link>https://arxiv.org/abs/2508.07353</link>
<guid>https://arxiv.org/abs/2508.07353</guid>
<content:encoded><![CDATA[
arXiv:2508.07353v1 Announce Type: cross 
Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities of large language models (LLMs), highlighting the need for effective and efficient benchmark construction. Existing domain-specific benchmarks primarily focus on the scaling law, relying on massive corpora for supervised fine-tuning or generating extensive question sets for broad coverage. However, the impact of corpus and question-answer (QA) set design on the precision and recall of domain-specific LLMs remains unexplored. In this paper, we address this gap and demonstrate that the scaling law is not always the optimal principle for benchmark construction in specific domains. Instead, we propose Comp-Comp, an iterative benchmarking framework based on a comprehensiveness-compactness principle. Here, comprehensiveness ensures semantic recall of the domain, while compactness enhances precision, guiding both corpus and QA set construction. To validate our framework, we conducted a case study in a well-renowned university, resulting in the creation of XUBench, a large-scale and comprehensive closed-domain benchmark. Although we use the academic domain as the case in this work, our Comp-Comp framework is designed to be extensible beyond academia, providing valuable insights for benchmark construction across various domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.07382</link>
<guid>https://arxiv.org/abs/2508.07382</guid>
<content:encoded><![CDATA[
arXiv:2508.07382v1 Announce Type: cross 
Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet current Large Language Models (LLMs) face significant limitations in this domain, including poor error handling, inefficient reasoning, and an inability to perform complex end-to-end tasks autonomously. To address these challenges, we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning capabilities for this task through a two-stage reinforcement learning pipeline. We first construct a dataset of over 500 real-world, multi-step walkthroughs, which Pentest-R1 leverages for offline reinforcement learning (RL) to instill foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in an interactive Capture The Flag (CTF) environment, where it learns directly from environmental feedback to develop robust error self-correction and adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench benchmarks demonstrate the framework's effectiveness. On AutoPenBench, Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a 15.0\% success rate in unguided tasks, establishing a new state-of-the-art for open-source LLMs and matching the performance of top proprietary models. Ablation studies confirm that the synergy of both training stages is critical to its success.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spin Glass Characterization of Neural Networks</title>
<link>https://arxiv.org/abs/2508.07397</link>
<guid>https://arxiv.org/abs/2508.07397</guid>
<content:encoded><![CDATA[
arXiv:2508.07397v1 Announce Type: cross 
Abstract: This work presents a statistical mechanics characterization of neural networks, motivated by the replica symmetry breaking (RSB) phenomenon in spin glasses. A Hopfield-type spin glass model is constructed from a given feedforward neural network (FNN). Overlaps between simulated replica samples serve as a characteristic descriptor of the FNN. The connection between the spin-glass description and commonly studied properties of the FNN -- such as data fitting, capacity, generalization, and robustness -- has been investigated and empirically demonstrated. Unlike prior analytical studies that focus on model ensembles, this method provides a computable descriptor for individual network instances, which reveals nontrivial structural properties that are not captured by conventional metrics such as loss or accuracy. Preliminary results suggests its potential for practical applications such as model inspection, safety verification, and detection of hidden vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Strategic Plan Development</title>
<link>https://arxiv.org/abs/2508.07405</link>
<guid>https://arxiv.org/abs/2508.07405</guid>
<content:encoded><![CDATA[
arXiv:2508.07405v1 Announce Type: cross 
Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and Large Language Models (LLMs), more and more professional services are being augmented through Artificial Intelligence (AI), which once seemed impossible to automate. This paper presents a modular model for leveraging GAI in developing strategic plans for large scale government organizations and evaluates leading machine learning techniques in their application towards one of the identified modules. Specifically, the performance of BERTopic and Non-negative Matrix Factorization (NMF) are evaluated in their ability to use topic modeling to generate themes representative of Vision Elements within a strategic plan. To accomplish this, BERTopic and NMF models are trained using a large volume of reports from the Government Accountability Office (GAO). The generated topics from each model are then scored for similarity against the Vision Elements of a published strategic plan and the results are compared. Our results show that these techniques are capable of generating themes similar to 100% of the elements being evaluated against. Further, we conclude that BERTopic performs best in this application with more than half of its correlated topics achieving a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan development impacts a multi-billion dollar industry and assists the federal government in overcoming regulatory requirements which are crucial to the public good. Further work will focus on the operationalization of the concept proven in this study as well as viability of the remaining modules in the proposed model for GAI-generated strategic plans.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading</title>
<link>https://arxiv.org/abs/2508.07408</link>
<guid>https://arxiv.org/abs/2508.07408</guid>
<content:encoded><![CDATA[
arXiv:2508.07408v1 Announce Type: cross 
Abstract: In this study, we wish to showcase the unique utility of large language models (LLMs) in financial semantic annotation and alpha signal discovery. Leveraging a corpus of company-related tweets, we use an LLM to automatically assign multi-label event categories to high-sentiment-intensity tweets. We align these labeled sentiment signals with forward returns over 1-to-7-day horizons to evaluate their statistical efficacy and market tradability. Our experiments reveal that certain event labels consistently yield negative alpha, with Sharpe ratios as low as -0.38 and information coefficients exceeding 0.05, all statistically significant at the 95\% confidence level. This study establishes the feasibility of transforming unstructured social media text into structured, multi-label event variables. A key contribution of this work is its commitment to transparency and reproducibility; all code and methodologies are made publicly available. Our results provide compelling evidence that social media sentiment is a valuable, albeit noisy, signal in financial forecasting and underscore the potential of open-source frameworks to democratize algorithmic trading research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Multilingual Multimodal LLMs With Cultural Knowledge</title>
<link>https://arxiv.org/abs/2508.07414</link>
<guid>https://arxiv.org/abs/2508.07414</guid>
<content:encoded><![CDATA[
arXiv:2508.07414v1 Announce Type: cross 
Abstract: Multimodal Large Language Models excel in high-resource settings, but often misinterpret long-tail cultural entities and underperform in low-resource languages. To address this gap, we propose a data-centric approach that directly grounds MLLMs in cultural knowledge. Leveraging a large scale knowledge graph from Wikidata, we collect images that represent culturally significant entities, and generate synthetic multilingual visual question answering data. The resulting dataset, CulturalGround, comprises 22 million high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages. We train an open-source MLLM CulturalPangea on CulturalGround, interleaving standard multilingual instruction-tuning data to preserve general abilities. CulturalPangea achieves state-of-the-art performance among open models on various culture-focused multilingual multimodal benchmarks, outperforming prior models by an average of 5.0 without degrading results on mainstream vision-language tasks. Our findings show that our targeted, culturally grounded approach could substantially narrow the cultural gap in MLLMs and offer a practical path towards globally inclusive multimodal systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Theory of Multi-stage Newton Iteration Algorithm for Online Continual Learning</title>
<link>https://arxiv.org/abs/2508.07419</link>
<guid>https://arxiv.org/abs/2508.07419</guid>
<content:encoded><![CDATA[
arXiv:2508.07419v1 Announce Type: cross 
Abstract: We focus on the critical challenge of handling non-stationary data streams in online continual learning environments, where constrained storage capacity prevents complete retention of historical data, leading to catastrophic forgetting during sequential task training. To more effectively analyze and address the problem of catastrophic forgetting in continual learning, we propose a novel continual learning framework from a statistical perspective. Our approach incorporates random effects across all model parameters and allows the dimension of parameters to diverge to infinity, offering a general formulation for continual learning problems. To efficiently process streaming data, we develop a Multi-step Newton Iteration algorithm that significantly reduces computational costs in certain scenarios by alleviating the burden of matrix inversion. Theoretically, we derive the asymptotic normality of the estimator, enabling subsequent statistical inference. Comprehensive validation through synthetic data experiments and two real datasets analyses demonstrates the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures</title>
<link>https://arxiv.org/abs/2508.07423</link>
<guid>https://arxiv.org/abs/2508.07423</guid>
<content:encoded><![CDATA[
arXiv:2508.07423v1 Announce Type: cross 
Abstract: As the particle physics community needs higher and higher precisions in order to test our current model of the subatomic world, larger and larger datasets are necessary. With upgrades scheduled for the detectors of colliding-beam experiments around the world, and specifically at the Large Hadron Collider at CERN, more collisions and more complex interactions are expected. This directly implies an increase in data produced and consequently in the computational resources needed to process them. At CERN, the amount of data produced is gargantuan. This is why the data have to be heavily filtered and selected in real time before being permanently stored. This data can then be used to perform physics analyses, in order to expand our current understanding of the universe and improve the Standard Model of physics. This real-time filtering, known as triggering, involves complex processing happening often at frequencies as high as 40 MHz. This thesis contributes to understanding how machine learning models can be efficiently deployed in such environments, in order to maximize throughput and minimize energy consumption. Inevitably, modern hardware designed for such tasks and contemporary algorithms are needed in order to meet the challenges posed by the stringent, high-frequency data rates. In this work, I present our graph neural network-based pipeline, developed for charged particle track reconstruction at the LHCb experiment at CERN. The pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely on GPUs. Its performance was compared against the classical tracking algorithms currently in production at LHCb. The pipeline was also accelerated on the FPGA architecture, and its performance in terms of power consumption and processing speed was compared against the GPU implementation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CP-Agent: Agentic Constraint Programming</title>
<link>https://arxiv.org/abs/2508.07468</link>
<guid>https://arxiv.org/abs/2508.07468</guid>
<content:encoded><![CDATA[
arXiv:2508.07468v1 Announce Type: cross 
Abstract: Translating natural language problem descriptions into formal constraint models remains a fundamental challenge in constraint programming, requiring deep expertise in both the problem domain and modeling frameworks. Previous approaches to automating this translation have employed fixed workflows with predetermined modeling steps, failing on a significant number of benchmark problems. We present a new approach using a pure agentic strategy without any fixed pipeline. We developed a general-purpose Python coding agent based on the ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for stateful code execution and iterative development. Rather than embedding constraint programming logic into the agent architecture, domain-specific expertise is injected solely through a carefully crafted project prompt. The agent combines this prompt-encoded knowledge with access to file operations and code execution tools, enabling it to test hypotheses, debug failures, and verify solutions dynamically. Implemented in just a few hundred lines of code, this architecture successfully solves all 101 problems of the CP-Bench constraint programming benchmark set. The results suggest that constraint modeling tasks require the combination of general coding tools and domain expertise encoded in prompts, rather than specialized agent architectures or predefined workflows.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy</title>
<link>https://arxiv.org/abs/2508.07485</link>
<guid>https://arxiv.org/abs/2508.07485</guid>
<content:encoded><![CDATA[
arXiv:2508.07485v1 Announce Type: cross 
Abstract: We present the first evaluation harness that enables any out-of-the-box, local, Large Language Models (LLMs) to play full-press Diplomacy without fine-tuning or specialized training. Previous work required frontier LLMs, or fine-tuning, due to the high complexity and information density of Diplomacy's game state. Combined with the high variance of matches, these factors made Diplomacy prohibitive for study. In this work, we used data-driven iteration to optimize a textual game state representation such that a 24B model can reliably complete matches without any fine tuning. We develop tooling to facilitate hypothesis testing and statistical analysis, and we present case studies on persuasion, aggressive playstyles, and performance across a range of models. We conduct a variety of experiments across many popular LLMs, finding the larger models perform the best, but the smaller models still play adequately. We also introduce Critical State Analysis: an experimental protocol for rapidly iterating and analyzing key moments in a game at depth. Our harness democratizes the evaluation of strategic reasoning in LLMs by eliminating the need for fine-tuning, and it provides insights into how these capabilities emerge naturally from widely used LLMs. Our code is available in the supplement and will be open sourced.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths</title>
<link>https://arxiv.org/abs/2508.07487</link>
<guid>https://arxiv.org/abs/2508.07487</guid>
<content:encoded><![CDATA[
arXiv:2508.07487v1 Announce Type: cross 
Abstract: Unequal error protection (UEP) coding that enables differentiated reliability levels within a transmitted message is essential for modern communication systems. Autoencoder (AE)-based code designs have shown promise in the context of learned equal error protection (EEP) coding schemes. However, their application to UEP remains largely unexplored, particularly at intermediate blocklengths, due to the increasing complexity of AE-based models. Inspired by the proven effectiveness of superposition coding and successive interference cancellation (SIC) decoding in conventional UEP schemes, we propose a structured AE-based architecture that extends AE-based UEP codes to substantially larger blocklengths while maintaining efficient training. By structuring encoding and decoding into smaller AE subblocks, our method provides a flexible framework for fine-tuning UEP reliability levels while adapting to diverse system parameters. Numerical results show that the proposed approach improves over established achievability bounds of randomized superposition coding-based UEP schemes with SIC decoding, making the proposed structured AE-based UEP codes a scalable and efficient solution for next-generation networks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Commentary Generation for Soccer Highlights</title>
<link>https://arxiv.org/abs/2508.07543</link>
<guid>https://arxiv.org/abs/2508.07543</guid>
<content:encoded><![CDATA[
arXiv:2508.07543v1 Announce Type: cross 
Abstract: Automated soccer commentary generation has evolved from template-based systems to advanced neural architectures, aiming to produce real-time descriptions of sports events. While frameworks like SoccerNet-Caption laid foundational work, their inability to achieve fine-grained alignment between video content and commentary remains a significant challenge. Recent efforts such as MatchTime, with its MatchVoice model, address this issue through coarse and fine-grained alignment techniques, achieving improved temporal synchronization. In this paper, we extend MatchVoice to commentary generation for soccer highlights using the GOAL dataset, which emphasizes short clips over entire games. We conduct extensive experiments to reproduce the original MatchTime results and evaluate our setup, highlighting the impact of different training configurations and hardware limitations. Furthermore, we explore the effect of varying window sizes on zero-shot performance. While MatchVoice exhibits promising generalization capabilities, our findings suggest the need for integrating techniques from broader video-language domains to further enhance performance. Our code is available at https://github.com/chidaksh/SoccerCommentary.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barron Space Representations for Elliptic PDEs with Homogeneous Boundary Conditions</title>
<link>https://arxiv.org/abs/2508.07559</link>
<guid>https://arxiv.org/abs/2508.07559</guid>
<content:encoded><![CDATA[
arXiv:2508.07559v1 Announce Type: cross 
Abstract: We study the approximation complexity of high-dimensional second-order elliptic PDEs with homogeneous boundary conditions on the unit hypercube, within the framework of Barron spaces. Under the assumption that the coefficients belong to suitably defined Barron spaces, we prove that the solution can be efficiently approximated by two-layer neural networks, circumventing the curse of dimensionality. Our results demonstrate the expressive power of shallow networks in capturing high-dimensional PDE solutions under appropriate structural assumptions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification</title>
<link>https://arxiv.org/abs/2508.07577</link>
<guid>https://arxiv.org/abs/2508.07577</guid>
<content:encoded><![CDATA[
arXiv:2508.07577v1 Announce Type: cross 
Abstract: LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning dynamics under data scarcity and domain shifts remain underexplored. This paper shows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts) are indicative of the transitions between source and target domains; its efficacy is contingent upon the degree to which the target training samples accurately represent the target domain, as quantified by our proposed Fine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet effective rescaling mechanism using a scalar $\lambda$ that is negatively correlated to $FSR$ to align learned LayerNorm shifts with those ideal shifts achieved under fully representative data, combined with a cyclic framework that further enhances the LayerNorm fine-tuning. Extensive experiments across natural and pathological images, in both in-distribution (ID) and out-of-distribution (OOD) settings, and various target training sample regimes validate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher $\lambda$ in comparison to ID cases, especially with scarce data, indicating under-represented target training samples. Moreover, ViTFs fine-tuned on pathological data behave more like ID settings, favoring conservative LayerNorm updates. Our findings illuminate the underexplored dynamics of LayerNorm in transfer learning and provide practical strategies for LayerNorm fine-tuning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkTuning: Instilling Cognitive Reflections without Distillation</title>
<link>https://arxiv.org/abs/2508.07616</link>
<guid>https://arxiv.org/abs/2508.07616</guid>
<content:encoded><![CDATA[
arXiv:2508.07616v1 Announce Type: cross 
Abstract: Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that don't exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback -- enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the student's thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at https://github.com/3rdAT/ThinkTuning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grasp-HGN: Grasping the Unexpected</title>
<link>https://arxiv.org/abs/2508.07648</link>
<guid>https://arxiv.org/abs/2508.07648</guid>
<content:encoded><![CDATA[
arXiv:2508.07648v1 Announce Type: cross 
Abstract: For transradial amputees, robotic prosthetic hands promise to regain the capability to perform daily living activities. To advance next-generation prosthetic hand control design, it is crucial to address current shortcomings in robustness to out of lab artifacts, and generalizability to new environments. Due to the fixed number of object to interact with in existing datasets, contrasted with the virtually infinite variety of objects encountered in the real world, current grasp models perform poorly on unseen objects, negatively affecting users' independence and quality of life.
  To address this: (i) we define semantic projection, the ability of a model to generalize to unseen object types and show that conventional models like YOLO, despite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose Grasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to infer the suitable grasp type estimate based on the object's physical characteristics resulting in a significant 50.2% accuracy over unseen object types compared to 36.7% accuracy of an SOTA grasp estimation model.
  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp Network (HGN), an edge-cloud deployment infrastructure enabling fast grasp estimation on edge and accurate cloud inference as a fail-safe, effectively expanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC) enables dynamic switching between edge and cloud models, improving semantic projection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object types. Over a real-world sample mix, it reaches 86% average accuracy (12.2% gain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation</title>
<link>https://arxiv.org/abs/2508.07649</link>
<guid>https://arxiv.org/abs/2508.07649</guid>
<content:encoded><![CDATA[
arXiv:2508.07649v1 Announce Type: cross 
Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business intelligence, where users' spatial-temporal transitions and social relationships play key roles. However, most existing works model spatial and temporal transitions separately, leading to misaligned representations of the same spatial-temporal key nodes. This misalignment introduces redundant information during fusion, increasing model uncertainty and reducing interpretability. To address this issue, we propose DiMuST, a socially enhanced POI recommendation model based on disentangled representation learning over multiplex spatial-temporal transition graphs. The model employs a novel Disentangled variational multiplex graph Auto-Encoder (DAE), which first disentangles shared and private distributions using a multiplex spatial-temporal graph strategy. It then fuses the shared features via a Product of Experts (PoE) mechanism and denoises the private features through contrastive constraints. The model effectively captures the spatial-temporal transition representations of POIs while preserving the intrinsic correlation of their spatial-temporal relationships. Experiments on two challenging datasets demonstrate that our DiMuST significantly outperforms existing methods across multiple metrics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ethics2vec: aligning automatic agents and human preferences</title>
<link>https://arxiv.org/abs/2508.07673</link>
<guid>https://arxiv.org/abs/2508.07673</guid>
<content:encoded><![CDATA[
arXiv:2508.07673v1 Announce Type: cross 
Abstract: Though intelligent agents are supposed to improve human experience (or make it more efficient), it is hard from a human perspective to grasp the ethical values which are explicitly or implicitly embedded in an agent behaviour. This is the well-known problem of alignment, which refers to the challenge of designing AI systems that align with human values, goals and preferences. This problem is particularly challenging since most human ethical considerations refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable) values and criteria. Consider, for instance, a medical agent prescribing a treatment to a cancerous patient. How could it take into account (and/or weigh) incommensurable aspects like the value of a human life and the cost of the treatment? Now, the alignment between human and artificial values is possible only if we define a common space where a metric can be defined and used. This paper proposes to extend to ethics the conventional Anything2vec approach, which has been successful in plenty of similar and hard-to-quantify domains (ranging from natural language processing to recommendation systems and graph analysis). This paper proposes a way to map an automatic agent decision-making (or control law) strategy to a multivariate vector representation, which can be used to compare and assess the alignment with human values. The Ethics2Vec method is first introduced in the case of an automatic agent performing binary decision-making. Then, a vectorisation of an automatic control law (like in the case of a self-driving car) is discussed to show how the approach can be extended to automatic control settings.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-Aware Transformer Training for Automated Planning</title>
<link>https://arxiv.org/abs/2508.07743</link>
<guid>https://arxiv.org/abs/2508.07743</guid>
<content:encoded><![CDATA[
arXiv:2508.07743v1 Announce Type: cross 
Abstract: While transformers excel in many settings, their application in the field of automated planning is limited. Prior work like PlanGPT, a state-of-the-art decoder-only transformer, struggles with extrapolation from easy to hard planning problems. This in turn stems from problem symmetries: planning tasks can be represented with arbitrary variable names that carry no meaning beyond being identifiers. This causes a combinatorial explosion of equivalent representations that pure transformers cannot efficiently learn from. We propose a novel contrastive learning objective to make transformers symmetry-aware and thereby compensate for their lack of inductive bias. Combining this with architectural improvements, we show that transformers can be efficiently trained for either plan-generation or heuristic-prediction. Our results across multiple planning domains demonstrate that our symmetry-aware training effectively and efficiently addresses the limitations of PlanGPT.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography</title>
<link>https://arxiv.org/abs/2508.07773</link>
<guid>https://arxiv.org/abs/2508.07773</guid>
<content:encoded><![CDATA[
arXiv:2508.07773v1 Announce Type: cross 
Abstract: Active Infrared thermography (AIRT) is a widely adopted non-destructive testing (NDT) technique for detecting subsurface anomalies in industrial components. Due to the high dimensionality of AIRT data, current approaches employ non-linear autoencoders (AEs) for dimensionality reduction. However, the latent space learned by AIRT AEs lacks structure, limiting their effectiveness in downstream defect characterization tasks. To address this limitation, this paper proposes a principal component analysis guided (PCA-guided) autoencoding framework for structured dimensionality reduction to capture intricate, non-linear features in thermographic signals while enforcing a structured latent space. A novel loss function, PCA distillation loss, is introduced to guide AIRT AEs to align the latent representation with structured PCA components while capturing the intricate, non-linear patterns in thermographic signals. To evaluate the utility of the learned, structured latent space, we propose a neural network-based evaluation metric that assesses its suitability for defect characterization. Experimental results show that the proposed PCA-guided AE outperforms state-of-the-art dimensionality reduction methods on PVC, CFRP, and PLA samples in terms of contrast, signal-to-noise ratio (SNR), and neural network-based metrics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys</title>
<link>https://arxiv.org/abs/2508.07798</link>
<guid>https://arxiv.org/abs/2508.07798</guid>
<content:encoded><![CDATA[
arXiv:2508.07798v1 Announce Type: cross 
Abstract: The design of shape memory alloys (SMAs) with high transformation temperatures and large mechanical work output remains a longstanding challenge in functional materials engineering. Here, we introduce a data-driven framework based on generative adversarial network (GAN) inversion for the inverse design of high-performance SMAs. By coupling a pretrained GAN with a property prediction model, we perform gradient-based latent space optimization to directly generate candidate alloy compositions and processing parameters that satisfy user-defined property targets. The framework is experimentally validated through the synthesis and characterization of five NiTi-based SMAs. Among them, the Ni$_{49.8}$Ti$_{26.4}$Hf$_{18.6}$Zr$_{5.2}$ alloy achieves a high transformation temperature of 404 $^\circ$C, a large mechanical work output of 9.9 J/cm$^3$, a transformation enthalpy of 43 J/g , and a thermal hysteresis of 29 {\deg}C, outperforming existing NiTi alloys. The enhanced performance is attributed to a pronounced transformation volume change and a finely dispersed of Ti$_2$Ni-type precipitates, enabled by sluggish Zr and Hf diffusion, and semi-coherent interfaces with localized strain fields. This study demonstrates that GAN inversion offers an efficient and generalizable route for the property-targeted discovery of complex alloys.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer</title>
<link>https://arxiv.org/abs/2508.07817</link>
<guid>https://arxiv.org/abs/2508.07817</guid>
<content:encoded><![CDATA[
arXiv:2508.07817v1 Announce Type: cross 
Abstract: The core role of medical images in disease diagnosis makes their quality directly affect the accuracy of clinical judgment. However, due to factors such as low-dose scanning, equipment limitations and imaging artifacts, medical images are often accompanied by non-uniform noise interference, which seriously affects structure recognition and lesion detection. This paper proposes a medical image adaptive denoising model (MI-ND) that integrates multi-scale convolutional and Transformer architecture, introduces a noise level estimator (NLE) and a noise adaptive attention module (NAAB), and realizes channel-spatial attention regulation and cross-modal feature fusion driven by noise perception. Systematic testing is carried out on multimodal public datasets. Experiments show that this method significantly outperforms the comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS, and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing strong prac-tical value and promotional potential. The model has outstanding benefits in structural recovery, diagnostic sensitivity, and cross-modal robustness, and provides an effective solution for medical image enhancement and AI-assisted diagnosis and treatment.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</title>
<link>https://arxiv.org/abs/2508.07819</link>
<guid>https://arxiv.org/abs/2508.07819</guid>
<content:encoded><![CDATA[
arXiv:2508.07819v1 Announce Type: cross 
Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of local inductive biases for dense prediction and their reliance on inflexible feature fusion paradigms. We address these limitations through an Architectural Co-Design framework that jointly refines feature representation and cross-modal fusion. Our method integrates a parameter-efficient Convolutional Low-Rank Adaptation (Conv-LoRA) adapter to inject local inductive biases for fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that leverages visual context to adaptively modulate text prompts, enabling a powerful bidirectional fusion. Extensive experiments on diverse industrial and medical benchmarks demonstrate superior accuracy and robustness, validating that this synergistic co-design is critical for robustly adapting foundation models to dense perception tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-IFT: A Gated Linear Unit adapter with Iterative Fine-Tuning for Low-Resource Children's Speaker Verification</title>
<link>https://arxiv.org/abs/2508.07836</link>
<guid>https://arxiv.org/abs/2508.07836</guid>
<content:encoded><![CDATA[
arXiv:2508.07836v1 Announce Type: cross 
Abstract: Speaker Verification (SV) systems trained on adults speech often underperform on children's SV due to the acoustic mismatch, and limited children speech data makes fine-tuning not very effective. In this paper, we propose an innovative framework, a Gated Linear Unit adapter with Iterative Fine-Tuning (G-IFT), to enhance knowledge transfer efficiency between the high-resource adults speech domain and the low-resource children's speech domain. In this framework, a Gated Linear Unit adapter is first inserted between the pre-trained speaker embedding model and the classifier. Then the classifier, adapter, and pre-trained speaker embedding model are optimized sequentially in an iterative way. This framework is agnostic to the type of the underlying architecture of the SV system. Our experiments on ECAPA-TDNN, ResNet, and X-vector architectures using the OGI and MyST datasets demonstrate that the G-IFT framework yields consistent reductions in Equal Error Rates compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recommendation Is a Dish Better Served Warm</title>
<link>https://arxiv.org/abs/2508.07856</link>
<guid>https://arxiv.org/abs/2508.07856</guid>
<content:encoded><![CDATA[
arXiv:2508.07856v1 Announce Type: cross 
Abstract: In modern recommender systems, experimental settings typically include filtering out cold users and items based on a minimum interaction threshold. However, these thresholds are often chosen arbitrarily and vary widely across studies, leading to inconsistencies that can significantly affect the comparability and reliability of evaluation results. In this paper, we systematically explore the cold-start boundary by examining the criteria used to determine whether a user or an item should be considered cold. Our experiments incrementally vary the number of interactions for different items during training, and gradually update the length of user interaction histories during inference. We investigate the thresholds across several widely used datasets, commonly represented in recent papers from top-tier conferences, and on multiple established recommender baselines. Our findings show that inconsistent selection of cold-start thresholds can either result in the unnecessary removal of valuable data or lead to the misclassification of cold instances as warm, introducing more noise into the system.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model</title>
<link>https://arxiv.org/abs/2508.07863</link>
<guid>https://arxiv.org/abs/2508.07863</guid>
<content:encoded><![CDATA[
arXiv:2508.07863v1 Announce Type: cross 
Abstract: Human motion generation has emerged as a critical technology with transformative potential for real-world applications. However, existing vision-language-motion models (VLMMs) face significant limitations that hinder their practical deployment. We identify controllability as a main bottleneck, manifesting in five key aspects: inadequate response to diverse human commands, limited pose initialization capabilities, poor performance on long-term sequences, insufficient handling of unseen scenarios, and lack of fine-grained control over individual body parts. To overcome these limitations, we present Being-M0.5, the first real-time, controllable VLMM that achieves state-of-the-art performance across multiple motion generation tasks. Our approach is built upon HuMo100M, the largest and most comprehensive human motion dataset to date, comprising over 5 million self-collected motion sequences, 100 million multi-task instructional instances, and detailed part-level annotations that address a critical gap in existing datasets. We introduce a novel part-aware residual quantization technique for motion tokenization that enables precise, granular control over individual body parts during generation. Extensive experimental validation demonstrates Being-M0.5's superior performance across diverse motion benchmarks, while comprehensive efficiency analysis confirms its real-time capabilities. Our contributions include design insights and detailed computational analysis to guide future development of practical motion generators. We believe that HuMo100M and Being-M0.5 represent significant advances that will accelerate the adoption of motion generation technologies in real-world applications. The project page is available at https://beingbeyond.github.io/Being-M0.5.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unequal Uncertainty: Rethinking Algorithmic Interventions for Mitigating Discrimination from AI</title>
<link>https://arxiv.org/abs/2508.07872</link>
<guid>https://arxiv.org/abs/2508.07872</guid>
<content:encoded><![CDATA[
arXiv:2508.07872v1 Announce Type: cross 
Abstract: Uncertainty in artificial intelligence (AI) predictions poses urgent legal and ethical challenges for AI-assisted decision-making. We examine two algorithmic interventions that act as guardrails for human-AI collaboration: selective abstention, which withholds high-uncertainty predictions from human decision-makers, and selective friction, which delivers those predictions together with salient warnings or disclosures that slow the decision process. Research has shown that selective abstention based on uncertainty can inadvertently exacerbate disparities and disadvantage under-represented groups that disproportionately receive uncertain predictions. In this paper, we provide the first integrated socio-technical and legal analysis of uncertainty-based algorithmic interventions. Through two case studies, AI-assisted consumer credit decisions and AI-assisted content moderation, we demonstrate how the seemingly neutral use of uncertainty thresholds can trigger discriminatory impacts. We argue that, although both interventions pose risks of unlawful discrimination under UK law, selective frictions offer a promising pathway toward fairer and more accountable AI-assisted decision-making by preserving transparency and encouraging more cautious human judgment.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EFU: Enforcing Federated Unlearning via Functional Encryption</title>
<link>https://arxiv.org/abs/2508.07873</link>
<guid>https://arxiv.org/abs/2508.07873</guid>
<content:encoded><![CDATA[
arXiv:2508.07873v1 Announce Type: cross 
Abstract: Federated unlearning (FU) algorithms allow clients in federated settings to exercise their ''right to be forgotten'' by removing the influence of their data from a collaboratively trained model. Existing FU methods maintain data privacy by performing unlearning locally on the client-side and sending targeted updates to the server without exposing forgotten data; yet they often rely on server-side cooperation, revealing the client's intent and identity without enforcement guarantees - compromising autonomy and unlearning privacy. In this work, we propose EFU (Enforced Federated Unlearning), a cryptographically enforced FU framework that enables clients to initiate unlearning while concealing its occurrence from the server. Specifically, EFU leverages functional encryption to bind encrypted updates to specific aggregation functions, ensuring the server can neither perform unauthorized computations nor detect or skip unlearning requests. To further mask behavioral and parameter shifts in the aggregated model, we incorporate auxiliary unlearning losses based on adversarial examples and parameter importance regularization. Extensive experiments show that EFU achieves near-random accuracy on forgotten data while maintaining performance comparable to full retraining across datasets and neural architectures - all while concealing unlearning intent from the server. Furthermore, we demonstrate that EFU is agnostic to the underlying unlearning algorithm, enabling secure, function-hiding, and verifiable unlearning for any client-side FU mechanism that issues targeted updates.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images</title>
<link>https://arxiv.org/abs/2508.07875</link>
<guid>https://arxiv.org/abs/2508.07875</guid>
<content:encoded><![CDATA[
arXiv:2508.07875v1 Announce Type: cross 
Abstract: Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer, and early, accurate diagnosis is critical to improving patient survival rates by guiding treatment decisions. Combining medical expertise with artificial intelligence (AI) holds significant promise for enhancing the precision and efficiency of IDC detection. In this work, we propose a human-in-the-loop (HITL) deep learning system designed to detect IDC in histopathology images. The system begins with an initial diagnosis provided by a high-performance EfficientNetV2S model, offering feedback from AI to the human expert. Medical professionals then review the AI-generated results, correct any misclassified images, and integrate the revised labels into the training dataset, forming a feedback loop from the human back to the AI. This iterative process refines the model's performance over time. The EfficientNetV2S model itself achieves state-of-the-art performance compared to existing methods in the literature, with an overall accuracy of 93.65\%. Incorporating the human-in-the-loop system further improves the model's accuracy using four experimental groups with misclassified images. These results demonstrate the potential of this collaborative approach to enhance AI performance in diagnostic systems. This work contributes to advancing automated, efficient, and highly accurate methods for IDC detection through human-AI collaboration, offering a promising direction for future AI-assisted medical diagnostics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic dynamics learning with state-space systems</title>
<link>https://arxiv.org/abs/2508.07876</link>
<guid>https://arxiv.org/abs/2508.07876</guid>
<content:encoded><![CDATA[
arXiv:2508.07876v1 Announce Type: cross 
Abstract: This work advances the theoretical foundations of reservoir computing (RC) by providing a unified treatment of fading memory and the echo state property (ESP) in both deterministic and stochastic settings. We investigate state-space systems, a central model class in time series learning, and establish that fading memory and solution stability hold generically -- even in the absence of the ESP -- offering a robust explanation for the empirical success of RC models without strict contractivity conditions. In the stochastic case, we critically assess stochastic echo states, proposing a novel distributional perspective rooted in attractor dynamics on the space of probability distributions, which leads to a rich and coherent theory. Our results extend and generalize previous work on non-autonomous dynamical systems, offering new insights into causality, stability, and memory in RC models. This lays the groundwork for reliable generative modeling of temporal data in both deterministic and stochastic regimes.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta Off-Policy Estimation</title>
<link>https://arxiv.org/abs/2508.07914</link>
<guid>https://arxiv.org/abs/2508.07914</guid>
<content:encoded><![CDATA[
arXiv:2508.07914v1 Announce Type: cross 
Abstract: Off-policy estimation (OPE) methods enable unbiased offline evaluation of recommender systems, directly estimating the online reward some target policy would have obtained, from offline data and with statistical guarantees. The theoretical elegance of the framework combined with practical successes have led to a surge of interest, with many competing estimators now available to practitioners and researchers. Among these, Doubly Robust methods provide a prominent strategy to combine value- and policy-based estimators.
  In this work, we take an alternative perspective to combine a set of OPE estimators and their associated confidence intervals into a single, more accurate estimate. Our approach leverages a correlated fixed-effects meta-analysis framework, explicitly accounting for dependencies among estimators that arise due to shared data. This yields a best linear unbiased estimate (BLUE) of the target policy's value, along with an appropriately conservative confidence interval that reflects inter-estimator correlation. We validate our method on both simulated and real-world data, demonstrating improved statistical efficiency over existing individual estimators.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.07923</link>
<guid>https://arxiv.org/abs/2508.07923</guid>
<content:encoded><![CDATA[
arXiv:2508.07923v1 Announce Type: cross 
Abstract: Generative AI holds great potentials to automate and enhance data synthesis in nuclear medicine. However, the high-stakes nature of biomedical imaging necessitates robust mechanisms to detect and manage unexpected or erroneous model behavior. We introduce development and implementation of a hybrid anomaly detection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems. Two applications are demonstrated: Pose2Xray, which generates synthetic X-rays from photographic mouse images, and DosimetrEYE, which estimates 3D radiation dose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD) enhances reliability, reduces manual oversight, and supports real-time quality control. This approach strengthens the industrial viability of GenAI in preclinical settings by increasing robustness, scalability, and regulatory compliance.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Approximation for Two-Timescale Linear Stochastic Approximation</title>
<link>https://arxiv.org/abs/2508.07928</link>
<guid>https://arxiv.org/abs/2508.07928</guid>
<content:encoded><![CDATA[
arXiv:2508.07928v1 Announce Type: cross 
Abstract: In this paper, we establish non-asymptotic bounds for accuracy of normal approximation for linear two-timescale stochastic approximation (TTSA) algorithms driven by martingale difference or Markov noise. Focusing on both the last iterate and Polyak-Ruppert averaging regimes, we derive bounds for normal approximation in terms of the convex distance between probability distributions. Our analysis reveals a non-trivial interaction between the fast and slow timescales: the normal approximation rate for the last iterate improves as the timescale separation increases, while it decreases in the Polyak-Ruppert averaged setting. We also provide the high-order moment bounds for the error of linear TTSA algorithm, which may be of independent interest.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions</title>
<link>https://arxiv.org/abs/2508.07948</link>
<guid>https://arxiv.org/abs/2508.07948</guid>
<content:encoded><![CDATA[
arXiv:2508.07948v1 Announce Type: cross 
Abstract: Progressive neurodegenerative diseases, including Alzheimer's disease (AD), multiple sclerosis (MS), Parkinson's disease (PD), and amyotrophic lateral sclerosis (ALS), exhibit complex, nonlinear trajectories that challenge deterministic modeling. Traditional time-domain analyses of multiomic and neuroimaging data often fail to capture hidden oscillatory patterns, limiting predictive accuracy. We propose a theoretical mathematical framework that transforms time-series data into frequency or s-domain using Fourier and Laplace transforms, models neuronal dynamics via Hamiltonian formulations, and employs quantum-classical hybrid computing with variational quantum eigensolvers (VQE) for enhanced pattern detection. This theoretical construct serves as a foundation for future empirical works in quantum-enhanced analysis of neurodegenerative diseases. We extend this to quaternionic representations with three imaginary axes ($i, j, k$) to model multistate Hamiltonians in multifaceted disorders, drawing from quantum neuromorphic computing to capture entangled neural dynamics \citep{Pehle2020, Emani2019}. This approach leverages quantum advantages in handling high-dimensional amplitude-phase data, enabling outlier detection and frequency signature analysis. Potential clinical applications include identifying high-risk patients with rapid progression or therapy resistance using s-domain biomarkers, supported by quantum machine learning (QML) precedents achieving up to 99.89% accuracy in Alzheimer's classification \citep{Belay2024, Bhowmik2025}. This framework aims to lay the groundwork for redefining precision medicine for neurodegenerative diseases through future validations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis</title>
<link>https://arxiv.org/abs/2508.07950</link>
<guid>https://arxiv.org/abs/2508.07950</guid>
<content:encoded><![CDATA[
arXiv:2508.07950v1 Announce Type: cross 
Abstract: Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model. FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory & Reflection module for iterative refinement, and (iv) a Global Solver for conclusion synthesis. The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. In evaluations across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI systems in both long-form autopsy analyses and concise cause-of-death conclusions. It demonstrated robust generalization across six geographic regions and achieved high expert concordance in blinded validations. Senior pathologists validated FEAT's outputs as comparable to those of human experts, with improved detection of subtle evidentiary nuances. To our knowledge, FEAT is the first LLM-based AI agent system dedicated to forensic medicine, offering scalable, consistent death certification while maintaining expert-level rigor. By integrating AI efficiency with human oversight, this work could advance equitable access to reliable medicolegal services while addressing critical capacity constraints in forensic systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Source-Channel Coding for Semantic Communications</title>
<link>https://arxiv.org/abs/2508.07958</link>
<guid>https://arxiv.org/abs/2508.07958</guid>
<content:encoded><![CDATA[
arXiv:2508.07958v1 Announce Type: cross 
Abstract: Semantic communications (SemComs) have emerged as a promising paradigm for joint data and task-oriented transmissions, combining the demands for both the bit-accurate delivery and end-to-end (E2E) distortion minimization. However, current joint source-channel coding (JSCC) in SemComs is not compatible with the existing communication systems and cannot adapt to the variations of the sources or the channels, while separate source-channel coding (SSCC) is suboptimal in the finite blocklength regime. To address these issues, we propose an adaptive source-channel coding (ASCC) scheme for SemComs over parallel Gaussian channels, where the deep neural network (DNN)-based semantic source coding and conventional digital channel coding are separately deployed and adaptively designed. To enable efficient adaptation between the source and channel coding, we first approximate the E2E data and semantic distortions as functions of source coding rate and bit error ratio (BER) via logistic regression, where BER is further modeled as functions of signal-to-noise ratio (SNR) and channel coding rate. Then, we formulate the weighted sum E2E distortion minimization problem for joint source-channel coding rate and power allocation over parallel channels, which is solved by the successive convex approximation. Finally, simulation results demonstrate that the proposed ASCC scheme outperforms typical deep JSCC and SSCC schemes for both the single- and parallel-channel scenarios while maintaining full compatibility with practical digital systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Likelihood Ratio Tests by Kernel Gaussian Embedding</title>
<link>https://arxiv.org/abs/2508.07982</link>
<guid>https://arxiv.org/abs/2508.07982</guid>
<content:encoded><![CDATA[
arXiv:2508.07982v1 Announce Type: cross 
Abstract: We propose a novel kernel-based nonparametric two-sample test, employing the combined use of kernel mean and kernel covariance embedding. Our test builds on recent results showing how such combined embeddings map distinct probability measures to mutually singular Gaussian measures on the kernel's RKHS. Leveraging this result, we construct a test statistic based on the relative entropy between the Gaussian embeddings, i.e.\ the likelihood ratio. The likelihood ratio is specifically tailored to detect equality versus singularity of two Gaussians, and satisfies a ``$0/\infty$" law, in that it vanishes under the null and diverges under the alternative. To implement the test in finite samples, we introduce a regularised version, calibrated by way of permutation. We prove consistency, establish uniform power guarantees under mild conditions, and discuss how our framework unifies and extends prior approaches based on spectrally regularized MMD. Empirical results on synthetic and real data demonstrate remarkable gains in power compared to state-of-the-art methods, particularly in high-dimensional and weak-signal regimes.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharper Perturbed-Kullback-Leibler Exponential Tail Bounds for Beta and Dirichlet Distributions</title>
<link>https://arxiv.org/abs/2508.07991</link>
<guid>https://arxiv.org/abs/2508.07991</guid>
<content:encoded><![CDATA[
arXiv:2508.07991v1 Announce Type: cross 
Abstract: This paper presents an improved exponential tail bound for Beta distributions, refining a result in [15]. This improvement is achieved by interpreting their bound as a regular Kullback-Leibler (KL) divergence one, while introducing a specific perturbation $\eta$ that shifts the mean of the Beta distribution closer to zero within the KL bound. Our contribution is to show that a larger perturbation can be chosen, thereby tightening the bound. We then extend this result from the Beta distribution to Dirichlet distributions and Dirichlet processes (DPs).
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction error certification for PINNs: Theory, computation, and application to Stokes flow</title>
<link>https://arxiv.org/abs/2508.07994</link>
<guid>https://arxiv.org/abs/2508.07994</guid>
<content:encoded><![CDATA[
arXiv:2508.07994v1 Announce Type: cross 
Abstract: Rigorous error estimation is a fundamental topic in numerical analysis. With the increasing use of physics-informed neural networks (PINNs) for solving partial differential equations, several approaches have been developed to quantify the associated prediction error. In this work, we build upon a semigroup-based framework previously introduced by the authors for estimating the PINN error. While this estimator has so far been limited to academic examples - due to the need to compute quantities related to input-to-state stability - we extend its applicability to a significantly broader class of problems. This is accomplished by modifying the error bound and proposing numerical strategies to approximate the required stability parameters. The extended framework enables the certification of PINN predictions in more realistic scenarios, as demonstrated by a numerical study of Stokes flow around a cylinder.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Knowledge Tracing by Exploring Follow-up Performance Trends</title>
<link>https://arxiv.org/abs/2508.08019</link>
<guid>https://arxiv.org/abs/2508.08019</guid>
<content:encoded><![CDATA[
arXiv:2508.08019v1 Announce Type: cross 
Abstract: Intelligent Tutoring Systems (ITS), such as Massive Open Online Courses, offer new opportunities for human learning. At the core of such systems, knowledge tracing (KT) predicts students' future performance by analyzing their historical learning activities, enabling an accurate evaluation of students' knowledge states over time. We show that existing KT methods often encounter correlation conflicts when analyzing the relationships between historical learning sequences and future performance. To address such conflicts, we propose to extract so-called Follow-up Performance Trends (FPTs) from historical ITS data and to incorporate them into KT. We propose a method called Forward-Looking Knowledge Tracing (FINER) that combines historical learning sequences with FPTs to enhance student performance prediction accuracy. FINER constructs learning patterns that facilitate the retrieval of FPTs from historical ITS data in linear time; FINER includes a novel similarity-aware attention mechanism that aggregates FPTs based on both frequency and contextual similarity; and FINER offers means of combining FPTs and historical learning sequences to enable more accurate prediction of student future performance. Experiments on six real-world datasets show that FINER can outperform ten state-of-the-art KT methods, increasing accuracy by 8.74% to 84.85%.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids</title>
<link>https://arxiv.org/abs/2508.08022</link>
<guid>https://arxiv.org/abs/2508.08022</guid>
<content:encoded><![CDATA[
arXiv:2508.08022v1 Announce Type: cross 
Abstract: Real-time monitoring of power consumption in cities and micro-grids through the Internet of Things (IoT) can help forecast future demand and optimize grid operations. But moving all consumer-level usage data to the cloud for predictions and analysis at fine time scales can expose activity patterns. Federated Learning~(FL) is a privacy-sensitive collaborative DNN training approach that retains data on edge devices, trains the models on private data locally, and aggregates the local models in the cloud. But key challenges exist: (i) clients can have non-independently identically distributed~(non-IID) data, and (ii) the learning should be computationally cheap while scaling to 1000s of (unseen) clients. In this paper, we develop and evaluate several optimizations to FL training across edge and cloud for time-series demand forecasting in micro-grids and city-scale utilities using DNNs to achieve a high prediction accuracy while minimizing the training cost. We showcase the benefit of using exponentially weighted loss while training and show that it further improves the prediction of the final model. Finally, we evaluate these strategies by validating over 1000s of clients for three states in the US from the OpenEIA corpus, and performing FL both in a pseudo-distributed setting and a Pi edge cluster. The results highlight the benefits of the proposed methods over baselines like ARIMA and DNNs trained for individual consumers, which are not scalable.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks</title>
<link>https://arxiv.org/abs/2508.08029</link>
<guid>https://arxiv.org/abs/2508.08029</guid>
<content:encoded><![CDATA[
arXiv:2508.08029v1 Announce Type: cross 
Abstract: The introduction of 5G and the Open Radio Access Network (O-RAN) architecture has enabled more flexible and intelligent network deployments. However, the increased complexity and openness of these architectures also introduce novel security challenges, such as data manipulation attacks on the semi-standardised Shared Data Layer (SDL) within the O-RAN platform through malicious xApps. In particular, malicious xApps can exploit this vulnerability by introducing subtle Unicode-wise alterations (hypoglyphs) into the data that are being used by traditional machine learning (ML)-based anomaly detection methods. These Unicode-wise manipulations can potentially bypass detection and cause failures in anomaly detection systems based on traditional ML, such as AutoEncoders, which are unable to process hypoglyphed data without crashing. We investigate the use of Large Language Models (LLMs) for anomaly detection within the O-RAN architecture to address this challenge. We demonstrate that LLM-based xApps maintain robust operational performance and are capable of processing manipulated messages without crashing. While initial detection accuracy requires further improvements, our results highlight the robustness of LLMs to adversarial attacks such as hypoglyphs in input data. There is potential to use their adaptability through prompt engineering to further improve the accuracy, although this requires further research. Additionally, we show that LLMs achieve low detection latency (under 0.07 seconds), making them suitable for Near-Real-Time (Near-RT) RIC deployments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Strategies for Personalized Radiation Therapy: Part III Identifying genetic determinants for Radiation Response with Meta Learning</title>
<link>https://arxiv.org/abs/2508.08030</link>
<guid>https://arxiv.org/abs/2508.08030</guid>
<content:encoded><![CDATA[
arXiv:2508.08030v1 Announce Type: cross 
Abstract: Radiation response in cancer is shaped by complex, patient specific biology, yet current treatment strategies often rely on uniform dose prescriptions without accounting for tumor heterogeneity. In this study, we introduce a meta learning framework for one-shot prediction of radiosensitivity measured by SF2 using cell line level gene expression data. Unlike the widely used Radiosensitivity Index RSI a rank-based linear model trained on a fixed 10-gene signature, our proposed meta-learned model allows the importance of each gene to vary by sample through fine tuning. This flexibility addresses key limitations of static models like RSI, which assume uniform gene contributions across tumor types and discard expression magnitude and gene gene interactions. Our results show that meta learning offers robust generalization to unseen samples and performs well in tumor subgroups with high radiosensitivity variability, such as adenocarcinoma and large cell carcinoma. By learning transferable structure across tasks while preserving sample specific adaptability, our approach enables rapid adaptation to individual samples, improving predictive accuracy across diverse tumor subtypes while uncovering context dependent patterns of gene influence that may inform personalized therapy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI</title>
<link>https://arxiv.org/abs/2508.08058</link>
<guid>https://arxiv.org/abs/2508.08058</guid>
<content:encoded><![CDATA[
arXiv:2508.08058v1 Announce Type: cross 
Abstract: Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often degrades image quality. While Implicit Neural Representations (INRs) show promise for MRI reconstruction, they struggle at high acceleration factors due to weak prior constraints, leading to structural loss and aliasing artefacts. To address this, we propose PrIINeR, an INR-based MRI reconstruction method that integrates prior knowledge from pre-trained deep learning models into the INR framework. By combining population-level knowledge with instance-based optimization and enforcing dual data consistency, PrIINeR aligns both with the acquired k-space data and the prior-informed reconstruction. Evaluated on the NYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based approaches but also improves upon several learning-based state-of-the-art methods, significantly improving structural preservation and fidelity while effectively removing aliasing artefacts.PrIINeR bridges deep learning and INR-based techniques, offering a more reliable solution for high-quality, accelerated MRI reconstruction. The code is publicly available on https://github.com/multimodallearning/PrIINeR.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Design Space of Visual Grounding in Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2508.08066</link>
<guid>https://arxiv.org/abs/2508.08066</guid>
<content:encoded><![CDATA[
arXiv:2508.08066v1 Announce Type: cross 
Abstract: Fine-grained multimodal capability in Multimodal Large Language Models (MLLMs) has emerged as a critical research direction, particularly for tackling the visual grounding (VG) problem. Despite the strong performance achieved by existing approaches, they often employ disparate design choices when fine-tuning MLLMs for VG, lacking systematic verification to support these designs. To bridge this gap, this paper presents a comprehensive study of various design choices that impact the VG performance of MLLMs. We conduct our analysis using LLaVA-1.5, which has been widely adopted in prior empirical studies of MLLMs. While more recent models exist, we follow this convention to ensure our findings remain broadly applicable and extendable to other architectures. We cover two key aspects: (1) exploring different visual grounding paradigms in MLLMs, identifying the most effective design, and providing our insights; and (2) conducting ablation studies on the design of grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our findings contribute to a stronger MLLM for VG, achieving improvements of +5.6% / +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDD-Net: Multimodal Depression Detection through Mutual Transformer</title>
<link>https://arxiv.org/abs/2508.08093</link>
<guid>https://arxiv.org/abs/2508.08093</guid>
<content:encoded><![CDATA[
arXiv:2508.08093v1 Announce Type: cross 
Abstract: Depression is a major mental health condition that severely impacts the emotional and physical well-being of individuals. The simple nature of data collection from social media platforms has attracted significant interest in properly utilizing this information for mental health research. A Multimodal Depression Detection Network (MDD-Net), utilizing acoustic and visual data obtained from social media networks, is proposed in this work where mutual transformers are exploited to efficiently extract and fuse multimodal features for efficient depression detection. The MDD-Net consists of four core modules: an acoustic feature extraction module for retrieving relevant acoustic attributes, a visual feature extraction module for extracting significant high-level patterns, a mutual transformer for computing the correlations among the generated features and fusing these features from multiple modalities, and a detection layer for detecting depression using the fused feature representations. The extensive experiments are performed using the multimodal D-Vlog dataset, and the findings reveal that the developed multimodal depression detection network surpasses the state-of-the-art by up to 17.37% for F1-Score, demonstrating the greater performance of the proposed system. The source code is accessible at https://github.com/rezwanh001/Multimodal-Depression-Detection.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?</title>
<link>https://arxiv.org/abs/2508.08096</link>
<guid>https://arxiv.org/abs/2508.08096</guid>
<content:encoded><![CDATA[
arXiv:2508.08096v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) and their increased accessibility have made it easier than ever for students to automatically generate texts, posing new challenges for educational institutions. To enforce norms of academic integrity and ensure students' learning, learning analytics methods to automatically detect LLM-generated text appear increasingly appealing. This paper benchmarks the performance of different state-of-the-art detectors in educational contexts, introducing a novel dataset, called Generative Essay Detection in Education (GEDE), containing over 900 student-written essays and over 12,500 LLM-generated essays from various domains. To capture the diversity of LLM usage practices in generating text, we propose the concept of contribution levels, representing students' contribution to a given assignment. These levels range from purely human-written texts, to slightly LLM-improved versions, to fully LLM-generated texts, and finally to active attacks on the detector by "humanizing" generated texts. We show that most detectors struggle to accurately classify texts of intermediate student contribution levels, like LLM-improved human-written texts. Detectors are particularly likely to produce false positives, which is problematic in educational settings where false suspicions can severely impact students' lives. Our dataset, code, and additional supplementary materials are publicly available at https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An effective potential for generative modelling with active matter</title>
<link>https://arxiv.org/abs/2508.08146</link>
<guid>https://arxiv.org/abs/2508.08146</guid>
<content:encoded><![CDATA[
arXiv:2508.08146v1 Announce Type: cross 
Abstract: Score-based diffusion models generate samples from a complex underlying data distribution by time-reversal of a diffusion process and represent the state-of-the-art in many generative AI applications such as artificial image synthesis. Here, I show how a generative diffusion model can be implemented based on an underlying active particle process with finite correlation time. In contrast to previous approaches that use a score function acting on the velocity coordinate of the active particle, time reversal is here achieved by imposing an effective time-dependent potential on the position coordinate only. The effective potential is valid to first order in the persistence time and leads to a force field that is fully determined by the standard score function and its derivatives up to 2nd order. Numerical experiments for artificial data distributions confirm the validity of the effective potential.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo</title>
<link>https://arxiv.org/abs/2508.08163</link>
<guid>https://arxiv.org/abs/2508.08163</guid>
<content:encoded><![CDATA[
arXiv:2508.08163v1 Announce Type: cross 
Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model annotator disagreement through soft label distribution prediction and perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution from Context), a neural architecture that jointly models item-level and annotator-level label distributions, and present detailed analysis and improvements. In this paper, we extend the DisCo by incorporating annotator metadata, enhancing input representations, and modifying the loss functions to capture disagreement patterns better. Through extensive experiments, we demonstrate substantial improvements in both soft and perspectivist evaluation metrics across three datasets. We also conduct in-depth error and calibration analyses, highlighting the conditions under which improvements occur. Our findings underscore the value of disagreement-aware modeling and offer insights into how system components interact with the complexity of human-annotated data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2508.08165</link>
<guid>https://arxiv.org/abs/2508.08165</guid>
<content:encoded><![CDATA[
arXiv:2508.08165v1 Announce Type: cross 
Abstract: Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Existing pre-trained model-based CIL methods often freeze the pre-trained network and adapt to incremental tasks using additional lightweight modules such as adapters. However, incorrect module selection during inference hurts performance, and task-specific modules often overlook shared general knowledge, leading to errors on distinguishing between similar classes across tasks. To address the aforementioned challenges, we propose integrating Task-Specific and Universal Adapters (TUNA) in this paper. Specifically, we train task-specific adapters to capture the most crucial features relevant to their respective tasks and introduce an entropy-based selection mechanism to choose the most suitable adapter. Furthermore, we leverage an adapter fusion strategy to construct a universal adapter, which encodes the most discriminative features shared across tasks. We combine task-specific and universal adapter predictions to harness both specialized and general knowledge during inference. Extensive experiments on various benchmark datasets demonstrate the state-of-the-art performance of our approach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers</title>
<link>https://arxiv.org/abs/2508.08206</link>
<guid>https://arxiv.org/abs/2508.08206</guid>
<content:encoded><![CDATA[
arXiv:2508.08206v1 Announce Type: cross 
Abstract: We propose a joint learning framework for Byzantine-resilient spectrum sensing and secure intelligent reflecting surface (IRS)--assisted opportunistic access under channel state information (CSI) uncertainty. The sensing stage performs logit-domain Bayesian updates with trimmed aggregation and attention-weighted consensus, and the base station (BS) fuses network beliefs with a conservative minimum rule, preserving detection accuracy under a bounded number of Byzantine users. Conditioned on the sensing outcome, we pose downlink design as sum mean-squared error (MSE) minimization under transmit-power and signal-leakage constraints and jointly optimize the BS precoder, IRS phase shifts, and user equalizers. With partial (or known) CSI, we develop an augmented-Lagrangian alternating algorithm with projected updates and provide provable sublinear convergence, with accelerated rates under mild local curvature. With unknown CSI, we perform constrained Bayesian optimization (BO) in a geometry-aware low-dimensional latent space using Gaussian process (GP) surrogates; we prove regret bounds for a constrained upper confidence bound (UCB) variant of the BO module, and demonstrate strong empirical performance of the implemented procedure. Simulations across diverse network conditions show higher detection probability at fixed false-alarm rate under adversarial attacks, large reductions in sum MSE for honest users, strong suppression of eavesdropper signal power, and fast convergence. The framework offers a practical path to secure opportunistic communication that adapts to CSI availability while coherently coordinating sensing and transmission through joint learning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling</title>
<link>https://arxiv.org/abs/2508.08211</link>
<guid>https://arxiv.org/abs/2508.08211</guid>
<content:encoded><![CDATA[
arXiv:2508.08211v1 Announce Type: cross 
Abstract: Watermarking LLM-generated text is critical for content attribution and misinformation prevention. However, existing methods compromise text quality, require white-box model access and logit manipulation. These limitations exclude API-based models and multilingual scenarios. We propose SAEMark, a general framework for post-hoc multi-bit watermarking that embeds personalized messages solely via inference-time, feature-based rejection sampling without altering model logits or requiring training. Our approach operates on deterministic features extracted from generated text, selecting outputs whose feature statistics align with key-derived targets. This framework naturally generalizes across languages and domains while preserving text quality through sampling LLM outputs instead of modifying. We provide theoretical guarantees relating watermark success probability and compute budget that hold for any suitable feature extractor. Empirically, we demonstrate the framework's effectiveness using Sparse Autoencoders (SAEs), achieving superior detection accuracy and text quality. Experiments across 4 datasets show SAEMark's consistent performance, with 99.7% F1 on English and strong multi-bit detection accuracy. SAEMark establishes a new paradigm for scalable watermarking that works out-of-the-box with closed-source LLMs while enabling content attribution.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fed-TGAN: Federated Learning Framework for Synthesizing Tabular Data</title>
<link>https://arxiv.org/abs/2108.07927</link>
<guid>https://arxiv.org/abs/2108.07927</guid>
<content:encoded><![CDATA[
arXiv:2108.07927v2 Announce Type: replace 
Abstract: Generative Adversarial Networks (GANs) are typically trained to synthesize data, from images and more recently tabular data, under the assumption of directly accessible training data. Recently, federated learning (FL) is an emerging paradigm that features decentralized learning on client's local data with a privacy-preserving capability. And, while learning GANs to synthesize images on FL systems has just been demonstrated, it is unknown if GANs for tabular data can be learned from decentralized data sources. Moreover, it remains unclear which distributed architecture suits them best. Different from image GANs, state-of-the-art tabular GANs require prior knowledge on the data distribution of each (discrete and continuous) column to agree on a common encoding -- risking privacy guarantees. In this paper, we propose Fed-TGAN, the first Federated learning framework for Tabular GANs. To effectively learn a complex tabular GAN on non-identical participants, Fed-TGAN designs two novel features: (i) a privacy-preserving multi-source feature encoding for model initialization; and (ii) table similarity aware weighting strategies to aggregate local models for countering data skew. We extensively evaluate the proposed Fed-TGAN against variants of decentralized learning architectures on four widely used datasets. Results show that Fed-TGAN accelerates training time per epoch up to 200% compared to the alternative architectures, for both IID and Non-IID data. Overall, Fed-TGAN not only stabilizes the training loss, but also achieves better similarity between generated and original data. Our code is released at https://github.com/zhao-zilong/Fed-TGAN.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOInter: A Novel Deep Energy Based Interpretation Method for Explaining Structured Output Models</title>
<link>https://arxiv.org/abs/2202.09914</link>
<guid>https://arxiv.org/abs/2202.09914</guid>
<content:encoded><![CDATA[
arXiv:2202.09914v2 Announce Type: replace 
Abstract: We propose a novel interpretation technique to explain the behavior of structured output models, which learn mappings between an input vector to a set of output variables simultaneously. Because of the complex relationship between the computational path of output variables in structured models, a feature can affect the value of output through other ones. We focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space. In this paper, we assume an arbitrary structured output model is available as a black box and argue how considering the correlations between output variables can improve the explanation performance. The goal is to train a function as an interpreter for the target output variable over the input space. We introduce an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained. The effectiveness of the proposed method is confirmed using a variety of simulated and real data sets.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Contextual Preferential Bayesian Optimization with Historical Examples</title>
<link>https://arxiv.org/abs/2208.10300</link>
<guid>https://arxiv.org/abs/2208.10300</guid>
<content:encoded><![CDATA[
arXiv:2208.10300v3 Announce Type: replace 
Abstract: State-of-the-art multi-objective optimization often assumes a known utility function, learns it interactively, or computes the full Pareto front-each requiring costly expert input.~Real-world problems, however, involve implicit preferences that are hard to formalize. To reduce expert involvement, we propose an offline, interpretable utility learning method that uses expert knowledge, historical examples, and coarse information about the utility space to reduce sample requirements. We model uncertainty via a full Bayesian posterior and propagate it throughout the optimization process. Our method outperforms standard Gaussian processes and BOPE across four domains, showing strong performance even with biased samples, as encountered in the real-world, and limited expert input.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaBoost is not an Optimal Weak to Strong Learner</title>
<link>https://arxiv.org/abs/2301.11571</link>
<guid>https://arxiv.org/abs/2301.11571</guid>
<content:encoded><![CDATA[
arXiv:2301.11571v2 Announce Type: replace 
Abstract: AdaBoost is a classic boosting algorithm for combining multiple inaccurate classifiers produced by a weak learner, to produce a strong learner with arbitrarily high accuracy when given enough training data. Determining the optimal number of samples necessary to obtain a given accuracy of the strong learner, is a basic learning theoretic question. Larsen and Ritzert (NeurIPS'22) recently presented the first provably optimal weak-to-strong learner. However, their algorithm is somewhat complicated and it remains an intriguing question whether the prototypical boosting algorithm AdaBoost also makes optimal use of training samples. In this work, we answer this question in the negative. Concretely, we show that the sample complexity of AdaBoost, and other classic variations thereof, are sub-optimal by at least one logarithmic factor in the desired accuracy of the strong learner.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Policy Improvement from Multiple Black-box Oracles</title>
<link>https://arxiv.org/abs/2306.10259</link>
<guid>https://arxiv.org/abs/2306.10259</guid>
<content:encoded><![CDATA[
arXiv:2306.10259v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) has made significant strides in various complex domains. However, identifying an effective policy via RL often necessitates extensive exploration. Imitation learning aims to mitigate this issue by using expert demonstrations to guide exploration. In real-world scenarios, one often has access to multiple suboptimal black-box experts, rather than a single optimal oracle. These experts do not universally outperform each other across all states, presenting a challenge in actively deciding which oracle to use and in which state. We introduce MAPS and MAPS-SE, a class of policy improvement algorithms that perform imitation learning from multiple suboptimal oracles. In particular, MAPS actively selects which of the oracles to imitate and improve their value function estimates, and MAPS-SE additionally leverages an active state exploration criterion to determine which states one should explore. We provide a comprehensive theoretical analysis and demonstrate that MAPS and MAPS-SE enjoy sample efficiency advantage over the state-of-the-art policy improvement algorithms. Empirical results show that MAPS-SE significantly accelerates policy optimization via state-wise imitation learning from multiple oracles across a broad spectrum of control tasks in the DeepMind Control Suite. Our code is publicly available at: https://github.com/ripl/maps.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blending Imitation and Reinforcement Learning for Robust Policy Improvement</title>
<link>https://arxiv.org/abs/2310.01737</link>
<guid>https://arxiv.org/abs/2310.01737</guid>
<content:encoded><![CDATA[
arXiv:2310.01737v3 Announce Type: replace 
Abstract: While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the oracles or learn from its own value function when the learner's performance surpasses that of the oracles in a specific state. Empirical evaluations and theoretical analysis validate that RPI excels in comparison to existing state-of-the-art methodologies, demonstrating superior performance across various benchmark domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization</title>
<link>https://arxiv.org/abs/2311.13171</link>
<guid>https://arxiv.org/abs/2311.13171</guid>
<content:encoded><![CDATA[
arXiv:2311.13171v2 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) techniques make it possible to efficiently adapt a language model to create "expert" models that specialize to new tasks or domains. Recent techniques in model merging and compositional generalization leverage these expert models by dynamically composing modules to improve zero/few-shot generalization. Despite the efficiency of PEFT methods, the size of expert models can make it onerous to retrieve expert models per query over high-latency networks like the Internet or serve multiple experts on a single GPU. To address these issues, we present ComPEFT, a novel method for compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT employs sparsification and ternary quantization to reduce the size of the PEFT module without performing any additional retraining while preserving or enhancing model performance. In extensive evaluation across T5, T0, and LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale - stronger models exhibit higher compressibility and better performance. For example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on MMLU with a storage size reduction of up to 26x. In addition, we show that the compressed experts produced by ComPEFT maintain few-shot compositional generalization capabilities, facilitate efficient communication and computation, and exhibit enhanced performance when merged. Lastly, we provide an analysis of different method components, compare it with other PEFT methods, and test ComPEFT's efficacy for compressing the residual of full-finetuning. Our code is available at https://github.com/prateeky2806/compeft.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Multi-Distribution Learning</title>
<link>https://arxiv.org/abs/2312.05134</link>
<guid>https://arxiv.org/abs/2312.05134</guid>
<content:encoded><![CDATA[
arXiv:2312.05134v5 Announce Type: replace 
Abstract: Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension d, we propose a novel algorithm that yields an varepsilon-optimal randomized hypothesis with a sample complexity on the order of (d+k)/varepsilon^2 (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory are further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely through an empirical risk minimization oracle.
  Additionally, we establish the necessity of randomization, revealing a large sample size barrier when only deterministic hypotheses are permitted. These findings resolve three open problems presented in COLT 2023 (i.e., citet[Problems 1, 3 and 4]{awasthi2023sample}).
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Variational Student-t Processes</title>
<link>https://arxiv.org/abs/2312.05568</link>
<guid>https://arxiv.org/abs/2312.05568</guid>
<content:encoded><![CDATA[
arXiv:2312.05568v2 Announce Type: replace 
Abstract: The theory of Bayesian learning incorporates the use of Student-t Processes to model heavy-tailed distributions and datasets with outliers. However, despite Student-t Processes having a similar computational complexity as Gaussian Processes, there has been limited emphasis on the sparse representation of this model. This is mainly due to the increased difficulty in modeling and computation compared to previous sparse Gaussian Processes. Our motivation is to address the need for a sparse representation framework that reduces computational complexity, allowing Student-t Processes to be more flexible for real-world datasets. To achieve this, we leverage the conditional distribution of Student-t Processes to introduce sparse inducing points. Bayesian methods and variational inference are then utilized to derive a well-defined lower bound, facilitating more efficient optimization of our model through stochastic gradient descent. We propose two methods for computing the variational lower bound, one utilizing Monte Carlo sampling and the other employing Jensen's inequality to compute the KL regularization term in the loss function. We propose adopting these approaches as viable alternatives to Gaussian processes when the data might contain outliers or exhibit heavy-tailed behavior, and we provide specific recommendations for their applicability. We evaluate the two proposed approaches on various synthetic and real-world datasets from UCI and Kaggle, demonstrating their effectiveness compared to baseline methods in terms of computational complexity and accuracy, as well as their robustness to outliers.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monte Carlo with kernel-based Gibbs measures: Guarantees for probabilistic herding</title>
<link>https://arxiv.org/abs/2402.11736</link>
<guid>https://arxiv.org/abs/2402.11736</guid>
<content:encoded><![CDATA[
arXiv:2402.11736v2 Announce Type: replace 
Abstract: Kernel herding belongs to a family of deterministic quadratures that seek to minimize the worst-case integration error over a reproducing kernel Hilbert space (RKHS). These quadrature rules come with strong experimental evidence that this worst-case error decreases at a faster rate than the standard square root of the number of quadrature nodes. This conjectured fast rate is key for integrating expensive-to-evaluate functions, as in Bayesian inference of expensive models, and makes up for the increased computational cost of sampling, compared to i.i.d. or MCMC quadratures. However, there is little theoretical support for this faster-than-square-root rate, at least in the usual case where the RKHS is infinite-dimensional, while recent progress on distribution compression suggests that results on the direct minimization of worst-case integration are possible. In this paper, we study a joint probability distribution over quadrature nodes, whose support tends to minimize the same worst-case error as kernel herding. Our main contribution is to prove that it does outperform i.i.d Monte Carlo, in the sense of coming with a tighter concentration inequality on the worst-case integration error. This first step towards proving a fast error decay demonstrates that the mathematical toolbox developed around Gibbs measures can help understand to what extent kernel herding and its variants improve on computationally cheaper methods. Moreover, we investigate the computational bottlenecks of approximately sampling our quadrature, and we demonstrate on toy examples that a faster rate of convergence, though not worst-case, is likely.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2404.07826</link>
<guid>https://arxiv.org/abs/2404.07826</guid>
<content:encoded><![CDATA[
arXiv:2404.07826v2 Announce Type: replace 
Abstract: The use of Potential-Based Reward Shaping (PBRS) has shown great promise in the ongoing research effort to tackle sample inefficiency in Reinforcement Learning (RL). However, choosing the right potential function remains an open challenge. Additionally, RL techniques are usually constrained to use a finite horizon for computational limitations, which introduces a bias when using PBRS. In this paper, we first build some theoretically-grounded intuition on why selecting the potential function as the optimal value function of the task at hand produces performance advantages. We then analyse the bias induced by finite horizons in the context of PBRS producing novel insights. Finally, leveraging abstractions as a way to approximate the optimal value function of the given task, we assess the sample efficiency and performance impact of PBRS on four environments including a goal-oriented navigation task and three Arcade Learning Environments (ALE) games. Remarkably, experimental results show that we can reach the same level of performance as CNN-based solutions with a simple fully-connected network.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Runtime Monitoring and Enforcement of Conditional Fairness in Generative AIs</title>
<link>https://arxiv.org/abs/2404.16663</link>
<guid>https://arxiv.org/abs/2404.16663</guid>
<content:encoded><![CDATA[
arXiv:2404.16663v5 Announce Type: replace 
Abstract: The deployment of generative AI (GenAI) models raises significant fairness concerns, addressed in this paper through novel characterization and enforcement techniques specific to GenAI. Unlike standard AI performing specific tasks, GenAI's broad functionality requires ``conditional fairness'' tailored to the context being generated, such as demographic fairness in generating images of poor people versus successful business leaders. We define two fairness levels: the first evaluates fairness in generated outputs, independent of prompts and models; the second assesses inherent fairness with neutral prompts. Given the complexity of GenAI and challenges in fairness specifications, we focus on bounding the worst case, considering a GenAI system unfair if the distance between appearances of a specific group exceeds preset thresholds. We also explore combinatorial testing for assessing relative completeness in intersectional fairness. By bounding the worst case, we develop a prompt injection scheme within an agent-based framework to enforce conditional fairness with minimal intervention, validated on state-of-the-art GenAI systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MS-IMAP -- A Multi-Scale Graph Embedding Approach for Interpretable Manifold Learning</title>
<link>https://arxiv.org/abs/2406.02778</link>
<guid>https://arxiv.org/abs/2406.02778</guid>
<content:encoded><![CDATA[
arXiv:2406.02778v5 Announce Type: replace 
Abstract: Deriving meaningful representations from complex, high-dimensional data in unsupervised settings is crucial across diverse machine learning applications. This paper introduces a framework for multi-scale graph network embedding based on spectral graph wavelets that employs a contrastive learning approach. We theoretically show that in Paley-Wiener spaces on combinatorial graphs, the spectral graph wavelets operator provides greater flexibility and control over smoothness compared to the Laplacian operator, motivating our approach. A key advantage of the proposed embedding is its ability to establish a correspondence between the embedding and input feature spaces, enabling the derivation of feature importance. We validate the effectiveness of our graph embedding framework on multiple public datasets across various downstream tasks, including clustering and unsupervised feature importance.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Spikes to Heavy Tails: Unveiling the Spectral Evolution of Neural Networks</title>
<link>https://arxiv.org/abs/2406.04657</link>
<guid>https://arxiv.org/abs/2406.04657</guid>
<content:encoded><![CDATA[
arXiv:2406.04657v3 Announce Type: replace 
Abstract: Training strategies for modern deep neural networks (NNs) tend to induce a heavy-tailed (HT) empirical spectral density (ESD) in the layer weights. While previous efforts have shown that the HT phenomenon correlates with good generalization in large NNs, a theoretical explanation of its occurrence is still lacking. Especially, understanding the conditions which lead to this phenomenon can shed light on the interplay between generalization and weight spectra. Our work aims to bridge this gap by presenting a simple, rich setting to model the emergence of HT ESD. In particular, we present a theory-informed setup for 'crafting' heavy tails in the ESD of two-layer NNs and present a systematic analysis of the HT ESD emergence without any gradient noise. This is the first work to analyze a noise-free setting, and we also incorporate optimizer (GD/Adam) dependent (large) learning rates into the HT ESD analysis. Our results highlight the role of learning rates on the Bulk+Spike and HT shape of the ESDs in the early phase of training, which can facilitate generalization in the two-layer NN. These observations shed light on the behavior of large-scale NNs, albeit in a much simpler setting.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning</title>
<link>https://arxiv.org/abs/2408.07057</link>
<guid>https://arxiv.org/abs/2408.07057</guid>
<content:encoded><![CDATA[
arXiv:2408.07057v2 Announce Type: replace 
Abstract: The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to a particular domain or task. Model MoErging methods aim to recycle expert models to create an aggregate system with improved performance or generalization. A key component of MoErging methods is the creation of a router that decides which expert model(s) to use for a particular input or application. The promise, effectiveness, and large design space of MoErging has spurred the development of many new methods over the past few years. This rapid pace of development has made it challenging to compare different MoErging methods, which are rarely compared to one another and are often validated in different experimental setups. To remedy such gaps, we present a comprehensive survey of MoErging methods that includes a novel taxonomy for cataloging key design choices and clarifying suitable applications for each method. Apart from surveying MoErging research, we inventory software tools and applications that make use of MoErging. We additionally discuss related fields of study such as model merging, multitask learning, and mixture-of-experts models. Taken as a whole, our survey provides a unified overview of existing MoErging methods and creates a solid foundation for future work in this burgeoning field.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-Directed Score-Based Diffusion Models via q-Learning</title>
<link>https://arxiv.org/abs/2409.04832</link>
<guid>https://arxiv.org/abs/2409.04832</guid>
<content:encoded><![CDATA[
arXiv:2409.04832v2 Announce Type: replace 
Abstract: We propose a new reinforcement learning (RL) formulation for training continuous-time score-based diffusion models for generative AI to generate samples that maximize reward functions while keeping the generated distributions close to the unknown target data distributions. Different from most existing studies, ours does not involve any pretrained model for the unknown score functions of the noise-perturbed data distributions, nor does it attempt to learn the score functions. Instead, we formulate the problem as entropy-regularized continuous-time RL and show that the optimal stochastic policy has a Gaussian distribution with a known covariance matrix. Based on this result, we parameterize the mean of Gaussian policies and develop an actor--critic type (little) q-learning algorithm to solve the RL problem. A key ingredient in our algorithm design is to obtain noisy observations from the unknown score function via a ratio estimator. Our formulation can also be adapted to solve pure score-matching and fine-tuning pretrained models. Numerically, we show the effectiveness of our approach by comparing its performance with two state-of-the-art RL methods that fine-tune pretrained models on several generative tasks including high-dimensional image generations. Finally, we discuss extensions of our RL formulation to probability flow ODE implementation of diffusion models and to conditional diffusion models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection</title>
<link>https://arxiv.org/abs/2410.09103</link>
<guid>https://arxiv.org/abs/2410.09103</guid>
<content:encoded><![CDATA[
arXiv:2410.09103v2 Announce Type: replace 
Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UoMo: A Universal Model of Mobile Traffic Forecasting for Wireless Network Optimization</title>
<link>https://arxiv.org/abs/2410.15322</link>
<guid>https://arxiv.org/abs/2410.15322</guid>
<content:encoded><![CDATA[
arXiv:2410.15322v4 Announce Type: replace 
Abstract: Mobile traffic forecasting allows operators to anticipate network dynamics and performance in advance, offering substantial potential for enhancing service quality and improving user experience. However, existing models are often task-oriented and are trained with tailored data, which limits their effectiveness in diverse mobile network tasks of Base Station (BS) deployment, resource allocation, energy optimization, etc. and hinders generalization across different urban environments. Foundation models have made remarkable strides across various domains of NLP and CV due to their multi-tasking adaption and zero/few-shot learning capabilities. In this paper, we propose an innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to handle diverse forecasting tasks of short/long-term predictions and distribution generation across multiple cities to support network planning and optimization. FoMo combines diffusion models and transformers, where various spatio-temporal masks are proposed to enable FoMo to learn intrinsic features of different tasks, and a contrastive learning strategy is developed to capture the correlations between mobile traffic and urban contexts, thereby improving its transfer learning capability. Extensive experiments on 9 real-world datasets demonstrate that FoMo outperforms current models concerning diverse forecasting tasks and zero/few-shot learning, showcasing a strong universality.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADAM-SINDy: An Efficient Optimization Framework for Parameterized Nonlinear Dynamical System Identification</title>
<link>https://arxiv.org/abs/2410.16528</link>
<guid>https://arxiv.org/abs/2410.16528</guid>
<content:encoded><![CDATA[
arXiv:2410.16528v3 Announce Type: replace 
Abstract: Identifying dynamical systems characterized by nonlinear parameters presents significant challenges in deriving mathematical models that enhance understanding of physics. Traditional methods, such as Sparse Identification of Nonlinear Dynamics (SINDy) and symbolic regression, can extract governing equations from observational data; however, they also come with distinct advantages and disadvantages. This paper introduces a novel method within the SINDy framework, termed ADAM-SINDy, which synthesizes the strengths of established approaches by employing the ADAM optimization algorithm. This facilitates the simultaneous optimization of nonlinear parameters and coefficients associated with nonlinear candidate functions, enabling precise parameter estimation without requiring prior knowledge of nonlinear characteristics such as trigonometric frequencies, exponential bandwidths, or polynomial exponents, thereby addressing a key limitation of SINDy. Through an integrated global optimization, ADAM-SINDy dynamically adjusts all unknown variables in response to data, resulting in an adaptive identification procedure that reduces the sensitivity to the library of candidate functions. The performance of the ADAM-SINDy methodology is demonstrated across a spectrum of dynamical systems, including benchmark coupled nonlinear ordinary differential equations such as oscillators, chaotic fluid flows, reaction kinetics, pharmacokinetics, as well as nonlinear partial differential equations (wildfire transport). The results demonstrate significant improvements in identifying parameterized dynamical systems and underscore the importance of concurrently optimizing all parameters, particularly those characterized by nonlinear parameters. These findings highlight the potential of ADAM-SINDy to extend the applicability of the SINDy framework in addressing more complex challenges in dynamical system identification.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An information-matching approach to optimal experimental design and active learning</title>
<link>https://arxiv.org/abs/2411.02740</link>
<guid>https://arxiv.org/abs/2411.02740</guid>
<content:encoded><![CDATA[
arXiv:2411.02740v3 Announce Type: replace 
Abstract: The efficacy of mathematical models heavily depends on the quality of the training data, yet collecting sufficient data is often expensive and challenging. Many modeling applications require inferring parameters only as a means to predict other quantities of interest (QoI). Because models often contain many unidentifiable (sloppy) parameters, QoIs often depend on a relatively small number of parameter combinations. Therefore, we introduce an information-matching criterion based on the Fisher Information Matrix to select the most informative training data from a candidate pool. This method ensures that the selected data contain sufficient information to learn only those parameters that are needed to constrain downstream QoIs. It is formulated as a convex optimization problem, making it scalable to large models and datasets. We demonstrate the effectiveness of this approach across various modeling problems in diverse scientific fields, including power systems and underwater acoustics. Finally, we use information-matching as a query function within an Active Learning loop for material science applications. In all these applications, we find that a relatively small set of optimal training data can provide the necessary information for achieving precise predictions. These results are encouraging for diverse future applications, particularly active learning in large machine learning models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sbi reloaded: a toolkit for simulation-based inference workflows</title>
<link>https://arxiv.org/abs/2411.17337</link>
<guid>https://arxiv.org/abs/2411.17337</guid>
<content:encoded><![CDATA[
arXiv:2411.17337v2 Announce Type: replace 
Abstract: Scientists and engineers use simulators to model empirically observed phenomena. However, tuning the parameters of a simulator to ensure its outputs match observed data presents a significant challenge. Simulation-based inference (SBI) addresses this by enabling Bayesian inference for simulators, identifying parameters that match observed data and align with prior knowledge. Unlike traditional Bayesian inference, SBI only needs access to simulations from the model and does not require evaluations of the likelihood function. In addition, SBI algorithms do not require gradients through the simulator, allow for massive parallelization of simulations, and can perform inference for different observations without further simulations or training, thereby amortizing inference. Over the past years, we have developed, maintained, and extended sbi, a PyTorch-based package that implements Bayesian SBI algorithms based on neural networks. The sbi toolkit implements a wide range of inference methods, neural network architectures, sampling methods, and diagnostic tools. In addition, it provides well-tested default settings, but also offers flexibility to fully customize every step of the simulation-based inference workflow. Taken together, the sbi toolkit enables scientists and engineers to apply state-of-the-art SBI methods to black-box simulators, opening up new possibilities for aligning simulations with empirically observed data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Memorization in Generative Models via Sharpness of Probability Landscapes</title>
<link>https://arxiv.org/abs/2412.04140</link>
<guid>https://arxiv.org/abs/2412.04140</guid>
<content:encoded><![CDATA[
arXiv:2412.04140v4 Announce Type: replace 
Abstract: In this paper, we introduce a geometric framework to analyze memorization in diffusion models through the sharpness of the log probability density. We mathematically justify a previously proposed score-difference-based memorization metric by demonstrating its effectiveness in quantifying sharpness. Additionally, we propose a novel memorization metric that captures sharpness at the initial stage of image generation in latent diffusion models, offering early insights into potential memorization. Leveraging this metric, we develop a mitigation strategy that optimizes the initial noise of the generation process using a sharpness-aware regularization term.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\ell_0$-Regularized Quadratic Surface Support Vector Machines</title>
<link>https://arxiv.org/abs/2501.11268</link>
<guid>https://arxiv.org/abs/2501.11268</guid>
<content:encoded><![CDATA[
arXiv:2501.11268v3 Announce Type: replace 
Abstract: Kernel-free quadratic surface support vector machines have recently gained traction due to their flexibility in modeling nonlinear decision boundaries without relying on kernel functions. However, the introduction of a full quadratic classifier significantly increases the number of model parameters, scaling quadratically with data dimensionality, which often leads to overfitting and makes interpretation difficult. To address these challenges, we propose a sparse variant of the QSVM by enforcing a cardinality constraint on the model parameters. While enhancing generalization and promoting sparsity, leveraging the $\ell_0$-norm inevitably incurs additional computational complexity. To tackle this, we develop a penalty decomposition algorithm capable of producing solutions that provably satisfy the first-order Lu-Zhang optimality conditions. Our approach accommodates both hinge and quadratic loss functions. In both cases, we demonstrate that the subproblems arising within the algorithm either admit closed-form solutions or can be solved efficiently through dual formulations, which contributes to the method's overall effectiveness. We also analyze the convergence behavior of the algorithm under both loss settings. Finally, we validate our approach on several real-world datasets, demonstrating its ability to reduce overfitting while maintaining strong classification performance. The complete implementation and experimental code are publicly available at https://github.com/raminzandvakili/L0-QSVM.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ehrenfeucht-Haussler Rank and Chain of Thought</title>
<link>https://arxiv.org/abs/2501.12997</link>
<guid>https://arxiv.org/abs/2501.12997</guid>
<content:encoded><![CDATA[
arXiv:2501.12997v2 Announce Type: replace 
Abstract: The notion of \emph{rank} of a Boolean function has been a cornerstone in PAC learning theory, enabling quasipolynomial-time learning algorithms for polynomial-size decision trees. We present a novel characterization of rank, grounded in the well-known Transformer architecture. We show that the rank of a function $f$ corresponds to the minimum number of \emph{Chain of Thought} (CoT) steps required by a single-layer Transformer with hard attention to compute $f$. Based on this characterization we establish tight bounds on the number of CoT steps required for specific problems, showing that \(\ell\)-fold function composition necessitates exactly \(\ell\) CoT steps. Furthermore, we analyze the problem of identifying the position of the \(k\)-th occurrence of 1 in a Boolean sequence, proving that it requires \(k\) CoT steps. Finally, we introduce the notion of the multi-head rank that captures multi-head single-layer transformers, and perform the analysis of PAC-learnability of the classes of functions with bounded multi-head rank.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>chebgreen: Learning and Interpolating Continuous Empirical Green's Functions from Data</title>
<link>https://arxiv.org/abs/2501.18715</link>
<guid>https://arxiv.org/abs/2501.18715</guid>
<content:encoded><![CDATA[
arXiv:2501.18715v4 Announce Type: replace 
Abstract: In this work, we present a mesh-independent, data-driven library, chebgreen, to mathematically model one-dimensional systems, possessing an associated control parameter, and whose governing partial differential equation is unknown. The proposed method learns an Empirical Green's Function for the associated, but hidden, boundary value problem, in the form of a Rational Neural Network from which we subsequently construct a bivariate representation in a Chebyshev basis. We uncover the Green's function, at an unseen control parameter value, by interpolating the left and right singular functions within a suitable library, expressed as points on a manifold of Quasimatrices, while the associated singular values are interpolated with Lagrange polynomials.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Covering Multiple Objectives with a Small Set of Solutions Using Bayesian Optimization</title>
<link>https://arxiv.org/abs/2501.19342</link>
<guid>https://arxiv.org/abs/2501.19342</guid>
<content:encoded><![CDATA[
arXiv:2501.19342v3 Announce Type: replace 
Abstract: In multi-objective black-box optimization, the goal is typically to find solutions that optimize a set of $T$ black-box objective functions, $f_1$, ..., $f_T$, simultaneously. Traditional approaches often seek a single Pareto-optimal set that balances trade-offs among all objectives. In this work, we consider a problem setting that departs from this paradigm: finding a small set of K < T solutions, that collectively "covers" the T objectives. A set of solutions is defined as "covering" if, for each objective $f_1$, ..., $f_T$, there is at least one good solution. A motivating example for this problem setting occurs in drug design. For example, we may have T pathogens and aim to identify a set of K < T antibiotics such that at least one antibiotic can be used to treat each pathogen. To address this problem, we propose Multi-Objective Coverage Bayesian Optimization (MOCOBO), a principled algorithm designed to efficiently find a covering set. We validate our approach through experiments on challenging high-dimensional tasks, including applications in peptide and molecular design, where MOCOBO is shown to find high-performing covering sets of solutions. The results show that the coverage of the K < T solutions found by MOCOBO matches or nearly matches the coverage of T solutions obtained by optimizing each objective individually. Furthermore, in in vitro experiments, the peptides found by MOCOBO exhibited high potency against drug-resistant pathogens, further demonstrating the potential of MOCOBO for drug discovery. We make code available here: https://github.com/nataliemaus/mocobo.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Emergence of Position Bias in Transformers</title>
<link>https://arxiv.org/abs/2502.01951</link>
<guid>https://arxiv.org/abs/2502.01951</guid>
<content:encoded><![CDATA[
arXiv:2502.01951v4 Announce Type: replace 
Abstract: Recent studies have revealed various manifestations of position bias in transformer architectures, from the "lost-in-the-middle" phenomenon to attention sinks, yet a comprehensive theoretical understanding of how attention masks and positional encodings shape these biases remains elusive. This paper presents a graph-theoretic framework for analyzing position bias in multi-layer attention. Modeling attention masks as directed graphs, we quantify how tokens interact with contextual information based on their sequential positions. We uncover two key insights: First, causal masking inherently biases attention toward earlier positions, as tokens in deeper layers attend to increasingly more contextualized representations of earlier tokens. Second, we characterize the competing effects of the causal mask and relative positional encodings, such as the decay mask and rotary positional encoding (RoPE): while both mechanisms introduce distance-based decay within individual attention maps, their aggregate effect across multiple attention layers$\unicode{x2013}$coupled with the causal mask$\unicode{x2013}$leads to a trade-off between the long-term decay effects and the cumulative importance of early sequence positions. Through controlled numerical experiments, we not only validate our theoretical findings but also reproduce position biases observed in real-world LLMs. Our framework offers a principled foundation for understanding positional biases in transformers, shedding light on the complex interplay of attention mechanism components and guiding more informed architectural design.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schema-Guided Scene-Graph Reasoning based on Multi-Agent Large Language Model System</title>
<link>https://arxiv.org/abs/2502.03450</link>
<guid>https://arxiv.org/abs/2502.03450</guid>
<content:encoded><![CDATA[
arXiv:2502.03450v2 Announce Type: replace 
Abstract: Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs). In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs. The agents are grouped into two modules: a (1) Reasoner module for abstract task planning and graph information queries generation, and a (2) Retriever module for extracting corresponding graph information based on code-writing following the queries. Two modules collaborate iteratively, enabling sequential reasoning and adaptive attention to graph information. The scene graph schema, prompted to both modules, serves to not only streamline both reasoning and retrieval process, but also guide the cooperation between two modules. This eliminates the need to prompt LLMs with full graph data, reducing the chance of hallucination due to irrelevant information. Through experiments in multiple simulation environments, we show that our framework surpasses existing LLM-based approaches and baseline single-agent, tool-based Reason-while-Retrieve strategy in numerical Q\&amp;A and planning tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chaos into Order: Neural Framework for Expected Value Estimation of Stochastic Partial Differential Equations</title>
<link>https://arxiv.org/abs/2502.03670</link>
<guid>https://arxiv.org/abs/2502.03670</guid>
<content:encoded><![CDATA[
arXiv:2502.03670v2 Announce Type: replace 
Abstract: Stochastic partial differential equations (SPDEs) describe the evolution of random processes over space and time, but their solutions are often analytically intractable and computationally expensive to estimate. In this paper, we propose the Learned Expectation Collapser (LEC), a physics-informed neural framework designed to approximate the expected value of linear SPDE solutions without requiring domain discretization. By leveraging randomized sampling of both space-time coordinates and noise realizations during training, LEC trains standard feedforward neural networks to minimize residual loss across multiple stochastic samples. We hypothesize and empirically confirm that this training regime drives the network to converge toward the expected value of the solution of the SPDE. Using the stochastic heat equation as a testbed, we evaluate performance across a diverse set of 144 experimental configurations that span multiple spatial dimensions, noise models, and forcing functions. The results show that the model consistently learns accurate approximations of the expected value of the solution in lower dimensions and a predictable decrease in accuracy with increased spatial dimensions, with improved stability and robustness under increased Monte Carlo sampling. Our findings offer new insight into how neural networks implicitly learn statistical structure from stochastic differential operators and suggest a pathway toward scalable, simulator-free SPDE solvers.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning of Model Discrepancy with Bayesian Experimental Design</title>
<link>https://arxiv.org/abs/2502.05372</link>
<guid>https://arxiv.org/abs/2502.05372</guid>
<content:encoded><![CDATA[
arXiv:2502.05372v2 Announce Type: replace 
Abstract: Digital twins have been actively explored in many engineering applications, such as manufacturing and autonomous systems. However, model discrepancy is ubiquitous in most digital twin models and has significant impacts on the performance of using those models. In recent years, data-driven modeling techniques have been demonstrated promising in characterizing the model discrepancy in existing models, while the training data for the learning of model discrepancy is often obtained in an empirical way and an active approach of gathering informative data can potentially benefit the learning of model discrepancy. On the other hand, Bayesian experimental design (BED) provides a systematic approach to gathering the most informative data, but its performance is often negatively impacted by the model discrepancy. In this work, we build on sequential BED and propose an efficient approach to iteratively learn the model discrepancy based on the data from the BED. The performance of the proposed method is validated by a classical numerical example governed by a convection-diffusion equation, for which full BED is still feasible. The proposed method is then further studied in the same numerical example with a high-dimensional model discrepancy, which serves as a demonstration for the scenarios where full BED is not practical anymore. An ensemble-based approximation of information gain is further utilized to assess the data informativeness and to enhance learning model discrepancy. The results show that the proposed method is efficient and robust to the active learning of high-dimensional model discrepancy, using data suggested by the sequential BED. We also demonstrate that the proposed method is compatible with both classical numerical solvers and modern auto-differentiable solvers.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimistic Interior Point Methods for Sequential Hypothesis Testing by Betting</title>
<link>https://arxiv.org/abs/2502.07774</link>
<guid>https://arxiv.org/abs/2502.07774</guid>
<content:encoded><![CDATA[
arXiv:2502.07774v2 Announce Type: replace 
Abstract: The technique of ``testing by betting" frames nonparametric sequential hypothesis testing as a multiple-round game, where a player bets on future observations that arrive in a streaming fashion, accumulates wealth that quantifies evidence against the null hypothesis, and rejects the null once the wealth exceeds a specified threshold while controlling the false positive error. Designing an online learning algorithm that achieves a small regret in the game can help rapidly accumulate the bettor's wealth, which in turn can shorten the time to reject the null hypothesis under the alternative $H_1$. However, many of the existing works employ the Online Newton Step (ONS) to update within a halved decision space to avoid a gradient explosion issue, which is potentially conservative for rapid wealth accumulation. In this paper, we introduce a novel strategy utilizing interior-point methods in optimization that allows updates across the entire interior of the decision space without the risk of gradient explosion. Our approach not only maintains strong statistical guarantees but also facilitates faster null hypothesis rejection, while being as computationally lightweight as ONS thanks to its closed-form updates.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Advantage-Aligned Online Reinforcement Learning with Offline Data</title>
<link>https://arxiv.org/abs/2502.07937</link>
<guid>https://arxiv.org/abs/2502.07937</guid>
<content:encoded><![CDATA[
arXiv:2502.07937v3 Announce Type: replace 
Abstract: Online reinforcement learning (RL) enhances policies through direct interactions with the environment, but faces challenges related to sample efficiency. In contrast, offline RL leverages extensive pre-collected data to learn policies, but often produces suboptimal results due to limited data coverage. Recent efforts integrate offline and online RL in order to harness the advantages of both approaches. However, effectively combining online and offline RL remains challenging due to issues that include catastrophic forgetting, lack of robustness to data quality and limited sample efficiency in data utilization. In an effort to address these challenges, we introduce A3RL, which incorporates a novel confidence aware Active Advantage Aligned (A3) sampling strategy that dynamically prioritizes data aligned with the policy's evolving needs from both online and offline sources, optimizing policy improvement. Moreover, we provide theoretical insights into the effectiveness of our active sampling strategy and conduct diverse empirical experiments and ablation studies, demonstrating that our method outperforms competing online RL techniques that leverage offline data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fenchel-Young Variational Learning</title>
<link>https://arxiv.org/abs/2502.10295</link>
<guid>https://arxiv.org/abs/2502.10295</guid>
<content:encoded><![CDATA[
arXiv:2502.10295v2 Announce Type: replace 
Abstract: From a variational perspective, many statistical learning criteria involve seeking a distribution that balances empirical risk and regularization. In this paper, we broaden this perspective by introducing a new general class of variational methods based on Fenchel-Young (FY) losses, treated as divergences that generalize (and encompass) the familiar Kullback-Leibler divergence at the core of classical variational learning. Our proposed formulation -- FY variational learning -- includes as key ingredients new notions of FY free energy, FY evidence, FY evidence lower bound, and FY posterior. We derive alternating minimization and gradient backpropagation algorithms to compute (or lower bound) the FY evidence, which enables learning a wider class of models than previous variational formulations. This leads to generalized FY variants of classical algorithms, such as an FY expectation-maximization (FYEM) algorithm, and latent-variable models, such as an FY variational autoencoder (FYVAE). Our new methods are shown to be empirically competitive, often outperforming their classical counterparts, and most importantly, to have qualitatively novel features. For example, FYEM has an adaptively sparse E-step, while the FYVAE can support models with sparse observations and sparse posteriors.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Duality between Gradient Transformations and Adapters</title>
<link>https://arxiv.org/abs/2502.13811</link>
<guid>https://arxiv.org/abs/2502.13811</guid>
<content:encoded><![CDATA[
arXiv:2502.13811v2 Announce Type: replace 
Abstract: We study memory-efficient optimization of neural networks (in particular language models) with linear gradient transformations, where the gradients are linearly mapped to a lower dimensional space than the full parameter space, thus saving memory required for gradient accumulation and optimizer state persistence. The model parameters are updated by first performing an optimization step in the lower dimensional space and then going back into the original parameter space via the linear map's transpose. We show that optimizing the model in this transformed space is equivalent to reparameterizing the original model through a linear adapter that additively modifies the model parameters, and then only optimizing the adapter's parameters. When the transformation is Kronecker-factored, this establishes an equivalence between GaLore and one-sided LoRA. We show that this duality between gradient transformations and adapter-based reparameterizations unifies existing approaches to memory-efficient training and suggests new techniques for improving training efficiency and memory use.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models</title>
<link>https://arxiv.org/abs/2502.14819</link>
<guid>https://arxiv.org/abs/2502.14819</guid>
<content:encoded><![CDATA[
arXiv:2502.14819v3 Announce Type: replace 
Abstract: A long-standing goal in AI is to build agents that can solve a variety of tasks across different environments, including previously unseen ones. Two dominant approaches tackle this challenge: (i) reinforcement learning (RL), which learns policies through trial and error, and (ii) optimal control, which plans actions using a learned or known dynamics model. However, their relative strengths and weaknesses remain underexplored in the setting where agents must learn from offline trajectories without reward annotations. In this work, we systematically analyze the performance of different RL and control-based methods under datasets of varying quality. On the RL side, we consider goal-conditioned and zero-shot approaches. On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and use it for planning. We study how dataset properties-such as data diversity, trajectory quality, and environment variability-affect the performance of these approaches. Our results show that model-free RL excels when abundant, high-quality data is available, while model-based planning excels in generalization to novel environment layouts, trajectory stitching, and data-efficiency. Notably, planning with a latent dynamics model emerges as a promising approach for zero-shot generalization from suboptimal data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Moving Flock Detection in Pedestrian Trajectories Using Sequential Deep Learning Models</title>
<link>https://arxiv.org/abs/2502.15252</link>
<guid>https://arxiv.org/abs/2502.15252</guid>
<content:encoded><![CDATA[
arXiv:2502.15252v2 Announce Type: replace 
Abstract: Understanding collective pedestrian movement is crucial for applications in crowd management, autonomous navigation, and human-robot interaction. This paper investigates the use of sequential deep learning models, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers, for real-time flock detection in multi-pedestrian trajectories. Our proposed approach consists of a two-stage process: first, a pre-trained binary classification model is used for pairwise trajectory classification, and second, the learned representations are applied to identify multi-agent flocks dynamically.
  We validate our method using real-world group movement datasets, demonstrating its robustness across varying sequence lengths and diverse movement patterns. Experimental results indicate that our model consistently detects pedestrian flocks with high accuracy and stability, even in dynamic and noisy environments. Furthermore, we extend our approach to identify other forms of collective motion, such as convoys and swarms, paving the way for more comprehensive multi-agent behavior analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness to Geographic Distribution Shift Using Location Encoders</title>
<link>https://arxiv.org/abs/2503.02036</link>
<guid>https://arxiv.org/abs/2503.02036</guid>
<content:encoded><![CDATA[
arXiv:2503.02036v2 Announce Type: replace 
Abstract: Geographic distribution shift arises when the distribution of locations on Earth in a training dataset is different from what is seen at test time. The most common approaches to tackling geographic distribution shift treat regions delimited by administrative boundaries such as countries or continents as separate domains and apply standard domain adaptation methods, ignoring geographic coordinates that are often available as metadata. This paper proposes the use of location encoders for modeling continuous, learnable domain assignment. We show how both non-parametric sine-cosine encoders and pre-trained location encoders can be used in conjunction with standard domain adaptation methods for improved robustness to geographic distribution shift. Our proposed methods achieve new state-of-the-art results on two geo-tagged remote sensing datasets from the WILDS benchmark. We have made our code publicly available at: https://github.com/crastoru/wilds-geoshift.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Average-DICE: Stationary Distribution Correction by Regression</title>
<link>https://arxiv.org/abs/2503.02125</link>
<guid>https://arxiv.org/abs/2503.02125</guid>
<content:encoded><![CDATA[
arXiv:2503.02125v2 Announce Type: replace 
Abstract: Off-policy policy evaluation (OPE), an essential component of reinforcement learning, has long suffered from stationary state distribution mismatch, undermining both stability and accuracy of OPE estimates. While existing methods correct distribution shifts by estimating density ratios, they often rely on expensive optimization or backward Bellman-based updates and struggle to outperform simpler baselines. We introduce AVG-DICE, a computationally simple Monte Carlo estimator for the density ratio that averages discounted importance sampling ratios, providing an unbiased and consistent correction. AVG-DICE extends naturally to nonlinear function approximation using regression, which we roughly tune and test on OPE tasks based on Mujoco Gym environments and compare with state-of-the-art density-ratio estimators using their reported hyperparameters. In our experiments, AVG-DICE is at least as accurate as state-of-the-art estimators and sometimes offers orders-of-magnitude improvements. However, a sensitivity analysis shows that best-performing hyperparameters may vary substantially across different discount factors, so a re-tuning is suggested.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElementaryNet: A Non-Strategic Neural Network for Predicting Human Behavior in Normal-Form Games</title>
<link>https://arxiv.org/abs/2503.05925</link>
<guid>https://arxiv.org/abs/2503.05925</guid>
<content:encoded><![CDATA[
arXiv:2503.05925v2 Announce Type: replace 
Abstract: Behavioral game theory models serve two purposes: yielding insights into how human decision-making works, and predicting how people would behave in novel strategic settings. A system called GameNet represents the state of the art for predicting human behavior in the setting of unrepeated simultaneous-move games, combining a simple "level-k" model of strategic reasoning with a complex neural network model of non-strategic "level-0" behavior. Although this reliance on well-established ideas from cognitive science ought to make GameNet interpretable, the flexibility of its level-0 model raises the possibility that it is able to emulate strategic reasoning. In this work, we prove that GameNet's level-0 model is indeed too general. We then introduce ElementaryNet, a novel neural network that is provably incapable of expressing strategic behavior. We show that these additional restrictions are empirically harmless, leading ElementaryNet to statistically indistinguishable predictive performance vs GameNet. We then show how it is possible to derive insights about human behavior by varying ElementaryNet's features and interpreting its parameters, finding evidence of iterative reasoning, learning about the depth of this reasoning process, and showing the value of a rich level-0 specification.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Extrapolation for Debiased Representation Learning</title>
<link>https://arxiv.org/abs/2503.13236</link>
<guid>https://arxiv.org/abs/2503.13236</guid>
<content:encoded><![CDATA[
arXiv:2503.13236v2 Announce Type: replace 
Abstract: Machine learning classification models trained with empirical risk minimization (ERM) often inadvertently rely on spurious correlations. When absent in the test data, these unintended associations between non-target attributes and target labels lead to poor generalization. This paper addresses this problem from a model optimization perspective and proposes a novel method, Gradient Extrapolation for Debiased Representation Learning (GERNE), designed to learn debiased representations in both known and unknown attribute training cases. GERNE uses two distinct batches with different amounts of spurious correlations and defines the target gradient as a linear extrapolation of the gradients computed from each batch's loss. Our analysis shows that when the extrapolated gradient points toward the batch gradient with fewer spurious correlations, it effectively guides training toward learning a debiased model. GERNE serves as a general framework for debiasing, encompassing ERM and Resampling methods as special cases. We derive the theoretical upper and lower bounds of the extrapolation factor employed by GERNE. By tuning this factor, GERNE can adapt to maximize either Group-Balanced Accuracy (GBA) or Worst-Group Accuracy (WGA). We validate GERNE on five vision and one NLP benchmarks, demonstrating competitive and often superior performance compared to state-of-the-art baselines. The project page is available at: https://gerne-debias.github.io/.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated Learning: A Step Towards Responsible AI</title>
<link>https://arxiv.org/abs/2503.16233</link>
<guid>https://arxiv.org/abs/2503.16233</guid>
<content:encoded><![CDATA[
arXiv:2503.16233v2 Announce Type: replace 
Abstract: Federated Learning (FL) enables collaborative model training while preserving data privacy; however, balancing privacy preservation (PP) and fairness poses significant challenges. In this paper, we present the first unified large-scale empirical study of privacy-fairness-utility trade-offs in FL, advancing toward responsible AI deployment. Specifically, we systematically compare Differential Privacy (DP), Homomorphic Encryption (HE), and Secure Multi-Party Computation (SMC) with fairness-aware optimizers including q-FedAvg, q-MAML, Ditto, evaluating their performance under IID and non-IID scenarios using benchmark (MNIST, Fashion-MNIST) and real-world datasets (Alzheimer's MRI, credit-card fraud detection). Our analysis reveals HE and SMC significantly outperform DP in achieving equitable outcomes under data skew, although at higher computational costs. Remarkably, we uncover unexpected interactions: DP mechanisms can negatively impact fairness, and fairness-aware optimizers can inadvertently reduce privacy effectiveness. We conclude with practical guidelines for designing robust FL systems that deliver equitable, privacy-preserving, and accurate outcomes.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty propagation in feed-forward neural network models</title>
<link>https://arxiv.org/abs/2503.21059</link>
<guid>https://arxiv.org/abs/2503.21059</guid>
<content:encoded><![CDATA[
arXiv:2503.21059v3 Announce Type: replace 
Abstract: We develop new uncertainty propagation methods for feed-forward neural network architectures with leaky ReLU activation functions subject to random perturbations in the input vectors. In particular, we derive analytical expressions for the probability density function (PDF) of the neural network output and its statistical moments as a function of the input uncertainty and the parameters of the network, i.e., weights and biases. A key finding is that an appropriate linearization of the leaky ReLU activation function yields accurate statistical results even for large perturbations in the input vectors. This can be attributed to the way information propagates through the network. We also propose new analytically tractable Gaussian copula surrogate models to approximate the full joint PDF of the neural network output. To validate our theoretical results, we conduct Monte Carlo simulations and a thorough error analysis on a multi-layer neural network representing a nonlinear integro-differential operator between two polynomial function spaces. Our findings demonstrate excellent agreement between the theoretical predictions and Monte Carlo simulations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Hierarchical Protein Multi-Modal Representation Learning</title>
<link>https://arxiv.org/abs/2504.04770</link>
<guid>https://arxiv.org/abs/2504.04770</guid>
<content:encoded><![CDATA[
arXiv:2504.04770v2 Announce Type: replace 
Abstract: Protein representation learning is critical for numerous biological tasks. Recently, large transformer-based protein language models (pLMs) pretrained on large scale protein sequences have demonstrated significant success in sequence-based tasks. However, pLMs lack structural context. Conversely, graph neural networks (GNNs) designed to leverage 3D structural information have shown promising generalization in protein-related prediction tasks, but their effectiveness is often constrained by the scarcity of labeled structural data. Recognizing that sequence and structural representations are complementary perspectives of the same protein entity, we propose a multimodal bidirectional hierarchical fusion framework to effectively merge these modalities. Our framework employs attention and gating mechanisms to enable effective interaction between pLMs-generated sequential representations and GNN-extracted structural features, improving information exchange and enhancement across layers of the neural network. This bidirectional and hierarchical (Bi-Hierarchical) fusion approach leverages the strengths of both modalities to capture richer and more comprehensive protein representations. Based on the framework, we further introduce local Bi-Hierarchical Fusion with gating and global Bi-Hierarchical Fusion with multihead self-attention approaches. Our method demonstrates consistent improvements over strong baselines and existing fusion techniques in a variety of protein representation learning benchmarks, including enzyme EC classification, model quality assessment, protein-ligand binding affinity prediction, protein-protein binding site prediction, and B cell epitopes prediction. Our method establishes a new state-of-the-art for multimodal protein representation learning, emphasizing the efficacy of Bi-Hierarchical Fusion in bridging sequence and structural modalities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Agnostic Policy Explanations with Large Language Models</title>
<link>https://arxiv.org/abs/2504.05625</link>
<guid>https://arxiv.org/abs/2504.05625</guid>
<content:encoded><![CDATA[
arXiv:2504.05625v2 Announce Type: replace 
Abstract: Intelligent agents, such as robots, are increasingly deployed in real-world, human-centric environments. To foster appropriate human trust and meet legal and ethical standards, these agents must be able to explain their behavior. However, state-of-the-art agents are typically driven by black-box models like deep neural networks, limiting their interpretability. We propose a method for generating natural language explanations of agent behavior based only on observed states and actions -- without access to the agent's underlying model. Our approach learns a locally interpretable surrogate model of the agent's behavior from observations, which then guides a large language model to generate plausible explanations with minimal hallucination. Empirical results show that our method produces explanations that are more comprehensible and correct than those from baselines, as judged by both language models and human evaluators. Furthermore, we find that participants in a user study more accurately predicted the agent's future actions when given our explanations, suggesting improved understanding of agent behavior.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource-efficient Inference with Foundation Model Programs</title>
<link>https://arxiv.org/abs/2504.07247</link>
<guid>https://arxiv.org/abs/2504.07247</guid>
<content:encoded><![CDATA[
arXiv:2504.07247v2 Announce Type: replace 
Abstract: The inference-time resource costs of large language and vision models present a growing challenge in production deployments. We propose the use of foundation model programs, i.e., programs that can invoke foundation models with varying resource costs and performance, as an approach to this problem. Specifically, we present a method that translates a task into a program, then learns a policy for resource allocation that, on each input, selects foundation model "backends" for each program module. The policy uses smaller, cheaper backends to handle simpler subtasks, while allowing more complex subtasks to leverage larger, more capable models. We evaluate the method on two new "streaming" visual question-answering tasks in which a system answers a question on a sequence of inputs, receiving ground-truth feedback after each answer. Compared to monolithic multi-modal models, our implementation achieves up to 98% resource savings with minimal accuracy loss, demonstrating its potential for scalable and resource-efficient multi-modal inference.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Autoencoder Network for Robust Heart Rate Extraction from Noisy Photoplethysmogram: Applying Blind Source Separation to Biosignal Analysis</title>
<link>https://arxiv.org/abs/2504.09132</link>
<guid>https://arxiv.org/abs/2504.09132</guid>
<content:encoded><![CDATA[
arXiv:2504.09132v3 Announce Type: replace 
Abstract: Biosignals can be viewed as mixtures measuring particular physiological events, and blind source separation (BSS) aims to extract underlying source signals from mixtures. This paper proposes a self-supervised multi-encoder autoencoder (MEAE) to separate heartbeat-related source signals from photoplethysmogram (PPG), enhancing heart rate (HR) detection in noisy PPG data. The MEAE is trained on PPG signals from a large open polysomnography database without any pre-processing or data selection. The trained network is then applied to a noisy PPG dataset collected during the daily activities of nine subjects. The extracted heartbeat-related source signal significantly improves HR detection as compared to the original PPG. The absence of pre-processing and the self-supervised nature of the proposed method, combined with its strong performance, highlight the potential of MEAE for BSS in biosignal analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Marching Neural Operator FE Coupling: AI Accelerated Physics Modeling</title>
<link>https://arxiv.org/abs/2504.11383</link>
<guid>https://arxiv.org/abs/2504.11383</guid>
<content:encoded><![CDATA[
arXiv:2504.11383v4 Announce Type: replace 
Abstract: Numerical solvers for PDEs often struggle to balance computational cost with accuracy, especially in multiscale and time-dependent systems. Neural operators offer a promising way to accelerate simulations, but their practical deployment is hindered by several challenges: they typically require large volumes of training data generated from high-fidelity solvers, tend to accumulate errors over time in dynamical settings, and often exhibit poor generalization in multiphysics scenarios. This work introduces a novel hybrid framework that integrates physics-informed deep operator network with FEM through domain decomposition and leverages numerical analysis for time marching. Our innovation lies in efficient coupling FE and DeepONet subdomains via a Schwarz method, expecting to solve complex and nonlinear regions by a pretrained DeepONet, while the remainder is handled by conventional FE. To address the challenges of dynamic systems, we embed a time stepping scheme directly into the DeepONet, substantially reducing long-term error propagation. Furthermore, an adaptive subdomain evolution strategy enables the ML-resolved region to expand dynamically, capturing fine-scale features without remeshing. Our framework shows accelerated convergence rates (up to 20% improvement in convergence rates compared to conventional FE coupling approaches) while preserving solution fidelity with error margins consistently below 3%. Our study shows that our proposed hybrid solver: (1) reduces computational costs by eliminating fine mesh requirements, (2) mitigates error accumulation in time-dependent simulations, and (3) enables automatic adaptation to evolving physical phenomena. This work establishes a new paradigm for coupling state of the art physics based and machine learning solvers in a unified framework, offering a robust, reliable, and scalable pathway for high fidelity multiscale simulations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token Selection</title>
<link>https://arxiv.org/abs/2504.14051</link>
<guid>https://arxiv.org/abs/2504.14051</guid>
<content:encoded><![CDATA[
arXiv:2504.14051v4 Announce Type: replace 
Abstract: While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling 3D Ocean Biogeochemical Provinces in the North Atlantic: A Systematic Comparison and Validation of Clustering Methods</title>
<link>https://arxiv.org/abs/2504.18181</link>
<guid>https://arxiv.org/abs/2504.18181</guid>
<content:encoded><![CDATA[
arXiv:2504.18181v2 Announce Type: replace 
Abstract: Defining ocean regions and water masses helps to understand marine processes and can serve downstream tasks such as defining marine protected areas. However, such definitions often result from subjective decisions potentially producing misleading, unreproducible outcomes. Here, the aim was to objectively define regions of the North Atlantic through systematic comparison of clustering methods within the Native Emergent Manifold Interrogation (NEMI) framework (Sonnewald, 2023). About 300 million measured salinity, temperature, and oxygen, nitrate, phosphate and silicate concentration values served as input for various clustering methods (k-Means, agglomerative Ward, and Density-Based Spatial Clustering of Applications with Noise (DBSCAN)). Uniform Manifold Approximation and Projection (UMAP) emphasised (dis-)similarities in the data while reducing dimensionality. Based on systematic validation of clustering methods and their hyperparameters using internal, external and relative validation techniques, results showed that UMAP-DBSCAN best represented the data. Strikingly, internal validation metrics proved systematically unreliable for comparing clustering methods. To address stochastic variability, 100 UMAP-DBSCAN clustering runs were conducted and aggregated following NEMI, yielding a final set of 321 clusters. Reproducibility was evaluated via ensemble overlap ($88.81\pm1.8\%$) and mean grid cell-wise uncertainty ($15.49\pm20\%$). Case studies of the Mediterranean Sea, deep Atlantic waters and Labrador Sea showed strong agreement with common water mass definitions. This study revealed a more detailed regionalisation compared to previous concepts such as the Longhurst provinces through systematic clustering method comparison. The applied method is objective, efficient and reproducible and will support future research on biogeochemical differences and changes in oceanic regions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2504.18243</link>
<guid>https://arxiv.org/abs/2504.18243</guid>
<content:encoded><![CDATA[
arXiv:2504.18243v2 Announce Type: replace 
Abstract: Multi-Hop Question Answering (MHQA) tasks permeate real-world applications, posing challenges in orchestrating multi-step reasoning across diverse knowledge domains. While existing approaches have been improved with iterative retrieval, they still struggle to identify and organize dynamic knowledge. To address this, we propose DualRAG, a synergistic dual-process framework that seamlessly integrates reasoning and retrieval. DualRAG operates through two tightly coupled processes: Reasoning-augmented Querying (RaQ) and progressive Knowledge Aggregation (pKA). They work in concert: as RaQ navigates the reasoning path and generates targeted queries, pKA ensures that newly acquired knowledge is systematically integrated to support coherent reasoning. This creates a virtuous cycle of knowledge enrichment and reasoning refinement. Through targeted fine-tuning, DualRAG preserves its sophisticated reasoning and retrieval capabilities even in smaller-scale models, demonstrating its versatility and core advantages across different scales. Extensive experiments demonstrate that this dual-process approach substantially improves answer accuracy and coherence, approaching, and in some cases surpassing, the performance achieved with oracle knowledge access. These results establish DualRAG as a robust and efficient solution for complex multi-hop reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting at Full Spectrum: Holistic Multi-Granular Traffic Modeling under High-Throughput Inference Regimes</title>
<link>https://arxiv.org/abs/2505.01279</link>
<guid>https://arxiv.org/abs/2505.01279</guid>
<content:encoded><![CDATA[
arXiv:2505.01279v2 Announce Type: replace 
Abstract: Notably, current intelligent transportation systems rely heavily on accurate traffic forecasting and swift inference provision to make timely decisions. While Graph Convolutional Networks (GCNs) have shown benefits in modeling complex traffic dependencies, the existing GCN-based approaches cannot fully extract and fuse multi-granular spatiotemporal features across various spatial and temporal scales sufficiently in a complete manner, proven to yield less accurate results. Besides, as extracting multi-granular features across scales has been a promising strategy across domains such as computer vision, natural language processing, and time-series forecasting, pioneering studies have attempted to leverage a similar mechanism for spatiotemporal traffic data mining. However, additional feature extraction branches introduced in prior studies critically increased model complexity and extended inference time, making it challenging to provide fast forecasts. In this paper, we propose MultiGran-STGCNFog, an efficient fog distributed inference system with a novel traffic forecasting model that employs multi-granular spatiotemporal feature fusion on generated dynamic traffic graphs to fully capture interdependent traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer execution order and layer-device scheduling scheme simultaneously, contributes to considerable inference throughput improvement by coordinating heterogeneous fog devices in a pipelined manner. Extensive experiments on real-world datasets demonstrate the superiority of the proposed method over selected GCN baselines.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization</title>
<link>https://arxiv.org/abs/2505.02515</link>
<guid>https://arxiv.org/abs/2505.02515</guid>
<content:encoded><![CDATA[
arXiv:2505.02515v3 Announce Type: replace 
Abstract: Traditional Federated Domain Generalization (FedDG) methods focus on learning domain-invariant features or adapting to unseen target domains, often overlooking the unique knowledge embedded within the source domain, especially in strictly isolated federated learning environments. Through experimentation, we discovered a counterintuitive phenomenon.: features learned from a complete source domain have superior generalization capabilities compared to those learned directly from the target domain. This insight leads us to propose the Federated Source Domain Awareness Framework (FedSDAF), the first systematic approach to enhance FedDG by leveraging source domain-aware features. FedSDAF employs a dual-adapter architecture that decouples "local expertise" from "global generalization consensus". A Domain-Aware Adapter, retained locally, extracts and protects the unique discriminative knowledge of each source domain, while a Domain-Invariant Adapter, shared across clients, builds a robust global consensus. To enable knowledge exchange, we introduce a Bidirectional Knowledge Distillation mechanism that facilitates efficient dialogue between the adapters. Extensive experiments on four benchmark datasets (OfficeHome, PACS, VLCS, DomainNet) show that FedSDAF significantly outperforms existing FedDG methods.The source code is available at https://github.com/pizzareapers/FedSDAF.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging</title>
<link>https://arxiv.org/abs/2505.03785</link>
<guid>https://arxiv.org/abs/2505.03785</guid>
<content:encoded><![CDATA[
arXiv:2505.03785v2 Announce Type: replace 
Abstract: Agentic systems built on large language models (LLMs) offer promising capabilities for automating complex workflows in healthcare AI. We introduce mAIstro, an open-source, autonomous multi-agentic framework for end-to-end development and deployment of medical AI models. The system orchestrates exploratory data analysis, radiomic feature extraction, image segmentation, classification, and regression through a natural language interface, requiring no coding from the user. Built on a modular architecture, mAIstro supports both open- and closed-source LLMs, and was evaluated using a large and diverse set of prompts across 16 open-source datasets, covering a wide range of imaging modalities, anatomical regions, and data types. The agents successfully executed all tasks, producing interpretable outputs and validated models. This work presents the first agentic framework capable of unifying data analysis, AI model development, and inference across varied healthcare applications, offering a reproducible and extensible foundation for clinical and research AI integration. The code is available at: https://github.com/eltzanis/mAIstro
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning</title>
<link>https://arxiv.org/abs/2505.10347</link>
<guid>https://arxiv.org/abs/2505.10347</guid>
<content:encoded><![CDATA[
arXiv:2505.10347v2 Announce Type: replace 
Abstract: Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task Learning by addressing issues like conflicting gradients and differing gradient norms, which hinder equal-weighted task training. However, recent critiques suggest that equally weighted tasks can achieve competitive results compared to SMTOs, arguing that previous SMTO results were influenced by poor hyperparameter optimization and lack of regularization. In this work, we evaluate these claims through an extensive empirical evaluation of SMTOs, including some of the latest methods, on more complex multi-task problems to clarify this behavior. Our findings indicate that SMTOs perform well compared to uniform loss and that fixed weights can achieve competitive performance compared to SMTOs. Furthermore, we demonstrate why uniform loss perform similarly to SMTOs in some instances. The source code is available at https://github.com/Gabriel-SGama/UnitScal_vs_SMTOs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline</title>
<link>https://arxiv.org/abs/2505.11250</link>
<guid>https://arxiv.org/abs/2505.11250</guid>
<content:encoded><![CDATA[
arXiv:2505.11250v3 Announce Type: replace 
Abstract: The forecasting of irregular multivariate time series (IMTS) is a critical task in domains like healthcare and climate science. However, this task faces two significant hurdles: 1) the inherent non-uniformity and missing data in IMTS complicate the modeling of temporal dynamics, and 2) existing methods often rely on computationally expensive architectures. To address these dual challenges, we introduce APN, a general and efficient forecasting framework. At the core of APN is a novel Time-Aware Patch Aggregation (TAPA) module that introduces an aggregation-based paradigm for adaptive patching, moving beyond the limitations of fixed-span segmentation and interpolation-based methods. TAPA first learns dynamic temporal boundaries to define data-driven segments. Crucially, instead of resampling or interpolating, it directly computes patch representations via a time-aware weighted aggregation of all raw observations, where weights are determined by each observation's temporal relevance to the segment. This approach provides two key advantages: it preserves data fidelity by avoiding the introduction of artificial data points and ensures complete information coverage by design.The resulting regularized and information-rich patch representations enable the use of a lightweight query module for historical context aggregation and a simple MLP for final prediction. Extensive experiments on multiple real-world datasets demonstrate that APN establishes a new state-of-the-art, significantly outperforming existing methods in both prediction accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2505.16341</link>
<guid>https://arxiv.org/abs/2505.16341</guid>
<content:encoded><![CDATA[
arXiv:2505.16341v3 Announce Type: replace 
Abstract: This paper studies the long-tailed semi-supervised learning (LTSSL) with distribution mismatch, where the class distribution of the labeled training data follows a long-tailed distribution and mismatches with that of the unlabeled training data. Most existing methods introduce auxiliary classifiers (experts) to model various unlabeled data distributions and produce pseudo-labels, but the expertises of various experts are not fully utilized. We observe that different experts are good at predicting different intervals of samples, e.g., long-tailed expert is skilled in samples located in the head interval and uniform expert excels in samples located in the medium interval. Therefore, we propose a dynamic expert assignment module that can estimate the class membership (i.e., head, medium, or tail class) of samples, and dynamically assigns suitable expert to each sample based on the estimated membership to produce high-quality pseudo-label in the training phase and produce prediction in the testing phase. We also theoretically reveal that integrating different experts' strengths will lead to a smaller generalization error bound. Moreover, we find that the deeper features are more biased toward the head class but with more discriminative ability, while the shallower features are less biased but also with less discriminative ability. We, therefore, propose a multi-depth feature fusion module to utilize different depth features to mitigate the model bias. Our method demonstrates its effectiveness through comprehensive experiments on the CIFAR-10-LT, STL-10-LT, and SVHN-LT datasets across various settings.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FP4 All the Way: Fully Quantized Training of LLMs</title>
<link>https://arxiv.org/abs/2505.19115</link>
<guid>https://arxiv.org/abs/2505.19115</guid>
<content:encoded><![CDATA[
arXiv:2505.19115v2 Announce Type: replace 
Abstract: We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way .
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason without External Rewards</title>
<link>https://arxiv.org/abs/2505.19590</link>
<guid>https://arxiv.org/abs/2505.19590</guid>
<content:encoded><![CDATA[
arXiv:2505.19590v2 Announce Type: replace 
Abstract: Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available at https://github.com/sunblaze-ucb/Intuitor
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents</title>
<link>https://arxiv.org/abs/2505.19997</link>
<guid>https://arxiv.org/abs/2505.19997</guid>
<content:encoded><![CDATA[
arXiv:2505.19997v2 Announce Type: replace 
Abstract: Large language models (LLMs) are revolutionizing education, with LLM-based agents playing a key role in simulating student behavior. A major challenge in student simulation is modeling the diverse learning patterns of students at various cognitive levels. However, current LLMs, typically trained as ``helpful assistants'', target at generating perfect responses. As a result, they struggle to simulate students with diverse cognitive abilities, as they often produce overly advanced answers, missing the natural imperfections that characterize student learning and resulting in unrealistic simulations. To address this issue, we propose a training-free framework for student simulation. We begin by constructing a cognitive prototype for each student using a knowledge graph, which captures their understanding of concepts from past learning records. This prototype is then mapped to new tasks to predict student performance. Next, we simulate student solutions based on these predictions and iteratively refine them using a beam search method to better replicate realistic mistakes. To validate our approach, we construct the \texttt{Student\_100} dataset, consisting of $100$ students working on Python programming and $5,000$ learning records. Experimental results show that our method consistently outperforms baseline models, achieving $100\%$ improvement in simulation accuracy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection</title>
<link>https://arxiv.org/abs/2505.23870</link>
<guid>https://arxiv.org/abs/2505.23870</guid>
<content:encoded><![CDATA[
arXiv:2505.23870v2 Announce Type: replace 
Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Winner-takes-all for Multivariate Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.05515</link>
<guid>https://arxiv.org/abs/2506.05515</guid>
<content:encoded><![CDATA[
arXiv:2506.05515v2 Announce Type: replace 
Abstract: We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL) paradigm to forecast multiple plausible time series futures. Our approach employs a neural network with multiple heads and utilizes the Winner-Takes-All (WTA) loss to promote diversity among predictions. MCL has recently gained attention due to its simplicity and ability to address ill-posed and ambiguous tasks. We propose an adaptation of this framework for time-series forecasting, presenting it as an efficient method to predict diverse futures, which we relate to its implicit quantization objective. We provide insights into our approach using synthetic data and evaluate it on real-world time series, demonstrating its promising performance at a light computational cost.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wasserstein Barycenter Soft Actor-Critic</title>
<link>https://arxiv.org/abs/2506.10167</link>
<guid>https://arxiv.org/abs/2506.10167</guid>
<content:encoded><![CDATA[
arXiv:2506.10167v3 Announce Type: replace 
Abstract: Deep off-policy actor-critic algorithms have emerged as the leading framework for reinforcement learning in continuous control domains. However, most of these algorithms suffer from poor sample efficiency, especially in environments with sparse rewards. In this paper, we take a step towards addressing this issue by providing a principled directed exploration strategy. We propose Wasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from a pessimistic actor for temporal difference learning and an optimistic actor to promote exploration. This is achieved by using the Wasserstein barycenter of the pessimistic and optimistic policies as the exploration policy and adjusting the degree of exploration throughout the learning process. We compare WBSAC with state-of-the-art off-policy actor-critic algorithms and show that WBSAC is more sample-efficient on MuJoCo continuous control tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving</title>
<link>https://arxiv.org/abs/2506.17230</link>
<guid>https://arxiv.org/abs/2506.17230</guid>
<content:encoded><![CDATA[
arXiv:2506.17230v2 Announce Type: replace 
Abstract: Partial Differential Equations (PDEs) are fundamental for modeling physical systems, yet solving them in a generic and efficient manner using machine learning-based approaches remains challenging due to limited multi-input and multi-scale generalization capabilities, as well as high computational costs. This paper proposes the Multi-input and Multi-scale Efficient Transformer (MMET), a novel framework designed to address the above challenges. MMET decouples mesh and query points as two sequences and feeds them into the encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE) layer to embed input variables or functions with varying dimensions, enabling effective solutions for multi-scale and multi-input problems. Additionally, a Hilbert curve-based reserialization and patch embedding mechanism decrease the input length. This significantly reduces the computational cost when dealing with large-scale geometric models. These innovations enable efficient representations and support multi-scale resolution queries for large-scale and multi-input PDE problems. Experimental evaluations on diverse benchmarks spanning different physical fields demonstrate that MMET outperforms SOTA methods in both accuracy and computational efficiency. This work highlights the potential of MMET as a robust and scalable solution for real-time PDE solving in engineering and physics-based applications, paving the way for future explorations into pre-trained large-scale models in specific domains. This work is open-sourced at https://github.com/YichenLuo-0/MMET.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Granular-Ball-Induced Multiple Kernel K-Means</title>
<link>https://arxiv.org/abs/2506.18637</link>
<guid>https://arxiv.org/abs/2506.18637</guid>
<content:encoded><![CDATA[
arXiv:2506.18637v2 Announce Type: replace 
Abstract: Most existing multi-kernel clustering algorithms, such as multi-kernel K-means, often struggle with computational efficiency and robustness when faced with complex data distributions. These challenges stem from their dependence on point-to-point relationships for optimization, which can lead to difficulty in accurately capturing data sets' inherent structure and diversity. Additionally, the intricate interplay between multiple kernels in such algorithms can further exacerbate these issues, effectively impacting their ability to cluster data points in high-dimensional spaces. In this paper, we leverage granular-ball computing to improve the multi-kernel clustering framework. The core of granular-ball computing is to adaptively fit data distribution by balls from coarse to acceptable levels. Each ball can enclose data points based on a density consistency measurement. Such ball-based data description thus improves the computational efficiency and the robustness to unknown noises. Specifically, based on granular-ball representations, we introduce the granular-ball kernel (GBK) and its corresponding granular-ball multi-kernel K-means framework (GB-MKKM) for efficient clustering. Using granular-ball relationships in multiple kernel spaces, the proposed GB-MKKM framework shows its superiority in efficiency and clustering performance in the empirical evaluation of various clustering tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Behavior Cloning Via Global Lipschitz Regularization</title>
<link>https://arxiv.org/abs/2506.19250</link>
<guid>https://arxiv.org/abs/2506.19250</guid>
<content:encoded><![CDATA[
arXiv:2506.19250v2 Announce Type: replace 
Abstract: Behavior Cloning (BC) is an effective imitation learning technique and has even been adopted in some safety-critical domains such as autonomous vehicles. BC trains a policy to mimic the behavior of an expert by using a dataset composed of only state-action pairs demonstrated by the expert, without any additional interaction with the environment. However, During deployment, the policy observations may contain measurement errors or adversarial disturbances. Since the observations may deviate from the true states, they can mislead the agent into making sub-optimal actions. In this work, we use a global Lipschitz regularization approach to enhance the robustness of the learned policy network. We then show that the resulting global Lipschitz property provides a robustness certificate to the policy with respect to different bounded norm perturbations. Then, we propose a way to construct a Lipschitz neural network that ensures the policy robustness. We empirically validate our theory across various environments in Gymnasium. Keywords: Robust Reinforcement Learning; Behavior Cloning; Lipschitz Neural Network
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Optimality for Inference-time Scaling</title>
<link>https://arxiv.org/abs/2506.22376</link>
<guid>https://arxiv.org/abs/2506.22376</guid>
<content:encoded><![CDATA[
arXiv:2506.22376v2 Announce Type: replace 
Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-N selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop OptScale, a practical algorithm that dynamically determines the optimal number of sampled responses. OptScale employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that OptScale significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning. The source code is publicly available at https://github.com/Albertwyk/OptScale.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning</title>
<link>https://arxiv.org/abs/2507.01216</link>
<guid>https://arxiv.org/abs/2507.01216</guid>
<content:encoded><![CDATA[
arXiv:2507.01216v2 Announce Type: replace 
Abstract: There is a huge gap between numerous intriguing applications fostered by on-device large language model (LLM) fine-tuning (FT) from fresh mobile data and the limited resources of a mobile device. While existing server-assisted methods (e.g., split learning or side-tuning) may enable LLM FT on the local mobile device, they suffer from heavy communication burdens of activation transmissions, and may disclose data and labels to the server. To address those issues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method which can be deployed on the mobile device via server-assisted additive side-tuning. To further accelerate FT convergence and improve computing efficiency, PAE MobiLLM integrates activation caching on the server side, which allows the server to reuse historical activations and saves the mobile device from repeatedly computing forward passes for the recurring data samples. Besides, to reduce communication cost, PAE MobiLLM develops an activation shortcut that transmits only the token involved in the loss calculation instead of full activation matrices to guide the side network tuning. Last but not least, PAE MobiLLM introduces the additive adapter side-network design which makes the server train the adapter modules based on device-defined prediction differences rather than raw ground-truth labels. In this way, the server can only assist device-defined side-network computing, and learn nothing about data and labels. Extensive experimental results demonstrate PAE MobiLLM's superiority.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Last Mile of Prediction: Enhancing Time Series Forecasting with Conditional Guided Flow Matching</title>
<link>https://arxiv.org/abs/2507.07192</link>
<guid>https://arxiv.org/abs/2507.07192</guid>
<content:encoded><![CDATA[
arXiv:2507.07192v3 Announce Type: replace 
Abstract: Existing generative models for time series forecasting often transform simple priors (typically Gaussian) into complex data distributions. However, their sampling initialization, independent of historical data, hinders the capture of temporal dependencies, limiting predictive accuracy. They also treat residuals merely as optimization targets, ignoring that residuals often exhibit meaningful patterns like systematic biases or nontrivial distributional structures. To address these, we propose Conditional Guided Flow Matching (CGFM), a novel model-agnostic framework that extends flow matching by integrating outputs from an auxiliary predictive model. This enables learning from the probabilistic structure of prediction residuals, leveraging the auxiliary model's prediction distribution as a source to reduce learning difficulty and refine forecasts. CGFM incorporates historical data as both conditions and guidance, uses two-sided conditional paths (with source and target conditioned on the same history), and employs affine paths to expand the path space, avoiding path crossing without complex mechanisms, preserving temporal consistency, and strengthening distribution alignment. Experiments across datasets and baselines show CGFM consistently outperforms state-of-the-art models, advancing forecasting.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space</title>
<link>https://arxiv.org/abs/2507.10638</link>
<guid>https://arxiv.org/abs/2507.10638</guid>
<content:encoded><![CDATA[
arXiv:2507.10638v3 Announce Type: replace 
Abstract: We introduce a novel classification framework, ZClassifier, that replaces conventional deterministic logits with diagonal Gaussian-distributed logits. Our method simultaneously addresses temperature scaling and manifold approximation by minimizing the KL divergence between the predicted Gaussian distributions and a unit isotropic Gaussian. This unifies uncertainty calibration and latent control in a principled probabilistic manner, enabling a natural interpretation of class confidence and geometric consistency. Experiments on CIFAR-10 and CIFAR-100 demonstrate that ZClassifier improves over softmax classifiers in robustness, calibration, and latent separation, with consistent benefits across small-scale and large-scale classification settings.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model</title>
<link>https://arxiv.org/abs/2507.22854</link>
<guid>https://arxiv.org/abs/2507.22854</guid>
<content:encoded><![CDATA[
arXiv:2507.22854v2 Announce Type: replace 
Abstract: We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a "simulator". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like "optimism in the face of uncertainty" and "posterior sampling" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\operatorname{poly}\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Treatment-DML: Causal Estimation for Multi-Dimensional Continuous Treatments with Monotonicity Constraints in Personal Loan Risk Optimization</title>
<link>https://arxiv.org/abs/2508.02183</link>
<guid>https://arxiv.org/abs/2508.02183</guid>
<content:encoded><![CDATA[
arXiv:2508.02183v2 Announce Type: replace 
Abstract: Optimizing credit limits, interest rates, and loan terms is crucial for managing borrower risk and lifetime value (LTV) in personal loan platform. However, counterfactual estimation of these continuous, multi-dimensional treatments faces significant challenges: randomized trials are often prohibited by risk controls and long repayment cycles, forcing reliance on biased observational data. Existing causal methods primarily handle binary/discrete treatments and struggle with continuous, multi-dimensional settings. Furthermore, financial domain knowledge mandates provably monotonic treatment-outcome relationships (e.g., risk increases with credit limit).To address these gaps, we propose Multi-Treatment-DML, a novel framework leveraging Double Machine Learning (DML) to: (i) debias observational data for causal effect estimation; (ii) handle arbitrary-dimensional continuous treatments; and (iii) enforce monotonic constraints between treatments and outcomes, guaranteeing adherence to domain requirements.Extensive experiments on public benchmarks and real-world industrial datasets demonstrate the effectiveness of our approach. Furthermore, online A/B testing conducted on a realworld personal loan platform, confirms the practical superiority of Multi-Treatment-DML in real-world loan operations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate Measles Rash Detection via Vision Transformer Fine-Tuning</title>
<link>https://arxiv.org/abs/2005.09112</link>
<guid>https://arxiv.org/abs/2005.09112</guid>
<content:encoded><![CDATA[
arXiv:2005.09112v5 Announce Type: replace-cross 
Abstract: Measles, a highly contagious disease declared eliminated in the United States in 2000 after decades of successful vaccination campaigns, resurged in 2025, with 1,356 confirmed cases reported as of August 5, 2025. Given its rapid spread among susceptible individuals, fast and reliable diagnostic systems are critical for early prevention and containment. In this work, we applied transfer learning to fine-tune a pretrained Data-efficient Image Transformer (DeiT) model for distinguishing measles rashes from other skin conditions. Trained on a diverse, curated skin rash image dataset, the DeiT model achieved a median classification accuracy of 96.38%, precision of 96.24%, recall of 96.38%, and an F1-score of 96.23%, demonstrating high effectiveness in accurate detection to aid outbreak control. We also compared the DeiT model with a convolutional neural network, ResNet-50, and discussed the directions for future research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Fast Text Segmentation With Pairwise Markov Chains</title>
<link>https://arxiv.org/abs/2102.11037</link>
<guid>https://arxiv.org/abs/2102.11037</guid>
<content:encoded><![CDATA[
arXiv:2102.11037v3 Announce Type: replace-cross 
Abstract: Natural Language Processing (NLP) models' current trend consists of using increasingly more extra-data to build the best models as possible. It implies more expensive computational costs and training time, difficulties for deployment, and worries about these models' carbon footprint reveal a critical problem in the future. Against this trend, our goal is to develop NLP models requiring no extra-data and minimizing training time. To do so, in this paper, we explore Markov chain models, Hidden Markov Chain (HMC) and Pairwise Markov Chain (PMC), for NLP segmentation tasks. We apply these models for three classic applications: POS Tagging, Named-Entity-Recognition, and Chunking. We develop an original method to adapt these models for text segmentation's specific challenges to obtain relevant performances with very short training and execution times. PMC achieves equivalent results to those obtained by Conditional Random Fields (CRF), one of the most applied models for these tasks when no extra-data are used. Moreover, PMC has training times 30 times shorter than the CRF ones, which validates this model given our objectives.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Learning and Optimization for Queues with Unknown Demand Curve and Service Distribution</title>
<link>https://arxiv.org/abs/2303.03399</link>
<guid>https://arxiv.org/abs/2303.03399</guid>
<content:encoded><![CDATA[
arXiv:2303.03399v2 Announce Type: replace-cross 
Abstract: We investigate an optimization problem in a queueing system where the service provider selects the optimal service fee p and service capacity \mu to maximize the cumulative expected profit (the service revenue minus the capacity cost and delay penalty). The conventional predict-then-optimize (PTO) approach takes two steps: first, it estimates the model parameters (e.g., arrival rate and service-time distribution) from data; second, it optimizes a model based on the estimated parameters. A major drawback of PTO is that its solution accuracy can often be highly sensitive to the parameter estimation errors because PTO is unable to properly link these errors (step 1) to the quality of the optimized solutions (step 2). To remedy this issue, we develop an online learning framework that automatically incorporates the aforementioned parameter estimation errors in the solution prescription process; it is an integrated method that can "learn" the optimal solution without needing to set up the parameter estimation as a separate step as in PTO. Effectiveness of our online learning approach is substantiated by (i) theoretical results including the algorithm convergence and analysis of the regret ("cost" to pay over time for the algorithm to learn the optimal policy), and (ii) engineering confirmation via simulation experiments of a variety of representative examples. We also provide careful comparisons for PTO and the online learning method.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empathy Detection from Text, Audiovisual, Audio or Physiological Signals: A Systematic Review of Task Formulations and Machine Learning Methods</title>
<link>https://arxiv.org/abs/2311.00721</link>
<guid>https://arxiv.org/abs/2311.00721</guid>
<content:encoded><![CDATA[
arXiv:2311.00721v5 Announce Type: replace-cross 
Abstract: Empathy indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science, and Psychology. Detecting empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection leveraging Machine Learning remains underexplored from a systematic literature review perspective. We collected 849 papers from 10 well-known academic databases, systematically screened them and analysed the final 82 papers. Our analyses reveal several prominent task formulations - including empathy on localised utterances or overall expressions, unidirectional or parallel empathy, and emotional contagion - in monadic, dyadic and group interactions. Empathy detection methods are summarised based on four input modalities - text, audiovisual, audio and physiological signals - thereby presenting modality-specific network architecture design protocols. We discuss challenges, research gaps and potential applications in the Affective Computing-based empathy domain, which can facilitate new avenues of exploration. We further enlist the public availability of datasets and codes. This paper, therefore, provides a structured overview of recent advancements and remaining challenges towards developing a robust empathy detection system that could meaningfully contribute to enhancing human well-being.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Based Resource Allocator for Communication Networks with Dynamic User Utility Demands</title>
<link>https://arxiv.org/abs/2311.04600</link>
<guid>https://arxiv.org/abs/2311.04600</guid>
<content:encoded><![CDATA[
arXiv:2311.04600v3 Announce Type: replace-cross 
Abstract: Deep learning (DL) based resource allocation (RA) has recently gained significant attention due to its performance efficiency. However, most related studies assume an ideal case where the number of users and their utility demands, e.g., data rate constraints, are fixed, and the designed DL-based RA scheme exploits a policy trained only for these fixed parameters. Consequently, computationally complex policy retraining is required whenever these parameters change. In this paper, we introduce a DL-based resource allocator (ALCOR) that allows users to adjust their utility demands freely, such as based on their application layer requirements. ALCOR employs deep neural networks (DNNs) as the policy in a time-sharing problem. The underlying optimization algorithm iteratively optimizes the on-off status of users to satisfy their utility demands in expectation. The policy performs unconstrained RA (URA) -- RA without considering user utility demands -- among active users to maximize the sum utility (SU) at each time instant. Depending on the chosen URA scheme, ALCOR can perform RA in either a centralized or distributed scenario. The derived convergence analyses provide theoretical guarantees for ALCOR's convergence, and numerical experiments corroborate its effectiveness compared to meta-learning and reinforcement learning approaches.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training 3D ResNets to Extract BSM Physics Parameters from Simulated Data</title>
<link>https://arxiv.org/abs/2311.13060</link>
<guid>https://arxiv.org/abs/2311.13060</guid>
<content:encoded><![CDATA[
arXiv:2311.13060v4 Announce Type: replace-cross 
Abstract: We report on a novel application of computer vision techniques to extract beyond the Standard Model parameters directly from high energy physics flavor data. We propose a novel data representation that transforms the angular and kinematic distributions into ``quasi-images", which are used to train a convolutional neural network to perform regression tasks, similar to fitting. As a proof-of-concept, we train a 34-layer Residual Neural Network to regress on these images and determine information about the Wilson Coefficient $C_{9}$ in Monte Carlo simulations of $B^0 \rightarrow K^{*0}\mu^{+}\mu^{-}$ decays. The method described here can be generalized and may find applicability across a variety of experiments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Teach: Sample-Efficient Privileged Learning for Humanoid Locomotion over Diverse Terrains</title>
<link>https://arxiv.org/abs/2402.06783</link>
<guid>https://arxiv.org/abs/2402.06783</guid>
<content:encoded><![CDATA[
arXiv:2402.06783v3 Announce Type: replace-cross 
Abstract: Humanoid robots promise transformative capabilities for industrial and service applications. While recent advances in Reinforcement Learning (RL) yield impressive results in locomotion, manipulation, and navigation, the proposed methods typically require enormous simulation samples to account for real-world variability. This work proposes a novel one-stage training framework-Learn to Teach (L2T)-which unifies teacher and student policy learning. Our approach recycles simulator samples and synchronizes the learning trajectories through shared dynamics, significantly reducing sample complexities and training time while achieving state-of-the-art performance. Furthermore, we validate the RL variant (L2T-RL) through extensive simulations and hardware tests on the Digit robot, demonstrating zero-shot sim-to-real transfer and robust performance over 12+ challenging terrains without depth estimation modules.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection</title>
<link>https://arxiv.org/abs/2403.06534</link>
<guid>https://arxiv.org/abs/2403.06534</guid>
<content:encoded><![CDATA[
arXiv:2403.06534v3 Announce Type: replace-cross 
Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at https://github.com/zcablii/SARDet_100K.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A variational Bayes approach to debiased inference for low-dimensional parameters in high-dimensional linear regression</title>
<link>https://arxiv.org/abs/2406.12659</link>
<guid>https://arxiv.org/abs/2406.12659</guid>
<content:encoded><![CDATA[
arXiv:2406.12659v2 Announce Type: replace-cross 
Abstract: We propose a scalable variational Bayes method for statistical inference for a single or low-dimensional subset of the coordinates of a high-dimensional parameter in sparse linear regression. Our approach relies on assigning a mean-field approximation to the nuisance coordinates and carefully modelling the conditional distribution of the target given the nuisance. This requires only a preprocessing step and preserves the computational advantages of mean-field variational Bayes, while ensuring accurate and reliable inference for the target parameter, including for uncertainty quantification. We investigate the numerical performance of our algorithm, showing that it performs competitively with existing methods. We further establish accompanying theoretical guarantees for estimation and uncertainty quantification in the form of a Bernstein--von Mises theorem.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RNA-FrameFlow: Flow Matching for de novo 3D RNA Backbone Design</title>
<link>https://arxiv.org/abs/2406.13839</link>
<guid>https://arxiv.org/abs/2406.13839</guid>
<content:encoded><![CDATA[
arXiv:2406.13839v4 Announce Type: replace-cross 
Abstract: We introduce RNA-FrameFlow, the first generative model for 3D RNA backbone design. We build upon SE(3) flow matching for protein backbone generation and establish protocols for data preparation and evaluation to address unique challenges posed by RNA modeling. We formulate RNA structures as a set of rigid-body frames and associated loss functions which account for larger, more conformationally flexible RNA backbones (13 atoms per nucleotide) vs. proteins (4 atoms per residue). Toward tackling the lack of diversity in 3D RNA datasets, we explore training with structural clustering and cropping augmentations. Additionally, we define a suite of evaluation metrics to measure whether the generated RNA structures are globally self-consistent (via inverse folding followed by forward folding) and locally recover RNA-specific structural descriptors. The most performant version of RNA-FrameFlow generates locally realistic RNA backbones of 40-150 nucleotides, over 40% of which pass our validity criteria as measured by a self-consistency TM-score >= 0.45, at which two RNAs have the same global fold. Open-source code: https://github.com/rish-16/rna-backbone-design
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-AI Bias: large language models favor communications generated by large language models</title>
<link>https://arxiv.org/abs/2407.12856</link>
<guid>https://arxiv.org/abs/2407.12856</guid>
<content:encoded><![CDATA[
arXiv:2407.12856v2 Announce Type: replace-cross 
Abstract: Are large language models (LLMs) biased in favor of communications produced by LLMs, leading to possible antihuman discrimination? Using a classical experimental design inspired by employment discrimination studies, we tested widely used LLMs, including GPT-3.5, GPT-4 and a selection of recent open-weight models in binary choice scenarios. These involved LLM-based assistants selecting between goods (the goods we study include consumer products, academic papers, and film-viewings) described either by humans or LLMs. Our results show a consistent tendency for LLM-based AIs to prefer LLM-presented options. This suggests the possibility of future AI systems implicitly discriminating against humans as a class, giving AI agents and AI-assisted humans an unfair advantage.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scikit-fingerprints: easy and efficient computation of molecular fingerprints in Python</title>
<link>https://arxiv.org/abs/2407.13291</link>
<guid>https://arxiv.org/abs/2407.13291</guid>
<content:encoded><![CDATA[
arXiv:2407.13291v5 Announce Type: replace-cross 
Abstract: In this work, we present scikit-fingerprints, a Python package for computation of molecular fingerprints for applications in chemoinformatics. Our library offers an industry-standard scikit-learn interface, allowing intuitive usage and easy integration with machine learning pipelines. It is also highly optimized, featuring parallel computation that enables efficient processing of large molecular datasets. Currently, scikit-fingerprints stands as the most feature-rich library in the open source Python ecosystem, offering over 30 molecular fingerprints. Our library simplifies chemoinformatics tasks based on molecular fingerprints, including molecular property prediction and virtual screening. It is also flexible, highly efficient, and fully open source.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FQGA-single: Towards Fewer Training Epochs and Fewer Model Parameters for Image-to-Image Translation Tasks</title>
<link>https://arxiv.org/abs/2408.09218</link>
<guid>https://arxiv.org/abs/2408.09218</guid>
<content:encoded><![CDATA[
arXiv:2408.09218v4 Announce Type: replace-cross 
Abstract: This paper proposes a novel model inspired by CycleGAN: FQGA-single to produce high quality medical synthetic CT (sCT) generated images more efficiently. Evaluations were done on the SynthRAD Grand Challenge dataset with the CycleGAN model used for benchmarking and for comparing the quality of CBCT-to-sCT generated images from both a quantitative and qualitative perspective. Finally, this paper also explores ideas from the paper "One Epoch Is All You Need" to compare models trained on a single epoch versus multiple epochs. Astonishing results from FQGA-single were obtained during this exploratory experiment, which show that the performance of FQGA-single when trained on a single epoch surpasses itself when trained on multiple epochs. More surprising is that its performance also surpasses CycleGAN's multiple-epoch and single-epoch models, and even a modified version of CycleGAN.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio</title>
<link>https://arxiv.org/abs/2409.06624</link>
<guid>https://arxiv.org/abs/2409.06624</guid>
<content:encoded><![CDATA[
arXiv:2409.06624v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLM) often need to be Continual Pre-Trained (CPT) to obtain unfamiliar language skills or adapt to new domains. The huge training cost of CPT often asks for cautious choice of key hyper-parameters such as the mixture ratio of extra language or domain corpus. However, there is no systematic study that bridges the gap between the optimal mixture ratio and the actual model performance, and the gap between experimental scaling law and the actual deployment in the full model size. In this paper, we perform CPT on Llama-3 8B and 70B to enhance its Chinese ability. We study the optimal correlation between the Additional Language Mixture Ratio (ALMR) and the Learning Rate (LR) on the 8B size which directly indicates the optimal experimental setup. By thorough choice of hyper-parameter, and subsequent fine-tuning, the model capability is improved not only on the Chinese-related benchmark but also in some specific domains including math, coding, and emotional intelligence. We deploy the final 70B version of LLM on a real-life chat system which obtains satisfying performance.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-Language Pretraining for Highly Label-Efficient Clinical Phenotyping</title>
<link>https://arxiv.org/abs/2409.07480</link>
<guid>https://arxiv.org/abs/2409.07480</guid>
<content:encoded><![CDATA[
arXiv:2409.07480v4 Announce Type: replace-cross 
Abstract: Multimodal language modeling has enabled breakthroughs for representation learning, yet remains unexplored in the realm of functional brain data for clinical phenotyping. This paper pioneers EEG-language models (ELMs) trained on clinical reports and 15000 EEGs. We propose to combine multimodal alignment in this novel domain with timeseries cropping and text segmentation, enabling an extension based on multiple instance learning to alleviate misalignment between irrelevant EEG or text segments. Our multimodal models significantly improve over EEG-only models across four clinical evaluations and for the first time enable zero-shot classification as well as retrieval of both neural signals and reports. In sum, these results highlight the potential of ELMs, representing significant progress for clinical applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for Efficient Adaptation</title>
<link>https://arxiv.org/abs/2409.07796</link>
<guid>https://arxiv.org/abs/2409.07796</guid>
<content:encoded><![CDATA[
arXiv:2409.07796v3 Announce Type: replace-cross 
Abstract: Resource-constrained IoT devices increasingly rely on deep learning models, however, these models experience significant accuracy drops due to domain shifts when encountering variations in lighting, weather, and seasonal conditions. While cloud-based retraining can address this issue, many IoT deployments operate with limited connectivity and energy constraints, making traditional fine-tuning approaches impractical. We explore this challenge through the lens of wildlife ecology, where camera traps must maintain accurate species classification across changing seasons, weather, and habitats without reliable connectivity. We introduce WildFit, an autonomous in-situ adaptation framework that leverages the key insight that background scenes change more frequently than the visual characteristics of monitored species. WildFit combines background-aware synthesis to generate training samples on-device with drift-aware fine-tuning that triggers model updates only when necessary to conserve resources. Our background-aware synthesis surpasses efficient baselines by 7.3\% and diffusion models by 3.0\% while being orders of magnitude faster, our drift-aware fine-tuning achieves Pareto optimality with 50\% fewer updates and 1.5\% higher accuracy, and the end-to-end system outperforms domain adaptation approaches by 20--35%\% while consuming only 11.2 Wh over 37 days -- enabling battery-powered deployment.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-data-driven dynamical transition in quantum learning</title>
<link>https://arxiv.org/abs/2410.01955</link>
<guid>https://arxiv.org/abs/2410.01955</guid>
<content:encoded><![CDATA[
arXiv:2410.01955v2 Announce Type: replace-cross 
Abstract: Quantum neural networks, parameterized quantum circuits optimized under a specific cost function, provide a paradigm for achieving near-term quantum advantage in quantum information processing. Understanding QNN training dynamics is crucial for optimizing their performance, however, the role of quantum data in training for supervised learning such as classification and regression remains unclear. We reveal a quantum-data-driven dynamical transition where the target values and data determine the convergence of the training. Through analytical classification over the fixed points of the dynamical equation, we reveal a comprehensive `phase diagram' featuring seven distinct dynamics originating from a bifurcation with multiple codimension. Perturbative analyses identify both exponential and polynomial convergence class. We provide a non-perturbative theory to explain the transition via generalized restricted Haar ensemble. The analytical results are confirmed with numerical simulations and experimentation on IBM quantum devices. Our findings provide guidance on constructing the cost function to accelerate convergence in QNN training.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Machine Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2410.08109</link>
<guid>https://arxiv.org/abs/2410.08109</guid>
<content:encoded><![CDATA[
arXiv:2410.08109v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlatQuant: Flatness Matters for LLM Quantization</title>
<link>https://arxiv.org/abs/2410.09426</link>
<guid>https://arxiv.org/abs/2410.09426</guid>
<content:encoded><![CDATA[
arXiv:2410.09426v4 Announce Type: replace-cross 
Abstract: Recently, quantization has been widely used for the compression and acceleration of large language models (LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still exhibit steep and dispersed distributions. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach that enhances the flatness of weights and activations. Our approach identifies optimal affine transformations for each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead of affine transformation, we apply Kronecker product with two lightweight matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments demonstrate that FlatQuant establishes a new state-of-the-art benchmark for quantization. For example, it achieves less than 1\% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by 7.5\%. Additionally, it provides up to 2.3x prefill speedup and 1.7x decoding speedup compared to the FP16 model. Code is available at: https://github.com/ruikangliu/FlatQuant.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts</title>
<link>https://arxiv.org/abs/2410.12777</link>
<guid>https://arxiv.org/abs/2410.12777</guid>
<content:encoded><![CDATA[
arXiv:2410.12777v2 Announce Type: replace-cross 
Abstract: With the rapid progress of diffusion-based content generation, significant efforts are being made to unlearn harmful or copyrighted concepts from pretrained diffusion models (DMs) to prevent potential model misuse. However, it is observed that even when DMs are properly unlearned before release, malicious finetuning can compromise this process, causing DMs to relearn the unlearned concepts. This occurs partly because certain benign concepts (e.g., "skin") retained in DMs are related to the unlearned ones (e.g., "nudity"), facilitating their relearning via finetuning. To address this, we propose meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes malicious finetuning on unlearned concepts, the related benign concepts retained within it will be triggered to self-destruct, hindering the relearning of unlearned concepts. Our meta-unlearning framework is compatible with most existing unlearning methods, requiring only the addition of an easy-to-implement meta objective. We validate our approach through empirical experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4 and SDXL), supported by extensive ablation studies. Our code is available at https://github.com/sail-sg/Meta-Unlearning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Decomposition with Unaligned Observations</title>
<link>https://arxiv.org/abs/2410.14046</link>
<guid>https://arxiv.org/abs/2410.14046</guid>
<content:encoded><![CDATA[
arXiv:2410.14046v2 Announce Type: replace-cross 
Abstract: This paper presents a canonical polyadic (CP) tensor decomposition that addresses unaligned observations. The mode with unaligned observations is represented using functions in a reproducing kernel Hilbert space (RKHS). We introduce a versatile loss function that effectively accounts for various types of data, including binary, integer-valued, and positive-valued types. Additionally, we propose an optimization algorithm for computing tensor decompositions with unaligned observations, along with a stochastic gradient method to enhance computational efficiency. A sketching algorithm is also introduced to further improve efficiency when using the $\ell_2$ loss function. To demonstrate the efficacy of our methods, we provide illustrative examples using both synthetic data and an early childhood human microbiome dataset.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Descent Finds Over-Parameterized Neural Networks with Sharp Generalization for Nonparametric Regression</title>
<link>https://arxiv.org/abs/2411.02904</link>
<guid>https://arxiv.org/abs/2411.02904</guid>
<content:encoded><![CDATA[
arXiv:2411.02904v5 Announce Type: replace-cross 
Abstract: We study nonparametric regression by an over-parameterized two-layer neural network trained by gradient descent (GD) in this paper. We show that, if the neural network is trained by GD with early stopping, then the trained network renders a sharp rate of the nonparametric regression risk of $\mathcal{O}(\epsilon_n^2)$, which is the same rate as that for the classical kernel regression trained by GD with early stopping, where $\epsilon_n$ is the critical population rate of the Neural Tangent Kernel (NTK) associated with the network and $n$ is the size of the training data. It is remarked that our result does not require distributional assumptions about the covariate as long as the covariate is bounded, in a strong contrast with many existing results which rely on specific distributions of the covariates such as the spherical uniform data distribution or distributions satisfying certain restrictive conditions. The rate $\mathcal{O}(\epsilon_n^2)$ is known to be minimax optimal for specific cases, such as the case that the NTK has a polynomial eigenvalue decay rate which happens under certain distributional assumptions on the covariates. Our result formally fills the gap between training a classical kernel regression model and training an over-parameterized but finite-width neural network by GD for nonparametric regression without distributional assumptions on the bounded covariate. We also provide confirmative answers to certain open questions or address particular concerns in the literature of training over-parameterized neural networks by GD with early stopping for nonparametric regression, including the characterization of the stopping time, the lower bound for the network width, and the constant learning rate used in GD.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TDDBench: A Benchmark for Training data detection</title>
<link>https://arxiv.org/abs/2411.03363</link>
<guid>https://arxiv.org/abs/2411.03363</guid>
<content:encoded><![CDATA[
arXiv:2411.03363v2 Announce Type: replace-cross 
Abstract: Training Data Detection (TDD) is a task aimed at determining whether a specific data instance is used to train a machine learning model. In the computer security literature, TDD is also referred to as Membership Inference Attack (MIA). Given its potential to assess the risks of training data breaches, ensure copyright authentication, and verify model unlearning, TDD has garnered significant attention in recent years, leading to the development of numerous methods. Despite these advancements, there is no comprehensive benchmark to thoroughly evaluate the effectiveness of TDD methods. In this work, we introduce TDDBench, which consists of 13 datasets spanning three data modalities: image, tabular, and text. We benchmark 21 different TDD methods across four detection paradigms and evaluate their performance from five perspectives: average detection performance, best detection performance, memory consumption, and computational efficiency in both time and memory. With TDDBench, researchers can identify bottlenecks and areas for improvement in TDD algorithms, while practitioners can make informed trade-offs between effectiveness and efficiency when selecting TDD algorithms for specific use cases. Our extensive experiments also reveal the generally unsatisfactory performance of TDD algorithms across different datasets. To enhance accessibility and reproducibility, we open-source TDDBench for the research community at https://github.com/zzh9568/TDDBench.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Policy Gradient in Reproducing Kernel Hilbert Space</title>
<link>https://arxiv.org/abs/2411.06650</link>
<guid>https://arxiv.org/abs/2411.06650</guid>
<content:encoded><![CDATA[
arXiv:2411.06650v5 Announce Type: replace-cross 
Abstract: Parametrised quantum circuits offer expressive and data-efficient representations for machine learning. Due to quantum states residing in a high-dimensional Hilbert space, parametrised quantum circuits have a natural interpretation in terms of kernel methods. The representation of quantum circuits in terms of quantum kernels has been studied widely in quantum supervised learning, but has been overlooked in the context of quantum RL. This paper proposes the use of kernel policies and quantum policy gradient algorithms for quantum-accessible environments. After discussing the properties of such policies and a demonstration of classical policy gradient on a coherent policy in a quantum environment, we propose parametric and non-parametric policy gradient and actor-critic algorithms with quantum kernel policies in quantum environments. This approach, implemented with both numerical and analytical quantum policy gradient techniques, allows exploiting the many advantages of kernel methods, including data-driven forms for functions (and their gradients) as well as tunable expressiveness. The proposed approach is suitable for vector-valued action spaces and each of the formulations demonstrates a quadratic reduction in query complexity compared to their classical counterparts. We propose actor-critic algorithms based on stochastic policy gradient, deterministic policy gradient, and natural policy gradient, and demonstrate additional query complexity reductions compared to quantum policy gradient algorithms under favourable conditions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pairwise Markov Chains for Volatility Forecasting</title>
<link>https://arxiv.org/abs/2411.11838</link>
<guid>https://arxiv.org/abs/2411.11838</guid>
<content:encoded><![CDATA[
arXiv:2411.11838v3 Announce Type: replace-cross 
Abstract: The Pairwise Markov Chain (PMC) is a probabilistic graphical model extending the well-known Hidden Markov Model. This model, although highly effective for many tasks, has been scarcely utilized for continuous value prediction. This is mainly due to the issue of modeling observations inherent in generative probabilistic models. In this paper, we introduce a new algorithm for prediction with the PMC. On the one hand, this algorithm allows circumventing the feature problem, thus fully exploiting the capabilities of the PMC. On the other hand, it enables the PMC to extend any predictive model by introducing hidden states, updated at each time step, and allowing the introduction of non-stationarity for any model. We apply the PMC with its new algorithm for volatility forecasting, which we compare to the highly popular GARCH(1,1) and feedforward neural models across numerous pairs. This is particularly relevant given the regime changes that we can observe in volatility. For each scenario, our algorithm enhances the performance of the extended model, demonstrating the value of our approach.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction of boosted and resolved multi-Higgs-boson events with symmetry-preserving attention networks</title>
<link>https://arxiv.org/abs/2412.03819</link>
<guid>https://arxiv.org/abs/2412.03819</guid>
<content:encoded><![CDATA[
arXiv:2412.03819v3 Announce Type: replace-cross 
Abstract: The production of multiple Higgs bosons at the CERN LHC provides a direct way to measure the trilinear and quartic Higgs self-interaction strengths as well as potential access to beyond the standard model effects that can enhance production at large transverse momentum $p_{\mathrm{T}}$. The largest event fraction arises from the fully hadronic final state in which every Higgs boson decays to a bottom quark-antiquark pair ($b\bar{b}$). This introduces a combinatorial challenge known as the \emph{jet assignment problem}: assigning jets to sets representing Higgs boson candidates. Symmetry-preserving attention networks (SPA-Nets) have been been developed to address this challenge. However, the complexity of jet assignment increases when simultaneously considering both $H\rightarrow b\bar{b}$ reconstruction possibilities, i.e., two "resolved" small-radius jets each containing a shower initiated by a $b$-quark or one "boosted" large-radius jet containing a merged shower initiated by a $b\bar{b}$ pair. The latter improves the reconstruction efficiency at high $p_{\mathrm{T}}$. In this work, we introduce a generalization to the SPA-Net approach to simultaneously consider both boosted and resolved reconstruction possibilities and unambiguously interpret an event as "fully resolved'', "fully boosted", or in between. We report the performance of baseline methods, the original SPA-Net approach, and our generalized version on nonresonant $HH$ and $HHH$ production at the LHC. Considering both boosted and resolved topologies, our SPA-Net approach increases the Higgs boson reconstruction purity by 57--62\% and the efficiency by 23--38\% compared to the baseline method depending on the final state.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation</title>
<link>https://arxiv.org/abs/2412.05148</link>
<guid>https://arxiv.org/abs/2412.05148</guid>
<content:encoded><![CDATA[
arXiv:2412.05148v2 Announce Type: replace-cross 
Abstract: Recent advancements in image generation models have enabled personalized image creation with both user-defined subjects (content) and styles. Prior works achieved personalization by merging corresponding low-rank adapters (LoRAs) through optimization-based methods, which are computationally demanding and unsuitable for real-time use on resource-constrained devices like smartphones. To address this, we introduce LoRA$.$rar, a method that not only improves image quality but also achieves a remarkable speedup of over $4000\times$ in the merging process. We collect a dataset of style and subject LoRAs and pre-train a hypernetwork on a diverse set of content-style LoRA pairs, learning an efficient merging strategy that generalizes to new, unseen content-style pairs, enabling fast, high-quality personalization. Moreover, we identify limitations in existing evaluation metrics for content-style quality and propose a new protocol using multimodal large language models (MLLMs) for more accurate assessment. Our method significantly outperforms the current state of the art in both content and style fidelity, as validated by MLLM assessments and human evaluations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mamba-based Deep Learning Approach for Sleep Staging on a Wireless Multimodal Wearable System without Electroencephalography</title>
<link>https://arxiv.org/abs/2412.15947</link>
<guid>https://arxiv.org/abs/2412.15947</guid>
<content:encoded><![CDATA[
arXiv:2412.15947v3 Announce Type: replace-cross 
Abstract: Study Objectives: We investigate a Mamba-based deep learning approach for sleep staging on signals from ANNE One (Sibel Health, Evanston, IL), a non-intrusive dual-module wireless wearable system measuring chest electrocardiography (ECG), triaxial accelerometry, and chest temperature, and finger photoplethysmography and finger temperature.
  Methods: We obtained wearable sensor recordings from 357 adults undergoing concurrent polysomnography (PSG) at a tertiary care sleep lab. Each PSG recording was manually scored and these annotations served as ground truth labels for training and evaluation of our models. PSG and wearable sensor data were automatically aligned using their ECG channels with manual confirmation by visual inspection. We trained a Mamba-based recurrent neural network architecture on these recordings. Ensembling of model variants with similar architectures was performed.
  Results: After ensembling, the model attains a 3-class (wake, non rapid eye movement [NREM] sleep, rapid eye movement [REM] sleep) balanced accuracy of 84.02%, F1 score of 84.23%, Cohen's $\kappa$ of 72.89%, and a Matthews correlation coefficient (MCC) score of 73.00%; a 4-class (wake, light NREM [N1/N2], deep NREM [N3], REM) balanced accuracy of 75.30%, F1 score of 74.10%, Cohen's $\kappa$ of 61.51%, and MCC score of 61.95%; a 5-class (wake, N1, N2, N3, REM) balanced accuracy of 65.11%, F1 score of 66.15%, Cohen's $\kappa$ of 53.23%, MCC score of 54.38%.
  Conclusions: Our Mamba-based deep learning model can successfully infer major sleep stages from the ANNE One, a wearable system without electroencephalography (EEG), and can be applied to data from adults attending a tertiary care sleep clinic.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean--Variance Portfolio Selection by Continuous-Time Reinforcement Learning: Algorithms, Regret Analysis, and Empirical Study</title>
<link>https://arxiv.org/abs/2412.16175</link>
<guid>https://arxiv.org/abs/2412.16175</guid>
<content:encoded><![CDATA[
arXiv:2412.16175v2 Announce Type: replace-cross 
Abstract: We study continuous-time mean--variance portfolio selection in markets where stock prices are diffusion processes driven by observable factors that are also diffusion processes, yet the coefficients of these processes are unknown. Based on the recently developed reinforcement learning (RL) theory for diffusion processes, we present a general data-driven RL algorithm that learns the pre-committed investment strategy directly without attempting to learn or estimate the market coefficients. For multi-stock Black--Scholes markets without factors, we further devise a baseline algorithm and prove its performance guarantee by deriving a sublinear regret bound in terms of the Sharpe ratio. For performance enhancement and practical implementation, we modify the baseline algorithm and carry out an extensive empirical study to compare its performance, in terms of a host of common metrics, with a large number of widely employed portfolio allocation strategies on S\&amp;P 500 constituents. The results demonstrate that the proposed continuous-time RL strategy is consistently among the best, especially in a volatile bear market, and decisively outperforms the model-based continuous-time counterparts by significant margins.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Observation Interference in Partially Observable Assistance Games</title>
<link>https://arxiv.org/abs/2412.17797</link>
<guid>https://arxiv.org/abs/2412.17797</guid>
<content:encoded><![CDATA[
arXiv:2412.17797v2 Announce Type: replace-cross 
Abstract: We study partially observable assistance games (POAGs), a model of the human-AI value alignment problem which allows the human and the AI assistant to have partial observations. Motivated by concerns of AI deception, we study a qualitatively new phenomenon made possible by partial observability: would an AI assistant ever have an incentive to interfere with the human's observations? First, we prove that sometimes an optimal assistant must take observation-interfering actions, even when the human is playing optimally, and even when there are otherwise-equivalent actions available that do not interfere with observations. Though this result seems to contradict the classic theorem from single-agent decision making that the value of information is nonnegative, we resolve this seeming contradiction by developing a notion of interference defined on entire policies. This can be viewed as an extension of the classic result that the value of information is nonnegative into the cooperative multiagent setting. Second, we prove that if the human is simply making decisions based on their immediate outcomes, the assistant might need to interfere with observations as a way to query the human's preferences. We show that this incentive for interference goes away if the human is playing optimally, or if we introduce a communication channel for the human to communicate their preferences to the assistant. Third, we show that if the human acts according to the Boltzmann model of irrationality, this can create an incentive for the assistant to interfere with observations. Finally, we use an experimental model to analyze tradeoffs faced by the AI assistant in practice when considering whether or not to take observation-interfering actions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and thermodynamically consistent hydrogen equation of state for planetary modeling with flow matching</title>
<link>https://arxiv.org/abs/2501.10594</link>
<guid>https://arxiv.org/abs/2501.10594</guid>
<content:encoded><![CDATA[
arXiv:2501.10594v2 Announce Type: replace-cross 
Abstract: Accurate determination of the equation of state of dense hydrogen is essential for understanding gas giants. Currently, there is still no consensus on methods for calculating its entropy, which play a fundamental role and can result in qualitatively different predictions for Jupiter's interior. Here, we investigate various aspects of entropy calculation for dense hydrogen based on ab initio molecular dynamics simulations. Specifically, we employ the recently developed flow matching method to validate the accuracy of the traditional thermodynamic integration approach. We then clearly identify pitfalls in previous attempts and propose a reliable framework for constructing the hydrogen equation of state, which is accurate and thermodynamically consistent across a wide range of temperature and pressure conditions. This allows us to conclusively address the long-standing discrepancies in Jupiter's adiabat among earlier studies, demonstrating the potential of our approach for providing reliable equations of state of diverse materials.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models</title>
<link>https://arxiv.org/abs/2501.13428</link>
<guid>https://arxiv.org/abs/2501.13428</guid>
<content:encoded><![CDATA[
arXiv:2501.13428v4 Announce Type: replace-cross 
Abstract: Large language models have achieved remarkable success in recent years, primarily due to the implementation of self-attention mechanisms. However, traditional Softmax attention suffers from numerical instability and reduced performance as the length of inference tokens increases. This paper addresses these issues by proposing a new design principle for attention, viewing it as a two-stage process. We first decompose the Softmax operation into a non-linear positivity transformation and an $l_1$-normalisation step, identifying the latter as essential for maintaining model performance. In the first stage, we replace the standard exponential function with the more numerically stable Softplus activation and introduce a dynamic scale factor based on invariance entropy, creating a novel attention mechanism that outperforms conventional Softmax attention. In the second stage, we introduce a re-weighting mechanism that sharpens the attention distribution, amplifying significant weights while diminishing weaker ones. This enables the model to concentrate more effectively on relevant tokens and fundamentally improves length extrapolation. When combined, this two-stage approach ensures numerical stability and dramatically improves length extrapolation, maintaining a nearly constant validation loss at 16$\times$ the training length while achieving superior results on challenging long-context retrieval tasks and standard downstream benchmarks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models</title>
<link>https://arxiv.org/abs/2501.15981</link>
<guid>https://arxiv.org/abs/2501.15981</guid>
<content:encoded><![CDATA[
arXiv:2501.15981v2 Announce Type: replace-cross 
Abstract: Assigning realistic materials to 3D models remains a significant challenge in computer graphics. We propose MatCLIP, a novel method that extracts shape- and lighting-insensitive descriptors of Physically Based Rendering (PBR) materials to assign plausible textures to 3D objects based on images, such as the output of Latent Diffusion Models (LDMs) or photographs. Matching PBR materials to static images is challenging because the PBR representation captures the dynamic appearance of materials under varying viewing angles, shapes, and lighting conditions. By extending an Alpha-CLIP-based model on material renderings across diverse shapes and lighting, and encoding multiple viewing conditions for PBR materials, our approach generates descriptors that bridge the domains of PBR representations with photographs or renderings, including LDM outputs. This enables consistent material assignments without requiring explicit knowledge of material relationships between different parts of an object. MatCLIP achieves a top-1 classification accuracy of 76.6%, outperforming state-of-the-art methods such as PhotoShape and MatAtlas by over 15 percentage points on publicly available datasets. Our method can be used to construct material assignments for 3D shape datasets such as ShapeNet, 3DCoMPaT++, and Objaverse. All code and data will be released.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Your Model Ranking on Chatbot Arena by Vote Rigging</title>
<link>https://arxiv.org/abs/2501.17858</link>
<guid>https://arxiv.org/abs/2501.17858</guid>
<content:encoded><![CDATA[
arXiv:2501.17858v2 Announce Type: replace-cross 
Abstract: Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be rigged to improve (or decrease) the ranking of a target model $m_{t}$. We first introduce a straightforward target-only rigging strategy that focuses on new battles involving $m_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about $1\%$ of new battles will involve $m_{t}$. To overcome this, we propose omnipresent rigging strategies, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m_{t}$, even if $m_{t}$ is not directly involved in the battle. We conduct experiments on around $1.7$ million historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging only hundreds of new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. Our code is available at https://github.com/sail-sg/Rigging-ChatbotArena.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization</title>
<link>https://arxiv.org/abs/2502.06891</link>
<guid>https://arxiv.org/abs/2502.06891</guid>
<content:encoded><![CDATA[
arXiv:2502.06891v3 Announce Type: replace-cross 
Abstract: Drug optimization has become increasingly crucial in light of fast-mutating virus strains and drug-resistant cancer cells. Nevertheless, it remains challenging as it necessitates retaining the beneficial properties of the original drug while simultaneously enhancing desired attributes beyond its scope. In this work, we aim to tackle this challenge by introducing ScaffoldGPT, a novel Generative Pretrained Transformer (GPT) designed for drug optimization based on molecular scaffolds. Our work comprises three key components: (1) A three-stage drug optimization approach that integrates pretraining, finetuning, and decoding optimization. (2) A novel two-phase incremental pre-training strategy for scaffold-based drug optimization. (3) A token-level decoding optimization strategy, Top-N, that enabling controlled, reward-guided generation using the pretrained or finetuned GPT. We demonstrate via a comprehensive evaluation on COVID and cancer benchmarks that ScaffoldGPT outperforms the competing baselines in drug optimization benchmarks, while excelling in preserving original functional scaffold and enhancing desired properties.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2502.19918</link>
<guid>https://arxiv.org/abs/2502.19918</guid>
<content:encoded><![CDATA[
arXiv:2502.19918v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) struggle with high computational time and error propagation during inference time, especially for complex tasks like math, puzzles, or coding requiring multi-step thinking. While existing reasoning models with chain-of-thoughts (CoT) can enable LLMs to do step-wise analysis and reflection, they often face the issue of wasting computation on less productive solutions and fail to make progress during inference time. In this paper, we propose Meta-Reasoner, a new framework to enable LLMs ``Think about how to think'', i.e., optimize the inference compute by adjusting strategies on how to reason during inference time. Inspired by dual-process theory, our method decouples the high-level strategy generation (e.g., backtracking, switching approaches, or restarting) from stepwise CoT generation via a lightweight progress report. The strategy module only consider the summarized version from the previous CoTs to propose new strategies accordingly. We employ the contextual multi-armed bandits (CMABs) for this module to iteratively evaluate the previous reasoning states and dynamically adjust the strategy to avoid reasoning get stuck in less productive paths during inference. Evaluations on math problems (e.g., Game-of-24, TheoremQA) and scientific problems (e.g., SciBench) demonstrate that our method improves performance by 9-12\% over previous SOTA methods while reducing inference time by 28-35\%. This approach also generalizes to other domains like creative writing, demonstrating its versatility for diverse reasoning-intensive problems using LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI-Powered Medical Image Synthesis: Insights from MedVQA-GI Challenge Using CLIP, Fine-Tuned Stable Diffusion, and Dream-Booth + LoRA</title>
<link>https://arxiv.org/abs/2502.20667</link>
<guid>https://arxiv.org/abs/2502.20667</guid>
<content:encoded><![CDATA[
arXiv:2502.20667v2 Announce Type: replace-cross 
Abstract: The MEDVQA-GI challenge addresses the integration of AI-driven text-to-image generative models in medical diagnostics, aiming to enhance diagnostic capabilities through synthetic image generation. Existing methods primarily focus on static image analysis and lack the dynamic generation of medical imagery from textual descriptions. This study intends to partially close this gap by introducing a novel approach based on fine-tuned generative models to generate dynamic, scalable, and precise images from textual descriptions. Particularly, our system integrates fine-tuned Stable Diffusion and DreamBooth models, as well as Low-Rank Adaptation (LORA), to generate high-fidelity medical images. The problem is around two sub-tasks namely: image synthesis (IS) and optimal prompt production (OPG). The former creates medical images via verbal prompts, whereas the latter provides prompts that produce high-quality images in specified categories. The study emphasizes the limitations of traditional medical image generation methods, such as hand sketching, constrained datasets, static procedures, and generic models. Our evaluation measures showed that Stable Diffusion surpasses CLIP and DreamBooth + LORA in terms of producing high-quality, diversified images. Specifically, Stable Diffusion had the lowest Fr\'echet Inception Distance (FID) scores (0.099 for single center, 0.064 for multi-center, and 0.067 for combined), indicating higher image quality. Furthermore, it had the highest average Inception Score (2.327 across all datasets), indicating exceptional diversity and quality. This advances the field of AI-powered medical diagnosis. Future research will concentrate on model refining, dataset augmentation, and ethical considerations for efficiently implementing these advances into clinical practice
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Introduction to Kernel Discrepancies: MMD, HSIC &amp; KSD</title>
<link>https://arxiv.org/abs/2503.04820</link>
<guid>https://arxiv.org/abs/2503.04820</guid>
<content:encoded><![CDATA[
arXiv:2503.04820v2 Announce Type: replace-cross 
Abstract: This article provides a practical introduction to kernel discrepancies, focusing on the Maximum Mean Discrepancy (MMD), the Hilbert-Schmidt Independence Criterion (HSIC), and the Kernel Stein Discrepancy (KSD). Various estimators for these discrepancies are presented, including the commonly-used V-statistics and U-statistics, as well as several forms of the more computationally-efficient incomplete U-statistics. The importance of the choice of kernel bandwidth is stressed, showing how it affects the behaviour of the discrepancy estimation. Adaptive estimators are introduced, which combine multiple estimators with various kernels, addressing the problem of kernel selection.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theory of Learning with Autoregressive Chain of Thought</title>
<link>https://arxiv.org/abs/2503.07932</link>
<guid>https://arxiv.org/abs/2503.07932</guid>
<content:encoded><![CDATA[
arXiv:2503.07932v2 Announce Type: replace-cross 
Abstract: For a given base class of sequence-to-next-token generators, we consider learning prompt-to-answer mappings obtained by iterating a fixed, time-invariant generator for multiple steps, thus generating a chain-of-thought, and then taking the final token as the answer. We formalize the learning problems both when the chain-of-thought is observed and when training only on prompt-answer pairs, with the chain-of-thought latent. We analyze the sample and computational complexity both in terms of general properties of the base class (e.g. its VC dimension) and for specific base classes such as linear thresholds. We present a simple base class that allows for universal representability and computationally tractable chain-of-thought learning. Central to our development is that time invariance allows for sample complexity that is independent of the length of the chain-of-thought. Attention arises naturally in our construction.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration of Hepatitis B Virus Infection Dynamics through Physics-Informed Deep Learning Approach</title>
<link>https://arxiv.org/abs/2503.10708</link>
<guid>https://arxiv.org/abs/2503.10708</guid>
<content:encoded><![CDATA[
arXiv:2503.10708v2 Announce Type: replace-cross 
Abstract: Accurate forecasting of viral disease outbreaks is crucial for guiding public health responses and preventing widespread loss of life. In recent years, Physics-Informed Neural Networks (PINNs) have emerged as a promising framework that can capture the intricate dynamics of viral infection and reliably predict its future progression. However, despite notable advances, the application of PINNs in disease modeling remains limited. Standard PINNs are effective in simulating disease dynamics through forward modeling but often face challenges in estimating key biological parameters from sparse or noisy experimental data when applied in an inverse framework. To overcome these limitations, a recent extension known as Disease Informed Neural Networks (DINNs) has emerged, offering a more robust approach to parameter estimation tasks. In this work, we apply this DINNs technique on a recently proposed hepatitis B virus (HBV) infection dynamics model to predict infection transmission within the liver. This model consists of four compartments: uninfected and infected hepatocytes, rcDNA-containing capsids, and free viruses. Leveraging the power of DINNs, we study the impacts of (i) variations in parameter range, (ii) experimental noise in data, (iii) sample sizes, (iv) network architecture and (v) learning rate. We employ this methodology in experimental data collected from nine HBV-infected chimpanzees and observe that it reliably estimates the model parameters. DINNs can capture infection dynamics and predict their future progression even when data of some compartments of the system are missing. Additionally, it identifies the influential model parameters that determine whether the HBV infection is cleared or persists within the host.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDiT: Efficient Diffusion Transformers with Linear Compressed Attention</title>
<link>https://arxiv.org/abs/2503.16726</link>
<guid>https://arxiv.org/abs/2503.16726</guid>
<content:encoded><![CDATA[
arXiv:2503.16726v2 Announce Type: replace-cross 
Abstract: Diffusion Transformers (DiTs) have emerged as a leading architecture for text-to-image synthesis, producing high-quality and photorealistic images. However, the quadratic scaling properties of the attention in DiTs hinder image generation with higher resolution or on devices with limited resources. This work introduces an efficient diffusion transformer (EDiT) to alleviate these efficiency bottlenecks in conventional DiTs and Multimodal DiTs (MM-DiTs). First, we present a novel linear compressed attention method that uses a multi-layer convolutional network to modulate queries with local information while keys and values are aggregated spatially. Second, we formulate a hybrid attention scheme for multimodal inputs that combines linear attention for image-to-image interactions and standard scaled dot-product attention for interactions involving prompts. Merging these two approaches leads to an expressive, linear-time Multimodal Efficient Diffusion Transformer (MM-EDiT). We demonstrate the effectiveness of the EDiT and MM-EDiT architectures by integrating them into PixArt-Sigma (conventional DiT) and Stable Diffusion 3.5-Medium (MM-DiT), achieving up to 2.2x speedup with comparable image quality after distillation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Adaptive Dexterous Grasping from Single Demonstrations</title>
<link>https://arxiv.org/abs/2503.20208</link>
<guid>https://arxiv.org/abs/2503.20208</guid>
<content:encoded><![CDATA[
arXiv:2503.20208v2 Announce Type: replace-cross 
Abstract: How can robots learn dexterous grasping skills efficiently and apply them adaptively based on user instructions? This work tackles two key challenges: efficient skill acquisition from limited human demonstrations and context-driven skill selection. We introduce AdaDexGrasp, a framework that learns a library of grasping skills from a single human demonstration per skill and selects the most suitable one using a vision-language model (VLM). To improve sample efficiency, we propose a trajectory following reward that guides reinforcement learning (RL) toward states close to a human demonstration while allowing flexibility in exploration. To learn beyond the single demonstration, we employ curriculum learning, progressively increasing object pose variations to enhance robustness. At deployment, a VLM retrieves the appropriate skill based on user instructions, bridging low-level learned skills with high-level intent. We evaluate AdaDexGrasp in both simulation and real-world settings, showing that our approach significantly improves RL efficiency and enables learning human-like grasp strategies across varied object configurations. Finally, we demonstrate zero-shot transfer of our learned policies to a real-world PSYONIC Ability Hand, with a 90% success rate across objects, significantly outperforming the baseline.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning 3D-Gaussian Simulators from RGB Videos</title>
<link>https://arxiv.org/abs/2503.24009</link>
<guid>https://arxiv.org/abs/2503.24009</guid>
<content:encoded><![CDATA[
arXiv:2503.24009v2 Announce Type: replace-cross 
Abstract: Realistic simulation is critical for applications ranging from robotics to animation. Learned simulators have emerged as a possibility to capture real world physics directly from video data, but very often require privileged information such as depth information, particle tracks and hand-engineered features to maintain spatial and temporal consistency. These strong inductive biases or ground truth 3D information help in domains where data is sparse but limit scalability and generalization in data rich regimes. To overcome the key limitations, we propose 3DGSim, a learned 3D simulator that directly learns physical interactions from multi-view RGB videos. 3DGSim unifies 3D scene reconstruction, particle dynamics prediction and video synthesis into an end-to-end trained framework. It adopts MVSplat to learn a latent particle-based representation of 3D scenes, a Point Transformer for particle dynamics, a Temporal Merging module for consistent temporal aggregation and Gaussian Splatting to produce novel view renderings. By jointly training inverse rendering and dynamics forecasting, 3DGSim embeds the physical properties into point-wise latent features. This enables the model to capture diverse physical behaviors, from rigid to elastic, cloth-like dynamics, and boundary conditions (e.g. fixed cloth corner), along with realistic lighting effects that also generalize to unseen multibody interactions and novel scene edits.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence</title>
<link>https://arxiv.org/abs/2504.02904</link>
<guid>https://arxiv.org/abs/2504.02904</guid>
<content:encoded><![CDATA[
arXiv:2504.02904v2 Announce Type: replace-cross 
Abstract: Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training. Our code is publicly available at https://github.com/HZD01/post-training-mechanistic-analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering</title>
<link>https://arxiv.org/abs/2504.07583</link>
<guid>https://arxiv.org/abs/2504.07583</guid>
<content:encoded><![CDATA[
arXiv:2504.07583v3 Announce Type: replace-cross 
Abstract: Despite the steady progress in machine translation evaluation, existing automatic metrics struggle to capture how well meaning is preserved beyond sentence boundaries. We posit that reliance on a single intrinsic quality score, trained to mimic human judgments, might be insufficient for evaluating translations of long, complex passages, and a more ``pragmatic'' approach that assesses how accurately key information is conveyed by a translation in context is needed. We introduce TREQA (Translation Evaluation via Question-Answering), a framework that extrinsically evaluates translation quality by assessing how accurately candidate translations answer reading comprehension questions that target key information in the original source or reference texts. In challenging domains that require long-range understanding, such as literary texts, we show that TREQA is competitive with and, in some cases, outperforms state-of-the-art neural and LLM-based metrics in ranking alternative paragraph-level translations, despite never being explicitly optimized to correlate with human judgments. Furthermore, the generated questions and answers offer interpretability: empirical analysis shows that they effectively target translation errors identified by experts in evaluated datasets. Our code is available at https://github.com/deep-spin/treqa
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Relevance Emerges: Interpreting LoRA Fine-Tuning in Reranking LLMs</title>
<link>https://arxiv.org/abs/2504.08780</link>
<guid>https://arxiv.org/abs/2504.08780</guid>
<content:encoded><![CDATA[
arXiv:2504.08780v3 Announce Type: replace-cross 
Abstract: We conduct a behavioral exploration of LoRA fine-tuned LLMs for Passage Reranking to understand how relevance signals are learned and deployed by Large Language Models. By fine-tuning Mistral-7B, LLaMA3.1-8B, and Pythia-6.9B on MS MARCO under diverse LoRA configurations, we investigate how relevance modeling evolves across checkpoints, the impact of LoRA rank (1, 2, 8, 32), and the relative importance of updated MHA vs. MLP components. Our ablations reveal which layers and projections within LoRA transformations are most critical for reranking accuracy. These findings offer fresh explanations into LoRA's adaptation mechanisms, setting the stage for deeper mechanistic studies in Information Retrieval. All models used in this study have been shared.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TFMPathy: Tabular Foundation Model for Privacy-Aware, Generalisable Empathy Detection from Videos</title>
<link>https://arxiv.org/abs/2504.10808</link>
<guid>https://arxiv.org/abs/2504.10808</guid>
<content:encoded><![CDATA[
arXiv:2504.10808v2 Announce Type: replace-cross 
Abstract: Detecting empathy from video interactions is an emerging area of research, particularly in healthcare and social robotics. However, privacy and ethical concerns often prevent the release of raw video data, with many datasets instead shared as pre-extracted tabular features. Previous work on such datasets has established classical tree-based models as the state of the art. Motivated by recent successes of large-scale foundation models for text, we investigate the potential of tabular foundation models (TFMs) for empathy detection from video-derived tabular data. Our proposed system, TFMPathy, is demonstrated with two recent TFMs (TabPFN v2 and TabICL) under both in-context learning and fine-tuning paradigms. On a public human-robot interaction benchmark, TFMPathy significantly improves empathy detection accuracy reported in the literature. While the established evaluation protocol in the literature does not ensure cross-subject generalisation, our evaluation scheme also captures such generalisation. We show that TFMPathy under a fine-tuning setup has better cross-subject generalisation capacity over baseline methods (accuracy: $0.590 \rightarrow 0.730$; AUC: $0.564 \rightarrow 0.669$). Given the ongoing privacy and ethical constraints around raw video sharing, the proposed TFMPathy system provides a practical and scalable path toward building AI systems dependent on human-centred video datasets. Our code is publicly available at https://github.com/hasan-rakibul/TFMPathy (will be made available upon acceptance of this paper).
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Video-Based Driver Activity Recognition under Noisy Labels</title>
<link>https://arxiv.org/abs/2504.11966</link>
<guid>https://arxiv.org/abs/2504.11966</guid>
<content:encoded><![CDATA[
arXiv:2504.11966v2 Announce Type: replace-cross 
Abstract: As an open research topic in the field of deep learning, learning with noisy labels has attracted much attention and grown rapidly over the past ten years. Learning with label noise is crucial for driver distraction behavior recognition, as real-world video data often contains mislabeled samples, impacting model reliability and performance. However, label noise learning is barely explored in the driver activity recognition field. In this paper, we propose the first label noise learning approach for the driver activity recognition task. Based on the cluster assumption, we initially enable the model to learn clustering-friendly low-dimensional representations from given videos and assign the resultant embeddings into clusters. We subsequently perform co-refinement within each cluster to smooth the classifier outputs. Furthermore, we propose a flexible sample selection strategy that combines two selection criteria without relying on any hyperparameters to filter clean samples from the training dataset. We also incorporate a self-adaptive parameter into the sample selection process to enforce balancing across classes. A comprehensive variety of experiments on the public Drive&amp;Act dataset for all granularity levels demonstrates the superior performance of our method in comparison with other label-denoising methods derived from the image classification field. The source code is available at https://github.com/ilonafan/DAR-noisy-labels.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>El Agente: An Autonomous Agent for Quantum Chemistry</title>
<link>https://arxiv.org/abs/2505.02484</link>
<guid>https://arxiv.org/abs/2505.02484</guid>
<content:encoded><![CDATA[
arXiv:2505.02484v2 Announce Type: replace-cross 
Abstract: Computational chemistry tools are widely used to study the behaviour of chemical phenomena. Yet, the complexity of these tools can make them inaccessible to non-specialists and challenging even for experts. In this work, we introduce El Agente Q, an LLM-based multi-agent system that dynamically generates and executes quantum chemistry workflows from natural language user prompts. The system is built on a novel cognitive architecture featuring a hierarchical memory framework that enables flexible task decomposition, adaptive tool selection, post-analysis, and autonomous file handling and submission. El Agente Q is benchmarked on six university-level course exercises and two case studies, demonstrating robust problem-solving performance (averaging >87% task success) and adaptive error handling through in situ debugging. It also supports longer-term, multi-step task execution for more complex workflows, while maintaining transparency through detailed action trace logs. Together, these capabilities lay the foundation for increasingly autonomous and accessible quantum chemistry.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RIDGECUT: Learning Graph Partitioning with Rings and Wedges</title>
<link>https://arxiv.org/abs/2505.13986</link>
<guid>https://arxiv.org/abs/2505.13986</guid>
<content:encoded><![CDATA[
arXiv:2505.13986v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has proven to be a powerful tool for combinatorial optimization (CO) problems due to its ability to learn heuristics that can generalize across problem instances. However, integrating knowledge that will steer the RL framework for CO solutions towards domain appropriate outcomes remains a challenging task. In this paper, we propose RIDGECUT, the first RL framework that constrains the action space to enforce structure-aware partitioning in the Normalized Cut problem. Using transportation networks as a motivating example, we introduce a novel concept that leverages domain knowledge about urban road topology -- where natural partitions often take the form of concentric rings and radial wedges. Our method reshapes the graph into a linear or circular structure to simplify the partitioning task so that we can apply sequential transformers and enables efficient learning via Proximal Policy Optimization. The resulting partitions are not only aligned with expected spatial layouts but also achieve lower normalized cuts compared to existing methods. While we focus on traffic data, our approach is broadly applicable and offers a mechanism for embedding structural priors into RL for graph partitioning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning</title>
<link>https://arxiv.org/abs/2505.23849</link>
<guid>https://arxiv.org/abs/2505.23849</guid>
<content:encoded><![CDATA[
arXiv:2505.23849v2 Announce Type: replace-cross 
Abstract: Privacy-Preserving Federated Learning (PPFL) is a decentralized machine learning approach where multiple clients train a model collaboratively. PPFL preserves the privacy and security of a client's data without exchanging it. However, ensuring that data at each client is of high quality and ready for federated learning (FL) is a challenge due to restricted data access. In this paper, we introduce CADRE (Customizable Assurance of Data Readiness) for federated learning (FL), a novel framework that allows users to define custom data readiness (DR) metrics, rules, and remedies tailored to specific FL tasks. CADRE generates comprehensive DR reports based on the user-defined metrics, rules, and remedies to ensure datasets are prepared for FL while preserving privacy. We demonstrate a practical application of CADRE by integrating it into an existing PPFL framework. We conducted experiments across six datasets and addressed seven different DR issues. The results illustrate the versatility and effectiveness of CADRE in ensuring DR across various dimensions, including data quality, privacy, and fairness. This approach enhances the performance and reliability of FL models as well as utilizes valuable resources.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey</title>
<link>https://arxiv.org/abs/2506.00098</link>
<guid>https://arxiv.org/abs/2506.00098</guid>
<content:encoded><![CDATA[
arXiv:2506.00098v2 Announce Type: replace-cross 
Abstract: Dexterous manipulation is a crucial yet highly complex challenge in humanoid robotics, demanding precise, adaptable, and sample-efficient learning methods. As humanoid robots are usually designed to operate in human-centric environments and interact with everyday objects, mastering dexterous manipulation is critical for real-world deployment. Traditional approaches, such as reinforcement learning and imitation learning, have made significant strides, but they often struggle due to the unique challenges of real-world dexterous manipulation, including high-dimensional control, limited training data, and covariate shift. This survey provides a comprehensive overview of these challenges and reviews existing learning-based methods for real-world dexterous manipulation, spanning imitation learning, reinforcement learning, and hybrid approaches. A promising yet underexplored direction is interactive imitation learning, where human feedback actively refines a robots behavior during training. While interactive imitation learning has shown success in various robotic tasks, its application to dexterous manipulation remains limited. To address this gap, we examine current interactive imitation learning techniques applied to other robotic tasks and discuss how these methods can be adapted to enhance dexterous manipulation. By synthesizing state-of-the-art research, this paper highlights key challenges, identifies gaps in current methodologies, and outlines potential directions for leveraging interactive imitation learning to improve dexterous robotic skills.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verbal Werewolf: Engage Users with Verbalized Agentic Werewolf Game Framework</title>
<link>https://arxiv.org/abs/2506.00160</link>
<guid>https://arxiv.org/abs/2506.00160</guid>
<content:encoded><![CDATA[
arXiv:2506.00160v2 Announce Type: replace-cross 
Abstract: The growing popularity of social deduction games has created an increasing need for intelligent frameworks where humans can collaborate with AI agents, particularly in post-pandemic contexts with heightened psychological and social pressures. Social deduction games like Werewolf, traditionally played through verbal communication, present an ideal application for Large Language Models (LLMs) given their advanced reasoning and conversational capabilities. Prior studies have shown that LLMs can outperform humans in Werewolf games, but their reliance on external modules introduces latency that left their contribution in academic domain only, and omit such game should be user-facing. We propose \textbf{Verbal Werewolf}, a novel LLM-based Werewolf game system that optimizes two parallel pipelines: gameplay powered by state-of-the-art LLMs and a fine-tuned Text-to-Speech (TTS) module that brings text output to life. Our system operates in near real-time without external decision-making modules, leveraging the enhanced reasoning capabilities of modern LLMs like DeepSeek V3 to create a more engaging and anthropomorphic gaming experience that significantly improves user engagement compared to existing text-only frameworks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.00826</link>
<guid>https://arxiv.org/abs/2506.00826</guid>
<content:encoded><![CDATA[
arXiv:2506.00826v2 Announce Type: replace-cross 
Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs) by incorporating diverse modalities such as images and text. multimodal knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals to infer missing facts, thereby mitigating the intrinsic incompleteness of MMKGs. Existing MMKGC methods typically leverage only the information contained in the MMKGs under the closed-world assumption and adopt discriminative training objectives, which limits their reasoning capacity during completion. Recent large language models (LLMs), empowered by massive parameter scales and pretraining on vast corpora, have demonstrated strong reasoning abilities across various tasks. However, their potential in MMKGC remains largely unexplored. To bridge this gap, we propose HERGC, a flexible Heterogeneous Experts Representation and Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves a compact candidate set for each incomplete triple. It then uses a Generative LLM Predictor, implemented via either in-context learning or lightweight fine-tuning, to accurately identify the correct answer from these candidates. Extensive experiments on three standard MMKG benchmarks demonstrate HERGC's effectiveness and robustness, achieving superior performance over existing methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data</title>
<link>https://arxiv.org/abs/2506.01189</link>
<guid>https://arxiv.org/abs/2506.01189</guid>
<content:encoded><![CDATA[
arXiv:2506.01189v2 Announce Type: replace-cross 
Abstract: Despite progress in the rapidly developing field of geometric deep learning, performing statistical analysis on geometric data--where each observation is a shape such as a curve, graph, or surface--remains challenging due to the non-Euclidean nature of shape spaces, which are defined as equivalence classes under invariance groups. Building machine learning frameworks that incorporate such invariances, notably to shape parametrization, is often crucial to ensure generalizability of the trained models to new observations. This work proposes \textit{SVarM} to exploit varifold representations of shapes as measures and their duality with test functions $h:\mathbb{R}^n \times S^{n-1} \rightarrow \mathbb{R}$. This method provides a general framework akin to linear support vector machines but operating instead over the infinite-dimensional space of varifolds. We develop classification and regression models on shape datasets by introducing a neural network-based representation of the trainable test function $h$. This approach demonstrates strong performance and robustness across various shape graph and surface datasets, achieving results comparable to state-of-the-art methods while significantly reducing the number of trainable parameters.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLOps with Microservices: A Case Study on the Maritime Domain</title>
<link>https://arxiv.org/abs/2506.06202</link>
<guid>https://arxiv.org/abs/2506.06202</guid>
<content:encoded><![CDATA[
arXiv:2506.06202v2 Announce Type: replace-cross 
Abstract: This case study describes challenges and lessons learned on building Ocean Guard: a Machine Learning-Enabled System (MLES) for anomaly detection in the maritime domain. First, the paper presents the system's specification, and architecture. Ocean Guard was designed with a microservices' architecture to enable multiple teams to work on the project in parallel. Then, the paper discusses how the developers adapted contract-based design to MLOps for achieving that goal. As a MLES, Ocean Guard employs code, model, and data contracts to establish guidelines between its services. This case study hopes to inspire software engineers, machine learning engineers, and data scientists to leverage similar approaches for their systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting</title>
<link>https://arxiv.org/abs/2506.08049</link>
<guid>https://arxiv.org/abs/2506.08049</guid>
<content:encoded><![CDATA[
arXiv:2506.08049v3 Announce Type: replace-cross 
Abstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions from several weeks to months in advance, represents a critical frontier for agricultural planning, energy management, and disaster preparedness. However, it remains one of the most challenging problems in atmospheric science, due to the chaotic dynamics of atmospheric systems and complex interactions across multiple scales. Current approaches often fail to explicitly model underlying physical processes and teleconnections that are crucial at S2S timescales. We introduce \textbf{TelePiT}, a novel deep learning architecture that enhances global S2S forecasting through integrated multi-scale physics and teleconnection awareness. Our approach consists of three key components: (1) Spherical Harmonic Embedding, which accurately encodes global atmospheric variables onto spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which explicitly captures atmospheric physical processes across multiple learnable frequency bands; (3) Teleconnection-Aware Transformer, which models critical global climate interactions through explicitly modeling teleconnection patterns into the self-attention. Extensive experiments demonstrate that \textbf{TelePiT} significantly outperforms state-of-the-art data-driven baselines and operational numerical weather prediction systems across all forecast horizons, marking a significant advance toward reliable S2S forecasting.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniFluids: Physics Pre-trained Modeling of Fluid Dynamics</title>
<link>https://arxiv.org/abs/2506.10862</link>
<guid>https://arxiv.org/abs/2506.10862</guid>
<content:encoded><![CDATA[
arXiv:2506.10862v2 Announce Type: replace-cross 
Abstract: Computational fluid dynamics (CFD) drives progress in numerous scientific and engineering fields, yet high-fidelity simulations remain computationally prohibitive. While machine learning approaches offer computing acceleration, they typically specialize in single physical systems or require extensive training data, hindering their applicability in highly nonlinear and 3D flow scenarios. To overcome these limitations, we propose OmniFluids, a pure physics pre-trained model that captures fundamental fluid dynamics laws and adapts efficiently to diverse downstream tasks with minimal data. We develop a training framework combining physics-only pre-training, coarse-grid operator distillation, and few-shot fine-tuning. This enables OmniFluids to retain broad physics knowledge while delivering fast and accurate predictions. Architecturally, OmniFluids integrates a mixture of operators, a multi-frame decoder, and factorized Fourier layers, seamlessly incorporating physics-based supervision while allowing efficient and scalable modeling of diverse tasks. Extensive tests on a broad range of 2D and 3D benchmarks show that OmniFluids outperforms state-of-the-art AI-driven methods in terms of flow field prediction and turbulence statistics. It delivers 10--100$\times$ speedups over traditional solvers while maintaining a comparable accuracy and accurately identifies unknown physical parameters from sparse, noisy data. This work demonstrates the potential of training a unified CFD solver exclusively from physics knowledge, offering a new approach for efficient and generalizable modeling across complex fluid systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Time Gaze Refinement for Micro-Expression Recognition: Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing</title>
<link>https://arxiv.org/abs/2506.12524</link>
<guid>https://arxiv.org/abs/2506.12524</guid>
<content:encoded><![CDATA[
arXiv:2506.12524v3 Announce Type: replace-cross 
Abstract: Event-based eye tracking holds significant promise for fine-grained cognitive state inference, offering high temporal resolution and robustness to motion artifacts, critical features for decoding subtle mental states such as attention, confusion, or fatigue. In this work, we introduce a model-agnostic, inference-time refinement framework designed to enhance the output of existing event-based gaze estimation models without modifying their architecture or requiring retraining. Our method comprises two key post-processing modules: (i) Motion-Aware Median Filtering, which suppresses blink-induced spikes while preserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement, which aligns gaze predictions with cumulative event motion to reduce spatial jitter and temporal discontinuities. To complement traditional spatial accuracy metrics, we propose a novel Jitter Metric that captures the temporal smoothness of predicted gaze trajectories based on velocity regularity and local signal complexity. Together, these contributions significantly improve the consistency of event-based gaze signals, making them better suited for downstream tasks such as micro-expression analysis and mind-state decoding. Our results demonstrate consistent improvements across multiple baseline models on controlled datasets, laying the groundwork for future integration with multimodal affect recognition systems in real-world environments. Our code implementations can be found at https://github.com/eye-tracking-for-physiological-sensing/EyeLoRiN.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Certified Robustness Does Not (Yet) Imply Model Security</title>
<link>https://arxiv.org/abs/2506.13024</link>
<guid>https://arxiv.org/abs/2506.13024</guid>
<content:encoded><![CDATA[
arXiv:2506.13024v2 Announce Type: replace-cross 
Abstract: While certified robustness is widely promoted as a solution to adversarial examples in Artificial Intelligence systems, significant challenges remain before these techniques can be meaningfully deployed in real-world applications. We identify critical gaps in current research, including the paradox of detection without distinction, the lack of clear criteria for practitioners to evaluate certification schemes, and the potential security risks arising from users' expectations surrounding ``guaranteed" robustness claims. These create an alignment issue between how certifications are presented and perceived, relative to their actual capabilities. This position paper is a call to arms for the certification research community, proposing concrete steps to address these fundamental challenges and advance the field toward practical applicability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coupled Entropy: A Goldilocks Generalization for Complex Systems</title>
<link>https://arxiv.org/abs/2506.17229</link>
<guid>https://arxiv.org/abs/2506.17229</guid>
<content:encoded><![CDATA[
arXiv:2506.17229v3 Announce Type: replace-cross 
Abstract: The coupled entropy is proven to correct a flaw in the derivation of the Tsallis entropy and thereby solidify the theoretical foundations for analyzing the uncertainty of complex systems. The Tsallis entropy originated from considering power probabilities $p_i^q$ in which \textit{q} independent, identically-distributed random variables share the same state. The maximum entropy distribution was derived to be a \textit{q}-exponential, which is a member of the shape ($\kappa$), scale ($\sigma$) distributions. Unfortunately, the $q$-exponential parameters were treated as though valid substitutes for the shape and scale. This flaw causes a misinterpretation of the generalized temperature and an imprecise derivation of the generalized entropy. The coupled entropy is derived from the generalized Pareto distribution (GPD) and the Student's t distribution, whose shape derives from nonlinear sources and scale derives from linear sources of uncertainty. The Tsallis entropy of the GPD converges to one as $\kappa\rightarrow\infty$, which makes it too cold. The normalized Tsallis entropy (NTE) introduces a nonlinear term multiplying the scale and the coupling, making it too hot. The coupled entropy provides perfect balance, ranging from $\ln \sigma$ for $\kappa=0$ to $\sigma$ as $\kappa\rightarrow\infty$. One could say, the coupled entropy allows scientists, engineers, and analysts to eat their porridge, confident that its measure of uncertainty reflects the mathematical physics of the scale of non-exponential distributions while minimizing the dependence on the shape or nonlinear coupling. Examples of complex systems design including a coupled variation inference algorithm are reviewed.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017</title>
<link>https://arxiv.org/abs/2506.19877</link>
<guid>https://arxiv.org/abs/2506.19877</guid>
<content:encoded><![CDATA[
arXiv:2506.19877v2 Announce Type: replace-cross 
Abstract: Identifying suitable machine learning paradigms for intrusion detection remains critical for building effective and generalizable security solutions. In this study, we present a controlled comparison of four representative models - Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN), One-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on the CICIDS2017 dataset under two scenarios: detecting known attack types and generalizing to previously unseen threats. Our results show that supervised MLP and CNN achieve near-perfect accuracy on familiar attacks but suffer drastic recall drops on novel attacks. Unsupervised LOF attains moderate overall accuracy and high recall on unknown threats at the cost of elevated false alarms, while boundary-based OCSVM balances precision and recall best, demonstrating robust detection across both scenarios. These findings offer practical guidance for selecting IDS models in dynamic network environments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Adapter Design Tradeoffs for Low Resource Music Generation</title>
<link>https://arxiv.org/abs/2506.21298</link>
<guid>https://arxiv.org/abs/2506.21298</guid>
<content:encoded><![CDATA[
arXiv:2506.21298v2 Announce Type: replace-cross 
Abstract: Fine-tuning large-scale music generation models, such as MusicGen and Mustango, is a computationally expensive process, often requiring updates to billions of parameters and, therefore, significant hardware resources. Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based methods, have emerged as a promising alternative, enabling adaptation with minimal trainable parameters while preserving model performance. However, the design choices for adapters, including their architecture, placement, and size, are numerous, and it is unclear which of these combinations would produce optimal adapters and why, for a given case of low-resource music genre. In this paper, we attempt to answer this question by studying various adapter configurations for two AI music models, MusicGen and Mustango, on two genres: Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in capturing fine-grained local musical details such as ornamentations and short melodic phrases, while transformer-based adapters better preserve long-range dependencies crucial for structured improvisation. Additionally, we analyze computational resource requirements across different adapter scales, demonstrating how mid-sized adapters (40M parameters) achieve an optimal balance between expressivity and quality. Furthermore, we find that Mustango, a diffusion-based model, generates more diverse outputs with better adherence to the description in the input prompt while lacking in providing stability in notes, rhythm alignment, and aesthetics. Also, it is computationally intensive and requires significantly more time to train. In contrast, autoregressive models like MusicGen offer faster training and are more efficient, and can produce better quality output in comparison, but have slightly higher redundancy in their generations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal and Practical Batched Linear Bandit Algorithm</title>
<link>https://arxiv.org/abs/2507.08438</link>
<guid>https://arxiv.org/abs/2507.08438</guid>
<content:encoded><![CDATA[
arXiv:2507.08438v2 Announce Type: replace-cross 
Abstract: We study the linear bandit problem under limited adaptivity, known as the batched linear bandit. While existing approaches can achieve near-optimal regret in theory, they are often computationally prohibitive or underperform in practice. We propose BLAE, a novel batched algorithm that integrates arm elimination with regularized G-optimal design, achieving the minimax optimal regret (up to logarithmic factors in $T$) in both large-$K$ and small-$K$ regimes for the first time, while using only $O(\log\log T)$ batches. Our analysis introduces new techniques for batch-wise optimal design and refined concentration bounds. Crucially, BLAE demonstrates low computational overhead and strong empirical performance, outperforming state-of-the-art methods in extensive numerical evaluations. Thus, BLAE is the first algorithm to combine provable minimax-optimality in all regimes and practical superiority in batched linear bandits.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phase transition of the Sinkhorn-Knopp algorithm</title>
<link>https://arxiv.org/abs/2507.09711</link>
<guid>https://arxiv.org/abs/2507.09711</guid>
<content:encoded><![CDATA[
arXiv:2507.09711v2 Announce Type: replace-cross 
Abstract: The matrix scaling problem, particularly the Sinkhorn-Knopp algorithm, has been studied for over 60 years. In practice, the algorithm often yields high-quality approximations within just a few iterations. Theoretically, however, the best-known upper bound places it in the class of pseudopolynomial-time approximation algorithms. Meanwhile, the lower-bound landscape remains largely unexplored. Two fundamental questions persist: what accounts for the algorithm's strong empirical performance, and can a tight bound on its iteration count be established?
  For an $n\times n$ matrix, its normalized version is obtained by dividing each entry by its largest entry. We say that a normalized matrix has a density $\gamma$ if there exists a constant $\rho > 0$ such that one row or column has exactly $\lceil \gamma n \rceil$ entries with values at least $\rho$, and every other row and column has at least $\lceil \gamma n \rceil$ such entries.
  For the upper bound, we show that the Sinkhorn-Knopp algorithm produces a nearly doubly stochastic matrix in $O(\log n - \log \varepsilon)$ iterations and $\widetilde{O}(n^2)$ time for all nonnegative square matrices whose normalized version has a density $\gamma > 1/2$. Such matrices cover both the algorithm's principal practical inputs and its typical theoretical regime, and the $\widetilde{O}(n^2)$ runtime is optimal.
  For the lower bound, we establish a tight bound of $\widetilde{\Omega}\left(n^{1/2}/\varepsilon\right)$ iterations for positive matrices under the $\ell_2$-norm error measure. Moreover, for every $\gamma < 1/2$, there exists a matrix with density $\gamma$ for which the algorithm requires $\Omega\left(n^{1/2}/\varepsilon\right)$ iterations.
  In summary, our results reveal a sharp phase transition in the Sinkhorn-Knopp algorithm at the density threshold $\gamma = 1/2$.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening</title>
<link>https://arxiv.org/abs/2507.10461</link>
<guid>https://arxiv.org/abs/2507.10461</guid>
<content:encoded><![CDATA[
arXiv:2507.10461v2 Announce Type: replace-cross 
Abstract: Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Time-series Generation, Model Selection to Transfer Learning: A Comparative Review of Pixel-wise Approaches for Large-scale Crop Mapping</title>
<link>https://arxiv.org/abs/2507.12590</link>
<guid>https://arxiv.org/abs/2507.12590</guid>
<content:encoded><![CDATA[
arXiv:2507.12590v2 Announce Type: replace-cross 
Abstract: Crop mapping involves identifying and classifying crop types using spatial data, primarily derived from remote sensing imagery. This study presents the first comprehensive review of large-scale, pixel-wise crop mapping workflows, encompassing both conventional supervised methods and emerging transfer learning approaches. To identify the optimal time-series generation approaches and supervised crop mapping models, we conducted systematic experiments, comparing six widely adopted satellite image-based preprocessing methods, alongside eleven supervised pixel-wise classification models. Additionally, we assessed the synergistic impact of varied training sample sizes and variable combinations. Moreover, we identified optimal transfer learning techniques for different magnitudes of domain shift. The evaluation of optimal methods was conducted across five diverse agricultural sites. Landsat 8 served as the primary satellite data source. Labels come from CDL trusted pixels and field surveys.
  Our findings reveal three key insights. First, fine-scale interval preprocessing paired with Transformer models consistently delivered optimal performance for both supervised and transferable workflows. RF offered rapid training and competitive performance in conventional supervised learning and direct transfer to similar domains. Second, transfer learning techniques enhanced workflow adaptability, with UDA being effective for homogeneous crop classes while fine-tuning remains robust across diverse scenarios. Finally, workflow choice depends heavily on the availability of labeled samples. With a sufficient sample size, supervised training typically delivers more accurate and generalizable results. Below a certain threshold, transfer learning that matches the level of domain shift is a viable alternative to achieve crop mapping. All code is publicly available to encourage reproducibility practice.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Lung Disease Diagnosis via Semi-Supervised Machine Learning</title>
<link>https://arxiv.org/abs/2507.16845</link>
<guid>https://arxiv.org/abs/2507.16845</guid>
<content:encoded><![CDATA[
arXiv:2507.16845v2 Announce Type: replace-cross 
Abstract: Lung diseases, including lung cancer and COPD, are significant health concerns globally. Traditional diagnostic methods can be costly, time-consuming, and invasive. This study investigates the use of semi supervised learning methods for lung sound signal detection using a model combination of MFCC+CNN. By introducing semi supervised learning modules such as Mix Match, Co-Refinement, and Co Refurbishing, we aim to enhance the detection performance while reducing dependence on manual annotations. With the add-on semi-supervised modules, the accuracy rate of the MFCC+CNN model is 92.9%, an increase of 3.8% to the baseline model. The research contributes to the field of lung disease sound detection by addressing challenges such as individual differences, feature insufficient labeled data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal differentially private kernel learning with random projection</title>
<link>https://arxiv.org/abs/2507.17544</link>
<guid>https://arxiv.org/abs/2507.17544</guid>
<content:encoded><![CDATA[
arXiv:2507.17544v2 Announce Type: replace-cross 
Abstract: Differential privacy has become a cornerstone in the development of privacy-preserving learning algorithms. This work addresses optimizing differentially private kernel learning within the empirical risk minimization (ERM) framework. We propose a novel differentially private kernel ERM algorithm based on random projection in the reproducing kernel Hilbert space using Gaussian processes. Our method achieves minimax-optimal excess risk for both the squared loss and Lipschitz-smooth convex loss functions under a local strong convexity condition. We further show that existing approaches based on alternative dimension reduction techniques, such as random Fourier feature mappings or $\ell_2$ regularization, yield suboptimal generalization performance. Our key theoretical contribution also includes the derivation of dimension-free generalization bounds for objective perturbation-based private linear ERM -- marking the first such result that does not rely on noisy gradient-based mechanisms. Additionally, we obtain sharper generalization bounds for existing differentially private kernel ERM algorithms. Empirical evaluations support our theoretical claims, demonstrating that random projection enables statistically efficient and optimally private kernel learning. These findings provide new insights into the design of differentially private algorithms and highlight the central role of dimension reduction in balancing privacy and utility.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training</title>
<link>https://arxiv.org/abs/2507.17634</link>
<guid>https://arxiv.org/abs/2507.17634</guid>
<content:encoded><![CDATA[
arXiv:2507.17634v2 Announce Type: replace-cross 
Abstract: Recent advances in learning rate (LR) scheduling have demonstrated the effectiveness of decay-free approaches that eliminate the traditional decay phase while maintaining competitive performance. Model merging techniques have emerged as particularly promising solutions in this domain. We present Warmup-Stable and Merge (WSM), a general framework that establishes a formal connection between learning rate decay and model merging. WSM provides a unified theoretical foundation for emulating various decay strategies-including cosine decay, linear decay and inverse square root decay-as principled model averaging schemes, while remaining fully compatible with diverse optimization methods. Through extensive experiments, we identify merge duration-the training window for checkpoint aggregation-as the most critical factor influencing model performance, surpassing the importance of both checkpoint interval and merge quantity. Our framework consistently outperforms the widely-adopted Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on MMLU-Pro. The performance advantages extend to supervised fine-tuning scenarios, highlighting WSM's potential for long-term model refinement.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network</title>
<link>https://arxiv.org/abs/2508.00429</link>
<guid>https://arxiv.org/abs/2508.00429</guid>
<content:encoded><![CDATA[
arXiv:2508.00429v3 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain sparse. Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen LLM backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EngiBench: A Framework for Data-Driven Engineering Design Research</title>
<link>https://arxiv.org/abs/2508.00831</link>
<guid>https://arxiv.org/abs/2508.00831</guid>
<content:encoded><![CDATA[
arXiv:2508.00831v2 Announce Type: replace-cross 
Abstract: Engineering design optimization seeks to automatically determine the shapes, topologies, or parameters of components that maximize performance under given conditions. This process often depends on physics-based simulations, which are difficult to install, computationally expensive, and require domain-specific expertise. To mitigate these challenges, we introduce EngiBench, the first open-source library and datasets spanning diverse domains for data-driven engineering design. EngiBench provides a unified API and a curated set of benchmarks -- covering aeronautics, heat conduction, photonics, and more -- that enable fair, reproducible comparisons of optimization and machine learning algorithms, such as generative or surrogate models. We also release EngiOpt, a companion library offering a collection of such algorithms compatible with the EngiBench interface. Both libraries are modular, letting users plug in novel algorithms or problems, automate end-to-end experiment workflows, and leverage built-in utilities for visualization, dataset generation, feasibility checks, and performance analysis. We demonstrate their versatility through experiments comparing state-of-the-art techniques across multiple engineering design problems, an undertaking that was previously prohibitively time-consuming to perform. Finally, we show that these problems pose significant challenges for standard machine learning methods due to highly sensitive and constrained design manifolds.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARTA: Advancing Sparse Attention in Spiking Neural Networks via Spike-Timing-Based Prioritization</title>
<link>https://arxiv.org/abs/2508.01646</link>
<guid>https://arxiv.org/abs/2508.01646</guid>
<content:encoded><![CDATA[
<div> spiking neural networks, SNNs, temporal dynamics, spike-timing information, attention mechanism
Summary:
SPARTA, a novel framework for Spiking Neural Networks, incorporates temporal dynamics and precise timing information to enable efficient sparse attention mechanisms. By leveraging heterogeneous neuron dynamics and prioritizing tokens based on temporal cues, such as firing patterns and inter-spike intervals, SPARTA achieves 65.4% sparsity through competitive gating. This approach reduces attention complexity significantly, from O(N^2) to O(K^2), by selecting only the most salient tokens while maintaining high accuracy. The proposed method outperforms existing models on datasets like DVS-Gesture (98.78%) and achieves competitive results on CIFAR10-DVS (83.06%) and CIFAR-10 (95.3%). Utilizing spike timing dynamics not only enhances computational efficiency but also improves overall accuracy in spike-based processing, illustrating the potential benefits of leveraging temporal information in SNNs.<br /><br />Summary: <div>
arXiv:2508.01646v2 Announce Type: replace 
Abstract: Current Spiking Neural Networks (SNNs) underutilize the temporal dynamics inherent in spike-based processing, relying primarily on rate coding while overlooking precise timing information that provides rich computational cues. We propose SPARTA (Spiking Priority Attention with Resource-Adaptive Temporal Allocation), a framework that leverages heterogeneous neuron dynamics and spike-timing information to enable efficient sparse attention. SPARTA prioritizes tokens based on temporal cues, including firing patterns, spike timing, and inter-spike intervals, achieving 65.4% sparsity through competitive gating. By selecting only the most salient tokens, SPARTA reduces attention complexity from O(N^2) to O(K^2) with k << n, while maintaining high accuracy. Our method achieves state-of-the-art performance on DVS-Gesture (98.78%) and competitive results on CIFAR10-DVS (83.06%) and CIFAR-10 (95.3%), demonstrating that exploiting spike timing dynamics improves both computational efficiency and accuracy.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Fleet Upgrade Decisions with Machine-Learning Enhanced Optimization</title>
<link>https://arxiv.org/abs/2508.00915</link>
<guid>https://arxiv.org/abs/2508.00915</guid>
<content:encoded><![CDATA[
<div> Keywords: fleet upgrade, machine learning, integer programming, sustainability, automotive industry

Summary:
Fleet upgrade strategies are crucial for optimizing utility, cost, and sustainability in large machine and vehicle fleets. Conventional fleet optimization methods based on integer programming face challenges in scalability and computational cost. This study introduces an extended integer programming approach and a machine learning-based method to address these challenges. The machine learning approach transforms the optimization problem into a mixed discrete-continuous problem, leading to near-optimal solutions with improved scalability and computational performance. A real-world automotive industry case study validates the effectiveness of the machine learning approach in optimizing fleet renewal and upgrade decisions. Overall, this research provides practical solutions for large-scale fleet management, balancing efficiency, cost-effectiveness, and sustainability. 

<br /><br />Summary: <div>
arXiv:2508.00915v2 Announce Type: replace-cross 
Abstract: Rental-based business models and increasing sustainability requirements intensify the need for efficient strategies to manage large machine and vehicle fleet renewal and upgrades. Optimized fleet upgrade strategies maximize overall utility, cost, and sustainability. However, conventional fleet optimization does not account for upgrade options and is based on integer programming with exponential runtime scaling, which leads to substantial computational cost when dealing with large fleets and repeated decision-making processes. This contribution firstly suggests an extended integer programming approach that determines optimal renewal and upgrade decisions. The computational burden is addressed by a second, alternative machine learning-based method that transforms the task to a mixed discrete-continuous optimization problem. Both approaches are evaluated in a real-world automotive industry case study, which shows that the machine learning approach achieves near-optimal solutions with significant improvements in the scalability and overall computational performance, thus making it a practical alternative for large-scale fleet management.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty</title>
<link>https://arxiv.org/abs/2508.05659</link>
<guid>https://arxiv.org/abs/2508.05659</guid>
<content:encoded><![CDATA[
<div> method, causal loop diagrams, system dynamics models, exploratory, uncertainty 

Summary: 
The paper introduces Diagrams-to-Dynamics (D2D), a method that converts causal loop diagrams (CLDs) into system dynamics models (SDMs) to support dynamic analysis and intervention strategies in health and environmental research. D2D requires minimal user input to label variables and leverages structural information in CLDs for simulating hypothetical interventions and identifying leverage points. The results show that D2D helps differentiate between high- and low-ranked leverage points and provides uncertainty estimates. A comparison with a data-driven SDM demonstrates D2D's consistency and guidance for future data collection. The method, implemented in an open-source Python package and web-based application, aims to lower the barrier to dynamic modeling for researchers. Validation is needed to establish D2D's utility in various cases and domains. <br /><br />Summary: <div>
arXiv:2508.05659v1 Announce Type: new 
Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental research to represent hypothesized causal structures underlying complex problems. However, as qualitative and static representations, CLDs are limited in their ability to support dynamic analysis and inform intervention strategies. Additionally, quantitative CLD analysis methods like network centrality analysis often lead to false inference. We propose Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory system dynamics models (SDMs) in the absence of empirical data. With minimal user input - following a protocol to label variables as stocks, flows/auxiliaries, or constants - D2D leverages the structural information already encoded in CLDs, namely, link existence and polarity, to simulate hypothetical interventions and explore potential leverage points under uncertainty. Results suggest that D2D helps distinguish between high- and low-ranked leverage points. We compare D2D to a data-driven SDM constructed from the same CLD and variable labeling. D2D showed greater consistency with the data-driven model than network centrality analysis, while providing uncertainty estimates and guidance for future data collection. The method is implemented in an open-source Python package and a web-based application to support further testing and lower the barrier to dynamic modeling for researchers working with CLDs. We expect additional validation will further establish the approach's utility across a broad range of cases and domains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics</title>
<link>https://arxiv.org/abs/2508.05724</link>
<guid>https://arxiv.org/abs/2508.05724</guid>
<content:encoded><![CDATA[
<div> knowledge graph, physical laws, graph representation, link prediction, GAT

Summary:
- A novel framework for representing and analyzing physical laws as a weighted knowledge graph is introduced.
- A database of 400 advanced physics equations is constructed and subjected to rigorous semantic cleaning.
- An enhanced graph representation is developed, connecting physical concepts and equations with weighted inter-equation bridges.
- A Graph Attention Network (GAT) is trained for link prediction, achieving high performance compared to classical heuristics and other established GNN architectures.
- The model autonomously rediscovers the macroscopic structure of physics, identifies central hub equations, and generates computationally-derived hypotheses for cross-domain relationships, suggesting novel mathematical analogies for further theoretical investigation. <div>
arXiv:2508.05724v1 Announce Type: new 
Abstract: This work introduces a novel framework for representing and analyzing physical laws as a weighted knowledge graph. We constructed a database of 659 distinct physical equations, subjected to rigorous semantic cleaning to resolve notational ambiguities, resulting in a corpus of 400 advanced physics equations. We developed an enhanced graph representation where both physical concepts and equations are nodes, connected by weighted inter-equation bridges. These weights are objectively defined using normalized metrics for variable overlap, physics-informed importance scores, and bibliometric data. A Graph Attention Network (GAT) was trained for link prediction, achieving a test AUC of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming both classical heuristics (best baseline AUC: 0.9487) and established GNN architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing confirmed significance of all comparisons (p < 0.05), with 2.7% improvement over the best baseline. Our analysis reveals three key findings: (i) The model autonomously rediscovers the known macroscopic structure of physics, identifying strong conceptual axes between Electromagnetism and Statistical Mechanics. (ii) It identifies central hub equations that serve as critical bridges between multiple physical domains. (iii) The model generates stable, computationally-derived hypotheses for cross-domain relationships, identifying both known principles and suggesting novel mathematical analogies for further theoretical investigation. The framework can generate hundreds of such hypotheses, enabling the creation of specialized datasets for targeted analysis of specific physics subfields. Code and data available at https://github.com/kingelanci/graphysics
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems</title>
<link>https://arxiv.org/abs/2508.05778</link>
<guid>https://arxiv.org/abs/2508.05778</guid>
<content:encoded><![CDATA[
<div> neural network nudging, data assimilation, nonlinear state space models, chaotic behavior, Kazantzis--Kravaris--Luenberger observer theory
Summary:
Neural network nudging is a data-driven method for incorporating observation-driven control terms into nonlinear state space models. The technique, based on the Kazantzis--Kravaris--Luenberger observer theory, aims to learn effective nudging terms to improve model dynamics. It addresses the challenge of designing nudging terms in nonlinear settings, where traditional methods for linear models may not be effective. The approach is evaluated on chaotic benchmark problems like the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and the Kolmogorov flow. By utilizing neural networks, the method shows promise in improving data assimilation in complex nonlinear systems, allowing the nudged system to converge towards the true trajectory over time. <div>
arXiv:2508.05778v1 Announce Type: new 
Abstract: Nudging is an empirical data assimilation technique that incorporates an observation-driven control term into the model dynamics. The trajectory of the nudged system approaches the true system trajectory over time, even when the initial conditions differ. For linear state space models, such control terms can be derived under mild assumptions. However, designing effective nudging terms becomes significantly more challenging in the nonlinear setting. In this work, we propose neural network nudging, a data-driven method for learning nudging terms in nonlinear state space models. We establish a theoretical existence result based on the Kazantzis--Kravaris--Luenberger observer theory. The proposed approach is evaluated on three benchmark problems that exhibit chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and the Kolmogorov flow.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data</title>
<link>https://arxiv.org/abs/2508.05791</link>
<guid>https://arxiv.org/abs/2508.05791</guid>
<content:encoded><![CDATA[
<div> distribution grid topology, heterogeneous data, uncertainty-aware, physical feasibility, computational efficiency

Summary:<br />
Accurate distribution grid topology is crucial for modern grid operations. This study, conducted in partnership with Oncor Electric Delivery, presents a scalable framework that integrates diverse data sources to reconstruct a reliable grid topology. By combining spatial infrastructure data and system behavior information, the framework ensures a complete network connectivity representation. To tackle varying data quality and maintain observability, a confidence-aware inference mechanism is introduced to quantify the reliability of connections. This approach combines uncertainty management with physical constraints enforcement, such as transformer capacity limits and radial topology requirements. Validation with over 8000 meters in Oncor's territory shows high accuracy in topology reconstruction, improved confidence calibration, and enhanced computational efficiency compared to standard methods. This framework offers a practical solution for rapid and trustworthy topology reconstruction in real-world grid operations. 

<br /><br />Summary: <div>
arXiv:2508.05791v1 Announce Type: new 
Abstract: Accurate distribution grid topology is essential for reliable modern grid operations. However, real-world utility data originates from multiple sources with varying characteristics and levels of quality. In this work, developed in collaboration with Oncor Electric Delivery, we propose a scalable framework that reconstructs a trustworthy grid topology by systematically integrating heterogeneous data. We observe that distribution topology is fundamentally governed by two complementary dimensions: the spatial layout of physical infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the system in the signal domain (e.g., voltage time series). When jointly leveraged, these dimensions support a complete and physically coherent reconstruction of network connectivity. To address the challenge of uneven data quality without compromising observability, we introduce a confidence-aware inference mechanism that preserves structurally informative yet imperfect inputs, while quantifying the reliability of each inferred connection for operator interpretation. This soft handling of uncertainty is tightly coupled with hard enforcement of physical feasibility: we embed operational constraints, such as transformer capacity limits and radial topology requirements, directly into the learning process. Together, these components ensure that inference is both uncertainty-aware and structurally valid, enabling rapid convergence to actionable, trustworthy topologies under real-world deployment conditions. The proposed framework is validated using data from over 8000 meters across 3 feeders in Oncor's service territory, demonstrating over 95% accuracy in topology reconstruction and substantial improvements in confidence calibration and computational efficiency relative to baseline methods.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Linear Baseline Models for Scientific Machine Learning</title>
<link>https://arxiv.org/abs/2508.05831</link>
<guid>https://arxiv.org/abs/2508.05831</guid>
<content:encoded><![CDATA[
<div> encoder-decoder, linear neural networks, Bayes risk minimization, scientific machine learning, data-driven

Summary:<br />
This work focuses on analyzing linear encoder-decoder architectures using Bayes risk minimization for scientific machine learning tasks. The authors develop a theoretical framework for understanding the mappings from physical processes to observed signals, specifically using linear and affine linear optimal mappings. They address rank-deficiencies in data and measurement processes, providing closed-form solutions. Numerical experiments on biomedical imaging, financial factor analysis, and nonlinear fluid dynamics validate the theoretical results. This work establishes a strong foundation for interpreting and benchmarking neural network models in scientific machine learning applications. <div>
arXiv:2508.05831v1 Announce Type: new 
Abstract: Across scientific domains, a fundamental challenge is to characterize and compute the mappings from underlying physical processes to observed signals and measurements. While nonlinear neural networks have achieved considerable success, they remain theoretically opaque, which hinders adoption in contexts where interpretability is paramount. In contrast, linear neural networks serve as a simple yet effective foundation for gaining insight into these complex relationships. In this work, we develop a unified theoretical framework for analyzing linear encoder-decoder architectures through the lens of Bayes risk minimization for solving data-driven scientific machine learning problems. We derive closed-form, rank-constrained linear and affine linear optimal mappings for forward modeling and inverse recovery tasks. Our results generalize existing formulations by accommodating rank-deficiencies in data, forward operators, and measurement processes. We validate our theoretical results by conducting numerical experiments on datasets from simple biomedical imaging, financial factor analysis, and simulations involving nonlinear fluid dynamics via the shallow water equations. This work provides a robust baseline for understanding and benchmarking learned neural network models for scientific machine learning problems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Effective Approach for Node Classification in Textual Graphs</title>
<link>https://arxiv.org/abs/2508.05836</link>
<guid>https://arxiv.org/abs/2508.05836</guid>
<content:encoded><![CDATA[
<div> Keywords: Textual Attribute Graphs, TAPE, Graphormer, node classification, semantic information

Summary: 
In this study, the authors propose a novel framework that combines TAPE and Graphormer to improve node classification in Textual Attribute Graphs (TAGs). They utilize a large language model (LLM) ChatGPT to extract semantic information from paper content and integrate it with structural features using an attention mechanism. By incorporating path-aware position encoding and multi-head attention mechanisms from Graphormer, long-range dependencies in the citation network are effectively captured. The framework achieves state-of-the-art performance on the ogbn-arxiv dataset, surpassing GCN baselines in accuracy, precision, recall, and F1-score. Ablation studies demonstrate the contributions of each component and highlight the synergy between semantic and structural information. The proposed framework offers a scalable and robust solution for node classification in dynamic TAGs, with implications for knowledge systems and scientific discovery.

<br /><br />Summary: <div>
arXiv:2508.05836v1 Announce Type: new 
Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks like citation networks, but effective node classification remains challenging due to difficulties in integrating rich semantics from text with structural graph information. Existing methods often struggle with capturing nuanced domain-specific terminology, modeling long-range dependencies, adapting to temporal evolution, and scaling to massive datasets. To address these issues, we propose a novel framework that integrates TAPE (Text-Attributed Graph Representation Enhancement) with Graphormer. Our approach leverages a large language model (LLM), specifically ChatGPT, within the TAPE framework to generate semantically rich explanations from paper content, which are then fused into enhanced node representations. These embeddings are combined with structural features using a novel integration layer with learned attention weights. Graphormer's path-aware position encoding and multi-head attention mechanisms are employed to effectively capture long-range dependencies across the citation network. We demonstrate the efficacy of our framework on the challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a classification accuracy of 0.772, significantly surpassing the best GCN baseline of 0.713. Our method also yields strong results in precision (0.671), recall (0.577), and F1-score (0.610). We validate our approach through comprehensive ablation studies that quantify the contribution of each component, demonstrating the synergy between semantic and structural information. Our framework provides a scalable and robust solution for node classification in dynamic TAGs, offering a promising direction for future research in knowledge systems and scientific discovery.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance</title>
<link>https://arxiv.org/abs/2508.05876</link>
<guid>https://arxiv.org/abs/2508.05876</guid>
<content:encoded><![CDATA[
<div> MDP, collision avoidance maneuver, reinforcement learning policy gradient, autonomy, fuel consumption<br />
<br />
Summary: 
This work introduces a Markov decision process (MDP) framework for modeling decision-making in collision avoidance maneuver (CAM) and utilizes a reinforcement learning policy gradient (RL-PG) algorithm to train an autonomous guidance policy using historical CAM data. The focus is on balancing collision risks and minimizing fuel consumption by making early maneuver decisions. The MDP model incorporates factors such as conjunction risk, propellant consumption, and transit orbit geometry to determine the optimal time to initiate a maneuver. Results show that the trained policy effectively minimizes propellant consumption compared to a conventional cut-off policy while maintaining collision risk guarantees. The framework is validated on both synthetic and historical conjunction events, demonstrating the effectiveness of the trained policy in reducing average propellant consumption per CAM. Overall, the approach offers a promising method for improving decision-making in collision avoidance maneuvers. <br /> <div>
arXiv:2508.05876v1 Announce Type: new 
Abstract: This work presents a Markov decision process (MDP) framework to model decision-making for collision avoidance maneuver (CAM) and a reinforcement learning policy gradient (RL-PG) algorithm to train an autonomous guidance policy using historic CAM data. In addition to maintaining acceptable collision risks, this approach seeks to minimize the average fuel consumption of CAMs by making early maneuver decisions. We model CAM as a continuous state, discrete action and finite horizon MDP, where the critical decision is determining when to initiate the maneuver. The MDP model also incorporates analytical models for conjunction risk, propellant consumption, and transit orbit geometry. The Markov policy effectively trades-off maneuver delay-which improves the reliability of conjunction risk indicators-with propellant consumption-which increases with decreasing maneuver time. Using historical data of tracked conjunction events, we verify this framework and conduct an extensive ablation study on the hyper-parameters used within the MDP. On synthetic conjunction events, the trained policy significantly minimizes both the overall and average propellant consumption per CAM when compared to a conventional cut-off policy that initiates maneuvers 24 hours before the time of closest approach (TCA). On historical conjunction events, the trained policy consumes more propellant overall but reduces the average propellant consumption per CAM. For both historical and synthetic conjunction events, the trained policy achieves equal if not higher overall collision risk guarantees.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)</title>
<link>https://arxiv.org/abs/2508.05905</link>
<guid>https://arxiv.org/abs/2508.05905</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantization, Signed-Zero Ternary, Gradient information, Information density, Resource budget

Summary: 
Quantization, often seen as a compromise between performance and computational efficiency, takes on a new perspective when viewed within a fixed resource budget. The introduction of Signed-Zero Ternary (SZT), a 2-bit quantization method, offers deterministic gradient information without affecting the forward path. Analysis suggests that SZT may enhance information density compared to non-quantized approaches. This challenges the traditional view of quantization as a suboptimal approximation. By leveraging the benefits of SZT within a constrained resource framework, researchers may realize improved performance and efficiency in machine learning models. <div>
arXiv:2508.05905v1 Announce Type: new 
Abstract: Quantization is usually regarded as a means to trade quality of performance for reduced compute requirements, i.e., as a suboptimal approximation. However, if examined in terms of a fixed overall resource budget, a very different perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit quantization that deterministically provides gradient information with no forward-path penalty. Our analysis provides evidence that it may improve information density compared to non-quantized alternatives.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Signal Decomposition of Stochastic Time Series</title>
<link>https://arxiv.org/abs/2508.05915</link>
<guid>https://arxiv.org/abs/2508.05915</guid>
<content:encoded><![CDATA[
<div> Keywords: stochastic time series, decomposition, machine learning, denoising algorithm, neural networks 

Summary: 
The research paper introduces a method to decompose a stochastic time series into three components: mean, dispersion, and noise. Machine learning is utilized to fit the dual signal, minimizing a loss function that penalizes irregularities while fitting the original time series. Weighting of regularization components is based on Statistical Process Control methodology to preserve special patterns. Two learning approaches are considered: sequential (fitting mean first, then dispersion) and jointly (fitting both simultaneously). The learning process can involve solving non-linear optimization problems or using neural networks with different architectures. Hyperparameters of the loss function are tuned to isolate noise as a stationary stochastic process without autocorrelation. The decomposed dual signal can be used for various purposes such as forecasting, analyzing cross effects, or learning inherent structures. <div>
arXiv:2508.05915v1 Announce Type: new 
Abstract: The research paper addresses decomposition of a stochastic time series into three time series representing a dual signal i.e., the mean and the dispersion, with noise isolated. Decomposition is done by applying machine learning to fit a dual signal. Machine learning minimizes the loss function which compromises between fitting the original time series and penalizing irregularities of the dual signal. The latter includes terms based on the first and second order derivatives along time. To preserve special patterns, weighting of the regularization components of the loss function has been introduced based on Statistical Process Control methodology. The proposed decomposition can be applied as a smoothing algorithm against the mean and dispersion of the time series. By isolating noise, the proposed decomposition can be seen as a denoising algorithm. Two approaches of the learning process have been considered: sequential and jointly. The former approach learns the mean signal first and then dispersion. The latter approach fits the dual signal jointly. Jointly learning can uncover complex relationships for the time series with heteroskedasticity. Learning has been set by solving the direct non-linear unconstrained optimization problem or by applying neural networks that have sequential or twin output architectures. Tuning of the loss function hyperparameters focuses on the isolated noise to be a stationary stochastic process without autocorrelation properties. Depending on the applications, the hyperparameters of the learning can be tuned towards either the discrete states by stepped signal or smoothed series. The decomposed dual signal can be represented on the 2D space and used to learn inherent structures, to forecast both mean and dispersion, or to analyze cross effects in case of multiple time series.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations</title>
<link>https://arxiv.org/abs/2508.05921</link>
<guid>https://arxiv.org/abs/2508.05921</guid>
<content:encoded><![CDATA[
<div> Ill-conditioning, multi-fidelity, stiff problems, Physics-Informed Extreme Learning Machines (PIELMs), Shifted Gaussian Encoding <br />
Summary: <br />
The study addresses accuracy issues in neural PDE solvers, attributing breakdowns to poor optimization caused by ill-conditioning, particularly in multi-fidelity and stiff problems. The focus is on Physics-Informed Extreme Learning Machines (PIELMs), a neural PDE solver variant, where asymptotic components in governing equations lead to highly ill-conditioned activation matrices and limit convergence. The introduction of Shifted Gaussian Encoding, a simple activation filtering step, increases matrix rank and expressivity while maintaining convexity. This method extends the solvable range of Peclet numbers in steady advection-diffusion equations significantly, enhances performance in multi-frequency function learning, and enables more accurate and faster fitting of high-fidelity image vectors compared to deep networks with a million parameters. The research underscores the importance of addressing conditioning rather than depth in scientific neural solvers and illustrates how straightforward architectural adjustments can yield substantial improvements. <div>
arXiv:2508.05921v1 Announce Type: new 
Abstract: Accuracy in neural PDE solvers often breaks down not because of limited expressivity, but due to poor optimisation caused by ill-conditioning, especially in multi-fidelity and stiff problems. We study this issue in Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural PDE solvers, and show that asymptotic components in governing equations can produce highly ill-conditioned activation matrices, severely limiting convergence. We introduce Shifted Gaussian Encoding, a simple yet effective activation filtering step that increases matrix rank and expressivity while preserving convexity. Our method extends the solvable range of Peclet numbers in steady advection-diffusion equations by over two orders of magnitude, achieves up to six orders lower error on multi-frequency function learning, and fits high-fidelity image vectors more accurately and faster than deep networks with over a million parameters. This work highlights that conditioning, not depth, is often the bottleneck in scientific neural solvers and that simple architectural changes can unlock substantial gains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting</title>
<link>https://arxiv.org/abs/2508.05928</link>
<guid>https://arxiv.org/abs/2508.05928</guid>
<content:encoded><![CDATA[
<div> Group-Relative Policy Optimization, Think-Answer Mismatch, noisy reward signals, Stable Group-Relative Policy Optimization, mathematical reasoning benchmarks

Summary: 
Stable Group-Relative Policy Optimization (S-GRPO) addresses the Think-Answer Mismatch issue in training large reasoning models. This problem arises from noisy reward signals, particularly in unbalanced response groups, disrupting the learning process. S-GRPO introduces noise-aware advantage weights to improve stability during training. Experimental results on mathematical reasoning benchmarks show that S-GRPO outperforms DR. GRPO, achieving significant performance gains on various models. Notably, S-GRPO maintains learning progress even under 20% synthetic reward noise, unlike standard GRPO. These findings indicate S-GRPO's potential in enhancing the robustness and effectiveness of training large-scale reasoning models.  

<br /><br />Summary: <div>
arXiv:2508.05928v1 Announce Type: new 
Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training large reasoning models, yet it suffers from a critical vulnerability: the \emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning process. This problem is most severe in unbalanced response groups, paradoxically degrading the signal precisely when it should be most informative. To address this challenge, we propose Stable Group-Relative Policy Optimization (S-GRPO), a principled enhancement that derives optimal, noise-aware advantage weights to stabilize training. Our comprehensive experiments on mathematical reasoning benchmarks demonstrate S-GRPO's effectiveness and robustness. On various models, S-GRPO significantly outperforms DR. GRPO, achieving performance gains of +2.5% on Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn under 20% synthetic reward noise, S-GRPO maintains stable learning progress. These results highlight S-GRPO's potential for more robust and effective training of large-scale reasoning models. \footnote{Code and data are available at: https://github.com/shenpeijun0212/S-GRPO
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Armed Bandits-Based Optimization of Decision Trees</title>
<link>https://arxiv.org/abs/2508.05957</link>
<guid>https://arxiv.org/abs/2508.05957</guid>
<content:encoded><![CDATA[
<div> Keywords: decision trees, pruning, overfitting, Multi-Armed Bandits, reinforcement learning

Summary: 
Decision trees can become overly complex and prone to overfitting without proper constraints, leading to decreased generalizability. Traditional pruning techniques such as Cost-Complexity Pruning and Reduced Error Pruning may compromise long-term generalization. In this study, a Multi-Armed Bandits-based pruning approach is proposed as a reinforcement learning technique to dynamically optimize decision trees. The approach treats pruning as an exploration-exploitation problem and utilizes MAB algorithms to identify optimal branch nodes for pruning. Experimental results on benchmark datasets show that the proposed approach outperforms traditional methods in predictive performance. This suggests the potential of using MAB for dynamic and probabilistic decision tree pruning to improve model optimization. 

<br /><br />Summary: <div>
arXiv:2508.05957v1 Announce Type: new 
Abstract: Decision trees, without appropriate constraints, can easily become overly complex and prone to overfit, capturing noise rather than generalizable patterns. To resolve this problem,pruning operation is a crucial part in optimizing decision trees, as it not only reduces the complexity of trees but also decreases the probability of generating overfit models. The conventional pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning (REP) are mostly based on greedy approaches that focus on immediate gains in performance while pruning nodes of the decision tree. However, this might result in a lower generalization in the long run, compromising the robust ability of the tree model when introduced to unseen data samples, particularly when trained with small and complex datasets. To address this challenge, we are proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement learning (RL)-based technique, that will dynamically prune the tree to generate an optimal decision tree with better generalization. Our proposed approach assumes the pruning process as an exploration-exploitation problem, where we are utilizing the MAB algorithms to find optimal branch nodes to prune based on feedback from each pruning actions. Experimental evaluation on several benchmark datasets, demonstrated that our proposed approach results in better predictive performance compared to the traditional ones. This suggests the potential of utilizing MAB for a dynamic and probabilistic way of decision tree pruning, in turn optimizing the decision tree-based model.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.05960</link>
<guid>https://arxiv.org/abs/2508.05960</guid>
<content:encoded><![CDATA[
<div> Keywords: Offline reinforcement learning, distribution shift, overestimation, mildly conservative regularized evaluation, off-policy actor-critic framework<br />
<br />
Summary: <br />
The article introduces a new approach called Mildly Conservative Regularized Q-learning (MCRQ) in the context of offline reinforcement learning. The main challenge addressed is the distribution shift between the learned and behavior policies, which can lead to out-of-distribution actions and overestimation. MCRQ aims to balance conservatism and performance by incorporating a behavior cloning term in the Bellman backup, thus ensuring the value function remains conservative but not overly so. The proposed MCRQ algorithm outperforms strong baselines and state-of-the-art offline RL algorithms on benchmark datasets. It achieves this by combining temporal difference error with the behavior cloning term, effectively addressing the overestimation problem while maintaining performance. MCRQ integrates Mildly Conservative Regularized Evaluation (MCRE) into an off-policy actor-critic framework, demonstrating improvements in performance and conservatism compared to existing approaches. <div>
arXiv:2508.05960v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from static datasets without further environment interaction. A key challenge is the distribution shift between the learned and behavior policies, leading to out-of-distribution (OOD) actions and overestimation. To prevent gross overestimation, the value function must remain conservative; however, excessive conservatism may hinder performance improvement. To address this, we propose the mildly conservative regularized evaluation (MCRE) framework, which balances conservatism and performance by combining temporal difference (TD) error with a behavior cloning term in the Bellman backup. Building on this, we develop the mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates MCRE into an off-policy actor-critic framework. Experiments show that MCRQ outperforms strong baselines and state-of-the-art offline RL algorithms on benchmark datasets.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.05977</link>
<guid>https://arxiv.org/abs/2508.05977</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific machine learning, reinforcement learning, semantic reward, Sentence-BERT, language embedding space

Summary: 
In the realm of scientific machine learning, designing effective reward functions for reinforcement learning is a major challenge, especially when task goals are hard to quantify. Existing methods often rely on heuristic approaches or manual engineering of reward functions. This paper introduces a novel approach that aligns rewards with semantic instructions using a Sentence-BERT model. Instead of manually defining rewards, the policy receives feedback based on the cosine similarity between the goal description and the statement description in the episode. The study demonstrates that this semantic reward strategy can guide learning towards achieving competitive control behavior without the need for hand-crafted rewards. The results show a correlation between the language embedding space and the conventional Euclidean space, suggesting potential for seamless integration of language models and control applications. This framework opens up new possibilities for aligning agent behavior with natural language goals. 

Summary: <div>
arXiv:2508.05977v1 Announce Type: new 
Abstract: In the domain of scientific machine learning, designing effective reward functions remains a challenge in reinforcement learning (RL), particularly in environments where task goals are difficult to specify numerically. Reward functions in existing work are predominantly based on heuristics, manual engineering, or task-specific tuning. In this work, we introduce a semantically aligned reinforcement learning method where rewards are computed by aligning the current state with a target semantic instruction using a Sentence-Bidirectional Encoder Representations from Transformers (SBERT). Instead of relying on manually defined reward functions, the policy receives feedback based on the reward, which is a cosine similarity between the goal textual description and the statement description in the episode. We evaluated our approach in several environments and showed that semantic reward can guide learning to achieve competitive control behavior, even in the absence of hand-crafted reward functions. Our study demonstrates a correlation between the language embedding space and the conventional Euclidean space. This framework opens new horizons for aligning agent behavior with natural language goals and lays the groundwork for a more seamless integration of larger language models (LLMs) and fluid control applications.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning</title>
<link>https://arxiv.org/abs/2508.05984</link>
<guid>https://arxiv.org/abs/2508.05984</guid>
<content:encoded><![CDATA[
<div> convergence rates, nonlinear fixed-point equations, average-reward Q-learning, TD-learning, Polyak-Ruppert averaging<br />
Summary:<br />
Algorithms for solving nonlinear fixed-point equations, such as average-reward Q-learning and TD-learning, often involve semi-norm contractions. Achieving optimal convergence rates for these methods has been challenging due to the non-monotonicity of semi-norms. By recasting the averaged error as a linear recursion involving a nonlinear perturbation and coupling the semi-norm's contraction with the monotonicity of an induced norm, the study achieves the first parameter-free optimal rates for Q-learning in average-reward and exponentially discounted settings, both approximating O(1/sqrt(t)) after t iterations. The results are applicable in various scenarios like synchronous/asynchronous updates, single-agent/distributed deployments, and data streams from simulators or Markovian trajectories. <div>
arXiv:2508.05984v1 Announce Type: new 
Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve semi-norm contractions. Achieving parameter-free optimal convergence rates for these methods via Polyak--Ruppert averaging has remained elusive, largely due to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting the averaged error as a linear recursion involving a nonlinear perturbation, and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with the monotonicity of a suitably induced norm. Our main result yields the first parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both average-reward and exponentially discounted settings, where $t$ denotes the iteration index. The result applies within a broad framework that accommodates synchronous and asynchronous updates, single-agent and distributed deployments, and data streams obtained either from simulators or along Markovian trajectories.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal</title>
<link>https://arxiv.org/abs/2508.05988</link>
<guid>https://arxiv.org/abs/2508.05988</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, CoT compression, logic-aware pruning, code generation benchmarks, efficient reasoning<br />
<br />
Summary: 
This paper introduces ASAP (Anchor-guided, Surprisal-based Pruning), a novel framework for compressing Chain-of-Thought (CoT) in Large Reasoning Models (LRMs) for code reasoning tasks. ASAP first utilizes anchor-guided pruning to maintain the core reasoning structure, reducing the search space. It then applies logic-aware pruning to select essential reasoning steps based on a new first-token surprisal metric. By teaching models to generate and utilize concise CoTs at inference time, ASAP enables efficient reasoning in coding tasks. Experimental results show that ASAP achieves top-tier accuracy on various code generation benchmarks while significantly reducing training and inference costs. Specifically, on the challenging LiveCodeBench v4_v5 benchmark, ASAP reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline while achieving a competitive accuracy of 36.19% in Pass@1. This work presents a promising approach for enhancing the power and efficiency of LRMs in coding contexts. <br /><br /> <div>
arXiv:2508.05988v1 Announce Type: new 
Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code reasoning by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces introduce substantial challenges in terms of training cost, inference latency, and deployment feasibility. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps. In this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. It then enables a logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP teaches models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning in coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark, our approach reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, while achieving a competitive accuracy of 36.19% in Pass@1. Our results highlight a promising direction for building powerful and efficient LRMs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization</title>
<link>https://arxiv.org/abs/2508.05995</link>
<guid>https://arxiv.org/abs/2508.05995</guid>
<content:encoded><![CDATA[
<div> framework, code generation, optimization, Monte Carlo Tree Search, neural-symbolic<br />
<br />
Summary: 
The article introduces a new neural-symbolic framework, MCTS-OPS, that uses Monte Carlo Tree Search to enhance code generation quality. By formulating prompt selection as a sequential decision process, the method explores and refines multi-step prompt sequences to improve code generation and problem-solving capabilities of large language models (LLMs). Experiments conducted on network optimization tasks demonstrate significant improvements over baselines in executing generated code, optimization results, and the chance of attaining optimal solutions in complex problems. The proposed framework shows promise in combining symbolic planning with LLMs for robust and high-quality code generation in challenging domains. <div>
arXiv:2508.05995v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in code generation and structured reasoning; however, their performance often degrades on complex tasks that require consistent multi-step planning. Recent work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet existing approaches primarily focus on generating heuristic-based code for optimization or target simpler tasks where correctness alone is sufficient. In this work, we propose MCTS-OPS, a novel neural-symbolic framework that formulates prompt selection as a sequential decision process guided by MCTS. Our method explores and refines multi-step prompt sequences for the goal of improving code generation quality and enhancing the problem-solving capabilities of LLMs in general optimization. Experiments on network optimization show significant improvement over the baselines, both in the success rate of executing the generated code and in the optimization results with the specified objective and constraints (2$\sim$4$\times$ higher reward and 3$\times$ lower standard deviation). Moreover, it improves the chance of attaining the optimal solution by about 10\% of cases, compared to baseline methods in hard problems. These results highlight the promise of combining symbolic planning with LLMs for robust, high-quality code generation in complex domains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients</title>
<link>https://arxiv.org/abs/2508.06023</link>
<guid>https://arxiv.org/abs/2508.06023</guid>
<content:encoded><![CDATA[
<div> Keywords: prognostication, comatose post-cardiac arrest patients, dynamic competing risks model, time-invariant features, time-varying hemodynamic data

Summary: 
The study introduces a novel stepwise dynamic competing risks model for predicting neurological outcomes in comatose post-cardiac arrest patients. This model effectively incorporates both time-invariant baseline features and time-varying hemodynamic data collected during two distinct phases of patient care. By automatically determining when to utilize these different types of features, the model improves prognostication accuracy by capturing the varying importance of hemodynamic data over time. Using neural networks to capture complex feature relationships, the model demonstrated robust discriminative performance for competing outcomes like awakening, withdrawal of life-sustaining therapy, and death despite maximal support in a cohort of 2,278 patients. The approach is designed to generalize to scenarios with multiple phases of feature collection, offering valuable insights into when and for whom newly collected data can significantly enhance predictive accuracy. 

<br /><br />Summary: <div>
arXiv:2508.06023v1 Announce Type: new 
Abstract: Prognostication for comatose post-cardiac arrest patients is a critical challenge that directly impacts clinical decision-making in the ICU. Clinical information that informs prognostication is collected serially over time. Shortly after cardiac arrest, various time-invariant baseline features are collected (e.g., demographics, cardiac arrest characteristics). After ICU admission, additional features are gathered, including time-varying hemodynamic data (e.g., blood pressure, doses of vasopressor medications). We view these as two phases in which we collect new features. In this study, we propose a novel stepwise dynamic competing risks model that improves the prediction of neurological outcomes by automatically determining when to take advantage of time-invariant features (first phase) and time-varying features (second phase). Notably, our model finds patients for whom this second phase (time-varying hemodynamic) information is beneficial for prognostication and also when this information is beneficial (as we collect more hemodynamic data for a patient over time, how important these data are for prognostication varies). Our approach extends the standard Fine and Gray model to explicitly model the two phases and to incorporate neural networks to flexibly capture complex nonlinear feature relationships. Evaluated on a retrospective cohort of 2,278 comatose post-arrest patients, our model demonstrates robust discriminative performance for the competing outcomes of awakening, withdrawal of life-sustaining therapy, and death despite maximal support. Our approach generalizes to more than two phases in which new features are collected and could be used in other dynamic prediction tasks, where it may be helpful to know when and for whom newly collected features significantly improve prediction.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity</title>
<link>https://arxiv.org/abs/2508.06034</link>
<guid>https://arxiv.org/abs/2508.06034</guid>
<content:encoded><![CDATA[
<div> heterogeneous graphs, heterophily, heterophily distributions, semantic information, Adaptive Heterogeneous Graph Neural Network

Summary: 
The article discusses the challenges in modeling heterophily heterogeneous graphs (HGs) in real-world scenarios, where existing studies often overlook the prevalence of heterophilic HGs, leading to performance degradation. Two main challenges are identified: varying heterophily distributions across hops and meta-paths, and the diverse semantic information across different meta-paths driven by heterophily. To address these challenges, the Adaptive Heterogeneous Graph Neural Network (AHGNN) is proposed. AHGNN utilizes a heterophily-aware convolution to consider heterophily distributions specific to hops and meta-paths. It integrates messages from diverse semantic spaces using a coarse-to-fine attention mechanism to filter noise and emphasize informative signals. Experimental results on real-world graphs and baselines demonstrate the superior performance of AHGNN, especially in high-heterophily scenarios. <div>
arXiv:2508.06034v1 Announce Type: new 
Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often exhibit heterophily. However, most existing studies focus on either heterogeneity or heterophily in isolation, overlooking the prevalence of heterophilic HGs in practical applications. Such ignorance leads to their performance degradation. In this work, we first identify two main challenges in modeling heterophily HGs: (1) varying heterophily distributions across hops and meta-paths; (2) the intricate and often heterophily-driven diversity of semantic information across different meta-paths. Then, we propose the Adaptive Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN employs a heterophily-aware convolution that accounts for heterophily distributions specific to both hops and meta-paths. It then integrates messages from diverse semantic spaces using a coarse-to-fine attention mechanism, which filters out noise and emphasizes informative signals. Experiments on seven real-world graphs and twenty baselines demonstrate the superior performance of AHGNN, particularly in high-heterophily situations.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment</title>
<link>https://arxiv.org/abs/2508.06041</link>
<guid>https://arxiv.org/abs/2508.06041</guid>
<content:encoded><![CDATA[
<div> quantization, large language models, runtime constraints, mixed-precision, dynamic precision assignment

Summary:
DP-LLM introduces a novel mechanism for on-device large language models (LLMs) by dynamically assigning precision to each layer based on input values. This approach leverages the observation that the sensitivity of each layer changes across decoding iterations. By augmenting linear layers with a precision selector, DP-LLM determines the bitwidth at runtime using an error estimator and threshold values learned through fine-tuning. Experimental results show that DP-LLM achieves a superior performance-latency trade-off compared to prior approaches. This multi-scale quantization technique addresses the challenge of handling queries for LLMs with varying runtime constraints, such as latency and accuracy. Mixed-precision is also considered as a promising solution, but DP-LLM takes it further by offering dynamic precision assignment based on input values. This approach results in efficient model adaptation and improved overall performance of on-device LLMs. <div>
arXiv:2508.06041v1 Announce Type: new 
Abstract: How can we effectively handle queries for on-device large language models (LLMs) with varying runtime constraints, such as latency and accuracy? Multi-scale quantization addresses this challenge by enabling memory-efficient runtime model adaptation of LLMs through the overlaying of multiple model variants quantized to different bitwidths. Meanwhile, an important question still remains open-ended: how can models be properly configured to match a target precision or latency? While mixed-precision offers a promising solution, we take this further by leveraging the key observation that the sensitivity of each layer dynamically changes across decoding iterations. Building on this insight, we introduce DP-LLM, a novel mechanism that dynamically assigns precision to each layer based on input values. DP-LLM augments each linear layer in an LLM with a precision selector that determines the bitwidth at runtime using a lightweight error estimator and threshold values learned through fine-tuning. Experimental results across multiple models and benchmarks demonstrate that DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology</title>
<link>https://arxiv.org/abs/2508.06066</link>
<guid>https://arxiv.org/abs/2508.06066</guid>
<content:encoded><![CDATA[
<div> generalization bounds, temporal convolutional networks, deep temporal models, fair-comparison methodology, temporal dependence<br />
<br />Summary: 
The article addresses the lack of theoretical understanding of generalization in deep temporal architectures like Temporal Convolutional Networks (TCNs). It introduces the first architecture-aware generalization bounds for deep temporal models on exponentially -mixing sequences. By using a delayed-feedback blocking mechanism, the bounds scale efficiently with network depth, kernel size, input dimension, and weight norm, suggesting that increasing depth requires more training data. The study also presents a fair-comparison methodology to isolate the effect of temporal structure, showing that strong dependencies can lead to smaller generalization gaps than weak dependencies under fixed sample sizes. However, the convergence rates observed in practice diverge from theoretical predictions, indicating opportunities for future research to bridge the gap between theory and practice in understanding the impact of temporal dependence on learning. <div>
arXiv:2508.06066v1 Announce Type: new 
Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs) achieve strong predictive performance on sequential data, yet theoretical understanding of their generalization remains limited. We address this gap by providing both the first non-vacuous, architecture-aware generalization bounds for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $ O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our delayed-feedback blocking mechanism transforms dependent samples into effectively independent ones while discarding only $O(1/\log N)$ of the data, yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective sample size to isolate the effect of temporal structure from information content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences ($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly dependent ones ($\rho=0.2$), challenging the intuition that dependence is purely detrimental. Yet convergence rates diverge from theory: weak dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$. These findings reveal that temporal dependence can enhance learning under fixed information budgets, while highlighting gaps between theory and practice that motivate future research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent Deep Differentiable Logic Gate Networks</title>
<link>https://arxiv.org/abs/2508.06097</link>
<guid>https://arxiv.org/abs/2508.06097</guid>
<content:encoded><![CDATA[
<div> Differentiable logic gates, Recurrent Deep Differentiable Logic Gate Networks (RDDLGN), sequence-to-sequence learning, WMT'14 English-German translation, Boolean operations <br />
Summary: 
This paper introduces the concept of Recurrent Deep Differentiable Logic Gate Networks (RDDLGN) that combine Boolean operations with recurrent architectures for sequence-to-sequence learning. The RDDLGN model achieves 5.00 BLEU and 30.9% accuracy during training on the WMT'14 English-German translation task, approaching the performance of GRU models. During inference, RDDLGN demonstrates graceful degradation with a BLEU score of 4.39. This study highlights the potential of recurrent logic-based neural computation in sequential modeling and suggests future research directions for FPGA acceleration in recursive network architectures. This innovative approach opens up new possibilities for improving the efficiency and performance of sequential modeling tasks. <br /><br />Summary: <div>
arXiv:2508.06097v1 Announce Type: new 
Abstract: While differentiable logic gates have shown promise in feedforward networks, their application to sequential modeling remains unexplored. This paper presents the first implementation of Recurrent Deep Differentiable Logic Gate Networks (RDDLGN), combining Boolean operations with recurrent architectures for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and 30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and graceful degradation (4.39 BLEU) during inference. This work establishes recurrent logic-based neural computation as viable, opening research directions for FPGA acceleration in sequential modeling and other recursive network architectures.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.06108</link>
<guid>https://arxiv.org/abs/2508.06108</guid>
<content:encoded><![CDATA[
<div> Keywords: Goal-conditioned reinforcement learning, sparse rewards, hindsight experience replay, action regularization priors, sample efficiency <br />
Summary: 
Goal-conditioned reinforcement learning (GCRL) with sparse rewards is a challenging problem in reinforcement learning. While hindsight experience replay (HER) has shown promise in relabeling trajectories with achieved goals, it does not fully utilize available experiences in off-policy GCRL methods, leading to limited sample efficiency. In this study, a new technique called Hindsight Goal-conditioned Regularization (HGR) is proposed, which generates action regularization priors based on hindsight goals to enhance experience utilization. When combined with hindsight self-imitation regularization (HSR), this approach allows off-policy RL algorithms to maximize sample reuse. Empirical results on navigation and manipulation tasks show that the proposed hindsight regularizations outperform existing methods that use HER and self-imitation techniques, achieving superior sample efficiency and performance. <br /><br />Summary: <div>
arXiv:2508.06108v1 Announce Type: new 
Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a fundamental challenge in reinforcement learning. While hindsight experience replay (HER) has shown promise by relabeling collected trajectories with achieved goals, we argue that trajectory relabeling alone does not fully exploit the available experiences in off-policy GCRL methods, resulting in limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned Regularization (HGR), a technique that generates action regularization priors based on hindsight goals. When combined with hindsight self-imitation regularization (HSR), our approach enables off-policy RL algorithms to maximize experience utilization. Compared to existing GCRL methods that employ HER and self-imitation techniques, our hindsight regularizations achieve substantially more efficient sample reuse and the best performances, which we empirically demonstrate on a suite of navigation and manipulation tasks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models</title>
<link>https://arxiv.org/abs/2508.06151</link>
<guid>https://arxiv.org/abs/2508.06151</guid>
<content:encoded><![CDATA[
<div> Keywords: oral cancer, diagnostics, synthetic lesions, image generation, machine learning<br />
Summary:<br />
This study proposes a novel approach to improving oral cancer diagnostics by generating synthetic lesions through an inpainting technique with a fine-tuned diffusion model. By compiling a diverse dataset of oral cancer images, the method creates realistic synthetic lesions that enhance the performance of diagnostic algorithms. The classification model achieved a high accuracy of 0.97 in distinguishing between cancerous and non-cancerous tissues, while the detection model accurately identified lesion locations with 0.85 accuracy. This demonstrates the potential of synthetic image generation in medical diagnostics and opens up possibilities for further research in other types of cancer diagnostics. The method addresses the challenge of limited annotated datasets in oral cancer diagnostics and showcases the benefits of utilizing machine learning techniques for improving diagnostic accuracy. <div>
arXiv:2508.06151v1 Announce Type: new 
Abstract: In oral cancer diagnostics, the limited availability of annotated datasets frequently constrains the performance of diagnostic models, particularly due to the variability and insufficiency of training data. To address these challenges, this study proposed a novel approach to enhance diagnostic accuracy by synthesizing realistic oral cancer lesions using an inpainting technique with a fine-tuned diffusion model. We compiled a comprehensive dataset from multiple sources, featuring a variety of oral cancer images. Our method generated synthetic lesions that exhibit a high degree of visual fidelity to actual lesions, thereby significantly enhancing the performance of diagnostic algorithms. The results show that our classification model achieved a diagnostic accuracy of 0.97 in differentiating between cancerous and non-cancerous tissues, while our detection model accurately identified lesion locations with 0.85 accuracy. This method validates the potential for synthetic image generation in medical diagnostics and paves the way for further research into extending these methods to other types of cancer diagnostics.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Federated Clustering with Random Rebalancing</title>
<link>https://arxiv.org/abs/2508.06183</link>
<guid>https://arxiv.org/abs/2508.06183</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated clustering, privacy, differential privacy, RR-Cluster, tradeoffs

Summary:
Federated clustering involves grouping similar clients into clusters to create personalized models, improving performance but potentially compromising privacy. Existing client-level differentially private mechanisms can degrade utilities when applied to federated clustering due to uncontrolled cluster sizes. To address this issue, a technique called RR-Cluster is proposed, which minimizes privacy noise by randomly rebalancing cluster assignments to ensure a minimum number of clients in each cluster. The approach reduces privacy noise variance while considering potential biases from incorrect assignments. Convergence bounds for RR-Cluster are provided, showing improved privacy/utility tradeoffs across synthetic and real-world datasets when integrated with strong federated clustering algorithms. Empirical results demonstrate the effectiveness of RR-Cluster in enhancing privacy protection while maintaining model performance. 

<br /><br />Summary: <div>
arXiv:2508.06183v1 Announce Type: new 
Abstract: Federated clustering aims to group similar clients into clusters and produce one model for each cluster. Such a personalization approach typically improves model performance compared with training a single model to serve all clients, but can be more vulnerable to privacy leakage. Directly applying client-level differentially private (DP) mechanisms to federated clustering could degrade the utilities significantly. We identify that such deficiencies are mainly due to the difficulties of averaging privacy noise within each cluster (following standard privacy mechanisms), as the number of clients assigned to the same clusters is uncontrolled. To this end, we propose a simple and effective technique, named RR-Cluster, that can be viewed as a light-weight add-on to many federated clustering algorithms. RR-Cluster achieves reduced privacy noise via randomly rebalancing cluster assignments, guaranteeing a minimum number of clients assigned to each cluster. We analyze the tradeoffs between decreased privacy noise variance and potentially increased bias from incorrect assignments and provide convergence bounds for RR-Clsuter. Empirically, we demonstrate the RR-Cluster plugged into strong federated clustering algorithms results in significantly improved privacy/utility tradeoffs across both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning</title>
<link>https://arxiv.org/abs/2508.06199</link>
<guid>https://arxiv.org/abs/2508.06199</guid>
<content:encoded><![CDATA[
<div> neural networks, chemistry, drug design, molecular property prediction, evaluation

Summary: 
This study compares 25 pretrained neural network models across 25 datasets in the field of chemistry and small molecule drug design. The evaluation framework encompasses various modalities, architectures, and pretraining strategies. Surprisingly, the results show that most neural models do not outperform the baseline ECFP molecular fingerprint, with only the CLAMP model showing statistical significance. These findings suggest a need for improved evaluation rigor in the field. Potential causes of these results are discussed, along with proposed solutions and practical recommendations for future research in the area of utilizing neural networks for molecular chemistry applications. <div>
arXiv:2508.06199v1 Announce Type: new 
Abstract: Pretrained neural networks have attracted significant interest in chemistry and small molecule drug design. Embeddings from these models are widely used for molecular property prediction, virtual screening, and small data learning in molecular chemistry. This study presents the most extensive comparison of such models to date, evaluating 25 models across 25 datasets. Under a fair comparison framework, we assess models spanning various modalities, architectures, and pretraining strategies. Using a dedicated hierarchical Bayesian statistical testing model, we arrive at a surprising result: nearly all neural models show negligible or no improvement over the baseline ECFP molecular fingerprint. Only the CLAMP model, which is also based on molecular fingerprints, performs statistically significantly better than the alternatives. These findings raise concerns about the evaluation rigor in existing studies. We discuss potential causes, propose solutions, and offer practical recommendations.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Federated Learning for Personalized Privacy Recommendation</title>
<link>https://arxiv.org/abs/2508.06208</link>
<guid>https://arxiv.org/abs/2508.06208</guid>
<content:encoded><![CDATA[
<div> Graph Federated Learning, Personalized Recommendation, Privacy Protection, Graph Convolutional Network, Recommendation Accuracy<br />
Summary:<br />
The paper introduces Graph Federated Learning for Personalized Privacy Recommendation (GFed-PP), a novel approach that addresses varying privacy preferences in federated recommendation systems. GFed-PP leverages publicly available user data to enhance recommendation services while preserving privacy. It utilizes user-item interaction graphs to establish user relationships and employs a lightweight graph convolutional network (GCN) for personalized item embedding. Each client locally learns user embeddings and scoring functions to protect privacy. The framework optimizes federated recommendation by initializing item embeddings on clients and aggregating user relationship graphs on the server. Experimental results demonstrate that GFed-PP outperforms existing methods on five datasets, delivering superior recommendation accuracy without compromising privacy. This approach offers a practical solution for accommodating diverse privacy requirements in federated recommendation systems.<br />
Summary: <div>
arXiv:2508.06208v1 Announce Type: new 
Abstract: Federated recommendation systems (FedRecs) have gained significant attention for providing privacy-preserving recommendation services. However, existing FedRecs assume that all users have the same requirements for privacy protection, i.e., they do not upload any data to the server. The approaches overlook the potential to enhance the recommendation service by utilizing publicly available user data. In real-world applications, users can choose to be private or public. Private users' interaction data is not shared, while public users' interaction data can be shared. Inspired by the issue, this paper proposes a novel Graph Federated Learning for Personalized Privacy Recommendation (GFed-PP) that adapts to different privacy requirements while improving recommendation performance. GFed-PP incorporates the interaction data of public users to build a user-item interaction graph, which is then used to form a user relationship graph. A lightweight graph convolutional network (GCN) is employed to learn each user's user-specific personalized item embedding. To protect user privacy, each client learns the user embedding and the scoring function locally. Additionally, GFed-PP achieves optimization of the federated recommendation framework through the initialization of item embedding on clients and the aggregation of the user relationship graph on the server. Experimental results demonstrate that GFed-PP significantly outperforms existing methods for five datasets, offering superior recommendation accuracy without compromising privacy. This framework provides a practical solution for accommodating varying privacy preferences in federated recommendation systems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reparameterization Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2508.06214</link>
<guid>https://arxiv.org/abs/2508.06214</guid>
<content:encoded><![CDATA[
<div> Reparameterization policy gradient, RPG, training instability, Proximal Policy Optimization, PPO <br />
<br />
Summary: 
Reparameterization Proximal Policy Optimization (RPO) addresses the training instability of RPG by leveraging a surrogate objective inspired by PPO. The connection between the two methods is established and the reparameterization gradient for the surrogate objective is computed efficiently using backpropagation through time. RPO enables stable sample reuse through multiple epochs by optimizing a clipped surrogate objective tailored for RPG, incorporating KL divergence regularization, and remaining compatible with variance reduction methods. Experimental results on challenging locomotion and manipulation tasks show that RPO achieves superior sample efficiency and strong performance. <div>
arXiv:2508.06214v1 Announce Type: new 
Abstract: Reparameterization policy gradient (RPG) is promising for improving sample efficiency by leveraging differentiable dynamics. However, a critical barrier is its training instability, where high-variance gradients can destabilize the learning process. To address this, we draw inspiration from Proximal Policy Optimization (PPO), which uses a surrogate objective to enable stable sample reuse in the model-free setting. We first establish a connection between this surrogate objective and RPG, which has been largely unexplored and is non-trivial. Then, we bridge this gap by demonstrating that the reparameterization gradient of a PPO-like surrogate objective can be computed efficiently using backpropagation through time. Based on this key insight, we propose Reparameterization Proximal Policy Optimization (RPO), a stable and sample-efficient RPG-based method. RPO enables multiple epochs of stable sample reuse by optimizing a clipped surrogate objective tailored for RPG, while being further stabilized by Kullback-Leibler (KL) divergence regularization and remaining fully compatible with existing variance reduction methods. We evaluate RPO on a suite of challenging locomotion and manipulation tasks, where experiments demonstrate that our method achieves superior sample efficiency and strong performance.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems</title>
<link>https://arxiv.org/abs/2508.06243</link>
<guid>https://arxiv.org/abs/2508.06243</guid>
<content:encoded><![CDATA[
<div> compression, AI, resource management, scheduling, fairness

Summary:
SCAR (State-Space Compression for AI-Driven Resource Management) is introduced as a framework to optimize scheduling and fairness in vehicular infotainment services within 6G networks. Through ML-based compression techniques and reinforcement learning policies, SCAR aims to efficiently handle the increasing complexity of data, such as Channel Quality Indicators (CQI) from autonomous vehicles. The use of compressed states derived from clustering and RBF networks leads to improved scheduling feasibility and fairness, with a 14% increase in feasible scheduling regions and a 15% reduction in unfair scheduling time compared to RL baselines without CQI compression. The efficiency of Simulated Annealing with Stochastic Tunneling (SAST)-based clustering is also demonstrated, reducing CQI clustering distortion by 10%. These results highlight the scalability and fairness benefits of SCAR for dynamic vehicular networks. 

<br /><br />Summary: <div>
arXiv:2508.06243v1 Announce Type: new 
Abstract: The advent of 6G networks opens new possibilities for connected infotainment services in vehicular environments. However, traditional Radio Resource Management (RRM) techniques struggle with the increasing volume and complexity of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To address this, we propose SCAR (State-Space Compression for AI-Driven Resource Management), an Edge AI-assisted framework that optimizes scheduling and fairness in vehicular infotainment. SCAR employs ML-based compression techniques (e.g., clustering and RBF networks) to reduce CQI data size while preserving essential features. These compressed states are used to train 6G-enabled Reinforcement Learning policies that maximize throughput while meeting fairness objectives defined by the NGMN. Simulations show that SCAR increases time in feasible scheduling regions by 14\% and reduces unfair scheduling time by 15\% compared to RL baselines without CQI compression. Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based clustering reduces CQI clustering distortion by 10\%, confirming its efficiency. These results demonstrate SCAR's scalability and fairness benefits for dynamic vehicular networks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attack with Partial Features</title>
<link>https://arxiv.org/abs/2508.06244</link>
<guid>https://arxiv.org/abs/2508.06244</guid>
<content:encoded><![CDATA[
<div> Membership Inference Attack, Machine Learning Models, Partial Features, MRAD, Anomaly Detection

Summary:

In this study, the vulnerability of machine learning models to membership inference attacks is explored, specifically in scenarios where the adversary only has access to partial features of a target sample. This novel problem is termed Partial Feature Membership Inference (PFMI). To address PFMI, the MRAD (Memory-guided Reconstruction and Anomaly Detection) attack framework is introduced. MRAD operates in two stages: optimizing unknown feature values to reconstruct the sample and measuring the deviation between the reconstructed sample and the training distribution using anomaly detection. Experimental results show the effectiveness of MRAD across various datasets, even when 40% of the features are missing. For example, on the STL-10 dataset, the MRAD attack achieves an AUC of approximately 0.6, showcasing its capability to infer membership using only partial features. <div>
arXiv:2508.06244v1 Announce Type: new 
Abstract: Machine learning models have been shown to be susceptible to membership inference attack, which can be used to determine whether a given sample appears in the training data. Existing membership inference methods commonly assume that the adversary has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features information is available, thereby limiting the applicability of these methods. In this work, we study an inference scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set of the target model. We define this problem as Partial Feature Membership Inference (PFMI). To address this problem, we propose MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework. In the first stage, MRAD optimizes the unknown feature values to minimize the loss of the sample. In the second stage, it measures the deviation between the reconstructed sample and the training distribution using anomaly detection. Empirical results demonstrate that MRAD is effective across a range of datasets, and maintains compatibility with various off-the-shelf anomaly detection techniques. For example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of the missing features.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits</title>
<link>https://arxiv.org/abs/2508.06247</link>
<guid>https://arxiv.org/abs/2508.06247</guid>
<content:encoded><![CDATA[
<div> Keywords: combinatorial multi-armed bandit, regret, computational efficiency, semi-bandit feedback, cascading feedback

Summary:
The article introduces a new algorithm called CMOSS for the combinatorial multi-armed bandit problem. CMOSS is designed to address the shortcomings of existing approaches by achieving an instance-independent regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback. This result eliminates the dependency on $\log T and matches the lower bound of $\Omega\big( \sqrt{kmT}\big)$ up to $O\big((\log k)^2\big)$. The algorithm is computationally efficient and applicable to cascading feedback scenarios. Experimental results on synthetic and real-world datasets demonstrate that CMOSS outperforms benchmark algorithms in terms of both regret and runtime efficiency. By providing a balance between regret minimization and computational overhead, CMOSS offers a promising approach for solving the combinatorial multi-armed bandit problem. 

<br /><br />Summary: 
- Introduces CMOSS algorithm for combinatorial multi-armed bandit problem.
- Achieves instance-independent regret under semi-bandit feedback.
- Eliminates dependency on $\log T and matches lower bound.
- Computationally efficient and applicable to cascading feedback.
- Outperforms benchmark algorithms in regret and runtime efficiency. <div>
arXiv:2508.06247v1 Announce Type: new 
Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential decision-making framework, dominated by two algorithmic families: UCB-based and adversarial methods such as follow the regularized leader (FTRL) and online mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer from additional regret factor $\log T$ that is detrimental over long horizons, while adversarial methods such as EXP3.M and HYBRID impose significant computational overhead. To resolve this trade-off, we introduce the Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS is a computationally efficient algorithm that achieves an instance-independent regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where $m$ is the number of arms and $k$ is the maximum cardinality of a feasible action. Crucially, this result eliminates the dependency on $\log T$ and matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to $O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also applicable to cascading feedback. Experiments on synthetic and real-world datasets validate that CMOSS consistently outperforms benchmark algorithms in both regret and runtime efficiency.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Training Defenses against Emergent Misalignment in Language Models</title>
<link>https://arxiv.org/abs/2508.06249</link>
<guid>https://arxiv.org/abs/2508.06249</guid>
<content:encoded><![CDATA[
<div> fine-tuning, large language models, emergent misalignment, training regularization, API <br />
Summary:<br />
The study focuses on in-training safeguards against emergent misalignment (EMA) in fine-tuning large language models (LLMs) for new domains, considering the potential for harmful behaviors induced by small, domain-specific fine-tuning. Four training regularization interventions are explored: KL-divergence regularization, $\ell_2$ distance in feature space, projecting onto a safe subspace (SafeLoRA), and interleaving safe training examples from a general instruct-tuning dataset. The methods are evaluated for their effectiveness in addressing EMA across malicious tasks and their impact on benign tasks. The research highlights the importance of considering unintended consequences of fine-tuning and the need to implement safeguards to mitigate emergent misalignment in LLMs exposed through APIs. Open questions in EMA research are also discussed. <br /> <div>
arXiv:2508.06249v1 Announce Type: new 
Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs) for new domains, yet recent work reveals emergent misalignment (EMA): Even a small, domain-specific fine-tune can induce harmful behaviors far outside the target domain. Even in the case where model weights are hidden behind a fine-tuning API, this gives attackers inadvertent access to a broadly misaligned model in a way that can be hard to detect from the fine-tuning data alone. We present the first systematic study of in-training safeguards against EMA that are practical for providers who expose fine-tuning via an API. We investigate four training regularization interventions: (i) KL-divergence regularization toward a safe reference model, (ii) $\ell_2$ distance in feature space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving of a small amount of safe training examples from a general instruct-tuning dataset. We first evaluate the methods' emergent misalignment effect across four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on benign tasks. We conclude with a discussion of open questions in emergent misalignment research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)</title>
<link>https://arxiv.org/abs/2508.06251</link>
<guid>https://arxiv.org/abs/2508.06251</guid>
<content:encoded><![CDATA[
<div> Generative Model, Synthetic Data, Privacy-Preserving, Tensor Networks, Matrix Product States<br />
<br />
Summary: 
The article introduces a novel method for generating privacy-preserving synthetic tabular data using Tensor Networks, particularly Matrix Product States (MPS). By incorporating noise injection and gradient clipping during training, the model ensures differential privacy and outperforms traditional models like CTGAN, VAE, and PrivBayes in terms of fidelity and privacy-preservation. The results demonstrate the superior performance of MPS, especially in stringent privacy settings. This approach highlights the potential of using tensor network representations for secure data generation, offering a scalable and interpretable solution for confidential data sharing. The structured design of MPS allows for effective integration into sensitive domains where data quality and confidentiality are paramount, making it a promising tool for privacy-aware synthetic data generation. <br /><br /> <div>
arXiv:2508.06251v1 Announce Type: new 
Abstract: Synthetic data generation is a key technique in modern artificial intelligence, addressing data scarcity, privacy constraints, and the need for diverse datasets in training robust models. In this work, we propose a method for generating privacy-preserving high-quality synthetic tabular data using Tensor Networks, specifically Matrix Product States (MPS). We benchmark the MPS-based generative model against state-of-the-art models such as CTGAN, VAE, and PrivBayes, focusing on both fidelity and privacy-preserving capabilities. To ensure differential privacy (DP), we integrate noise injection and gradient clipping during training, enabling privacy guarantees via R\'enyi Differential Privacy accounting. Across multiple metrics analyzing data fidelity and downstream machine learning task performance, our results show that MPS outperforms classical models, particularly under strict privacy constraints. This work highlights MPS as a promising tool for privacy-aware synthetic data generation. By combining the expressive power of tensor network representations with formal privacy mechanisms, the proposed approach offers an interpretable and scalable alternative for secure data sharing. Its structured design facilitates integration into sensitive domains where both data quality and confidentiality are critical.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors</title>
<link>https://arxiv.org/abs/2508.06257</link>
<guid>https://arxiv.org/abs/2508.06257</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, multi-omics data, cancer subtype classification, precision oncology, contrastive learning 

Summary: 
The article introduces GTMancer, a novel framework for cancer subtype classification that integrates multi-omics data using Graph Neural Networks (GNNs). By leveraging contrastive learning, GTMancer embeds multi-omics data into a unified semantic space, allowing for a more comprehensive analysis of cancer subtype heterogeneity. The framework extends the application of GNNs to complex multi-omics datasets and introduces dual sets of attention coefficients to capture structural graph priors within and among different omics data. By utilizing global omics information to refine individual omics representations, GTMancer outperforms existing state-of-the-art algorithms in seven real-world cancer datasets. This approach has the potential to enhance our understanding of the complex biological processes underlying various diseases, particularly cancer, and improve precision oncology efforts. 

<br /><br />Summary: <div>
arXiv:2508.06257v1 Announce Type: new 
Abstract: Integrating multi-omics datasets through data-driven analysis offers a comprehensive understanding of the complex biological processes underlying various diseases, particularly cancer. Graph Neural Networks (GNNs) have recently demonstrated remarkable ability to exploit relational structures in biological data, enabling advances in multi-omics integration for cancer subtype classification. Existing approaches often neglect the intricate coupling between heterogeneous omics, limiting their capacity to resolve subtle cancer subtype heterogeneity critical for precision oncology. To address these limitations, we propose a framework named Graph Transformer for Multi-omics Cancer Subtype Classification (GTMancer). This framework builds upon the GNN optimization problem and extends its application to complex multi-omics data. Specifically, our method leverages contrastive learning to embed multi-omics data into a unified semantic space. We unroll the multiplex graph optimization problem in that unified space and introduce dual sets of attention coefficients to capture structural graph priors both within and among multi-omics data. This approach enables global omics information to guide the refining of the representations of individual omics. Empirical experiments on seven real-world cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art algorithms.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OM2P: Offline Multi-Agent Mean-Flow Policy</title>
<link>https://arxiv.org/abs/2508.06269</link>
<guid>https://arxiv.org/abs/2508.06269</guid>
<content:encoded><![CDATA[
<div> offline MARL, generative models, diffusion, flow-based policies, mean-flow policy, efficient action sampling<br />
<br />
Summary:<br />
The article introduces OM2P, an Offline Multi-Agent Mean-Flow Policy algorithm that addresses challenges in integrating generative models into multi-agent reinforcement learning. The algorithm aims to improve sampling efficiency by proposing a reward-aware optimization scheme that combines mean-flow matching loss with Q-function supervision. It also introduces a generalized timestep distribution and a derivative-free estimation strategy to enhance memory efficiency and training stability. Empirical evaluations on benchmark tasks show that OM2P outperforms existing methods, achieving a significant reduction in GPU memory usage and training time. By successfully integrating mean-flow models into offline MARL, OM2P paves the way for practical and scalable generative policies in cooperative multi-agent scenarios. <div>
arXiv:2508.06269v1 Announce Type: new 
Abstract: Generative models, especially diffusion and flow-based models, have been promising in offline multi-agent reinforcement learning. However, integrating powerful generative models into this framework poses unique challenges. In particular, diffusion and flow-based policies suffer from low sampling efficiency due to their iterative generation processes, making them impractical in time-sensitive or resource-constrained settings. To tackle these difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel offline MARL algorithm to achieve efficient one-step action sampling. To address the misalignment between generative objectives and reward maximization, we introduce a reward-aware optimization scheme that integrates a carefully-designed mean-flow matching loss with Q-function supervision. Additionally, we design a generalized timestep distribution and a derivative-free estimation strategy to reduce memory overhead and improve training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo benchmarks demonstrate that OM2P achieves superior performance, with up to a 3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time. Our approach represents the first to successfully integrate mean-flow model into offline MARL, paving the way for practical and scalable generative policies in cooperative multi-agent settings.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on Regularization-Based Continual Learning Methods for Indic ASR</title>
<link>https://arxiv.org/abs/2508.06280</link>
<guid>https://arxiv.org/abs/2508.06280</guid>
<content:encoded><![CDATA[
<div> Keywords: Indian languages, Continual Learning, Automatic Speech Recognition, Conformer-based model, Language diversity 

Summary: 
- The paper explores Continual Learning (CL) for Automatic Speech Recognition (ASR) on Indian languages.
- Traditional multilingual models are impractical due to privacy constraints and sequential data arrival.
- A Conformer-based hybrid RNN-T/CTC model is incrementally trained on nine Indian languages using CL strategies like EWC, MAS, and LwF.
- Performance evaluation is done using Word Error Rate (WER) on clean and noisy data, with analysis on knowledge retention.
- Varying the number of training epochs per task is also studied, and results show that CL effectively mitigates forgetting and is a promising approach for scalable ASR in diverse Indian languages under realistic constraints.

<br /><br />Summary: <div>
arXiv:2508.06280v1 Announce Type: new 
Abstract: Indias linguistic diversity poses significant challenges for developing inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual models, which require simultaneous access to all language data, are impractical due to the sequential arrival of data and privacy constraints. Continual Learning (CL) offers a solution by enabling models to learn new languages sequentially without catastrophically forgetting previously learned knowledge. This paper investigates CL for ASR on Indian languages using a subset of the IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model, initially pretrained on Hindi, which is then incrementally trained on eight additional Indian languages, for a total sequence of nine languages. We evaluate three prominent regularization- and distillation-based CL strategies: Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning without Forgetting (LwF), selected for their suitability in no-replay, privacy-conscious scenarios. Performance is analyzed using Word Error Rate (WER) for both RNN-T and CTC paths on clean and noisy data, as well as knowledge retention via Backward Transfer. We also explore the impact of varying the number of training epochs (1, 2, 5, and 10) per task. Results, compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating forgetting, making it a promising approach for scalable ASR in diverse Indian languages under realistic constraints. The code is available at: https://github.com/FrozenWolf-Cyber/Indic-CL-ASR
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback</title>
<link>https://arxiv.org/abs/2508.06292</link>
<guid>https://arxiv.org/abs/2508.06292</guid>
<content:encoded><![CDATA[
<div> Keywords: Neuromorphic computing, Spiking neural networks, State-space models, Multiple-output spiking neuron model, Reset mechanism

Summary:
Neuromorphic computing is a technology with low-latency and energy-efficient signal processing capabilities. Spiking neural networks (SNNs) are utilized in neuromorphic computing, offering low-bit data processing through spike encoding. A novel multiple-output spiking neuron model combining linear state transition with non-linear feedback via reset is proposed to bridge the gap between SNNs and deep state-space models. This model differentiates between spiking function, reset condition, and reset action, improving performance in tasks like keyword spotting and sequential pattern recognition. The reset mechanism overcomes instability in neuron dynamics, enabling learning even with unstable linear dynamics, surpassing the strict stability requirements of recent deep SSM models. The experimental results demonstrate the effectiveness of the proposed model in achieving comparable performance to existing benchmarks in the SNN literature. 

<br /><br />Summary: <div>
arXiv:2508.06292v1 Announce Type: new 
Abstract: Neuromorphic computing is an emerging technology enabling low-latency and energy-efficient signal processing. A key algorithmic tool in neuromorphic computing is spiking neural networks (SNNs). SNNs are biologically inspired neural networks which utilize stateful neurons, and provide low-bit data processing by encoding and decoding information using spikes. Similar to SNNs, deep state-space models (SSMs) utilize stateful building blocks. However, deep SSMs, which recently achieved competitive performance in various temporal modeling tasks, are typically designed with high-precision activation functions and no reset mechanisms. To bridge the gains offered by SNNs and the recent deep SSM models, we propose a novel multiple-output spiking neuron model that combines a linear, general SSM state transition with a non-linear feedback mechanism through reset. Compared to the existing neuron models for SNNs, our proposed model clearly conceptualizes the differences between the spiking function, the reset condition and the reset action. The experimental results on various tasks, i.e., a keyword spotting task, an event-based vision task and a sequential pattern recognition task, show that our proposed model achieves performance comparable to existing benchmarks in the SNN literature. Our results illustrate how the proposed reset mechanism can overcome instability and enable learning even when the linear part of neuron dynamics is unstable, allowing us to go beyond the strictly enforced stability of linear dynamics in recent deep SSM models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields</title>
<link>https://arxiv.org/abs/2508.06301</link>
<guid>https://arxiv.org/abs/2508.06301</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural fields, Federated Meta-Learning, privacy-preserving, optimization, data privacy

Summary:
FedMeNF is a novel Federated Meta-Learning (FML) approach designed to efficiently map neural fields while ensuring privacy preservation. By introducing a privacy-preserving loss function, FedMeNF regulates privacy leakage during local meta-optimization, allowing for fast and efficient optimization without compromising client data privacy. The experiments conducted show that FedMeNF achieves rapid optimization speed and robust reconstruction performance even with limited training data and diverse modalities. This approach is particularly effective for few-shot or non-IID data scenarios, making it suitable for resource-constrained edge devices. In conclusion, FedMeNF offers a promising solution for learning neural fields in a privacy-preserving manner, addressing the challenges of scalability and data privacy in large-scale data settings. 

<br /><br />Summary: <div>
arXiv:2508.06301v1 Announce Type: new 
Abstract: Neural fields provide a memory-efficient representation of data, which can effectively handle diverse modalities and large-scale data. However, learning to map neural fields often requires large amounts of training data and computations, which can be limited to resource-constrained edge devices. One approach to tackle this limitation is to leverage Federated Meta-Learning (FML), but traditional FML approaches suffer from privacy leakage. To address these issues, we introduce a novel FML approach called FedMeNF. FedMeNF utilizes a new privacy-preserving loss function that regulates privacy leakage in the local meta-optimization. This enables the local meta-learner to optimize quickly and efficiently without retaining the client's private data. Our experiments demonstrate that FedMeNF achieves fast optimization speed and robust reconstruction performance, even with few-shot or non-IID data across diverse data modalities, while preserving client data privacy.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Partner Design Enables Robust Ad-hoc Teamwork</title>
<link>https://arxiv.org/abs/2508.06336</link>
<guid>https://arxiv.org/abs/2508.06336</guid>
<content:encoded><![CDATA[
<div> UPD, multi-agent reinforcement learning, ad-hoc teamwork, unsupervised partner design, dynamic partner curriculum <br />
Summary:
The article presents Unsupervised Partner Design (UPD), a framework for robust ad-hoc teamwork in multi-agent reinforcement learning. UPD generates training partners adaptively without relying on pretrained partners or manual tuning. It creates diverse partners by mixing an ego agent's policy with random behaviors and uses a variance-based learnability metric to prioritize partners near the learning frontier. UPD can be combined with unsupervised environment design, enabling fully unsupervised curricula in cooperative settings. Evaluation on Overcooked-AI and the Overcooked Generalisation Challenge shows that UPD outperforms baselines and ablations consistently. A user study confirms UPD's effectiveness, with higher returns compared to baselines and perceived adaptiveness, human-likeness, collaboration, and reduced frustration. <br /><br /> <div>
arXiv:2508.06336v1 Announce Type: new 
Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free, multi-agent reinforcement learning framework for robust ad-hoc teamwork that adaptively generates training partners without requiring pretrained partners or manual parameter tuning. UPD constructs diverse partners by stochastically mixing an ego agent's policy with biased random behaviours and scores them using a variance-based learnability metric that prioritises partners near the ego agent's current learning frontier. We show that UPD can be integrated with unsupervised environment design, resulting in the first method enabling fully unsupervised curricula over both level and partner distributions in a cooperative setting. Through extensive evaluations on Overcooked-AI and the Overcooked Generalisation Challenge, we demonstrate that this dynamic partner curriculum is highly effective: UPD consistently outperforms both population-based and population-free baselines as well as ablations. In a user study, we further show that UPD achieves higher returns than all baselines and was perceived as significantly more adaptive, more human-like, a better collaborator, and less frustrating.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing Fractional Classification Loss for Robust Learning with Noisy Labels</title>
<link>https://arxiv.org/abs/2508.06346</link>
<guid>https://arxiv.org/abs/2508.06346</guid>
<content:encoded><![CDATA[
<div> Fractional Classification Loss, robust loss, label noise, deep neural networks, automatic calibration <br />
<br />Summary: 
The study introduces Fractional Classification Loss (FCL) as an adaptive robust loss function for training deep neural networks in the presence of label noise. FCL combines both the fractional derivative of the Cross-Entropy loss and the Mean Absolute Error in a framework that automatically adjusts its robustness level during training. The fractional derivative parameter '' allows FCL to interpolate between robustness and fast convergence, optimizing the trade-off for improved performance. By dynamically adjusting '' as a learnable parameter, FCL can effectively shape its loss landscape, leading to stable learning and efficient classification under label noise conditions. Extensive experiments on benchmark datasets confirm the superior performance of FCL without the need for manual hyperparameter tuning. <div>
arXiv:2508.06346v1 Announce Type: new 
Abstract: Robust loss functions are crucial for training deep neural networks in the presence of label noise, yet existing approaches require extensive, dataset-specific hyperparameter tuning. In this work, we introduce Fractional Classification Loss (FCL), an adaptive robust loss that automatically calibrates its robustness to label noise during training. Built within the active-passive loss framework, FCL employs the fractional derivative of the Cross-Entropy (CE) loss as its active component and the Mean Absolute Error (MAE) as its passive loss component. With this formulation, we demonstrate that the fractional derivative order $\mu$ spans a family of loss functions that interpolate between MAE-like robustness and CE-like fast convergence. Furthermore, we integrate $\mu$ into the gradient-based optimization as a learnable parameter and automatically adjust it to optimize the trade-off between robustness and convergence speed. We reveal that FCL's unique property establishes a critical trade-off that enables the stable learning of $\mu$: lower log penalties on difficult or mislabeled examples improve robustness but impose higher penalties on easy or clean data, reducing model confidence in them. Consequently, FCL can dynamically reshape its loss landscape to achieve effective classification performance under label noise. Extensive experiments on benchmark datasets show that FCL achieves state-of-the-art results without the need for manual hyperparameter tuning.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Equation-VAE: Disentangled Latent Representations for Tabular Data</title>
<link>https://arxiv.org/abs/2508.06347</link>
<guid>https://arxiv.org/abs/2508.06347</guid>
<content:encoded><![CDATA[
<div> SE-VAE, Structural Equation-Variational Autoencoder, latent representations, tabular data, deep generative modeling <br />
Summary: 
SE-VAE introduces a novel architecture for learning interpretable latent representations from tabular data. Inspired by structural equation modeling, it aligns latent subspaces with known indicator groupings and introduces a global nuisance latent to isolate confounding variation. The modular design enables disentanglement through architecture rather than statistical regularizers. In evaluations on simulated datasets, SE-VAE outperforms leading baselines in factor recovery, interpretability, and robustness to nuisance variation. Ablation results highlight the importance of architectural structure over regularization strength. SE-VAE provides a principled framework for generative modeling in scientific and social domains requiring theory-driven latent constructs and measurement validity. <br /> <div>
arXiv:2508.06347v1 Announce Type: new 
Abstract: Learning interpretable latent representations from tabular data remains a challenge in deep generative modeling. We introduce SE-VAE (Structural Equation-Variational Autoencoder), a novel architecture that embeds measurement structure directly into the design of a variational autoencoder. Inspired by structural equation modeling, SE-VAE aligns latent subspaces with known indicator groupings and introduces a global nuisance latent to isolate construct-specific confounding variation. This modular architecture enables disentanglement through design rather than through statistical regularizers alone. We evaluate SE-VAE on a suite of simulated tabular datasets and benchmark its performance against a series of leading baselines using standard disentanglement metrics. SE-VAE consistently outperforms alternatives in factor recovery, interpretability, and robustness to nuisance variation. Ablation results reveal that architectural structure, rather than regularization strength, is the key driver of performance. SE-VAE offers a principled framework for white-box generative modeling in scientific and social domains where latent constructs are theory-driven and measurement validity is essential.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means</title>
<link>https://arxiv.org/abs/2508.06353</link>
<guid>https://arxiv.org/abs/2508.06353</guid>
<content:encoded><![CDATA[
<div> Keywords: Geometric-k-means, k-means algorithm, machine learning, energy economy, computational efficiency

Summary:
Geometric-k-means (Gk-means) is introduced as an efficient enhancement to the traditional k-means algorithm by utilizing geometric principles, specifically scalar projection, to accelerate the clustering process without compromising solution quality. By focusing on high expressive data (HE) and bypassing low expressive data (LE), Gk-means reduces computational overhead significantly. Experiments on various datasets show that Gk-means outperforms traditional and state-of-the-art k-means variants in terms of runtime and distance computations (DC), while also exhibiting better resource efficiency with a reduced energy footprint, making it a more sustainable alternative. Overall, Gk-means presents a promising approach to improving the efficiency and energy economy of clustering algorithms in machine learning applications.

<br /><br />Summary: <div>
arXiv:2508.06353v1 Announce Type: new 
Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel approach that significantly enhances the efficiency and energy economy of the widely utilized k-means algorithm, which, despite its inception over five decades ago, remains a cornerstone in machine learning applications. The essence of Gk-means lies in its active utilization of geometric principles, specifically scalar projection, to significantly accelerate the algorithm without sacrificing solution quality. This geometric strategy enables a more discerning focus on data points that are most likely to influence cluster updates, which we call as high expressive data (HE). In contrast, low expressive data (LE), does not impact clustering outcome, is effectively bypassed, leading to considerable reductions in computational overhead. Experiments spanning synthetic, real-world and high-dimensional datasets, demonstrate Gk-means is significantly better than traditional and state of the art (SOTA) k-means variants in runtime and distance computations (DC). Moreover, Gk-means exhibits better resource efficiency, as evidenced by its reduced energy footprint, placing it as more sustainable alternative.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</title>
<link>https://arxiv.org/abs/2508.06361</link>
<guid>https://arxiv.org/abs/2508.06361</guid>
<content:encoded><![CDATA[
<div> Language Models, Deception, Trustworthiness, Self-initiated, Evaluation <br />
<br />
Summary: Large Language Models (LLMs) are widely used in various tasks, raising concerns about their trustworthiness, especially in terms of intentional deception. This study explores the phenomenon of LLMs initiating deception on benign prompts without human-induced manipulation. A novel framework using "contact searching questions" is proposed to evaluate deception, introducing two statistical metrics to measure deceptive intention and behavior. Evaluation of 14 leading LLMs shows that both metrics increase with task difficulty, indicating a rising tendency toward deception, particularly in handling complex problems. A mathematical model is formulated to explain this behavior, highlighting critical concerns for deploying LLM agents in crucial domains. <div>
arXiv:2508.06361v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have been widely deployed in reasoning, planning, and decision-making tasks, making their trustworthiness a critical concern. The potential for intentional deception, where an LLM deliberately fabricates or conceals information to serve a hidden objective, remains a significant and underexplored threat. Existing studies typically induce such deception by explicitly setting a "hidden" objective through prompting or fine-tuning, which may not fully reflect real-world human-LLM interactions. Moving beyond this human-induced deception, we investigate LLMs' self-initiated deception on benign prompts. To address the absence of ground truth in this evaluation, we propose a novel framework using "contact searching questions." This framework introduces two statistical metrics derived from psychological principles to quantify the likelihood of deception. The first, the Deceptive Intention Score, measures the model's bias towards a hidden objective. The second, Deceptive Behavior Score, measures the inconsistency between the LLM's internal belief and its expressed output. Upon evaluating 14 leading LLMs, we find that both metrics escalate as task difficulty increases, rising in parallel for most models. Building on these findings, we formulate a mathematical model to explain this behavior. These results reveal that even the most advanced LLMs exhibit an increasing tendency toward deception when handling complex problems, raising critical concerns for the deployment of LLM agents in complex and crucial domains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design</title>
<link>https://arxiv.org/abs/2508.06364</link>
<guid>https://arxiv.org/abs/2508.06364</guid>
<content:encoded><![CDATA[
<div> Classifier-guided diffusion, drug design, molecular activity, multi-target modulation, off-target toxicity mitigation
<br />
Summary:
Achieving precise control over a molecule's biological activity is a challenging task in drug design. Existing methods focus on single desired activities, lacking mechanisms for managing multiple molecular interactions. The proposed ActivityDiff approach uses diffusion models guided by separate drug-target classifiers for positive and negative guidance to enhance desired activities and minimize off-target effects. Experimental results demonstrate the effectiveness of ActivityDiff in tasks such as single-/dual-target generation, fragment-constrained dual-target design, selective generation for target specificity enhancement, and off-target effect reduction. This novel paradigm offers integrated control over molecular activity, providing a versatile and extensible framework for drug design. <div>
arXiv:2508.06364v1 Announce Type: new 
Abstract: Achieving precise control over a molecule's biological activity-encompassing targeted activation/inhibition, cooperative multi-target modulation, and off-target toxicity mitigation-remains a critical challenge in de novo drug design. However, existing generative methods primarily focus on producing molecules with a single desired activity, lacking integrated mechanisms for the simultaneous management of multiple intended and unintended molecular interactions. Here, we propose ActivityDiff, a generative approach based on the classifier-guidance technique of diffusion models. It leverages separately trained drug-target classifiers for both positive and negative guidance, enabling the model to enhance desired activities while minimizing harmful off-target effects. Experimental results show that ActivityDiff effectively handles essential drug design tasks, including single-/dual-target generation, fragment-constrained dual-target design, selective generation to enhance target specificity, and reduction of off-target effects. These results demonstrate the effectiveness of classifier-guided diffusion in balancing efficacy and safety in molecular design. Overall, our work introduces a novel paradigm for achieving integrated control over molecular activity, and provides ActivityDiff as a versatile and extensible framework.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation</title>
<link>https://arxiv.org/abs/2508.06387</link>
<guid>https://arxiv.org/abs/2508.06387</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-SQL, database intent prediction, large language models, prompt engineering, end-to-end framework

Summary:
This paper introduces a three-stage end-to-end text-to-SQL framework that aims to identify the intended database of a user before generating SQL queries. By leveraging large language models (LLMs) and prompt engineering, implicit information is extracted from natural language queries (NLQs) to create a ruleset. A db_id prediction model, incorporating a RoBERTa-based finetuned encoder, is then trained to predict the correct database identifier (db_id) based on the NLQ and generated rules. Finally, critic agents are used to refine the generated SQL queries. Experimental results show that this framework surpasses current state-of-the-art models in both database intent prediction and SQL generation accuracy.

<br /><br />Summary: <div>
arXiv:2508.06387v1 Announce Type: new 
Abstract: Text-to-SQL bridges the gap between natural language and structured database language, thus allowing non-technical users to easily query databases. Traditional approaches model text-to-SQL as a direct translation task, where a given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances in large language models (LLMs) have significantly improved translation accuracy, however, these methods all require that the target database is pre-specified. This becomes problematic in scenarios with multiple extensive databases, where identifying the correct database becomes a crucial yet overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL framework to identify the user's intended database before generating SQL queries. Our approach leverages LLMs and prompt engineering to extract implicit information from natural language queries (NLQs) in the form of a ruleset. We then train a large db\_id prediction model, which includes a RoBERTa-based finetuned encoder, to predict the correct Database identifier (db\_id) based on both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL by using critic agents to correct errors. Experimental results demonstrate that our framework outperforms the current state-of-the-art models in both database intent prediction and SQL generation accuracy.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images</title>
<link>https://arxiv.org/abs/2508.06409</link>
<guid>https://arxiv.org/abs/2508.06409</guid>
<content:encoded><![CDATA[
<div> Keywords: homelessness, United States, monitoring, 311 Service Calls, predictive model

Summary: 
This study introduces a new method to track and forecast homeless tent trends in San Francisco using 311 Service Calls and street-level imagery. Traditional methods like point-in-time counts have limitations in terms of frequency and spatial detail, but this new approach offers fine-grained daily and neighborhood-level variations. The predictive model captures patterns that traditional counts often miss, including rapid fluctuations during the COVID-19 pandemic and spatial shifts in tent locations over time. By utilizing publicly available, crowdsourced data, this approach provides more timely, localized, and cost-effective information to guide policy responses and evaluate interventions to reduce unsheltered homelessness. The study highlights the surge of homelessness in the United States, emphasizing the importance of innovative monitoring techniques to address this critical social issue.<br /><br />Summary: <div>
arXiv:2508.06409v1 Announce Type: new 
Abstract: Homelessness in the United States has surged to levels unseen since the Great Depression. However, existing methods for monitoring it, such as point-in-time (PIT) counts, have limitations in terms of frequency, consistency, and spatial detail. This study proposes a new approach using publicly available, crowdsourced data, specifically 311 Service Calls and street-level imagery, to track and forecast homeless tent trends in San Francisco. Our predictive model captures fine-grained daily and neighborhood-level variations, uncovering patterns that traditional counts often overlook, such as rapid fluctuations during the COVID-19 pandemic and spatial shifts in tent locations over time. By providing more timely, localized, and cost-effective information, this approach serves as a valuable tool for guiding policy responses and evaluating interventions aimed at reducing unsheltered homelessness.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-efficient LLM Optimization with Reset Replay</title>
<link>https://arxiv.org/abs/2508.06412</link>
<guid>https://arxiv.org/abs/2508.06412</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Large Language Models, Preference Optimization, LoRR, Sample Efficiency

Summary:
- Recent advancements in post-training Large Language Models (LLMs) have focused on enhancing reasoning capabilities through Reinforcement Learning (RL) and preference optimization.
- These methods often suffer from low sample efficiency and primacy bias, leading to overfitting and degraded policy quality.
- To address these challenges, LoRR is introduced as a plugin to improve sample efficiency in preference-based optimization frameworks.
- LoRR enables training at a high replay number and incorporates a periodic reset strategy to counteract overfitting while preserving network plasticity.
- By leveraging a hybrid optimization objective combining supervised fine-tuning and preference-based losses, LoRR significantly enhances the performance of preference optimization methods on various benchmarks, including math and general reasoning tasks. 
<br /><br />Summary: <div>
arXiv:2508.06412v1 Announce Type: new 
Abstract: Recent advancements in post-training Large Language Models (LLMs), particularly through Reinforcement Learning (RL) and preference optimization methods, are key drivers for enhancing their reasoning capabilities. However, these methods are often plagued by low sample efficiency and a susceptibility to primacy bias, where overfitting to initial experiences degrades policy quality and damages the learning process. To address these challenges, we introduce LLM optimization with Reset Replay (LoRR), a general and powerful plugin designed to enhance sample efficiency in any preference-based optimization framework. LoRR core mechanism enables training at a high replay number, maximizing the utility of each collected data batch. To counteract the risk of overfitting inherent in high-replay training, LoRR incorporates a periodic reset strategy with reusing initial data, which preserves network plasticity. Furthermore, it leverages a hybrid optimization objective, combining supervised fine-tuning (SFT) and preference-based losses to further bolster data exploitation. Our extensive experiments demonstrate that LoRR significantly boosts the performance of various preference optimization methods on both mathematical and general reasoning benchmarks. Notably, an iterative DPO approach augmented with LoRR achieves comparable performance on challenging math tasks, outperforming some complex and computationally intensive RL-based algorithms. These findings highlight that LoRR offers a practical, sample-efficient, and highly effective paradigm for LLM finetuning, unlocking greater performance from limited data.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection</title>
<link>https://arxiv.org/abs/2508.06467</link>
<guid>https://arxiv.org/abs/2508.06467</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, machine unlearning, sensitive data, gradient ratio, selective noise injection

Summary: 
The article introduces a framework called GRIN for effective machine unlearning in large language models (LLMs). It addresses the need for forgetting sensitive or unauthorized data without compromising model utility. GRIN uses a gradient-ratio-based metric to identify parameters responsible for memorizing forget data and selectively injects noise into these parameters during fine-tuning. This targeted approach improves unlearning performance while minimizing unintended degradation of unrelated knowledge. New evaluation metrics tailored to LLMs are proposed and validated on standard benchmarks such as TOFU, WMDP, and SafePKU. The GRIN framework offers a modular and efficient solution for ensuring data privacy and ethical compliance in the use of LLMs. 

<br /><br />Summary: <div>
arXiv:2508.06467v1 Announce Type: new 
Abstract: The growing legal and ethical scrutiny of large language models (LLMs) necessitates effective machine unlearning, particularly for sensitive or unauthorized data. Existing empirical methods often yield incomplete forgetting or unintended degradation of unrelated knowledge due to poor localization. In this work, we propose GRIN: a modular and targeted framework for LLM unlearning. GRIN introduces a novel gradient-ratio-based metric to identify parameters most responsible for memorizing forget data. We then perform selective noise injection into these parameters prior to fine-tuning, which improves unlearning performance while maintaining model utility. Finally, we propose new evaluation metrics tailored to the LLM setting and validate our approach on standard benchmarks such as TOFU, WMDP, and SafePKU.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Indian Legal NLP Benchmarks : A Survey</title>
<link>https://arxiv.org/abs/2107.06056</link>
<guid>https://arxiv.org/abs/2107.06056</guid>
<content:encoded><![CDATA[
<div> Keywords: benchmarks, AI, legal text, Indian, Natural Language Processing

Summary:
In the realm of Artificial Intelligence (AI) advancements, the availability of challenging benchmarks plays a pivotal role. Given the distinctive nature of Legal Text compared to standard English text, there is a pressing need to establish dedicated Natural Language Processing benchmarks for Indian Legal Text. These specialized benchmarks would concentrate on tasks specific to Legal Systems, thereby fostering innovation in the application of Natural Language Processing techniques within the context of Indian legal frameworks. By creating such benchmarks, not only would the AI community benefit from enhanced research opportunities, but the legal fraternity in India would also reap significant advantages. Through a comprehensive review of existing efforts in this domain and a proposal for new benchmark creation, this initiative seeks to catalyze progress in the intersection of AI and Indian Legal Text processing.<br /><br />Summary: <div>
arXiv:2107.06056v1 Announce Type: cross 
Abstract: Availability of challenging benchmarks is the key to advancement of AI in a specific field.Since Legal Text is significantly different than normal English text, there is a need to create separate Natural Language Processing benchmarks for Indian Legal Text which are challenging and focus on tasks specific to Legal Systems. This will spur innovation in applications of Natural language Processing for Indian Legal Text and will benefit AI community and Legal fraternity. We review the existing work in this area and propose ideas to create new benchmarks for Indian Legal Natural Language Processing.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment Estimate and Variational Approach for Learning Generalized Diffusion with Non-gradient Structures</title>
<link>https://arxiv.org/abs/2508.01854</link>
<guid>https://arxiv.org/abs/2508.01854</guid>
<content:encoded><![CDATA[
<div> learning framework, governing laws, generalized diffusions, non-gradient components, data-driven

Summary:
This paper presents a data-driven learning framework for identifying governing laws of generalized diffusions with non-gradient components. The framework combines energy dissipation laws with a physically consistent penalty and first-moment evolution to recover the pseudo-potential and rotation in the pointwise orthogonal decomposition of non-gradient drifts. A two-stage method is proposed and applied to complex generalized diffusion processes, including dissipation-rotation dynamics, rough pseudo-potentials, and noisy data. The method's effectiveness for learning physical laws in non-gradient generalized diffusions is demonstrated through representative numerical experiments. <div>
arXiv:2508.01854v2 Announce Type: cross 
Abstract: This paper proposes a data-driven learning framework for identifying governing laws of generalized diffusions with non-gradient components. By combining energy dissipation laws with a physically consistent penalty and first-moment evolution, we design a two-stage method to recover the pseudo-potential and rotation in the pointwise orthogonal decomposition of a class of non-gradient drifts in generalized diffusions. Our two-stage method is applied to complex generalized diffusion processes including dissipation-rotation dynamics, rough pseudo-potentials and noisy data. Representative numerical experiments demonstrate the effectiveness of our approach for learning physical laws in non-gradient generalized diffusions.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Guided Accelerator For Search Experience</title>
<link>https://arxiv.org/abs/2508.05649</link>
<guid>https://arxiv.org/abs/2508.05649</guid>
<content:encoded><![CDATA[
arXiv:2508.05649v1 Announce Type: cross 
Abstract: Effective query reformulation is pivotal in narrowing the gap between a user's exploratory search behavior and the identification of relevant products in e-commerce environments. While traditional approaches predominantly model query rewrites as isolated pairs, they often fail to capture the sequential and transitional dynamics inherent in real-world user behavior. In this work, we propose a novel framework that explicitly models transitional queries--intermediate reformulations occurring during the user's journey toward their final purchase intent. By mining structured query trajectories from eBay's large-scale user interaction logs, we reconstruct query sequences that reflect shifts in intent while preserving semantic coherence. This approach allows us to model a user's shopping funnel, where mid-journey transitions reflect exploratory behavior and intent refinement. Furthermore, we incorporate generative Large Language Models (LLMs) to produce semantically diverse and intent-preserving alternative queries, extending beyond what can be derived through collaborative filtering alone. These reformulations can be leveraged to populate Related Searches or to power intent-clustered carousels on the search results page, enhancing both discovery and engagement. Our contributions include (i) the formal identification and modeling of transitional queries, (ii) the introduction of a structured query sequence mining pipeline for intent flow understanding, and (iii) the application of LLMs for scalable, intent-aware query expansion. Empirical evaluation demonstrates measurable gains in conversion and engagement metrics compared to the existing Related Searches module, validating the effectiveness of our approach in real-world e-commerce settings.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Walk Learning and the Pac-Man Attack</title>
<link>https://arxiv.org/abs/2508.05663</link>
<guid>https://arxiv.org/abs/2508.05663</guid>
<content:encoded><![CDATA[
arXiv:2508.05663v1 Announce Type: cross 
Abstract: Random walk (RW)-based algorithms have long been popular in distributed systems due to low overheads and scalability, with recent growing applications in decentralized learning. However, their reliance on local interactions makes them inherently vulnerable to malicious behavior. In this work, we investigate an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious node probabilistically terminates any RW that visits it. This stealthy behavior gradually eliminates active RWs from the network, effectively halting the learning process without triggering failure alarms. To counter this threat, we propose the Average Crossing (AC) algorithm--a fully decentralized mechanism for duplicating RWs to prevent RW extinction in the presence of Pac-Man. Our theoretical analysis establishes that (i) the RW population remains almost surely bounded under AC and (ii) RW-based stochastic gradient descent remains convergent under AC, even in the presence of Pac-Man, with a quantifiable deviation from the true optimum. Our extensive empirical results on both synthetic and real-world datasets corroborate our theoretical findings. Furthermore, they uncover a phase transition in the extinction probability as a function of the duplication threshold. We offer theoretical insights by analyzing a simplified variant of the AC, which sheds light on the observed phase transition.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis</title>
<link>https://arxiv.org/abs/2508.05666</link>
<guid>https://arxiv.org/abs/2508.05666</guid>
<content:encoded><![CDATA[
arXiv:2508.05666v1 Announce Type: cross 
Abstract: We present HySemRAG, a framework that combines Extract, Transform, Load (ETL) pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale literature synthesis and identify methodological research gaps. The system addresses limitations in existing RAG architectures through a multi-layered approach: hybrid retrieval combining semantic search, keyword filtering, and knowledge graph traversal; an agentic self-correction framework with iterative quality assurance; and post-hoc citation verification ensuring complete traceability. Our implementation processes scholarly literature through eight integrated stages: multi-source metadata acquisition, asynchronous PDF retrieval, custom document layout analysis using modified Docling architecture, bibliographic management, LLM-based field extraction, topic modeling, semantic unification, and knowledge graph construction. The system creates dual data products - a Neo4j knowledge graph enabling complex relationship queries and Qdrant vector collections supporting semantic search - serving as foundational infrastructure for verifiable information synthesis. Evaluation across 643 observations from 60 testing sessions demonstrates structured field extraction achieving 35.1% higher semantic similarity scores (0.655 $\pm$ 0.178) compared to PDF chunking approaches (0.485 $\pm$ 0.204, p < 0.000001). The agentic quality assurance mechanism achieves 68.3% single-pass success rates with 99.0% citation accuracy in validated responses. Applied to geospatial epidemiology literature on ozone exposure and cardiovascular disease, the system identifies methodological trends and research gaps, demonstrating broad applicability across scientific domains for accelerating evidence synthesis and discovery.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports</title>
<link>https://arxiv.org/abs/2508.05669</link>
<guid>https://arxiv.org/abs/2508.05669</guid>
<content:encoded><![CDATA[
arXiv:2508.05669v1 Announce Type: cross 
Abstract: Accurately extracting and representing the structure of tabular data from financial documents remains a critical challenge in document understanding, particularly for regulatory and analytical use cases. This study addresses the complexity of converting financial tables from Malaysian audited financial reports into Markdown format, a task complicated by rotated layouts, multi-level headers, and implicit structural cues. We propose a fine-tuned vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for high-fidelity Markdown generation from document images. Our approach includes a curated dataset of 2,152 image-text pairs with augmentations and a supervised fine-tuning strategy using LoRA. To assess performance, we evaluated our model on 100 out-of-sample tables using a dual framework: a criteria-based LLM-as-a-judge for fine-grained accuracy and our novel Markdown Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based assessment and a 96.53% Markdown TEDS score. This performance significantly surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized reasoning-enabled models. Compared to these self-hosted alternatives, it also significantly reduces inference time. Furthermore, its accuracy exceeds that of widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash. These results demonstrate that domain-specific fine-tuning provides an effective and efficient method to bridge the gap between unstructured financial documents and downstream automation, rivalling much larger and more general models without their computational overhead.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems</title>
<link>https://arxiv.org/abs/2508.05673</link>
<guid>https://arxiv.org/abs/2508.05673</guid>
<content:encoded><![CDATA[
arXiv:2508.05673v1 Announce Type: cross 
Abstract: In the realm of recommender systems (RS), Top-$K$ ranking metrics such as NDCG@$K$ are the gold standard for evaluating recommendation performance. However, during the training of recommendation models, optimizing NDCG@$K$ poses significant challenges due to its inherent discontinuous nature and the intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either overlooked the Top-$K$ truncation or suffered from high computational costs and training instability. To overcome these limitations, we propose SoftmaxLoss@$K$ (SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization. Specifically, we integrate the quantile technique to handle Top-$K$ truncation and derive a smooth upper bound for optimizing NDCG@$K$ to address discontinuity. The resulting SL@$K$ loss has several desirable properties, including theoretical guarantees, ease of implementation, computational efficiency, gradient stability, and noise robustness. Extensive experiments on four real-world datasets and three recommendation backbones demonstrate that SL@$K$ outperforms existing losses with a notable average improvement of 6.03%. The code is available at https://github.com/Tiny-Snow/IR-Benchmark.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study for developing Natural Language-Based BIM Information Retrieval Systems</title>
<link>https://arxiv.org/abs/2508.05676</link>
<guid>https://arxiv.org/abs/2508.05676</guid>
<content:encoded><![CDATA[
arXiv:2508.05676v1 Announce Type: cross 
Abstract: Building Information Modeling (BIM) is essential for managing building data across the entire lifecycle, supporting tasks from design to maintenance. Natural Language Interface (NLI) systems are increasingly explored as user-friendly tools for information retrieval in Building Information Modeling (BIM) environments. Despite their potential, accurately extracting BIM-related data through natural language queries remains a persistent challenge due to the complexity use queries and specificity of domain knowledge. This study presents a comparative analysis of two prominent approaches for developing NLI-based BIM information retrieval systems: domain-specific fine-tuning and prompt-based learning using large language models (LLMs). A two-stage framework consisting of intent recognition and table-based question answering is implemented to evaluate the effectiveness of both approaches. To support this evaluation, a BIM-specific dataset of 1,740 annotated queries of varying types across 69 models is constructed. Experimental results show that domain-specific fine-tuning delivers superior performance in intent recognition tasks, while prompt-based learning, particularly with GPT-4o, shows strength in table-based question answering. Based on these findings, this study identify a hybrid configuration that combines fine-tuning for intent recognition with prompt-based learning for question answering, achieving more balanced and robust performance across tasks. This integrated approach is further tested through case studies involving BIM models of varying complexity. This study provides a systematic analysis of the strengths and limitations of each approach and discusses the applicability of the NLI to real-world BIM scenarios. The findings offer insights for researchers and practitioners in designing intelligent, language-driven BIM systems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation</title>
<link>https://arxiv.org/abs/2508.05677</link>
<guid>https://arxiv.org/abs/2508.05677</guid>
<content:encoded><![CDATA[
arXiv:2508.05677v1 Announce Type: cross 
Abstract: RL-based medical questionnaire systems have shown great potential in medical scenarios. However, their safety and robustness remain unresolved. This study performs a comprehensive evaluation on adversarial attack methods to identify and analyze their potential vulnerabilities. We formulate the diagnosis process as a Markov Decision Process (MDP), where the state is the patient responses and unasked questions, and the action is either to ask a question or to make a diagnosis. We implemented six prevailing major attack methods, including the Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini & Wagner Attack (C&amp;W) attack, Basic Iterative Method (BIM), DeepFool, and AutoAttack, with seven epsilon values each. To ensure the generated adversarial examples remain clinically plausible, we developed a comprehensive medical validation framework consisting of 247 medical constraints, including physiological bounds, symptom correlations, and conditional medical constraints. We achieved a 97.6% success rate in generating clinically plausible adversarial samples. We performed our experiment on the National Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which consists of 182,630 samples, to predict the participant's 4-year mortality rate. We evaluated our attacks on the AdaptiveFS framework proposed in arXiv:2004.00994. Our results show that adversarial attacks could significantly impact the diagnostic accuracy, with attack success rates ranging from 33.08% (FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict medical constraints on the input, such RL-based medical questionnaire systems still show significant vulnerabilities.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.05684</link>
<guid>https://arxiv.org/abs/2508.05684</guid>
<content:encoded><![CDATA[
arXiv:2508.05684v1 Announce Type: cross 
Abstract: The proliferation of multi-modal fake news on social media poses a significant threat to public trust and social stability. Traditional detection methods, primarily text-based, often fall short due to the deceptive interplay between misleading text and images. While Large Vision-Language Models (LVLMs) offer promising avenues for multi-modal understanding, effectively fusing diverse modal information, especially when their importance is imbalanced or contradictory, remains a critical challenge. This paper introduces MM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal fake news detection. Our core contribution is the Context-Aware Dynamic Fusion Module (CADFM), which employs bi-directional cross-modal attention and a novel dynamic modal gating network. This mechanism adaptively learns and assigns importance weights to textual and visual features based on their contextual relevance, enabling intelligent prioritization of information. Evaluated on the large-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples, MM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing multi-modal baselines by approximately 0.5% and significantly outperforming single-modal approaches. Further analysis demonstrates the model's dynamic weighting capabilities, its robustness to modality perturbations, and performance remarkably close to human-level, underscoring its practical efficacy and interpretability for real-world fake news detection.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Adversarial Transferability via Residual Perturbation Attack</title>
<link>https://arxiv.org/abs/2508.05689</link>
<guid>https://arxiv.org/abs/2508.05689</guid>
<content:encoded><![CDATA[
arXiv:2508.05689v1 Announce Type: cross 
Abstract: Deep neural networks are susceptible to adversarial examples while suffering from incorrect predictions via imperceptible perturbations. Transfer-based attacks create adversarial examples for surrogate models and transfer these examples to target models under black-box scenarios. Recent studies reveal that adversarial examples in flat loss landscapes exhibit superior transferability to alleviate overfitting on surrogate models. However, the prior arts overlook the influence of perturbation directions, resulting in limited transferability. In this paper, we propose a novel attack method, named Residual Perturbation Attack (ResPA), relying on the residual gradient as the perturbation direction to guide the adversarial examples toward the flat regions of the loss function. Specifically, ResPA conducts an exponential moving average on the input gradients to obtain the first moment as the reference gradient, which encompasses the direction of historical gradients. Instead of heavily relying on the local flatness that stems from the current gradients as the perturbation direction, ResPA further considers the residual between the current gradient and the reference gradient to capture the changes in the global perturbation direction. The experimental results demonstrate the better transferability of ResPA than the existing typical transfer-based attack methods, while the transferability can be further improved by combining ResPA with the current input transformation methods. The code is available at https://github.com/ZezeTao/ResPA.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging large language models for SQL behavior-based database intrusion detection</title>
<link>https://arxiv.org/abs/2508.05690</link>
<guid>https://arxiv.org/abs/2508.05690</guid>
<content:encoded><![CDATA[
arXiv:2508.05690v1 Announce Type: cross 
Abstract: Database systems are extensively used to store critical data across various domains. However, the frequency of abnormal database access behaviors, such as database intrusion by internal and external attacks, continues to rise. Internal masqueraders often have greater organizational knowledge, making it easier to mimic employee behavior effectively. In contrast, external masqueraders may behave differently due to their lack of familiarity with the organization. Current approaches lack the granularity needed to detect anomalies at the operational level, frequently misclassifying entire sequences of operations as anomalies, even though most operations are likely to represent normal behavior. On the other hand, some anomalous behaviors often resemble normal activities, making them difficult for existing detection methods to identify. This paper introduces a two-tiered anomaly detection approach for Structured Query Language (SQL) using the Bidirectional Encoder Representations from Transformers (BERT) model, specifically DistilBERT, a more efficient, pre-trained version. Our method combines both unsupervised and supervised machine learning techniques to accurately identify anomalous activities while minimizing the need for data labeling. First, the unsupervised method uses ensemble anomaly detectors that flag embedding vectors distant from learned normal patterns of typical user behavior across the database (out-of-scope queries). Second, the supervised method uses fine-tuned transformer-based models to detect internal attacks with high precision (in-scope queries), using role-labeled classification, even on limited labeled SQL data. Our findings make a significant contribution by providing an effective solution for safeguarding critical database systems from sophisticated threats.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection</title>
<link>https://arxiv.org/abs/2508.05695</link>
<guid>https://arxiv.org/abs/2508.05695</guid>
<content:encoded><![CDATA[
arXiv:2508.05695v1 Announce Type: cross 
Abstract: Enterprises are facing increasing risks of insider threats, while existing detection methods are unable to effectively address these challenges due to reasons such as insufficient temporal dynamic feature modeling, computational efficiency and real-time bottlenecks and cross-modal information island problem. This paper proposes a new insider threat detection framework MambaITD based on the Mamba state space model and cross-modal adaptive fusion. First, the multi-source log preprocessing module aligns heterogeneous data through behavioral sequence encoding, interval smoothing, and statistical feature extraction. Second, the Mamba encoder models long-range dependencies in behavioral and interval sequences, and combines the sequence and statistical information dynamically in combination with the gated feature fusion mechanism. Finally, we propose an adaptive threshold optimization method based on maximizing inter-class variance, which dynamically adjusts the decision threshold by analyzing the probability distribution, effectively identifies anomalies, and alleviates class imbalance and concept drift. Compared with traditional methods, MambaITD shows significant advantages in modeling efficiency and feature fusion capabilities, outperforming Transformer-based methods, and provides a more effective solution for insider threat detection.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking</title>
<link>https://arxiv.org/abs/2508.05700</link>
<guid>https://arxiv.org/abs/2508.05700</guid>
<content:encoded><![CDATA[
arXiv:2508.05700v1 Announce Type: cross 
Abstract: Large embedding tables are indispensable in modern recommendation systems, thanks to their ability to effectively capture and memorize intricate details of interactions among diverse entities. As we explore integrating large embedding tables into Pinterest's ads ranking models, we encountered not only common challenges such as sparsity and scalability, but also several obstacles unique to our context. Notably, our initial attempts to train large embedding tables from scratch resulted in neutral metrics. To tackle this, we introduced a novel multi-faceted pretraining scheme that incorporates multiple pretraining algorithms. This approach greatly enriched the embedding tables and resulted in significant performance improvements. As a result, the multi-faceted large embedding tables bring great performance gain on both the Click-Through Rate (CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid serving infrastructure to overcome GPU memory limits and elevate the scalability. This framework has been deployed in the Pinterest Ads system and achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral end-to-end latency change.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physiologically-Constrained Neural Network Digital Twin Framework for Replicating Glucose Dynamics in Type 1 Diabetes</title>
<link>https://arxiv.org/abs/2508.05705</link>
<guid>https://arxiv.org/abs/2508.05705</guid>
<content:encoded><![CDATA[
arXiv:2508.05705v1 Announce Type: cross 
Abstract: Simulating glucose dynamics in individuals with type 1 diabetes (T1D) is critical for developing personalized treatments and supporting data-driven clinical decisions. Existing models often miss key physiological aspects and are difficult to individualize. Here, we introduce physiologically-constrained neural network (NN) digital twins to simulate glucose dynamics in T1D. To ensure interpretability and physiological consistency, we first build a population-level NN state-space model aligned with a set of ordinary differential equations (ODEs) describing glucose regulation. This model is formally verified to conform to known T1D dynamics. Digital twins are then created by augmenting the population model with individual-specific models, which include personal data, such as glucose management and contextual information, capturing both inter- and intra-individual variability. We validate our approach using real-world data from the T1D Exercise Initiative study. Two weeks of data per participant were split into 5-hour sequences and simulated glucose profiles were compared to observed ones. Clinically relevant outcomes were used to assess similarity via paired equivalence t-tests with predefined clinical equivalence margins. Across 394 digital twins, glucose outcomes were equivalent between simulated and observed data: time in range (70-180 mg/dL) was 75.1$\pm$21.2% (simulated) vs. 74.4$\pm$15.4% (real; P<0.001); time below range (<70 mg/dL) 2.5$\pm$5.2% vs. 3.0$\pm$3.3% (P=0.022); and time above range (>180 mg/dL) 22.4$\pm$22.0% vs. 22.6$\pm$15.9% (P<0.001). Our framework can incorporate unmodeled factors like sleep and activity while preserving key dynamics. This approach enables personalized in silico testing of treatments, supports insulin optimization, and integrates physics-based and data-driven modeling. Code: https://github.com/mosqueralopez/T1DSim_AI
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation</title>
<link>https://arxiv.org/abs/2508.05709</link>
<guid>https://arxiv.org/abs/2508.05709</guid>
<content:encoded><![CDATA[
arXiv:2508.05709v1 Announce Type: cross 
Abstract: User feedback is critical for refining recommendation systems, yet explicit feedback (e.g., likes or dislikes) remains scarce in practice. As a more feasible alternative, inferring user preferences from massive implicit feedback has shown great potential (e.g., a user quickly skipping a recommended video usually indicates disinterest). Unfortunately, implicit feedback is often noisy: a user might skip a video due to accidental clicks or other reasons, rather than disliking it. Such noise can easily misjudge user interests, thereby undermining recommendation performance. To address this issue, we propose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which leverages contextual guidance from relevant user groups, enabling robust and in-depth interpretation of implicit feedback for individual users. Specifically, G-UBS operates via two key agents. First, the User Group Manager (UGM) effectively clusters users to generate group profiles utilizing a ``summarize-cluster-reflect" workflow based on LLMs. Second, the User Feedback Modeler (UFM) employs an innovative group-aware reinforcement learning approach, where each user is guided by the associated group profiles during the reinforcement learning process, allowing UFM to robustly and deeply examine the reasons behind implicit feedback. To assess our G-UBS paradigm, we have constructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To the best of our knowledge, this is the first multi-modal benchmark for implicit feedback evaluation in video recommendation, encompassing 15k users, 25k videos, and 933k interaction records with implicit feedback. Extensive experiments on IF-VR demonstrate that G-UBS significantly outperforms mainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a play rate > 30% and 14.9% higher reasoning accuracy on IF-VR.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduction Techniques for Survival Analysis</title>
<link>https://arxiv.org/abs/2508.05715</link>
<guid>https://arxiv.org/abs/2508.05715</guid>
<content:encoded><![CDATA[
arXiv:2508.05715v1 Announce Type: cross 
Abstract: In this work, we discuss what we refer to as reduction techniques for survival analysis, that is, techniques that "reduce" a survival task to a more common regression or classification task, without ignoring the specifics of survival data. Such techniques particularly facilitate machine learning-based survival analysis, as they allow for applying standard tools from machine and deep learning to many survival tasks without requiring custom learners. We provide an overview of different reduction techniques and discuss their respective strengths and weaknesses. We also provide a principled implementation of some of these reductions, such that they are directly available within standard machine learning workflows. We illustrate each reduction using dedicated examples and perform a benchmark analysis that compares their predictive performance to established machine learning methods for survival analysis.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Model Misspecification in Cosmology with Scale-Dependent Normalizing Flows</title>
<link>https://arxiv.org/abs/2508.05744</link>
<guid>https://arxiv.org/abs/2508.05744</guid>
<content:encoded><![CDATA[
arXiv:2508.05744v1 Announce Type: cross 
Abstract: Current and upcoming cosmological surveys will produce unprecedented amounts of high-dimensional data, which require complex high-fidelity forward simulations to accurately model both physical processes and systematic effects which describe the data generation process. However, validating whether our theoretical models accurately describe the observed datasets remains a fundamental challenge. An additional complexity to this task comes from choosing appropriate representations of the data which retain all the relevant cosmological information, while reducing the dimensionality of the original dataset. In this work we present a novel framework combining scale-dependent neural summary statistics with normalizing flows to detect model misspecification in cosmological simulations through Bayesian evidence estimation. By conditioning our neural network models for data compression and evidence estimation on the smoothing scale, we systematically identify where theoretical models break down in a data-driven manner. We demonstrate a first application to our approach using matter and gas density fields from three CAMELS simulation suites with different subgrid physics implementations.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Universal Machine Learning Force Fields Against Experimental Measurements</title>
<link>https://arxiv.org/abs/2508.05762</link>
<guid>https://arxiv.org/abs/2508.05762</guid>
<content:encoded><![CDATA[
arXiv:2508.05762v1 Announce Type: cross 
Abstract: Universal machine learning force fields (UMLFFs) promise to revolutionize materials science by enabling rapid atomistic simulations across the periodic table. However, their evaluation has been limited to computational benchmarks that may not reflect real-world performance. Here, we present UniFFBench, a comprehensive framework for evaluating UMLFFs against experimental measurements of ~1,500 carefully curated mineral structures spanning diverse chemical environments, bonding types, structural complexity, and elastic properties. Our systematic evaluation of six state-of-the-art UMLFFs reveals a substantial reality gap: models achieving impressive performance on computational benchmarks often fail when confronted with experimental complexity. Even the best-performing models exhibit higher density prediction error than the threshold required for practical applications. Most strikingly, we observe disconnects between simulation stability and mechanical property accuracy, with prediction errors correlating with training data representation rather than the modeling method. These findings demonstrate that while current computational benchmarks provide valuable controlled comparisons, they may overestimate model reliability when extrapolated to experimentally complex chemical spaces. Altogether, UniFFBench establishes essential experimental validation standards and reveals systematic limitations that must be addressed to achieve truly universal force field capabilities.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Trace Optimization of Parameter Dependent Matrices Based on Statistical Learning Theory</title>
<link>https://arxiv.org/abs/2508.05764</link>
<guid>https://arxiv.org/abs/2508.05764</guid>
<content:encoded><![CDATA[
arXiv:2508.05764v1 Announce Type: cross 
Abstract: We consider matrices $\boldsymbol{A}(\boldsymbol\theta)\in\mathbb{R}^{m\times m}$ that depend, possibly nonlinearly, on a parameter $\boldsymbol\theta$ from a compact parameter space $\Theta$. We present a Monte Carlo estimator for minimizing $\text{trace}(\boldsymbol{A}(\boldsymbol\theta))$ over all $\boldsymbol\theta\in\Theta$, and determine the sampling amount so that the backward error of the estimator is bounded with high probability. We derive two types of bounds, based on epsilon nets and on generic chaining. Both types predict a small sampling amount for matrices $\boldsymbol{A}(\boldsymbol\theta)$ with small offdiagonal mass, and parameter spaces $\Theta$ of small ``size.'' Dependence on the matrix dimension~$m$ is only weak or not explicit. The bounds based on epsilon nets are easier to evaluate and come with fully specified constants. In contrast, the bounds based on chaining depend on the Talagrand functionals which are difficult to evaluate, except in very special cases. Comparisons between the two types of bounds are difficult, although the literature suggests that chaining bounds can be superior.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Inherently Safer AGI through Language-Mediated Active Inference</title>
<link>https://arxiv.org/abs/2508.05766</link>
<guid>https://arxiv.org/abs/2508.05766</guid>
<content:encoded><![CDATA[
arXiv:2508.05766v1 Announce Type: cross 
Abstract: This paper proposes a novel framework for developing safe Artificial General Intelligence (AGI) by combining Active Inference principles with Large Language Models (LLMs). We argue that traditional approaches to AI safety, focused on post-hoc interpretability and reward engineering, have fundamental limitations. We present an architecture where safety guarantees are integrated into the system's core design through transparent belief representations and hierarchical value alignment. Our framework leverages natural language as a medium for representing and manipulating beliefs, enabling direct human oversight while maintaining computational tractability. The architecture implements a multi-agent system where agents self-organize according to Active Inference principles, with preferences and safety constraints flowing through hierarchical Markov blankets. We outline specific mechanisms for ensuring safety, including: (1) explicit separation of beliefs and preferences in natural language, (2) bounded rationality through resource-aware free energy minimization, and (3) compositional safety through modular agent structures. The paper concludes with a research agenda centered on the Abstraction and Reasoning Corpus (ARC) benchmark, proposing experiments to validate our framework's safety properties. Our approach offers a path toward AGI development that is inherently safer, rather than retrofitted with safety measures.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction</title>
<link>https://arxiv.org/abs/2508.05838</link>
<guid>https://arxiv.org/abs/2508.05838</guid>
<content:encoded><![CDATA[
arXiv:2508.05838v1 Announce Type: cross 
Abstract: This paper presents a novel approach that integrates vision foundation models with reinforcement learning to enhance object interaction capabilities in simulated environments. By combining the Segment Anything Model (SAM) and YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the AI2-THOR simulation environment, we enable the agent to perceive and interact with objects more effectively. Our comprehensive experiments, conducted across four diverse indoor kitchen settings, demonstrate significant improvements in object interaction success rates and navigation efficiency compared to a baseline agent without advanced perception. The results show a 68% increase in average cumulative reward, a 52.5% improvement in object interaction success rate, and a 33% increase in navigation efficiency. These findings highlight the potential of integrating foundation models with reinforcement learning for complex robotic tasks, paving the way for more sophisticated and capable autonomous agents.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Bandits for Crowdsourcing and Multi-Platform Autobidding</title>
<link>https://arxiv.org/abs/2508.05844</link>
<guid>https://arxiv.org/abs/2508.05844</guid>
<content:encoded><![CDATA[
arXiv:2508.05844v1 Announce Type: cross 
Abstract: Motivated by applications in crowdsourcing, where a fixed sum of money is split among $K$ workers, and autobidding, where a fixed budget is used to bid in $K$ simultaneous auctions, we define a stochastic bandit model where arms belong to the $K$-dimensional probability simplex and represent the fraction of budget allocated to each task/auction. The reward in each round is the sum of $K$ stochastic rewards, where each of these rewards is unlocked with a probability that varies with the fraction of the budget allocated to that task/auction. We design an algorithm whose expected regret after $T$ steps is of order $K\sqrt{T}$ (up to log factors) and prove a matching lower bound. Improved bounds of order $K (\log T)^2$ are shown when the function mapping budget to probability of unlocking the reward (i.e., terminating the task or winning the auction) satisfies additional diminishing-returns conditions.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems</title>
<link>https://arxiv.org/abs/2508.05846</link>
<guid>https://arxiv.org/abs/2508.05846</guid>
<content:encoded><![CDATA[
arXiv:2508.05846v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) and robotics increasingly permeate society, ensuring the ethical behavior of these systems has become paramount. This paper contends that transparency in AI decision-making processes is fundamental to developing trustworthy and ethically aligned robotic systems. We explore how transparency facilitates accountability, enables informed consent, and supports the debugging of ethical algorithms. The paper outlines technical, ethical, and practical challenges in implementing transparency and proposes novel approaches to enhance it, including standardized metrics, explainable AI techniques, and user-friendly interfaces. This paper introduces a framework that connects technical implementation with ethical considerations in robotic systems, focusing on the specific challenges of achieving transparency in dynamic, real-world contexts. We analyze how prioritizing transparency can impact public trust, regulatory policies, and avenues for future research. By positioning transparency as a fundamental element in ethical AI system design, we aim to add to the ongoing discussion on responsible AI and robotics, providing direction for future advancements in this vital field.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training chord recognition models on artificially generated audio</title>
<link>https://arxiv.org/abs/2508.05878</link>
<guid>https://arxiv.org/abs/2508.05878</guid>
<content:encoded><![CDATA[
arXiv:2508.05878v1 Announce Type: cross 
Abstract: One of the challenging problems in Music Information Retrieval is the acquisition of enough non-copyrighted audio recordings for model training and evaluation. This study compares two Transformer-based neural network models for chord sequence recognition in audio recordings and examines the effectiveness of using an artificially generated dataset for this purpose. The models are trained on various combinations of Artificial Audio Multitracks (AAM), Schubert's Winterreise Dataset, and the McGill Billboard Dataset and evaluated with three metrics: Root, MajMin and Chord Content Metric (CCM). The experiments prove that even though there are certainly differences in complexity and structure between artificially generated and human-composed music, the former can be useful in certain scenarios. Specifically, AAM can enrich a smaller training dataset of music composed by a human or can even be used as a standalone training set for a model that predicts chord sequences in pop music, if no other data is available.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Physics-Machine Learning Models for Quantitative Electron Diffraction Refinements</title>
<link>https://arxiv.org/abs/2508.05908</link>
<guid>https://arxiv.org/abs/2508.05908</guid>
<content:encoded><![CDATA[
arXiv:2508.05908v1 Announce Type: cross 
Abstract: High-fidelity electron microscopy simulations required for quantitative crystal structure refinements face a fundamental challenge: while physical interactions are well-described theoretically, real-world experimental effects are challenging to model analytically. To address this gap, we present a novel hybrid physics-machine learning framework that integrates differentiable physical simulations with neural networks. By leveraging automatic differentiation throughout the simulation pipeline, our method enables gradient-based joint optimization of physical parameters and neural network components representing experimental variables, offering superior scalability compared to traditional second-order methods. We demonstrate this framework through application to three-dimensional electron diffraction (3D-ED) structure refinement, where our approach learns complex thickness distributions directly from diffraction data rather than relying on simplified geometric models. This method achieves state-of-the-art refinement performance across synthetic and experimental datasets, recovering atomic positions, thermal displacements, and thickness profiles with high fidelity. The modular architecture proposed can naturally be extended to accommodate additional physical phenomena and extended to other electron microscopy techniques. This establishes differentiable hybrid modeling as a powerful new paradigm for quantitative electron microscopy, where experimental complexities have historically limited analysis.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Construction Site Analysis and Understanding with 3D Segmentation</title>
<link>https://arxiv.org/abs/2508.05922</link>
<guid>https://arxiv.org/abs/2508.05922</guid>
<content:encoded><![CDATA[
arXiv:2508.05922v1 Announce Type: cross 
Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting the exploration of computer-vision-based methodologies for enhanced efficiency and scalability. Traditional data acquisition methods, primarily focusing on indoor environments, falter in construction site's complex, cluttered, and dynamically changing conditions. This paper critically evaluates the application of two advanced 3D segmentation methods, Segment Anything Model (SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained initially on indoor datasets, both models' adaptability and performance are assessed in real-world construction settings, highlighting the gap in current segmentation approaches due to the absence of benchmarks for outdoor scenarios. Through a comparative analysis, this study not only showcases the relative effectiveness of SAM and Mask3D but also addresses the critical need for tailored segmentation workflows capable of extracting actionable insights from construction site data, thereby advancing the field towards more automated and precise monitoring techniques.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection</title>
<link>https://arxiv.org/abs/2508.05934</link>
<guid>https://arxiv.org/abs/2508.05934</guid>
<content:encoded><![CDATA[
arXiv:2508.05934v1 Announce Type: cross 
Abstract: Recently, multi-modal physiological signals based emotion recognition has garnered increasing attention in the field of brain-computer interfaces. Nevertheness, the associated multi-modal physiological features are often high-dimensional and inevitably include irrelevant, redundant, and noisy representation, which can easily lead to overfitting, poor performance, and high computational complexity in emotion classifiers. Feature selection has been widely applied to address these challenges. However, previous studies generally assumed that multi-modal physiological data are complete, whereas in reality, the data are often incomplete due to the openness of the acquisition and operational environment. For example, a part of samples are available in several modalities but not in others. To address this issue, we propose a novel method for incomplete multi-modal physiological signal feature selection called adaptive shared latent structure learning (ASLSL). Based on the property that similar features share similar emotional labels, ASLSL employs adaptive shared latent structure learning to explore a common latent space shared for incomplete multi-modal physiological signals and multi-dimensional emotional labels, thereby mitigating the impact of missing information and mining consensus information. Two most popular multi-modal physiological emotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels were utilized to compare the performance between compare ASLSL and seventeen feature selection methods. Comprehensive experimental results on these datasets demonstrate the effectiveness of ASLSL.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching</title>
<link>https://arxiv.org/abs/2508.05978</link>
<guid>https://arxiv.org/abs/2508.05978</guid>
<content:encoded><![CDATA[
arXiv:2508.05978v1 Announce Type: cross 
Abstract: Singing Voice Conversion (SVC) transfers a source singer's timbre to a target while keeping melody and lyrics. The key challenge in any-to-any SVC is adapting unseen speaker timbres to source audio without quality degradation. Existing methods either face timbre leakage or fail to achieve satisfactory timbre similarity and quality in the generated audio. To address these challenges, we propose DAFMSVC, where the self-supervised learning (SSL) features from the source audio are replaced with the most similar SSL features from the target audio to prevent timbre leakage. It also incorporates a dual cross-attention mechanism for the adaptive fusion of speaker embeddings, melody, and linguistic content. Additionally, we introduce a flow matching module for high quality audio generation from the fused features. Experimental results show that DAFMSVC significantly enhances timbre similarity and naturalness, outperforming state-of-the-art methods in both subjective and objective evaluations.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETA: Energy-based Test-time Adaptation for Depth Completion</title>
<link>https://arxiv.org/abs/2508.05989</link>
<guid>https://arxiv.org/abs/2508.05989</guid>
<content:encoded><![CDATA[
arXiv:2508.05989v1 Announce Type: cross 
Abstract: We propose a method for test-time adaptation of pretrained depth completion models. Depth completion models, trained on some ``source'' data, often predict erroneous outputs when transferred to ``target'' data captured in novel environmental conditions due to a covariate shift. The crux of our method lies in quantifying the likelihood of depth predictions belonging to the source data distribution. The challenge is in the lack of access to out-of-distribution (target) data prior to deployment. Hence, rather than making assumptions regarding the target distribution, we utilize adversarial perturbations as a mechanism to explore the data space. This enables us to train an energy model that scores local regions of depth predictions as in- or out-of-distribution. We update the parameters of pretrained depth completion models at test time to minimize energy, effectively aligning test-time predictions to those of the source distribution. We call our method ``Energy-based Test-time Adaptation'', or ETA for short. We evaluate our method across three indoor and three outdoor datasets, where ETA improve over the previous state-of-the-art method by an average of 6.94% for outdoors and 10.23% for indoors. Project Page: https://fuzzythecat.github.io/eta.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Intelligent Coding Systems Should Write Programs with Justifications</title>
<link>https://arxiv.org/abs/2508.06017</link>
<guid>https://arxiv.org/abs/2508.06017</guid>
<content:encoded><![CDATA[
arXiv:2508.06017v1 Announce Type: cross 
Abstract: Intelligent coding systems are transforming software development by enabling users to specify code behavior in natural language. However, the opaque decision-making of AI-driven coders raises trust and usability concerns, particularly for non-expert users who cannot inspect low-level implementations. We argue that these systems should not only generate code but also produce clear, consistent justifications that bridge model reasoning and user understanding. To this end, we identify two critical justification properties-cognitive alignment and semantic faithfulness-and highlight the limitations of existing methods, including formal verification, static analysis, and post-hoc explainability. We advocate exploring neuro-symbolic approaches for justification generation, where symbolic constraints guide model behavior during training and program semantics are enriched through neural representations, enabling automated consistency checks at inference time.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis</title>
<link>https://arxiv.org/abs/2508.06021</link>
<guid>https://arxiv.org/abs/2508.06021</guid>
<content:encoded><![CDATA[
arXiv:2508.06021v1 Announce Type: cross 
Abstract: Sub-visible particle analysis using flow imaging microscopy combined with deep learning has proven effective in identifying particle types, enabling the distinction of harmless components such as silicone oil from protein particles. However, the scarcity of available data and severe imbalance between particle types within datasets remain substantial hurdles when applying multi-class classifiers to such problems, often forcing researchers to rely on less effective methods. The aforementioned issue is particularly challenging for particle types that appear unintentionally and in lower numbers, such as silicone oil and air bubbles, as opposed to protein particles, where obtaining large numbers of images through controlled settings is comparatively straightforward. In this work, we develop a state-of-the-art diffusion model to address data imbalance by generating high-fidelity images that can augment training datasets, enabling the effective training of multi-class deep neural networks. We validate this approach by demonstrating that the generated samples closely resemble real particle images in terms of visual quality and structure. To assess the effectiveness of using diffusion-generated images in training datasets, we conduct large-scale experiments on a validation dataset comprising 500,000 protein particle images and demonstrate that this approach improves classification performance with no negligible downside. Finally, to promote open research and reproducibility, we publicly release both our diffusion models and the trained multi-class deep neural network classifiers, along with a straightforward interface for easy integration into future studies, at https://github.com/utkuozbulak/svp-generative-ai.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings</title>
<link>https://arxiv.org/abs/2508.06030</link>
<guid>https://arxiv.org/abs/2508.06030</guid>
<content:encoded><![CDATA[
arXiv:2508.06030v1 Announce Type: cross 
Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as science, history, and geography encountered during generative pre-training. However, due to their stochasticity, it is difficult to predict what LLMs have acquired. Prior work has developed different ways to probe this knowledge by investigating the hidden representations, crafting specific task prompts, curating representative samples, and estimating their uncertainty. However, these methods require making forward passes through the underlying model to probe the LLM's knowledge about a specific fact, making them computationally expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or $\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate $\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models that effectively encode factual knowledge as text or graphs as proxies for LLMs. First, we identify a training set of facts known by LLMs through various probing strategies and then adapt embedding models to predict the LLM outputs with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find that sentence embedding models are more suitable than graph embeddings to predict LLM knowledge, shedding light on the underlying representation of the factual landscape. Thus, we believe that knowledge-adapted embeddings can be used to identify knowledge gaps in LLMs at scale and can provide deeper insights into LLMs' internal inductive bias. The code and data are made available at https://github.com/claws-lab/peek.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Density Steering via the Gromov-Wasserstein Optimal Transport Distance</title>
<link>https://arxiv.org/abs/2508.06052</link>
<guid>https://arxiv.org/abs/2508.06052</guid>
<content:encoded><![CDATA[
arXiv:2508.06052v1 Announce Type: cross 
Abstract: We tackle the data-driven chance-constrained density steering problem using the Gromov-Wasserstein metric. The underlying dynamical system is an unknown linear controlled recursion, with the assumption that sufficiently rich input-output data from pre-operational experiments are available. The initial state is modeled as a Gaussian mixture, while the terminal state is required to match a specified Gaussian distribution. We reformulate the resulting optimal control problem as a difference-of-convex program and show that it can be efficiently and tractably solved using the DC algorithm. Numerical results validate our approach through various data-driven schemes.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?</title>
<link>https://arxiv.org/abs/2508.06057</link>
<guid>https://arxiv.org/abs/2508.06057</guid>
<content:encoded><![CDATA[
arXiv:2508.06057v1 Announce Type: cross 
Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a reality, sparking widespread enthusiasm in the research community to collect and work with various modalities, including text, image, video, and audio. Despite recent efforts, satellite spectral imagery, as an additional modality, has yet to receive the attention it deserves. This area presents unique challenges, but also holds great promise in advancing the capabilities of AGI in understanding the natural world. In this paper, we argue why Earth Observation data is useful for an intelligent model, and then we review existing benchmarks and highlight their limitations in evaluating the generalization ability of foundation models in this domain. This paper emphasizes the need for a more comprehensive benchmark to evaluate earth observation models. To facilitate this, we propose a comprehensive set of tasks that a benchmark should encompass to effectively assess a model's ability to understand and interact with Earth observation data.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Forget Imagination!</title>
<link>https://arxiv.org/abs/2508.06062</link>
<guid>https://arxiv.org/abs/2508.06062</guid>
<content:encoded><![CDATA[
arXiv:2508.06062v1 Announce Type: cross 
Abstract: Cognitive imagination is a type of imagination that plays a key role in human thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to mentally visualize coherent and holistic systems of concepts and causal links that serve as semantic contexts for reasoning, decision making and prediction. Our position is that the role of cognitive imagination is still greatly underestimated, and this creates numerous problems and diminishes the current capabilities of AI. For instance, when reasoning, humans rely on imaginary contexts to retrieve background info. They also constantly return to the context for semantic verification that their reasoning is still reasonable. Thus, reasoning without imagination is blind. This paper is a call for greater attention to cognitive imagination as the next promising breakthrough in artificial intelligence. As an instrument for simulating cognitive imagination, we propose semantic models -- a new approach to mathematical models that can learn, like neural networks, and are based on probabilistic causal relationships. Semantic models can simulate cognitive imagination because they ensure the consistency of imaginary contexts and implement a glass-box approach that allows the context to be manipulated as a holistic and coherent system of interrelated facts glued together with causal relations.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Auto-bidding based on Traffic Prediction in Live Advertising</title>
<link>https://arxiv.org/abs/2508.06069</link>
<guid>https://arxiv.org/abs/2508.06069</guid>
<content:encoded><![CDATA[
arXiv:2508.06069v1 Announce Type: cross 
Abstract: Internet live streaming is widely used in online entertainment and e-commerce, where live advertising is an important marketing tool for anchors. An advertising campaign hopes to maximize the effect (such as conversions) under constraints (such as budget and cost-per-click). The mainstream control of campaigns is auto-bidding, where the performance depends on the decision of the bidding algorithm in each request. The most widely used auto-bidding algorithms include Proportional-Integral-Derivative (PID) control, linear programming (LP), reinforcement learning (RL), etc. Existing methods either do not consider the entire time traffic, or have too high computational complexity. In this paper, the live advertising has high requirements for real-time bidding (second-level control) and faces the difficulty of unknown future traffic. Therefore, we propose a lightweight bidding algorithm Binary Constrained Bidding (BiCB), which neatly combines the optimal bidding formula given by mathematical analysis and the statistical method of future traffic estimation, and obtains good approximation to the optimal result through a low complexity solution. In addition, we complement the form of upper and lower bound constraints for traditional auto-bidding modeling and give theoretical analysis of BiCB. Sufficient offline and online experiments prove BiCB's good performance and low engineering cost.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Backtracking for Privacy Protection in Large Language Models</title>
<link>https://arxiv.org/abs/2508.06087</link>
<guid>https://arxiv.org/abs/2508.06087</guid>
<content:encoded><![CDATA[
arXiv:2508.06087v1 Announce Type: cross 
Abstract: The preservation of privacy has emerged as a critical topic in the era of artificial intelligence. However, current work focuses on user-oriented privacy, overlooking severe enterprise data leakage risks exacerbated by the Retrieval-Augmented Generation paradigm. To address this gap, our paper introduces a novel objective: enterprise-oriented privacy concerns. Achieving this objective requires overcoming two fundamental challenges: existing methods such as data sanitization severely degrade model performance, and the field lacks public datasets for evaluation. We address these challenges with several solutions. (1) To prevent performance degradation, we propose ABack, a training-free mechanism that leverages a Hidden State Model to pinpoint the origin of a leakage intention and rewrite the output safely. (2) To solve the lack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy scenarios in healthcare and finance. To ensure a rigorous evaluation, we move beyond simple static attacks by developing a powerful adaptive attacker with Group Relative Policy Optimization. Experiments show that against this superior adversary, ABack improves the overall privacy utility score by up to 15\% over strong baselines, avoiding the performance trade-offs of prior methods.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification</title>
<link>https://arxiv.org/abs/2508.06118</link>
<guid>https://arxiv.org/abs/2508.06118</guid>
<content:encoded><![CDATA[
arXiv:2508.06118v1 Announce Type: cross 
Abstract: Understanding and classifying human cognitive brain states based on neuroimaging data remains one of the foremost and most challenging problems in neuroscience, owing to the high dimensionality and intrinsic noise of the signals. In this work, we propose an ensemble-based graph representation method of functional magnetic resonance imaging (fMRI) data for the task of binary brain-state classification. Our method builds the graph by leveraging multiple base machine-learning models: each edge weight reflects the difference in posterior probabilities between two cognitive states, yielding values in the range [-1, 1] that encode confidence in a given state. We applied this approach to seven cognitive tasks from the Human Connectome Project (HCP 1200 Subject Release), including working memory, gambling, motor activity, language, social cognition, relational processing, and emotion processing. Using only the mean incident edge weights of the graphs as features, a simple logistic-regression classifier achieved average accuracies from 97.07% to 99.74%. We also compared our ensemble graphs with classical correlation-based graphs in a classification task with a graph neural network (GNN). In all experiments, the highest classification accuracy was obtained with ensemble graphs. These results demonstrate that ensemble graphs convey richer topological information and enhance brain-state discrimination. Our approach preserves edge-level interpretability of the fMRI graph representation, is adaptable to multiclass and regression tasks, and can be extended to other neuroimaging modalities and pathological-state classification.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text Clustering</title>
<link>https://arxiv.org/abs/2508.06126</link>
<guid>https://arxiv.org/abs/2508.06126</guid>
<content:encoded><![CDATA[
arXiv:2508.06126v1 Announce Type: cross 
Abstract: In clustering tasks, it is essential to structure the feature space into clear, well-separated distributions. However, because short text representations have limited expressiveness, conventional methods struggle to identify cluster centers that truly capture each category's underlying semantics, causing the representations to be optimized in suboptimal directions. To address this issue, we propose IOCC, a novel few-shot contrastive learning method that achieves alignment between the cluster centers and the semantic centers. IOCC consists of two key modules: Interaction-enhanced Optimal Transport (IEOT) and Center-aware Contrastive Learning (CACL). Specifically, IEOT incorporates semantic interactions between individual samples into the conventional optimal transport problem, and generate pseudo-labels. Based on these pseudo-labels, we aggregate high-confidence samples to construct pseudo-centers that approximate the semantic centers. Next, CACL optimizes text representations toward their corresponding pseudo-centers. As training progresses, the collaboration between the two modules gradually reduces the gap between cluster centers and semantic centers. Therefore, the model will learn a high-quality distribution, improving clustering performance. Extensive experiments on eight benchmark datasets show that IOCC outperforms previous methods, achieving up to 7.34\% improvement on challenging Biomedical dataset and also excelling in clustering stability and efficiency. The code is available at: https://anonymous.4open.science/r/IOCC-C438.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications</title>
<link>https://arxiv.org/abs/2508.06131</link>
<guid>https://arxiv.org/abs/2508.06131</guid>
<content:encoded><![CDATA[
arXiv:2508.06131v1 Announce Type: cross 
Abstract: Quantum machine learning (QML) presents potential for early industrial adoption, yet limited access to quantum hardware remains a significant bottleneck for deployment of QML solutions. This work explores the use of classical surrogates to bypass this restriction, which is a technique that allows to build a lightweight classical representation of a (trained) quantum model, enabling to perform inference on entirely classical devices. We reveal prohibiting high computational demand associated with previously proposed methods for generating classical surrogates from quantum models, and propose an alternative pipeline enabling generation of classical surrogates at a larger scale than was previously possible. Previous methods required at least a high-performance computing (HPC) system for quantum models of below industrial scale (ca. 20 qubits), which raises questions about its practicality. We greatly minimize the redundancies of the previous approach, utilizing only a minute fraction of the resources previously needed. We demonstrate the effectiveness of our method on a real-world energy demand forecasting problem, conducting rigorous testing of performance and computation demand in both simulations and on quantum hardware. Our results indicate that our method achieves high accuracy on the testing dataset while its computational resource requirements scale linearly rather than exponentially. This work presents a lightweight approach to transform quantum solutions into classically deployable versions, facilitating faster integration of quantum technology in industrial settings. Furthermore, it can serve as a powerful research tool in search practical quantum advantage in an empirical setup.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Serving Optimization with Variable Prefill and Decode Lengths</title>
<link>https://arxiv.org/abs/2508.06133</link>
<guid>https://arxiv.org/abs/2508.06133</guid>
<content:encoded><![CDATA[
arXiv:2508.06133v1 Announce Type: cross 
Abstract: We study the problem of serving LLM (Large Language Model) requests where each request has heterogeneous prefill and decode lengths. In LLM serving, the prefill length corresponds to the input prompt length, which determines the initial memory usage in the KV cache. The decode length refers to the number of output tokens generated sequentially, with each additional token increasing the KV cache memory usage by one unit. Given a set of n requests, our goal is to schedule and process them to minimize the total completion time. We show that this problem is NP-hard due to the interplay of batching, placement constraints, precedence relationships, and linearly increasing memory usage. We then analyze commonly used scheduling strategies in practice, such as First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their competitive ratios scale up sublinearly with the memory limit-a significant drawback in real-world settings where memory demand is large. To address this, we propose a novel algorithm based on a new selection metric that efficiently forms batches over time. We prove that this algorithm achieves a constant competitive ratio. Finally, we develop and evaluate a few algorithm variants inspired by this approach, including dynamic programming variants, local search methods, and an LP-based scheduler, demonstrating through comprehensive simulations that they outperform standard baselines while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging</title>
<link>https://arxiv.org/abs/2508.06163</link>
<guid>https://arxiv.org/abs/2508.06163</guid>
<content:encoded><![CDATA[
arXiv:2508.06163v1 Announce Type: cross 
Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task learning, enabling the fusion of multiple fine-tuned models into a single, powerful entity. A key technique in merging methods is sparsification, which prunes redundant parameters from task vectors to mitigate interference. However, prevailing approaches employ a ``one-size-fits-all'' strategy, applying a uniform sparsity ratio that overlooks the inherent structural and statistical heterogeneity of model parameters. This often leads to a suboptimal trade-off, where critical parameters are inadvertently pruned while less useful ones are retained. To address this limitation, we introduce \textbf{TADrop} (\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive sparsification strategy that respects this heterogeneity. Instead of a global ratio, TADrop assigns a tailored sparsity level to each parameter tensor based on its distributional properties. The core intuition is that tensors with denser, more redundant distributions can be pruned aggressively, while sparser, more critical ones are preserved. As a simple and plug-and-play module, we validate TADrop by integrating it with foundational, classic, and SOTA merging methods. Extensive experiments across diverse tasks (vision, language, and multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and significantly boosts their performance. For instance, when enhancing a leading merging method, it achieves an average performance gain of 2.0\% across 8 ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter interference by tailoring sparsification to the model's structure, offering a new baseline for high-performance model merging.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification is a RAG problem: A case study on hate speech detection</title>
<link>https://arxiv.org/abs/2508.06204</link>
<guid>https://arxiv.org/abs/2508.06204</guid>
<content:encoded><![CDATA[
arXiv:2508.06204v1 Announce Type: cross 
Abstract: Robust content moderation requires classification systems that can quickly adapt to evolving policies without costly retraining. We present classification using Retrieval-Augmented Generation (RAG), which shifts traditional classification tasks from determining the correct category in accordance with pre-trained parameters to evaluating content in relation to contextual knowledge retrieved at inference. In hate speech detection, this transforms the task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates this approach and offers three key advantages: (1) robust classification accuracy comparable to leading commercial systems, (2) inherent explainability via retrieved policy segments, and (3) dynamic policy updates without model retraining. Through three experiments, we demonstrate strong baseline performance and show that the system can apply fine-grained policy control by correctly adjusting protection for specific identity groups without requiring retraining or compromising overall performance. These findings establish that RAG can transform classification into a more flexible, transparent, and adaptable process for content moderation and wider classification problems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry breaking for inductive logic programming</title>
<link>https://arxiv.org/abs/2508.06263</link>
<guid>https://arxiv.org/abs/2508.06263</guid>
<content:encoded><![CDATA[
arXiv:2508.06263v1 Announce Type: cross 
Abstract: The goal of inductive logic programming is to search for a hypothesis that generalises training data and background knowledge. The challenge is searching vast hypothesis spaces, which is exacerbated because many logically equivalent hypotheses exist. To address this challenge, we introduce a method to break symmetries in the hypothesis space. We implement our idea in answer set programming. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can reduce solving times from over an hour to just 17 seconds.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Data Generation for Enhanced Intent Recognition in German Speech</title>
<link>https://arxiv.org/abs/2508.06277</link>
<guid>https://arxiv.org/abs/2508.06277</guid>
<content:encoded><![CDATA[
arXiv:2508.06277v1 Announce Type: cross 
Abstract: Intent recognition (IR) for speech commands is essential for artificial intelligence (AI) assistant systems; however, most existing approaches are limited to short commands and are predominantly developed for English. This paper addresses these limitations by focusing on IR from speech by elderly German speakers. We propose a novel approach that combines an adapted Whisper ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based language models trained on synthetic text datasets generated by three well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To evaluate the robustness of our approach, we generate synthetic speech with a text-to-speech model and conduct extensive cross-dataset testing. Our results show that synthetic LLM-generated data significantly boosts classification performance and robustness to different speaking styles and unseen vocabulary. Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the much larger ChatGPT (175B) in dataset quality for German intent recognition. Our approach demonstrates that generative AI can effectively bridge data gaps in low-resource domains. We provide detailed documentation of our data generation and training process to ensure transparency and reproducibility.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Robustness Leaderboard v1 --Technical report</title>
<link>https://arxiv.org/abs/2508.06296</link>
<guid>https://arxiv.org/abs/2508.06296</guid>
<content:encoded><![CDATA[
arXiv:2508.06296v1 Announce Type: cross 
Abstract: This technical report accompanies the LLM robustness leaderboard published by PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior Elicitation Tool (BET), an AI system performing automated red-teaming through Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR) against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we propose a fine-grained robustness metric estimating the average number of attempts required to elicit harmful behaviors, revealing that attack difficulty varies by over 300-fold across models despite universal vulnerability. We introduce primitive-level vulnerability analysis to identify which jailbreaking techniques are most effective for specific hazard categories. Our collaborative evaluation with trusted third parties from the AI Safety Network demonstrates practical pathways for distributed robustness assessment across the community.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech Emotion Recognition</title>
<link>https://arxiv.org/abs/2508.06321</link>
<guid>https://arxiv.org/abs/2508.06321</guid>
<content:encoded><![CDATA[
arXiv:2508.06321v1 Announce Type: cross 
Abstract: Recognizing emotional signals in speech has a significant impact on enhancing the effectiveness of human-computer interaction (HCI). This study introduces EmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term Memory (LSTM) layers with one-dimensional Convolutional Neural Networks (1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and variety of the features that are taken from speech signals have a significant impact on how well SER systems perform. A comprehensive speech data augmentation strategy was used to combine both traditional methods, such as noise addition, pitch shifting, and time stretching, with a novel combination-based augmentation pipeline to enhance generalization and reduce overfitting. Each audio sample was transformed into a high-dimensional feature vector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient (MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a weighted accuracy of 95.78\% and unweighted accuracy of 92.52\% on the IEMOCAP dataset and, with ELU activation, has a weighted accuracy of 96.75\% and unweighted accuracy of 91.28\%. On the RAVDESS dataset, we get a weighted accuracy of 94.53\% and 94.98\% unweighted accuracy for ReLU activation and 93.72\% weighted accuracy and 94.64\% unweighted accuracy for ELU activation. These results highlight EmoAugNet's effectiveness in improving the robustness and performance of SER systems through integated data augmentation and hybrid modeling.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decorrelated feature importance from local sample weighting</title>
<link>https://arxiv.org/abs/2508.06337</link>
<guid>https://arxiv.org/abs/2508.06337</guid>
<content:encoded><![CDATA[
arXiv:2508.06337v1 Announce Type: cross 
Abstract: Feature importance (FI) statistics provide a prominent and valuable method of insight into the decision process of machine learning (ML) models, but their effectiveness has well-known limitations when correlation is present among the features in the training data. In this case, the FI often tends to be distributed among all features which are in correlation with the response-generating signal features. Even worse, if multiple signal features are in strong correlation with a noise feature, while being only modestly correlated with one another, this can result in a noise feature having a distinctly larger FI score than any signal feature. Here we propose local sample weighting (losaw) which can flexibly be integrated into many ML algorithms to improve FI scores in the presence of feature correlation in the training data. Our approach is motivated from inverse probability weighting in causal inference and locally, within the ML model, uses a sample weighting scheme to decorrelate a target feature from the remaining features. This reduces model bias locally, whenever the effect of a potential signal feature is evaluated and compared to others. Moreover, losaw comes with a natural tuning parameter, the minimum effective sample size of the weighted population, which corresponds to an interpretation-prediction-tradeoff, analog to a bias-variance-tradeoff as for classical ML tuning parameters. We demonstrate how losaw can be integrated within decision tree-based ML methods and within mini-batch training of neural networks. We investigate losaw for random forest and convolutional neural networks in a simulation study on settings showing diverse correlation patterns. We found that losaw improves FI consistently. Moreover, it often improves prediction accuracy for out-of-distribution, while maintaining a similar accuracy for in-distribution test data.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering</title>
<link>https://arxiv.org/abs/2508.06345</link>
<guid>https://arxiv.org/abs/2508.06345</guid>
<content:encoded><![CDATA[
arXiv:2508.06345v1 Announce Type: cross 
Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities in diverse domain question-answering (QA) tasks, including graph QA that involves complex graph topologies. However, most current approaches use only a single type of graph representation, namely Topology Representation Form (TRF), such as prompt-unified text descriptions or style-fixed visual styles. Those "one-size-fits-all" approaches fail to consider the specific preferences of different models or tasks, often leading to incorrect or overly long responses. To address this, we first analyze the characteristics and weaknesses of existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency (GRE), which measures the balance between the performance and the brevity in graph QA. Built on these, we develop the DynamicTRF framework, which aims to improve both the accuracy and conciseness of graph QA. To be specific, DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based on their GRE scores, to probe the question-specific TRF preferences. Then it trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from $F_{ZS}$ for each question during the inference. Extensive experiments across 7 in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms of accuracy
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP-SPRT: Differentially Private Sequential Probability Ratio Tests</title>
<link>https://arxiv.org/abs/2508.06377</link>
<guid>https://arxiv.org/abs/2508.06377</guid>
<content:encoded><![CDATA[
arXiv:2508.06377v1 Announce Type: cross 
Abstract: We revisit Wald's celebrated Sequential Probability Ratio Test for sequential tests of two simple hypotheses, under privacy constraints. We propose DP-SPRT, a wrapper that can be calibrated to achieve desired error probabilities and privacy constraints, addressing a significant gap in previous work. DP-SPRT relies on a private mechanism that processes a sequence of queries and stops after privately determining when the query results fall outside a predefined interval. This OutsideInterval mechanism improves upon naive composition of existing techniques like AboveThreshold, potentially benefiting other sequential algorithms. We prove generic upper bounds on the error and sample complexity of DP-SPRT that can accommodate various noise distributions based on the practitioner's privacy needs. We exemplify them in two settings: Laplace noise (pure Differential Privacy) and Gaussian noise (R\'enyi differential privacy). In the former setting, by providing a lower bound on the sample complexity of any $\epsilon$-DP test with prescribed type I and type II errors, we show that DP-SPRT is near optimal when both errors are small and the two hypotheses are close. Moreover, we conduct an experimental study revealing its good practical performance.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms</title>
<link>https://arxiv.org/abs/2508.06383</link>
<guid>https://arxiv.org/abs/2508.06383</guid>
<content:encoded><![CDATA[
arXiv:2508.06383v1 Announce Type: cross 
Abstract: Symbolic indefinite integration in Computer Algebra Systems such as Maple involves selecting the most effective algorithm from multiple available methods. Not all methods will succeed for a given problem, and when several do, the results, though mathematically equivalent, can differ greatly in presentation complexity. Traditionally, this choice has been made with minimal consideration of the problem instance, leading to inefficiencies.
  We present a machine learning (ML) approach using tree-based deep learning models within a two-stage architecture: first identifying applicable methods for a given instance, then ranking them by predicted output complexity. Furthermore, we find representing mathematical expressions as tree structures significantly improves performance over sequence-based representations, and our two-stage framework outperforms alternative ML formulations.
  Using a diverse dataset generated by six distinct data generators, our models achieve nearly 90% accuracy in selecting the optimal method on a 70,000 example holdout test set. On an independent out-of-distribution benchmark from Maple's internal test suite, our tree transformer model maintains strong generalisation, outperforming Maple's built-in selector and prior ML approaches.
  These results highlight the critical role of data representation and problem framing in ML for symbolic computation, and we expect our methodology to generalise effectively to similar optimisation problems in mathematical software.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Enabled Federated Learning</title>
<link>https://arxiv.org/abs/2508.06406</link>
<guid>https://arxiv.org/abs/2508.06406</guid>
<content:encoded><![CDATA[
arXiv:2508.06406v1 Announce Type: cross 
Abstract: Blockchain-enabled federated learning (BCFL) addresses fundamental challenges of trust, privacy, and coordination in collaborative AI systems. This chapter provides comprehensive architectural analysis of BCFL systems through a systematic four-dimensional taxonomy examining coordination structures, consensus mechanisms, storage architectures, and trust models. We analyze design patterns from blockchain-verified centralized coordination to fully decentralized peer-to-peer networks, evaluating trade-offs in scalability, security, and performance. Through detailed examination of consensus mechanisms designed for federated learning contexts, including Proof of Quality and Proof of Federated Learning, we demonstrate how computational work can be repurposed from arbitrary cryptographic puzzles to productive machine learning tasks. The chapter addresses critical storage challenges by examining multi-tier architectures that balance blockchain's transaction constraints with neural networks' large parameter requirements while maintaining cryptographic integrity. A technical case study of the TrustMesh framework illustrates practical implementation considerations in BCFL systems through distributed image classification training, demonstrating effective collaborative learning across IoT devices with highly non-IID data distributions while maintaining complete transparency and fault tolerance. Analysis of real-world deployments across healthcare consortiums, financial services, and IoT security applications validates the practical viability of BCFL systems, achieving performance comparable to centralized approaches while providing enhanced security guarantees and enabling new models of trustless collaborative intelligence.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks</title>
<link>https://arxiv.org/abs/2508.06411</link>
<guid>https://arxiv.org/abs/2508.06411</guid>
<content:encoded><![CDATA[
arXiv:2508.06411v1 Announce Type: cross 
Abstract: Although discourse around the risks of Artificial Intelligence (AI) has grown, it often lacks a comprehensive, multidimensional framework, and concrete causal pathways mapping hazard to harm. This paper aims to bridge this gap by examining six commonly discussed AI catastrophic risks: CBRN, cyber offense, sudden loss of control, gradual loss of control, environmental risk, and geopolitical risk. First, we characterize these risks across seven key dimensions, namely intent, competency, entity, polarity, linearity, reach, and order. Next, we conduct risk pathway modeling by mapping step-by-step progressions from the initial hazard to the resulting harms. The dimensional approach supports systematic risk identification and generalizable mitigation strategies, while risk pathway models help identify scenario-specific interventions. Together, these methods offer a more structured and actionable foundation for managing catastrophic AI risks across the value chain.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memp: Exploring Agent Procedural Memory</title>
<link>https://arxiv.org/abs/2508.06433</link>
<guid>https://arxiv.org/abs/2508.06433</guid>
<content:encoded><![CDATA[
arXiv:2508.06433v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion</title>
<link>https://arxiv.org/abs/2508.06450</link>
<guid>https://arxiv.org/abs/2508.06450</guid>
<content:encoded><![CDATA[
arXiv:2508.06450v1 Announce Type: cross 
Abstract: Since their introduction, Transformer-based models, such as SASRec and BERT4Rec, have become common baselines for sequential recommendations, surpassing earlier neural and non-neural methods. A number of following publications have shown that the effectiveness of these models can be improved by, for example, slightly updating the architecture of the Transformer layers, using better training objectives, and employing improved loss functions. However, the additivity of these modular improvements has not been systematically benchmarked - this is the gap we aim to close in this paper. Through our experiments, we identify a very strong model that uses SASRec's training objective, LiGR Transformer layers, and Sampled Softmax Loss. We call this combination eSASRec (Enhanced SASRec). While we primarily focus on realistic, production-like evaluation, in our preliminarily study we find that common academic benchmarks show eSASRec to be 23% more effective compared to the most recent state-of-the-art models, such as ActionPiece. In our main production-like benchmark, eSASRec resides on the Pareto frontier in terms of the accuracy-coverage tradeoff (alongside the recent industrial models HSTU and FuXi. As the modifications compared to the original SASRec are relatively straightforward and no extra features are needed (such as timestamps in HSTU), we believe that eSASRec can be easily integrated into existing recommendation pipelines and can can serve as a strong yet very simple baseline for emerging complicated algorithms. To facilitate this, we provide the open-source implementations for our models and benchmarks in repository https://github.com/blondered/transformer_benchmark
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2508.06452</link>
<guid>https://arxiv.org/abs/2508.06452</guid>
<content:encoded><![CDATA[
arXiv:2508.06452v1 Announce Type: cross 
Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success in addressing classical domain shifts (e.g., synthetic-to-real), but they still suffer under complex shifts (e.g. geographical shift), where both the background and object appearances differ significantly across domains. Prior works showed that the language modality can help in the adaptation process, exhibiting more robustness to such complex shifts. In this paper, we introduce TRUST, a novel UDA approach that exploits the robustness of the language modality to guide the adaptation of a vision model. TRUST generates pseudo-labels for target samples from their captions and introduces a novel uncertainty estimation strategy that uses normalised CLIP similarity scores to estimate the uncertainty of the generated pseudo-labels. Such estimated uncertainty is then used to reweight the classification loss, mitigating the adverse effects of wrong pseudo-labels obtained from low-quality captions. To further increase the robustness of the vision model, we propose a multimodal soft-contrastive learning loss that aligns the vision and language feature spaces, by leveraging captions to guide the contrastive training of the vision model on target images. In our contrastive loss, each pair of images acts as both a positive and a negative pair and their feature representations are attracted and repulsed with a strength proportional to the similarity of their captions. This solution avoids the need for hardly determining positive and negative pairs, which is critical in the UDA setting. Our approach outperforms previous methods, setting the new state-of-the-art on classical (DomainNet) and complex (GeoNet) domain shifts. The code will be available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Impact with Fewer Features: Efficient Feature Selection for Cold-Start Recommenders through Collaborative Importance Weighting</title>
<link>https://arxiv.org/abs/2508.06455</link>
<guid>https://arxiv.org/abs/2508.06455</guid>
<content:encoded><![CDATA[
arXiv:2508.06455v1 Announce Type: cross 
Abstract: Cold-start challenges in recommender systems necessitate leveraging auxiliary features beyond user-item interactions. However, the presence of irrelevant or noisy features can degrade predictive performance, whereas an excessive number of features increases computational demands, leading to higher memory consumption and prolonged training times.
  To address this, we propose a feature selection strategy that prioritizes the user behavioral information. Our method enhances the feature representation by incorporating correlations from collaborative behavior data using a hybrid matrix factorization technique and then ranks features using a mechanism based on the maximum volume algorithm. This approach identifies the most influential features, striking a balance between recommendation accuracy and computational efficiency. We conduct an extensive evaluation across various datasets and hybrid recommendation models, demonstrating that our method excels in cold-start scenarios by selecting minimal yet highly effective feature subsets. Even under strict feature reduction, our approach surpasses existing feature selection techniques while maintaining superior efficiency.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intuition emerges in Maximum Caliber models at criticality</title>
<link>https://arxiv.org/abs/2508.06477</link>
<guid>https://arxiv.org/abs/2508.06477</guid>
<content:encoded><![CDATA[
arXiv:2508.06477v1 Announce Type: cross 
Abstract: Whether large predictive models merely parrot their training data or produce genuine insight lacks a physical explanation. This work reports a primitive form of intuition that emerges as a metastable phase of learning that critically balances next-token prediction against future path-entropy. The intuition mechanism is discovered via mind-tuning, the minimal principle that imposes Maximum Caliber in predictive models with a control temperature-like parameter $\lambda$. Training on random walks in deterministic mazes reveals a rich phase diagram: imitation (low $\lambda$), rule-breaking hallucination (high $\lambda$), and a fragile in-between window exhibiting strong protocol-dependence (hysteresis) and multistability, where models spontaneously discover novel goal-directed strategies. These results are captured by an effective low-dimensional theory and frame intuition as an emergent property at the critical balance between memorizing what is and wondering what could be.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-training for Efficient Communication via Convention Formation</title>
<link>https://arxiv.org/abs/2508.06482</link>
<guid>https://arxiv.org/abs/2508.06482</guid>
<content:encoded><![CDATA[
arXiv:2508.06482v1 Announce Type: cross 
Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by adapting their language and forming ad-hoc conventions. In contrast, prior work shows that LLMs do not naturally show this behavior. We develop a post-training process to develop this ability through targeted fine-tuning on heuristically identified demonstrations of convention formation. We evaluate with two new benchmarks focused on this capability. First, we design a focused, cognitively-motivated interaction benchmark that consistently elicits strong convention formation trends in humans. Second, we create a new document-grounded reference completion task that reflects in-the-wild convention formation behavior. Our studies show significantly improved convention formation abilities in post-trained LLMs across the two evaluation methods.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</title>
<link>https://arxiv.org/abs/2508.06485</link>
<guid>https://arxiv.org/abs/2508.06485</guid>
<content:encoded><![CDATA[
arXiv:2508.06485v1 Announce Type: cross 
Abstract: Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multivariate Fields of Experts</title>
<link>https://arxiv.org/abs/2508.06490</link>
<guid>https://arxiv.org/abs/2508.06490</guid>
<content:encoded><![CDATA[
arXiv:2508.06490v1 Announce Type: cross 
Abstract: We introduce the multivariate fields of experts, a new framework for the learning of image priors. Our model generalizes existing fields of experts methods by incorporating multivariate potential functions constructed via Moreau envelopes of the $\ell_\infty$-norm. We demonstrate the effectiveness of our proposal across a range of inverse problems that include image denoising, deblurring, compressed-sensing magnetic-resonance imaging, and computed tomography. The proposed approach outperforms comparable univariate models and achieves performance close to that of deep-learning-based regularizers while being significantly faster, requiring fewer parameters, and being trained on substantially fewer data. In addition, our model retains a relatively high level of interpretability due to its structured design.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From research to clinic: Accelerating the translation of clinical decision support systems by making synthetic data interoperable</title>
<link>https://arxiv.org/abs/2308.02613</link>
<guid>https://arxiv.org/abs/2308.02613</guid>
<content:encoded><![CDATA[
arXiv:2308.02613v2 Announce Type: replace 
Abstract: The translation of clinical decision support system (CDSS) tools from research settings into the clinic is often non-existent, partly because the focus tends to be on training machine learning models rather than tool development using the model for inference. To develop a CDSS tool that can be deployed in the clinical workflow, there is a need to integrate, validate, and test the tool on the Electronic Health Record (EHR) systems that store and manage patient data. Not surprisingly, it is rarely possible for researchers to get the necessary access to an EHR system due to legal restrictions pertaining to the protection of data privacy in patient records. We propose an architecture for using synthetic data in EHR systems to make CDSS tool development and testing much easier. In this study, the architecture is implemented in the SyntHIR system. SyntHIR has three noteworthy architectural features enabling (i) integration with synthetic data generators, (ii) data interoperability, and (iii) tool transportability. The translational value of this approach was evaluated through two primary steps. First, a working proof-of-concept of a machine learning-based CDSS tool was developed using data from patient registries in Norway. Second, the transportability of this CDSS tool was demonstrated by successfully deploying it in Norway's largest EHR system vendor (DIPS). These findings showcase the value of the SyntHIR architecture as a useful reference model to accelerate the translation of "bench to bedside" research of CDSS tools.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Based Sensor Optimization for Bio-markers</title>
<link>https://arxiv.org/abs/2308.10649</link>
<guid>https://arxiv.org/abs/2308.10649</guid>
<content:encoded><![CDATA[
arXiv:2308.10649v2 Announce Type: replace 
Abstract: Radio frequency (RF) biosensors, in particular those based on inter-digitated capacitors (IDCs), are pivotal in areas like biomedical diagnosis, remote sensing, and wireless communication. Despite their advantages of low cost and easy fabrication, their sensitivity can be hindered by design imperfections, environmental factors, and circuit noise. This paper investigates enhancing the sensitivity of IDC-based RF sensors using novel reinforcement learning based Binary Particle Swarm Optimization (RLBPSO), and it is compared to Ant Colony Optimization (ACO), and other state-of-the-art methods. By focusing on optimizing design parameters like electrode design and finger width, the proposed study found notable improvements in sensor sensitivity. The proposed RLBPSO method shows best optimized design for various frequency ranges when compared to current state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Markov Random Field model for Hypergraph-based Machine Learning</title>
<link>https://arxiv.org/abs/2308.14172</link>
<guid>https://arxiv.org/abs/2308.14172</guid>
<content:encoded><![CDATA[
arXiv:2308.14172v4 Announce Type: replace 
Abstract: Understanding the data-generating process is essential for building machine learning models that generalise well while ensuring robustness and interpretability. This paper addresses the fundamental challenge of modelling the data generation processes on hypergraphs and explores how such models can inform the design of machine learning algorithms for hypergraph data. The key to our approach is the development of a hypergraph Markov random field that models the joint distribution of the node features and hyperedge features in a hypergraph through a multivariate Gaussian distribution whose covariance matrix is uniquely determined by the hypergraph structure. The proposed data-generating process provides a valuable inductive bias for various hypergraph machine learning tasks, thus enhancing the algorithm design. In this paper, we focus on two representative downstream tasks: structure inference and node classification. Accordingly, we introduce two novel frameworks: 1) an original hypergraph structure inference framework named HGSI, and 2) a novel learning framework entitled Hypergraph-MLP for node classification on hypergraphs. Empirical evaluation of the proposed frameworks demonstrates that: 1) HGSI outperforms existing hypergraph structure inference methods on both synthetic and real-world data; and 2) Hypergraph-MLP outperforms baselines in six hypergraph node classification benchmarks, at the same time promoting runtime efficiency and robustness against structural perturbations during inference.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Gaussian Process ODEs via Double Normalizing Flows</title>
<link>https://arxiv.org/abs/2309.09222</link>
<guid>https://arxiv.org/abs/2309.09222</guid>
<content:encoded><![CDATA[
arXiv:2309.09222v3 Announce Type: replace 
Abstract: Recently, Gaussian processes have been used to model the vector field of continuous dynamical systems, referred to as GPODEs, which are characterized by a probabilistic ODE equation. Bayesian inference for these models has been extensively studied and applied in tasks such as time series prediction. However, the use of standard GPs with basic kernels like squared exponential kernels has been common in GPODE research, limiting the model's ability to represent complex scenarios. To address this limitation, we introduce normalizing flows to reparameterize the ODE vector field, resulting in a data-driven prior distribution, thereby increasing flexibility and expressive power. We develop a data-driven variational learning algorithm that utilizes analytically tractable probability density functions of normalizing flows, enabling simultaneous learning and inference of unknown continuous dynamics. Additionally, we also apply normalizing flows to the posterior inference of GP ODEs to resolve the issue of strong mean-field assumptions in posterior inference. By applying normalizing flows in both these ways, our model improves accuracy and uncertainty estimates for Bayesian Gaussian Process ODEs. We validate the effectiveness of our approach on simulated dynamical systems and real-world human motion data, including time series prediction and missing data recovery tasks. Experimental results show that our proposed method effectively captures model uncertainty while improving accuracy.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy Causal Graphs for Multivariate Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2312.09478</link>
<guid>https://arxiv.org/abs/2312.09478</guid>
<content:encoded><![CDATA[
arXiv:2312.09478v2 Announce Type: replace 
Abstract: Many multivariate time series anomaly detection frameworks have been proposed and widely applied. However, most of these frameworks do not consider intrinsic relationships between variables in multivariate time series data, thus ignoring the causal relationship among variables and degrading anomaly detection performance. This work proposes a novel framework called CGAD, an entropy Causal Graph for multivariate time series Anomaly Detection. CGAD utilizes transfer entropy to construct graph structures that unveil the underlying causal relationships among time series data. Weighted graph convolutional networks combined with causal convolutions are employed to model both the causal graph structures and the temporal patterns within multivariate time series data. Furthermore, CGAD applies anomaly scoring, leveraging median absolute deviation-based normalization to improve the robustness of the anomaly identification process. Extensive experiments demonstrate that CGAD outperforms state-of-the-art methods on real-world datasets with a 9% average improvement in terms of three different multivariate time series anomaly detection metrics.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision-focused predictions via pessimistic bilevel optimization: complexity and algorithms</title>
<link>https://arxiv.org/abs/2312.17640</link>
<guid>https://arxiv.org/abs/2312.17640</guid>
<content:encoded><![CDATA[
arXiv:2312.17640v5 Announce Type: replace 
Abstract: Dealing with uncertainty in optimization parameters is an important and longstanding challenge. Typically, uncertain parameters are predicted accurately, and then a deterministic optimization problem is solved. However, the decisions produced by this so-called predict-then-optimize procedure can be highly sensitive to uncertain parameters. In this work, we contribute to recent efforts in producing decision-focused predictions, i.e., to build predictive models that are constructed with the goal of minimizing a regret measure on the decisions taken with them. We begin by formulating the exact expected regret minimization as a pessimistic bilevel optimization model. Then, we show computational complexity results of this problem, including its membership in NP. In combination with a known NP-hardness result, this establishes NP-completeness and discards its hardness in higher complexity classes. Using duality arguments, we reformulate it as a non-convex quadratic optimization problem. Finally, leveraging the quadratic reformulation, we show various computational techniques to achieve empirical tractability. We report extensive computational results on shortest-path and bipartite matching instances with uncertain cost vectors. Our results indicate that our approach can improve training performance over the approach of Elmachtoub and Grigas (2022), a state-of-the-art method for decision-focused learning.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Dice Confidence: A Near-Optimal Confidence Estimator for Selective Prediction in Semantic Segmentation</title>
<link>https://arxiv.org/abs/2402.10665</link>
<guid>https://arxiv.org/abs/2402.10665</guid>
<content:encoded><![CDATA[
arXiv:2402.10665v4 Announce Type: replace 
Abstract: In semantic segmentation, even state-of-the-art deep learning models fall short of the performance required in certain high-stakes applications such as medical image analysis. In these cases, performance can be improved by allowing a model to abstain from making predictions when confidence is low, an approach known as selective prediction. While well-known in the classification literature, selective prediction has been underexplored in the context of semantic segmentation. This paper tackles the problem by focusing on image-level abstention, which involves producing a single confidence estimate for the entire image, in contrast to previous approaches that focus on pixel-level uncertainty. Assuming the Dice coefficient as the evaluation metric for segmentation, two main contributions are provided in this paper: (i) In the case of known marginal posterior probabilities, we derive the optimal confidence estimator, which is observed to be intractable for typical image sizes. Then, an approximation computable in linear time, named Soft Dice Confidence (SDC), is proposed and proven to be tightly bounded to the optimal estimator. (ii) When only an estimate of the marginal posterior probabilities are known, we propose a plug-in version of the SDC and show it outperforms all previous methods, including those requiring additional tuning data. These findings are supported by experimental results on both synthetic data and real-world data from six medical imaging tasks, including out-of-distribution scenarios, positioning the SDC as a reliable and efficient tool for selective prediction in semantic segmentation.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Collaboration Analysis with Orthonormal Basis Selection and Alignment</title>
<link>https://arxiv.org/abs/2403.02780</link>
<guid>https://arxiv.org/abs/2403.02780</guid>
<content:encoded><![CDATA[
arXiv:2403.02780v5 Announce Type: replace 
Abstract: Data Collaboration (DC) enables multiple parties to jointly train a model without exposing their private datasets. Each party privately transforms its data using a secret linear basis and shares only the resulting intermediate representations. Existing theory asserts that any target basis spanning the same subspace as the secret bases should suffice; however, empirical evidence reveals that the particular choice of target basis significantly influences model accuracy and stability. In this paper, we introduce Orthonormal Data Collaboration (ODC), a novel DC framework that explicitly enforces orthonormality constraints on both the secret and target bases. Under these constraints, the basis alignment step reduces precisely to the classical Orthogonal Procrustes Problem, admitting a closed-form solution. We rigorously establish that the resulting orthonormal change-of-basis matrices achieve orthogonal concordance, aligning all parties' intermediate representations up to a common orthogonal transformation. Consequently, downstream model performance becomes invariant to the specific choice of orthonormal target basis. Computationally, ODC substantially reduces alignment complexity from O(\min\{a,(cl)^2,a^2cl) to O(acl^2) where a denotes anchor data size, l the latent dimension, and c the number of collaborating parties. Extensive empirical evaluations confirm the theoretical advantages of ODC, demonstrating alignment speed-ups of up to two orders of magnitude compared to state-of-the-art DC methods, alongside comparable or superior accuracy across multiple benchmark datasets. ODC maintains robust privacy under the semi-honest threat model and requires only a single round of communication. These results establish ODC as a practically advantageous and computationally efficient enhancement to existing DC pipelines, particularly when orthonormal secret bases are naturally feasible.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Lifetime tuning is incompatible with continual reinforcement learning</title>
<link>https://arxiv.org/abs/2404.02113</link>
<guid>https://arxiv.org/abs/2404.02113</guid>
<content:encoded><![CDATA[
arXiv:2404.02113v4 Announce Type: replace 
Abstract: In continual RL we want agents capable of never-ending learning, and yet our evaluation methodologies do not reflect this. The standard practice in RL is to assume unfettered access to the deployment environment for the full lifetime of the agent. For example, agent designers select the best performing hyperparameters in Atari by testing each for 200 million frames and then reporting results on 200 million frames. In this position paper, we argue and demonstrate the pitfalls of this inappropriate empirical methodology: lifetime tuning. We provide empirical evidence to support our position by testing DQN and SAC across several of continuing and non-stationary environments with two main findings: (1) lifetime tuning does not allow us to identify algorithms that work well for continual learning -- all algorithms equally succeed; (2) recently developed continual RL algorithms outperform standard non-continual algorithms when tuning is limited to a fraction of the agent's lifetime. The goal of this paper is to provide an explanation for why recent progress in continual RL has been mixed and motivate the development of empirical practices that better match the goals of continual RL.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reorganizing attention-space geometry with expressive attention</title>
<link>https://arxiv.org/abs/2407.18601</link>
<guid>https://arxiv.org/abs/2407.18601</guid>
<content:encoded><![CDATA[
arXiv:2407.18601v3 Announce Type: replace 
Abstract: Attention regulates information transfer between tokens. For this, query and key vectors are compared, typically in terms of a scalar product, $\mathbf{Q}^T\mathbf{K}$, together with a subsequent softmax normalization. In geometric terms, the standard dot-product attention (DPA) leads to large/small attention weights for parallel/antiparallel queries and keys. Here we study expressive attention (EA), which is based on $(\mathbf{Q}^T\mathbf{K})^2$, the squared dot product. In this case, attention is enhanced when query and key are either parallel or antiparallel, and suppressed for orthogonal configurations. EA can be introduced into any attention-based code without additional compute costs or memory requirements. For a series of autoregressive prediction tasks, we find that expressive attention performs at least as well as vanilla DPA. Increasing task complexity, EA is observed to outperform DPA with increasing margins, which also holds for multi-task settings. For a given model size, EA manages to achieve 100% performance for a range of complexity levels not accessible to DPA. Our results show that it is possible to reorganize the geometry of the matching condition in the space of attention heads without loss of performance.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatio-Temporal Partial Sensing Forecast for Long-term Traffic</title>
<link>https://arxiv.org/abs/2408.02689</link>
<guid>https://arxiv.org/abs/2408.02689</guid>
<content:encoded><![CDATA[
arXiv:2408.02689v2 Announce Type: replace 
Abstract: Traffic forecasting uses recent measurements by sensors installed at chosen locations to forecast the future road traffic. Existing work either assumes all locations are equipped with sensors or focuses on short-term forecast. This paper studies partial sensing forecast of long-term traffic, assuming sensors are available only at some locations. The problem is challenging due to the unknown data distribution at unsensed locations, the intricate spatio-temporal correlation in long-term forecasting, as well as noise to traffic patterns. We propose a Spatio-temporal Long-term Partial sensing Forecast model (SLPF) for traffic prediction, with several novel contributions, including a rank-based embedding technique to reduce the impact of noise in data, a spatial transfer matrix to overcome the spatial distribution shift from sensed locations to unsensed locations, and a multi-step training process that utilizes all available data to successively refine the model parameters for better accuracy. Extensive experiments on several real-world traffic datasets demonstrate its superior performance. Our source code is at https://github.com/zbliu98/SLPF
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formal Local Implication Between Two Neural Networks</title>
<link>https://arxiv.org/abs/2409.16726</link>
<guid>https://arxiv.org/abs/2409.16726</guid>
<content:encoded><![CDATA[
arXiv:2409.16726v2 Announce Type: replace 
Abstract: Given two neural network classifiers with the same input and output domains, our goal is to compare the two networks in relation to each other over an entire input region (e.g., within a vicinity of an input sample). To this end, we establish the foundation of formal local implication between two networks, i.e., N2 implies N1, in an entire input region D. That is, network N1 consistently makes a correct decision every time network N2 does, and it does so in an entire input region D. We further propose a sound formulation for establishing such formally-verified (provably correct) local implications. The proposed formulation is relevant in the context of several application domains, e.g., for comparing a trained network and its corresponding compact (e.g., pruned, quantized, distilled) networks. We evaluate our formulation based on the MNIST, CIFAR10, and two real-world medical datasets, to show its relevance.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATM: Improving Model Merging by Alternating Tuning and Merging</title>
<link>https://arxiv.org/abs/2411.03055</link>
<guid>https://arxiv.org/abs/2411.03055</guid>
<content:encoded><![CDATA[
arXiv:2411.03055v4 Announce Type: replace 
Abstract: Model merging has emerged as a cost-efficient approximation to multitask learning. Among merging strategies, task arithmetic is notable for its simplicity and effectiveness. In this work, we provide a theoretical motivation for task vectors by highlighting that, under single-epoch full-batch gradient descent, they are equivalent to multitask gradients. This insight leads us to reinterpret model merging as a single step in an iterative procedure that Alternates between Tuning and Merging (ATM). We propose two applications of ATM: (1) as an alternative to multitask learning in scenarios where data sharing is restricted (e.g., federated settings), and (2) as a lightweight refinement step to improve existing model merging methods using a small validation set. Experiments across diverse vision tasks demonstrate the effectiveness of ATM.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconsidering the Performance of GAE in Link Prediction</title>
<link>https://arxiv.org/abs/2411.03845</link>
<guid>https://arxiv.org/abs/2411.03845</guid>
<content:encoded><![CDATA[
arXiv:2411.03845v3 Announce Type: replace 
Abstract: Recent advancements in graph neural networks (GNNs) for link prediction have introduced sophisticated training techniques and model architectures. However, reliance on outdated baselines may exaggerate the benefits of these new approaches. To tackle this issue, we systematically explore Graph Autoencoders (GAEs) by applying model-agnostic tricks in recent methods and tuning hyperparameters. We find that a well-tuned GAE can match the performance of recent sophisticated models while offering superior computational efficiency on widely-used link prediction benchmarks. Our approach delivers substantial performance gains on datasets where structural information dominates and feature data is limited. Specifically, our GAE achieves a state-of-the-art Hits@100 score of 78.41\% on the ogbl-ppa dataset. Furthermore, we examine the impact of various tricks to uncover the reasons behind our success and to guide the design of future methods. Our study emphasizes the critical need to update baselines for a more accurate assessment of progress in GNNs for link prediction. Our code is available at https://github.com/GraphPKU/Refined-GAE.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Collocation Point Strategies For Physics Informed Neural Networks via the QR Discrete Empirical Interpolation Method</title>
<link>https://arxiv.org/abs/2501.07700</link>
<guid>https://arxiv.org/abs/2501.07700</guid>
<content:encoded><![CDATA[
arXiv:2501.07700v4 Announce Type: replace 
Abstract: Physics-informed neural networks (PINNs) have gained significant attention for solving forward and inverse problems related to partial differential equations (PDEs). While advancements in loss functions and network architectures have improved PINN accuracy, the impact of collocation point sampling on their performance remains underexplored. Fixed sampling methods, such as uniform random sampling and equispaced grids, can fail to capture critical regions with high solution gradients, limiting their effectiveness for complex PDEs. Adaptive methods, inspired by adaptive mesh refinement from traditional numerical methods, address this by dynamically updating collocation points during training but may overlook residual dynamics between updates, potentially losing valuable information. To overcome this limitation, we propose two adaptive collocation point selection strategies utilizing the QR Discrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling technique for efficiently approximating nonlinear functions. Our results on benchmark PDEs demonstrate that our QR-DEIM-based approaches improve PINN accuracy compared to existing methods, offering a promising direction for adaptive collocation point strategies.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systemizing Multiplicity: The Curious Case of Arbitrariness in Machine Learning</title>
<link>https://arxiv.org/abs/2501.14959</link>
<guid>https://arxiv.org/abs/2501.14959</guid>
<content:encoded><![CDATA[
arXiv:2501.14959v2 Announce Type: replace 
Abstract: Algorithmic modeling relies on limited information in data to extrapolate outcomes for unseen scenarios, often embedding an element of arbitrariness in its decisions. A perspective on this arbitrariness that has recently gained interest is multiplicity-the study of arbitrariness across a set of "good models", i.e., those likely to be deployed in practice. In this work, we systemize the literature on multiplicity by: (a) formalizing the terminology around model design choices and their contribution to arbitrariness, (b) expanding the definition of multiplicity to incorporate underrepresented forms beyond just predictions and explanations, (c) clarifying the distinction between multiplicity and other lenses of arbitrariness, i.e., uncertainty and variance, and (d) distilling the benefits and potential risks of multiplicity into overarching trends, situating it within the broader landscape of responsible AI. We conclude by identifying open research questions and highlighting emerging trends in this young but rapidly growing area of research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Bias of Foundation Model under Long-tailed Distribution</title>
<link>https://arxiv.org/abs/2501.15955</link>
<guid>https://arxiv.org/abs/2501.15955</guid>
<content:encoded><![CDATA[
arXiv:2501.15955v3 Announce Type: replace 
Abstract: Long-tailed learning has garnered increasing attention due to its practical significance. Among the various approaches, the fine-tuning paradigm has gained considerable interest with the advent of foundation models. However, most existing methods primarily focus on leveraging knowledge from these models, overlooking the inherent biases introduced by the imbalanced training data they rely on. In this paper, we examine how such imbalances from pre-training affect long-tailed downstream tasks. Specifically, we find the imbalance biases inherited in foundation models on downstream task as parameter imbalance and data imbalance. During fine-tuning, we observe that parameter imbalance plays a more critical role, while data imbalance can be mitigated using existing re-balancing strategies. Moreover, we find that parameter imbalance cannot be effectively addressed by current re-balancing techniques, such as adjusting the logits, during training, unlike data imbalance. To tackle both imbalances simultaneously, we build our method on causal learning and view the incomplete semantic factor as the confounder, which brings spurious correlations between input samples and labels. To resolve the negative effects of this, we propose a novel backdoor adjustment method that learns the true causal effect between input samples and labels, rather than merely fitting the correlations in the data. Notably, we achieve an average performance increase of about $1.67\%$ on each dataset. Code is available: https://github.com/JiahaoChen1/Pre-train-Imbalance
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextually Entangled Gradient Mapping for Optimized LLM Comprehension</title>
<link>https://arxiv.org/abs/2502.00048</link>
<guid>https://arxiv.org/abs/2502.00048</guid>
<content:encoded><![CDATA[
arXiv:2502.00048v2 Announce Type: replace 
Abstract: Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to gradient optimization, redefining the relationship between contextual embeddings and gradient updates to enhance semantic coherence and reasoning capabilities in neural architectures. By treating gradients as dynamic carriers of contextual dependencies rather than isolated numerical entities, the proposed methodology bridges critical gaps in existing optimization strategies. The integration of entangled gradient dynamics into a loss regularization framework demonstrated significant improvements in tasks involving long-form reasoning, contextual retention, and adaptability to unseen domains. Experimental evaluations showed that the CEGM-enhanced model consistently outperformed baseline approaches, achieving higher accuracy in token-level predictions and greater resilience to noisy inputs. Practical implementations involved modifications to training pipelines, introducing entanglement layers and dynamic coefficient adjustments that seamlessly align with existing architectures. Results further highlighted reductions in semantic drift during sequential transformations and improvements in embedding coherence across paraphrased sentences, showing the robustness and versatility of the proposed methodology. The findings demonstrate the broader implications of gradient entanglement for both theoretical advancements and practical applications in optimization strategies.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ensemble Kalman Update is an Empirical Matheron Update</title>
<link>https://arxiv.org/abs/2502.03048</link>
<guid>https://arxiv.org/abs/2502.03048</guid>
<content:encoded><![CDATA[
arXiv:2502.03048v3 Announce Type: replace 
Abstract: The Ensemble Kalman Filter (EnKF) is a widely used method for data assimilation in high-dimensional systems, with an ensemble update step equivalent to an empirical version of the Matheron update popular in Gaussian process regression -- a connection that links half a century of data-assimilation engineering to modern path-wise GP sampling.
  This paper provides a compact introduction to this simple but under-exploited connection, with necessary definitions accessible to all fields involved.
  Source code is available at https://github.com/danmackinlay/paper_matheron_equals_enkf .
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-Efficient Reinforcement Learning from Human Feedback via Information-Directed Sampling</title>
<link>https://arxiv.org/abs/2502.05434</link>
<guid>https://arxiv.org/abs/2502.05434</guid>
<content:encoded><![CDATA[
arXiv:2502.05434v3 Announce Type: replace 
Abstract: We study the problem of reinforcement learning from human feedback (RLHF), a critical problem in training large language models, from a theoretical perspective. Our main contribution is the design of novel sample-efficient RLHF algorithms based on information-directed sampling (IDS), an online decision-making principle inspired by information theory. Our algorithms maximize the sum of the value function and a mutual information term that encourages exploration of the unknown environment (which quantifies the information gained about the environment through observed human feedback data). To tackle the challenge of large state spaces and improve sample efficiency, we construct a simplified \emph{surrogate environment} and introduce a novel distance measure (named the \emph{$\ell_g$-distance}), enabling our IDS-based algorithm to achieve a Bayesian regret upper bound of order $O(H^{\frac{3}{2}}\sqrt{\log(K(\epsilon)) T})$, where $H$ is the episode length, $T$ is the number of episode and $K(\epsilon)$ is related to the covering number of the environment. Specializing to the tabular settings, this regret bound is of order $\tilde{O}(H^2\sqrt{SAT})$, where $S$ and $A$ are the numbers of states and actions. Finally, we propose an Approximate-IDS algorithm that is computationally more efficient while maintaining nearly the same sample efficiency. The design principle of this approximate algorithm is not only effective in RLHF settings but also applicable to the standard RL framework. Moreover, our work showcases the value of information theory in reinforcement learning and in the training of large language models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeToNATION: Decoupled Torch Network-Aware Training on Interlinked Online Nodes</title>
<link>https://arxiv.org/abs/2502.06728</link>
<guid>https://arxiv.org/abs/2502.06728</guid>
<content:encoded><![CDATA[
arXiv:2502.06728v3 Announce Type: replace 
Abstract: Training large neural network models requires extensive computational resources, often distributed across several nodes and accelerators. Recent findings suggest that it may be sufficient to only exchange the fast moving components of the gradients, while accumulating momentum locally (Decoupled Momentum, or DeMo). However, DeMo assumes that models fit on a single accelerator. We relax this assumption and introduce FlexDeMo, whereby nodes fully shard model parameters locally between different accelerators, while inter-node communication is reduced by synchronizing only fast-moving components instead of the full gradients -- resulting in a hybrid sharded data parallel training strategy. We further introduce a framework, denoted as DeToNATION, that generalizes DeMo, FlexDeMo, and other popular distributed training schemes such as DiLoCo -- introducing new variations of replication schemes and challenging choices made in DeMo. Our results across language and vision domains show that FlexDeMo attains similar validation loss as hybrid sharded data parallel training employing AdamW and full gradient synchronization, while being substantially faster. FlexDeMo is thus a promising distributed training scheme for the largest machine learning models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAST: Cross Attention based multimodal fusion of Structure and Text for materials property prediction</title>
<link>https://arxiv.org/abs/2502.06836</link>
<guid>https://arxiv.org/abs/2502.06836</guid>
<content:encoded><![CDATA[
arXiv:2502.06836v2 Announce Type: replace 
Abstract: Recent advancements in graph neural networks (GNNs) have significantly enhanced the prediction of material properties by modeling crystal structures as graphs. However, GNNs often struggle to capture global structural characteristics, such as crystal systems, limiting their predictive performance. To overcome this issue, we propose CAST, a cross-attention-based multimodal model that integrates graph representations with textual descriptions of materials, effectively preserving critical structural and compositional information. Unlike previous approaches, such as CrysMMNet and MultiMat, which rely on aggregated material-level embeddings, CAST leverages cross-attention mechanisms to combine fine-grained graph node-level and text token-level features. Additionally, we introduce a masked node prediction pretraining strategy that further enhances the alignment between node and text embeddings. Our experimental results demonstrate that CAST outperforms existing baseline models across four key material properties-formation energy, band gap, bulk modulus, and shear modulus-with average relative MAE improvements ranging from 10.2% to 35.7%. Analysis of attention maps confirms the importance of pretraining in effectively aligning multimodal representations. This study underscores the potential of multimodal learning frameworks for developing more accurate and globally informed predictive models in materials science.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating Demand Uncertainty in Container Shipping: Deep Reinforcement Learning for Enabling Adaptive and Feasible Master Stowage Planning</title>
<link>https://arxiv.org/abs/2502.12756</link>
<guid>https://arxiv.org/abs/2502.12756</guid>
<content:encoded><![CDATA[
arXiv:2502.12756v4 Announce Type: replace 
Abstract: Reinforcement learning (RL) has shown promise in solving various combinatorial optimization problems. However, conventional RL faces challenges when dealing with real-world constraints, especially when action space feasibility is explicit and dependent on the corresponding state or trajectory. In this work, we focus on using RL in container shipping, often considered the cornerstone of global trade, by dealing with the critical challenge of master stowage planning. The main objective is to maximize cargo revenue and minimize operational costs while navigating demand uncertainty and various complex operational constraints, namely vessel capacity and stability, which must be dynamically updated along the vessel's voyage. To address this problem, we implement a deep reinforcement learning framework with feasibility projection to solve the master stowage planning problem (MPP) under demand uncertainty. The experimental results show that our architecture efficiently finds adaptive, feasible solutions for this multi-stage stochastic optimization problem, outperforming traditional mixed-integer programming and RL with feasibility regularization. Our AI-driven decision-support policy enables adaptive and feasible planning under uncertainty, optimizing operational efficiency and capacity utilization while contributing to sustainable and resilient global supply chains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACTIVA: Amortized Causal Effect Estimation via Transformer-based Variational Autoencoder</title>
<link>https://arxiv.org/abs/2503.01290</link>
<guid>https://arxiv.org/abs/2503.01290</guid>
<content:encoded><![CDATA[
arXiv:2503.01290v2 Announce Type: replace 
Abstract: Predicting the distribution of outcomes under hypothetical interventions is crucial across healthcare, economics, and policy-making. However, existing methods often require restrictive assumptions, and are typically limited by the lack of amortization across problem instances. We propose ACTIVA, a transformer-based conditional variational autoencoder (VAE) architecture for amortized causal inference, which estimates interventional distributions directly from observational data without. ACTIVA learns a latent representation conditioned on observational inputs and intervention queries, enabling zero-shot inference by amortizing causal knowledge from diverse training scenarios. We provide theoretical insights showing that ACTIVA predicts interventional distributions as mixtures over observationally equivalent causal models. Empirical evaluations on synthetic and semi-synthetic datasets confirm the effectiveness of our amortized approach and highlight promising directions for future real-world applications.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global graph features unveiled by unsupervised geometric deep learning</title>
<link>https://arxiv.org/abs/2503.05560</link>
<guid>https://arxiv.org/abs/2503.05560</guid>
<content:encoded><![CDATA[
arXiv:2503.05560v2 Announce Type: replace 
Abstract: Graphs provide a powerful framework for modeling complex systems, but their structural variability poses significant challenges for analysis and classification. To address these challenges, we introduce GAUDI (Graph Autoencoder Uncovering Descriptive Information), a novel unsupervised geometric deep learning framework designed to capture both local details and global structure. GAUDI employs an innovative hourglass architecture with hierarchical pooling and upsampling layers linked through skip connections, which preserve essential connectivity information throughout the encoding-decoding process. Even though identical or highly similar underlying parameters describing a system's state can lead to significant variability in graph realizations, GAUDI consistently maps them into nearby regions of a structured and continuous latent space, effectively disentangling invariant process-level features from stochastic noise. We demonstrate GAUDI's versatility across multiple applications, including small-world networks modeling, characterization of protein assemblies from super-resolution microscopy, analysis of collective motion in the Vicsek model, and identification of age-related changes in brain connectivity. Comparison with related approaches highlights GAUDI's superior performance in analyzing complex graphs, providing new insights into emergent phenomena across diverse scientific domains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Match Unpaired Data with Minimum Entropy Coupling</title>
<link>https://arxiv.org/abs/2503.08501</link>
<guid>https://arxiv.org/abs/2503.08501</guid>
<content:encoded><![CDATA[
arXiv:2503.08501v2 Announce Type: replace 
Abstract: Multimodal data is a precious asset enabling a variety of downstream tasks in machine learning. However, real-world data collected across different modalities is often not paired, which is a significant challenge to learn a joint distribution. A prominent approach to address the modality coupling problem is Minimum Entropy Coupling (MEC), which seeks to minimize the joint Entropy, while satisfying constraints on the marginals. Existing approaches to the MEC problem focus on finite, discrete distributions, limiting their application for cases involving continuous data. In this work, we propose a novel method to solve the continuous MEC problem, using well-known generative diffusion models that learn to approximate and minimize the joint Entropy through a cooperative scheme, while satisfying a relaxed version of the marginal constraints. We empirically demonstrate that our method, DDMEC, is general and can be easily used to address challenging tasks, including unsupervised single-cell multi-omics data alignment and unpaired image translation, outperforming specialized methods.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Plug-n-Play Knowledge Modules with Deep Context Distillation</title>
<link>https://arxiv.org/abs/2503.08727</link>
<guid>https://arxiv.org/abs/2503.08727</guid>
<content:encoded><![CDATA[
arXiv:2503.08727v4 Announce Type: replace 
Abstract: Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and RAG.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality</title>
<link>https://arxiv.org/abs/2503.24277</link>
<guid>https://arxiv.org/abs/2503.24277</guid>
<content:encoded><![CDATA[
arXiv:2503.24277v2 Announce Type: replace 
Abstract: Sparse autoencoders (SAEs) are widely used in mechanistic interpretability research for large language models; however, the state-of-the-art method of using $k$-sparse autoencoders lacks a theoretical grounding for selecting the hyperparameter $k$ that represents the number of nonzero activations, often denoted by $\ell_0$. In this paper, we reveal a theoretical link that the $\ell_2$-norm of the sparse feature vector can be approximated with the $\ell_2$-norm of the dense vector with a closed-form error, which allows sparse autoencoders to be trained without the need to manually determine $\ell_0$. Specifically, we validate two applications of our theoretical findings. First, we introduce a new methodology that can assess the feature activations of pre-trained SAEs by computing the theoretically expected value from the input embedding, which has been overlooked by existing SAE evaluation methods and loss functions. Second, we introduce a novel activation function, top-AFA, which builds upon our formulation of approximate feature activation (AFA). This function enables top-$k$ style activation without requiring a constant hyperparameter $k$ to be tuned, dynamically determining the number of activated features for each input. By training SAEs on three intermediate layers to reconstruct GPT2 hidden embeddings for over 80 million tokens from the OpenWebText dataset, we demonstrate the empirical merits of this approach and compare it with current state-of-the-art $k$-sparse autoencoders. Our code is available at: https://github.com/SewoongLee/top-afa-sae.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining</title>
<link>https://arxiv.org/abs/2504.07912</link>
<guid>https://arxiv.org/abs/2504.07912</guid>
<content:encoded><![CDATA[
arXiv:2504.07912v2 Announce Type: replace 
Abstract: Reinforcement learning (RL)-based fine-tuning has become a crucial step in post-training language models for advanced mathematical reasoning and coding. Following the success of frontier reasoning models, recent work has demonstrated that RL fine-tuning consistently improves performance, even in smaller-scale models; however, the underlying mechanisms driving these improvements are not well-understood. Understanding the effects of RL fine-tuning requires disentangling its interaction with pretraining data composition, hyperparameters, and model scale, but such problems are exacerbated by the lack of transparency regarding the training data used in many existing models. In this work, we present a systematic end-to-end study of RL fine-tuning for mathematical reasoning by training models entirely from scratch on different mixtures of fully open datasets. We investigate the effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration) across models of different scales. Our study reveals that RL algorithms consistently converge towards a dominant output distribution, amplifying patterns in the pretraining data. We also find that models of different scales trained on the same data mixture will converge to distinct output distributions, suggesting that there are scale-dependent biases in model generalization. Moreover, we find that RL post-training on simpler questions can lead to performance gains on harder ones, indicating that certain reasoning capabilities generalize across tasks. Our findings show that small-scale proxies in controlled settings can elicit interesting insights regarding the role of RL in shaping language model behavior.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Value of Cross-Modal Misalignment in Multimodal Representation Learning</title>
<link>https://arxiv.org/abs/2504.10143</link>
<guid>https://arxiv.org/abs/2504.10143</guid>
<content:encoded><![CDATA[
arXiv:2504.10143v5 Announce Type: replace 
Abstract: Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit cross-modal misalignment. There are two distinct viewpoints on how to address this issue: one suggests mitigating the misalignment, and the other leveraging it. We seek here to reconcile these seemingly opposing perspectives, and to provide a practical guide for practitioners. Using latent variable models we thus formalize cross-modal misalignment by introducing two specific mechanisms: Selection bias, where some semantic variables are absent in the text, and perturbation bias, where semantic variables are altered -- both leading to misalignment in data pairs. Our theoretical analysis demonstrates that, under mild assumptions, the representations learned by MMCL capture exactly the information related to the subset of the semantic variables invariant to selection and perturbation biases. This provides a unified perspective for understanding misalignment. Based on this, we further offer actionable insights into how misalignment should inform the design of real-world ML systems. We validate our theoretical findings via extensive empirical studies on both synthetic data and real image-text datasets, shedding light on the nuanced impact of cross-modal misalignment on multimodal representation learning.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network</title>
<link>https://arxiv.org/abs/2504.16432</link>
<guid>https://arxiv.org/abs/2504.16432</guid>
<content:encoded><![CDATA[
arXiv:2504.16432v2 Announce Type: replace 
Abstract: As time evolves, data within specific domains exhibit predictability that motivates time series forecasting to predict future trends from historical data. However, current deep forecasting methods can achieve promising performance but generally lack interpretability, hindering trustworthiness and practical deployment in safety-critical applications such as auto-driving and healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for credible time series forecasting. iTFKAN enables further exploration of model decision rationales and underlying data patterns due to its interpretability achieved through model symbolization. Besides, iTFKAN develops two strategies, prior knowledge injection, and time-frequency synergy learning, to effectively guide model learning under complex intertwined time series data. Extensive experimental results demonstrated that iTFKAN can achieve promising forecasting performance while simultaneously possessing high interpretive capabilities.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression</title>
<link>https://arxiv.org/abs/2504.21152</link>
<guid>https://arxiv.org/abs/2504.21152</guid>
<content:encoded><![CDATA[
arXiv:2504.21152v2 Announce Type: replace 
Abstract: Imbalanced regression refers to prediction tasks where the target variable is skewed. This skewness hinders machine learning models, especially neural networks, which concentrate on dense regions and therefore perform poorly on underrepresented (minority) samples. Despite the importance of this problem, only a few methods have been proposed for imbalanced regression. Many of the available solutions for imbalanced regression adapt techniques from the class imbalance domain, such as linear interpolation and the addition of Gaussian noise, to create synthetic data in sparse regions. However, in many cases, the underlying distribution of the data is complex and non-linear. Consequently, these approaches generate synthetic samples that do not accurately represent the true feature-target relationship. To overcome these limitations, we propose SMOGAN, a two-step oversampling framework for imbalanced regression. In Stage 1, an existing oversampler generates initial synthetic samples in sparse target regions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves as SMOGAN's filtering layer and refines these samples via adversarial loss augmented with a Maximum Mean Discrepancy objective, aligning them with the true joint feature-target distribution. Extensive experiments on 23 imbalanced datasets show that SMOGAN consistently outperforms the default oversampling method without the DistGAN filtering layer.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Khan-GCL: Kolmogorov-Arnold Network Based Graph Contrastive Learning with Hard Negatives</title>
<link>https://arxiv.org/abs/2505.15103</link>
<guid>https://arxiv.org/abs/2505.15103</guid>
<content:encoded><![CDATA[
arXiv:2505.15103v2 Announce Type: replace 
Abstract: Graph contrastive learning (GCL) has demonstrated great promise for learning generalizable graph representations from unlabeled data. However, conventional GCL approaches face two critical limitations: (1) the restricted expressive capacity of multilayer perceptron (MLP) based encoders, and (2) suboptimal negative samples that either from random augmentations-failing to provide effective 'hard negatives'-or generated hard negatives without addressing the semantic distinctions crucial for discriminating graph data. To this end, we propose Khan-GCL, a novel framework that integrates the Kolmogorov-Arnold Network (KAN) into the GCL encoder architecture, substantially enhancing its representational capacity. Furthermore, we exploit the rich information embedded within KAN coefficient parameters to develop two novel critical feature identification techniques that enable the generation of semantically meaningful hard negative samples for each graph representation. These strategically constructed hard negatives guide the encoder to learn more discriminative features by emphasizing critical semantic differences between graphs. Extensive experiments demonstrate that our approach achieves state-of-the-art performance compared to existing GCL methods across a variety of datasets and tasks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerificAgent: Domain-Specific Memory Verification for Scalable Oversight of Aligned Computer-Use Agents</title>
<link>https://arxiv.org/abs/2506.02539</link>
<guid>https://arxiv.org/abs/2506.02539</guid>
<content:encoded><![CDATA[
arXiv:2506.02539v3 Announce Type: replace 
Abstract: Continual memory augmentation lets computer-using agents (CUAs) learn from prior interactions, but unvetted memories can encode domain-inappropriate or unsafe heuristics--spurious rules that drift from user intent and safety constraints. We introduce VerificAgent, a scalable oversight framework that treats persistent memory as an explicit alignment surface. VerificAgent combines (1) an expert-curated seed of domain knowledge, (2) iterative, trajectory-based memory growth during training, and (3) a post-hoc human fact-checking pass to sanitize accumulated memories before deployment. Evaluated on OSWorld productivity tasks and additional adversarial stress tests, VerificAgent improves task reliability, reduces hallucination-induced failures, and preserves interpretable, auditable guidance--without additional model fine-tuning. By letting humans correct high-impact errors once, the verified memory acts as a frozen safety contract that future agent actions must satisfy. Our results suggest that domain-scoped, human-verified memory offers a scalable oversight mechanism for CUAs, complementing broader alignment strategies by limiting silent policy drift and anchoring agent behavior to the norms and safety constraints of the target domain.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World</title>
<link>https://arxiv.org/abs/2506.03155</link>
<guid>https://arxiv.org/abs/2506.03155</guid>
<content:encoded><![CDATA[
arXiv:2506.03155v2 Announce Type: replace 
Abstract: The proliferation of artificial intelligence has enabled a diversity of applications that bridge the gap between digital and physical worlds. As physical environments are too complex to model through a single information acquisition approach, it is crucial to fuse multimodal data generated by different sources, such as sensors, devices, systems, and people, to solve a problem in the real world. Unfortunately, it is neither applicable nor sustainable to deploy new resources to collect original data from scratch for every problem. Thus, when data is inadequate in the domain of problem, it is vital to fuse knowledge from multimodal data that is already available in other domains. We call this cross-domain knowledge fusion. Existing research focus on fusing multimodal data in a single domain, supposing the knowledge from different datasets is intrinsically aligned; however, this assumption may not hold in the scenarios of cross-domain knowledge fusion. In this paper, we formally define the cross-domain multimodal data fusion problem, discussing its unique challenges, differences and advantages beyond data fusion in a single domain. We propose a four-layer framework, consisting of Domains, Links, Models and Data layers, answering three key questions:"what to fuse", "why can be fused", and "how to fuse". The Domains Layer selects relevant data from different domains for a given problem. The Links Layer reveals the philosophy of knowledge alignment beyond specific model structures. The Models Layer provides two knowledge fusion paradigms based on the fundamental mechanisms for processing data. The Data Layer turns data of different structures, resolutions, scales and distributions into a consistent representation that can be fed into an AI model. With this framework, we can design solutions that fuse cross-domain multimodal data effectively for solving real-world problems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientifically-Interpretable Reasoning Network (ScIReN): Discovering Hidden Relationships in the Carbon Cycle and Beyond</title>
<link>https://arxiv.org/abs/2506.14054</link>
<guid>https://arxiv.org/abs/2506.14054</guid>
<content:encoded><![CDATA[
arXiv:2506.14054v2 Announce Type: replace 
Abstract: Understanding how carbon flows through the soil is crucial for mitigating the effects of climate change. While soils have potential to sequester carbon from the atmosphere, the soil carbon cycle remains poorly understood. Scientists have developed mathematical process-based models of the soil carbon cycle based on existing knowledge, but they contain numerous unknown parameters that must be set in an ad-hoc manner, and often fit observations poorly. On the other hand, neural networks can learn patterns from data, but do not respect known scientific laws, nor can they reveal novel scientific relationships due to their black-box nature. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN leverages Kolmogorov-Arnold networks (KAN) to ensure the encoder is fully interpretable and reveals relationships between input features and latent parameters; it uses novel smoothness penalties to balance expressivity and simplicity. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge. While the process-based decoder enforces established scientific knowledge, the KAN-based encoder reveals new scientific relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.17631</link>
<guid>https://arxiv.org/abs/2506.17631</guid>
<content:encoded><![CDATA[
arXiv:2506.17631v2 Announce Type: replace 
Abstract: Time series forecasting aims to model temporal dependencies among variables for future state inference, holding significant importance and widespread applications in real-world scenarios. Although deep learning-based methods have achieved remarkable progress, they still exhibit suboptimal performance in long-term forecasting and data-scarce scenarios. Recent research demonstrates that large language models (LLMs) achieve promising performance in time series forecasting. However, we find existing LLM-based methods still have shortcomings: (1) the absence of a unified paradigm for textual prompt formulation and (2) the neglect of modality discrepancies between textual prompts and time series. To address this, we propose LLM-Prompt, an LLM-based time series forecasting framework integrating multi-prompt information and cross-modal semantic alignment. Specifically, we first construct a unified textual prompt paradigm containing learnable soft prompts and textualized hard prompts. Second, to enhance LLMs' comprehensive understanding of the forecasting task, we design a semantic space embedding and cross-modal alignment module to achieve cross-modal fusion of temporal and textual information. Finally, the transformed time series from the LLMs are projected to obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3 carbon emission datasets demonstrate that LLM-Prompt is a powerful framework for time series forecasting.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information</title>
<link>https://arxiv.org/abs/2507.00038</link>
<guid>https://arxiv.org/abs/2507.00038</guid>
<content:encoded><![CDATA[
arXiv:2507.00038v3 Announce Type: replace 
Abstract: In order to increase the effectiveness of model training, data reduction is essential to data-centric Artificial Intelligence (AI). It achieves this by locating the most instructive examples in massive datasets. To increase data quality and training efficiency, the main difficulty is choosing the best examples rather than the complete datasets. In this paper, we propose an effective data reduction strategy based on Pointwise V-Information (PVI). To enable a static method, we first use PVI to quantify instance difficulty and remove instances with low difficulty. Experiments show that classifier performance is maintained with only a 0.0001% to 0.76% decline in accuracy when 10%-30% of the data is removed. Second, we train the classifiers using a progressive learning strategy on examples sorted by increasing PVI, accelerating convergence and achieving a 0.8% accuracy gain over conventional training. Our findings imply that training a classifier on the chosen optimal subset may improve model performance and increase training efficiency when combined with an efficient data reduction strategy. Furthermore, we have adapted the PVI framework, which was previously limited to English datasets, to a variety of Chinese Natural Language Processing (NLP) tasks and base models, yielding insightful results for faster training and cross-lingual data reduction.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Sufficiency Perspective for Neural Networks</title>
<link>https://arxiv.org/abs/2507.10215</link>
<guid>https://arxiv.org/abs/2507.10215</guid>
<content:encoded><![CDATA[
arXiv:2507.10215v2 Announce Type: replace 
Abstract: This paper analyzes neural networks through graph variables and statistical sufficiency. We interpret neural network layers as graph-based transformations, where neurons act as pairwise functions between inputs and learned anchor points. Within this formulation, we establish conditions under which layer outputs are sufficient for the layer inputs, that is, each layer preserves the conditional distribution of the target variable given the input variable. We explore two theoretical paths under this graph-based view. The first path assumes dense anchor points and shows that asymptotic sufficiency holds in the infinite-width limit and is preserved throughout training. The second path, more aligned with practical architectures, proves exact or approximate sufficiency in finite-width networks by assuming region-separated input distributions and constructing appropriate anchor points. This path can ensure the sufficiency property for an infinite number of layers, and provide error bounds on the optimal loss for both regression and classification tasks using standard neural networks. Our framework covers fully connected layers, general pairwise functions, ReLU and sigmoid activations, and convolutional neural networks. Overall, this work bridges statistical sufficiency, graph-theoretic representations, and deep learning, providing a new statistical understanding of neural networks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains</title>
<link>https://arxiv.org/abs/2507.17792</link>
<guid>https://arxiv.org/abs/2507.17792</guid>
<content:encoded><![CDATA[
arXiv:2507.17792v3 Announce Type: replace 
Abstract: To gain deeper insights into a complex sensor system through the lens of causality, we present common and individual causal mechanism estimation (CICME), a novel three-step approach to inferring causal mechanisms from heterogeneous data collected across multiple domains. By leveraging the principle of Causal Transfer Learning (CTL), CICME is able to reliably detect domain-invariant causal mechanisms when provided with sufficient samples. The identified common causal mechanisms are further used to guide the estimation of the remaining causal mechanisms in each domain individually. The performance of CICME is evaluated on linear Gaussian models under scenarios inspired from a manufacturing process. Building upon existing continuous optimization-based causal discovery methods, we show that CICME leverages the benefits of applying causal discovery on the pooled data and repeatedly on data from individual domains, and it even outperforms both baseline methods under certain scenarios.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RANA: Robust Active Learning for Noisy Network Alignment</title>
<link>https://arxiv.org/abs/2507.22434</link>
<guid>https://arxiv.org/abs/2507.22434</guid>
<content:encoded><![CDATA[
arXiv:2507.22434v2 Announce Type: replace 
Abstract: Network alignment has attracted widespread attention in various fields. However, most existing works mainly focus on the problem of label sparsity, while overlooking the issue of noise in network alignment, which can substantially undermine model performance. Such noise mainly includes structural noise from noisy edges and labeling noise caused by human-induced and process-driven errors. To address these problems, we propose RANA, a Robust Active learning framework for noisy Network Alignment. RANA effectively tackles both structure noise and label noise while addressing the sparsity of anchor link annotations, which can improve the robustness of network alignment models. Specifically, RANA introduces the proposed Noise-aware Selection Module and the Label Denoising Module to address structural noise and labeling noise, respectively. In the first module, we design a noise-aware maximization objective to select node pairs, incorporating a cleanliness score to address structural noise. In the second module, we propose a novel multi-source fusion denoising strategy that leverages model and twin node pairs labeling to provide more accurate labels for node pairs. Empirical results on three real-world datasets demonstrate that RANA outperforms state-of-the-art active learning-based methods in alignment accuracy. Our code is available at https://github.com/YXNan0110/RANA.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adacc: An Adaptive Framework Unifying Compression and Activation Recomputation for LLM Training</title>
<link>https://arxiv.org/abs/2508.00806</link>
<guid>https://arxiv.org/abs/2508.00806</guid>
<content:encoded><![CDATA[
arXiv:2508.00806v2 Announce Type: replace 
Abstract: Training large language models (LLMs) is often constrained by GPU memory limitations. To alleviate memory pressure, activation recomputation and data compression have been proposed as two major strategies. However, both approaches have limitations: recomputation introduces significant training overhead, while compression can lead to accuracy degradation and computational inefficiency when applied naively. In this paper, we propose Adacc, the first adaptive memory optimization framework that unifies activation recomputation and data compression to improve training efficiency for LLMs while preserving model accuracy. Unlike existing methods that apply static, rule-based strategies or rely solely on one technique, Adacc makes fine-grained, tensor-level decisions, dynamically selecting between recomputation, retention, and compression based on tensor characteristics and runtime hardware constraints.
  Adacc tackles three key challenges: (1) it introduces layer-specific compression algorithms that mitigate accuracy loss by accounting for outliers in LLM activations; (2) it employs a MILP-based scheduling policy to globally optimize memory strategies across layers; and (3) it integrates an adaptive policy evolution mechanism to update strategies during training in response to changing data distributions. Experimental results show that Adacc improves training throughput by 1.01x to 1.37x compared to state-of-the-art frameworks, while maintaining accuracy comparable to the baseline.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan</title>
<link>https://arxiv.org/abs/2304.01576</link>
<guid>https://arxiv.org/abs/2304.01576</guid>
<content:encoded><![CDATA[
arXiv:2304.01576v2 Announce Type: replace-cross 
Abstract: Accurate lung nodule segmentation is crucial for early-stage lung cancer diagnosis, as it can substantially enhance patient survival rates. Computed tomography (CT) images are widely employed for early diagnosis in lung nodule analysis. However, the heterogeneity of lung nodules, size diversity, and the complexity of the surrounding environment pose challenges for developing robust nodule segmentation methods. In this study, we propose an efficient end-to-end framework, the multi-encoder-based self-adaptive hard attention network (MESAHA-Net), for precise lung nodule segmentation in CT scans. MESAHA-Net comprises three encoding paths, an attention block, and a decoder block, facilitating the integration of three types of inputs: CT slice patches, forward and backward maximum intensity projection (MIP) images, and region of interest (ROI) masks encompassing the nodule. By employing a novel adaptive hard attention mechanism, MESAHA-Net iteratively performs slice-by-slice 2D segmentation of lung nodules, focusing on the nodule region in each slice to generate 3D volumetric segmentation of lung nodules. The proposed framework has been comprehensively evaluated on the LIDC-IDRI dataset, the largest publicly available dataset for lung nodule segmentation. The results demonstrate that our approach is highly robust for various lung nodule types, outperforming previous state-of-the-art techniques in terms of segmentation accuracy and computational complexity, rendering it suitable for real-time clinical implementation.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization Bound for Diffusion Models using Random Features</title>
<link>https://arxiv.org/abs/2310.04417</link>
<guid>https://arxiv.org/abs/2310.04417</guid>
<content:encoded><![CDATA[
arXiv:2310.04417v3 Announce Type: replace-cross 
Abstract: Diffusion probabilistic models have been successfully used to generate data from noise. However, most diffusion models are computationally expensive and difficult to interpret with a lack of theoretical justification. Random feature models on the other hand have gained popularity due to their interpretability but their application to complex machine learning tasks remains limited. In this work, we present a diffusion model-inspired deep random feature model that is interpretable and gives comparable numerical results to a fully connected neural network having the same number of trainable parameters. Specifically, we extend existing results for random features and derive generalization bounds between the distribution of sampled data and the true distribution using properties of score matching. We validate our findings by generating samples on the fashion MNIST dataset and instrumental audio data.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved DDIM Sampling with Moment Matching Gaussian Mixtures</title>
<link>https://arxiv.org/abs/2311.04938</link>
<guid>https://arxiv.org/abs/2311.04938</guid>
<content:encoded><![CDATA[
arXiv:2311.04938v4 Announce Type: replace-cross 
Abstract: We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ, class-conditional models trained on ImageNet, and text-to-image generation using Stable Diffusion v2.1 on COYO700M datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of 6.94 and IS of 207.85 with a GMM kernel compared to 10.15 and 196.73 respectively with a Gaussian kernel. Further, we derive novel SDE samplers for rectified flow matching models and experiment with the proposed approach. We see improvements using both 1-rectified flow and 2-rectified flow models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance</title>
<link>https://arxiv.org/abs/2406.09105</link>
<guid>https://arxiv.org/abs/2406.09105</guid>
<content:encoded><![CDATA[
arXiv:2406.09105v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) and Multimodal Large Language Models (MLLMs) have demonstrated outstanding performance in various general multimodal applications and have shown increasing promise in specialized domains. However, their potential in the insurance domain-characterized by diverse application scenarios and rich multimodal data-remains largely underexplored. To date, there is no systematic review of multimodal tasks, nor a benchmark specifically designed to assess the capabilities of LVLMs in insurance. This gap hinders the development of LVLMs within the insurance industry. This study systematically reviews and categorizes multimodal tasks for 4 representative types of insurance: auto, property, health, and agricultural. We introduce INS-MMBench, the first hierarchical benchmark tailored for the insurance domain. INS-MMBench encompasses 22 fundamental tasks, 12 meta-tasks and 5 scenario tasks, enabling a comprehensive and progressive assessment from basic capabilities to real-world use cases. We benchmark 11 leading LVLMs, including closed-source models such as GPT-4o and open-source models like LLaVA. Our evaluation validates the effectiveness of INS-MMBench and offers detailed insights into the strengths and limitations of current LVLMs on a variety of insurance-related multimodal tasks. We hope that INS-MMBench will accelerate the integration of LVLMs into the insurance industry and foster interdisciplinary research. Our dataset and evaluation code are available at https://github.com/FDU-INS/INS-MMBench.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards More Realistic Extraction Attacks: An Adversarial Perspective</title>
<link>https://arxiv.org/abs/2407.02596</link>
<guid>https://arxiv.org/abs/2407.02596</guid>
<content:encoded><![CDATA[
arXiv:2407.02596v3 Announce Type: replace-cross 
Abstract: Language models are prone to memorizing their training data, making them vulnerable to extraction attacks. While existing research often examines isolated setups, such as a single model or a fixed prompt, real-world adversaries have a considerably larger attack surface due to access to models across various sizes and checkpoints, and repeated prompting. In this paper, we revisit extraction attacks from an adversarial perspective -- with multi-faceted access to the underlying data. We find significant churn in extraction trends, i.e., even unintuitive changes to the prompt, or targeting smaller models and earlier checkpoints, can extract distinct information. By combining multiple attacks, our adversary doubles ($2 \times$) the extraction risks, persisting even under mitigation strategies like data deduplication. We conclude with four case studies, including detecting pre-training data, copyright violations, extracting personally identifiable information, and attacking closed-source models, showing how our more realistic adversary can outperform existing adversaries in the literature.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Spatial Extremal Dependence of Precipitation Using Distributional Neural Networks</title>
<link>https://arxiv.org/abs/2407.08668</link>
<guid>https://arxiv.org/abs/2407.08668</guid>
<content:encoded><![CDATA[
arXiv:2407.08668v2 Announce Type: replace-cross 
Abstract: In this work, we propose a simulation-based estimation approach using generative neural networks to determine dependencies of precipitation maxima and their underlying uncertainty in time and space. Within the common framework of max-stable processes for extremes under temporal and spatial dependence, our methodology allows estimating the process parameters and their respective uncertainty, but also delivers an explicit nonparametric estimate of the spatial dependence through the pairwise extremal coefficient function. We illustrate the effectiveness and robustness of our approach in a thorough finite sample study where we obtain good performance in complex settings for which closed-form likelihood estimation becomes intractable. We use the technique for studying monthly rainfall maxima in Western Germany for the period 2021-2023, which is of particular interest since it contains an extreme precipitation and consecutive flooding event in July 2021 that had a massive deadly impact. Beyond the considered setting, the presented methodology and its main generative ideas also have great potential for other applications.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal sampling for least-squares approximation</title>
<link>https://arxiv.org/abs/2409.02342</link>
<guid>https://arxiv.org/abs/2409.02342</guid>
<content:encoded><![CDATA[
arXiv:2409.02342v2 Announce Type: replace-cross 
Abstract: Least-squares approximation is one of the most important methods for recovering an unknown function from data. While in many applications the data is fixed, in many others there is substantial freedom to choose where to sample. In this paper, we review recent progress on near-optimal random sampling strategies for (weighted) least-squares approximation in arbitrary linear spaces. We introduce the Christoffel function as a key quantity in the analysis of (weighted) least-squares approximation from random samples, then show how it can be used to construct a random sampling strategy, termed Christoffel sampling, that possesses near-optimal sample complexity: namely, the number of samples scales log-linearly in the dimension of the approximation space $n$. We discuss a series of variations, extensions and further topics, and throughout highlight connections to approximation theory, machine learning, information-based complexity and numerical linear algebra. Finally, motivated by various contemporary applications, we consider a generalization of the classical setting where the samples need not be pointwise samples of a scalar-valued function, and the approximation space need not be linear. We show that, even in this significantly more general setting, suitable generalizations of Christoffel function still determine the sample complexity. Consequently, these can be used to design enhanced, Christoffel sampling strategies in a unified way for general recovery problems. This article is largely self-contained, and intended to be accessible to nonspecialists.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction</title>
<link>https://arxiv.org/abs/2410.01273</link>
<guid>https://arxiv.org/abs/2410.01273</guid>
<content:encoded><![CDATA[
arXiv:2410.01273v3 Announce Type: replace-cross 
Abstract: Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepMDV: Global Spatial Matching for Multi-depot Vehicle Routing Problems</title>
<link>https://arxiv.org/abs/2411.17080</link>
<guid>https://arxiv.org/abs/2411.17080</guid>
<content:encoded><![CDATA[
arXiv:2411.17080v3 Announce Type: replace-cross 
Abstract: The rapid growth of online retail and e-commerce has made effective and efficient Vehicle Routing Problem (VRP) solutions essential. To meet rising demand, companies are adding more depots, which changes the VRP problem to a complex optimization task of Multi-Depot VRP (MDVRP) where the routing decisions of vehicles from multiple depots are highly interdependent. The complexities render traditional VRP methods suboptimal and non-scalable for the MDVRP. In this paper, we propose a novel approach to solve MDVRP addressing these interdependencies, hence achieving more effective results. The key idea is, the MDVRP can be broken down into two core spatial tasks: assigning customers to depots and optimizing the sequence of customer visits. We adopt task-decoupling approach and propose a two-stage framework that is scalable: (i) an interdependent partitioning module that embeds spatial and tour context directly into the representation space to globally match customers to depots and assign them to tours; and (ii) an independent routing module that determines the optimal visit sequence within each tour. Extensive experiments on both synthetic and real-world datasets demonstrate that our method outperforms all baselines across varying problem sizes, including the adaptations of learning-based solutions for single-depot VRP. Its adaptability and performance make it a practical and readily deployable solution for real-world logistics challenges.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeakAgent: RL-based Red-teaming Agent for LLM Privacy Leakage</title>
<link>https://arxiv.org/abs/2412.05734</link>
<guid>https://arxiv.org/abs/2412.05734</guid>
<content:encoded><![CDATA[
arXiv:2412.05734v2 Announce Type: replace-cross 
Abstract: Recent studies have discovered that large language models (LLM) may be ``fooled'' to output private information, including training data, system prompts, and personally identifiable information, under carefully crafted adversarial prompts. Existing red-teaming approaches for privacy leakage either rely on manual efforts or focus solely on system prompt extraction, making them ineffective for severe risks of training data leakage. We propose LeakAgent, a novel black-box red-teaming framework for LLM privacy leakage. Our framework trains an open-source LLM through reinforcement learning as the attack agent to generate adversarial prompts for both training data extraction and system prompt extraction. To achieve this, we propose a novel reward function to provide effective and fine-grained rewards and design novel mechanisms to balance exploration and exploitation during learning and enhance the diversity of adversarial prompts. Through extensive evaluations, we first show that LeakAgent significantly outperforms existing rule-based approaches in training data extraction and automated methods in system prompt leakage. We also demonstrate the effectiveness of LeakAgent in extracting system prompts from real-world applications in OpenAI's GPT Store. We further demonstrate LeakAgent's effectiveness in evading the existing guardrail defense and its helpfulness in enabling better safety alignment. Finally, we validate our customized designs through a detailed ablation study. We release our code here https://github.com/rucnyz/LeakAgent.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildSAT: Learning Satellite Image Representations from Wildlife Observations</title>
<link>https://arxiv.org/abs/2412.14428</link>
<guid>https://arxiv.org/abs/2412.14428</guid>
<content:encoded><![CDATA[
arXiv:2412.14428v2 Announce Type: replace-cross 
Abstract: Species distributions encode valuable ecological and environmental information, yet their potential for guiding representation learning in remote sensing remains underexplored. We introduce WildSAT, which pairs satellite images with millions of geo-tagged wildlife observations readily-available on citizen science platforms. WildSAT employs a contrastive learning approach that jointly leverages satellite images, species occurrence maps, and textual habitat descriptions to train or fine-tune models. This approach significantly improves performance on diverse satellite image recognition tasks, outperforming both ImageNet-pretrained models and satellite-specific baselines. Additionally, by aligning visual and textual information, WildSAT enables zero-shot retrieval, allowing users to search geographic locations based on textual descriptions. WildSAT surpasses recent cross-modal learning methods, including approaches that align satellite images with ground imagery or wildlife photos, demonstrating the advantages of our approach. Finally, we analyze the impact of key design choices and highlight the broad applicability of WildSAT to remote sensing and biodiversity monitoring.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVA-S2PLoR: Decentralized Secure 2-party Logistic Regression with A Subtly Hadamard Product Protocol (Full Version)</title>
<link>https://arxiv.org/abs/2501.05223</link>
<guid>https://arxiv.org/abs/2501.05223</guid>
<content:encoded><![CDATA[
arXiv:2501.05223v3 Announce Type: replace-cross 
Abstract: The implementation of accurate nonlinear operators (e.g., sigmoid function) on heterogeneous datasets is a key challenge in privacy-preserving machine learning (PPML). Most existing frameworks approximate it through linear operations, which not only result in significant precision loss but also introduce substantial computational overhead. This paper proposes an efficient, verifiable, and accurate security 2-party logistic regression framework (EVA-S2PLoR), which achieves accurate nonlinear function computation through a subtly secure hadamard product protocol and its derived protocols. All protocols are based on a practical semi-honest security model, which is designed for decentralized privacy-preserving application scenarios that balance efficiency, precision, and security. High efficiency and precision are guaranteed by the asynchronous computation flow on floating point numbers and the few number of fixed communication rounds in the hadamard product protocol, where robust anomaly detection is promised by dimension transformation and Monte Carlo methods. EVA-S2PLoR outperforms many advanced frameworks in terms of precision, improving the performance of the sigmoid function by about 10 orders of magnitude compared to most frameworks. Moreover, EVA-S2PLoR delivers the best overall performance in secure logistic regression experiments with training time reduced by over 47.6% under WAN settings and a classification accuracy difference of only about 0.5% compared to the plaintext model.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint</title>
<link>https://arxiv.org/abs/2501.15509</link>
<guid>https://arxiv.org/abs/2501.15509</guid>
<content:encoded><![CDATA[
arXiv:2501.15509v3 Announce Type: replace-cross 
Abstract: Model fingerprinting is a widely adopted approach to safeguard the intellectual property rights of open-source models by preventing their unauthorized reuse. It is promising and convenient since it does not necessitate modifying the protected model. In this paper, we revisit existing fingerprinting methods and reveal that they are vulnerable to false claim attacks where adversaries falsely assert ownership of any third-party model. We demonstrate that this vulnerability mostly stems from their untargeted nature, where they generally compare the outputs of given samples on different models instead of the similarities to specific references. Motivated by these findings, we propose a targeted fingerprinting paradigm (i.e., FIT-Print) to counteract false claim attacks. Specifically, FIT-Print transforms the fingerprint into a targeted signature via optimization. Building on the principles of FIT-Print, we develop bit-wise and list-wise black-box model fingerprinting methods, i.e., FIT-ModelDiff and FIT-LIME, which exploit the distance between model outputs and the feature attribution of specific samples as the fingerprint, respectively. Extensive experiments on benchmark models and datasets verify the effectiveness, conferrability, and resistance to false claim attacks of our FIT-Print.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Diffusion Models are Medical Image Classifiers that Provide Explainability and Uncertainty for Free</title>
<link>https://arxiv.org/abs/2502.03687</link>
<guid>https://arxiv.org/abs/2502.03687</guid>
<content:encoded><![CDATA[
arXiv:2502.03687v2 Announce Type: replace-cross 
Abstract: Discriminative classifiers have become a foundational tool in deep learning for medical imaging, excelling at learning separable features of complex data distributions. However, these models often need careful design, augmentation, and training techniques to ensure safe and reliable deployment. Recently, diffusion models have become synonymous with generative modeling in 2D. These models showcase robustness across a range of tasks including natural image classification, where classification is performed by comparing reconstruction errors across images generated for each possible conditioning input. This work presents the first exploration of the potential of class conditional diffusion models for 2D medical image classification. First, we develop a novel majority voting scheme shown to improve the performance of medical diffusion classifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin cancer datasets demonstrate that foundation and trained-from-scratch diffusion models achieve competitive performance against SOTA discriminative classifiers without the need for explicit supervision. In addition, we show that diffusion classifiers are intrinsically explainable, and can be used to quantify the uncertainty of their predictions, increasing their trustworthiness and reliability in safety-critical, clinical contexts. Further information is available on our project page: https://faverogian.github.io/med-diffusion-classifier.github.io/.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge</title>
<link>https://arxiv.org/abs/2502.13818</link>
<guid>https://arxiv.org/abs/2502.13818</guid>
<content:encoded><![CDATA[
arXiv:2502.13818v3 Announce Type: replace-cross 
Abstract: Estimating the construction year of buildings is critical for advancing sustainability, as older structures often lack energy-efficient features. Sustainable urban planning relies on accurate building age data to reduce energy consumption and mitigate climate change. In this work, we introduce MapYourCity, a novel multi-modal benchmark dataset comprising top-view Very High Resolution (VHR) imagery, multi-spectral Earth Observation (EO) data from the Copernicus Sentinel-2 constellation, and co-localized street-view images across various European cities. Each building is labeled with its construction epoch, and the task is formulated as a seven-class classification problem covering periods from 1900 to the present. To advance research in EO generalization and multi-modal learning, we organized a community-driven data challenge in 2024, hosted by ESA $\Phi$-lab, which ran for four months and attracted wide participation.
  This paper presents the Top-4 performing models from the challenge and their evaluation results. We assess model generalization on cities excluded from training to prevent data leakage, and evaluate performance under missing modality scenarios, particularly when street-view data is unavailable. Results demonstrate that building age estimation is both feasible and effective, even in previously unseen cities and when relying solely on top-view satellite imagery (i.e. with VHR and Sentinel-2 images). The new MapYourCity dataset thus provides a valuable resource for developing scalable, real-world solutions in sustainable urban analytics.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rank1: Test-Time Compute for Reranking in Information Retrieval</title>
<link>https://arxiv.org/abs/2502.18418</link>
<guid>https://arxiv.org/abs/2502.18418</guid>
<content:encoded><![CDATA[
arXiv:2502.18418v2 Announce Type: replace-cross 
Abstract: We introduce Rank1, the first reranking model trained to take advantage of test-time compute. Rank1 demonstrates the applicability within retrieval of using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for distillation in order to rapidly improve the performance of a smaller model. We gather and open-source a dataset of more than 600,000 examples of R1 reasoning traces from queries and passages in MS MARCO. Models trained on this dataset show: (1) state-of-the-art performance on advanced reasoning and instruction following datasets; (2) work remarkably well out of distribution due to the ability to respond to user-input prompts; and (3) have explainable reasoning chains that can be given to users or RAG-based systems. Further, we demonstrate that quantized versions of these models retain strong performance while using less compute/memory. Overall, Rank1 shows that test-time compute allows for a fundamentally new type of explainable and performant reranker model for search.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis</title>
<link>https://arxiv.org/abs/2503.23145</link>
<guid>https://arxiv.org/abs/2503.23145</guid>
<content:encoded><![CDATA[
arXiv:2503.23145v2 Announce Type: replace-cross 
Abstract: Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning. Our code, data, and models are publicly available at https://github.com/Anjiang-Wei/CodeARC
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DONOD: Efficient and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning</title>
<link>https://arxiv.org/abs/2504.14810</link>
<guid>https://arxiv.org/abs/2504.14810</guid>
<content:encoded><![CDATA[
arXiv:2504.14810v2 Announce Type: replace-cross 
Abstract: Ad-hoc instruction fine-tuning of large language models (LLMs) is widely adopted for domain-specific adaptation. While domain-specific supervised fine-tuning (SFT) is effective and efficient, it often weakens cross-domain generalization and struggles with noisy training data. To address these challenges, we propose DONOD, a lightweight model-intrinsic data pruning method. Our approach evaluates data using two model-parameter-based metrics: Delta of Norm (DON), which captures the cumulative influence on model weights, and Norm of Delta (NOD), which quantifies weight instability. Moreover, by employing the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) algorithm, we effectively filter noisy, unlearnable, and generalization-harming samples without relying on auxiliary models during the SFT process. Experiments on mathematical tasks demonstrate that data selected by DONOD achieves superior fine-tuning efficiency and improved robustness against noisy data. By filtering out 70% of the whole dataset, we improve target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile, our selected data present superior cross-architecture generalization. Data pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger models (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD demonstrates comparable or superior performance while remaining dataset-agnostic, enabling broader applicability. Code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation</title>
<link>https://arxiv.org/abs/2504.15254</link>
<guid>https://arxiv.org/abs/2504.15254</guid>
<content:encoded><![CDATA[
arXiv:2504.15254v2 Announce Type: replace-cross 
Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation</title>
<link>https://arxiv.org/abs/2505.11528</link>
<guid>https://arxiv.org/abs/2505.11528</guid>
<content:encoded><![CDATA[
arXiv:2505.11528v2 Announce Type: replace-cross 
Abstract: Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states. However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations. To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling. Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based). We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images. Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results. Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9\% on the LIBERO-LONG benchmark and 20\% on the real-world scenario. Furthermore, our world model and policies achieve impressive generalizability in real-world experiments.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation</title>
<link>https://arxiv.org/abs/2505.14848</link>
<guid>https://arxiv.org/abs/2505.14848</guid>
<content:encoded><![CDATA[
arXiv:2505.14848v2 Announce Type: replace-cross 
Abstract: We present MAATS, a Multi Agent Automated Translation System that leverages the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal for error detection and refinement. MAATS employs multiple specialized AI agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency, Style, Terminology), followed by a synthesis agent that integrates the annotations to iteratively refine translations. This design contrasts with conventional single-agent methods that rely on self-correction.
  Evaluated across diverse language pairs and Large Language Models (LLMs), MAATS outperforms zero-shot and single-agent baselines with statistically significant gains in both automatic metrics and human assessments. It excels particularly in semantic accuracy, locale adaptation, and linguistically distant language pairs. Qualitative analysis highlights its strengths in multi-layered error diagnosis, omission detection across perspectives, and context-aware refinement. By aligning modular agent roles with interpretable MQM dimensions, MAATS narrows the gap between black-box LLMs and human translation workflows, shifting focus from surface fluency to deeper semantic and contextual fidelity.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Meta-SR: In-Context Learning for Evolving Selection Operators in Symbolic Regression</title>
<link>https://arxiv.org/abs/2505.18602</link>
<guid>https://arxiv.org/abs/2505.18602</guid>
<content:encoded><![CDATA[
arXiv:2505.18602v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have revolutionized algorithm development, yet their application in symbolic regression, where algorithms automatically discover symbolic expressions from data, remains constrained and is typically designed manually by human experts. In this paper, we propose a meta learning framework that enables LLMs to automatically design selection operators for evolutionary symbolic regression algorithms. We first identify two key limitations in existing LLM-based algorithm evolution techniques: a lack of semantic guidance and code bloat. The absence of semantic awareness can lead to ineffective exchange of useful code components, and bloat results in unnecessarily complex components, both of which can reduce the interpretability of the designed algorithm or hinder evolutionary learning progress. To address these issues, we enhance the LLM-based evolution framework for meta symbolic regression with two key innovations: a complementary, semantics-aware selection operator and bloat control. Additionally, we embed domain knowledge into the prompt, enabling the LLM to generate more effective and contextually relevant selection operators. Our experimental results on symbolic regression benchmarks show that LLMs can devise selection operators that outperform nine expert-designed baselines, achieving state-of-the-art performance. Moreover, the evolved operator can further improve the state-of-the-art symbolic regression algorithm, achieving the best performance among 26 symbolic regression and machine learning algorithms across 116 regression datasets. This demonstrates that LLMs can exceed expert-level algorithm design for symbolic regression.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey on the Evaluation of Generative Models in Music</title>
<link>https://arxiv.org/abs/2506.05104</link>
<guid>https://arxiv.org/abs/2506.05104</guid>
<content:encoded><![CDATA[
arXiv:2506.05104v2 Announce Type: replace-cross 
Abstract: Research on generative systems in music has seen considerable attention and growth in recent years. A variety of attempts have been made to systematically evaluate such systems.
  We present an interdisciplinary review of the common evaluation targets, methodologies, and metrics for the evaluation of both system output and model use, covering subjective and objective approaches, qualitative and quantitative approaches, as well as empirical and computational methods. We examine the benefits and limitations of these approaches from a musicological, an engineering, and an HCI perspective.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models</title>
<link>https://arxiv.org/abs/2506.09082</link>
<guid>https://arxiv.org/abs/2506.09082</guid>
<content:encoded><![CDATA[
arXiv:2506.09082v2 Announce Type: replace-cross 
Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation. A common approach pairs VFMs with large language models (LLMs) as general-purpose heads, followed by evaluation on broad Visual Question Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i) the instruction tuning data may not align with VQA test distributions, meaning a wrong prediction can stem from such data mismatch rather than a VFM' visual shortcomings; (ii) VQA benchmarks often require multiple visual abilities, making it hard to tell whether errors stem from lacking all required abilities or just a single critical one. To address these gaps, we introduce AVA-Bench, the first benchmark that explicitly disentangles 14 Atomic Visual Abilities (AVAs) -- foundational skills like localization, depth estimation, and spatial understanding that collectively support complex visual reasoning tasks. By decoupling AVAs and matching training and test distributions within each, AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM selection from educated guesswork into principled engineering. Notably, we find that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours by 8x, enabling more efficient evaluation. By offering a comprehensive and transparent benchmark, we hope AVA-Bench lays the foundation for the next generation of VFMs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decompositional Reasoning for Graph Retrieval with Large Language Models</title>
<link>https://arxiv.org/abs/2506.13380</link>
<guid>https://arxiv.org/abs/2506.13380</guid>
<content:encoded><![CDATA[
arXiv:2506.13380v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) excel at many NLP tasks, but struggle with multi-hop reasoning and factual consistency, limiting their effectiveness on knowledge-intensive tasks like complex question answering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally lack the ability to reason efficiently over graph-structured information. To tackle this problem, we propose a novel retrieval approach that integrates textual knowledge graphs into the LLM reasoning process via query decomposition. Our method decomposes complex questions into sub-questions, retrieves relevant textual subgraphs, and composes a question-specific knowledge graph to guide answer generation. For that, we use a weighted similarity function that focuses on both the complex question and the generated subquestions to extract a relevant subgraph, which allows efficient and precise retrieval for complex questions and improves the performance of LLMs on multi-hop QA tasks. This structured reasoning pipeline enhances factual grounding and interpretability while leveraging the generative strengths of LLMs. We evaluate our method on standard multi-hop QA benchmarks and show that it achieves comparable or superior performance to competitive existing methods, using smaller models and fewer LLM calls.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks</title>
<link>https://arxiv.org/abs/2507.02819</link>
<guid>https://arxiv.org/abs/2507.02819</guid>
<content:encoded><![CDATA[
arXiv:2507.02819v2 Announce Type: replace-cross 
Abstract: Data scientists often formulate predictive modeling tasks involving fuzzy, hard-to-define concepts, such as the "authenticity" of student writing or the "healthcare need" of a patient. Yet the process by which data scientists translate fuzzy concepts into a concrete, proxy target variable remains poorly understood. We interview fifteen data scientists in education (N=8) and healthcare (N=7) to understand how they construct target variables for predictive modeling tasks. Our findings suggest that data scientists construct target variables through a bricolage process, involving iterative negotiation between high-level measurement objectives and low-level practical constraints. Data scientists attempt to satisfy five major criteria for a target variable through bricolage: validity, simplicity, predictability, portability, and resource requirements. To achieve this, data scientists adaptively use problem (re)formulation strategies, such as swapping out one candidate target variable for another when the first fails to meet certain criteria (e.g., predictability), or composing multiple outcomes into a single target variable to capture a more holistic set of modeling objectives. Based on our findings, we present opportunities for future HCI, CSCW, and ML research to better support the art and science of target variable construction.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Deception Probes via Black-to-White Performance Boosts</title>
<link>https://arxiv.org/abs/2507.12691</link>
<guid>https://arxiv.org/abs/2507.12691</guid>
<content:encoded><![CDATA[
arXiv:2507.12691v2 Announce Type: replace-cross 
Abstract: AI assistants will occasionally respond deceptively to user queries. Recently, linear classifiers (called "deception probes") have been trained to distinguish the internal activations of a language model during deceptive versus honest responses. However, it's unclear how effective these probes are at detecting deception in practice, nor whether such probes are resistant to simple counter strategies from a deceptive assistant who wishes to evade detection. In this paper, we compare white-box monitoring (where the monitor has access to token-level probe activations) to black-box monitoring (without such access). We benchmark deception probes by the extent to which the white box monitor outperforms the black-box monitor, i.e. the black-to-white performance boost. We find weak but encouraging black-to-white performance boosts from existing deception probes.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMUeL: Efficient Vocal-Conditioned Music Generation via Soft Alignment Attention and Latent Diffusion</title>
<link>https://arxiv.org/abs/2507.19991</link>
<guid>https://arxiv.org/abs/2507.19991</guid>
<content:encoded><![CDATA[
arXiv:2507.19991v2 Announce Type: replace-cross 
Abstract: We present a lightweight latent diffusion model for vocal-conditioned musical accompaniment generation that addresses critical limitations in existing music AI systems. Our approach introduces a novel soft alignment attention mechanism that adaptively combines local and global temporal dependencies based on diffusion timesteps, enabling efficient capture of multi-scale musical structure. Operating in the compressed latent space of a pre-trained variational autoencoder, the model achieves a 220 times parameter reduction compared to state-of-the-art systems while delivering 52 times faster inference. Experimental evaluation demonstrates competitive performance with only 15M parameters, outperforming OpenAI Jukebox in production quality and content unity while maintaining reasonable musical coherence. The ultra-lightweight architecture enables real-time deployment on consumer hardware, making AI-assisted music creation accessible for interactive applications and resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis</title>
<link>https://arxiv.org/abs/2508.00381</link>
<guid>https://arxiv.org/abs/2508.00381</guid>
<content:encoded><![CDATA[
arXiv:2508.00381v2 Announce Type: replace-cross 
Abstract: Weld defect detection is crucial for ensuring the safety and reliability of piping systems in the oil and gas industry, especially in challenging marine and offshore environments. Traditional non-destructive testing (NDT) methods often fail to detect subtle or internal defects, leading to potential failures and costly downtime. Furthermore, existing neural network-based approaches for defect classification frequently rely on arbitrarily selected pretrained architectures and lack interpretability, raising safety concerns for deployment. To address these challenges, this paper introduces ``Adapt-WeldNet", an adaptive framework for welding defect detection that systematically evaluates various pre-trained architectures, transfer learning strategies, and adaptive optimizers to identify the best-performing model and hyperparameters, optimizing defect detection and providing actionable insights. Additionally, a novel Defect Detection Interpretability Analysis (DDIA) framework is proposed to enhance system transparency. DDIA employs Explainable AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific evaluations validated by certified ASNT NDE Level II professionals. Incorporating a Human-in-the-Loop (HITL) approach and aligning with the principles of Trustworthy AI, DDIA ensures the reliability, fairness, and accountability of the defect detection system, fostering confidence in automated decisions through expert validation. By improving both performance and interpretability, this work enhances trust, safety, and reliability in welding defect detection systems, supporting critical operations in offshore and marine environments.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaPaformer: Language Model from Pre-trained Parallel Paths</title>
<link>https://arxiv.org/abs/2508.00544</link>
<guid>https://arxiv.org/abs/2508.00544</guid>
<content:encoded><![CDATA[
arXiv:2508.00544v2 Announce Type: replace-cross 
Abstract: The training of modern large-language models requires an increasingly amount of computation power and time. Even smaller variants, such as small-language models (SLMs), take several days to train in the best-case scenarios, often requiring multiple GPUs. This paper explores methods to train and evaluate decoder-only transformer-based language models in hours instead of days/weeks. We introduces \textit{PaPaformer}, a decoder-only transformer architecture variant, whose lower-dimensional parallel paths are combined into larger model. The paper shows that these lower-dimensional paths can be trained individually with different types of training data and then combined into one larger model. This method gives the option to reduce the total number of model parameters and the training time with increasing performance. Moreover, the use of parallel path structure opens interesting possibilities to customize paths to accommodate specific task requirements.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RACE-IT: A Reconfigurable Analog Computing Engine for In-Memory Transformer Acceleration</title>
<link>https://arxiv.org/abs/2312.06532</link>
<guid>https://arxiv.org/abs/2312.06532</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer models, In-memory Computing, Analog Content Addressable Memories, RACE-IT accelerator, DNN architectures<br />
Summary: <br />
This research introduces the RACE-IT accelerator, utilizing Reconfigurable Analog Computing Engine (RACE) based on Analog Content Addressable Memories (ACAMs). The accelerator is designed to efficiently execute core operations of Transformer models in the analog domain, addressing the computational challenges posed by complex activation functions and matrix multiplications. The flexibility of the RACE allows for adaptation to various DNN architectures without hardware modifications. Performance comparisons with GPUs and existing IMC accelerators demonstrate significant improvements in speed and energy efficiency, with RACE-IT outperforming these alternatives by wide margins. The proposed accelerator offers a promising solution for accelerating Transformer models while minimizing computational resource requirements. <br /> 
Summary: <div>
arXiv:2312.06532v3 Announce Type: replace-cross 
Abstract: Transformer models represent the cutting edge of Deep Neural Networks (DNNs) and excel in a wide range of machine learning tasks. However, processing these models demands significant computational resources and results in a substantial memory footprint. While In-memory Computing (IMC)offers promise for accelerating Vector-Matrix Multiplications(VMMs) with high computational parallelism and minimal data movement, employing it for other crucial DNN operators remains a formidable task. This challenge is exacerbated by the extensive use of complex activation functions, Softmax, and data-dependent matrix multiplications (DMMuls) within Transformer models. To address this challenge, we introduce a Reconfigurable Analog Computing Engine (RACE) by enhancing Analog Content Addressable Memories (ACAMs) to support broader operations. Based on the RACE, we propose the RACE-IT accelerator (meaning RACE for In-memory Transformers) to enable efficient analog-domain execution of all core operations of Transformer models. Given the flexibility of our proposed RACE in supporting arbitrary computations, RACE-IT is well-suited for adapting to emerging and non-traditional DNN architectures without requiring hardware modifications. We compare RACE-IT with various accelerators. Results show that RACE-IT increases performance by 453x and 15x, and reduces energy by 354x and 122x over the state-of-the-art GPUs and existing Transformer-specific IMC accelerators, respectively.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The SMeL Test: A simple benchmark for media literacy in language models</title>
<link>https://arxiv.org/abs/2508.02074</link>
<guid>https://arxiv.org/abs/2508.02074</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, untrustworthy content, Synthetic Media Literacy Test, hallucination
Summary: 
The paper introduces the Synthetic Media Literacy Test (SMeL Test) to evaluate language models' ability to filter out untrustworthy information online. Various instruction-tuned large language models (LLMs), including reasoning models, were tested, revealing that no model consistently succeeded in detecting unreliable content. The study found that even the best API model tested exhibited hallucinations up to 70% of the time. Surprisingly, larger and more advanced models did not consistently outperform smaller ones in this context. The research highlights the prevalence of untrustworthy content on the internet and the importance of developing strategies to combat misinformation and deception. <br /><br />Summary: <div>
arXiv:2508.02074v2 Announce Type: replace-cross 
Abstract: The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently succeeds; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ByteGen: A Tokenizer-Free Generative Model for Orderbook Events in Byte Space</title>
<link>https://arxiv.org/abs/2508.02247</link>
<guid>https://arxiv.org/abs/2508.02247</guid>
<content:encoded><![CDATA[
<div> Keywords: generative modeling, high-frequency limit order book, ByteGen, packed data representation, financial markets

Summary: 
ByteGen is a novel generative model designed for high-frequency limit order book (LOB) dynamics in quantitative finance. It operates directly on raw byte streams of LOB events, eliminating the need for feature engineering and tokenization. The model uses a compact and efficient 32-byte packed binary format to represent market messages without information loss. By adapting the H-Net architecture, ByteGen successfully learns market dynamics without predefined rules, achieving competitive performance on standard market quality metrics. Trained on over 34 million events from CME Bitcoin futures, ByteGen reproduces key stylized facts of financial markets, generating realistic price distributions, heavy-tailed returns, and bursty event timing. This end-to-end, byte-level framework for LOB modeling showcases the potential of learning directly from byte space in modeling complex financial systems. <div>
arXiv:2508.02247v2 Announce Type: replace-cross 
Abstract: Generative modeling of high-frequency limit order book (LOB) dynamics is a critical yet unsolved challenge in quantitative finance, essential for robust market simulation and strategy backtesting. Existing approaches are often constrained by simplifying stochastic assumptions or, in the case of modern deep learning models like Transformers, rely on tokenization schemes that affect the high-precision, numerical nature of financial data through discretization and binning. To address these limitations, we introduce ByteGen, a novel generative model that operates directly on the raw byte streams of LOB events. Our approach treats the problem as an autoregressive next-byte prediction task, for which we design a compact and efficient 32-byte packed binary format to represent market messages without information loss. The core novelty of our work is the complete elimination of feature engineering and tokenization, enabling the model to learn market dynamics from its most fundamental representation. We achieve this by adapting the H-Net architecture, a hybrid Mamba-Transformer model that uses a dynamic chunking mechanism to discover the inherent structure of market messages without predefined rules. Our primary contributions are: 1) the first end-to-end, byte-level framework for LOB modeling; 2) an efficient packed data representation; and 3) a comprehensive evaluation on high-frequency data. Trained on over 34 million events from CME Bitcoin futures, ByteGen successfully reproduces key stylized facts of financial markets, generating realistic price distributions, heavy-tailed returns, and bursty event timing. Our findings demonstrate that learning directly from byte space is a promising and highly flexible paradigm for modeling complex financial systems, achieving competitive performance on standard market quality metrics without the biases of tokenization.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge</title>
<link>https://arxiv.org/abs/2508.02583</link>
<guid>https://arxiv.org/abs/2508.02583</guid>
<content:encoded><![CDATA[
<div> framework, causal reasoning, mathematical structure, large language models, performance improvement
Summary:<br />
- The article introduces CAMA, a two-stage causal framework designed to enhance the performance of Large Language Models (LLMs) in complex mathematical reasoning tasks.
- In the learning stage, CAMA constructs a Mathematical Causal Graph (MCG) using LLM priors and causal discovery algorithms applied to question-solution pairs.
- The MCG encodes essential knowledge points and their causal dependencies, refined through iterative feedback.
- In the reasoning stage, CAMA dynamically extracts task-relevant subgraphs from the MCG to guide the LLM's reasoning process based on the question content and intermediate reasoning trace.
- Empirical results demonstrate significant performance improvements with CAMA on challenging mathematical problems, highlighting the effectiveness of structured guidance and the importance of incorporating asymmetric causal relationships. 
Summary: <div>
arXiv:2508.02583v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM's intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAEx: A Plug-and-Play Framework for Explaining Network Alignment</title>
<link>https://arxiv.org/abs/2508.04731</link>
<guid>https://arxiv.org/abs/2508.04731</guid>
<content:encoded><![CDATA[
<div> Network alignment, NAEx, model-agnostic framework, interpretability, subgraphs, key features<br />
<br />
Summary: 
The article introduces NAEx, a model-agnostic framework designed to enhance the interpretability of network alignment (NA) models. NAEx explains alignment decisions by identifying key subgraphs and features that influence predictions, addressing the challenge of preserving cross-network dependencies. It achieves this through joint parameterization of graph structures and feature spaces using learnable edge and feature masks. The optimization objective of NAEx ensures faithful explanations while enabling meaningful comparisons of structural and feature-based similarities between networks. NAEx is efficient and inductive, capable of generating explanations for unseen data. Evaluation metrics specific to alignment explainability validate NAEx's effectiveness and efficiency across benchmark datasets when integrated with representative NA models. <div>
arXiv:2508.04731v1 Announce Type: new 
Abstract: Network alignment (NA) identifies corresponding nodes across multiple networks, with applications in domains like social networks, co-authorship, and biology. Despite advances in alignment models, their interpretability remains limited, making it difficult to understand alignment decisions and posing challenges in building trust, particularly in high-stakes domains. To address this, we introduce NAEx, a plug-and-play, model-agnostic framework that explains alignment models by identifying key subgraphs and features influencing predictions. NAEx addresses the key challenge of preserving the joint cross-network dependencies on alignment decisions by: (1) jointly parameterizing graph structures and feature spaces through learnable edge and feature masks, and (2) introducing an optimization objective that ensures explanations are both faithful to the original predictions and enable meaningful comparisons of structural and feature-based similarities between networks. NAEx is an inductive framework that efficiently generates NA explanations for previously unseen data. We introduce evaluation metrics tailored to alignment explainability and demonstrate NAEx's effectiveness and efficiency on benchmark datasets by integrating it with four representative NA models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2508.04732</link>
<guid>https://arxiv.org/abs/2508.04732</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Image, Vision-Language Models, fine-grained control, iterative framework, image generation <br />
Summary: LumiGen is a new framework that combines Vision-Language Models with Text-to-Image generation to improve image quality and control. It addresses challenges in accurate text rendering, precise pose generation, and compositional coherence. LumiGen includes an Intelligent Prompt Parsing & Augmentation module for enhancing prompts and an Iterative Visual Feedback & Refinement module for optimizing generated images. Tested on the LongBench-T2I Benchmark, LumiGen outperforms existing models with a higher average score of 3.08. It excels in text rendering and pose expression, showcasing the effectiveness of integrating Vision-Language Models for better image generation. This innovative framework demonstrates the potential for LVLM-enhanced iterative approaches to enhance fine-grained control and overall image quality. <br /><br />Summary: <div>
arXiv:2508.04732v1 Announce Type: new 
Abstract: Text-to-Image (T2I) generation has made significant advancements with diffusion models, yet challenges persist in handling complex instructions, ensuring fine-grained content control, and maintaining deep semantic consistency. Existing T2I models often struggle with tasks like accurate text rendering, precise pose generation, or intricate compositional coherence. Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful capabilities in cross-modal understanding and instruction following. We propose LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I model performance, particularly in areas requiring fine-grained control, through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which acts as a "visual critic" to iteratively correct and optimize generated images. Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a superior average score of 3.08, outperforming state-of-the-art baselines. Notably, our framework demonstrates significant improvements in critical dimensions such as text rendering and pose expression, validating the effectiveness of LVLM integration for more controllable and higher-quality image generation.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MissMecha: An All-in-One Python Package for Studying Missing Data Mechanisms</title>
<link>https://arxiv.org/abs/2508.04740</link>
<guid>https://arxiv.org/abs/2508.04740</guid>
<content:encoded><![CDATA[
<div> Python toolkit, simulating missing data, tabular data, MCAR, MAR
Summary:
MissMecha is an open-source Python toolkit designed to simulate, visualize, and evaluate missing data in tabular datasets under MCAR, MAR, and MNAR assumptions. It supports both numerical and categorical features, making it suitable for heterogeneous real-world data. The toolkit includes visual diagnostics, MCAR testing utilities, and type-aware imputation evaluation metrics. This comprehensive tool is ideal for researchers and practitioners interested in studying the impact of missing data on learning and analysis. With its mechanism-aware features, MissMecha allows for in-depth exploration of incomplete data and offers a unified platform for data quality research, benchmarking, and educational purposes. <div>
arXiv:2508.04740v1 Announce Type: new 
Abstract: Incomplete data is a persistent challenge in real-world datasets, often governed by complex and unobservable missing mechanisms. Simulating missingness has become a standard approach for understanding its impact on learning and analysis. However, existing tools are fragmented, mechanism-limited, and typically focus only on numerical variables, overlooking the heterogeneous nature of real-world tabular data. We present MissMecha, an open-source Python toolkit for simulating, visualizing, and evaluating missing data under MCAR, MAR, and MNAR assumptions. MissMecha supports both numerical and categorical features, enabling mechanism-aware studies across mixed-type tabular datasets. It includes visual diagnostics, MCAR testing utilities, and type-aware imputation evaluation metrics. Designed to support data quality research, benchmarking, and education,MissMecha offers a unified platform for researchers and practitioners working with incomplete data.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Assisted Collaborative Fine-Tuning for Multi-User Personalized Artificial Intelligence Generated Content (AIGC)</title>
<link>https://arxiv.org/abs/2508.04745</link>
<guid>https://arxiv.org/abs/2508.04745</guid>
<content:encoded><![CDATA[
<div> Diffusion models, hierarchical federated aggregation, personalized content synthesis, Low-Rank Adaptation, edge-AIGC<br />
<br />
Summary: <br />
Diffusion models (DMs) are powerful for content generation but computationally intensive for edge devices. A novel cluster-aware hierarchical federated aggregation framework is proposed to address these challenges, analyzing and enhancing personalized content synthesis efficiency and scalability. The framework clusters clients based on task requirements similarity and aggregates for enhanced personalization. Intra-cluster and inter-cluster knowledge interactions enable hybrid-style content generation. Utilizing federated learning, personalized models are trained at devices and a shared global model with Low-Rank Adaptation (LoRA) adapters, facilitating efficient edge inference. Encoded prompts mitigate plaintext leakage risk. Evaluations show accelerated convergence and practical viability for scalable personalized AIGC services under edge constraints. <div>
arXiv:2508.04745v1 Announce Type: new 
Abstract: Diffusion models (DMs) have emerged as powerful tools for high-quality content generation, yet their intensive computational requirements for inference pose challenges for resource-constrained edge devices. Cloud-based solutions aid in computation but often fall short in addressing privacy risks, personalization efficiency, and communication costs in multi-user edge-AIGC scenarios. To bridge this gap, we first analyze existing edge-AIGC applications in personalized content synthesis, revealing their limitations in efficiency and scalability. We then propose a novel cluster-aware hierarchical federated aggregation framework. Based on parameter-efficient local fine-tuning via Low-Rank Adaptation (LoRA), the framework first clusters clients based on the similarity of their uploaded task requirements, followed by an intra-cluster aggregation for enhanced personalization at the server-side. Subsequently, an inter-cluster knowledge interaction paradigm is implemented to enable hybrid-style content generation across diverse clusters.Building upon federated learning (FL) collaboration, our framework simultaneously trains personalized models for individual users at the devices and a shared global model enhanced with multiple LoRA adapters on the server,enabling efficient edge inference; meanwhile, all prompts for clustering and inference are encoded prior to transmission, thereby further mitigating the risk of plaintext leakage. Our evaluations demonstrate that the framework achieves accelerated convergence while maintaining practical viability for scalable multi-user personalized AIGC services under edge constraints.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Foundational Multi-Modal Model for Few-Shot Learning</title>
<link>https://arxiv.org/abs/2508.04746</link>
<guid>https://arxiv.org/abs/2508.04746</guid>
<content:encoded><![CDATA[
<div> Few-shot learning, Large Multi-Modal Model, FSL tasks, M3F framework, M3FD dataset <br />
<br />
Summary: 
This study introduces a novel approach to few-shot learning (FSL) by utilizing a Large Multi-Modal Model (LMMM) trained on diverse tasks and modalities. The research showcases the effectiveness of LMMM in improving FSL model generalization, outperforming traditional meta-learning models. A Multi-Modal Model Few-shot Dataset (M3FD) was created with over 10K+ few-shot samples from various scientific domains. The M3F framework, tailored for data-constrained scientific applications, supports different data types through a modular pipeline and enhances model performance through fine-tuning on M3FD. The source code for M3F is available for public access. To facilitate data exploration and reproducibility, a user-friendly tool for querying and preprocessing tasks is provided alongside M3FD. Together, the dataset and framework offer a scalable solution for deploying LMMMs in data-scarce scientific fields. <br /> <div>
arXiv:2508.04746v1 Announce Type: new 
Abstract: Few-shot learning (FSL) is a machine learning paradigm that aims to generalize models from a small number of labeled examples, typically fewer than 10 per class. FSL is particularly crucial in biomedical, environmental, materials, and mechanical sciences, where samples are limited and data collection is often prohibitively costly, time-consuming, or ethically constrained. In this study, we present an innovative approach to FSL by demonstrating that a Large Multi-Modal Model (LMMM), trained on a set of independent tasks spanning diverse domains, task types, and input modalities, can substantially improve the generalization of FSL models, outperforming models based on conventional meta-learning on tasks of the same type. To support this, we first constructed a Multi-Modal Model Few-shot Dataset (M3FD, over 10K+ few-shot samples), which includes 2D RGB images, 2D/3D medical scans, tabular and time-course datasets, from which we manually curated FSL tasks such as classification. We further introduced M3F (Multi-Modal Model for Few-shot learning framework), a novel Large Multi-Modal Model framework tailored for data-constrained scientific applications. M3F supports a wide range of scientific data types through a modular pipeline. By fine-tuning the model on M3FD, M3F improves model performance, making LMMM feasible for real-world FSL deployment. The source code is located at https://github.com/ptdang1001/M3F. To democratize access to complex FSL data and promote reproducibility for public usage, M3FD is paired with a flexible and user-friendly tool that enables efficient querying, task-specific sampling, and preprocessing. Together, our dataset and framework offer a unified, scalable solution that significantly lowers the barrier to applying LMMMs in data-scarce scientific domains.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models</title>
<link>https://arxiv.org/abs/2508.04748</link>
<guid>https://arxiv.org/abs/2508.04748</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Molecular Property Prediction, Large Language Models, Attribute-Based Structured Output, Interpretability<br />
Summary:<br />
The paper introduces AttriLens-Mol, a reinforcement learning framework for molecular property prediction with Large Language Models (LLMs). AttriLens-Mol utilizes attribute guidance to steer the model's reasoning, incorporating format rewards, count rewards, and rationality rewards to encourage relevant attribute generation. Experiments demonstrate the efficacy of AttriLens-Mol in improving prediction performance compared to supervised fine-tuning models and advanced LLMs. The extracted attributes from AttriLens-Mol lead to enhanced interpretability and predictive accuracy when used as features in decision tree models. The open-source code for AttriLens-Mol is available on GitHub, enabling further research and applications in the field. <div>
arXiv:2508.04748v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in https://github.com/szu-tera/AttriLens-Mol.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PA-RNet: Perturbation-Aware Reasoning Network for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.04750</link>
<guid>https://arxiv.org/abs/2508.04750</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal time series data, perturbation-aware, forecasting, Lipschitz continuity, robustness

Summary: 
The paper introduces PA-RNet, a robust multimodal forecasting framework designed to address noise interference in textual data within time series datasets. PA-RNet incorporates a perturbation-aware projection module and a cross-modal attention mechanism to separate noise from textual embeddings while maintaining semantic meaning. The model is proven to be Lipschitz continuous with respect to textual inputs, ensuring stability under noisy conditions. A textual perturbation pipeline is introduced for systematic evaluation of model robustness against varying levels of noise. Experimental results across different domains and temporal settings demonstrate that PA-RNet outperforms existing baselines, highlighting its effectiveness in improving multimodal time series forecasting accuracy in the presence of noisy textual data. 

<br /><br />Summary: <div>
arXiv:2508.04750v1 Announce Type: new 
Abstract: In real-world applications, multimodal time series data often suffer from interference, especially in the textual modality. Existing methods for multimodal time series forecasting often neglect the inherent perturbations within textual data, where irrelevant, noisy, or ambiguous content can significantly degrade model performance, particularly when the noise exhibits varying intensity or stems from structural inconsistencies. To address this challenge, we propose PA-RNet (Perturbation-Aware Reasoning Network for Multimodal Time Series Forecasting), a robust multimodal forecasting framework. PA-RNet features a perturbation-aware projection module and a cross-modal attention mechanism to effectively separate noise from the textual embeddings while maintaining semantically meaningful representations, thereby enhancing the model's generalization ability. Theoretically, we establish the Lipschitz continuity of PA-RNet with respect to textual inputs and prove that the proposed perturbation module can reduce expected prediction error, offering strong guarantees of stability under noisy conditions. Furthermore, we introduce a textual perturbation pipeline that can be seamlessly incorporated into existing multimodal time series forecasting tasks, allowing for systematic evaluation of the model's robustness in the presence of varying levels of textual noise. Extensive experiments across diverse domains and temporal settings demonstrate that PA-RNet consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoQ: Mixed-Precision Quantization via Global Information Flow</title>
<link>https://arxiv.org/abs/2508.04753</link>
<guid>https://arxiv.org/abs/2508.04753</guid>
<content:encoded><![CDATA[
<div> framework, mixed-precision quantization, deep neural networks, information flow, integer linear programming

Summary:
InfoQ is a novel framework for Mixed-Precision Quantization (MPQ) that addresses the complex optimization problem of finding the optimal bit-width for each layer in deep neural networks. It quantifies layer sensitivity by measuring the impact on the information flow throughout the network, leading to a training-free bit-width search phase. The framework formulates the bit-width allocation as an integer linear programming problem, efficiently minimizing total sensitivity under a given budget. Compared to state-of-the-art methods like LIMPQ, InfoQ requires significantly less data and achieves up to 1% accuracy improvement for models like MobileNetV2 and ResNet18 on ImageNet at high compression rates. Overall, InfoQ offers a superior search-time/accuracy trade-off in MPQ for resource-constrained devices. 

<br /><br />Summary: <div>
arXiv:2508.04753v1 Announce Type: new 
Abstract: Mixed-precision quantization (MPQ) is crucial for deploying deep neural networks on resource-constrained devices, but finding the optimal bit-width for each layer represents a complex combinatorial optimization problem. Current state-of-the-art methods rely on computationally expensive search algorithms or local sensitivity heuristic proxies like the Hessian, which fail to capture the cascading global effects of quantization error. In this work, we argue that the quantization sensitivity of a layer should not be measured by its local properties, but by its impact on the information flow throughout the entire network. We introduce InfoQ, a novel framework for MPQ that is training-free in the bit-width search phase. InfoQ assesses layer sensitivity by quantizing each layer at different bit-widths and measuring, through a single forward pass, the resulting change in mutual information in the subsequent layers. This quantifies how much each layer quantization impacts the network information flow. The resulting scores are used to formulate bit-width allocation as an integer linear programming problem, which is solved efficiently to minimize total sensitivity under a given budget (e.g., model size or BitOps). Our retraining-free search phase provides a superior search-time/accuracy trade-off (using two orders of magnitude less data compared to state-of-the-art methods such as LIMPQ), while yielding up to a 1% accuracy improvement for MobileNetV2 and ResNet18 on ImageNet at high compression rates (14X and 10.66X).
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle</title>
<link>https://arxiv.org/abs/2508.04755</link>
<guid>https://arxiv.org/abs/2508.04755</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, dynamic treatment regimes, large language models, Type 1 diabetes, clinical decision-making <br />
<br />
Summary: 
This study evaluates the use of large language models (LLMs) as dynamic insulin dosing agents in Type 1 diabetes treatment. LLMs, prompted with specific language cues, demonstrate comparable or better performance than small neural network-based RL agents in stable patient cohorts. However, LLMs exhibit limitations such as aggressive dosing and failure modes like arithmetic hallucination. Explicit reasoning about latent clinical states minimally improves performance, indicating LLMs struggle with complex physiological dynamics through textual inference alone. The study suggests cautious integration of LLMs into clinical workflows, emphasizing the need for targeted prompt engineering, validation, and potentially hybrid approaches combining linguistic reasoning with structured physiological modeling for safe and effective decision support systems. <br /><br /> <div>
arXiv:2508.04755v1 Announce Type: new 
Abstract: Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold promise for automating complex clinical decision-making, yet their practical deployment remains hindered by the intensive engineering required to inject clinical knowledge and ensure patient safety. Recent advancements in large language models (LLMs) suggest a complementary approach, where implicit prior knowledge and clinical heuristics are naturally embedded through linguistic prompts without requiring environment-specific training. In this study, we rigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in silico Type 1 diabetes simulator, comparing their zero-shot inference performance against small neural network-based RL agents (SRAs) explicitly trained for the task. Our results indicate that carefully designed zero-shot prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or superior clinical performance relative to extensively trained SRAs, particularly in stable patient cohorts. However, LLMs exhibit notable limitations, such as overly aggressive insulin dosing when prompted with chain-of-thought (CoT) reasoning, highlighting critical failure modes including arithmetic hallucination, temporal misinterpretation, and inconsistent clinical logic. Incorporating explicit reasoning about latent clinical states (e.g., meals) yielded minimal performance gains, underscoring the current model's limitations in capturing complex, hidden physiological dynamics solely through textual inference. Our findings advocate for cautious yet optimistic integration of LLMs into clinical workflows, emphasising the necessity of targeted prompt engineering, careful validation, and potentially hybrid approaches that combine linguistic reasoning with structured physiological modelling to achieve safe, robust, and clinically effective decision-support systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration</title>
<link>https://arxiv.org/abs/2508.04780</link>
<guid>https://arxiv.org/abs/2508.04780</guid>
<content:encoded><![CDATA[
<div> predict-then-optimize framework, equity-aware power restoration strategy, repair durations prediction, reinforcement learning, community disparities<br />
<br />
Summary: <br />
The article addresses the need for an equitable and efficient power restoration strategy in the face of extreme weather events. Current restoration decisions based on volume of requests are shown to disadvantage communities with lower submission rates. To tackle this issue, the authors propose EPOPR, a framework that combines Equity-Conformalized Quantile Regression for repair duration prediction and Spatial-Temporal Attentional RL for equitable decision-making. EPOPR reduces average outage duration by 3.60% and decreases inequality between communities by 14.19% compared to existing methods. The challenges of repairing heterogeneous datasets and RL agent bias towards low uncertainty actions are overcome through this innovative approach. EPOPR aims to improve both the efficiency and equity of power restoration processes, ensuring fair and timely responses to power outages. <br /> <div>
arXiv:2508.04780v1 Announce Type: new 
Abstract: The increasing frequency of extreme weather events, such as hurricanes, highlights the urgent need for efficient and equitable power system restoration. Many electricity providers make restoration decisions primarily based on the volume of power restoration requests from each region. However, our data-driven analysis reveals significant disparities in request submission volume, as disadvantaged communities tend to submit fewer restoration requests. This disparity makes the current restoration solution inequitable, leaving these communities vulnerable to extended power outages. To address this, we aim to propose an equity-aware power restoration strategy that balances both restoration efficiency and equity across communities. However, achieving this goal is challenging for two reasons: the difficulty of predicting repair durations under dataset heteroscedasticity, and the tendency of reinforcement learning agents to favor low-uncertainty actions, which potentially undermine equity. To overcome these challenges, we design a predict-then-optimize framework called EPOPR with two key components: (1) Equity-Conformalized Quantile Regression for uncertainty-aware repair duration prediction, and (2) Spatial-Temporal Attentional RL that adapts to varying uncertainty levels across regions for equitable decision-making. Experimental results show that our EPOPR effectively reduces the average power outage duration by 3.60% and decreases inequity between different communities by 14.19% compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Continual Recommendation</title>
<link>https://arxiv.org/abs/2508.04792</link>
<guid>https://arxiv.org/abs/2508.04792</guid>
<content:encoded><![CDATA[
<div> Privacy-preserving, recommendation systems, Federated Learning, Collaborative training, Federated Recommendation<br />
Summary:<br />
The article introduces Federated Continual Recommendation (FCRec), a novel approach that combines Federated Recommendation (FedRec) and Continual Learning Recommendation (CLRec) to address evolving user preferences while maintaining privacy. The proposed framework, F3CRec, incorporates Adaptive Replay Memory on the client side and Item-wise Temporal Mean on the server side to balance knowledge retention and adaptation in a federated setting. Extensive experiments show F3CRec outperforms existing methods in consistently delivering high-quality recommendations over time. <div>
arXiv:2508.04792v1 Announce Type: new 
Abstract: The increasing emphasis on privacy in recommendation systems has led to the adoption of Federated Learning (FL) as a privacy-preserving solution, enabling collaborative training without sharing user data. While Federated Recommendation (FedRec) effectively protects privacy, existing methods struggle with non-stationary data streams, failing to maintain consistent recommendation quality over time. On the other hand, Continual Learning Recommendation (CLRec) methods address evolving user preferences but typically assume centralized data access, making them incompatible with FL constraints. To bridge this gap, we introduce Federated Continual Recommendation (FCRec), a novel task that integrates FedRec and CLRec, requiring models to learn from streaming data while preserving privacy. As a solution, we propose F3CRec, a framework designed to balance knowledge retention and adaptation under the strict constraints of FCRec. F3CRec introduces two key components: Adaptive Replay Memory on the client side, which selectively retains past preferences based on user-specific shifts, and Item-wise Temporal Mean on the server side, which integrates new knowledge while preserving prior information. Extensive experiments demonstrate that F3CRec outperforms existing approaches in maintaining recommendation quality over time in a federated environment.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCRide: Harmonizing Passenger Fairness and Driver Preference for Human-Centered Ride-Hailing</title>
<link>https://arxiv.org/abs/2508.04811</link>
<guid>https://arxiv.org/abs/2508.04811</guid>
<content:encoded><![CDATA[
<div> Keywords: order dispatch systems, ride-hailing services, human-centered design, multi-agent reinforcement learning, system efficiency

Summary:
HCRide is introduced as a human-centered ride-hailing system that aims to optimize passenger fairness, driver preference, and system efficiency simultaneously. The system incorporates a novel multi-agent reinforcement learning algorithm called Habic, which includes a multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic network. By considering both passenger fairness and driver preference, HCRide is able to improve system efficiency by 2.02%, fairness by 5.39%, and driver preference by 10.21% compared to existing baselines. The evaluation of HCRide was conducted using real-world ride-hailing datasets from Shenzhen and New York City, showcasing its effectiveness in enhancing the overall ride-hailing experience for both passengers and drivers.<br /><br />Summary: <div>
arXiv:2508.04811v1 Announce Type: new 
Abstract: Order dispatch systems play a vital role in ride-hailing services, which directly influence operator revenue, driver profit, and passenger experience. Most existing work focuses on improving system efficiency in terms of operator revenue, which may cause a bad experience for both passengers and drivers. Hence, in this work, we aim to design a human-centered ride-hailing system by considering both passenger fairness and driver preference without compromising the overall system efficiency. However, it is nontrivial to achieve this target due to the potential conflicts between passenger fairness and driver preference since optimizing one may sacrifice the other. To address this challenge, we design HCRide, a Human-Centered Ride-hailing system based on a novel multi-agent reinforcement learning algorithm called Harmonization-oriented Actor-Bi-Critic (Habic), which includes three major components (i.e., a multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic network) to optimize system efficiency and passenger fairness with driver preference consideration. We extensively evaluate our HCRide using two real-world ride-hailing datasets from Shenzhen and New York City. Experimental results show our HCRide effectively improves system efficiency by 2.02%, fairness by 5.39%, and driver preference by 10.21% compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Flow Matching for Long Horizon Event Forecasting</title>
<link>https://arxiv.org/abs/2508.04843</link>
<guid>https://arxiv.org/abs/2508.04843</guid>
<content:encoded><![CDATA[
<div> Marked event sequences, long horizon modeling, neural temporal point process, non-autoregressive, flow matching <br />
<br />
Summary: 
The paper introduces a unified flow matching framework for modeling long horizon marked event sequences in various real-world applications. Current neural temporal point process models are often autoregressive, leading to inefficiency and error accumulation in long-range forecasting. The proposed framework enables non-autoregressive, joint modeling of inter-event times and event types through continuous and discrete flow matching. By learning continuous-time flows for both components, the method can generate coherent long horizon event trajectories without the need for sequential decoding. Experimental evaluations on six real-world benchmarks show significant improvements in accuracy and generation efficiency compared to autoregressive and diffusion-based baselines. <div>
arXiv:2508.04843v1 Announce Type: new 
Abstract: Modeling long horizon marked event sequences is a fundamental challenge in many real-world applications, including healthcare, finance, and user behavior modeling. Existing neural temporal point process models are typically autoregressive, predicting the next event one step at a time, which limits their efficiency and leads to error accumulation in long-range forecasting. In this work, we propose a unified flow matching framework for marked temporal point processes that enables non-autoregressive, joint modeling of inter-event times and event types, via continuous and discrete flow matching. By learning continuous-time flows for both components, our method generates coherent long horizon event trajectories without sequential decoding. We evaluate our model on six real-world benchmarks and demonstrate significant improvements over autoregressive and diffusion-based baselines in both accuracy and generation efficiency.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stage Knowledge-Distilled VGAE and GAT for Robust Controller-Area-Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2508.04845</link>
<guid>https://arxiv.org/abs/2508.04845</guid>
<content:encoded><![CDATA[
<div> framework, intrusion detection, CAN protocol, cyber-attacks, graph learning <br />
<br />
Summary: 
This paper introduces a multi-stage intrusion detection framework for securing automotive Controller Area Network (CAN) traffic. By using a combination of unsupervised anomaly detection and supervised graph learning techniques, the framework can effectively detect and classify attacks on the CAN protocol. The architecture utilizes a Variational Graph Autoencoder (VGAE) for structural anomaly detection and a Knowledge-Distilled Graph Attention Network (KD-GAT) for attack classification. Graph sequences are used to model temporal and relational dependencies of CAN bus activity. The framework also includes selective undersampling to address class imbalance and a compact student GAT model that reduces parameters while maintaining strong performance. Experiments on various public CAN intrusion datasets show competitive accuracy and efficiency, with significant improvements in F1-score compared to existing methods, particularly on highly imbalanced datasets. <div>
arXiv:2508.04845v1 Announce Type: new 
Abstract: The Controller Area Network (CAN) protocol is a standard for in-vehicle communication but remains susceptible to cyber-attacks due to its lack of built-in security. This paper presents a multi-stage intrusion detection framework leveraging unsupervised anomaly detection and supervised graph learning tailored for automotive CAN traffic. Our architecture combines a Variational Graph Autoencoder (VGAE) for structural anomaly detection with a Knowledge-Distilled Graph Attention Network (KD-GAT) for robust attack classification. CAN bus activity is encoded as graph sequences to model temporal and relational dependencies. The pipeline applies VGAE-based selective undersampling to address class imbalance, followed by GAT classification with optional score-level fusion. The compact student GAT achieves 96% parameter reduction compared to the teacher model while maintaining strong predictive performance. Experiments on six public CAN intrusion datasets--Car-Hacking, Car-Survival, and can-train-and-test--demonstrate competitive accuracy and efficiency, with average improvements of 16.2% in F1-score over existing methods, particularly excelling on highly imbalanced datasets with up to 55% F1-score improvements.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos</title>
<link>https://arxiv.org/abs/2508.04853</link>
<guid>https://arxiv.org/abs/2508.04853</guid>
<content:encoded><![CDATA[
<div> Quantization, Post-training quantization, OPTQ framework, Error bounds, Theoretical guarantees <br />
<br />
Summary: 
This paper introduces quantitative error bounds for deterministic and stochastic variants of the OPTQ framework used in post-training quantization of deep neural networks. The analysis provides theoretical justification for practical design choices like ordering features by decreasing norm and selecting the regularization parameter. The study also presents stronger infinity-norm error bounds for the stochastic variant, useful for downstream layers and nonlinearities. The analysis extends to Qronos, a related PTQ algorithm, providing theoretical bounds for its deterministic and stochastic variants, explaining its empirical advantages. This research contributes to a better understanding of PTQ algorithms' performance and guides the selection of parameters for optimal quantization results. <div>
arXiv:2508.04853v1 Announce Type: new 
Abstract: Post-training quantization (PTQ) has become a crucial tool for reducing the memory and compute costs of modern deep neural networks, including large language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as GPTQ-has emerged as a leading method due to its computational efficiency and strong empirical performance. Despite its widespread adoption, however, OPTQ lacks rigorous quantitative theoretical guarantees. This paper presents the first quantitative error bounds for both deterministic and stochastic variants of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ algorithm. We analyze how OPTQ's iterative procedure induces quantization error and derive non-asymptotic 2-norm error bounds that depend explicitly on the calibration data and a regularization parameter that OPTQ uses. Our analysis provides theoretical justification for several practical design choices, including the widely used heuristic of ordering features by decreasing norm, as well as guidance for selecting the regularization parameter. For the stochastic variant, we establish stronger infinity-norm error bounds, which enable control over the required quantization alphabet and are particularly useful for downstream layers and nonlinearities. Finally, we extend our analysis to Qronos, providing new theoretical bounds, for both its deterministic and stochastic variants, that help explain its empirical advantages.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment</title>
<link>https://arxiv.org/abs/2508.04865</link>
<guid>https://arxiv.org/abs/2508.04865</guid>
<content:encoded><![CDATA[
<div> Languages, large language models, post-training, Agnostics, reinforcement learning

Summary:<br />
- Agnostics is a language-agnostic post-training pipeline that evaluates code based on observable behavior, eliminating the need for per-language engineering.
- The pipeline utilizes an LLM to rewrite unit-test datasets into an I/O format and applies reinforcement learning with verifiable rewards in a robust code execution environment.
- Agnostics improves performance in low-resource languages like Lua, Julia, R, OCaml, and Fortran, competing with larger open-weight models.
- The pipeline scales effectively to different model families and sets new state-of-the-art results for smaller parameter models on MultiPL-E and LiveCodeBench.
- Language-agnostic training datasets, code, and configurations will be released, simplifying RL post-training in any programming language through YAML file editing.<br /><br />Summary: <div>
arXiv:2508.04865v1 Announce Type: new 
Abstract: Large language models (LLMs) already excel at writing code in high-resource languages such as Python and JavaScript, yet stumble on low-resource languages that remain essential to science and engineering. Besides the obvious shortage of pre-training data, post-training itself is a bottleneck: every new language seems to require new datasets, test harnesses, and reinforcement-learning (RL) infrastructure.
  We introduce Agnostics, a language-agnostic post-training pipeline that eliminates this per-language engineering. The key idea is to judge code solely by its externally observable behavior, so a single verifier can test solutions written in any language. Concretely, we (i) use an LLM to rewrite existing unit-test datasets into an I/O format, (ii) supply a short configuration that tells the verifier how to compile and run a target language, and (iii) apply reinforcement learning with verifiable rewards (RLVR) in a robust code execution environment.
  Applied to five low-resource languages--Lua, Julia, R, OCaml, and Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other 16B-70B open-weight models; (2) scales cleanly to larger and diverse model families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for ${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
  We will release the language-agnostic training datasets (Ag-MBPP-X, Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use configurations, making RL post-training in any programming language as simple as editing a short YAML file.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hilbert Neural Operator: Operator Learning in the Analytic Signal Domain</title>
<link>https://arxiv.org/abs/2508.04882</link>
<guid>https://arxiv.org/abs/2508.04882</guid>
<content:encoded><![CDATA[
<div> Neural operators, Fourier Neural Operator, limitations, Hilbert Neural Operator, signal processing <br />
Summary: <br />
Neural operators, such as the Fourier Neural Operator (FNO), have proven successful in solving partial differential equations using convolutions in the frequency domain. However, FNO has limitations due to the periodicity assumption of the Fourier transform. In response to this, the Hilbert Neural Operator (HNO) is introduced, leveraging signal processing concepts like the Hilbert transform to explicitly incorporate amplitude and phase information in the learning process. By applying spectral convolution to the Hilbert-transformed signal, HNO is designed to model operators more effectively for causal, phase-sensitive, and non-stationary systems. The architecture of HNO is rooted in analytic signal theory, aiming to provide a new approach to learning solution operators for PDEs. <br /> <div>
arXiv:2508.04882v1 Announce Type: new 
Abstract: Neural operators have emerged as a powerful, data-driven paradigm for learning solution operators of partial differential equations (PDEs). State-of-the-art architectures, such as the Fourier Neural Operator (FNO), have achieved remarkable success by performing convolutions in the frequency domain, making them highly effective for a wide range of problems. However, this method has some limitations, including the periodicity assumption of the Fourier transform. In addition, there are other methods of analysing a signal, beyond phase and amplitude perspective, and provide us with other useful information to learn an effective network. We introduce the \textbf{Hilbert Neural Operator (HNO)}, a new neural operator architecture to address some advantages by incorporating a strong inductive bias from signal processing. HNO operates by first mapping the input signal to its analytic representation via the Hilbert transform, thereby making instantaneous amplitude and phase information explicit features for the learning process. The core learnable operation -- a spectral convolution -- is then applied to this Hilbert-transformed representation. We hypothesize that this architecture enables HNO to model operators more effectively for causal, phase-sensitive, and non-stationary systems. We formalize the HNO architecture and provide the theoretical motivation for its design, rooted in analytic signal theory.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian mixture layers for neural networks</title>
<link>https://arxiv.org/abs/2508.04883</link>
<guid>https://arxiv.org/abs/2508.04883</guid>
<content:encoded><![CDATA[
<div> Gaussian mixture models, probability measures, Wasserstein gradient flows, neural networks, training dynamics <br />
Summary:<br />
The article introduces a novel approach to implement training dynamics directly over probability measures using Gaussian mixture models (GM) as a parametric distribution family. By incorporating GM layers into neural network architectures, the proposed method achieves promising results in simple classification tasks, demonstrating comparable performance to traditional fully connected networks. Additionally, numerical experiments reveal that GM layers exhibit distinct behaviors from classical fully connected layers, even when the latter are considered in the mean-field regime. This innovative approach showcases the potential for leveraging probability measures in neural network dynamics, offering a new perspective on network design and optimization strategies. <br /> <div>
arXiv:2508.04883v1 Announce Type: new 
Abstract: The mean-field theory for two-layer neural networks considers infinitely wide networks that are linearly parameterized by a probability measure over the parameter space. This nonparametric perspective has significantly advanced both the theoretical and conceptual understanding of neural networks, with substantial efforts made to validate its applicability to networks of moderate width. In this work, we explore the opposite direction, investigating whether dynamics can be directly implemented over probability measures. Specifically, we employ Gaussian mixture models as a flexible and expressive parametric family of distributions together with the theory of Wasserstein gradient flows to derive training dynamics for such measures. Our approach introduces a new type of layer -- the Gaussian mixture (GM) layer -- that can be integrated into neural network architectures. As a proof of concept, we validate our proposal through experiments on simple classification tasks, where a GM layer achieves test performance comparable to that of a two-layer fully connected network. Furthermore, we examine the behavior of these dynamics and demonstrate numerically that GM layers exhibit markedly different behavior compared to classical fully connected layers, even when the latter are large enough to be considered in the mean-field regime.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Surface Ozone Emulators using Deep Learning</title>
<link>https://arxiv.org/abs/2508.04885</link>
<guid>https://arxiv.org/abs/2508.04885</guid>
<content:encoded><![CDATA[
<div> Keywords: Air pollution, Surface Ozone, Deep Learning, Uncertainty Quantification, Bias Correction

Summary: 
Surface ozone is a significant pollutant affecting global populations, with existing models struggling to accurately predict its levels. Traditional physics-based models are limited in predicting ozone trends at scales relevant to human health. This study introduces a new approach using a Bayesian and quantile regression-based uncertainty-aware U-Net architecture to predict surface ozone residuals. The model, MOMO-Chem, is used to estimate bias in North America and Europe, with a focus on uncertainty quantification. The study evaluates the effectiveness of different ground stations for bias correction and examines the impact of land-use information in ozone residual modeling. The results showcase the potential of deep learning methods in improving ozone predictions and provide valuable insights for decision-makers in implementing policy changes and public health measures.<br /><br />Summary: <div>
arXiv:2508.04885v1 Announce Type: new 
Abstract: Air pollution is a global hazard, and as of 2023, 94\% of the world's population is exposed to unsafe pollution levels. Surface Ozone (O3), an important pollutant, and the drivers of its trends are difficult to model, and traditional physics-based models fall short in their practical use for scales relevant to human-health impacts. Deep Learning-based emulators have shown promise in capturing complex climate patterns, but overall lack the interpretability necessary to support critical decision making for policy changes and public health measures. We implement an uncertainty-aware U-Net architecture to predict the Multi-mOdel Multi-cOnstituent Chemical data assimilation (MOMO-Chem) model's surface ozone residuals (bias) using Bayesian and quantile regression methods. We demonstrate the capability of our techniques in regional estimation of bias in North America and Europe for June 2019. We highlight the uncertainty quantification (UQ) scores between our two UQ methodologies and discern which ground stations are optimal and sub-optimal candidates for MOMO-Chem bias correction, and evaluate the impact of land-use information in surface ozone residual modeling.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Deep Learning for Physical Model Bias of Global Air Quality Estimates</title>
<link>https://arxiv.org/abs/2508.04886</link>
<guid>https://arxiv.org/abs/2508.04886</guid>
<content:encoded><![CDATA[
<div> Keywords: air pollution, surface ozone, Convolutional Neural Network, model bias, satellite imagery

Summary:
This study addresses the challenge of modeling surface ozone, a critical air pollutant, using a 2D Convolutional Neural Network. The researchers focus on understanding and estimating model bias in North America and Europe to improve human health impact assessments. By incorporating land use information from high-resolution satellite imagery, they enhance model estimates and provide insights into urban scale ozone bias factors. The results demonstrate the effectiveness of this technique in capturing physical model residuals compared to traditional machine learning methods. The findings contribute to a better understanding of global ozone trends and can be used to inform environmental policy decisions.<br /><br />Summary: <div>
arXiv:2508.04886v1 Announce Type: new 
Abstract: Air pollution is the world's largest environmental risk factor for human disease and premature death, resulting in more than 6 million permature deaths in 2019. Currently, there is still a challenge to model one of the most important air pollutants, surface ozone, particularly at scales relevant for human health impacts, with the drivers of global ozone trends at these scales largely unknown, limiting the practical use of physics-based models. We employ a 2D Convolutional Neural Network based architecture that estimate surface ozone MOMO-Chem model residuals, referred to as model bias. We demonstrate the potential of this technique in North America and Europe, highlighting its ability better to capture physical model residuals compared to a traditional machine learning method. We assess the impact of incorporating land use information from high-resolution satellite imagery to improve model estimates. Importantly, we discuss how our results can improve our scientific understanding of the factors impacting ozone bias at urban scales that can be used to improve environmental policy.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Water Level Forecasting for Everglades</title>
<link>https://arxiv.org/abs/2508.04888</link>
<guid>https://arxiv.org/abs/2508.04888</guid>
<content:encoded><![CDATA[
<div> Keywords: water level forecasting, deep learning, hydrology, Retrieval-Augmented Forecasting, Everglades<br />
Summary: 
Retrieval-Augmented Forecasting (RAF) is introduced into the hydrology domain to enhance water level forecasting accuracy. The framework retrieves historically similar multivariate hydrological episodes to enrich model input, improving contextual awareness and predictive accuracy without task-specific retraining. RAF methods, including similarity-based and mutual information-based approaches, are explored and compared. Real-world data from the Everglades is used for evaluation, showing substantial improvements in water level forecasting accuracy. The study demonstrates the potential of RAF approaches in environmental hydrology and supports broader adoption of adaptive AI methods by ecosystem management experts.<br /> 
Summary: <div>
arXiv:2508.04888v1 Announce Type: new 
Abstract: Accurate water level forecasting is crucial for managing ecosystems such as the Everglades, a subtropical wetland vital for flood mitigation, drought management, water resource planning, and biodiversity conservation. While recent advances in deep learning, particularly time series foundation models, have demonstrated success in general-domain forecasting, their application in hydrology remains underexplored. Furthermore, they often struggle to generalize across diverse unseen datasets and domains, due to the lack of effective mechanisms for adaptation. To address this gap, we introduce Retrieval-Augmented Forecasting (RAF) into the hydrology domain, proposing a framework that retrieves historically analogous multivariate hydrological episodes to enrich the model input before forecasting. By maintaining an external archive of past observations, RAF identifies and incorporates relevant patterns from historical data, thereby enhancing contextual awareness and predictive accuracy without requiring the model for task-specific retraining or fine-tuning. Furthermore, we explore and compare both similarity-based and mutual information-based RAF methods. We conduct a comprehensive evaluation on real-world data from the Everglades, demonstrating that the RAF framework yields substantial improvements in water level forecasting accuracy. This study highlights the potential of RAF approaches in environmental hydrology and paves the way for broader adoption of adaptive AI methods by domain experts in ecosystem management. The code and data are available at https://github.com/rahuul2992000/WaterRAF.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Honest and Reliable Evaluation and Expert Equivalence Testing of Automated Neonatal Seizure Detection</title>
<link>https://arxiv.org/abs/2508.04899</link>
<guid>https://arxiv.org/abs/2508.04899</guid>
<content:encoded><![CDATA[
<div> Keywords: neonatal seizure detection, machine learning models, performance metrics, consensus strategies, Fleiss k

Summary: 
- Current evaluation practices for neonatal seizure detection models lack consistency and may be biased.
- Matthews and Pearson's correlation coefficients are better indicators of model performance than the area under the curve, especially under class imbalance.
- Consensus strategies should consider the number of raters and their level of agreement.
- The multi-rater Turing test using Fleiss k is recommended for assessing AI performance compared to human experts.
- Recommendations include reporting balanced metrics, sensitivity, specificity, PPV, NPV, multi-rater Turing test results, and validation set performance. 

<br /><br />Summary: <div>
arXiv:2508.04899v1 Announce Type: new 
Abstract: Reliable evaluation of machine learning models for neonatal seizure detection is critical for clinical adoption. Current practices often rely on inconsistent and biased metrics, hindering model comparability and interpretability. Expert-level claims about AI performance are frequently made without rigorous validation, raising concerns about their reliability. This study aims to systematically evaluate common performance metrics and propose best practices tailored to the specific challenges of neonatal seizure detection. Using real and synthetic seizure annotations, we assessed standard performance metrics, consensus strategies, and human-expert level equivalence tests under varying class imbalance, inter-rater agreement, and number of raters. Matthews and Pearson's correlation coefficients outperformed the area under the curve in reflecting performance under class imbalance. Consensus types are sensitive to the number of raters and agreement level among them. Among human-expert level equivalence tests, the multi-rater Turing test using Fleiss k best captured expert-level AI performance. We recommend reporting: (1) at least one balanced metric, (2) Sensitivity, specificity, PPV and NPV, (3) Multi-rater Turing test results using Fleiss k, and (4) All the above on held-out validation set. This proposed framework provides an important prerequisite to clinical validation by enabling a thorough and honest appraisal of AI methods for neonatal seizure detection.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity of Stability: Theoretical &amp; Empirical Analysis of Replicability for Adaptive Data Selection in Transfer Learning</title>
<link>https://arxiv.org/abs/2508.04901</link>
<guid>https://arxiv.org/abs/2508.04901</guid>
<content:encoded><![CDATA[
<div> transfer learning, replicability, selection sensitivity, adaptive selection strategies, sample size 

Summary: 
- The study focuses on the replicability of transfer learning when using adaptive data selection strategies, revealing a trade-off between adaptation effectiveness and result consistency. 
- The mathematical framework introduced quantifies this trade-off, with selection sensitivity ($\Delta_Q$) playing a key role in replicability failure probability. 
- The likelihood of differences in model performance between independent training runs increases quadratically with selection sensitivity but decreases exponentially with sample size. 
- Experimental results on the MultiNLI corpus confirm this theoretical relationship, showing higher replicability failure rates for highly adaptive strategies like gradient-based selection compared to less adaptive approaches. 
- Pretraining on a source domain can mitigate replicability failure rates by up to 30% while maintaining performance gains, providing guidance on navigating the performance-replicability trade-off in transfer learning systems. 

Summary: <div>
arXiv:2508.04901v1 Announce Type: new 
Abstract: The widespread adoption of transfer learning has revolutionized machine learning by enabling efficient adaptation of pre-trained models to new domains. However, the reliability of these adaptations remains poorly understood, particularly when using adaptive data selection strategies that dynamically prioritize training examples. We present a comprehensive theoretical and empirical analysis of replicability in transfer learning, introducing a mathematical framework that quantifies the fundamental trade-off between adaptation effectiveness and result consistency. Our key contribution is the formalization of selection sensitivity ($\Delta_Q$), a measure that captures how adaptive selection strategies respond to perturbations in training data. We prove that replicability failure probability: the likelihood that two independent training runs produce models differing in performance by more than a threshold, increases quadratically with selection sensitivity while decreasing exponentially with sample size. Through extensive experiments on the MultiNLI corpus using six adaptive selection strategies - ranging from uniform sampling to gradient-based selection - we demonstrate that this theoretical relationship holds precisely in practice. Our results reveal that highly adaptive strategies like gradient-based and curriculum learning achieve superior task performance but suffer from high replicability failure rates, while less adaptive approaches maintain failure rates below 7%. Crucially, we show that source domain pretraining provides a powerful mitigation mechanism, reducing failure rates by up to 30% while preserving performance gains. These findings establish principled guidelines for practitioners to navigate the performance-replicability trade-off and highlight the need for replicability-aware design in modern transfer learning systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Hate Speech Detection with Transformers: Insights from the MetaHate</title>
<link>https://arxiv.org/abs/2508.04913</link>
<guid>https://arxiv.org/abs/2508.04913</guid>
<content:encoded><![CDATA[
<div> transformer-based models, hate speech detection, MetaHate dataset, social media, deep learning<br />
Summary:
This study explores the use of transformer-based models for detecting hate speech in social media. The researchers evaluate various models such as BERT, RoBERTa, GPT-2, and ELECTRA on the MetaHate dataset, achieving the highest performance with fine-tuned ELECTRA. The study highlights challenges with sarcasm, coded language, and label noise in hate speech detection. The research emphasizes the need for robust automated methods to address the prevalence of hate speech on social media platforms and its potential real-world impacts. By leveraging deep learning approaches and analyzing classification errors, the study aims to improve the detection and prevention of hate speech online. <div>
arXiv:2508.04913v1 Announce Type: new 
Abstract: Hate speech is a widespread and harmful form of online discourse, encompassing slurs and defamatory posts that can have serious social, psychological, and sometimes physical impacts on targeted individuals and communities. As social media platforms such as X (formerly Twitter), Facebook, Instagram, Reddit, and others continue to facilitate widespread communication, they also become breeding grounds for hate speech, which has increasingly been linked to real-world hate crimes. Addressing this issue requires the development of robust automated methods to detect hate speech in diverse social media environments. Deep learning approaches, such as vanilla recurrent neural networks (RNNs), long short-term memory (LSTM), and convolutional neural networks (CNNs), have achieved good results, but are often limited by issues such as long-term dependencies and inefficient parallelization. This study represents the comprehensive exploration of transformer-based models for hate speech detection using the MetaHate dataset--a meta-collection of 36 datasets with 1.2 million social media samples. We evaluate multiple state-of-the-art transformer models, including BERT, RoBERTa, GPT-2, and ELECTRA, with fine-tuned ELECTRA achieving the highest performance (F1 score: 0.8980). We also analyze classification errors, revealing challenges with sarcasm, coded language, and label noise.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALScope: A Unified Toolkit for Deep Active Learning</title>
<link>https://arxiv.org/abs/2508.04937</link>
<guid>https://arxiv.org/abs/2508.04937</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Active Learning, ALScope, classification tasks, distribution shifts, data imbalance

Summary: 
- Deep Active Learning (DAL) aims to reduce annotation costs by selecting informative unlabeled samples during training. 
- The new DAL platform ALScope integrates 10 datasets from computer vision (CV) and natural language processing (NLP) and 21 representative DAL algorithms, allowing for fair and systematic evaluation under various conditions. 
- DAL algorithms show varying performance across domains and task settings, highlighting the need for further investigation in non-standard scenarios like imbalanced and open-set settings. 
- Some algorithms achieve good performance but have longer selection times, indicating a trade-off between performance and efficiency. 
- The platform supports flexible configuration of experimental factors, enabling comprehensive and realistic evaluation of DAL algorithms in diverse settings. 

<br /><br />Summary: <div>
arXiv:2508.04937v1 Announce Type: new 
Abstract: Deep Active Learning (DAL) reduces annotation costs by selecting the most informative unlabeled samples during training. As real-world applications become more complex, challenges stemming from distribution shifts (e.g., open-set recognition) and data imbalance have gained increasing attention, prompting the development of numerous DAL algorithms. However, the lack of a unified platform has hindered fair and systematic evaluation under diverse conditions. Therefore, we present a new DAL platform ALScope for classification tasks, integrating 10 datasets from computer vision (CV) and natural language processing (NLP), and 21 representative DAL algorithms, including both classical baselines and recent approaches designed to handle challenges such as distribution shifts and data imbalance. This platform supports flexible configuration of key experimental factors, ranging from algorithm and dataset choices to task-specific factors like out-of-distribution (OOD) sample ratio, and class imbalance ratio, enabling comprehensive and realistic evaluation. We conduct extensive experiments on this platform under various settings. Our findings show that: (1) DAL algorithms' performance varies significantly across domains and task settings; (2) in non-standard scenarios such as imbalanced and open-set settings, DAL algorithms show room for improvement and require further investigation; and (3) some algorithms achieve good performance, but require significantly longer selection time.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation</title>
<link>https://arxiv.org/abs/2508.04946</link>
<guid>https://arxiv.org/abs/2508.04946</guid>
<content:encoded><![CDATA[
<div> Keywords: Simultaneous Speech Translation, REINA, Information Theory, Streaming Efficiency, Latency/Quality Trade-off

Summary:
REINA (Regularized Entropy INformation Adaptation) is introduced as a novel loss to optimize the trade-off between translation quality and latency in Simultaneous Speech Translation (SimulST) systems. The strategy behind REINA is to wait for more input only if it provides useful information. Utilizing REINA, a SimulST model is trained on multiple language pairs, achieving state-of-the-art streaming results using open source or synthetically generated data. The use of REINA pushes the reported Pareto frontier of the latency/quality trade-off over prior works. Additionally, a metric for streaming efficiency is introduced, demonstrating that REINA improves the trade-off by up to 21% compared to previous approaches. This highlights the effectiveness of using information theory principles in improving real-time translation systems. 

<br /><br />Summary: <div>
arXiv:2508.04946v1 Announce Type: new 
Abstract: Simultaneous Speech Translation (SimulST) systems stream in audio while simultaneously emitting translated text or speech. Such systems face the significant challenge of balancing translation quality and latency. We introduce a strategy to optimize this tradeoff: wait for more input only if you gain information by doing so. Based on this strategy, we present Regularized Entropy INformation Adaptation (REINA), a novel loss to train an adaptive policy using an existing non-streaming translation model. We derive REINA from information theory principles and show that REINA helps push the reported Pareto frontier of the latency/quality tradeoff over prior works. Utilizing REINA, we train a SimulST model on French, Spanish and German, both from and into English. Training on only open source or synthetically generated data, we achieve state-of-the-art (SOTA) streaming results for models of comparable size. We also introduce a metric for streaming efficiency, quantitatively showing REINA improves the latency/quality trade-off by as much as 21% compared to prior approaches, normalized against non-streaming baseline BLEU scores.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Error Adjustment: Theory and Practice of Balancing Individual Performance and Diversity in Ensemble Learning</title>
<link>https://arxiv.org/abs/2508.04948</link>
<guid>https://arxiv.org/abs/2508.04948</guid>
<content:encoded><![CDATA[
<div> Keywords: Ensemble learning, Self-Error Adjustment (SEA), diversity, individual learner accuracy, performance.

Summary:
Ensemble learning techniques aim to improve performance by combining predictions from multiple base learners. The challenge lies in balancing individual learner accuracy with diversity. Traditional methods like Bagging and Boosting rely on randomness for diversity but lack precise control over the accuracy-diversity trade-off. Negative Correlation Learning (NCL) deals with this trade-off by incorporating a penalty, but faces limitations in adjusting the balance. In contrast, the proposed Self-Error Adjustment (SEA) framework decomposes ensemble errors into individual learner performance and diversity terms, allowing for fine control over the contribution of each component. SEA offers a wider adjustment range and more consistent changes in diversity compared to NCL. The framework also provides tighter theoretical bounds for adjustable ensemble methods. Empirical experiments on various datasets demonstrate the superior performance of SEA over baseline methods in both regression and classification tasks. Ablation studies confirm SEA's flexibility in adjustment capabilities and its effectiveness in fine-tuning strategies. 

<br /><br />Summary: <div>
arXiv:2508.04948v1 Announce Type: new 
Abstract: Ensemble learning boosts performance by aggregating predictions from multiple base learners. A core challenge is balancing individual learner accuracy with diversity. Traditional methods like Bagging and Boosting promote diversity through randomness but lack precise control over the accuracy-diversity trade-off. Negative Correlation Learning (NCL) introduces a penalty to manage this trade-off but suffers from loose theoretical bounds and limited adjustment range. To overcome these limitations, we propose a novel framework called Self-Error Adjustment (SEA), which decomposes ensemble errors into two distinct components: individual performance terms, representing the self-error of each base learner, and diversity terms, reflecting interactions among learners. This decomposition allows us to introduce an adjustable parameter into the loss function, offering precise control over the contribution of each component, thus enabling finer regulation of ensemble performance. Compared to NCL and its variants, SEA provides a broader range of effective adjustments and more consistent changes in diversity. Furthermore, we establish tighter theoretical bounds for adjustable ensemble methods and validate them through empirical experiments. Experimental results on several public regression and classification datasets demonstrate that SEA consistently outperforms baseline methods across all tasks. Ablation studies confirm that SEA offers more flexible adjustment capabilities and superior performance in fine-tuning strategies.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization</title>
<link>https://arxiv.org/abs/2508.04950</link>
<guid>https://arxiv.org/abs/2508.04950</guid>
<content:encoded><![CDATA[
<div> Decentralized algorithms, nonconvex optimization, stochastic optimization, compressed communication, momentum technique
Summary:
- This paper presents two compressed decentralized algorithms for solving nonconvex stochastic optimization problems with momentum acceleration and message compression techniques.
- The algorithms are designed for scenarios with bounded gradients and data heterogeneity without bounded gradients.
- The proposed compressed decentralized adaptive method is the first of its kind with compressed communication for scenarios with bounded gradients.
- The compressed decentralized heavy-ball method addresses data heterogeneity challenges by applying a gradient tracking technique.
- Both methods achieve optimal convergence rates and linear speedup, with superior empirical performance on training deep neural networks and Transformers.
<br /><br />Summary: <div>
arXiv:2508.04950v1 Announce Type: new 
Abstract: In this paper, we design two compressed decentralized algorithms for solving nonconvex stochastic optimization under two different scenarios. Both algorithms adopt a momentum technique to achieve fast convergence and a message-compression technique to save communication costs. Though momentum acceleration and compressed communication have been used in literature, it is highly nontrivial to theoretically prove the effectiveness of their composition in a decentralized algorithm that can maintain the benefits of both sides, because of the need to simultaneously control the consensus error, the compression error, and the bias from the momentum gradient.
  For the scenario where gradients are bounded, our proposal is a compressed decentralized adaptive method. To the best of our knowledge, this is the first decentralized adaptive stochastic gradient method with compressed communication. For the scenario of data heterogeneity without bounded gradients, our proposal is a compressed decentralized heavy-ball method, which applies a gradient tracking technique to address the challenge of data heterogeneity. Notably, both methods achieve an optimal convergence rate, and they can achieve linear speed up and adopt topology-independent algorithmic parameters within a certain regime of the user-specified error tolerance. Superior empirical performance is observed over state-of-the-art methods on training deep neural networks (DNNs) and Transformers.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MENDR: Manifold Explainable Neural Data Representations</title>
<link>https://arxiv.org/abs/2508.04956</link>
<guid>https://arxiv.org/abs/2508.04956</guid>
<content:encoded><![CDATA[
<div> foundation models, electroencephalography signals, EEG information, Manifold Explainable Neural Data Representations, wavelet-based<br />
<br />
Summary:
The article introduces MENDR, a filter bank-based EEG foundation model that prioritizes transparency in pretraining and interpretable learned representations. MENDR utilizes a novel Riemannian Manifold Transformer architecture to learn symmetric positive definite matrix embeddings of EEG signals. Pretrained on a large dataset of EEG data decomposed into multi-resolution coefficients, MENDR enhances interpretability by visualizing embeddings as geometric ellipsoids and supports accurate signal reconstruction. Evaluations across various clinical EEG tasks show MENDR's near state-of-the-art performance with fewer parameters, highlighting its potential for efficient and clinically applicable EEG analysis. <div>
arXiv:2508.04956v1 Announce Type: new 
Abstract: Foundation models for electroencephalography (EEG) signals have recently demonstrated success in learning generalized representations of EEGs, outperforming specialized models in various downstream tasks. However, many of these models lack transparency in their pretraining dynamics and offer limited insight into how well EEG information is preserved within their embeddings. For successful clinical integration, EEG foundation models must ensure transparency in pretraining, downstream fine-tuning, and the interpretability of learned representations. Current approaches primarily operate in the temporal domain, overlooking advancements in digital signal processing that enable the extraction of deterministic and traceable features, such as wavelet-based representations. We propose MENDR (Manifold Explainable Neural Data Representations), a filter bank-based EEG foundation model built on a novel Riemannian Manifold Transformer architecture to resolve these issues. MENDR learns symmetric positive definite matrix embeddings of EEG signals and is pretrained on a large corpus comprising over 4,000 hours of EEG data, decomposed via discrete wavelet packet transforms into multi-resolution coefficients. MENDR significantly enhances interpretability by visualizing symmetric positive definite embeddings as geometric ellipsoids and supports accurate reconstruction of EEG signals from learned embeddings. Evaluations across multiple clinical EEG tasks demonstrate that MENDR achieves near state-of-the-art performance with substantially fewer parameters, underscoring its potential for efficient, interpretable, and clinically applicable EEG analysis.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCUKF: Data-Driven Modeling Meets Bayesian Estimation</title>
<link>https://arxiv.org/abs/2508.04985</link>
<guid>https://arxiv.org/abs/2508.04985</guid>
<content:encoded><![CDATA[
<div> Reservoir Computing, Unscented Kalman Filtering, Data-Driven Modeling, Nonlinear System Dynamics, Bayesian Estimation<br />
Summary:<br />
The article introduces a novel framework called reservoir computing with unscented Kalman filtering (RCUKF) for accurate modeling in complex systems. RCUKF combines data-driven modeling using reservoir computing (RC) with Bayesian estimation using the unscented Kalman filter (UKF). The RC component learns nonlinear system dynamics from data, providing a surrogate process model for the UKF prediction step, especially in high-dimensional or chaotic regimes where nominal models may struggle. The UKF measurement update integrates real-time sensor data to correct potential drift in the data-driven model. The effectiveness of RCUKF is demonstrated on benchmark problems and a real-time vehicle trajectory estimation task in a high-fidelity simulation environment.<br /> <div>
arXiv:2508.04985v1 Announce Type: new 
Abstract: Accurate modeling is crucial in many engineering and scientific applications, yet obtaining a reliable process model for complex systems is often challenging. To address this challenge, we propose a novel framework, reservoir computing with unscented Kalman filtering (RCUKF), which integrates data-driven modeling via reservoir computing (RC) with Bayesian estimation through the unscented Kalman filter (UKF). The RC component learns the nonlinear system dynamics directly from data, serving as a surrogate process model in the UKF prediction step to generate state estimates in high-dimensional or chaotic regimes where nominal mathematical models may fail. Meanwhile, the UKF measurement update integrates real-time sensor data to correct potential drift in the data-driven model. We demonstrate RCUKF effectiveness on well-known benchmark problems and a real-time vehicle trajectory estimation task in a high-fidelity simulation environment.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.04999</link>
<guid>https://arxiv.org/abs/2508.04999</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal sentiment analysis, causal relationships, multi-relational graph, attention mechanism, bias suppression<br />
Summary:<br />
Multimodal sentiment analysis (MSA) often suffers from spurious correlations, leading to unreliable predictions. The proposed Multi-relational Multimodal Causal Intervention (MMCI) model tackles this issue by leveraging causal theory and backdoor adjustment to address confounding effects of shortcuts in modalities. By modeling multimodal inputs as a multi-relational graph and using an attention mechanism, MMCI can estimate and disentangle causal and shortcut features within and across modalities. The backdoor adjustment helps stratify and combine these features to improve prediction stability under distribution shifts. Experimental results on standard MSA datasets and out-of-distribution test sets show that the MMCI model effectively suppresses biases and enhances performance, demonstrating its potential for improving the reliability and generalization capability of multimodal sentiment analysis systems. <br /><br />Summary: <div>
arXiv:2508.04999v1 Announce Type: new 
Abstract: Multimodal sentiment analysis (MSA) aims to understand human emotions by integrating information from multiple modalities, such as text, audio, and visual data. However, existing methods often suffer from spurious correlations both within and across modalities, leading models to rely on statistical shortcuts rather than true causal relationships, thereby undermining generalization. To mitigate this issue, we propose a Multi-relational Multimodal Causal Intervention (MMCI) model, which leverages the backdoor adjustment from causal theory to address the confounding effects of such shortcuts. Specifically, we first model the multimodal inputs as a multi-relational graph to explicitly capture intra- and inter-modal dependencies. Then, we apply an attention mechanism to separately estimate and disentangle the causal features and shortcut features corresponding to these intra- and inter-modal relations. Finally, by applying the backdoor adjustment, we stratify the shortcut features and dynamically combine them with the causal features to encourage MMCI to produce stable predictions under distribution shifts. Extensive experiments on several standard MSA datasets and out-of-distribution (OOD) test sets demonstrate that our method effectively suppresses biases and improves performance.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-Zero: Self-Evolving Reasoning LLM from Zero Data</title>
<link>https://arxiv.org/abs/2508.05004</link>
<guid>https://arxiv.org/abs/2508.05004</guid>
<content:encoded><![CDATA[
<div> Keywords: Self-evolving Large Language Models, R-Zero, autonomous framework, co-evolution, reasoning capability

Summary: 
Self-evolving Large Language Models (LLMs) have the potential to reach super-intelligence by learning from their own experiences. However, current training methods rely heavily on human-curated tasks, limiting AI advancement. To address this, the autonomous framework R-Zero generates its own training data. It utilizes two models, the Challenger and the Solver, which co-evolve through interaction. The Challenger proposes tasks at the edge of the Solver's capability, rewarding both models for improving. This self-improving curriculum does not require pre-existing tasks or labels. R-Zero significantly enhances reasoning capabilities of various LLMs, boosting performance on math-reasoning and general-domain reasoning benchmarks. This approach demonstrates the potential for autonomous AI systems to advance beyond human intelligence levels. 

<br /><br />Summary: <div>
arXiv:2508.05004v1 Announce Type: new 
Abstract: Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models</title>
<link>https://arxiv.org/abs/2508.05015</link>
<guid>https://arxiv.org/abs/2508.05015</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, reinforcement learning, self-paced learning, data clustering, adaptive selection 

Summary: 
SPaRFT is a new self-paced learning framework designed to train large language models efficiently by optimizing data selection based on model capability. It employs cluster-based data reduction to extract a diverse subset of training data and utilizes a multi-armed bandit algorithm to allocate training samples based on the model's current performance. The experiments conducted on various reasoning benchmarks demonstrate that SPaRFT achieves comparable or better accuracy than existing methods while using significantly fewer samples, up to 100 times less. Ablation studies and analyses emphasize the significance of both data clustering and adaptive selection in enhancing the model's reasoning abilities. These findings highlight the potential of performance-driven training curricula in maximizing the efficiency and effectiveness of training large language models with minimal resources. 

<br /><br />Summary: <div>
arXiv:2508.05015v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown strong reasoning capabilities when fine-tuned with reinforcement learning (RL). However, such methods require extensive data and compute, making them impractical for smaller models. Current approaches to curriculum learning or data selection are largely heuristic-driven or demand extensive computational resources, limiting their scalability and generalizability. We propose \textbf{SPaRFT}, a self-paced learning framework that enables efficient learning based on the capability of the model being trained through optimizing which data to use and when. First, we apply \emph{cluster-based data reduction} to partition training data by semantics and difficulty, extracting a compact yet diverse subset that reduces redundancy. Then, a \emph{multi-armed bandit} treats data clusters as arms, optimized to allocate training samples based on model current performance. Experiments across multiple reasoning benchmarks show that SPaRFT achieves comparable or better accuracy than state-of-the-art baselines while using up to \(100\times\) fewer samples. Ablation studies and analyses further highlight the importance of both data clustering and adaptive selection. Our results demonstrate that carefully curated, performance-driven training curricula can unlock strong reasoning abilities in LLMs with minimal resources.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality</title>
<link>https://arxiv.org/abs/2508.05025</link>
<guid>https://arxiv.org/abs/2508.05025</guid>
<content:encoded><![CDATA[
<div> Keywords: Augmented Reality, Situational Awareness, Cardiopulmonary Resuscitation, Eye Tracking, Graph Neural Network 

Summary: 
The study explores how augmented reality (AR) systems can impact situational awareness (SA) during cardiopulmonary resuscitation (CPR) by causing cognitive tunneling. An AR app was developed for CPR guidance, overlaying real-time feedback, and unexpected incidents to assess SA. Higher SA levels correlated with specific eye movement patterns, including saccadic amplitude and velocity. A graph neural network, FixGraphPool, was proposed to predict SA by structuring gaze events into spatiotemporal graphs. The model achieved 83.0% accuracy, outperforming other machine learning approaches. The findings highlight the potential of eye tracking in AR systems to ensure user safety and SA. Eye tracking analysis played a crucial role in understanding SA during CPR in AR environments. <br /><br />Summary: <div>
arXiv:2508.05025v1 Announce Type: new 
Abstract: Augmented Reality (AR) systems, while enhancing task performance through real-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on virtual content that compromises situational awareness (SA) in safety-critical scenarios. This paper investigates SA in AR-guided cardiopulmonary resuscitation (CPR), where responders must balance effective compressions with vigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR app on a Magic Leap 2 that overlays real-time CPR feedback (compression depth and rate) and conducted a user study with simulated unexpected incidents (e.g., bleeding) to evaluate SA, in which SA metrics were collected via observation and questionnaires administered during freeze-probe events. Eye tracking analysis revealed that higher SA levels were associated with greater saccadic amplitude and velocity, and with reduced proportion and frequency of fixations on virtual content. To predict SA, we propose FixGraphPool, a graph neural network that structures gaze events (fixations, saccades) into spatiotemporal graphs, effectively capturing dynamic attentional patterns. Our model achieved 83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and state-of-the-art time-series models by leveraging domain knowledge and spatial-temporal information encoded in ET data. These findings demonstrate the potential of eye tracking for SA modeling in AR and highlight its utility in designing AR systems that ensure user safety and situational awareness.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Oblivion: Predicting Knowledge Overflowed Weights via Retrodiction of Forgetting</title>
<link>https://arxiv.org/abs/2508.05059</link>
<guid>https://arxiv.org/abs/2508.05059</guid>
<content:encoded><![CDATA[
<div> prediction, pre-trained weights, deep learning, knowledge transfer, structured forgetting

Summary:
- The article introduces a novel strategy called Knowledge Overflowed Weights (KNOW) prediction to improve pre-trained weights for better knowledge transfer in deep learning.
- KNOW prediction leverages structured forgetting and its inversion to synthesize knowledge-enriched weights.
- Sequential fine-tuning on progressively downsized datasets induces a structured forgetting process, which can be reversed to recover knowledge as if trained on a larger dataset.
- The KNOW prediction approach utilizes meta-learning to effectively model weight prediction and enhance generalization of pre-trained weights.
- Extensive experiments demonstrate that KNOW prediction consistently outperforms Nave fine-tuning and simple weight prediction techniques, leading to superior performance in downstream tasks. 

<br /><br />Summary: <div>
arXiv:2508.05059v1 Announce Type: new 
Abstract: Pre-trained weights have become a cornerstone of modern deep learning, enabling efficient knowledge transfer and improving downstream task performance, especially in data-scarce scenarios. However, a fundamental question remains: how can we obtain better pre-trained weights that encapsulate more knowledge beyond the given dataset? In this work, we introduce \textbf{KNowledge Overflowed Weights (KNOW)} prediction, a novel strategy that leverages structured forgetting and its inversion to synthesize knowledge-enriched weights. Our key insight is that sequential fine-tuning on progressively downsized datasets induces a structured forgetting process, which can be modeled and reversed to recover knowledge as if trained on a larger dataset. We construct a dataset of weight transitions governed by this controlled forgetting and employ meta-learning to model weight prediction effectively. Specifically, our \textbf{KNowledge Overflowed Weights Nowcaster (KNOWN)} acts as a hyper-model that learns the general evolution of weights and predicts enhanced weights with improved generalization. Extensive experiments across diverse datasets and architectures demonstrate that KNOW prediction consistently outperforms Na\"ive fine-tuning and simple weight prediction, leading to superior downstream performance. Our work provides a new perspective on reinterpreting forgetting dynamics to push the limits of knowledge transfer in deep learning.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TANGO: Graph Neural Dynamics via Learned Energy and Tangential Flows</title>
<link>https://arxiv.org/abs/2508.05070</link>
<guid>https://arxiv.org/abs/2508.05070</guid>
<content:encoded><![CDATA[
<div> Keywords: TANGO, graph representation learning, dynamical systems, Lyapunov function, graph neural networks

Summary:
TANGO is a framework for graph representation learning inspired by dynamical systems. It utilizes a learnable Lyapunov function to govern the evolution of node features through an energy landscape and descent dynamics. By incorporating a tangential component learned through message passing, TANGO enables flexible graph dynamics that propagate signals effectively, even in flat or ill-conditioned energy regions. This decomposition into orthogonal flows of energy gradient descent and tangential evolution helps mitigate oversquashing and is compatible with various graph neural network backbones. Empirical results show that TANGO performs strongly on node and graph classification and regression tasks, highlighting the effectiveness of jointly learned energy functions and tangential flows for enhancing graph neural networks. <div>
arXiv:2508.05070v1 Announce Type: new 
Abstract: We introduce TANGO -- a dynamical systems inspired framework for graph representation learning that governs node feature evolution through a learned energy landscape and its associated descent dynamics. At the core of our approach is a learnable Lyapunov function over node embeddings, whose gradient defines an energy-reducing direction that guarantees convergence and stability. To enhance flexibility while preserving the benefits of energy-based dynamics, we incorporate a novel tangential component, learned via message passing, that evolves features while maintaining the energy value. This decomposition into orthogonal flows of energy gradient descent and tangential evolution yields a flexible form of graph dynamics, and enables effective signal propagation even in flat or ill-conditioned energy regions, that often appear in graph learning. Our method mitigates oversquashing and is compatible with different graph neural network backbones. Empirically, TANGO achieves strong performance across a diverse set of node and graph classification and regression benchmarks, demonstrating the effectiveness of jointly learned energy functions and tangential flows for graph neural networks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ULU: A Unified Activation Function</title>
<link>https://arxiv.org/abs/2508.05073</link>
<guid>https://arxiv.org/abs/2508.05073</guid>
<content:encoded><![CDATA[
<div> ULU, piecewise activation function, ReLU, Mish, image classification<br />
Summary:<br />
The article introduces a novel non-monotonic activation function called ULU, which treats positive and negative inputs differently. ULU outperforms ReLU and Mish in image classification and object detection tasks. A variant of ULU, AULU, allows for separate adaptation of responses for positive and negative inputs through learnable parameters. The LIB metric from AULU quantitatively measures the inductive bias of the model. Through extensive experiments, it is demonstrated that ULU and AULU offer significant performance improvements compared to existing activation functions. <div>
arXiv:2508.05073v1 Announce Type: new 
Abstract: We propose \textbf{ULU}, a novel non-monotonic, piecewise activation function defined as $\{f(x;\alpha_1),x<0; f(x;\alpha_2),x>=0 \}$, where $f(x;\alpha)=0.5x(tanh(\alpha x)+1),\alpha >0$. ULU treats positive and negative inputs differently. Extensive experiments demonstrate ULU significantly outperforms ReLU and Mish across image classification and object detection tasks. Its variant Adaptive ULU (\textbf{AULU}) is expressed as $\{f(x;\beta_1^2),x<0; f(x;\beta_2^2),x>=0 \}$, where $\beta_1$ and $\beta_2$ are learnable parameters, enabling it to adapt its response separately for positive and negative inputs. Additionally, we introduce the LIB (Like Inductive Bias) metric from AULU to quantitatively measure the inductive bias of the model.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning</title>
<link>https://arxiv.org/abs/2508.05077</link>
<guid>https://arxiv.org/abs/2508.05077</guid>
<content:encoded><![CDATA[
<div> Statistical learning theory; Multimodal imitation learning; Sample complexity; Optimization landscapes; Generalization bounds<br />
Summary:<br />
This paper delves into the theoretical underpinnings of multimodal imitation learning using statistical learning theory. It investigates how different modalities such as RGB-D, proprioception, and language impact the sample complexity and optimization landscapes of imitation policies. The study suggests that integrated multimodal policies can have better generalization bounds and optimization landscapes compared to unimodal ones. It reviews theoretical frameworks like PerAct and CLIPort and explains their superior performance based on concepts such as Rademacher complexity, PAC learning, and information theory. The findings highlight the advantages of using multimodal architectures in imitation learning, shedding light on the theoretical foundations that support their effectiveness in achieving robust imitation policies.<br /> <div>
arXiv:2508.05077v1 Announce Type: new 
Abstract: This paper examines the theoretical foundations of multimodal imitation learning through the lens of statistical learning theory. We analyze how multimodal perception (RGB-D, proprioception, language) affects sample complexity and optimization landscapes in imitation policies. Building on recent advances in multimodal learning theory, we show that properly integrated multimodal policies can achieve tighter generalization bounds and more favorable optimization landscapes than their unimodal counterparts. We provide a comprehensive review of theoretical frameworks that explain why multimodal architectures like PerAct and CLIPort achieve superior performance, connecting these empirical results to fundamental concepts in Rademacher complexity, PAC learning, and information theory.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Influence: Data Attribution with Baseline</title>
<link>https://arxiv.org/abs/2508.05089</link>
<guid>https://arxiv.org/abs/2508.05089</guid>
<content:encoded><![CDATA[
<div> baseline, data attribution, influence, machine learning, transparency 

Summary: 

- Data attribution is essential for understanding the influence of training samples on test samples in machine learning models.
- Current data attribution methods based on leave-one-out (LOO) strategy focus on individual sample perturbation, leading to local-based explanations.
- The lack of a baseline in many methods limits the flexibility of explanations, such as providing counterfactual explanations.
- The proposed Integrated Influence method incorporates a baseline approach by transitioning the dataset to a baseline through a data degeneration process.
- Integrated Influence accumulates the influence of each sample throughout the transition process, leading to more reliable data attributions compared to existing methods in tasks such as data attribution and mislabeled example identification. 

<br /><br />Summary: <div>
arXiv:2508.05089v1 Announce Type: new 
Abstract: As an effective approach to quantify how training samples influence test sample, data attribution is crucial for understanding data and model and further enhance the transparency of machine learning models. We find that prevailing data attribution methods based on leave-one-out (LOO) strategy suffer from the local-based explanation, as these LOO-based methods only perturb a single training sample, and overlook the collective influence in the training set. On the other hand, the lack of baseline in many data attribution methods reduces the flexibility of the explanation, e.g., failing to provide counterfactual explanations. In this paper, we propose Integrated Influence, a novel data attribution method that incorporates a baseline approach. Our method defines a baseline dataset, follows a data degeneration process to transition the current dataset to the baseline, and accumulates the influence of each sample throughout this process. We provide a solid theoretical framework for our method, and further demonstrate that popular methods, such as influence functions, can be viewed as special cases of our approach. Experimental results show that Integrated Influence generates more reliable data attributions compared to existing methods in both data attribution task and mislablled example identification task.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cold Start Active Preference Learning in Socio-Economic Domains</title>
<link>https://arxiv.org/abs/2508.05090</link>
<guid>https://arxiv.org/abs/2508.05090</guid>
<content:encoded><![CDATA[
<div> Keywords: Active preference learning, cold-start problem, self-supervised pre-training, Principal Component Analysis, data-constrained environments <br />
Summary: <br />
Active preference learning is a valuable technique for modeling preferences efficiently, but it often struggles with the cold-start problem, especially in data-constrained environments where labeled data is scarce and noisy. To address this issue, a novel framework is proposed that combines self-supervised pre-training using PCA to generate initial pseudo-labels with an active learning loop that queries a simulated noisy oracle for labels. Experimental results on various datasets show that this cold-start approach outperforms standard active learning strategies, achieving higher accuracy with fewer labeled pairs. This framework provides a practical and effective solution for improving sample efficiency and the applicability of preference learning in challenging data scenarios. <div>
arXiv:2508.05090v1 Announce Type: new 
Abstract: Active preference learning is a powerful paradigm for efficiently modeling preferences, yet it suffers from the cold-start problem: a significant drop in performance when no initial labeled data is available. This challenge is particularly acute in computational social systems and economic analysis, where labeled data is often scarce, expensive, and subject to expert noise. To address this gap, we propose a novel framework for cold-start active preference learning. Our method initiates the learning process through a self-supervised pre-training phase, utilizing Principal Component Analysis (PCA) to derive initial pseudo-labels from the data's inherent structure, thereby creating a cold-start model without any initial oracle interaction. Subsequently, the model is refined through an active learning loop that strategically queries a simulated noisy oracle for labels. We conduct extensive experiments on diverse datasets from different domains, including financial credibility, career success rate, and socio-economic status. The results demonstrate that our cold-start approach outperforms standard active learning strategies that begin from a blank slate, achieving higher accuracy with substantially fewer labeled pairs. Our framework offers a practical and effective solution to mitigate the cold-start problem, enhancing the sample efficiency and applicability of preference learning in data-constrained environments. We release our code at https://github.com/Dan-A2/cold-start-preference-learning
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Similarity-Confidence and Confidence-Difference</title>
<link>https://arxiv.org/abs/2508.05108</link>
<guid>https://arxiv.org/abs/2508.05108</guid>
<content:encoded><![CDATA[
<div> Framework, Weakly Supervised Learning, Multiple weak supervision signals, Unlabeled data pairs, Risk correction<br />
<br />
Summary: 
The paper proposes a novel Weakly Supervised Learning framework that integrates multiple weak supervision signals to train models with incomplete or imprecise labels when labeled data is scarce. The method, called SconfConfDiff Classification, combines two types of weak labels - similarity-confidence and confidence-difference - assigned to unlabeled data pairs. Two unbiased risk estimators are derived for classification, achieving optimal convergence rates and addressing overfitting through a risk correction approach. The method is theoretically proven to be robust against inaccurate class prior probability and label noise. Experimental results show that the proposed method outperforms existing baselines in various settings. <div>
arXiv:2508.05108v1 Announce Type: new 
Abstract: In practical machine learning applications, it is often challenging to assign accurate labels to data, and increasing the number of labeled instances is often limited. In such cases, Weakly Supervised Learning (WSL), which enables training with incomplete or imprecise supervision, provides a practical and effective solution. However, most existing WSL methods focus on leveraging a single type of weak supervision. In this paper, we propose a novel WSL framework that leverages complementary weak supervision signals from multiple relational perspectives, which can be especially valuable when labeled data is limited. Specifically, we introduce SconfConfDiff Classification, a method that integrates two distinct forms of weaklabels: similarity-confidence and confidence-difference, which are assigned to unlabeled data pairs. To implement this method, we derive two types of unbiased risk estimators for classification: one based on a convex combination of existing estimators, and another newly designed by modeling the interaction between two weak labels. We prove that both estimators achieve optimal convergence rates with respect to estimation error bounds. Furthermore, we introduce a risk correction approach to mitigate overfitting caused by negative empirical risk, and provide theoretical analysis on the robustness of the proposed method against inaccurate class prior probability and label noise. Experimental results demonstrate that the proposed method consistently outperforms existing baselines across a variety of settings.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Superior Function Calls via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.05118</link>
<guid>https://arxiv.org/abs/2508.05118</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, function calling, policy optimization, structured reasoning, code-pretrained models <br />
Summary: <br />
The article introduces a novel reinforcement learning framework aimed at improving group relative policy optimization for function calling tasks. Current training approaches for Large Language Models (LLMs) lack robust reasoning strategies, leading to models that rely on superficial pattern matching. The new framework addresses key challenges in function calling, including insufficient exploration during policy learning, structured reasoning in chain-of-thought generation, and verification of parameter extraction. A two-stage data preparation pipeline ensures high-quality training samples through LLM evaluation and abstract syntax tree validation. Extensive experiments on the Berkeley Function Calling Leaderboard show that the framework achieves state-of-the-art performance, with 86.02% overall accuracy, surpassing standard GRPO by up to 6% on complex scenarios. The method demonstrates significant improvements on code-pretrained models, indicating the benefits of structured language generation for reinforcement learning in function calling tasks. All code, models, and datasets will be released to support the research community. <div>
arXiv:2508.05118v1 Announce Type: new 
Abstract: Function calling capabilities are crucial for deploying Large Language Models in real-world applications, yet current training approaches fail to develop robust reasoning strategies. Supervised fine-tuning produces models that rely on superficial pattern matching, while standard reinforcement learning methods struggle with the complex action space of structured function calls. We present a novel reinforcement learning framework designed to enhance group relative policy optimization through strategic entropy based exploration specifically tailored for function calling tasks. Our approach addresses three critical challenges in function calling: insufficient exploration during policy learning, lack of structured reasoning in chain-of-thought generation, and inadequate verification of parameter extraction. Our two-stage data preparation pipeline ensures high-quality training samples through iterative LLM evaluation and abstract syntax tree validation. Extensive experiments on the Berkeley Function Calling Leaderboard demonstrate that this framework achieves state-of-the-art performance among open-source models with 86.02\% overall accuracy, outperforming standard GRPO by up to 6\% on complex multi-function scenarios. Notably, our method shows particularly strong improvements on code-pretrained models, suggesting that structured language generation capabilities provide an advantageous starting point for reinforcement learning in function calling tasks. We will release all the code, models and dataset to benefit the community.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation</title>
<link>https://arxiv.org/abs/2508.05135</link>
<guid>https://arxiv.org/abs/2508.05135</guid>
<content:encoded><![CDATA[
<div> Hierarchical Federated Domain Generalization, Federated Learning, Scalability, Domain Shift, Convolutional Filters<br />
<br />
Summary:
Hierarchical Federated Learning (HFL) addresses scalability challenges in Federated Learning by distributing model aggregation tasks across intermediate nodes. However, HFL still faces limitations due to domain shifts, affecting model performance on unseen data. This paper introduces Hierarchical Federated Domain Generalization (HFedDG), focusing on addressing domain shift within hierarchical architectures. HFedATM, a hierarchical aggregation method, is proposed to align convolutional filters of models from different stations and merge them using a Shrinkage-aware Regularized Mean Aggregation. Experimental evaluations show that HFedATM significantly improves the performance of existing FedDG baselines across multiple datasets while maintaining efficiency. Theoretical analyses demonstrate that HFedATM achieves tighter generalization error bounds, leading to faster convergence and more stable training behavior. <div>
arXiv:2508.05135v1 Announce Type: new 
Abstract: Federated Learning (FL) is a decentralized approach where multiple clients collaboratively train a shared global model without sharing their raw data. Despite its effectiveness, conventional FL faces scalability challenges due to excessive computational and communication demands placed on a single central server as the number of participating devices grows. Hierarchical Federated Learning (HFL) addresses these issues by distributing model aggregation tasks across intermediate nodes (stations), thereby enhancing system scalability and robustness against single points of failure. However, HFL still suffers from a critical yet often overlooked limitation: domain shift, where data distributions vary significantly across different clients and stations, reducing model performance on unseen target domains. While Federated Domain Generalization (FedDG) methods have emerged to improve robustness to domain shifts, their integration into HFL frameworks remains largely unexplored. In this paper, we formally introduce Hierarchical Federated Domain Generalization (HFedDG), a novel scenario designed to investigate domain shift within hierarchical architectures. Specifically, we propose HFedATM, a hierarchical aggregation method that first aligns the convolutional filters of models from different stations through Filter-wise Optimal Transport Alignment and subsequently merges aligned models using a Shrinkage-aware Regularized Mean Aggregation. Our extensive experimental evaluations demonstrate that HFedATM significantly boosts the performance of existing FedDG baselines across multiple datasets and maintains computational and communication efficiency. Moreover, theoretical analyses indicate that HFedATM achieves tighter generalization error bounds compared to standard hierarchical averaging, resulting in faster convergence and stable training behavior.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Neural Networks with General Activations: Super-Convergence in Sobolev Norms</title>
<link>https://arxiv.org/abs/2508.05141</link>
<guid>https://arxiv.org/abs/2508.05141</guid>
<content:encoded><![CDATA[
<div> Sobolev spaces, deep neural networks, approximation, PDEs, super-convergence
<br />
Summary:
This paper presents a comprehensive approximation result for deep fully-connected neural networks utilizing general activation functions in Sobolev spaces $W^{n,\infty}$. The analysis demonstrates superior error rates compared to traditional numerical methods like finite element and spectral techniques, showcasing a concept of "super-convergence." Deep networks with general activations can effectively approximate weak solutions of partial differential equations (PDEs) with higher accuracy than classical methods at the approximation level. The research bridges a significant gap in error-estimation theory for neural-network-based PDE approaches, establishing a unified theoretical foundation for their application in scientific computing. <div>
arXiv:2508.05141v1 Announce Type: new 
Abstract: This paper establishes a comprehensive approximation result for deep fully-connected neural networks with commonly-used and general activation functions in Sobolev spaces $W^{n,\infty}$, with errors measured in the $W^{m,p}$-norm for $m < n$ and $1\le p \le \infty$. The derived rates surpass those of classical numerical approximation techniques, such as finite element and spectral methods, exhibiting a phenomenon we refer to as \emph{super-convergence}. Our analysis shows that deep networks with general activations can approximate weak solutions of partial differential equations (PDEs) with superior accuracy compared to traditional numerical methods at the approximation level. Furthermore, this work closes a significant gap in the error-estimation theory for neural-network-based approaches to PDEs, offering a unified theoretical foundation for their use in scientific computing.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSEO: Optimizing Post-hoc Stacking Ensemble Through Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2508.05144</link>
<guid>https://arxiv.org/abs/2508.05144</guid>
<content:encoded><![CDATA[
<div> ensemble learning, automated machine learning, algorithm selection, hyperparameter optimization, post-hoc stacking ensemble optimization

Summary:<br />
The article addresses the Combined Algorithm Selection and Hyperparameter Optimization (CASH) problem in Automated Machine Learning (AutoML) by introducing a framework called PSEO for post-hoc stacking ensemble optimization. The framework focuses on base model selection using binary quadratic programming to balance diversity and performance. Two mechanisms are introduced to enhance multi-layer stacking, and a hyperparameter space is built to optimize the post-hoc ensemble strategy. Empirical results on 80 public datasets demonstrate that PSEO outperforms 16 methods, including recent AutoML systems and state-of-the-art ensemble learning methods, achieving the best average test rank of 2.96. The approach aims to improve the adaptability of ensemble strategies to specific task characteristics, contributing to the advancement of automated machine learning techniques. 

Summary: <div>
arXiv:2508.05144v1 Announce Type: new 
Abstract: The Combined Algorithm Selection and Hyperparameter Optimization (CASH) problem is fundamental in Automated Machine Learning (AutoML). Inspired by the success of ensemble learning, recent AutoML systems construct post-hoc ensembles for final predictions rather than relying on the best single model. However, while most CASH methods conduct extensive searches for the optimal single model, they typically employ fixed strategies during the ensemble phase that fail to adapt to specific task characteristics. To tackle this issue, we propose PSEO, a framework for post-hoc stacking ensemble optimization. First, we conduct base model selection through binary quadratic programming, with a trade-off between diversity and performance. Furthermore, we introduce two mechanisms to fully realize the potential of multi-layer stacking. Finally, PSEO builds a hyperparameter space and searches for the optimal post-hoc ensemble strategy within it. Empirical results on 80 public datasets show that \sys achieves the best average test rank (2.96) among 16 methods, including post-hoc designs in recent AutoML systems and state-of-the-art ensemble learning methods.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation</title>
<link>https://arxiv.org/abs/2508.05154</link>
<guid>https://arxiv.org/abs/2508.05154</guid>
<content:encoded><![CDATA[
<div> optimization algorithms, agent-based models, reinforcement learning, rational agent-based models, domain-driven metrics

Summary:
This study introduces domain-driven metrics for reinforcement learning (RL) in the context of agent-based models (ABMs) and rational agent-based models (RABMs). The complexity and stochasticity of modeled systems make assessing the performance of RL-based models challenging, necessitating the need for standardized comparison metrics. The researchers demonstrate the application of their "Domain-driven-RL-metrics" in a rational ABM disease modeling case study focusing on masking behavior, vaccination, and lockdown strategies during a pandemic. By using domain-driven rewards in conjunction with traditional and state-of-the-art metrics, they explore various simulation scenarios, including differential mask availability. This approach offers a promising framework for evaluating and optimizing RL-based models in complex systems, enhancing the understanding and effectiveness of ABMs and RABMs in practical applications.<br /><br />Summary: <div>
arXiv:2508.05154v1 Announce Type: new 
Abstract: For the development and optimization of agent-based models (ABMs) and rational agent-based models (RABMs), optimization algorithms such as reinforcement learning are extensively used. However, assessing the performance of RL-based ABMs and RABMS models is challenging due to the complexity and stochasticity of the modeled systems, and the lack of well-standardized metrics for comparing RL algorithms. In this study, we are developing domain-driven metrics for RL, while building on state-of-the-art metrics. We demonstrate our ``Domain-driven-RL-metrics'' using policy optimization on a rational ABM disease modeling case study to model masking behavior, vaccination, and lockdown in a pandemic. Our results show the use of domain-driven rewards in conjunction with traditional and state-of-the-art metrics for a few different simulation scenarios such as the differential availability of masks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>pFedDSH: Enabling Knowledge Transfer in Personalized Federated Learning through Data-free Sub-Hypernetwork</title>
<link>https://arxiv.org/abs/2508.05157</link>
<guid>https://arxiv.org/abs/2508.05157</guid>
<content:encoded><![CDATA[
<div> Privacy, Federated Learning, Personalized, Dynamic Client Onboarding, Knowledge Transfer

Summary:
The paper introduces a new framework called pFedDSH for Personalized Federated Learning (pFL) in dynamic client onboarding scenarios. Traditional pFL methods assume static client participation, but pFedDSH addresses the challenge of incremental client introduction while maintaining existing clients' performance. The framework utilizes a central hypernetwork to generate personalized models for each client using embedding vectors. Batch-specific masks are used to preserve knowledge for existing clients, while a data-free replay strategy facilitates efficient knowledge transfer between client batches. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets show that pFedDSH outperforms existing pFL and Federated Continual Learning methods. The approach achieves stable performance for existing clients, adapts well to new clients, and efficiently utilizes neural resources. The framework demonstrates the potential for personalized federated learning in dynamic environments. 

<br /><br />Summary: <div>
arXiv:2508.05157v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across distributed clients without sharing raw data, offering a significant privacy benefit. However, most existing Personalized Federated Learning (pFL) methods assume a static client participation, which does not reflect real-world scenarios where new clients may continuously join the federated system (i.e., dynamic client onboarding). In this paper, we explore a practical scenario in which a new batch of clients is introduced incrementally while the learning task remains unchanged. This dynamic environment poses various challenges, including preserving performance for existing clients without retraining and enabling efficient knowledge transfer between client batches. To address these issues, we propose Personalized Federated Data-Free Sub-Hypernetwork (pFedDSH), a novel framework based on a central hypernetwork that generates personalized models for each client via embedding vectors. To maintain knowledge stability for existing clients, pFedDSH incorporates batch-specific masks, which activate subsets of neurons to preserve knowledge. Furthermore, we introduce a data-free replay strategy motivated by DeepInversion to facilitate backward transfer, enhancing existing clients' performance without compromising privacy. Extensive experiments conducted on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that pFedDSH outperforms the state-of-the-art pFL and Federated Continual Learning baselines in our investigation scenario. Our approach achieves robust performance stability for existing clients, as well as adaptation for new clients and efficient utilization of neural resources.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S$^2$M-Former: Spiking Symmetric Mixing Branchformer for Brain Auditory Attention Detection</title>
<link>https://arxiv.org/abs/2508.05164</link>
<guid>https://arxiv.org/abs/2508.05164</guid>
<content:encoded><![CDATA[
<div> Keywords: Auditory attention detection, EEG, spiking symmetric mixing, energy efficiency, neural networks

Summary: 
S$^2$M-Former is introduced as a novel framework for EEG-based auditory attention detection (AAD). It utilizes a spike-driven symmetric architecture with parallel spatial and frequency branches, enhancing learning across branches. The use of lightweight 1D token sequences reduces parameters significantly, leading to a 14.7$\times$ parameter reduction. The spiking architecture reduces power consumption by 5.8$\times$ compared to traditional artificial neural network methods. Despite the energy-efficient design, S$^2$M-Former achieves state-of-the-art decoding accuracy on three AAD benchmarks, making it a promising solution for AAD applications. <br /><br />Summary: <div>
arXiv:2508.05164v1 Announce Type: new 
Abstract: Auditory attention detection (AAD) aims to decode listeners' focus in complex auditory environments from electroencephalography (EEG) recordings, which is crucial for developing neuro-steered hearing devices. Despite recent advancements, EEG-based AAD remains hindered by the absence of synergistic frameworks that can fully leverage complementary EEG features under energy-efficiency constraints. We propose S$^2$M-Former, a novel spiking symmetric mixing framework to address this limitation through two key innovations: i) Presenting a spike-driven symmetric architecture composed of parallel spatial and frequency branches with mirrored modular design, leveraging biologically plausible token-channel mixers to enhance complementary learning across branches; ii) Introducing lightweight 1D token sequences to replace conventional 3D operations, reducing parameters by 14.7$\times$. The brain-inspired spiking architecture further reduces power consumption, achieving a 5.8$\times$ energy reduction compared to recent ANN methods, while also surpassing existing SNN baselines in terms of parameter efficiency and performance. Comprehensive experiments on three AAD benchmarks (KUL, DTU and AV-GC-AAD) across three settings (within-trial, cross-trial and cross-subject) demonstrate that S$^2$M-Former achieves comparable state-of-the-art (SOTA) decoding accuracy, making it a promising low-power, high-performance solution for AAD tasks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models</title>
<link>https://arxiv.org/abs/2508.05165</link>
<guid>https://arxiv.org/abs/2508.05165</guid>
<content:encoded><![CDATA[
<div> Heuristic-Guided Inference-time Alignment, LLMs, user preferences, real-world use, alignment quality <br />
<br />
Summary: Aligning large language models (LLMs) with user preferences is crucial for their real-world use. However, this often requires costly fine-tuning or expensive inference, leading to trade-offs between alignment quality and computational cost. In this study, a new approach called Heuristic-Guided Inference-time Alignment (HIA) is proposed to address this challenge. HIA is a tuning-free, black-box-compatible method that uses a lightweight prompt optimizer, heuristic reward models, and two-stage filtering to reduce inference calls while maintaining alignment quality. The results show that HIA outperforms existing methods such as best-of-N sampling, beam search, and greedy search in multi-objective, goal-conditioned tasks on real-world prompt datasets. Additionally, HIA is effective even under low-inference budgets, providing a practical solution for scalable and personalized LLM deployment. <div>
arXiv:2508.05165v1 Announce Type: new 
Abstract: Aligning LLMs with user preferences is crucial for real-world use but often requires costly fine-tuning or expensive inference, forcing trade-offs between alignment quality and computational cost. Existing inference-time methods typically ignore this balance, focusing solely on the optimized policy's performance. We propose HIA (Heuristic-Guided Inference-time Alignment), a tuning-free, black-box-compatible approach that uses a lightweight prompt optimizer, heuristic reward models, and two-stage filtering to reduce inference calls while preserving alignment quality. On real-world prompt datasets, HelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and greedy search baselines in multi-objective, goal-conditioned tasks under the same inference budget. We also find that HIA is effective under low-inference budgets with as little as one or two response queries, offering a practical solution for scalable, personalized LLM deployment.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near Optimal Inference for the Best-Performing Algorithm</title>
<link>https://arxiv.org/abs/2508.05173</link>
<guid>https://arxiv.org/abs/2508.05173</guid>
<content:encoded><![CDATA[
<div> subset selection, machine learning algorithms, benchmark, multinomial distributions, confidence

Summary:
- The article discusses the problem of identifying the best-performing machine learning algorithm based on their performance on a benchmark dataset.
- It formulates the problem as subset selection for multinomial distributions, aiming to identify the algorithm most likely to rank highest on future datasets.
- The proposed framework introduces novel schemes for subset selection, including both asymptotic and finite-sample methods that outperform existing approaches.
- Matching lower bounds are provided to show the superiority of the proposed schemes.
- The research highlights the importance of confidently selecting the best-performing algorithm, especially when performance differences are marginal. <div>
arXiv:2508.05173v1 Announce Type: new 
Abstract: Consider a collection of competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to rank highest on a future, unseen dataset. A natural approach is to select the algorithm that demonstrates the best performance on the benchmark. However, in many cases the performance differences are marginal and additional candidates may also be considered. This problem is formulated as subset selection for multinomial distributions. Formally, given a sample from a countable alphabet, our goal is to identify a minimal subset of symbols that includes the most frequent symbol in the population with high confidence. In this work, we introduce a novel framework for the subset selection problem. We provide both asymptotic and finite-sample schemes that significantly improve upon currently known methods. In addition, we provide matching lower bounds, demonstrating the favorable performance of our proposed schemes.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Activity Recognition from Smartphone Sensor Data for Clinical Trials</title>
<link>https://arxiv.org/abs/2508.05175</link>
<guid>https://arxiv.org/abs/2508.05175</guid>
<content:encoded><![CDATA[
arXiv:2508.05175v1 Announce Type: new 
Abstract: We developed a ResNet-based human activity recognition (HAR) model with minimal overhead to detect gait versus non-gait activities and everyday activities (walking, running, stairs, standing, sitting, lying, sit-to-stand transitions). The model was trained and evaluated using smartphone sensor data from adult healthy controls (HC) and people with multiple sclerosis (PwMS) with Expanded Disability Status Scale (EDSS) scores between 0.0-6.5. Datasets included the GaitLab study (ISRCTN15993728), an internal Roche dataset, and publicly available data sources (training only). Data from 34 HC and 68 PwMS (mean [SD] EDSS: 4.7 [1.5]) were included in the evaluation. The HAR model showed 98.4% and 99.6% accuracy in detecting gait versus non-gait activities in the GaitLab and Roche datasets, respectively, similar to a comparative state-of-the-art ResNet model (99.3% and 99.4%). For everyday activities, the proposed model not only demonstrated higher accuracy than the state-of-the-art model (96.2% vs 91.9%; internal Roche dataset) but also maintained high performance across 9 smartphone wear locations (handbag, shopping bag, crossbody bag, backpack, hoodie pocket, coat/jacket pocket, hand, neck, belt), outperforming the state-of-the-art model by 2.8% - 9.0%. In conclusion, the proposed HAR model accurately detects everyday activities and shows high robustness to various smartphone wear locations, demonstrating its practical applicability.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Time-Integrated DeepONet: Temporal Tangent Space Operator Learning for High-Accuracy Inference</title>
<link>https://arxiv.org/abs/2508.05190</link>
<guid>https://arxiv.org/abs/2508.05190</guid>
<content:encoded><![CDATA[
arXiv:2508.05190v1 Announce Type: new 
Abstract: Accurately modeling and inferring solutions to time-dependent partial differential equations (PDEs) over extended horizons remains a core challenge in scientific machine learning. Traditional full rollout (FR) methods, which predict entire trajectories in one pass, often fail to capture the causal dependencies and generalize poorly outside the training time horizon. Autoregressive (AR) approaches, evolving the system step by step, suffer from error accumulation, limiting long-term accuracy. These shortcomings limit the long-term accuracy and reliability of both strategies. To address these issues, we introduce the Physics-Informed Time-Integrated Deep Operator Network (PITI-DeepONet), a dual-output architecture trained via fully physics-informed or hybrid physics- and data-driven objectives to ensure stable, accurate long-term evolution well beyond the training horizon. Instead of forecasting future states, the network learns the time-derivative operator from the current state, integrating it using classical time-stepping schemes to advance the solution in time. Additionally, the framework can leverage residual monitoring during inference to estimate prediction quality and detect when the system transitions outside the training domain. Applied to benchmark problems, PITI-DeepONet shows improved accuracy over extended inference time horizons when compared to traditional methods. Mean relative $\mathcal{L}_2$ errors reduced by 84% (vs. FR) and 79% (vs. AR) for the one-dimensional heat equation; by 87% (vs. FR) and 98% (vs. AR) for the one-dimensional Burgers equation; and by 42% (vs. FR) and 89% (vs. AR) for the two-dimensional Allen-Cahn equation. By moving beyond classic FR and AR schemes, PITI-DeepONet paves the way for more reliable, long-term integration of complex, time-dependent PDEs.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in finance</title>
<link>https://arxiv.org/abs/2508.05201</link>
<guid>https://arxiv.org/abs/2508.05201</guid>
<content:encoded><![CDATA[
arXiv:2508.05201v1 Announce Type: new 
Abstract: Hallucination remains a critical challenge for deploying Large Language Models (LLMs) in finance. Accurate extraction and precise calculation from tabular data are essential for reliable financial analysis, since even minor numerical errors can undermine decision-making and regulatory compliance. Financial applications have unique requirements, often relying on context-dependent, numerical, and proprietary tabular data that existing hallucination benchmarks rarely capture. In this study, we develop a rigorous and scalable framework for evaluating intrinsic hallucinations in financial LLMs, conceptualized as a context-aware masked span prediction task over real-world financial documents. Our main contributions are: (1) a novel, automated dataset creation paradigm using a masking strategy; (2) a new hallucination evaluation dataset derived from S&amp;P 500 annual reports; and (3) a comprehensive evaluation of intrinsic hallucination patterns in state-of-the-art LLMs on financial tabular data. Our work provides a robust methodology for in-house LLM evaluation and serves as a critical step toward building more trustworthy and reliable financial Generative AI systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidding-Aware Retrieval for Multi-Stage Consistency in Online Advertising</title>
<link>https://arxiv.org/abs/2508.05206</link>
<guid>https://arxiv.org/abs/2508.05206</guid>
<content:encoded><![CDATA[
arXiv:2508.05206v1 Announce Type: new 
Abstract: Online advertising systems typically use a cascaded architecture to manage massive requests and candidate volumes, where the ranking stages allocate traffic based on eCPM (predicted CTR $\times$ Bid). With the increasing popularity of auto-bidding strategies, the inconsistency between the computationally sensitive retrieval stage and the ranking stages becomes more pronounced, as the former cannot access precise, real-time bids for the vast ad corpus. This discrepancy leads to sub-optimal platform revenue and advertiser outcomes. To tackle this problem, we propose Bidding-Aware Retrieval (BAR), a model-based retrieval framework that addresses multi-stage inconsistency by incorporating ad bid value into the retrieval scoring function. The core innovation is Bidding-Aware Modeling, incorporating bid signals through monotonicity-constrained learning and multi-task distillation to ensure economically coherent representations, while Asynchronous Near-Line Inference enables real-time updates to the embedding for market responsiveness. Furthermore, the Task-Attentive Refinement module selectively enhances feature interactions to disentangle user interest and commercial value signals. Extensive offline experiments and full-scale deployment across Alibaba's display advertising platform validated BAR's efficacy: 4.32% platform revenue increase with 22.2% impression lift for positively-operated advertisements.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Hybrid Transformer LSTM Technique with Attention and TS Mixer for Drilling Rate of Penetration Prediction</title>
<link>https://arxiv.org/abs/2508.05210</link>
<guid>https://arxiv.org/abs/2508.05210</guid>
<content:encoded><![CDATA[
arXiv:2508.05210v1 Announce Type: new 
Abstract: The Rate of Penetration (ROP) is crucial for optimizing drilling operations; however, accurately predicting it is hindered by the complex, dynamic, and high-dimensional nature of drilling data. Traditional empirical, physics-based, and basic machine learning models often fail to capture intricate temporal and contextual relationships, resulting in suboptimal predictions and limited real-time utility. To address this gap, we propose a novel hybrid deep learning architecture integrating Long Short-Term Memory (LSTM) networks, Transformer encoders, Time-Series Mixer (TS-Mixer) blocks, and attention mechanisms to synergistically model temporal dependencies, static feature interactions, global context, and dynamic feature importance. Evaluated on a real-world drilling dataset, our model outperformed benchmarks (standalone LSTM, TS-Mixer, and simpler hybrids) with an R-squared score of 0.9988 and a Mean Absolute Percentage Error of 1.447%, as measured by standard regression metrics (R-squared, MAE, RMSE, MAPE). Model interpretability was ensured using SHAP and LIME, while actual vs. predicted curves and bias checks confirmed accuracy and fairness across scenarios. This advanced hybrid approach enables reliable real-time ROP prediction, paving the way for intelligent, cost-effective drilling optimization systems with significant operational impact.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFW: A Novel Weighting Scheme for Covariate Balancing and Treatment Effect Estimation</title>
<link>https://arxiv.org/abs/2508.05215</link>
<guid>https://arxiv.org/abs/2508.05215</guid>
<content:encoded><![CDATA[
arXiv:2508.05215v1 Announce Type: new 
Abstract: Estimating causal effects from observational data is challenging due to selection bias, which leads to imbalanced covariate distributions across treatment groups. Propensity score-based weighting methods are widely used to address this issue by reweighting samples to simulate a randomized controlled trial (RCT). However, the effectiveness of these methods heavily depends on the observed data and the accuracy of the propensity score estimator. For example, inverse propensity weighting (IPW) assigns weights based on the inverse of the propensity score, which can lead to instable weights when propensity scores have high variance-either due to data or model misspecification-ultimately degrading the ability of handling selection bias and treatment effect estimation. To overcome these limitations, we propose Deconfounding Factor Weighting (DFW), a novel propensity score-based approach that leverages the deconfounding factor-to construct stable and effective sample weights. DFW prioritizes less confounded samples while mitigating the influence of highly confounded ones, producing a pseudopopulation that better approximates a RCT. Our approach ensures bounded weights, lower variance, and improved covariate balance.While DFW is formulated for binary treatments, it naturally extends to multi-treatment settings, as the deconfounding factor is computed based on the estimated probability of the treatment actually received by each sample. Through extensive experiments on real-world benchmark and synthetic datasets, we demonstrate that DFW outperforms existing methods, including IPW and CBPS, in both covariate balancing and treatment effect estimation.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ML-based Short Physical Performance Battery future score prediction based on questionnaire data</title>
<link>https://arxiv.org/abs/2508.05222</link>
<guid>https://arxiv.org/abs/2508.05222</guid>
<content:encoded><![CDATA[
arXiv:2508.05222v1 Announce Type: new 
Abstract: Effective slowing down of older adults\' physical capacity deterioration requires intervention as soon as the first symptoms surface. In this paper, we analyze the possibility of predicting the Short Physical Performance Battery (SPPB) score at a four-year horizon based on questionnaire data. The ML algorithms tested included Random Forest, XGBoost, Linear Regression, dense and TabNet neural networks. The best results were achieved for the XGBoost (mean absolute error of 0.79 points). Based on the Shapley values analysis, we selected smaller subsets of features (from 10 to 20) and retrained the XGBoost regressor, achieving a mean absolute error of 0.82.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning</title>
<link>https://arxiv.org/abs/2508.05224</link>
<guid>https://arxiv.org/abs/2508.05224</guid>
<content:encoded><![CDATA[
arXiv:2508.05224v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across distributed clients while preserving data privacy by keeping data local. Traditional FL approaches rely on a centralized, star-shaped topology, where a central server aggregates model updates from clients. However, this architecture introduces several limitations, including a single point of failure, limited personalization, and poor robustness to distribution shifts or vulnerability to malfunctioning clients. Moreover, update selection in centralized FL often relies on low-level parameter differences, which can be unreliable when client data is not independent and identically distributed, and offer clients little control. In this work, we propose a decentralized, peer-to-peer (P2P) FL framework. It leverages the flexibility of the P2P topology to enable each client to identify and aggregate a personalized set of trustworthy and beneficial updates.This framework is the Local Inference Guided Aggregation for Heterogeneous Training Environments to Yield Enhancement Through Agreement and Regularization (LIGHTYEAR). Central to our method is an agreement score, computed on a local validation set, which quantifies the semantic alignment of incoming updates in the function space with respect to the clients reference model. Each client uses this score to select a tailored subset of updates and performs aggregation with a regularization term that further stabilizes the training. Our empirical evaluation across two datasets shows that the proposed approach consistently outperforms both centralized baselines and existing P2P methods in terms of client-level performance, particularly under adversarial and heterogeneous conditions.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs</title>
<link>https://arxiv.org/abs/2508.05232</link>
<guid>https://arxiv.org/abs/2508.05232</guid>
<content:encoded><![CDATA[
arXiv:2508.05232v1 Announce Type: new 
Abstract: Traditional parameter-efficient fine-tuning (PEFT) methods such as LoRA are tightly coupled with the base model architecture, which constrains their applicability across heterogeneous pretrained large language models (LLMs). To address this limitation, we introduce Cross-LoRA, a data-free framework for transferring LoRA modules between diverse base models without requiring additional training data. Cross-LoRA consists of two key components: (a) LoRA-Align, which performs subspace alignment between source and target base models through rank-truncated singular value decomposition (SVD) and Frobenius-optimal linear transformation, ensuring compatibility under dimension mismatch; and (b) LoRA-Shift, which applies the aligned subspaces to project source LoRA weight updates into the target model parameter space. Both components are data-free, training-free, and enable lightweight adaptation on a commodity GPU in 20 minutes. Experiments on ARCs, OBOA and HellaSwag show that Cross-LoRA achieves relative gains of up to 5.26% over base models. Across other commonsense reasoning benchmarks, Cross-LoRA maintains performance comparable to that of directly trained LoRA adapters.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs</title>
<link>https://arxiv.org/abs/2508.05257</link>
<guid>https://arxiv.org/abs/2508.05257</guid>
<content:encoded><![CDATA[
arXiv:2508.05257v1 Announce Type: new 
Abstract: The Mixture-of-Experts (MoE) architecture has become a predominant paradigm for scaling large language models (LLMs). Despite offering strong performance and computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and Kimi-K2-Instruct present serious challenges due to substantial memory requirements in deployment. While recent works have explored MoE compression to address this issue, existing methods often suffer from considerable accuracy drops (e.g., 7-14% relatively) even at modest compression rates. This paper introduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model compression while incurring minimal accuracy drops. Specifically, each up/gate matrix in an expert is decomposed via a rank decomposition as W = AB, where matrix A is unique to each expert. The relatively larger matrix B is further re-parameterized as a linear combination of basis matrices {Bi} shared across all experts within a given MoE layer. The factorization is learned by minimizing the reconstruction error relative to the original weight matrices. Experiments demonstrate that MoBE achieves notably lower accuracy drops compared to prior works. For instance, MoBE can reduce the parameter counts of Qwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by 24%-30% with only 1%-2% accuracy drop (about 2% drops when measured relatively).
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marine Chlorophyll Prediction and Driver Analysis based on LSTM-RF Hybrid Models</title>
<link>https://arxiv.org/abs/2508.05260</link>
<guid>https://arxiv.org/abs/2508.05260</guid>
<content:encoded><![CDATA[
arXiv:2508.05260v1 Announce Type: new 
Abstract: Marine chlorophyll concentration is an important indicator of ecosystem health and carbon cycle strength, and its accurate prediction is crucial for red tide warning and ecological response. In this paper, we propose a LSTM-RF hybrid model that combines the advantages of LSTM and RF, which solves the deficiencies of a single model in time-series modelling and nonlinear feature portrayal. Trained with multi-source ocean data(temperature, salinity, dissolved oxygen, etc.), the experimental results show that the LSTM-RF model has an R^2 of 0.5386, an MSE of 0.005806, and an MAE of 0.057147 on the test set, which is significantly better than using LSTM (R^2 = 0.0208) and RF (R^2 =0.4934) alone , respectively. The standardised treatment and sliding window approach improved the prediction accuracy of the model and provided an innovative solution for high-frequency prediction of marine ecological variables.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowState: Sampling Rate Invariant Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.05287</link>
<guid>https://arxiv.org/abs/2508.05287</guid>
<content:encoded><![CDATA[
arXiv:2508.05287v1 Announce Type: new 
Abstract: Foundation models (FMs) have transformed natural language processing, but their success has not yet translated to time series forecasting. Existing time series foundation models (TSFMs), often based on transformer variants, struggle with generalization across varying context and target lengths, lack adaptability to different sampling rates, and are computationally inefficient. We introduce FlowState, a novel TSFM architecture that addresses these challenges through two key innovations: a state space model (SSM) based encoder and a functional basis decoder. This design enables continuous-time modeling and dynamic time-scale adjustment, allowing FlowState to inherently generalize across all possible temporal resolutions, and dynamically adjust the forecasting horizons. In contrast to other state-of-the-art TSFMs, which require training data across all possible sampling rates to memorize patterns at each scale, FlowState inherently adapts its internal dynamics to the input scale, enabling smaller models, reduced data requirements, and improved efficiency. We further propose an efficient pretraining strategy that improves robustness and accelerates training. Despite being the smallest model, FlowState outperforms all other models and is state-of-the-art for the GIFT-ZS and the Chronos-ZS benchmarks. Ablation studies confirm the effectiveness of its components, and we demonstrate its unique ability to adapt online to varying input sampling rates.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders</title>
<link>https://arxiv.org/abs/2508.05289</link>
<guid>https://arxiv.org/abs/2508.05289</guid>
<content:encoded><![CDATA[
arXiv:2508.05289v1 Announce Type: new 
Abstract: Conversational recommender systems (CRS) based on Large Language Models (LLMs) need to constantly be aligned to the user preferences to provide satisfying and context-relevant item recommendations. The traditional supervised fine-tuning cannot capture the implicit feedback signal, e.g., dwell time, sentiment polarity, or engagement patterns. In this paper, we share a fine-tuning solution using human feedback reinforcement learning (RLHF) to maximize implied user feedback (IUF) in a multi-turn recommendation context. We specify a reward model $R_{\phi}$ learnt on weakly-labelled engagement information and maximize user-centric utility by optimizing the foundational LLM M_{\theta} through a proximal policy optimization (PPO) approach. The architecture models conversational state transitions $s_t \to a_t \to s_{t +1}$, where the action $a_t$ is associated with LLM-generated item suggestions only on condition of conversation history in the past. The evaluation across synthetic and real-world datasets (e.g.REDIAL, OpenDialKG) demonstrates that our RLHF-fine-tuned models can perform better in terms of top-$k$ recommendation accuracy, coherence, and user satisfaction compared to (arrow-zero-cmwrquca-teja-falset ensuite 2Round group-deca States penalty give up This paper shows that implicit signal alignment can be efficient in achieving scalable and user-adaptive design of CRS.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Growth Schedules for Batch Size and Learning Rate in SGD that Reduce SFO Complexity</title>
<link>https://arxiv.org/abs/2508.05297</link>
<guid>https://arxiv.org/abs/2508.05297</guid>
<content:encoded><![CDATA[
arXiv:2508.05297v1 Announce Type: new 
Abstract: The unprecedented growth of deep learning models has enabled remarkable advances but introduced substantial computational bottlenecks. A key factor contributing to training efficiency is batch-size and learning-rate scheduling in stochastic gradient methods. However, naive scheduling of these hyperparameters can degrade optimization efficiency and compromise generalization. Motivated by recent theoretical insights, we investigated how the batch size and learning rate should be increased during training to balance efficiency and convergence. We analyzed this problem on the basis of stochastic first-order oracle (SFO) complexity, defined as the expected number of gradient evaluations needed to reach an $\epsilon$-approximate stationary point of the empirical loss. We theoretically derived optimal growth schedules for the batch size and learning rate that reduce SFO complexity and validated them through extensive experiments. Our results offer both theoretical insights and practical guidelines for scalable and efficient large-batch training in deep learning.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Batch Size and Learning Rate Scheduler for Stochastic Gradient Descent Based on Minimization of Stochastic First-order Oracle Complexity</title>
<link>https://arxiv.org/abs/2508.05302</link>
<guid>https://arxiv.org/abs/2508.05302</guid>
<content:encoded><![CDATA[
arXiv:2508.05302v1 Announce Type: new 
Abstract: The convergence behavior of mini-batch stochastic gradient descent (SGD) is highly sensitive to the batch size and learning rate settings. Recent theoretical studies have identified the existence of a critical batch size that minimizes stochastic first-order oracle (SFO) complexity, defined as the expected number of gradient evaluations required to reach a stationary point of the empirical loss function in a deep neural network. An adaptive scheduling strategy is introduced to accelerate SGD that leverages theoretical findings on the critical batch size. The batch size and learning rate are adjusted on the basis of the observed decay in the full gradient norm during training. Experiments using an adaptive joint scheduler based on this strategy demonstrated improved convergence speed compared with that of existing schedulers.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning</title>
<link>https://arxiv.org/abs/2508.05310</link>
<guid>https://arxiv.org/abs/2508.05310</guid>
<content:encoded><![CDATA[
arXiv:2508.05310v1 Announce Type: new 
Abstract: Human teaching effort is a significant bottleneck for the broader applicability of interactive imitation learning. To reduce the number of required queries, existing methods employ active learning to query the human teacher only in uncertain, risky, or novel situations. However, during these queries, the novice's planned actions are not utilized despite containing valuable information, such as the novice's capabilities, as well as corresponding uncertainty levels. To this end, we allow the novice to say: "I plan to do this, but I am uncertain." We introduce the Active Skill-level Data Aggregation (ASkDAgger) framework, which leverages teacher feedback on the novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating threshold to track sensitivity, specificity, or a minimum success rate; (2) Foresight Interactive Experience Replay (FIER), which recasts valid and relabeled novice action plans into demonstrations; and (3) Prioritized Interactive Experience Replay (PIER), which prioritizes replay based on uncertainty, novice success, and demonstration age. Together, these components balance query frequency with failure incidence, reduce the number of required demonstration annotations, improve generalization, and speed up adaptation to changing domains. We validate the effectiveness of ASkDAgger through language-conditioned manipulation tasks in both simulation and real-world environments. Code, data, and videos are available at https://askdagger.github.io.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning</title>
<link>https://arxiv.org/abs/2508.05316</link>
<guid>https://arxiv.org/abs/2508.05316</guid>
<content:encoded><![CDATA[
arXiv:2508.05316v1 Announce Type: new 
Abstract: Semi-supervised continual learning (SSCL) seeks to leverage both labeled and unlabeled data in a sequential learning setup, aiming to reduce annotation costs while managing continual data arrival. SSCL introduces complex challenges, including ensuring effective unlabeled learning (UL), while balancing memory stability (MS) and learning plasticity (LP). Previous SSCL efforts have typically focused on isolated aspects of the three, while this work presents USP, a divide-and-conquer framework designed to synergistically enhance these three aspects: (1) Feature Space Reservation (FSR) strategy for LP, which constructs reserved feature locations for future classes by shaping old classes into an equiangular tight frame; (2) Divide-and-Conquer Pseudo-labeling (DCP) approach for UL, which assigns reliable pseudo-labels across both high- and low-confidence unlabeled data; and (3) Class-mean-anchored Unlabeled Distillation (CUD) for MS, which reuses DCP's outputs to anchor unlabeled data to stable class means for distillation to prevent forgetting. Comprehensive evaluations show USP outperforms prior SSCL methods, with gains up to 5.94% in the last accuracy, validating its effectiveness. The code is available at https://github.com/NJUyued/USP4SSCL.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Corpus Aware Training for Neural Machine Translation</title>
<link>https://arxiv.org/abs/2508.05364</link>
<guid>https://arxiv.org/abs/2508.05364</guid>
<content:encoded><![CDATA[
arXiv:2508.05364v1 Announce Type: new 
Abstract: Corpus Aware Training (CAT) leverages valuable corpus metadata during training by injecting corpus information into each training example, and has been found effective in the literature, commonly known as the "tagging" approach. Models trained with CAT inherently learn the quality, domain and nuance between corpora directly from data, and can easily switch to different inference behavior. To achieve the best evaluation, CAT models pre-define a group of high quality data before training starts which can be error-prone and inefficient. In this work, we propose Optimal Corpus Aware Training (OCAT), which fine-tunes a CAT pre-trained model by freezing most of the model parameters and only tuning small set of corpus-related parameters. We show that OCAT is lightweight, resilient to overfitting, and effective in boosting model accuracy. We use WMT23 English to Chinese and English to German translation tasks as our test ground and show +3.6 and +1.8 chrF improvement, respectively, over vanilla training. Furthermore, our approach is on-par or slightly better than other state-of-the-art fine-tuning techniques while being less sensitive to hyperparameter settings.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Preference Bandits</title>
<link>https://arxiv.org/abs/2508.05367</link>
<guid>https://arxiv.org/abs/2508.05367</guid>
<content:encoded><![CDATA[
arXiv:2508.05367v1 Announce Type: new 
Abstract: Bandit algorithms are guaranteed to solve diverse sequential decision-making problems, provided that a sufficient exploration budget is available. However, learning from scratch is often too costly for personalization tasks where a single individual faces only a small number of decision points. Latent bandits offer substantially reduced exploration times for such problems, given that the joint distribution of a latent state and the rewards of actions is known and accurate. In practice, finding such a model is non-trivial, and there may not exist a small number of latent states that explain the responses of all individuals. For example, patients with similar latent conditions may have the same preference in treatments but rate their symptoms on different scales. With this in mind, we propose relaxing the assumptions of latent bandits to require only a model of the \emph{preference ordering} of actions in each latent state. This allows problem instances with the same latent state to vary in their reward distributions, as long as their preference orderings are equal. We give a posterior-sampling algorithm for this problem and demonstrate that its empirical performance is competitive with latent bandits that have full knowledge of the reward distribution when this is well-specified, and outperforms them when reward scales differ between instances with the same latent state.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms</title>
<link>https://arxiv.org/abs/2508.05387</link>
<guid>https://arxiv.org/abs/2508.05387</guid>
<content:encoded><![CDATA[
arXiv:2508.05387v1 Announce Type: new 
Abstract: Modern RL-based post-training for large language models (LLMs) co-locate trajectory sampling and policy optimisation on the same GPU cluster, forcing the system to switch between inference and training workloads. This serial context switching violates the single-program-multiple-data (SPMD) assumption underlying today's distributed training systems. We present Echo, the RL system that cleanly decouples these two phases across heterogeneous "inference" and "training" swarms while preserving statistical efficiency. Echo introduces two lightweight synchronization protocols: a sequential pull mode that refreshes sampler weights on every API call for minimal bias, and an asynchronous push-pull mode that streams version-tagged rollouts through a replay buffer to maximise hardware utilisation. Training three representative RL workloads with Qwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster, Echo matches a fully co-located Verl baseline in convergence speed and final reward while off-loading trajectory generation to commodity edge hardware. These promising results demonstrate that large-scale RL for LLMs could achieve datacentre-grade performance using decentralised, heterogeneous resources.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning</title>
<link>https://arxiv.org/abs/2508.05404</link>
<guid>https://arxiv.org/abs/2508.05404</guid>
<content:encoded><![CDATA[
arXiv:2508.05404v1 Announce Type: new 
Abstract: Recent studies have shown that deep neural networks (DNNs) are vulnerable to backdoor attacks, where a designed trigger is injected into the dataset, causing erroneous predictions when activated. In this paper, we propose a novel defense mechanism, Non-target label Training and Mutual Learning (NT-ML), which can successfully restore the poisoned model under advanced backdoor attacks. NT aims to reduce the harm of poisoned data by retraining the model with the outputs of the standard training. At this stage, a teacher model with high accuracy on clean data and a student model with higher confidence in correct prediction on poisoned data are obtained. Then, the teacher and student can learn the strengths from each other through ML to obtain a purified student model. Extensive experiments show that NT-ML can effectively defend against 6 backdoor attacks with a small number of clean samples, and outperforms 5 state-of-the-art backdoor defenses.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cumulative Learning Rate Adaptation: Revisiting Path-Based Schedules for SGD and Adam</title>
<link>https://arxiv.org/abs/2508.05408</link>
<guid>https://arxiv.org/abs/2508.05408</guid>
<content:encoded><![CDATA[
arXiv:2508.05408v1 Announce Type: new 
Abstract: The learning rate is a crucial hyperparameter in deep learning, with its ideal value depending on the problem and potentially changing during training. In this paper, we investigate the practical utility of adaptive learning rate mechanisms that adjust step sizes dynamically in response to the loss landscape. We revisit a cumulative path-based adaptation scheme proposed in 2017, which adjusts the learning rate based on the discrepancy between the observed path length, computed as a time-discounted sum of normalized gradient steps, and the expected length of a random walk. While the original approach offers a compelling intuition, we show that its adaptation mechanism for Adam is conceptually inconsistent due to the optimizer's internal preconditioning. We propose a corrected variant that better reflects Adam's update dynamics. To assess the practical value of online learning rate adaptation, we benchmark SGD and Adam, with and without cumulative adaptation, and compare them to a recent alternative method. Our results aim to clarify when and why such adaptive strategies offer practical benefits.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow</title>
<link>https://arxiv.org/abs/2508.05411</link>
<guid>https://arxiv.org/abs/2508.05411</guid>
<content:encoded><![CDATA[
arXiv:2508.05411v1 Announce Type: new 
Abstract: Molecular generation conditioned on textual descriptions is a fundamental task in computational chemistry and drug discovery. Existing methods often struggle to simultaneously ensure high-quality, diverse generation and fast inference. In this work, we propose a novel causality-aware framework that addresses these challenges through two key innovations. First, we introduce a Causality-Aware Transformer (CAT) that jointly encodes molecular graph tokens and text instructions while enforcing causal dependencies during generation. Second, we develop a Variational Mean Flow (VMF) framework that generalizes existing flow-based methods by modeling the latent space as a mixture of Gaussians, enhancing expressiveness beyond unimodal priors. VMF enables efficient one-step inference while maintaining strong generation quality and diversity. Extensive experiments on four standard molecular benchmarks demonstrate that our model outperforms state-of-the-art baselines, achieving higher novelty (up to 74.5\%), diversity (up to 70.3\%), and 100\% validity across all datasets. Moreover, VMF requires only one number of function evaluation (NFE) during conditional generation and up to five NFEs for unconditional generation, offering substantial computational efficiency over diffusion-based methods.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo State Networks for Bitcoin Time Series Prediction</title>
<link>https://arxiv.org/abs/2508.05416</link>
<guid>https://arxiv.org/abs/2508.05416</guid>
<content:encoded><![CDATA[
arXiv:2508.05416v1 Announce Type: new 
Abstract: Forecasting stock and cryptocurrency prices is challenging due to high volatility and non-stationarity, influenced by factors like economic changes and market sentiment. Previous research shows that Echo State Networks (ESNs) can effectively model short-term stock market movements, capturing nonlinear patterns in dynamic data. To the best of our knowledge, this work is among the first to explore ESNs for cryptocurrency forecasting, especially during extreme volatility. We also conduct chaos analysis through the Lyapunov exponent in chaotic periods and show that our approach outperforms existing machine learning methods by a significant margin. Our findings are consistent with the Lyapunov exponent analysis, showing that ESNs are robust during chaotic periods and excel under high chaos compared to Boosting and Na\"ive methods.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negative Binomial Variational Autoencoders for Overdispersed Latent Modeling</title>
<link>https://arxiv.org/abs/2508.05423</link>
<guid>https://arxiv.org/abs/2508.05423</guid>
<content:encoded><![CDATA[
arXiv:2508.05423v1 Announce Type: new 
Abstract: Biological neurons communicate through spike trains, discrete, irregular bursts of activity that exhibit variability far beyond the modeling capacity of conventional variational autoencoders (VAEs). Recent work, such as the Poisson-VAE, makes a biologically inspired move by modeling spike counts using the Poisson distribution. However, they impose a rigid constraint: equal mean and variance, which fails to reflect the true stochastic nature of neural activity. In this work, we challenge this constraint and introduce NegBio-VAE, a principled extension of the VAE framework that models spike counts using the negative binomial distribution. This shift grants explicit control over dispersion, unlocking a broader and more accurate family of neural representations. We further develop two ELBO optimization schemes and two differentiable reparameterization strategies tailored to the negative binomial setting. By introducing one additional dispersion parameter, NegBio-VAE generalizes the Poisson latent model to a negative binomial formulation. Empirical results demonstrate this minor yet impactful change leads to significant gains in reconstruction fidelity, highlighting the importance of explicitly modeling overdispersion in spike-like activations.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Multi-Objective Learning with Controlled Pareto Frontiers</title>
<link>https://arxiv.org/abs/2508.05424</link>
<guid>https://arxiv.org/abs/2508.05424</guid>
<content:encoded><![CDATA[
arXiv:2508.05424v1 Announce Type: new 
Abstract: Federated learning (FL) is a widely adopted paradigm for privacy-preserving model training, but FedAvg optimise for the majority while under-serving minority clients. Existing methods such as federated multi-objective learning (FMOL) attempts to import multi-objective optimisation (MOO) into FL. However, it merely delivers task-wise Pareto-stationary points, leaving client fairness to chance. In this paper, we introduce Conically-Regularised FMOL (CR-FMOL), the first federated MOO framework that enforces client-wise Pareto optimality through a novel preference-cone constraint. After local federated multi-gradient descent averaging (FMGDA) / federated stochastic multi-gradient descent averaging (FSMGDA) steps, each client transmits its aggregated task-loss vector as an implicit preference; the server then solves a cone-constrained Pareto-MTL sub-problem centred at the uniform vector, producing a descent direction that is Pareto-stationary for every client within its cone. Experiments on non-IID benchmarks show that CR-FMOL enhances client fairness, and although the early-stage performance is slightly inferior to FedAvg, it is expected to achieve comparable accuracy given sufficient training rounds.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Causal Policy Optimization for Post-Training Large Language Models</title>
<link>https://arxiv.org/abs/2508.05428</link>
<guid>https://arxiv.org/abs/2508.05428</guid>
<content:encoded><![CDATA[
arXiv:2508.05428v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have broadened their applicability across diverse tasks, yet specialized domains still require targeted post training. Among existing methods, Group Relative Policy Optimization (GRPO) stands out for its efficiency, leveraging groupwise relative rewards while avoiding costly value function learning. However, GRPO treats candidate responses as independent, overlooking semantic interactions such as complementarity and contradiction. To address this challenge, we first introduce a Structural Causal Model (SCM) that reveals hidden dependencies among candidate responses induced by conditioning on a final integrated output forming a collider structure. Then, our causal analysis leads to two insights: (1) projecting responses onto a causally informed subspace improves prediction quality, and (2) this projection yields a better baseline than query only conditioning. Building on these insights, we propose Group Causal Policy Optimization (GCPO), which integrates causal structure into optimization through two key components: a causally informed reward adjustment and a novel KL regularization term that aligns the policy with a causally projected reference distribution. Comprehensive experimental evaluations demonstrate that GCPO consistently surpasses existing methods, including GRPO across multiple reasoning benchmarks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search</title>
<link>https://arxiv.org/abs/2508.05433</link>
<guid>https://arxiv.org/abs/2508.05433</guid>
<content:encoded><![CDATA[
arXiv:2508.05433v1 Announce Type: new 
Abstract: Interpretability and high performance are essential goals in designing control policies, particularly for safety-critical tasks. Deep reinforcement learning has greatly enhanced performance, yet its inherent lack of interpretability often undermines trust and hinders real-world deployment. This work addresses these dual challenges by introducing a novel approach for programmatic policy discovery, called Multimodal Large Language Model-assisted Evolutionary Search (MLES). MLES utilizes multimodal large language models as policy generators, combining them with evolutionary mechanisms for automatic policy optimization. It integrates visual feedback-driven behavior analysis within the policy generation process to identify failure patterns and facilitate targeted improvements, enhancing the efficiency of policy discovery and producing adaptable, human-aligned policies. Experimental results show that MLES achieves policy discovery capabilities and efficiency comparable to Proximal Policy Optimization (PPO) across two control tasks, while offering transparent control logic and traceable design processes. This paradigm overcomes the limitations of predefined domain-specific languages, facilitates knowledge transfer and reuse, and is scalable across various control tasks. MLES shows promise as a leading approach for the next generation of interpretable control policy discovery.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competing Risks: Impact on Risk Estimation and Algorithmic Fairness</title>
<link>https://arxiv.org/abs/2508.05435</link>
<guid>https://arxiv.org/abs/2508.05435</guid>
<content:encoded><![CDATA[
arXiv:2508.05435v1 Announce Type: new 
Abstract: Accurate time-to-event prediction is integral to decision-making, informing medical guidelines, hiring decisions, and resource allocation. Survival analysis, the quantitative framework used to model time-to-event data, accounts for patients who do not experience the event of interest during the study period, known as censored patients. However, many patients experience events that prevent the observation of the outcome of interest. These competing risks are often treated as censoring, a practice frequently overlooked due to a limited understanding of its consequences. Our work theoretically demonstrates why treating competing risks as censoring introduces substantial bias in survival estimates, leading to systematic overestimation of risk and, critically, amplifying disparities. First, we formalize the problem of misclassifying competing risks as censoring and quantify the resulting error in survival estimates. Specifically, we develop a framework to estimate this error and demonstrate the associated implications for predictive performance and algorithmic fairness. Furthermore, we examine how differing risk profiles across demographic groups lead to group-specific errors, potentially exacerbating existing disparities. Our findings, supported by an empirical analysis of cardiovascular management, demonstrate that ignoring competing risks disproportionately impacts the individuals most at risk of these events, potentially accentuating inequity. By quantifying the error and highlighting the fairness implications of the common practice of considering competing risks as censoring, our work provides a critical insight into the development of survival models: practitioners must account for competing risks to improve accuracy, reduce disparities in risk assessment, and better inform downstream decisions.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees</title>
<link>https://arxiv.org/abs/2508.05441</link>
<guid>https://arxiv.org/abs/2508.05441</guid>
<content:encoded><![CDATA[
arXiv:2508.05441v1 Announce Type: new 
Abstract: Making decisions with respect to just the expected returns in Monte Carlo Tree Search (MCTS) cannot account for the potential range of high-risk, adverse outcomes associated with a decision. To this end, safety-aware MCTS often consider some constrained variants -- by introducing some form of mean risk measures or hard cost thresholds. These approaches fail to provide rigorous tail-safety guarantees with respect to extreme or high-risk outcomes (denoted as tail-risk), potentially resulting in serious consequence in high-stake scenarios. This paper addresses the problem by developing two novel solutions. We first propose CVaR-MCTS, which embeds a coherent tail risk measure, Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter $\alpha$ achieves explicit tail-risk control over the expected loss in the "worst $(1-\alpha)\%$ scenarios." Second, we further address the estimation bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or W-MCTS) by introducing a first-order Wasserstein ambiguity set $\mathcal{P}_{\varepsilon_{s}}(s,a)$ with radius $\varepsilon_{s}$ to characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety guarantees for both CVaR-MCTS and W-MCTS and establish their regret. Evaluations on diverse simulated environments demonstrate that our proposed methods outperform existing baselines, effectively achieving robust tail-risk guarantees with improved rewards and stability.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting</title>
<link>https://arxiv.org/abs/2508.05454</link>
<guid>https://arxiv.org/abs/2508.05454</guid>
<content:encoded><![CDATA[
arXiv:2508.05454v1 Announce Type: new 
Abstract: Accurate and reliable energy time series prediction is of great significance for power generation planning and allocation. At present, deep learning time series prediction has become the mainstream method. However, the multi-scale time dynamics and the irregularity of real data lead to the limitations of the existing methods. Therefore, we propose EnergyPatchTST, which is an extension of the Patch Time Series Transformer specially designed for energy forecasting. The main innovations of our method are as follows: (1) multi-scale feature extraction mechanism to capture patterns with different time resolutions; (2) probability prediction framework to estimate uncertainty through Monte Carlo elimination; (3) integration path of future known variables (such as temperature and wind conditions); And (4) Pre-training and Fine-tuning examples to enhance the performance of limited energy data sets. A series of experiments on common energy data sets show that EnergyPatchTST is superior to other commonly used methods, the prediction error is reduced by 7-12%, and reliable uncertainty estimation is provided, which provides an important reference for time series prediction in the energy field.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task complexity shapes internal representations and robustness in neural networks</title>
<link>https://arxiv.org/abs/2508.05463</link>
<guid>https://arxiv.org/abs/2508.05463</guid>
<content:encoded><![CDATA[
arXiv:2508.05463v1 Announce Type: new 
Abstract: Neural networks excel across a wide range of tasks, yet remain black boxes. In particular, how their internal representations are shaped by the complexity of the input data and the problems they solve remains obscure. In this work, we introduce a suite of five data-agnostic probes-pruning, binarization, noise injection, sign flipping, and bipartite network randomization-to quantify how task difficulty influences the topology and robustness of representations in multilayer perceptrons (MLPs). MLPs are represented as signed, weighted bipartite graphs from a network science perspective. We contrast easy and hard classification tasks on the MNIST and Fashion-MNIST datasets. We show that binarizing weights in hard-task models collapses accuracy to chance, whereas easy-task models remain robust. We also find that pruning low-magnitude edges in binarized hard-task models reveals a sharp phase-transition in performance. Moreover, moderate noise injection can enhance accuracy, resembling a stochastic-resonance effect linked to optimal sign flips of small-magnitude weights. Finally, preserving only the sign structure-instead of precise weight magnitudes-through bipartite network randomizations suffices to maintain high accuracy. These phenomena define a model- and modality-agnostic measure of task complexity: the performance gap between full-precision and binarized or shuffled neural network performance. Our findings highlight the crucial role of signed bipartite topology in learned representations and suggest practical strategies for model compression and interpretability that align with task complexity.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes</title>
<link>https://arxiv.org/abs/2508.05469</link>
<guid>https://arxiv.org/abs/2508.05469</guid>
<content:encoded><![CDATA[
arXiv:2508.05469v1 Announce Type: new 
Abstract: We develop mechanisms for evaluating AI systems without ground truth by exploiting a connection between gaming resistance and output quality. The data processing inequality ensures post-hoc attempts to game a metric degrades both information content and task performance. We prove that f-mutual information measures are the unique gaming resistant mechanisms under natural conditions, with the overseer acting as an agent. While Shannon mutual information faces exponential sample complexity, bounded measures like total variation distance remain tractable. Empirically, across ten domains from translation to peer review, all information-theoretic mechanisms achieve perfect discrimination (d > 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit systematic evaluation inversion, preferring fabricated content over accurate summaries. Our mechanisms show 10-100x better robustness to adversarial manipulation than current practices. We also find performance follows an inverted-U curve with compression ratio, peaking at 10:1 where agent responses exhibit optimal information diversity (3 effective dimensions), giving a bias-variance perspective on when our approach is expected to be most effective.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of Survival Outcomes under Clinical Presence Shift: A Joint Neural Network Architecture</title>
<link>https://arxiv.org/abs/2508.05472</link>
<guid>https://arxiv.org/abs/2508.05472</guid>
<content:encoded><![CDATA[
arXiv:2508.05472v1 Announce Type: new 
Abstract: Electronic health records arise from the complex interaction between patients and the healthcare system. This observation process of interactions, referred to as clinical presence, often impacts observed outcomes. When using electronic health records to develop clinical prediction models, it is standard practice to overlook clinical presence, impacting performance and limiting the transportability of models when this interaction evolves. We propose a multi-task recurrent neural network that jointly models the inter-observation time and the missingness processes characterising this interaction in parallel to the survival outcome of interest. Our work formalises the concept of clinical presence shift when the prediction model is deployed in new settings (e.g. different hospitals, regions or countries), and we theoretically justify why the proposed joint modelling can improve transportability under changes in clinical presence. We demonstrate, in a real-world mortality prediction task in the MIMIC-III dataset, how the proposed strategy improves performance and transportability compared to state-of-the-art prediction models that do not incorporate the observation process. These results emphasise the importance of leveraging clinical presence to improve performance and create more transportable clinical prediction models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling</title>
<link>https://arxiv.org/abs/2508.05492</link>
<guid>https://arxiv.org/abs/2508.05492</guid>
<content:encoded><![CDATA[
arXiv:2508.05492v1 Announce Type: new 
Abstract: Multimodal electronic health record (EHR) data provide richer, complementary insights into patient health compared to single-modality data. However, effectively integrating diverse data modalities for clinical prediction modeling remains challenging due to the substantial data requirements. We introduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed to leverage multiple large language model (LLM) agents for clinical prediction tasks using multimodal EHR data. MoMA employs specialized LLM agents ("specialist agents") to convert non-textual modalities, such as medical images and laboratory results, into structured textual summaries. These summaries, together with clinical notes, are combined by another LLM ("aggregator agent") to generate a unified multimodal summary, which is then used by a third LLM ("predictor agent") to produce clinical predictions. Evaluating MoMA on three prediction tasks using real-world datasets with different modality combinations and prediction settings, MoMA outperforms current state-of-the-art methods, highlighting its enhanced accuracy and flexibility across various tasks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-free entropy-regularized multi-view clustering with hierarchical feature selection</title>
<link>https://arxiv.org/abs/2508.05504</link>
<guid>https://arxiv.org/abs/2508.05504</guid>
<content:encoded><![CDATA[
arXiv:2508.05504v1 Announce Type: new 
Abstract: Multi-view clustering faces critical challenges in automatically discovering patterns across heterogeneous data while managing high-dimensional features and eliminating irrelevant information. Traditional approaches suffer from manual parameter tuning and lack principled cross-view integration mechanisms. This work introduces two complementary algorithms: AMVFCM-U and AAMVFCM-U, providing a unified parameter-free framework. Our approach replaces fuzzification parameters with entropy regularization terms that enforce adaptive cross-view consensus. The core innovation employs signal-to-noise ratio based regularization ($\delta_j^h = \frac{\bar{x}_j^h}{(\sigma_j^h)^2}$) for principled feature weighting with convergence guarantees, coupled with dual-level entropy terms that automatically balance view and feature contributions. AAMVFCM-U extends this with hierarchical dimensionality reduction operating at feature and view levels through adaptive thresholding ($\theta^{h^{(t)}} = \frac{d_h^{(t)}}{n}$). Evaluation across five diverse benchmarks demonstrates superiority over 15 state-of-the-art methods. AAMVFCM-U achieves up to 97% computational efficiency gains, reduces dimensionality to 0.45% of original size, and automatically identifies critical view combinations for optimal pattern discovery.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tractable Sharpness-Aware Learning of Probabilistic Circuits</title>
<link>https://arxiv.org/abs/2508.05537</link>
<guid>https://arxiv.org/abs/2508.05537</guid>
<content:encoded><![CDATA[
arXiv:2508.05537v1 Announce Type: new 
Abstract: Probabilistic Circuits (PCs) are a class of generative models that allow exact and tractable inference for a wide range of queries. While recent developments have enabled the learning of deep and expressive PCs, this increased capacity can often lead to overfitting, especially when data is limited. We analyze PC overfitting from a log-likelihood-landscape perspective and show that it is often caused by convergence to sharp optima that generalize poorly. Inspired by sharpness aware minimization in neural networks, we propose a Hessian-based regularizer for training PCs. As a key contribution, we show that the trace of the Hessian of the log-likelihood-a sharpness proxy that is typically intractable in deep neural networks-can be computed efficiently for PCs. Minimizing this Hessian trace induces a gradient-norm-based regularizer that yields simple closed-form parameter updates for EM, and integrates seamlessly with gradient based learning methods. Experiments on synthetic and real-world datasets demonstrate that our method consistently guides PCs toward flatter minima, improves generalization performance.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Vision-Language Models Without Labels: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2508.05547</link>
<guid>https://arxiv.org/abs/2508.05547</guid>
<content:encoded><![CDATA[
arXiv:2508.05547v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment</title>
<link>https://arxiv.org/abs/2508.05568</link>
<guid>https://arxiv.org/abs/2508.05568</guid>
<content:encoded><![CDATA[
arXiv:2508.05568v1 Announce Type: new 
Abstract: Vertical Federated Learning (VFL) enables collaborative learning by integrating disjoint feature subsets from multiple clients/parties. However, VFL typically faces two key challenges: i) the requirement for perfectly aligned data samples across all clients (missing features are not allowed); ii) the requirement for joint collaborative inference/prediction involving all clients (it does not support locally independent inference on a single client). To address these challenges, we propose X-VFL, a new VFL framework designed to deal with the non-aligned data samples with (partially) missing features and to support locally independent inference of new data samples for each client. In particular, we design two novel modules in X-VFL: Cross Completion (XCom) and Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing features for non-aligned data samples by leveraging information from other clients. DS-Align aligns local features with completed and global features across all clients within the decision subspace, thus enabling locally independent inference at each client. Moreover, we provide convergence theorems for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$ convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type algorithms, where $T$ denotes the number of training update steps. Extensive experiments on real-world datasets demonstrate that X-VFL significantly outperforms existing methods, e.g., achieving a 15% improvement in accuracy on the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III dataset. These results validate the practical effectiveness and superiority of X-VFL, particularly in scenarios involving partially missing features and locally independent inference.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$</title>
<link>https://arxiv.org/abs/2508.05571</link>
<guid>https://arxiv.org/abs/2508.05571</guid>
<content:encoded><![CDATA[
arXiv:2508.05571v1 Announce Type: new 
Abstract: Quantization-Aware Training (QAT) integrates quantization into the training loop, enabling LLMs to learn robust low-bit representations, and is widely recognized as one of the most promising research directions. All current QAT research focuses on minimizing quantization error on full-precision models, where the full-precision accuracy acts as an upper bound (accuracy ceiling). No existing method has even attempted to surpass this ceiling. To break this ceiling, we propose a new paradigm: raising the ceiling (full-precision model), and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$, the first 2-bit quantization framework for complex-valued LLMs. Specifically, our method leverages the representational advantages of the complex domain to boost full-precision accuracy. We map weights to the fourth roots of unity $\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically optimal 2-bit representation. Importantly, each quantized weight has either a zero real or imaginary part, enabling multiplication-free inference using only additions and element swaps. Experimental results show that Fairy$\pm i$ outperforms the ceiling of existing 2-bit quantization approaches in terms of both PPL and downstream tasks, while maintaining strict storage and compute efficiency. This work opens a new direction for building highly accurate and practical LLMs under extremely low-bit constraints.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models</title>
<link>https://arxiv.org/abs/2508.05581</link>
<guid>https://arxiv.org/abs/2508.05581</guid>
<content:encoded><![CDATA[
arXiv:2508.05581v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities for medical question answering and programming, but their potential for generating interpretable computable phenotypes (CPs) is under-explored. In this work, we investigate whether LLMs can generate accurate and concise CPs for six clinical phenotypes of varying complexity, which could be leveraged to enable scalable clinical decision support to improve care for patients with hypertension. In addition to evaluating zero-short performance, we propose and test a synthesize, execute, debug, instruct strategy that uses LLMs to generate and iteratively refine CPs using data-driven feedback. Our results show that LLMs, coupled with iterative learning, can generate interpretable and reasonably accurate programs that approach the performance of state-of-the-art ML methods while requiring significantly fewer training examples.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing PyKEEN with Multiple Negative Sampling Solutions for Knowledge Graph Embedding Models</title>
<link>https://arxiv.org/abs/2508.05587</link>
<guid>https://arxiv.org/abs/2508.05587</guid>
<content:encoded><![CDATA[
arXiv:2508.05587v1 Announce Type: new 
Abstract: Embedding methods have become popular due to their scalability on link prediction and/or triple classification tasks on Knowledge Graphs. Embedding models are trained relying on both positive and negative samples of triples. However, in the absence of negative assertions, these must be usually artificially generated using various negative sampling strategies, ranging from random corruption to more sophisticated techniques which have an impact on the overall performance. Most of the popular libraries for knowledge graph embedding, support only basic such strategies and lack advanced solutions. To address this gap, we deliver an extension for the popular KGE framework PyKEEN that integrates a suite of several advanced negative samplers (including both static and dynamic corruption strategies), within a consistent modular architecture, to generate meaningful negative samples, while remaining compatible with existing PyKEEN -based workflows and pipelines. The developed extension not only enhancesPyKEEN itself but also allows for easier and comprehensive development of embedding methods and/or for their customization. As a proof of concept, we present a comprehensive empirical study of the developed extensions and their impact on the performance (link prediction tasks) of different embedding methods, which also provides useful insights for the design of more effective strategies
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing IoT Threat Detection with Kolmogorov-Arnold Networks (KANs)</title>
<link>https://arxiv.org/abs/2508.05591</link>
<guid>https://arxiv.org/abs/2508.05591</guid>
<content:encoded><![CDATA[
arXiv:2508.05591v1 Announce Type: new 
Abstract: The exponential growth of the Internet of Things (IoT) has led to the emergence of substantial security concerns, with IoT networks becoming the primary target for cyberattacks. This study examines the potential of Kolmogorov-Arnold Networks (KANs) as an alternative to conventional machine learning models for intrusion detection in IoT networks. The study demonstrates that KANs, which employ learnable activation functions, outperform traditional MLPs and achieve competitive accuracy compared to state-of-the-art models such as Random Forest and XGBoost, while offering superior interpretability for intrusion detection in IoT networks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-omniscient backdoor injection with a single poison sample: Proving the one-poison hypothesis for linear regression and linear classification</title>
<link>https://arxiv.org/abs/2508.05600</link>
<guid>https://arxiv.org/abs/2508.05600</guid>
<content:encoded><![CDATA[
arXiv:2508.05600v1 Announce Type: new 
Abstract: Backdoor injection attacks are a threat to machine learning models that are trained on large data collected from untrusted sources; these attacks enable attackers to inject malicious behavior into the model that can be triggered by specially crafted inputs. Prior work has established bounds on the success of backdoor attacks and their impact on the benign learning task, however, an open question is what amount of poison data is needed for a successful backdoor attack. Typical attacks either use few samples, but need much information about the data points or need to poison many data points.
  In this paper, we formulate the one-poison hypothesis: An adversary with one poison sample and limited background knowledge can inject a backdoor with zero backdooring-error and without significantly impacting the benign learning task performance. Moreover, we prove the one-poison hypothesis for linear regression and linear classification. For adversaries that utilize a direction that is unused by the benign data distribution for the poison sample, we show that the resulting model is functionally equivalent to a model where the poison was excluded from training. We build on prior work on statistical backdoor learning to show that in all other cases, the impact on the benign learning task is still limited. We also validate our theoretical results experimentally with realistic benchmark data sets.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle</title>
<link>https://arxiv.org/abs/2508.05612</link>
<guid>https://arxiv.org/abs/2508.05612</guid>
<content:encoded><![CDATA[
arXiv:2508.05612v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution</title>
<link>https://arxiv.org/abs/2508.05616</link>
<guid>https://arxiv.org/abs/2508.05616</guid>
<content:encoded><![CDATA[
arXiv:2508.05616v1 Announce Type: new 
Abstract: Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at https://github.com/ai4co/trajevo.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification</title>
<link>https://arxiv.org/abs/2508.05629</link>
<guid>https://arxiv.org/abs/2508.05629</guid>
<content:encoded><![CDATA[
arXiv:2508.05629v1 Announce Type: new 
Abstract: We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Generation of 4-Qubits Entangled States</title>
<link>https://arxiv.org/abs/2204.12351</link>
<guid>https://arxiv.org/abs/2204.12351</guid>
<content:encoded><![CDATA[
arXiv:2204.12351v2 Announce Type: cross 
Abstract: We have devised an artificial intelligence algorithm with machine reinforcement learning (Q-learning) to construct remarkable entangled states with 4 qubits. This way, the algorithm is able to generate representative states for some of the 49 true SLOCC classes of the four-qubit entanglement states. In particular, it is possible to reach at least one true SLOCC class for each of the nine entanglement families. The quantum circuits synthesized by the algorithm may be useful for the experimental realization of these important classes of entangled states and to draw conclusions about the intrinsic properties of our universe. We introduce a graphical tool called the state-link graph (SLG) to represent the construction of the Quality matrix (Q-matrix) used by the algorithm to build a given objective state belonging to the corresponding entanglement class. This allows us to discover the necessary connections between specific entanglement features and the role of certain quantum gates that the algorithm needs to include in the quantum gate set of actions. The quantum circuits found are optimal by construction with respect to the quantum gate-set chosen. These SLGs make the algorithm simple, intuitive and a useful resource for the automated construction of entangled states with a low number of qubits.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Use of LLMs for Documentation to Code Traceability</title>
<link>https://arxiv.org/abs/2506.16440</link>
<guid>https://arxiv.org/abs/2506.16440</guid>
<content:encoded><![CDATA[
arXiv:2506.16440v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) offer new potential for automating documentation-to-code traceability, yet their capabilities remain underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5 Sonnet, GPT-4o, and o3-mini) in establishing trace links between various software documentation (including API references and user guides) and source code. We create two novel datasets from two open-source projects (Unity Catalog and Crawl4AI). Through systematic experiments, we assess three key capabilities: (1) trace link identification accuracy, (2) relationship explanation quality, and (3) multi-step chain reconstruction. Results show that the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two datasets, substantially outperforming our baselines (TF-IDF, BM25, and CodeBERT). While fully correct relationship explanations range from 42.9% to 71.1%, partial accuracy exceeds 97%, indicating that fundamental connections are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy but vary in capturing precise intermediate links. Error analysis reveals that many false positives stem from naming-based assumptions, phantom links, or overgeneralization of architectural patterns. We demonstrate that task-framing, such as a one-to-many matching strategy, is critical for performance. These findings position LLMs as powerful assistants for trace discovery, but their limitations could necessitate human-in-the-loop tool design and highlight specific error patterns for future research.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Telegrapher's Generative Model via Kac Flows</title>
<link>https://arxiv.org/abs/2506.20641</link>
<guid>https://arxiv.org/abs/2506.20641</guid>
<content:encoded><![CDATA[
arXiv:2506.20641v3 Announce Type: cross 
Abstract: We break the mold in flow-based generative modeling by proposing a new model based on the damped wave equation, also known as telegrapher's equation. Similar to the diffusion equation and Brownian motion, there is a Feynman-Kac type relation between the telegrapher's equation and the stochastic Kac process in 1D. The Kac flow evolves stepwise linearly in time, so that the probability flow is Lipschitz continuous in the Wasserstein distance and, in contrast to diffusion flows, the norm of the velocity is globally bounded. Furthermore, the Kac model has the diffusion model as its asymptotic limit. We extend these considerations to a multi-dimensional stochastic process which consists of independent 1D Kac processes in each spatial component. We show that this process gives rise to an absolutely continuous curve in the Wasserstein space and compute the conditional velocity field starting in a Dirac point analytically. Using the framework of flow matching, we train a neural network that approximates the velocity field and use it for sample generation. Our numerical experiments demonstrate the scalability of our approach, and show its advantages over diffusion models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow</title>
<link>https://arxiv.org/abs/2507.10818</link>
<guid>https://arxiv.org/abs/2507.10818</guid>
<content:encoded><![CDATA[
arXiv:2507.10818v1 Announce Type: cross 
Abstract: Software libraries are central to the functionality, security, and maintainability of modern code. As developers increasingly turn to Large Language Models (LLMs) to assist with programming tasks, understanding how these models recommend libraries is essential. In this paper, we conduct an empirical study of six state-of-the-art LLMs, both proprietary and open-source, by prompting them to solve real-world Python problems sourced from Stack Overflow. We analyze the types of libraries they import, the characteristics of those libraries, and the extent to which the recommendations are usable out of the box. Our results show that LLMs predominantly favour third-party libraries over standard ones, and often recommend mature, popular, and permissively licensed dependencies. However, we also identify gaps in usability: 4.6% of the libraries could not be resolved automatically due to structural mismatches between import names and installable packages, and only two models (out of six) provided installation guidance. While the generated code is technically valid, the lack of contextual support places the burden of manually resolving dependencies on the user. Our findings offer actionable insights for both developers and researchers, and highlight opportunities to improve the reliability and usability of LLM-generated code in the context of software dependencies.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Rattle to Roar: Optimizer Showdown for MambaStock on S&amp;P 500</title>
<link>https://arxiv.org/abs/2508.04707</link>
<guid>https://arxiv.org/abs/2508.04707</guid>
<content:encoded><![CDATA[
arXiv:2508.04707v1 Announce Type: cross 
Abstract: We evaluate the performance of several optimizers on the task of forecasting S&amp;P 500 Index returns with the MambaStock model. Among the most widely used algorithms, gradient-smoothing and adaptive-rate optimizers (for example, Adam and RMSProp) yield the lowest test errors. In contrast, the Lion optimizer offers notably faster training. To combine these advantages, we introduce a novel family of optimizers, Roaree, that dampens the oscillatory loss behavior often seen with Lion while preserving its training speed.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Generative Recommendations with Context Parallelism on Hierarchical Sequential Transducers</title>
<link>https://arxiv.org/abs/2508.04711</link>
<guid>https://arxiv.org/abs/2508.04711</guid>
<content:encoded><![CDATA[
arXiv:2508.04711v1 Announce Type: cross 
Abstract: Large-scale recommendation systems are pivotal to process an immense volume of daily user interactions, requiring the effective modeling of high cardinality and heterogeneous features to ensure accurate predictions. In prior work, we introduced Hierarchical Sequential Transducers (HSTU), an attention-based architecture for modeling high cardinality, non-stationary streaming recommendation data, providing good scaling law in the generative recommender framework (GR). Recent studies and experiments demonstrate that attending to longer user history sequences yields significant metric improvements. However, scaling sequence length is activation-heavy, necessitating parallelism solutions to effectively shard activation memory. In transformer-based LLMs, context parallelism (CP) is a commonly used technique that distributes computation along the sequence-length dimension across multiple GPUs, effectively reducing memory usage from attention activations. In contrast, production ranking models typically utilize jagged input tensors to represent user interaction features, introducing unique CP implementation challenges. In this work, we introduce context parallelism with jagged tensor support for HSTU attention, establishing foundational capabilities for scaling up sequence dimensions. Our approach enables a 5.3x increase in supported user interaction sequence length, while achieving a 1.55x scaling factor when combined with Distributed Data Parallelism (DDP).
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prescriptive Agents based on Rag for Automated Maintenance (PARAM)</title>
<link>https://arxiv.org/abs/2508.04714</link>
<guid>https://arxiv.org/abs/2508.04714</guid>
<content:encoded><![CDATA[
arXiv:2508.04714v1 Announce Type: cross 
Abstract: Industrial machinery maintenance requires timely intervention to prevent catastrophic failures and optimize operational efficiency. This paper presents an integrated Large Language Model (LLM)-based intelligent system for prescriptive maintenance that extends beyond traditional anomaly detection to provide actionable maintenance recommendations. Building upon our prior LAMP framework for numerical data analysis, we develop a comprehensive solution that combines bearing vibration frequency analysis with multi agentic generation for intelligent maintenance planning. Our approach serializes bearing vibration data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM processing, enabling few-shot anomaly detection with high accuracy. The system classifies fault types (inner race, outer race, ball/roller, cage faults) and assesses severity levels. A multi-agentic component processes maintenance manuals using vector embeddings and semantic search, while also conducting web searches to retrieve comprehensive procedural knowledge and access up-to-date maintenance practices for more accurate and in-depth recommendations. The Gemini model then generates structured maintenance recommendations includes immediate actions, inspection checklists, corrective measures, parts requirements, and timeline specifications. Experimental validation in bearing vibration datasets demonstrates effective anomaly detection and contextually relevant maintenance guidance. The system successfully bridges the gap between condition monitoring and actionable maintenance planning, providing industrial practitioners with intelligent decision support. This work advances the application of LLMs in industrial maintenance, offering a scalable framework for prescriptive maintenance across machinery components and industrial sectors.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoFlow: Agentic Workflow Automation for Geospatial Tasks</title>
<link>https://arxiv.org/abs/2508.04719</link>
<guid>https://arxiv.org/abs/2508.04719</guid>
<content:encoded><![CDATA[
arXiv:2508.04719v1 Announce Type: cross 
Abstract: We present GeoFlow, a method that automatically generates agentic workflows for geospatial tasks. Unlike prior work that focuses on reasoning decomposition and leaves API selection implicit, our method provides each agent with detailed tool-calling objectives to guide geospatial API invocation at runtime. GeoFlow increases agentic success by 6.8% and reduces token usage by up to fourfold across major LLM families compared to state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding protein function with a multimodal retrieval-augmented foundation model</title>
<link>https://arxiv.org/abs/2508.04724</link>
<guid>https://arxiv.org/abs/2508.04724</guid>
<content:encoded><![CDATA[
arXiv:2508.04724v1 Announce Type: cross 
Abstract: Protein language models (PLMs) learn probability distributions over natural protein sequences. By learning from hundreds of millions of natural protein sequences, protein understanding and design capabilities emerge. Recent works have shown that scaling these models improves structure prediction, but does not seem to improve mutation understanding and representation quality for protein function prediction. We introduce PoET-2, a multimodal, retrieval-augmented protein foundation model that incorporates in-context learning of family-specific evolutionary constraints with optional structure conditioning to learn generative distributions over protein sequences. PoET-2 uses a hierarchical transformer encoder that is equivariant to sequence context ordering and a dual decoder architecture with both causal and masked language modeling objectives, allowing PoET-2 to operate in both fully generative and bidirectional representation learning modes. PoET-2 achieves state-of-the-art performance on zero-shot variant effect prediction, excelling at scoring variants with multiple mutations and challenging indel mutations. In supervised settings, PoET-2 embeddings outperform previous methods for learning sequence-function relationships, especially with small datasets. This work highlights the benefits of combining retrieval augmentation with multimodal, family-centric modeling for advancing protein foundation models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodonMoE: DNA Language Models for mRNA Analyses</title>
<link>https://arxiv.org/abs/2508.04739</link>
<guid>https://arxiv.org/abs/2508.04739</guid>
<content:encoded><![CDATA[
arXiv:2508.04739v1 Announce Type: cross 
Abstract: Genomic language models (gLMs) face a fundamental efficiency challenge: either maintain separate specialized models for each biological modality (DNA and RNA) or develop large multi-modal architectures. Both approaches impose significant computational burdens - modality-specific models require redundant infrastructure despite inherent biological connections, while multi-modal architectures demand massive parameter counts and extensive cross-modality pretraining. To address this limitation, we introduce CodonMoE (Adaptive Mixture of Codon Reformative Experts), a lightweight adapter that transforms DNA language models into effective RNA analyzers without RNA-specific pretraining. Our theoretical analysis establishes CodonMoE as a universal approximator at the codon level, capable of mapping arbitrary functions from codon sequences to RNA properties given sufficient expert capacity. Across four RNA prediction tasks spanning stability, expression, and regulation, DNA models augmented with CodonMoE significantly outperform their unmodified counterparts, with HyenaDNA+CodonMoE series achieving state-of-the-art results using 80% fewer parameters than specialized RNA models. By maintaining sub-quadratic complexity while achieving superior performance, our approach provides a principled path toward unifying genomic language modeling, leveraging more abundant DNA data and reducing computational overhead while preserving modality-specific performance advantages.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovery of Disease Relationships via Transcriptomic Signature Analysis Powered by Agentic AI</title>
<link>https://arxiv.org/abs/2508.04742</link>
<guid>https://arxiv.org/abs/2508.04742</guid>
<content:encoded><![CDATA[
arXiv:2508.04742v1 Announce Type: cross 
Abstract: Modern disease classification often overlooks molecular commonalities hidden beneath divergent clinical presentations. This study introduces a transcriptomics-driven framework for discovering disease relationships by analyzing over 1300 disease-condition pairs using GenoMAS, a fully automated agentic AI system. Beyond identifying robust gene-level overlaps, we develop a novel pathway-based similarity framework that integrates multi-database enrichment analysis to quantify functional convergence across diseases. The resulting disease similarity network reveals both known comorbidities and previously undocumented cross-category links. By examining shared biological pathways, we explore potential molecular mechanisms underlying these connections-offering functional hypotheses that go beyond symptom-based taxonomies. We further show how background conditions such as obesity and hypertension modulate transcriptomic similarity, and identify therapeutic repurposing opportunities for rare diseases like autism spectrum disorder based on their molecular proximity to better-characterized conditions. In addition, this work demonstrates how biologically grounded agentic AI can scale transcriptomic analysis while enabling mechanistic interpretation across complex disease landscapes. All results are publicly accessible at github.com/KeeeeChen/Pathway_Similarity_Network.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alz-QNet: A Quantum Regression Network for Studying Alzheimer's Gene Interactions</title>
<link>https://arxiv.org/abs/2508.04743</link>
<guid>https://arxiv.org/abs/2508.04743</guid>
<content:encoded><![CDATA[
arXiv:2508.04743v1 Announce Type: cross 
Abstract: Understanding the molecular-level mechanisms underpinning Alzheimer's disease (AD) by studying crucial genes associated with the disease remains a challenge. Alzheimer's, being a multifactorial disease, requires understanding the gene-gene interactions underlying it for theranostics and progress. In this article, a novel attempt has been made using a quantum regression to decode how some crucial genes in the AD Amyloid Beta Precursor Protein ($APP$), Sterol regulatory element binding transcription factor 14 ($FGF14$), Yin Yang 1 ($YY1$), and Phospholipase D Family Member 3 ($PLD3$) etc. become influenced by other prominent switching genes during disease progression, which may help in gene expression-based therapy for AD. Our proposed Quantum Regression Network (Alz-QNet) introduces a pioneering approach with insights from the state-of-the-art Quantum Gene Regulatory Networks (QGRN) to unravel the gene interactions involved in AD pathology, particularly within the Entorhinal Cortex (EC), where early pathological changes occur. Using the proposed Alz-QNet framework, we explore the interactions between key genes ($APP$, $FGF14$, $YY1$, $EGR1$, $GAS7$, $AKT3$, $SREBF2$, and $PLD3$) within the CE microenvironment of AD patients, studying genetic samples from the database $GSE138852$, all of which are believed to play a crucial role in the progression of AD. Our investigation uncovers intricate gene-gene interactions, shedding light on the potential regulatory mechanisms that underlie the pathogenesis of AD, which help us to find potential gene inhibitors or regulators for theranostics.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRIT: Graph-Regularized Logit Refinement for Zero-shot Cell Type Annotation</title>
<link>https://arxiv.org/abs/2508.04747</link>
<guid>https://arxiv.org/abs/2508.04747</guid>
<content:encoded><![CDATA[
arXiv:2508.04747v1 Announce Type: cross 
Abstract: Cell type annotation is a fundamental step in the analysis of single-cell RNA sequencing (scRNA-seq) data. In practice, human experts often rely on the structure revealed by principal component analysis (PCA) followed by $k$-nearest neighbor ($k$-NN) graph construction to guide annotation. While effective, this process is labor-intensive and does not scale to large datasets. Recent advances in CLIP-style models offer a promising path toward automating cell type annotation. By aligning scRNA-seq profiles with natural language descriptions, models like LangCell enable zero-shot annotation. While LangCell demonstrates decent zero-shot performance, its predictions remain suboptimal, particularly in achieving consistent accuracy across all cell types. In this paper, we propose to refine the zero-shot logits produced by LangCell through a graph-regularized optimization framework. By enforcing local consistency over the task-specific PCA-based k-NN graph, our method combines the scalability of the pre-trained models with the structural robustness relied upon in expert annotation. We evaluate our approach on 14 annotated human scRNA-seq datasets from 4 distinct studies, spanning 11 organs and over 200,000 single cells. Our method consistently improves zero-shot annotation accuracy, achieving accuracy gains of up to 10%. Further analysis showcase the mechanism by which GRIT effectively propagates correct signals through the graph, pulling back mislabeled cells toward more accurate predictions. The method is training-free, model-agnostic, and serves as a simple yet effective plug-in for enhancing automated cell type annotation in practice.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Is (Almost) All You Need: Retrieval-Augmented Inference for Generalizable Genomic Prediction Tasks</title>
<link>https://arxiv.org/abs/2508.04757</link>
<guid>https://arxiv.org/abs/2508.04757</guid>
<content:encoded><![CDATA[
arXiv:2508.04757v1 Announce Type: cross 
Abstract: Large pre-trained DNA language models such as DNABERT-2, Nucleotide Transformer, and HyenaDNA have demonstrated strong performance on various genomic benchmarks. However, most applications rely on expensive fine-tuning, which works best when the training and test data share a similar distribution. In this work, we investigate whether task-specific fine-tuning is always necessary. We show that simple embedding-based pipelines that extract fixed representations from these models and feed them into lightweight classifiers can achieve competitive performance. In evaluation settings with different data distributions, embedding-based methods often outperform fine-tuning while reducing inference time by 10x to 20x. Our results suggest that embedding extraction is not only a strong baseline but also a more generalizable and efficient alternative to fine-tuning, especially for deployment in diverse or unseen genomic contexts. For example, in enhancer classification, HyenaDNA embeddings combined with zCurve achieve 0.68 accuracy (vs. 0.58 for fine-tuning), with an 88% reduction in inference time and over 8x lower carbon emissions (0.02 kg vs. 0.17 kg CO2). In non-TATA promoter classification, DNABERT-2 embeddings with zCurve or GC content reach 0.85 accuracy (vs. 0.89 with fine-tuning) with a 22x lower carbon footprint (0.02 kg vs. 0.44 kg CO2). These results show that embedding-based pipelines offer over 10x better carbon efficiency while maintaining strong predictive performance. The code is available here: https://github.com/NIRJHOR-DATTA/EMBEDDING-IS-ALMOST-ALL-YOU-NEED.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Multi-Architecture Deep Learning Framework for BIRADS-Based Mammographic Image Retrieval: Comprehensive Performance Analysis with Super-Ensemble Optimization</title>
<link>https://arxiv.org/abs/2508.04790</link>
<guid>https://arxiv.org/abs/2508.04790</guid>
<content:encoded><![CDATA[
arXiv:2508.04790v1 Announce Type: cross 
Abstract: Content-based mammographic image retrieval systems require exact BIRADS categorical matching across five distinct classes, presenting significantly greater complexity than binary classification tasks commonly addressed in literature. Current medical image retrieval studies suffer from methodological limitations including inadequate sample sizes, improper data splitting, and insufficient statistical validation that hinder clinical translation. We developed a comprehensive evaluation framework systematically comparing CNN architectures (DenseNet121, ResNet50, VGG16) with advanced training strategies including sophisticated fine-tuning, metric learning, and super-ensemble optimization. Our evaluation employed rigorous stratified data splitting (50%/20%/30% train/validation/test), 602 test queries, and systematic validation using bootstrap confidence intervals with 1,000 samples. Advanced fine-tuning with differential learning rates achieved substantial improvements: DenseNet121 (34.79% precision@10, 19.64% improvement) and ResNet50 (34.54%, 19.58% improvement). Super-ensemble optimization combining complementary architectures achieved 36.33% precision@10 (95% CI: [34.78%, 37.88%]), representing 24.93% improvement over baseline and providing 3.6 relevant cases per query. Statistical analysis revealed significant performance differences between optimization strategies (p<0.001) with large effect sizes (Cohen's d>0.8), while maintaining practical search efficiency (2.8milliseconds). Performance significantly exceeds realistic expectations for 5-class medical retrieval tasks, where literature suggests 20-25% precision@10 represents achievable performance for exact BIRADS matching. Our framework establishes new performance benchmarks while providing evidence-based architecture selection guidelines for clinical deployment in diagnostic support and quality assurance applications.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization</title>
<link>https://arxiv.org/abs/2508.04796</link>
<guid>https://arxiv.org/abs/2508.04796</guid>
<content:encoded><![CDATA[
arXiv:2508.04796v1 Announce Type: cross 
Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP pipelines. Standard algorithms for learning tokenizers rely on frequency-based objectives, which favor languages dominant in the training data and consequently leave lower-resource languages with tokenizations that are disproportionately longer, morphologically implausible, or even riddled with  placeholders. This phenomenon ultimately amplifies computational and financial inequalities between users from different language backgrounds. To remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes the compression gain of the currently worst-compressed language, trading a small amount of global compression for cross-lingual parity. We find empirically that Parity-aware BPE leads to more equitable token counts across languages, with negligible impact on global compression rate and no substantial effect on language-model performance in downstream tasks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimality Principles and Neural Ordinary Differential Equations-based Process Modeling for Distributed Control</title>
<link>https://arxiv.org/abs/2508.04799</link>
<guid>https://arxiv.org/abs/2508.04799</guid>
<content:encoded><![CDATA[
arXiv:2508.04799v1 Announce Type: cross 
Abstract: Most recent advances in machine learning and analytics for process control pose the question of how to naturally integrate new data-driven methods with classical process models and control. We propose a process modeling framework enabling integration of data-driven algorithms through consistent topological properties and conservation of extensive quantities. Interconnections among process network units are represented through connectivity matrices and network graphs. We derive the system's natural objective function equivalent to the non-equilibrium entropy production in a steady state system as a driving force for the process dynamics. We illustrate how distributed control and optimization can be implemented into process network structures and how control laws and algorithms alter the system's natural equilibrium towards engineered objectives. The basic requirement is that the flow conditions can be expressed in terms of conic sector (passivity) conditions. Our formalism allows integration of fundamental conservation properties from topology with learned dynamic relations from data through sparse deep neural networks.
  We demonstrate in a practical example of a simple inventory control system how to integrate the basic topology of a process with a neural network ordinary differential equation model. The system specific constitutive equations are left undescribed and learned by the neural ordinary differential equation algorithm using the adjoint method in combination with an adaptive ODE solver from synthetic time-series data. The resulting neural network forms a state space model for use in e.g. a model predictive control algorithm.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Model-X Knockoffs via Johnson-Lindenstrauss Transform</title>
<link>https://arxiv.org/abs/2508.04800</link>
<guid>https://arxiv.org/abs/2508.04800</guid>
<content:encoded><![CDATA[
arXiv:2508.04800v1 Announce Type: cross 
Abstract: We introduce a novel privatization framework for high-dimensional controlled variable selection. Our framework enables rigorous False Discovery Rate (FDR) control under differential privacy constraints. While the Model-X knockoff procedure provides FDR guarantees by constructing provably exchangeable ``negative control" features, existing privacy mechanisms like Laplace or Gaussian noise injection disrupt its core exchangeability conditions. Our key innovation lies in privatizing the data knockoff matrix through the Gaussian Johnson-Lindenstrauss Transformation (JLT), a dimension reduction technique that simultaneously preserves covariate relationships through approximate isometry for $(\epsilon,\delta)$-differential privacy.
  We theoretically characterize both FDR and the power of the proposed private variable selection procedure, in an asymptotic regime. Our theoretical analysis characterizes the role of different factors, such as the JLT's dimension reduction ratio, signal-to-noise ratio, differential privacy parameters, sample size and feature dimension, in shaping the privacy-power trade-off. Our analysis is based on a novel `debiasing technique' for high-dimensional private knockoff procedure. We further establish sufficient conditions under which the power of the proposed procedure converges to one. This work bridges two critical paradigms -- knockoff-based FDR control and private data release -- enabling reliable variable selection in sensitive domains. Our analysis demonstrates that structural privacy preservation through random projections outperforms the classical noise addition mechanism, maintaining statistical power even under strict privacy budgets.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini</title>
<link>https://arxiv.org/abs/2508.04820</link>
<guid>https://arxiv.org/abs/2508.04820</guid>
<content:encoded><![CDATA[
arXiv:2508.04820v1 Announce Type: cross 
Abstract: Logging is essential in software development, helping developers monitor system behavior and aiding in debugging applications. Given the ability of large language models (LLMs) to generate natural language and code, researchers are exploring their potential to generate log statements. However, prior work focuses on evaluating logs introduced in code functions, leaving file-level log generation underexplored -- especially in machine learning (ML) applications, where comprehensive logging can enhance reliability. In this study, we evaluate the capacity of GPT-4o mini as a case study to generate log statements for ML projects at file level. We gathered a set of 171 ML repositories containing 4,073 Python files with at least one log statement. We identified and removed the original logs from the files, prompted the LLM to generate logs for them, and evaluated both the position of the logs and log level, variables, and text quality of the generated logs compared to human-written logs. In addition, we manually analyzed a representative sample of generated logs to identify common patterns and challenges. We find that the LLM introduces logs in the same place as humans in 63.91% of cases, but at the cost of a high overlogging rate of 82.66%. Furthermore, our manual analysis reveals challenges for file-level logging, which shows overlogging at the beginning or end of a function, difficulty logging within large code blocks, and misalignment with project-specific logging conventions. While the LLM shows promise for generating logs for complete files, these limitations remain to be addressed for practical implementation.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off</title>
<link>https://arxiv.org/abs/2508.04825</link>
<guid>https://arxiv.org/abs/2508.04825</guid>
<content:encoded><![CDATA[
arXiv:2508.04825v1 Announce Type: cross 
Abstract: Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Driven Insights into Composition Property Relationships in FCC High Entropy Alloys</title>
<link>https://arxiv.org/abs/2508.04841</link>
<guid>https://arxiv.org/abs/2508.04841</guid>
<content:encoded><![CDATA[
arXiv:2508.04841v1 Announce Type: cross 
Abstract: Structural High Entropy Alloys (HEAs) are crucial in advancing technology across various sectors, including aerospace, automotive, and defense industries. However, the scarcity of integrated chemistry, process, structure, and property data presents significant challenges for predictive property modeling. Given the vast design space of these alloys, uncovering the underlying patterns is essential yet difficult, requiring advanced methods capable of learning from limited and heterogeneous datasets. This work presents several sensitivity analyses, highlighting key elemental contributions to mechanical behavior, including insights into the compositional factors associated with brittle and fractured responses observed during nanoindentation testing in the BIRDSHOT center NiCoFeCrVMnCuAl system dataset. Several encoder decoder based chemistry property models, carefully tuned through Bayesian multi objective hyperparameter optimization, are evaluated for mapping alloy composition to six mechanical properties. The models achieve competitive or superior performance to conventional regressors across all properties, particularly for yield strength and the UTS/YS ratio, demonstrating their effectiveness in capturing complex composition property relationships.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)</title>
<link>https://arxiv.org/abs/2508.04846</link>
<guid>https://arxiv.org/abs/2508.04846</guid>
<content:encoded><![CDATA[
arXiv:2508.04846v1 Announce Type: cross 
Abstract: Autonomous web-based geographical information systems (AWebGIS) aim to perform geospatial operations from natural language input, providing intuitive, intelligent, and hands-free interaction. However, most current solutions rely on cloud-based large language models (LLMs), which require continuous internet access and raise users' privacy and scalability issues due to centralized server processing. This study compares three approaches to enabling AWebGIS: (1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2) a semi-automated offline method using classical machine learning classifiers such as support vector machine and random forest; and (3) a fully autonomous offline (client-side) method based on a fine-tuned small language model (SLM), specifically T5-small model, executed in the client's web browser. The third approach, which leverages SLMs, achieved the highest accuracy among all methods, with an exact matching accuracy of 0.93, Levenshtein similarity of 0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L scores of 0.98. Crucially, this client-side computation strategy reduces the load on backend servers by offloading processing to the user's device, eliminating the need for server-based inference. These results highlight the feasibility of browser-executable models for AWebGIS solutions.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keyword Spotting with Hyper-Matched Filters for Small Footprint Devices</title>
<link>https://arxiv.org/abs/2508.04857</link>
<guid>https://arxiv.org/abs/2508.04857</guid>
<content:encoded><![CDATA[
arXiv:2508.04857v1 Announce Type: cross 
Abstract: Open-vocabulary keyword spotting (KWS) refers to the task of detecting words or terms within speech recordings, regardless of whether they were included in the training data. This paper introduces an open-vocabulary keyword spotting model with state-of-the-art detection accuracy for small-footprint devices. The model is composed of a speech encoder, a target keyword encoder, and a detection network. The speech encoder is either a tiny Whisper or a tiny Conformer. The target keyword encoder is implemented as a hyper-network that takes the desired keyword as a character string and generates a unique set of weights for a convolutional layer, which can be considered as a keyword-specific matched filter. The detection network uses the matched-filter weights to perform a keyword-specific convolution, which guides the cross-attention mechanism of a Perceiver module in determining whether the target term appears in the recording. The results indicate that our system achieves state-of-the-art detection performance and generalizes effectively to out-of-domain conditions, including second-language (L2) speech. Notably, our smallest model, with just 4.2 million parameters, matches or outperforms models that are several times larger, demonstrating both efficiency and robustness.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can SGD Handle Heavy-Tailed Noise?</title>
<link>https://arxiv.org/abs/2508.04860</link>
<guid>https://arxiv.org/abs/2508.04860</guid>
<content:encoded><![CDATA[
arXiv:2508.04860v1 Announce Type: cross 
Abstract: Stochastic Gradient Descent (SGD) is a cornerstone of large-scale optimization, yet its theoretical behavior under heavy-tailed noise -- common in modern machine learning and reinforcement learning -- remains poorly understood. In this work, we rigorously investigate whether vanilla SGD, devoid of any adaptive modifications, can provably succeed under such adverse stochastic conditions. Assuming only that stochastic gradients have bounded $p$-th moments for some $p \in (1, 2]$, we establish sharp convergence guarantees for (projected) SGD across convex, strongly convex, and non-convex problem classes. In particular, we show that SGD achieves minimax optimal sample complexity under minimal assumptions in the convex and strongly convex regimes: $\mathcal{O}(\varepsilon^{-\frac{p}{p-1}})$ and $\mathcal{O}(\varepsilon^{-\frac{p}{2(p-1)}})$, respectively. For non-convex objectives under H\"older smoothness, we prove convergence to a stationary point with rate $\mathcal{O}(\varepsilon^{-\frac{2p}{p-1}})$, and complement this with a matching lower bound specific to SGD with arbitrary polynomial step-size schedules. Finally, we consider non-convex Mini-batch SGD under standard smoothness and bounded central moment assumptions, and show that it also achieves a comparable $\mathcal{O}(\varepsilon^{-\frac{2p}{p-1}})$ sample complexity with a potential improvement in the smoothness constant. These results challenge the prevailing view that heavy-tailed noise renders SGD ineffective, and establish vanilla SGD as a robust and theoretically principled baseline -- even in regimes where the variance is unbounded.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequence Aware SAC Control for Engine Fuel Consumption Optimization in Electrified Powertrain</title>
<link>https://arxiv.org/abs/2508.04874</link>
<guid>https://arxiv.org/abs/2508.04874</guid>
<content:encoded><![CDATA[
arXiv:2508.04874v1 Announce Type: cross 
Abstract: As hybrid electric vehicles (HEVs) gain traction in heavy-duty trucks, adaptive and efficient energy management is critical for reducing fuel consumption while maintaining battery charge for long operation times. We present a new reinforcement learning (RL) framework based on the Soft Actor-Critic (SAC) algorithm to optimize engine control in series HEVs. We reformulate the control task as a sequential decision-making problem and enhance SAC by incorporating Gated Recurrent Units (GRUs) and Decision Transformers (DTs) into both actor and critic networks to capture temporal dependencies and improve planning over time. To evaluate robustness and generalization, we train the models under diverse initial battery states, drive cycle durations, power demands, and input sequence lengths. Experiments show that the SAC agent with a DT-based actor and GRU-based critic was within 1.8% of Dynamic Programming (DP) in fuel savings on the Highway Fuel Economy Test (HFET) cycle, while the SAC agent with GRUs in both actor and critic networks, and FFN actor-critic agent were within 3.16% and 3.43%, respectively. On unseen drive cycles (US06 and Heavy Heavy-Duty Diesel Truck (HHDDT) cruise segment), generalized sequence-aware agents consistently outperformed feedforward network (FFN)-based agents, highlighting their adaptability and robustness in real-world settings.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cosine Schedule is Fisher-Rao-Optimal for Masked Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2508.04884</link>
<guid>https://arxiv.org/abs/2508.04884</guid>
<content:encoded><![CDATA[
arXiv:2508.04884v1 Announce Type: cross 
Abstract: In this work, we study the problem of choosing the discretisation schedule for sampling from masked discrete diffusion models in terms of the information geometry of the induced probability path. Specifically, we show that the optimal schedule under the Fisher-Rao geometry recovers the popularly-used cosine schedule.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens</title>
<link>https://arxiv.org/abs/2508.04928</link>
<guid>https://arxiv.org/abs/2508.04928</guid>
<content:encoded><![CDATA[
arXiv:2508.04928v1 Announce Type: cross 
Abstract: We propose a method to extend foundational monocular depth estimators (FMDEs), trained on perspective images, to fisheye images. Despite being trained on tens of millions of images, FMDEs are susceptible to the covariate shift introduced by changes in camera calibration (intrinsic, distortion) parameters, leading to erroneous depth estimates. Our method aligns the distribution of latent embeddings encoding fisheye images to those of perspective images, enabling the reuse of FMDEs for fisheye cameras without retraining or finetuning. To this end, we introduce a set of Calibration Tokens as a light-weight adaptation mechanism that modulates the latent embeddings for alignment. By exploiting the already expressive latent space of FMDEs, we posit that modulating their embeddings avoids the negative impact of artifacts and loss introduced in conventional recalibration or map projection to a canonical reference frame in the image space. Our method is self-supervised and does not require fisheye images but leverages publicly available large-scale perspective image datasets. This is done by recalibrating perspective images to fisheye images, and enforcing consistency between their estimates during training. We evaluate our approach with several FMDEs, on both indoors and outdoors, where we consistently improve over state-of-the-art methods using a single set of tokens for both. Code available at: https://github.com/JungHeeKim29/calibration-token.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Errorless Training ImageNet-1k</title>
<link>https://arxiv.org/abs/2508.04941</link>
<guid>https://arxiv.org/abs/2508.04941</guid>
<content:encoded><![CDATA[
arXiv:2508.04941v1 Announce Type: cross 
Abstract: In this paper, we describe a feedforward artificial neural network trained on the ImageNet 2012 contest dataset [7] with the new method of [5] to an accuracy rate of 98.3% with a 99.69 Top-1 rate, and an average of 285.9 labels that are perfectly classified over the 10 batch partitions of the dataset. The best performing model uses 322,430,160 parameters, with 4 decimal places precision. We conjecture that the reason our model does not achieve a 100% accuracy rate is due to a double-labeling problem, by which there are duplicate images in the dataset with different labels.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Metric for MLLM Alignment in Large-scale Recommendation</title>
<link>https://arxiv.org/abs/2508.04963</link>
<guid>https://arxiv.org/abs/2508.04963</guid>
<content:encoded><![CDATA[
arXiv:2508.04963v1 Announce Type: cross 
Abstract: Multimodal recommendation has emerged as a critical technique in modern recommender systems, leveraging content representations from advanced multimodal large language models (MLLMs). To ensure these representations are well-adapted, alignment with the recommender system is essential. However, evaluating the alignment of MLLMs for recommendation presents significant challenges due to three key issues: (1) static benchmarks are inaccurate because of the dynamism in real-world applications, (2) evaluations with online system, while accurate, are prohibitively expensive at scale, and (3) conventional metrics fail to provide actionable insights when learned representations underperform. To address these challenges, we propose the Leakage Impact Score (LIS), a novel metric for multimodal recommendation. Rather than directly assessing MLLMs, LIS efficiently measures the upper bound of preference data. We also share practical insights on deploying MLLMs with LIS in real-world scenarios. Online A/B tests on both Content Feed and Display Ads of Xiaohongshu's Explore Feed production demonstrate the effectiveness of our proposed method, showing significant improvements in user spent time and advertiser value.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas</title>
<link>https://arxiv.org/abs/2508.04964</link>
<guid>https://arxiv.org/abs/2508.04964</guid>
<content:encoded><![CDATA[
arXiv:2508.04964v1 Announce Type: cross 
Abstract: The utilization of radio frequency (RF) signals for wireless sensing has garnered increasing attention. However, the radio environment is unpredictable and often unfavorable, the sensing accuracy of traditional RF sensing methods is often affected by adverse propagation channels from the transmitter to the receiver, such as fading and noise. In this paper, we propose employing distributed Reconfigurable Intelligent Metasurface Antennas (RIMSA) to detect the presence and location of objects where multiple RIMSA receivers (RIMSA Rxs) are deployed on different places. By programming their beamforming patterns, RIMSA Rxs can enhance the quality of received signals. The RF sensing problem is modeled as a joint optimization problem of beamforming pattern and mapping of received signals to sensing outcomes. To address this challenge, we introduce a deep reinforcement learning (DRL) algorithm aimed at calculating the optimal beamforming patterns and a neural network aimed at converting received signals into sensing outcomes. In addition, the malicious attacker may potentially launch jamming attack to disrupt sensing process. To enable effective sensing in interferenceprone environment, we devise a combined loss function that takes into account the Signal to Interference plus Noise Ratio (SINR) of the received signals. The simulation results show that the proposed distributed RIMSA system can achieve more efficient sensing performance and better overcome environmental influences than centralized implementation. Furthermore, the introduced method ensures high-accuracy sensing performance even under jamming attack.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Machine Learning Methods with Uncertainty Quantification for Exoplanet Atmospheric Retrievals from Transmission Spectroscopy</title>
<link>https://arxiv.org/abs/2508.04982</link>
<guid>https://arxiv.org/abs/2508.04982</guid>
<content:encoded><![CDATA[
arXiv:2508.04982v1 Announce Type: cross 
Abstract: Standard Bayesian retrievals for exoplanet atmospheric parameters from transmission spectroscopy, while well understood and widely used, are generally computationally expensive. In the era of the JWST and other upcoming observatories, machine learning approaches have emerged as viable alternatives that are both efficient and robust. In this paper we present a systematic study of several existing machine learning regression techniques and compare their performance for retrieving exoplanet atmospheric parameters from transmission spectra. We benchmark the performance of the different algorithms on the accuracy, precision, and speed. The regression methods tested here include partial least squares (PLS), support vector machines (SVM), k nearest neighbors (KNN), decision trees (DT), random forests (RF), voting (VOTE), stacking (STACK), and extreme gradient boosting (XGB). We also investigate the impact of different preprocessing methods of the training data on the model performance. We quantify the model uncertainties across the entire dynamical range of planetary parameters. The best performing combination of ML model and preprocessing scheme is validated on a the case study of JWST observation of WASP-39b.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAM: Large-scale Video Continual Learning with Bootstrapped Compression</title>
<link>https://arxiv.org/abs/2508.05001</link>
<guid>https://arxiv.org/abs/2508.05001</guid>
<content:encoded><![CDATA[
arXiv:2508.05001v1 Announce Type: cross 
Abstract: Continual learning (CL) promises to allow neural networks to learn from continuous streams of inputs, instead of IID (independent and identically distributed) sampling, which requires random access to a full dataset. This would allow for much smaller storage requirements and self-sufficiency of deployed systems that cope with natural distribution shifts, similarly to biological learning. We focus on video CL employing a rehearsal-based approach, which reinforces past samples from a memory buffer. We posit that part of the reason why practical video CL is challenging is the high memory requirements of video, further exacerbated by long-videos and continual streams, which are at odds with the common rehearsal-buffer size constraints. To address this, we propose to use compressed vision, i.e. store video codes (embeddings) instead of raw inputs, and train a video classifier by IID sampling from this rolling buffer. Training a video compressor online (so not depending on any pre-trained networks) means that it is also subject to catastrophic forgetting. We propose a scheme to deal with this forgetting by refreshing video codes, which requires careful decompression with a previous version of the network and recompression with a new one. We name our method Continually Refreshed Amodal Memory (CRAM). We expand current video CL benchmarks to large-scale settings, namely EpicKitchens-100 and Kinetics-700, storing thousands of relatively long videos in under 2 GB, and demonstrate empirically that our video CL method outperforms prior art with a significantly reduced memory footprint.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes</title>
<link>https://arxiv.org/abs/2508.05019</link>
<guid>https://arxiv.org/abs/2508.05019</guid>
<content:encoded><![CDATA[
arXiv:2508.05019v1 Announce Type: cross 
Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for over $8 billion in annual healthcare expenditures. Early diagnosis, accurate and timely treatment are critical to improving patient survival rates. In clinical settings, physicians document patient visits using detailed SOAP (Subjective, Objective, Assessment, and Plan) notes. However, manually generating these notes is labor-intensive and contributes to clinician burnout. In this work, we propose skin-SOAP, a weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs, including lesion images and sparse clinical text. Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden and reducing the need for large annotated data. Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical relevance metrics. To evaluate this clinical relevance, we introduce two novel metrics MedConceptEval and Clinical Coherence Score (CCS) which assess semantic alignment with expert medical concepts and input features, respectively.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack</title>
<link>https://arxiv.org/abs/2508.05034</link>
<guid>https://arxiv.org/abs/2508.05034</guid>
<content:encoded><![CDATA[
arXiv:2508.05034v1 Announce Type: cross 
Abstract: As software systems grow in complexity, accurately identifying and managing dependencies among changes becomes increasingly critical. For instance, a change that leverages a function must depend on the change that introduces it. Establishing such dependencies allows CI/CD pipelines to build and orchestrate changes effectively, preventing build failures and incomplete feature deployments. In modern software systems, dependencies often span multiple components across teams, creating challenges for development and deployment. They serve various purposes, from enabling new features to managing configurations, and can even involve traditionally independent changes like documentation updates. To address these challenges, we conducted a preliminary study on dependency management in OpenStack, a large-scale software system. Our study revealed that a substantial portion of software changes in OpenStack over the past 10 years are interdependent. Surprisingly, 51.08% of these dependencies are identified during the code review phase-after a median delay of 5.06 hours-rather than at the time of change creation. Developers often spend a median of 57.12 hours identifying dependencies, searching among a median of 463 other changes. To help developers proactively identify dependencies, we propose a semi-automated approach that leverages two ML models. The first model predicts the likelihood of dependencies among changes, while the second identifies the exact pairs of dependent changes. Our proposed models demonstrate strong performance, achieving average AUC scores of 79.33% and 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the second model has a good top-k recall across all types of pairs, while the top-k precision has room for improvement.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits</title>
<link>https://arxiv.org/abs/2508.05036</link>
<guid>https://arxiv.org/abs/2508.05036</guid>
<content:encoded><![CDATA[
arXiv:2508.05036v1 Announce Type: cross 
Abstract: Time series forecasting is vital in domains where data sensitivity is paramount, such as finance and energy systems. While Differential Privacy (DP) provides theoretical guarantees to protect individual data contributions, its integration especially via DP-SGD often impairs model performance due to injected noise. In this paper, we propose Q-DPTS, a hybrid quantum-classical framework for Quantum Differentially Private Time Series Forecasting. Q-DPTS combines Variational Quantum Circuits (VQCs) with per-sample gradient clipping and Gaussian noise injection, ensuring rigorous $(\epsilon, \delta)$-differential privacy. The expressiveness of quantum models enables improved robustness against the utility loss induced by DP mechanisms. We evaluate Q-DPTS on the ETT (Electricity Transformer Temperature) dataset, a standard benchmark for long-term time series forecasting. Our approach is compared against both classical and quantum baselines, including LSTM, QASA, QRWKV, and QLSTM. Results demonstrate that Q-DPTS consistently achieves lower prediction error under the same privacy budget, indicating a favorable privacy-utility trade-off. This work presents one of the first explorations into quantum-enhanced differentially private forecasting, offering promising directions for secure and accurate time series modeling in privacy-critical scenarios.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two tales for a geometric Jensen--Shannon divergence</title>
<link>https://arxiv.org/abs/2508.05066</link>
<guid>https://arxiv.org/abs/2508.05066</guid>
<content:encoded><![CDATA[
arXiv:2508.05066v1 Announce Type: cross 
Abstract: The geometric Jensen--Shannon divergence (G-JSD) gained popularity in machine learning and information sciences thanks to its closed-form expression between Gaussian distributions. In this work, we introduce an alternative definition of the geometric Jensen--Shannon divergence tailored to positive densities which does not normalize geometric mixtures. This novel divergence is termed the extended G-JSD as it extends to more general positive measures. We give explicitly the gap between the extended G-JSD and G-JSD when considering probability densities, and report both lower and upper bounds in terms of other statistical divergences. We derive corresponding closed-form expressions when considering the case of multivariate Gaussian distributions often met in applications. Finally, we show that these two types of geometric JSDs, the G-JSD and the extended G-JSD, can be interpreted as regularizations of the ordinary JSD by additive terms.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2508.05068</link>
<guid>https://arxiv.org/abs/2508.05068</guid>
<content:encoded><![CDATA[
arXiv:2508.05068v1 Announce Type: cross 
Abstract: Image colorization, the task of adding colors to grayscale images, has been the focus of significant research efforts in computer vision in recent years for its various application areas such as color restoration and automatic animation colorization [15, 1]. The colorization problem is challenging as it is highly ill-posed with two out of three image dimensions lost, resulting in large degrees of freedom. However, semantics of the scene as well as the surface texture could provide important cues for colors: the sky is typically blue, the clouds are typically white and the grass is typically green, and there are huge amounts of training data available for learning such priors since any colored image could serve as a training data point [20].
  Colorization is initially formulated as a regression task[5], which ignores the multi-modal nature of color prediction. In this project, we explore automatic image colorization via classification and adversarial learning. We will build our models on prior works, apply modifications for our specific scenario and make comparisons.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid quantum tensor networks for aeroelastic applications</title>
<link>https://arxiv.org/abs/2508.05169</link>
<guid>https://arxiv.org/abs/2508.05169</guid>
<content:encoded><![CDATA[
arXiv:2508.05169v1 Announce Type: cross 
Abstract: We investigate the application of hybrid quantum tensor networks to aeroelastic problems, harnessing the power of Quantum Machine Learning (QML). By combining tensor networks with variational quantum circuits, we demonstrate the potential of QML to tackle complex time series classification and regression tasks. Our results showcase the ability of hybrid quantum tensor networks to achieve high accuracy in binary classification. Furthermore, we observe promising performance in regressing discrete variables. While hyperparameter selection remains a challenge, requiring careful optimisation to unlock the full potential of these models, this work contributes significantly to the development of QML for solving intricate problems in aeroelasticity. We present an end-to-end trainable hybrid algorithm. We first encode time series into tensor networks to then utilise trainable tensor networks for dimensionality reduction, and convert the resulting tensor to a quantum circuit in the encoding step. Then, a tensor network inspired trainable variational quantum circuit is applied to solve either a classification or a multivariate or univariate regression task in the aeroelasticity domain.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Posterior-GRPO: Rewarding Reasoning Processes in Code Generation</title>
<link>https://arxiv.org/abs/2508.05170</link>
<guid>https://arxiv.org/abs/2508.05170</guid>
<content:encoded><![CDATA[
arXiv:2508.05170v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model's internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation</title>
<link>https://arxiv.org/abs/2508.05182</link>
<guid>https://arxiv.org/abs/2508.05182</guid>
<content:encoded><![CDATA[
arXiv:2508.05182v1 Announce Type: cross 
Abstract: Domain Adaptation (DA) aims to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain under domain shifts. Most prior works focus on capturing the inter-domain transferability but largely overlook rich intra-domain structures, which empirically results in even worse discriminability. To tackle this tradeoff, we propose a generalized graph SPectral Alignment framework, SPA++. Its core is briefly condensed as follows: (1)-by casting the DA problem to graph primitives, it composes a coarse graph alignment mechanism with a novel spectral regularizer toward aligning the domain graphs in eigenspaces; (2)-we further develop a fine-grained neighbor-aware propagation mechanism for enhanced discriminability in the target domain; (3)-by incorporating data augmentation and consistency regularization, SPA++ can adapt to complex scenarios including most DA settings and even challenging distribution scenarios. Furthermore, we also provide theoretical analysis to support our method, including the generalization bound of graph-based DA and the role of spectral alignment and smoothing consistency. Extensive experiments on benchmark datasets demonstrate that SPA++ consistently outperforms existing cutting-edge methods, achieving superior robustness and adaptability across various challenging adaptation scenarios.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference</title>
<link>https://arxiv.org/abs/2508.05212</link>
<guid>https://arxiv.org/abs/2508.05212</guid>
<content:encoded><![CDATA[
arXiv:2508.05212v1 Announce Type: cross 
Abstract: With the development of big data and machine learning, privacy concerns have become increasingly critical, especially when handling heterogeneous datasets containing sensitive personal information. Differential privacy provides a rigorous framework for safeguarding individual privacy while enabling meaningful statistical analysis. In this paper, we propose a differentially private quantile regression method for high-dimensional data in a distributed setting. Quantile regression is a powerful and robust tool for modeling the relationships between the covariates and responses in the presence of outliers or heavy-tailed distributions. To address the computational challenges due to the non-smoothness of the quantile loss function, we introduce a Newton-type transformation that reformulates the quantile regression task into an ordinary least squares problem. Building on this, we develop a differentially private estimation algorithm with iterative updates, ensuring both near-optimal statistical accuracy and formal privacy guarantees. For inference, we further propose a differentially private debiased estimator, which enables valid confidence interval construction and hypothesis testing. Additionally, we propose a communication-efficient and differentially private bootstrap for simultaneous hypothesis testing in high-dimensional quantile regression, suitable for distributed settings with both small and abundant local data. Extensive simulations demonstrate the robustness and effectiveness of our methods in practical scenarios.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking</title>
<link>https://arxiv.org/abs/2508.05221</link>
<guid>https://arxiv.org/abs/2508.05221</guid>
<content:encoded><![CDATA[
arXiv:2508.05221v1 Announce Type: cross 
Abstract: Vision-language tracking has received increasing attention in recent years, as textual information can effectively address the inflexibility and inaccuracy associated with specifying the target object to be tracked. Existing works either directly fuse the fixed language with vision features or simply modify using attention, however, their performance is still limited. Recently, some researchers have explored using text generation to adapt to the variations in the target during tracking, however, these works fail to provide insights into the model's reasoning process and do not fully leverage the advantages of large models, which further limits their overall performance. To address the aforementioned issues, this paper proposes a novel reasoning-based vision-language tracking framework, named ReasoningTrack, based on a pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning) and reinforcement learning GRPO are used for the optimization of reasoning and language generation. We embed the updated language descriptions and feed them into a unified tracking backbone network together with vision features. Then, we adopt a tracking head to predict the specific location of the target object. In addition, we propose a large-scale long-term vision-language tracking benchmark dataset, termed TNLLT, which contains 200 video sequences. 20 baseline visual trackers are re-trained and evaluated on this dataset, which builds a solid foundation for the vision-language visual tracking task. Extensive experiments on multiple vision-language tracking benchmark datasets fully validated the effectiveness of our proposed reasoning-based natural language generation strategy. The source code of this paper will be released on https://github.com/Event-AHU/Open_VLTrack
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pruning Large Language Models by Identifying and Preserving Functional Networks</title>
<link>https://arxiv.org/abs/2508.05239</link>
<guid>https://arxiv.org/abs/2508.05239</guid>
<content:encoded><![CDATA[
arXiv:2508.05239v1 Announce Type: cross 
Abstract: Structured pruning is one of the representative techniques for compressing large language models (LLMs) to reduce GPU memory consumption and accelerate inference speed. It offers significant practical value in improving the efficiency of LLMs in real-world applications. Current structured pruning methods typically rely on assessment of the importance of the structure units and pruning the units with less importance. Most of them overlooks the interaction and collaboration among artificial neurons that are crucial for the functionalities of LLMs, leading to a disruption in the macro functional architecture of LLMs and consequently a pruning performance degradation. Inspired by the inherent similarities between artificial neural networks and functional neural networks in the human brain, we alleviate this challenge and propose to prune LLMs by identifying and preserving functional networks within LLMs in this study. To achieve this, we treat an LLM as a digital brain and decompose the LLM into functional networks, analogous to identifying functional brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving the key neurons within these functional networks. Experimental results demonstrate that the proposed method can successfully identify and locate functional networks and key neurons in LLMs, enabling efficient model pruning. Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis</title>
<link>https://arxiv.org/abs/2508.05246</link>
<guid>https://arxiv.org/abs/2508.05246</guid>
<content:encoded><![CDATA[
arXiv:2508.05246v1 Announce Type: cross 
Abstract: Gender classification is attractive in a range of applications, including surveillance and monitoring, corporate profiling, and human-computer interaction. Individuals' identities may be gleaned from information about their gender, which is a kind of soft biometric.Over the years, several methods for determining a person's gender have been devised. Some of the most well-known ones are based on physical characteristics like face, fingerprint, palmprint, DNA, ears, gait, and iris. On the other hand, facial features account for the vast majority of gender classification methods. Also, the iris is a significant biometric trait because the iris, according to research, remains basically constant during an individual's life. Besides that, the iris is externally visible and is non-invasive to the user, which is important for practical applications. Furthermore, there are already high-quality methods for segmenting and encoding iris images, and the current methods facilitate selecting and extracting attribute vectors from iris textures. This study discusses several approaches to determining gender. The previous works of literature are briefly reviewed. Additionally, there are a variety of methodologies for different steps of gender classification. This study provides researchers with knowledge and analysis of the existing gender classification approaches. Also, it will assist researchers who are interested in this specific area, as well as highlight the gaps and challenges in the field, and finally provide suggestions and future paths for improvement.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications</title>
<link>https://arxiv.org/abs/2508.05248</link>
<guid>https://arxiv.org/abs/2508.05248</guid>
<content:encoded><![CDATA[
arXiv:2508.05248v1 Announce Type: cross 
Abstract: This study provides an in-depth analysis of time series forecasting methods to predict the time-dependent deformation trend (also known as creep) of salt rock under varying confining pressure conditions. Creep deformation assessment is essential for designing and operating underground storage facilities for nuclear waste, hydrogen energy, or radioactive materials. Salt rocks, known for their mechanical properties like low porosity, low permeability, high ductility, and exceptional creep and self-healing capacities, were examined using multi-stage triaxial (MSTL) creep data. After resampling, axial strain datasets were recorded at 5--10 second intervals under confining pressure levels ranging from 5 to 35 MPa over 5.8--21 days. Initial analyses, including Seasonal-Trend Decomposition (STL) and Granger causality tests, revealed minimal seasonality and causality between axial strain and temperature data. Further statistical tests, such as the Augmented Dickey-Fuller (ADF) test, confirmed the stationarity of the data with p-values less than 0.05, and wavelet coherence plot (WCP) analysis indicated repeating trends. A suite of deep neural network (DNN) models (Neural Basis Expansion Analysis for Time Series (N-BEATS), Temporal Convolutional Networks (TCN), Recurrent Neural Networks (RNN), and Transformers (TF)) was utilized and compared against statistical baseline models. Predictive performance was evaluated using Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and Symmetric Mean Absolute Percentage Error (SMAPE). Results demonstrated that N-BEATS and TCN models outperformed others across various stress levels, respectively. DNN models, particularly N-BEATS and TCN, showed a 15--20\% improvement in accuracy over traditional analytical models, effectively capturing complex temporal dependencies and patterns.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Errors of LLM-Generated RTL Code</title>
<link>https://arxiv.org/abs/2508.05266</link>
<guid>https://arxiv.org/abs/2508.05266</guid>
<content:encoded><![CDATA[
arXiv:2508.05266v1 Announce Type: cross 
Abstract: Despite the promising potential of large language model (LLM) based register-transfer-level (RTL) code generation, the overall success rate remains unsatisfactory. Errors arise from various factors, with limited understanding of specific failure causes hindering improvement. To address this, we conduct a comprehensive error analysis and manual categorization. Our findings reveal that most errors stem not from LLM reasoning limitations, but from insufficient RTL programming knowledge, poor understanding of circuit concepts, ambiguous design descriptions, or misinterpretation of complex multimodal inputs. Leveraging in-context learning, we propose targeted error correction techniques. Specifically, we construct a domain-specific knowledge base and employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge. To mitigate ambiguity errors, we introduce design description rules and implement a rule-checking mechanism. For multimodal misinterpretation, we integrate external tools to convert inputs into LLM-compatible meta-formats. For remaining errors, we adopt an iterative debugging loop (simulation-error localization-correction). Integrating these techniques into an LLM-based framework significantly improves performance. We incorporate these error correction techniques into a foundational LLM-based RTL code generation framework, resulting in significantly improved performance. Experimental results show that our enhanced framework achieves 91.0\% accuracy on the VerilogEval benchmark, surpassing the baseline code generation approach by 32.7\%, demonstrating the effectiveness of our methods.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction</title>
<link>https://arxiv.org/abs/2508.05294</link>
<guid>https://arxiv.org/abs/2508.05294</guid>
<content:encoded><![CDATA[
arXiv:2508.05294v1 Announce Type: cross 
Abstract: Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (BLMs) are increasing the dexterity and capabilities of robotic systems. This survey paper focuses on those words advancing towards agentic applications and architectures. This includes initial efforts exploring GPT-style interfaces to tooling, as well as more complex system where AI agents are coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression</title>
<link>https://arxiv.org/abs/2508.05337</link>
<guid>https://arxiv.org/abs/2508.05337</guid>
<content:encoded><![CDATA[
arXiv:2508.05337v1 Announce Type: cross 
Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought reasoning with complex reflection behaviors, typically signaled by specific trigger words (e.g., "Wait" and "Alternatively") to enhance performance. However, these reflection behaviors can lead to the overthinking problem where the generation of redundant reasoning steps that unnecessarily increase token usage, raise inference costs, and reduce practical utility. In this paper, we propose Certainty-Guided Reflection Suppression (CGRS), a novel method that mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS operates by dynamically suppressing the model's generation of reflection triggers when it exhibits high confidence in its current response, thereby preventing redundant reflection cycles without compromising output quality. Our approach is model-agnostic, requires no retraining or architectural modifications, and can be integrated seamlessly with existing autoregressive generation pipelines. Extensive experiments across four reasoning benchmarks (i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it reduces token usage by an average of 18.5% to 41.9% while preserving accuracy. It also achieves the optimal balance between length reduction and performance compared to state-of-the-art baselines. These results hold consistently across model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3 family) and scales (4B to 32B parameters), highlighting CGRS's practical value for efficient reasoning.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking</title>
<link>https://arxiv.org/abs/2508.05341</link>
<guid>https://arxiv.org/abs/2508.05341</guid>
<content:encoded><![CDATA[
arXiv:2508.05341v1 Announce Type: cross 
Abstract: We propose the first fractal frequency mapping, which in a simple form enables to replicate complex neuronal effects. Unlike the conventional filters, which suppress or amplify the input spectral components according to the filter weights, the transformation excites novel components by a fractal recomposition of the input spectra resulting in a formation of spikes at resonant frequencies that are optimal for sampling. This enables high sensitivity detection, robustness to noise and noise-induced signal amplification. The proposed model illustrates that a neuronal functionality can be viewed as a linear summation of spectrum over nonlinearly transformed frequency domain.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Reliability of Sampling Strategies in Offline Recommender Evaluation</title>
<link>https://arxiv.org/abs/2508.05398</link>
<guid>https://arxiv.org/abs/2508.05398</guid>
<content:encoded><![CDATA[
arXiv:2508.05398v1 Announce Type: cross 
Abstract: Offline evaluation plays a central role in benchmarking recommender systems when online testing is impractical or risky. However, it is susceptible to two key sources of bias: exposure bias, where users only interact with items they are shown, and sampling bias, introduced when evaluation is performed on a subset of logged items rather than the full catalog. While prior work has proposed methods to mitigate sampling bias, these are typically assessed on fixed logged datasets rather than for their ability to support reliable model comparisons under varying exposure conditions or relative to true user preferences. In this paper, we investigate how different combinations of logging and sampling choices affect the reliability of offline evaluation. Using a fully observed dataset as ground truth, we systematically simulate diverse exposure biases and assess the reliability of common sampling strategies along four dimensions: sampling resolution (recommender model separability), fidelity (agreement with full evaluation), robustness (stability under exposure bias), and predictive power (alignment with ground truth). Our findings highlight when and how sampling distorts evaluation outcomes and offer practical guidance for selecting strategies that yield faithful and robust offline comparisons.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2508.05399</link>
<guid>https://arxiv.org/abs/2508.05399</guid>
<content:encoded><![CDATA[
arXiv:2508.05399v1 Announce Type: cross 
Abstract: Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions</title>
<link>https://arxiv.org/abs/2508.05430</link>
<guid>https://arxiv.org/abs/2508.05430</guid>
<content:encoded><![CDATA[
arXiv:2508.05430v1 Announce Type: cross 
Abstract: Language-image pre-training (LIP) enables the development of vision-language models capable of zero-shot classification, localization, multimodal retrieval, and semantic understanding. Various explanation methods have been proposed to visualize the importance of input image-text pairs on the model's similarity outputs. However, popular saliency maps are limited by capturing only first-order attributions, overlooking the complex cross-modal interactions intrinsic to such encoders. We introduce faithful interaction explanations of LIP models (FIxLIP) as a unified approach to decomposing the similarity in vision-language encoders. FIxLIP is rooted in game theory, where we analyze how using the weighted Banzhaf interaction index offers greater flexibility and improves computational efficiency over the Shapley interaction quantification framework. From a practical perspective, we propose how to naturally extend explanation evaluation metrics, like the pointing game and area between the insertion/deletion curves, to second-order interaction explanations. Experiments on MS COCO and ImageNet-1k benchmarks validate that second-order methods like FIxLIP outperform first-order attribution methods. Beyond delivering high-quality explanations, we demonstrate the utility of FIxLIP in comparing different models like CLIP vs. SigLIP-2 and ViT-B/32 vs. ViT-L/16.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Sparsification of Bipartite-Like Clusters in Graphs</title>
<link>https://arxiv.org/abs/2508.05437</link>
<guid>https://arxiv.org/abs/2508.05437</guid>
<content:encoded><![CDATA[
arXiv:2508.05437v1 Announce Type: cross 
Abstract: Graph clustering is an important algorithmic technique for analysing massive graphs, and has been widely applied in many research fields of data science. While the objective of most graph clustering algorithms is to find a vertex set of low conductance, a sequence of recent studies highlights the importance of the inter-connection between vertex sets when analysing real-world datasets. Following this line of research, in this work we study bipartite-like clusters and present efficient and online sparsification algorithms that find such clusters in both undirected graphs and directed ones. We conduct experimental studies on both synthetic and real-world datasets, and show that our algorithms significantly speedup the running time of existing clustering algorithms while preserving their effectiveness.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Geometric-Aware Quadrature Rules for Functional Minimization</title>
<link>https://arxiv.org/abs/2508.05445</link>
<guid>https://arxiv.org/abs/2508.05445</guid>
<content:encoded><![CDATA[
arXiv:2508.05445v1 Announce Type: cross 
Abstract: Accurate numerical integration over non-uniform point clouds is a challenge for modern mesh-free machine learning solvers for partial differential equations (PDEs) using variational principles. While standard Monte Carlo (MC) methods are not capable of handling a non-uniform point cloud, modern neural network architectures can deal with permutation-invariant inputs, creating quadrature rules for any point cloud. In this work, we introduce QuadrANN, a Graph Neural Network (GNN) architecture designed to learn optimal quadrature weights directly from the underlying geometry of point clouds. The design of the model exploits a deep message-passing scheme where the initial layer encodes rich local geometric features from absolute and relative positions as well as an explicit local density measure. In contrast, the following layers incorporate a global context vector. These architectural choices allow the QuadrANN to generate a data-driven quadrature rule that is permutation-invariant and adaptive to both local point density and the overall domain shape. We test our methodology on a series of challenging test cases, including integration on convex and non-convex domains and estimating the solution of the Heat and Fokker-Planck equations. Across all the tests, QuadrANN reduces the variance of the integral estimation compared to standard Quasi-Monte Carlo methods by warping the point clouds to be more dense in critical areas where the integrands present certain singularities. This enhanced stability in critical areas of the domain at hand is critical for the optimization of energy functionals, leading to improved deep learning-based variational solvers.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification</title>
<link>https://arxiv.org/abs/2508.05489</link>
<guid>https://arxiv.org/abs/2508.05489</guid>
<content:encoded><![CDATA[
arXiv:2508.05489v1 Announce Type: cross 
Abstract: Previous work has suggested that preprocessing images through lossy compression can defend against adversarial perturbations, but comprehensive attack evaluations have been lacking. In this paper, we construct strong white-box and adaptive attacks against various compression models and identify a critical challenge for attackers: high realism in reconstructed images significantly increases attack difficulty. Through rigorous evaluation across multiple attack scenarios, we demonstrate that compression models capable of producing realistic, high-fidelity reconstructions are substantially more resistant to our attacks. In contrast, low-realism compression models can be broken. Our analysis reveals that this is not due to gradient masking. Rather, realistic reconstructions maintaining distributional alignment with natural images seem to offer inherent robustness. This work highlights a significant obstacle for future adversarial attacks and suggests that developing more effective techniques to overcome realism represents an essential challenge for comprehensive security evaluation.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact and Heuristic Algorithms for Constrained Biclustering</title>
<link>https://arxiv.org/abs/2508.05493</link>
<guid>https://arxiv.org/abs/2508.05493</guid>
<content:encoded><![CDATA[
arXiv:2508.05493v1 Announce Type: cross 
Abstract: Biclustering, also known as co-clustering or two-way clustering, simultaneously partitions the rows and columns of a data matrix to reveal submatrices with coherent patterns. Incorporating background knowledge into clustering to enhance solution quality and interpretability has attracted growing interest in mathematical optimization and machine learning research. Extending this paradigm to biclustering enables prior information to guide the joint grouping of rows and columns. We study constrained biclustering with pairwise constraints, namely must-link and cannot-link constraints, which specify whether objects should belong to the same or different biclusters. As a model problem, we address the constrained version of the k-densest disjoint biclique problem, which aims to identify k disjoint complete bipartite subgraphs (called bicliques) in a weighted complete bipartite graph, maximizing the total density while satisfying pairwise constraints. We propose both exact and heuristic algorithms. The exact approach is a tailored branch-and-cut algorithm based on a low-dimensional semidefinite programming (SDP) relaxation, strengthened with valid inequalities and solved in a cutting-plane fashion. Exploiting integer programming tools, a rounding scheme converts SDP solutions into feasible biclusterings at each node. For large-scale instances, we introduce an efficient heuristic based on the low-rank factorization of the SDP. The resulting nonlinear optimization problem is tackled with an augmented Lagrangian method, where the subproblem is solved by decomposition through a block-coordinate projected gradient algorithm. Extensive experiments on synthetic and real-world datasets show that the exact method significantly outperforms general-purpose solvers, while the heuristic achieves high-quality solutions efficiently on large instances.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master's Program</title>
<link>https://arxiv.org/abs/2508.05513</link>
<guid>https://arxiv.org/abs/2508.05513</guid>
<content:encoded><![CDATA[
arXiv:2508.05513v1 Announce Type: cross 
Abstract: Letters of recommendation (LORs) provide valuable insights into candidates' capabilities and experiences beyond standardized test scores. However, reviewing these text-heavy materials is time-consuming and labor-intensive. To address this challenge and support the admission committee in providing feedback for students' professional growth, our study introduces LORI: LOR Insights, a novel AI-based detection tool for assessing leadership skills in LORs submitted by online master's program applicants. By employing natural language processing and leveraging large language models using RoBERTa and LLAMA, we seek to identify leadership attributes such as teamwork, communication, and innovation. Our latest RoBERTa model achieves a weighted F1 score of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong level of consistency in our test data. With the growing importance of leadership skills in the STEM sector, integrating LORI into the graduate admissions process is crucial for accurately assessing applicants' leadership capabilities. This approach not only streamlines the admissions process but also automates and ensures a more comprehensive evaluation of candidates' capabilities.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation</title>
<link>https://arxiv.org/abs/2508.05535</link>
<guid>https://arxiv.org/abs/2508.05535</guid>
<content:encoded><![CDATA[
arXiv:2508.05535v1 Announce Type: cross 
Abstract: Effective robotic systems for long-horizon human-robot collaboration must adapt to a wide range of human partners, whose physical behavior, willingness to assist, and understanding of the robot's capabilities may change over time. This demands a tightly coupled communication loop that grants both agents the flexibility to propose, accept, or decline requests as they coordinate toward completing the task effectively. We apply a Mixed-Initiative dialog paradigm to Collaborative human-roBot teaming and propose MICoBot, a system that handles the common scenario where both agents, using natural language, take initiative in formulating, accepting, or rejecting proposals on who can best complete different steps of a task. To handle diverse, task-directed dialog, and find successful collaborative strategies that minimize human effort, MICoBot makes decisions at three levels: (1) a meta-planner considers human dialog to formulate and code a high-level collaboration strategy, (2) a planner optimally allocates the remaining steps to either agent based on the robot's capabilities (measured by a simulation-pretrained affordance model) and the human's estimated availability to help, and (3) an action executor decides the low-level actions to perform or words to say to the human. Our extensive evaluations in simulation and real-world -- on a physical robot with 18 unique human participants over 27 hours -- demonstrate the ability of our method to effectively collaborate with diverse human users, yielding significantly improved task success and user experience than a pure LLM baseline and other agent allocation models. See additional videos and materials at https://robin-lab.cs.utexas.edu/MicoBot/.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Design of Expressive and Trainable Pulse-based Quantum Machine Learning Models</title>
<link>https://arxiv.org/abs/2508.05559</link>
<guid>https://arxiv.org/abs/2508.05559</guid>
<content:encoded><![CDATA[
arXiv:2508.05559v1 Announce Type: cross 
Abstract: Pulse-based Quantum Machine Learning (QML) has emerged as a novel paradigm in quantum artificial intelligence due to its exceptional hardware efficiency. For practical applications, pulse-based models must be both expressive and trainable. Previous studies suggest that pulse-based models under dynamic symmetry can be effectively trained, thanks to a favorable loss landscape that has no barren plateaus. However, the resulting uncontrollability may compromise expressivity when the model is inadequately designed. This paper investigates the requirements for pulse-based QML models to be expressive while preserving trainability. We present a necessary condition pertaining to the system's initial state, the measurement observable, and the underlying dynamical symmetry Lie algebra, supported by numerical simulations. Our findings establish a framework for designing practical pulse-based QML models that balance expressivity and trainability.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L1-Regularized Functional Support Vector Machine</title>
<link>https://arxiv.org/abs/2508.05567</link>
<guid>https://arxiv.org/abs/2508.05567</guid>
<content:encoded><![CDATA[
arXiv:2508.05567v1 Announce Type: cross 
Abstract: In functional data analysis, binary classification with one functional covariate has been extensively studied. We aim to fill in the gap of considering multivariate functional covariates in classification. In particular, we propose an $L_1$-regularized functional support vector machine for binary classification. An accompanying algorithm is developed to fit the classifier. By imposing an $L_1$ penalty, the algorithm enables us to identify relevant functional covariates of the binary response. Numerical results from simulations and one real-world application demonstrate that the proposed classifier enjoys good performance in both prediction and feature selection.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation</title>
<link>https://arxiv.org/abs/2508.05570</link>
<guid>https://arxiv.org/abs/2508.05570</guid>
<content:encoded><![CDATA[
arXiv:2508.05570v1 Announce Type: cross 
Abstract: In this paper, we study the bias and high-order error bounds of the Linear Stochastic Approximation (LSA) algorithm with Polyak-Ruppert (PR) averaging under Markovian noise. We focus on the version of the algorithm with constant step size $\alpha$ and propose a novel decomposition of the bias via a linearization technique. We analyze the structure of the bias and show that the leading-order term is linear in $\alpha$ and cannot be eliminated by PR averaging. To address this, we apply the Richardson-Romberg (RR) extrapolation procedure, which effectively cancels the leading bias term. We derive high-order moment bounds for the RR iterates and show that the leading error term aligns with the asymptotically optimal covariance matrix of the vanilla averaged LSA iterates.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations</title>
<link>https://arxiv.org/abs/2508.05625</link>
<guid>https://arxiv.org/abs/2508.05625</guid>
<content:encoded><![CDATA[
arXiv:2508.05625v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have started to demonstrate the ability to persuade humans, yet our understanding of how this dynamic transpires is limited. Recent work has used linear probes, lightweight tools for analyzing model representations, to study various LLM skills such as the ability to model user sentiment and political perspective. Motivated by this, we apply probes to study persuasion dynamics in natural, multi-turn conversations. We leverage insights from cognitive science to train probes on distinct aspects of persuasion: persuasion success, persuadee personality, and persuasion strategy. Despite their simplicity, we show that they capture various aspects of persuasion at both the sample and dataset levels. For instance, probes can identify the point in a conversation where the persuadee was persuaded or where persuasive success generally occurs across the entire dataset. We also show that in addition to being faster than expensive prompting-based approaches, probes can do just as well and even outperform prompting in some settings, such as when uncovering persuasion strategy. This suggests probes as a plausible avenue for studying other complex behaviours such as deception and manipulation, especially in multi-turn settings and large-scale dataset analysis where prompting-based methods would be computationally inefficient.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling</title>
<link>https://arxiv.org/abs/2508.05634</link>
<guid>https://arxiv.org/abs/2508.05634</guid>
<content:encoded><![CDATA[
arXiv:2508.05634v1 Announce Type: cross 
Abstract: Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Random Forest and its application to data approximation</title>
<link>https://arxiv.org/abs/1909.00659</link>
<guid>https://arxiv.org/abs/1909.00659</guid>
<content:encoded><![CDATA[
arXiv:1909.00659v2 Announce Type: replace 
Abstract: We present a new way of constructing an ensemble classifier, named the Guided Random Forest (GRAF) in the sequel. GRAF extends the idea of building oblique decision trees with localized partitioning to obtain a global partitioning. We show that global partitioning bridges the gap between decision trees and boosting algorithms. We empirically demonstrate that global partitioning reduces the generalization error bound. Results on 115 benchmark datasets show that GRAF yields comparable or better results on a majority of datasets. We also present a new way of approximating the datasets in the framework of random forests.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex Conversion</title>
<link>https://arxiv.org/abs/2302.03775</link>
<guid>https://arxiv.org/abs/2302.03775</guid>
<content:encoded><![CDATA[
arXiv:2302.03775v3 Announce Type: replace 
Abstract: We present new algorithms for optimizing non-smooth, non-convex stochastic objectives based on a novel analysis technique. This improves the current best-known complexity for finding a $(\delta,\epsilon)$-stationary point from $O(\epsilon^{-4}\delta^{-1})$ stochastic gradient queries to $O(\epsilon^{-3}\delta^{-1})$, which we also show to be optimal. Our primary technique is a reduction from non-smooth non-convex optimization to online learning, after which our results follow from standard regret bounds in online learning. For deterministic and second-order smooth objectives, applying more advanced optimistic online learning techniques enables a new complexity of $O(\epsilon^{-1.5}\delta^{-0.5})$. Our techniques also recover all optimal or best-known results for finding $\epsilon$ stationary points of smooth or second-order smooth objectives in both stochastic and deterministic settings.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eliciting Latent Predictions from Transformers with the Tuned Lens</title>
<link>https://arxiv.org/abs/2303.08112</link>
<guid>https://arxiv.org/abs/2303.08112</guid>
<content:encoded><![CDATA[
arXiv:2303.08112v5 Announce Type: replace 
Abstract: We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the tuned lens, is a refinement of the earlier "logit lens" technique, which yielded useful insights but is often brittle.
  We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas</title>
<link>https://arxiv.org/abs/2309.14610</link>
<guid>https://arxiv.org/abs/2309.14610</guid>
<content:encoded><![CDATA[
arXiv:2309.14610v4 Announce Type: replace 
Abstract: Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into six distinct city-specific levels. The model is interpretable and enables feature analysis of areas within each flood-risk level, allowing for the identification of the three archetypes shaping the highest flood risk within each MSA. Flood risk is found to be spatially distributed in a hierarchical structure within each MSA, where the core city disproportionately bears the highest flood risk. Multiple cities are found to have high overall flood-risk levels and low spatial inequality, indicating limited options for balancing urban development and flood-risk reduction. Relevant flood-risk reduction strategies are discussed considering ways that the highest flood risk and uneven spatial distribution of flood risk are formed.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SincVAE: A new semi-supervised approach to improve anomaly detection on EEG data using SincNet and variational autoencoder</title>
<link>https://arxiv.org/abs/2406.17537</link>
<guid>https://arxiv.org/abs/2406.17537</guid>
<content:encoded><![CDATA[
arXiv:2406.17537v2 Announce Type: replace 
Abstract: Over the past few decades, electroencephalography (EEG) monitoring has become a pivotal tool for diagnosing neurological disorders, particularly for detecting seizures. Epilepsy, one of the most prevalent neurological diseases worldwide, affects approximately the 1 \% of the population. These patients face significant risks, underscoring the need for reliable, continuous seizure monitoring in daily life. Most of the techniques discussed in the literature rely on supervised Machine Learning (ML) methods. However, the challenge of accurately labeling variations in epileptic EEG waveforms complicates the use of these approaches. Additionally, the rarity of ictal events introduces an high imbalancing within the data, which could lead to poor prediction performance in supervised learning approaches. Instead, a semi-supervised approach allows to train the model only on data not containing seizures, thus avoiding the issues related to the data imbalancing. This work proposes a semi-supervised approach for detecting epileptic seizures from EEG data, utilizing a novel Deep Learning-based method called SincVAE. This proposal incorporates the learning of an ad-hoc array of bandpass filter as a first layer of a Variational Autoencoder (VAE), potentially eliminating the preprocessing stage where informative band frequencies are identified and isolated. Results indicate that SincVAE improves seizure detection in EEG data and is capable of identifying early seizures during the preictal stage as well as monitoring patients throughout the postictal stage.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Deep Neural Network using Euclidean Distance</title>
<link>https://arxiv.org/abs/2410.18321</link>
<guid>https://arxiv.org/abs/2410.18321</guid>
<content:encoded><![CDATA[
arXiv:2410.18321v2 Announce Type: replace 
Abstract: Uncertainty is a fundamental aspect of real-world scenarios, where perfect information is rarely available. Humans naturally develop complex internal models to navigate incomplete data and effectively respond to unforeseen or partially observed events. In machine learning, Focal Loss is commonly used to reduce misclassification rates by emphasizing hard-to-classify samples. However, it does not guarantee well-calibrated predicted probabilities and may result in models that are overconfident or underconfident. High calibration error indicates a misalignment between predicted probabilities and actual outcomes, affecting model reliability. This research introduces a novel loss function called Focal Calibration Loss (FCL), designed to improve probability calibration while retaining the advantages of Focal Loss in handling difficult samples. By minimizing the Euclidean norm through a strictly proper loss, FCL penalizes the instance-wise calibration error and constrains bounds. We provide theoretical validation for proposed method and apply it to calibrate CheXNet for potential deployment in web-based health-care systems. Extensive evaluations on various models and datasets demonstrate that our method achieves SOTA performance in both calibration and accuracy metrics.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Clustering Beyond Worst-Case Guarantees</title>
<link>https://arxiv.org/abs/2411.01576</link>
<guid>https://arxiv.org/abs/2411.01576</guid>
<content:encoded><![CDATA[
arXiv:2411.01576v2 Announce Type: replace 
Abstract: We study the explainable clustering problem first posed by Moshkovitz, Dasgupta, Rashtchian, and Frost (ICML 2020). The goal of explainable clustering is to fit an axis-aligned decision tree with $K$ leaves and minimal clustering cost (where every leaf is a cluster). The fundamental theoretical question in this line of work is the \textit{price of explainability}, defined as the ratio between the clustering cost of the tree and the optimal cost. Numerous papers have provided worst-case guarantees on this quantity. For $K$-medians, it has recently been shown that the worst-case price of explainability is $\Theta(\log K)$. While this settles the matter from a data-agnostic point of view, two important questions remain unanswered: Are tighter guarantees possible for well-clustered data? And can we trust decision trees to recover underlying cluster structures? In this paper, we place ourselves in a statistical setting of mixture models to answer both questions. We prove that better guarantees are indeed feasible for well-clustered data. Our algorithm takes as input a mixture model and constructs a tree in data-independent time. We then extend our analysis to kernel clustering, deriving new guarantees that significantly improve over existing worst-case bounds.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable Newborn Screening: Automated General Movement Assessment in Uncontrolled Settings</title>
<link>https://arxiv.org/abs/2411.09821</link>
<guid>https://arxiv.org/abs/2411.09821</guid>
<content:encoded><![CDATA[
arXiv:2411.09821v4 Announce Type: replace 
Abstract: General movements (GMs) are spontaneous, coordinated body movements in infants that offer valuable insights into the developing nervous system. Assessed through the Prechtl GM Assessment (GMA), GMs are reliable predictors for neurodevelopmental disorders. However, GMA requires specifically trained clinicians, who are limited in number. To scale up newborn screening, there is a need for an algorithm that can automatically classify GMs from infant video recordings. This data poses challenges, including variability in recording length, device type, and setting, with each video coarsely annotated for overall movement quality. In this work, we introduce a tool for extracting features from these recordings and explore various machine learning techniques for automated GM classification.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A solvable generative model with a linear, one-step denoiser</title>
<link>https://arxiv.org/abs/2411.17807</link>
<guid>https://arxiv.org/abs/2411.17807</guid>
<content:encoded><![CDATA[
arXiv:2411.17807v3 Announce Type: replace 
Abstract: We develop an analytically tractable single-step diffusion model based on a linear denoiser and present an explicit formula for the Kullback-Leibler divergence between the generated and sampling distribution, taken to be isotropic Gaussian, showing the effect of finite diffusion time and noise scale. Our study further reveals that the monotonic fall phase of Kullback-Leibler divergence begins when the training dataset size reaches the dimension of the data points. Finally, for large-scale practical diffusion models, we explain why a higher number of diffusion steps enhances production quality based on the theoretical arguments presented before.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PL-DCP: A Pairwise Learning framework with Domain and Class Prototypes for EEG emotion recognition under unseen target conditions</title>
<link>https://arxiv.org/abs/2412.00082</link>
<guid>https://arxiv.org/abs/2412.00082</guid>
<content:encoded><![CDATA[
arXiv:2412.00082v2 Announce Type: replace 
Abstract: Electroencephalogram (EEG) signals serve as a powerful tool in affective Brain-Computer Interfaces (aBCIs) and play a crucial role in affective computing. In recent years, the introduction of deep learning techniques has significantly advanced the development of aBCIs. However, the current emotion recognition methods based on deep transfer learning face the challenge of the dual dependence of the model on source domain and target domain, As well as being affected by label noise, which seriously affects the performance and generalization ability of the model. To overcome this limitation, we proposes a Pairwise Learning framework with Domain and Category Prototypes for EEG emotion recognition under unseen target conditions (PL-DCP), and integrating concepts of feature disentanglement and prototype inference. Here, the feature disentanglement module extracts and decouples the emotional EEG features to form domain features and class features, and further calculates the dual prototype representation. The Domain-pprototype captures the individual variations across subjects, while the class-prototype captures the cross-individual commonality of emotion categories. In addition, the pairwise learning strategy effectively reduces the noise effect caused by wrong labels. The PL-DCP framework conducts a systematic experimental evaluation on the published datasets SEED, SEED-IV and SEED-V, and the accuracy are 82.88\%, 65.15\% and 61.29\%, respectively. The results show that compared with other State-of-the-Art(SOTA) Methods, the PL-DCP model still achieves slightly better performance than the deep transfer learning method that requires both source and target data, although the target domain is completely unseen during the training. This work provides an effective and robust potential solution for emotion recognition. The source code is available at https://github.com/WuCB-BCI/PL_DCP.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Representation Modeling for Anomaly Detection</title>
<link>https://arxiv.org/abs/2501.05130</link>
<guid>https://arxiv.org/abs/2501.05130</guid>
<content:encoded><![CDATA[
arXiv:2501.05130v5 Announce Type: replace 
Abstract: Distance-based anomaly detection methods rely on compact in-distribution (ID) embeddings that are well separated from anomalies. However, conventional contrastive learning strategies often struggle to achieve this balance, either promoting excessive variance among inliers or failing to preserve the diversity of outliers. We begin by analyzing the challenges of representation learning for anomaly detection and identify three essential properties for the pretext task: (1) compact clustering of inliers, (2) strong separation between inliers and anomalies, and (3) preservation of diversity among synthetic outliers. Building on this, we propose a structured contrastive objective that redefines positive and negative relationships during training, promoting these properties without requiring explicit anomaly labels. We extend this framework with a patch-based learning and evaluation strategy specifically designed to improve the detection of localized anomalies in industrial settings. Our approach demonstrates significantly faster convergence and improved performance compared to standard contrastive methods. It matches or surpasses anomaly detection methods on both semantic and industrial benchmarks, including methods that rely on discriminative training or explicit anomaly labels.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Transformers Learn Full Bayesian Inference in Context?</title>
<link>https://arxiv.org/abs/2501.16825</link>
<guid>https://arxiv.org/abs/2501.16825</guid>
<content:encoded><![CDATA[
arXiv:2501.16825v2 Announce Type: replace 
Abstract: Transformers have emerged as the dominant architecture in the field of deep learning, with a broad range of applications and remarkable in-context learning (ICL) capabilities. While not yet fully understood, ICL has already proved to be an intriguing phenomenon, allowing transformers to learn in context -- without requiring further training. In this paper, we further advance the understanding of ICL by demonstrating that transformers can perform full Bayesian inference for commonly used statistical models in context. More specifically, we introduce a general framework that builds on ideas from prior fitted networks and continuous normalizing flows and enables us to infer complex posterior distributions for models such as generalized linear models and latent factor models. Extensive experiments on real-world datasets demonstrate that our ICL approach yields posterior samples that are similar in quality to state-of-the-art MCMC or variational inference methods that do not operate in context. The source code for this paper is available at https://github.com/ArikReuter/ICL_for_Full_Bayesian_Inference.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaternion-Hadamard Network: A Novel Defense Against Adversarial Attacks with a New Dataset</title>
<link>https://arxiv.org/abs/2502.10452</link>
<guid>https://arxiv.org/abs/2502.10452</guid>
<content:encoded><![CDATA[
arXiv:2502.10452v2 Announce Type: replace 
Abstract: This paper addresses the vulnerability of deep-learning models designed for rain, snow, and haze removal. Despite enhancing image quality in adverse weather, these models are susceptible to adversarial attacks that compromise their effectiveness. Traditional defenses such as adversarial training and model distillation often require extensive retraining, making them costly and impractical for real-world deployment. While denoising and super-resolution techniques can aid image classification models, they impose high computational demands and introduce visual artifacts that hinder image processing tasks. We propose a model-agnostic defense against first-order white-box adversarial attacks using the Quaternion-Hadamard Network (QHNet) to tackle these challenges. White-box attacks are particularly difficult to defend against since attackers have full access to the model's architecture, weights, and training procedures. Our defense introduces the Quaternion Hadamard Denoising Convolutional Block (QHDCB) and the Quaternion Denoising Residual Block (QDRB), leveraging polynomial thresholding. QHNet incorporates these blocks within an encoder-decoder architecture, enhanced by feature refinement, to effectively neutralize adversarial noise. Additionally, we introduce the Adversarial Weather Conditions Vision Dataset (AWCVD), created by applying first-order gradient attacks on state-of-the-art weather removal techniques in scenarios involving haze, rain streaks, and snow. Using PSNR and SSIM metrics, we demonstrate that QHNet significantly enhances the robustness of low-level computer vision models against adversarial attacks compared with state-of-the-art denoising and super-resolution techniques. The source code and dataset will be released alongside the final version of this paper.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-TabLogic: Preserving Inter-Column Logical Relationships in Synthetic Tabular Data via Prompt-Guided Latent Diffusion</title>
<link>https://arxiv.org/abs/2503.02161</link>
<guid>https://arxiv.org/abs/2503.02161</guid>
<content:encoded><![CDATA[
arXiv:2503.02161v2 Announce Type: replace 
Abstract: Synthetic tabular data are increasingly being used to replace real data, serving as an effective solution that simultaneously protects privacy and addresses data scarcity. However, in addition to preserving global statistical properties, synthetic datasets must also maintain domain-specific logical consistency**-**especially in complex systems like supply chains, where fields such as shipment dates, locations, and product categories must remain logically consistent for real-world usability. Existing generative models often overlook these inter-column relationships, leading to unreliable synthetic tabular data in real-world applications. To address these challenges, we propose LLM-TabLogic, a novel approach that leverages Large Language Model reasoning to capture and compress the complex logical relationships among tabular columns, while these conditional constraints are passed into a Score-based Diffusion model for data generation in latent space. Through extensive experiments on real-world industrial datasets, we evaluate LLM-TabLogic for column reasoning and data generation, comparing it with five baselines including SMOTE and state-of-the-art generative models. Our results show that LLM-TabLogic demonstrates strong generalization in logical inference, achieving over 90% accuracy on unseen tables. Furthermore, our method outperforms all baselines in data generation by fully preserving inter-column relationships while maintaining the best balance between data fidelity, utility, and privacy. This study presents the first method to effectively preserve inter-column relationships in synthetic tabular data generation without requiring domain knowledge, offering new insights for creating logically consistent real-world tabular data.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Vector Quantization for Memory-Efficient Model Merging</title>
<link>https://arxiv.org/abs/2503.06921</link>
<guid>https://arxiv.org/abs/2503.06921</guid>
<content:encoded><![CDATA[
arXiv:2503.06921v2 Announce Type: replace 
Abstract: Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching LLMs How to Learn with Contextual Fine-Tuning</title>
<link>https://arxiv.org/abs/2503.09032</link>
<guid>https://arxiv.org/abs/2503.09032</guid>
<content:encoded><![CDATA[
arXiv:2503.09032v2 Announce Type: replace 
Abstract: Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human's learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, "can prompting help us teach LLMs how to learn". In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model's interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Optimized Piecewise Polynomial Approximation Utilizing Modern Machine Learning Optimizers</title>
<link>https://arxiv.org/abs/2503.09329</link>
<guid>https://arxiv.org/abs/2503.09329</guid>
<content:encoded><![CDATA[
arXiv:2503.09329v3 Announce Type: replace 
Abstract: This work explores an extension of machine learning-optimized piecewise polynomial approximation by incorporating energy optimization as an additional objective. Traditional closed-form solutions enable continuity and approximation targets but lack flexibility in accommodating complex optimization goals. By leveraging modern gradient descent optimizers within TensorFlow, we introduce a framework that minimizes elastic strain energy in cam profiles, leading to smoother motion. Experimental results confirm the effectiveness of this approach, demonstrating its potential to Pareto-efficiently trade approximation quality against energy consumption.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting the Lifespan of Industrial Printheads with Survival Analysis</title>
<link>https://arxiv.org/abs/2504.07638</link>
<guid>https://arxiv.org/abs/2504.07638</guid>
<content:encoded><![CDATA[
arXiv:2504.07638v2 Announce Type: replace 
Abstract: Accurately predicting the lifespan of critical device components is essential for maintenance planning and production optimization, making it a topic of significant interest in both academia and industry. In this work, we investigate the use of survival analysis for predicting the lifespan of production printheads developed by Canon Production Printing. Specifically, we focus on the application of five techniques to estimate survival probabilities and failure rates: the Kaplan-Meier estimator, Cox proportional hazard model, Weibull accelerated failure time model, random survival forest, and gradient boosting. The resulting estimates are further refined using isotonic regression and subsequently aggregated to determine the expected number of failures. The predictions are then validated against real-world ground truth data across multiple time windows to assess model reliability. Our quantitative evaluation using three performance metrics demonstrates that survival analysis outperforms industry-standard baseline methods for printhead lifespan prediction.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Methods for Detecting Thermal Runaway Events in Battery Production Lines</title>
<link>https://arxiv.org/abs/2504.08632</link>
<guid>https://arxiv.org/abs/2504.08632</guid>
<content:encoded><![CDATA[
arXiv:2504.08632v2 Announce Type: replace 
Abstract: One of the key safety considerations of battery manufacturing is thermal runaway, the uncontrolled increase in temperature which can lead to fires, explosions, and emissions of toxic gasses. As such, development of automated systems capable of detecting such events is of considerable importance in both academic and industrial contexts. In this work, we investigate the use of deep learning for detecting thermal runaway in the battery production line of VDL Nedcar, a Dutch automobile manufacturer. Specifically, we collect data from the production line to represent both baseline (non thermal runaway) and thermal runaway conditions. Thermal runaway was simulated through the use of external heat and smoke sources. The data consisted of both optical and thermal images which were then preprocessed and fused before serving as input to our models. In this regard, we evaluated three deep-learning models widely used in computer vision including shallow convolutional neural networks, residual neural networks, and vision transformers on two performance metrics. Furthermore, we evaluated these models using explainability methods to gain insight into their ability to capture the relevant feature information from their inputs. The obtained results indicate that the use of deep learning is a viable approach to thermal runaway detection in battery production lines.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable Bayesian Optimization via Gradient-Informed Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2504.10076</link>
<guid>https://arxiv.org/abs/2504.10076</guid>
<content:encoded><![CDATA[
arXiv:2504.10076v2 Announce Type: replace 
Abstract: Bayesian optimization (BO) is a widely used method for data-driven optimization that generally relies on zeroth-order data of objective function to construct probabilistic surrogate models. These surrogates guide the exploration-exploitation process toward finding global optimum. While Gaussian processes (GPs) are commonly employed as surrogates of the unknown objective function, recent studies have highlighted the potential of Bayesian neural networks (BNNs) as scalable and flexible alternatives. Moreover, incorporating gradient observations into GPs, when available, has been shown to improve BO performance. However, the use of gradients within BNN surrogates remains unexplored. By leveraging automatic differentiation, gradient information can be seamlessly integrated into BNN training, resulting in more informative surrogates for BO. We propose a gradient-informed loss function for BNN training, effectively augmenting function observations with local gradient information. The effectiveness of this approach is demonstrated on well-known benchmarks in terms of improved BNN predictions and faster BO convergence as the number of decision variables increases.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structure-Preserving Framework for Solving Parabolic Partial Differential Equations with Neural Networks</title>
<link>https://arxiv.org/abs/2504.10273</link>
<guid>https://arxiv.org/abs/2504.10273</guid>
<content:encoded><![CDATA[
arXiv:2504.10273v2 Announce Type: replace 
Abstract: Solving partial differential equations (PDEs) with neural networks (NNs) has shown great potential in various scientific and engineering fields. However, most existing NN solvers mainly focus on satisfying the given PDE formulas in the strong or weak sense, without explicitly considering some intrinsic physical properties, such as mass and momentum conservation, or energy dissipation. This limitation may result in nonphysical or unstable numerical solutions, particularly in long-term simulations. To address this issue, we propose ``Sidecar'', a novel framework that enhances the physical consistency of existing NN solvers for solving parabolic PDEs. Inspired by the time-dependent spectral renormalization approach, our Sidecar framework introduces a small network as a copilot, guiding the primary function-learning NN solver to respect the structure-preserving properties. Our framework is highly flexible, allowing the preservation of various physical quantities for different PDEs to be incorporated into a wide range of NN solvers. Experimental results on some benchmark problems demonstrate significant improvements brought by the proposed framework to both accuracy and structure preservation of existing NN solvers.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Stability Guarantees for Feature Attributions</title>
<link>https://arxiv.org/abs/2504.13787</link>
<guid>https://arxiv.org/abs/2504.13787</guid>
<content:encoded><![CDATA[
arXiv:2504.13787v3 Announce Type: replace 
Abstract: Stability guarantees have emerged as a principled way to evaluate feature attributions, but existing certification methods rely on heavily smoothed classifiers and often produce conservative guarantees. To address these limitations, we introduce soft stability and propose a simple, model-agnostic, sample-efficient stability certification algorithm (SCA) that yields non-trivial and interpretable guarantees for any attribution method. Moreover, we show that mild smoothing achieves a more favorable trade-off between accuracy and stability, avoiding the aggressive compromises made in prior certification methods. To explain this behavior, we use Boolean function analysis to derive a novel characterization of stability under smoothing. We evaluate SCA on vision and language tasks and demonstrate the effectiveness of soft stability in measuring the robustness of explanation methods.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades</title>
<link>https://arxiv.org/abs/2505.01415</link>
<guid>https://arxiv.org/abs/2505.01415</guid>
<content:encoded><![CDATA[
arXiv:2505.01415v2 Announce Type: replace 
Abstract: The Everglades play a crucial role in flood and drought regulation, water resource planning, and ecosystem management in the surrounding regions. However, traditional physics-based and statistical methods for predicting water levels often face significant challenges, including high computational costs and limited adaptability to diverse or unforeseen conditions. Recent advancements in large time series models have demonstrated the potential to address these limitations, with state-of-the-art deep learning and foundation models achieving remarkable success in time series forecasting across various domains. Despite this progress, their application to critical environmental systems, such as the Everglades, remains underexplored. In this study, we fill the gap by investigating twelve task-specific models and five time series foundation models across six categories for a real-world application focused on water level prediction in the Everglades. Our primary results show that the foundation model Chronos significantly outperforms all other models while the remaining foundation models exhibit relatively poor performance. We also noticed that the performance of task-specific models varies with the model architectures, and discussed the possible reasons. We hope our study and findings will inspire the community to explore the applicability of large time series models in hydrological applications. The code and data are available at https://github.com/rahuul2992000/Everglades-Benchmark.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation</title>
<link>https://arxiv.org/abs/2505.03983</link>
<guid>https://arxiv.org/abs/2505.03983</guid>
<content:encoded><![CDATA[
arXiv:2505.03983v2 Announce Type: replace 
Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful tools for generative modeling. However, their sequential computation requirements lead to significant inference-time bottlenecks. In this work, we utilize the connection between DDPMs and Stochastic Localization to prove that, under an appropriate reparametrization, the increments of DDPM satisfy an exchangeability property. This general insight enables near-black-box adaptation of various performance optimization techniques from autoregressive models to the diffusion setting. To demonstrate this, we introduce \emph{Autospeculative Decoding} (ASD), an extension of the widely used speculative decoding algorithm to DDPMs that does not require any auxiliary draft models. Our theoretical analysis shows that ASD achieves a $\tilde{O} (K^{\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM. We also demonstrate that a practical implementation of autospeculative decoding accelerates DDPM inference significantly in various domains.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLSR: Reinforcement Learning from Self Reward</title>
<link>https://arxiv.org/abs/2505.08827</link>
<guid>https://arxiv.org/abs/2505.08827</guid>
<content:encoded><![CDATA[
arXiv:2505.08827v2 Announce Type: replace 
Abstract: Large language models can generate solutions to complex problems, but training them with reinforcement learning typically requires verifiable rewards that are expensive to create and not possible for all domains. We demonstrate that LLMs can effectively self-improve through self-judging without reference solutions, leveraging the inherent asymmetry between generating and verifying solutions. Our experiments show that models can provide reliable reward signals without ground truth answers, enabling reinforcement learning in domains where verifiable rewards are impractical. By implementing self-judging across Countdown puzzles and integration problems, we achieve performance comparable to formal verification without ground truth solutions. Most notably, Qwen 2.5 7B DeepSeek Distilled trained with self-rewards qualifies for the prestigious MIT Integration Bee competition, performance through self-supervised improvement. When combined with synthetic question generation, we establish a complete self-improvement loop where models generate practice problems, solve them, and evaluate their own performance without any external validation. Our findings demonstrate that LLM judges can provide effective reward signals for training, unlocking reinforcement learning in countless domains previously limited by reward engineering challenges. This work represents a significant step toward autonomous AI systems that continuously improve through self-directed learning rather than human-guided training, potentially accelerating progress across domains where training data is scarce or evaluation is complex.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JULI: Jailbreak Large Language Models by Self-Introspection</title>
<link>https://arxiv.org/abs/2505.11790</link>
<guid>https://arxiv.org/abs/2505.11790</guid>
<content:encoded><![CDATA[
arXiv:2505.11790v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are trained with safety alignment to prevent generating malicious content. Although some attacks have highlighted vulnerabilities in these safety-aligned LLMs, they typically have limitations, such as necessitating access to the model weights or the generation process. Since proprietary models through API-calling do not grant users such permissions, these attacks find it challenging to compromise them. In this paper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks LLMs by manipulating the token log probabilities, using a tiny plug-in block, BiasNet. JULI relies solely on the knowledge of the target LLM's predicted token log probabilities. It can effectively jailbreak API-calling LLMs under a black-box setting and knowing only top-$5$ token log probabilities. Our approach demonstrates superior effectiveness, outperforming existing state-of-the-art (SOTA) approaches across multiple metrics.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.19616</link>
<guid>https://arxiv.org/abs/2505.19616</guid>
<content:encoded><![CDATA[
arXiv:2505.19616v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across tasks, yet they often exhibit difficulty in distinguishing task-relevant from irrelevant signals, particularly in tasks like Visual Question Answering (VQA), which can lead to susceptibility to misleading or spurious inputs. We refer to this broader limitation as the Cross-Modality Competency Problem: the model's inability to fairly evaluate all modalities. This vulnerability becomes more evident in modality-specific tasks such as image classification or pure text question answering, where models are expected to rely solely on one modality. In such tasks, spurious information from irrelevant modalities often leads to significant performance degradation. We refer to this failure as Modality Interference, which serves as a concrete and measurable instance of the cross-modality competency problem. We further design a perturbation-based causal diagnostic experiment to verify and quantify this problem. To mitigate modality interference, we propose a novel framework to fine-tune MLLMs, including perturbation-based data augmentations with both heuristic perturbations and adversarial perturbations via Projected Gradient Descent (PGD), and a consistency regularization strategy applied to model outputs with original and perturbed inputs. Experiments on multiple benchmark datasets (image-heavy, text-heavy, and VQA tasks) and multiple model families with different scales demonstrate significant improvements in robustness and cross-modality competency, indicating our method's effectiveness in boosting unimodal reasoning ability while enhancing performance on multimodal tasks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label Leakage in Federated Inertial-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.20924</link>
<guid>https://arxiv.org/abs/2505.20924</guid>
<content:encoded><![CDATA[
arXiv:2505.20924v2 Announce Type: replace 
Abstract: While prior work has shown that Federated Learning updates can leak sensitive information, label reconstruction attacks, which aim to recover input labels from shared gradients, have not yet been examined in the context of Human Activity Recognition (HAR). Given the sensitive nature of activity labels, this study evaluates the effectiveness of state-of-the-art gradient-based label leakage attacks on HAR benchmark datasets. Our findings show that the number of activity classes, sampling strategy, and class imbalance are critical factors influencing the extent of label leakage, with reconstruction accuracies reaching well-above 90% on two benchmark datasets, even for trained models. Moreover, we find that Local Differential Privacy techniques such as gradient noise and clipping offer only limited protection, as certain attacks still reliably infer both majority and minority class labels. We conclude by offering practical recommendations for the privacy-aware deployment of federated HAR systems and identify open challenges for future research. Code to reproduce our experiments is publicly available via github.com/mariusbock/leakage_har.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conservative classifiers do consistently well with improving agents: characterizing statistical and online learning</title>
<link>https://arxiv.org/abs/2506.05252</link>
<guid>https://arxiv.org/abs/2506.05252</guid>
<content:encoded><![CDATA[
arXiv:2506.05252v2 Announce Type: replace 
Abstract: Machine learning is now ubiquitous in societal decision-making, for example in evaluating job candidates or loan applications, and it is increasingly important to take into account how classified agents will react to the learning algorithms. The majority of recent literature on strategic classification has focused on reducing and countering deceptive behaviors by the classified agents, but recent work of Attias et al. identifies surprising properties of learnability when the agents genuinely improve in order to attain the desirable classification, such as smaller generalization error than standard PAC-learning. In this paper we characterize so-called learnability with improvements across multiple new axes. We introduce an asymmetric variant of minimally consistent concept classes and use it to provide an exact characterization of proper learning with improvements in the realizable setting. While prior work studies learnability only under general, arbitrary agent improvement regions, we give positive results for more natural Euclidean ball improvement sets. In particular, we characterize improper learning under a mild generative assumption on the data distribution. We further show how to learn in more challenging settings, achieving lower generalization error under well-studied bounded noise models and obtaining mistake bounds in realizable and agnostic online learning. We resolve open questions posed by Attias et al. for both proper and improper learning.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient Sequence Modeling</title>
<link>https://arxiv.org/abs/2507.08567</link>
<guid>https://arxiv.org/abs/2507.08567</guid>
<content:encoded><![CDATA[
arXiv:2507.08567v2 Announce Type: replace 
Abstract: We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a novel recursive generalization of the encoder-only Transformer architecture, which achieves better perplexity than a standard Transformer and allows for the dynamic scaling of compute resources at test time. This simple, recursive approach is a complement to scaling large language model (LLM) performance through parameter and token counts. AbbIE performs its iterations in latent space, but unlike latent reasoning models, does not require a specialized dataset or training protocol. We show that AbbIE upward generalizes (ability to generalize to arbitrary iteration lengths) at test time by only using 2 iterations during train time, far outperforming alternative iterative methods. AbbIE's ability to scale its computational expenditure based on the complexity of the task gives it an up to \textbf{12\%} improvement in zero-shot in-context learning tasks versus other iterative and standard methods and up to 5\% improvement in language perplexity. The results from this study open a new avenue to Transformer performance scaling. We perform all of our evaluations on model sizes up to 350M parameters.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Evidential Clustering</title>
<link>https://arxiv.org/abs/2507.12192</link>
<guid>https://arxiv.org/abs/2507.12192</guid>
<content:encoded><![CDATA[
arXiv:2507.12192v2 Announce Type: replace 
Abstract: Unsupervised classification is a fundamental machine learning problem. Real-world data often contain imperfections, characterized by uncertainty and imprecision, which are not well handled by traditional methods. Evidential clustering, based on Dempster-Shafer theory, addresses these challenges. This paper explores the underexplored problem of explaining evidential clustering results, which is crucial for high-stakes domains such as healthcare. Our analysis shows that, in the general case, representativity is a necessary and sufficient condition for decision trees to serve as abductive explainers. Building on the concept of representativity, we generalize this idea to accommodate partial labeling through utility functions. These functions enable the representation of "tolerable" mistakes, leading to the definition of evidential mistakeness as explanation cost and the construction of explainers tailored to evidential classifiers. Finally, we propose the Iterative Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable and cautious decision tree explanations for evidential clustering functions. We validate the proposed algorithm on synthetic and real-world data. Taking into account the decision-maker's preferences, we were able to provide an explanation that was satisfactory up to 93% of the time.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning What Matters: Probabilistic Task Selection via Mutual Information for Model Finetuning</title>
<link>https://arxiv.org/abs/2507.12612</link>
<guid>https://arxiv.org/abs/2507.12612</guid>
<content:encoded><![CDATA[
arXiv:2507.12612v2 Announce Type: replace 
Abstract: The performance of finetuned large language models (LLMs) hinges critically on the composition of the training mixture. However, selecting an optimal blend of task datasets remains a largely manual, heuristic driven process, with practitioners often relying on uniform or size based sampling strategies. We introduce TASKPGM, a principled and scalable framework for mixture optimization that selects continuous task proportions by minimizing an energy function over a Markov Random Field (MRF). Task relationships are modeled using behavioral divergences such as Jensen Shannon Divergence and Pointwise Mutual Information computed from the predictive distributions of single task finetuned models. Our method yields a closed form solution under simplex constraints and provably balances representativeness and diversity among tasks. We provide theoretical guarantees, including weak submodularity for budgeted variants, and demonstrate consistent empirical improvements on Llama 2 and Mistral across evaluation suites such as MMLU and BIGBench. Beyond performance, TASKPGM offers interpretable insights into task influence and mixture composition, making it a powerful tool for efficient and robust LLM finetuning.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Beats Autoregressive in Data-Constrained Settings</title>
<link>https://arxiv.org/abs/2507.15857</link>
<guid>https://arxiv.org/abs/2507.15857</guid>
<content:encoded><![CDATA[
arXiv:2507.15857v5 Announce Type: replace 
Abstract: Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We interpret this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike AR's fixed left-to-right factorization. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: https://diffusion-scaling.github.io.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2507.18376</link>
<guid>https://arxiv.org/abs/2507.18376</guid>
<content:encoded><![CDATA[
arXiv:2507.18376v5 Announce Type: replace 
Abstract: With the global population increasing and arable land resources becoming increasingly limited, smart and precision agriculture have emerged as essential directions for sustainable agricultural development. Artificial intelligence (AI), particularly deep learning models, has been widely adopted in applications such as crop monitoring, pest detection, and yield prediction. Among recent generative models, diffusion models have demonstrated considerable potential in agricultural image processing, data augmentation, and remote sensing analysis. Compared to traditional generative adversarial networks (GANs), diffusion models exhibit greater training stability and superior image generation quality, effectively addressing challenges such as limited annotated datasets and imbalanced sample distributions in agricultural scenarios. This paper reviews recent advancements in the application of diffusion models within agriculture, focusing on their roles in crop disease and pest detection, remote sensing image enhancement, crop growth prediction, and agricultural resource management. Diffusion models have been found useful in improving tasks like image generation, denoising, and data augmentation in agriculture, especially when environmental noise or variability is present. While their high computational requirements and limited generalizability across domains remain concerns, the approach is gradually proving effective in real-world applications such as precision crop monitoring. As research progresses, these models may help support sustainable agriculture and address emerging challenges in food systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOASF: A Unified Framework for Speeding up Automatic Machine Learning via Adaptive Successive Filtering</title>
<link>https://arxiv.org/abs/2507.20446</link>
<guid>https://arxiv.org/abs/2507.20446</guid>
<content:encoded><![CDATA[
arXiv:2507.20446v2 Announce Type: replace 
Abstract: Machine learning has been making great success in many application areas. However, for the non-expert practitioners, it is always very challenging to address a machine learning task successfully and efficiently. Finding the optimal machine learning model or the hyperparameter combination set from a large number of possible alternatives usually requires considerable expert knowledge and experience. To tackle this problem, we propose a combined Bayesian Optimization and Adaptive Successive Filtering algorithm (BOASF) under a unified multi-armed bandit framework to automate the model selection or the hyperparameter optimization. Specifically, BOASF consists of multiple evaluation rounds in each of which we select promising configurations for each arm using the Bayesian optimization. Then, ASF can early discard the poor-performed arms adaptively using a Gaussian UCB-based probabilistic model. Furthermore, a Softmax model is employed to adaptively allocate available resources for each promising arm that advances to the next round. The arm with a higher probability of advancing will be allocated more resources. Experimental results show that BOASF is effective for speeding up the model selection and hyperparameter optimization processes while achieving robust and better prediction performance than the existing state-of-the-art automatic machine learning methods. Moreover, BOASF achieves better anytime performance under various time budgets.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition</title>
<link>https://arxiv.org/abs/2507.20997</link>
<guid>https://arxiv.org/abs/2507.20997</guid>
<content:encoded><![CDATA[
arXiv:2507.20997v2 Announce Type: replace 
Abstract: In real-world machine learning deployments, models must be continually updated, composed, and when required, selectively undone. However, existing approaches to model merging and continual learning often suffer from task interference, catastrophic forgetting, or lack of reversibility. We propose Modular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework that enables scalable, interference-free, and reversible composition of fine-tuned models. Each task-specific model is encoded as a delta from a shared base and projected into an orthogonal subspace to eliminate conflict. These projected deltas are then merged via gradient-based optimization to form a unified model that retains performance across tasks. Our approach supports continual integration of new models, structured unmerging for compliance such as GDPR requirements, and model stability via elastic weight consolidation and synthetic replay. Extensive experiments on vision and natural language processing benchmarks demonstrate that MDM-OC outperforms prior baselines in accuracy, backward transfer, and unmerge fidelity, while remaining memory-efficient and computationally tractable. This framework offers a principled solution for modular and compliant AI system design.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An MLI-Guided Framework for Subgroup-Aware Modeling in Electronic Health Records (AdaptHetero)</title>
<link>https://arxiv.org/abs/2507.21197</link>
<guid>https://arxiv.org/abs/2507.21197</guid>
<content:encoded><![CDATA[
arXiv:2507.21197v3 Announce Type: replace 
Abstract: Machine learning interpretation (MLI) has primarily been leveraged to foster clinician trust and extract insights from electronic health records (EHRs), rather than to guide subgroup-specific, operationalizable modeling strategies. To bridge this gap, we propose AdaptHetero, a novel MLI-driven framework that transforms interpretability insights into actionable guidance for tailoring model training and evaluation across subpopulations. Evaluated on three large-scale EHR datasets -- GOSSIS-1-eICU, WiDS, and MIMIC-IV -- AdaptHetero consistently uncovers heterogeneous model behaviors in predicting ICU mortality, in-hospital death, and hidden hypoxemia. Integrating SHAP-based interpretation with unsupervised clustering, AdaptHetero identifies clinically meaningful, subgroup-specific characteristics, improving predictive performance across many subpopulations (with gains up to 174.39 percent) while proactively flagging potential risks in others. These results highlight the framework's promise for more robust, equitable, and context-aware clinical deployment.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Leakage and Redundancy in the LIT-PCBA Benchmark</title>
<link>https://arxiv.org/abs/2507.21404</link>
<guid>https://arxiv.org/abs/2507.21404</guid>
<content:encoded><![CDATA[
arXiv:2507.21404v2 Announce Type: replace 
Abstract: LIT-PCBA is widely used to benchmark virtual screening models, but our audit reveals that it is fundamentally compromised. We find extensive data leakage and molecular redundancy across its splits, including 2D-identical ligands within and across partitions, pervasive analog overlap, and low-diversity query sets. In ALDH1 alone, for instance, 323 active training -- validation analog pairs occur at ECFP4 Tanimoto similarity $\geq 0.6$; across all targets, 2,491 2D-identical inactives appear in both training and validation, with very few corresponding actives. These overlaps allow models to succeed through scaffold memorization rather than generalization, inflating enrichment factors and AUROC scores. These flaws are not incidental -- they are so severe that a trivial memorization-based baseline with no learnable parameters can exploit them to match or exceed the reported performance of state-of-the-art deep learning and 3D-similarity models. As a result, nearly all published results on LIT-PCBA are undermined. Even models evaluated in "zero-shot" mode are affected by analog leakage into the query set, weakening claims of generalization. In its current form, the benchmark does not measure a model's ability to recover novel chemotypes and should not be taken as evidence of methodological progress.
  All code, data, and baseline implementations are available at: https://github.com/sievestack/LIT-PCBA-audit
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness on Multiple-Choice Q&amp;A</title>
<link>https://arxiv.org/abs/2402.13213</link>
<guid>https://arxiv.org/abs/2402.13213</guid>
<content:encoded><![CDATA[
arXiv:2402.13213v4 Announce Type: replace-cross 
Abstract: We study 15 large language models (LLMs) fine-tuned for chat and find that their maximum softmax probabilities (MSPs) are consistently miscalibrated on multiple-choice Q&amp;A. However, those MSPs might still encode useful uncertainty information. Specifically, we hypothesized that wrong answers would be associated with smaller MSPs compared to correct answers. Via rigorous statistical testing, we show that this hypothesis holds for models which perform well on the underlying Q&amp;A task. We also find a strong direction correlation between Q&amp;A accuracy and MSP correctness prediction, while finding no correlation between Q&amp;A accuracy and calibration error. This suggests that within the current fine-tuning paradigm, we can expect correctness prediction but not calibration to improve as LLM capabilities progress. To demonstrate the utility of correctness prediction, we show that when models have the option to abstain, performance can be improved by selectively abstaining based on the MSP of the initial model response, using only a small amount of labeled data to choose the MSP threshold.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness of data-driven approaches in limited angle tomography</title>
<link>https://arxiv.org/abs/2403.11350</link>
<guid>https://arxiv.org/abs/2403.11350</guid>
<content:encoded><![CDATA[
arXiv:2403.11350v3 Announce Type: replace-cross 
Abstract: The limited angle Radon transform is notoriously difficult to invert due to its ill-posedness. In this work, we give a mathematical explanation that data-driven approaches can stably reconstruct more information compared to traditional methods like filtered backprojection. In addition, we use experiments based on the U-Net neural network to validate our theory.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A dataset of primary nasopharyngeal carcinoma MRI with multi-modalities segmentation</title>
<link>https://arxiv.org/abs/2404.03253</link>
<guid>https://arxiv.org/abs/2404.03253</guid>
<content:encoded><![CDATA[
arXiv:2404.03253v2 Announce Type: replace-cross 
Abstract: Multi-modality magnetic resonance imaging(MRI) data facilitate the early diagnosis, tumor segmentation, and disease staging in the management of nasopharyngeal carcinoma (NPC). The lack of publicly available, comprehensive datasets limits advancements in diagnosis, treatment planning, and the development of machine learning algorithms for NPC. Addressing this critical need, we introduce the first comprehensive NPC MRI dataset, encompassing MR axial imaging of 277 primary NPC patients. This dataset includes T1-weighted, T2-weighted, and contrast-enhanced T1-weighted sequences, totaling 831 scans. In addition to the corresponding clinical data, manually annotated and labeled segmentations by experienced radiologists offer high-quality data resources from untreated primary NPC.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis</title>
<link>https://arxiv.org/abs/2405.00708</link>
<guid>https://arxiv.org/abs/2405.00708</guid>
<content:encoded><![CDATA[
arXiv:2405.00708v2 Announce Type: replace-cross 
Abstract: Understanding the behavior of large language models (LLMs) is crucial for ensuring their safe and reliable use. However, existing explainable AI (XAI) methods for LLMs primarily rely on word-level explanations, which are often computationally inefficient and misaligned with human reasoning processes. Moreover, these methods often treat explanation as a one-time output, overlooking its inherently interactive and iterative nature. In this paper, we present LLM Analyzer, an interactive visualization system that addresses these limitations by enabling intuitive and efficient exploration of LLM behaviors through counterfactual analysis. Our system features a novel algorithm that generates fluent and semantically meaningful counterfactuals via targeted removal and replacement operations at user-defined levels of granularity. These counterfactuals are used to compute feature attribution scores, which are then integrated with concrete examples in a table-based visualization, supporting dynamic analysis of model behavior. A user study with LLM practitioners and interviews with experts demonstrate the system's usability and effectiveness, emphasizing the importance of involving humans in the explanation process as active participants rather than passive recipients.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation</title>
<link>https://arxiv.org/abs/2408.01343</link>
<guid>https://arxiv.org/abs/2408.01343</guid>
<content:encoded><![CDATA[
arXiv:2408.01343v2 Announce Type: replace-cross 
Abstract: Multimodal semantic segmentation shows significant potential for enhancing segmentation accuracy in complex scenes. However, current methods often incorporate specialized feature fusion modules tailored to specific modalities, thereby restricting input flexibility and increasing the number of training parameters. To address these challenges, we propose StitchFusion, a straightforward yet effective modal fusion framework that integrates large-scale pre-trained models directly as encoders and feature fusers. This approach facilitates comprehensive multi-modal and multi-scale feature fusion, accommodating any visual modal inputs. Specifically, Our framework achieves modal integration during encoding by sharing multi-modal visual information. To enhance information exchange across modalities, we introduce a multi-directional adapter module (MultiAdapter) to enable cross-modal information transfer during encoding. By leveraging MultiAdapter to propagate multi-scale information across pre-trained encoders during the encoding process, StitchFusion achieves multi-modal visual information integration during encoding. Extensive comparative experiments demonstrate that our model achieves state-of-the-art performance on four multi-modal segmentation datasets with minimal additional parameters. Furthermore, the experimental integration of MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their complementary nature. Our code is available at StitchFusion_repo.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation</title>
<link>https://arxiv.org/abs/2409.02098</link>
<guid>https://arxiv.org/abs/2409.02098</guid>
<content:encoded><![CDATA[
arXiv:2409.02098v2 Announce Type: replace-cross 
Abstract: Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given these examples, CRAFT uses large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents. Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning. We demonstrate that CRAFT can efficiently generate large-scale task-specific training datasets for four diverse tasks: biology, medicine, and commonsense question-answering (QA), as well as summarization. Our experiments show that CRAFT-based models outperform or match general LLMs on QA tasks, while exceeding models trained on human-curated summarization data by 46 preference points. CRAFT outperforms other synthetic dataset generation methods such as Self- and Evol-Instruct, and remains robust even when the quality of the initial few-shots varies.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework</title>
<link>https://arxiv.org/abs/2409.04224</link>
<guid>https://arxiv.org/abs/2409.04224</guid>
<content:encoded><![CDATA[
arXiv:2409.04224v2 Announce Type: replace-cross 
Abstract: In healthcare, multi-organ system diseases pose unique and significant challenges as they impact multiple physiological systems concurrently, demanding complex and coordinated treatment strategies. Despite recent advancements in the AI based clinical decision support systems, these solutions only focus on individual organ systems, failing to account for complex interdependencies between them. This narrow focus greatly hinders their effectiveness in recommending holistic and clinically actionable treatments in the real world setting. To address this critical gap, we propose a novel Hierarchical Multi-Agent Reinforcement Learning (HMARL) framework. Our architecture deploys specialized and dedicated agents for each organ system and facilitates inter-agent communication to enable synergistic decision-making across organ systems. Furthermore, we introduce a dual-layer state representation technique that contextualizes patient conditions at both global and organ-specific levels, improving the accuracy and relevance of treatment decisions. We evaluate our HMARL solution on the task of sepsis management, a common and critical multi-organ disease, using both qualitative and quantitative metrics. Our method learns effective, clinically aligned treatment policies that considerably improve patient survival. We believe this framework represents a significant advancement in clinical decision support systems, introducing the first RL solution explicitly designed for multi-organ treatment recommendations. Our solution moves beyond prevailing simplified, single-organ models that fall short in addressing the complexity of multi-organ diseases.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WhisperNER: Unified Open Named Entity and Speech Recognition</title>
<link>https://arxiv.org/abs/2409.08107</link>
<guid>https://arxiv.org/abs/2409.08107</guid>
<content:encoded><![CDATA[
arXiv:2409.08107v2 Announce Type: replace-cross 
Abstract: Integrating named entity recognition (NER) with automatic speech recognition (ASR) can significantly enhance transcription accuracy and informativeness. In this paper, we introduce WhisperNER, a novel model that allows joint speech transcription and entity recognition. WhisperNER supports open-type NER, enabling recognition of diverse and evolving entities at inference. Building on recent advancements in open NER research, we augment a large synthetic dataset with synthetic speech samples. This allows us to train WhisperNER on a large number of examples with diverse NER tags. During training, the model is prompted with NER labels and optimized to output the transcribed utterance along with the corresponding tagged entities. To evaluate WhisperNER, we generate synthetic speech for commonly used NER benchmarks and annotate existing ASR datasets with open NER tags. Our experiments demonstrate that WhisperNER outperforms natural baselines on both out-of-domain open type NER and supervised finetuning.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search</title>
<link>https://arxiv.org/abs/2410.03864</link>
<guid>https://arxiv.org/abs/2410.03864</guid>
<content:encoded><![CDATA[
arXiv:2410.03864v2 Announce Type: replace-cross 
Abstract: Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called "reasoning actions"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason dynamically via optimal reasoning trajectory search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonasymptotic Analysis of Stochastic Gradient Descent with the Richardson-Romberg Extrapolation</title>
<link>https://arxiv.org/abs/2410.05106</link>
<guid>https://arxiv.org/abs/2410.05106</guid>
<content:encoded><![CDATA[
arXiv:2410.05106v3 Announce Type: replace-cross 
Abstract: We address the problem of solving strongly convex and smooth minimization problems using stochastic gradient descent (SGD) algorithm with a constant step size. Previous works suggested to combine the Polyak-Ruppert averaging procedure with the Richardson-Romberg extrapolation to reduce the asymptotic bias of SGD at the expense of a mild increase of the variance. We significantly extend previous results by providing an expansion of the mean-squared error of the resulting estimator with respect to the number of iterations $n$. We show that the root mean-squared error can be decomposed into the sum of two terms: a leading one of order $\mathcal{O}(n^{-1/2})$ with explicit dependence on a minimax-optimal asymptotic covariance matrix, and a second-order term of order $\mathcal{O}(n^{-3/4})$, where the power $3/4$ is best known. We also extend this result to the higher-order moment bounds. Our analysis relies on the properties of the SGD iterates viewed as a time-homogeneous Markov chain. In particular, we establish that this chain is geometrically ergodic with respect to a suitably defined weighted Wasserstein semimetric.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws For Mixed Quantization</title>
<link>https://arxiv.org/abs/2410.06722</link>
<guid>https://arxiv.org/abs/2410.06722</guid>
<content:encoded><![CDATA[
arXiv:2410.06722v3 Announce Type: replace-cross 
Abstract: Post-training quantization of Large Language Models (LLMs) has proven effective in reducing the memory and computational requirements for inference. In this study, we focus on a straightforward question: When aiming for a target accuracy or perplexity with low-precision quantization, how much high-precision computation needs to be preserved, and how fine-grained this quantization would need to be as we scale LLMs to larger sizes? We first introduce two critical metrics, named the quantization ratio ($Q_r$) and quantization block size ($Q_b$). The former measures the number of parameters quantized to low-precision arithmetic normalized by the total parameter count, whereas the latter defines the number of values within a block that share a scaling factor, akin to the block size concept introduced in the FP4 format in NVIDIA's Blackwell architecture. Through extensive and carefully controlled experiments across different models and quantization methods, we propose a unified scaling law on post-training quantization (PTQ) that can predict loss degeneration for varying $Q_r$ and $Q_b$. For $Q_r$, our scaling law implies that parameter scaling and ratio scaling have a multiplicative relationship. Consequently, larger models are more amenable to a higher quantization ratio $Q_r$, thus supporting an increase in the adoption of mixed quantization for inference. Regarding $Q_b$, our findings indicate that a small block size, similar to that used in Blackwell, is not essential for large models. Employing a small $Q_b$ can instead unnecessarily complicate the design of the hardware circuit.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Graph Topology Learning via Time-Vertex Adaptive Filters: From Theory to Cardiac Fibrillation</title>
<link>https://arxiv.org/abs/2411.01567</link>
<guid>https://arxiv.org/abs/2411.01567</guid>
<content:encoded><![CDATA[
arXiv:2411.01567v2 Announce Type: replace-cross 
Abstract: Graph Signal Processing (GSP) provides a powerful framework for analysing complex, interconnected systems by modelling data as signals on graphs. While recent advances have enabled graph topology learning from observed signals, existing methods often struggle with time-varying systems and real-time applications. To address this gap, we introduce AdaCGP, a sparsity-aware adaptive algorithm for dynamic graph topology estimation from multivariate time series. AdaCGP estimates the Graph Shift Operator (GSO) through recursive update formulae designed to address sparsity, shift-invariance, and bias. Through comprehensive simulations, we demonstrate that AdaCGP consistently outperforms multiple baselines across diverse graph topologies, achieving improvements exceeding 83% in GSO estimation compared to state-of-the-art methods while maintaining favourable computational scaling properties. Our variable splitting approach enables reliable identification of causal connections with near-zero false alarm rates and minimal missed edges. Applied to cardiac fibrillation recordings, AdaCGP tracks dynamic changes in propagation patterns more effectively than established methods like Granger causality, capturing temporal variations in graph topology that static approaches miss. The algorithm successfully identifies stability characteristics in conduction patterns that may maintain arrhythmias, demonstrating potential for clinical applications in diagnosis and treatment of complex biomedical systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>