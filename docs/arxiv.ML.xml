<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>Artificial intelligence for methane detection: from continuous monitoring to verified mitigation</title>
<link>https://arxiv.org/abs/2511.21777</link>
<guid>https://arxiv.org/abs/2511.21777</guid>
<content:encoded><![CDATA[
<div> Methane, greenhouse gas, satellite detection, machine learning, emissions mitigation<br><br>Summary:<br><br>Methane significantly contributes to global warming, accounting for approximately 30% of the increase since pre-industrial times. Large methane emissions are often concentrated at a few major point sources, presenting an opportunity for targeted emission reductions by addressing a limited number of sites. Detecting and attributing these emissions at scale remains complex, particularly for issuing timely notifications to asset owners. The study introduces MARS-S2L, a machine learning model designed to identify methane emissions using publicly available multispectral satellite imagery. This model was trained on a large, manually curated dataset of over 80,000 images, enabling high-resolution and frequent detection every two days. MARS-S2L achieved detection of 78% of methane plumes with an 8% false positive rate across 697 previously unseen sites. Operational deployment of MARS-S2L has resulted in issuance of 1,015 notifications across 20 countries, facilitating verified and permanent mitigation of six persistent methane emitters, including a newly discovered site in Libya. These findings highlight a scalable and effective approach that connects satellite-based detection directly to actionable methane reduction efforts at the facility level. <div>
arXiv:2511.21777v1 Announce Type: new 
Abstract: Methane is a potent greenhouse gas, responsible for roughly 30\% of warming since pre-industrial times. A small number of large point sources account for a disproportionate share of emissions, creating an opportunity for substantial reductions by targeting relatively few sites. Detection and attribution of large emissions at scale for notification to asset owners remains challenging. Here, we introduce MARS-S2L, a machine learning model that detects methane emissions in publicly available multispectral satellite imagery. Trained on a manually curated dataset of over 80,000 images, the model provides high-resolution detections every two days, enabling facility-level attribution and identifying 78\% of plumes with an 8\% false positive rate at 697 previously unseen sites. Deployed operationally, MARS-S2L has issued 1,015 notifications to stakeholders in 20 countries, enabling verified, permanent mitigation of six persistent emitters, including a previously unknown site in Libya. These results demonstrate a scalable pathway from satellite detection to quantifiable methane mitigation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Spiking Neural Networks via Conservative Flux Quantization</title>
<link>https://arxiv.org/abs/2511.21784</link>
<guid>https://arxiv.org/abs/2511.21784</guid>
<content:encoded><![CDATA[
<div> Keywords: Physics-Informed Neural Networks, Spiking Neural Networks, Conservative Leaky Integrate-and-Fire, Conservative Flux Quantization, Mass Conservation<br><br>Summary: This paper addresses the challenge of real-time, physically-consistent predictions on low-power edge devices, which is crucial for embodied AI systems. It highlights the limitations of existing Physics-Informed Neural Networks (PINNs), which are energy-intensive and often fail to strictly enforce physical conservation laws. To overcome these limitations, the authors introduce a novel Physics-Informed Spiking Neural Network (PISNN) framework. Central to this framework is the design of the Conservative Leaky Integrate-and-Fire (C-LIF) neuron, which intrinsically guarantees local mass preservation through its dynamics. Additionally, a Conservative Flux Quantization (CFQ) strategy is proposed to improve temporal generalization by interpreting neural spikes as discrete packets of physical flux. This approach enables the PISNN to learn a time-invariant physical evolution operator, making it a conservative-by-construction general-purpose solver. The effectiveness of the PISNN is validated through experiments on the 1D heat equation and 2D Laplace’s equation, where it demonstrates accurate system dynamics simulation alongside perfect mass conservation—a challenging goal for traditional PINNs. This work bridges scientific computing rigor with neuromorphic efficiency, setting the foundation for long-term, energy-efficient, and physically faithful predictions in intelligent edge AI systems. <div>
arXiv:2511.21784v1 Announce Type: new 
Abstract: Real-time, physically-consistent predictions on low-power edge devices is critical for the next generation embodied AI systems, yet it remains a major challenge. Physics-Informed Neural Networks (PINNs) combine data-driven learning with physics-based constraints to ensure the model's predictions are with underlying physical principles.However, PINNs are energy-intensive and struggle to strictly enforce physical conservation laws. Brain-inspired spiking neural networks (SNNs) have emerged as a promising solution for edge computing and real-time processing. However, naively converting PINNs to SNNs degrades physical fidelity and fails to address long-term generalization issues. To this end, this paper introduce a novel Physics-Informed Spiking Neural Network (PISNN) framework. Importantly, to ensure strict physical conservation, we design the Conservative Leaky Integrate-and-Fire (C-LIF) neuron, whose dynamics structurally guarantee local mass preservation. To achieve robust temporal generalization, we introduce a novel Conservative Flux Quantization (CFQ) strategy, which redefines neural spikes as discrete packets of physical flux. Our CFQ learns a time-invariant physical evolution operator, enabling the PISNN to become a general-purpose solver -- conservative-by-construction. Extensive experiments show that our PISNN excels on diverse benchmarks. For both the canonical 1D heat equation and the more challenging 2D Laplace's Equation, it accurately simulates the system dynamics while maintaining perfect mass conservation by design -- a feat that is challenging for conventional PINNs. This work establishes a robust framework for fusing the rigor of scientific computing with the efficiency of neuromorphic engineering, paving the way for complex, long-term, and energy-efficient physics predictions for intelligent systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2511.21787</link>
<guid>https://arxiv.org/abs/2511.21787</guid>
<content:encoded><![CDATA[
<div> Keywords: Implicit Neural Representations, spectral bias, dynamical system, expressivity, generalization<br><br>Summary: Implicit Neural Representations (INRs) excel at modeling continuous visual and geometric signals but face challenges from spectral bias, which limits their capacity to represent high-frequency details effectively. This paper introduces Dynamical Implicit Neural Representations (DINR), a novel framework that conceptualizes feature evolution as a continuous-time dynamical system rather than a conventional layer-based architecture. By doing so, DINR alleviates spectral bias, allowing for richer and more adaptive frequency representations through continuous feature changes. Theoretical insights grounded in Rademacher complexity and Neural Tangent Kernel analyses reveal that DINR improves model expressivity and enhances the dynamics of training. Importantly, the framework includes a regularization mechanism on the complexity of the underlying dynamics, offering a principled approach to balance the trade-off between expressivity and generalization. Extensive empirical evaluations illustrate that DINR achieves more stable convergence rates, yields higher fidelity signal reconstructions, and outperforms traditional static INRs in generalization tasks across domains such as image representation, field reconstruction, and data compression. This comprehensive approach addresses the fundamental limitations of current INRs and establishes DINR as a promising direction for advancing continuous neural representation learning. <div>
arXiv:2511.21787v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs) provide a powerful continuous framework for modeling complex visual and geometric signals, but spectral bias remains a fundamental challenge, limiting their ability to capture high-frequency details. Orthogonal to existing remedy strategies, we introduce Dynamical Implicit Neural Representations (DINR), a new INR modeling framework that treats feature evolution as a continuous-time dynamical system rather than a discrete stack of layers. This dynamical formulation mitigates spectral bias by enabling richer, more adaptive frequency representations through continuous feature evolution. Theoretical analysis based on Rademacher complexity and the Neural Tangent Kernel demonstrates that DINR enhances expressivity and improves training dynamics. Moreover, regularizing the complexity of the underlying dynamics provides a principled way to balance expressivity and generalization. Extensive experiments on image representation, field reconstruction, and data compression confirm that DINR delivers more stable convergence, higher signal fidelity, and stronger generalization than conventional static INRs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiclass threshold-based classification and model evaluation</title>
<link>https://arxiv.org/abs/2511.21794</link>
<guid>https://arxiv.org/abs/2511.21794</guid>
<content:encoded><![CDATA[
<div> Keywords: multiclass classification, threshold tuning, multidimensional simplex, ROC analysis, Distance From Point (DFP) score  

<br><br>Summary:  
This paper proposes a novel threshold-based framework for multiclass classification which extends beyond the traditional argmax decision rule. Instead of relying solely on the probabilistic interpretation of softmax outputs, the method uses a geometric interpretation on the multidimensional simplex, introducing a multidimensional threshold for classification decisions. This novel perspective allows for an \textit{a posteriori} optimization of classification scores through threshold tuning, a technique commonly used in binary classification but less explored in the multiclass context. The approach enables refinement of prediction performance of any pre-trained classification network without retraining. Experimental results demonstrate that tuning multidimensional thresholds consistently improves performance across various neural network architectures and datasets. Furthermore, the paper introduces a new multiclass ROC analysis framework based on what the authors call "ROC clouds," representing attainable (False Positive Rate, True Positive Rate) operating points generated by a single multiclass threshold. To summarize these ROC clouds, a metric called Distance From Point (DFP) to the ideal operating point (0,1) is defined, offering a coherent alternative to standard One-vs-Rest ROC curves. The DFP score also correlates with the observed performance gains from threshold tuning, providing insights into classifier behavior and tuning benefits. <div>
arXiv:2511.21794v1 Announce Type: new 
Abstract: In this paper, we introduce a threshold-based framework for multiclass classification that generalizes the standard argmax rule. This is done by replacing the probabilistic interpretation of softmax outputs with a geometric one on the multidimensional simplex, where the classification depends on a multidimensional threshold. This change of perspective enables for any trained classification network an \textit{a posteriori} optimization of the classification score by means of threshold tuning, as usually carried out in the binary setting, thus allowing for a further refinement of the prediction capability of any network. Our experiments show indeed that multidimensional threshold tuning yields performance improvements across various networks and datasets. Moreover, we derive a multiclass ROC analysis based on \emph{ROC clouds} -- the attainable (FPR,TPR) operating points induced by a single multiclass threshold -- and summarize them via a \emph{Distance From Point} (DFP) score to $(0,1)$. This yields a coherent alternative to standard One-vs-Rest (OvR) curves and aligns with the observed tuning gains.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Double-Edged Nature of the Rashomon Set for Trustworthy Machine Learning</title>
<link>https://arxiv.org/abs/2511.21799</link>
<guid>https://arxiv.org/abs/2511.21799</guid>
<content:encoded><![CDATA[
<div> Rashomon set, robustness, privacy, interpretable models, information leakage  

<br><br>Summary:  
Real-world machine learning (ML) pipelines typically yield not just a single optimal model but a Rashomon set—a collection of many near-optimal models. This multiplicity significantly affects trustworthiness in ML. First, at the individual model level, sparse and interpretable models often help preserve privacy but are vulnerable to adversarial attacks due to their fragility. Second, the diversity inherent in a large Rashomon set fosters reactive robustness, meaning that even if one model falls victim to an attack, others in the set usually maintain accuracy and reliability. Third, Rashomon sets show stability when faced with small shifts in data distribution, enhancing their practical utility. However, this diversity introduces a privacy risk: revealing multiple near-optimal models can increase information leakage, as attackers gain richer insights into the training data by analyzing these models collectively. Through a blend of theoretical analysis and empirical experiments focusing on sparse decision trees and linear models, the study characterizes this critical trade-off between robustness and privacy within Rashomon sets. Ultimately, Rashomon sets embody a dual nature, serving as both a valuable resource for building trustworthy ML systems and a potential risk factor concerning privacy and information security. <div>
arXiv:2511.21799v1 Announce Type: new 
Abstract: Real-world machine learning (ML) pipelines rarely produce a single model; instead, they produce a Rashomon set of many near-optimal ones. We show that this multiplicity reshapes key aspects of trustworthiness. At the individual-model level, sparse interpretable models tend to preserve privacy but are fragile to adversarial attacks. In contrast, the diversity within a large Rashomon set enables reactive robustness: even when an attack breaks one model, others often remain accurate. Rashomon sets are also stable under small distribution shifts. However, this same diversity increases information leakage, as disclosing more near-optimal models provides an attacker with progressively richer views of the training data. Through theoretical analysis and empirical studies of sparse decision trees and linear models, we characterize this robustness-privacy trade-off and highlight the dual role of Rashomon sets as both a resource and a risk for trustworthy ML.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Anomaly Detection for Smart IoT Devices: Performance and Resource Comparison</title>
<link>https://arxiv.org/abs/2511.21842</link>
<guid>https://arxiv.org/abs/2511.21842</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, Anomaly Detection, Isolation Forest, One-Class SVM, Cybersecurity

<br><br>Summary:  
This study addresses the increasing cybersecurity vulnerabilities in IoT deployments due to their rapid proliferation. Traditional signature-based anomaly detection systems struggle to identify emerging and zero-day threats, highlighting the need for more effective unsupervised techniques. The research evaluates two such methods—Isolation Forest (IF) and One-Class Support Vector Machine (OC-SVM)—using the TON_IoT thermostat dataset. Performance was assessed through key metrics including accuracy, precision, recall, and F1-score, alongside resource utilization metrics such as inference time, model size, and peak RAM usage. Experimental results show that Isolation Forest consistently outperforms OC-SVM by delivering higher accuracy, superior precision and recall, and a significantly better F1-score. Additionally, IF demonstrated a more efficient computational footprint, which is critical for deployment on resource-constrained edge devices common in IoT settings. The findings emphasize the robustness of Isolation Forest in handling high-dimensional and imbalanced IoT data, making it highly suitable for real-time anomaly detection. This research underscores the practical applicability of Isolation Forest for enhancing IoT cybersecurity by effectively detecting threats while maintaining low resource consumption. <div>
arXiv:2511.21842v1 Announce Type: new 
Abstract: The rapid expansion of Internet of Things (IoT) deployments across diverse sectors has significantly enhanced operational efficiency, yet concurrently elevated cybersecurity vulnerabilities due to increased exposure to cyber threats. Given the limitations of traditional signature-based Anomaly Detection Systems (ADS) in identifying emerging and zero-day threats, this study investigates the effectiveness of two unsupervised anomaly detection techniques, Isolation Forest (IF) and One-Class Support Vector Machine (OC-SVM), using the TON_IoT thermostat dataset. A comprehensive evaluation was performed based on standard metrics (accuracy, precision, recall, and F1-score) alongside critical resource utilization metrics such as inference time, model size, and peak RAM usage. Experimental results revealed that IF consistently outperformed OC-SVM, achieving higher detection accuracy, superior precision, and recall, along with a significantly better F1-score. Furthermore, Isolation Forest demonstrated a markedly superior computational footprint, making it more suitable for deployment on resource-constrained IoT edge devices. These findings underscore Isolation Forest's robustness in high-dimensional and imbalanced IoT environments and highlight its practical viability for real-time anomaly detection.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics</title>
<link>https://arxiv.org/abs/2511.21848</link>
<guid>https://arxiv.org/abs/2511.21848</guid>
<content:encoded><![CDATA[
<div> Keywords: sensorimotor transformations, musculoskeletal model, imitation learning, energy constraints, biomechanics<br><br>Summary:  
This article presents the development of a general-purpose simulation platform aimed at understanding embodied control by modeling sensorimotor transformations that link brain activity to body movement. The authors describe a pipeline that converts kinematic data from neuroscience experiments into biomechanical models that can replicate natural mouse forelimb movements. To achieve this, they implement an imitation learning framework that trains a musculoskeletal model to perform dexterous reaching tasks within a physics-based simulation environment. The training process is accelerated using GPU capabilities via JAX and Mujoco-MJX, achieving over one million training steps per second. A key finding is that incorporating biologically inspired constraints, such as energy expenditure and velocity limits, into the simulations produces musculoskeletal activity patterns that better correspond to actual electromyography (EMG) recordings. This suggests that energy efficiency and control limitations are important factors in accurately modeling motor control. Overall, the work bridges neuroscience data, biomechanics, and computational modeling to enhance our understanding of how the brain controls movement through complex neural and musculoskeletal interactions. <div>
arXiv:2511.21848v1 Announce Type: new 
Abstract: The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high fidelity behavioral dynamics, biomechanics, and neural circuit architectures underlying embodied control. We present a pipeline for taking kinematics data from the neuroscience lab and creating a pipeline for recapitulating those natural movements in a biomechanical model. We implement a imitation learning framework to perform a dexterous forelimb reaching task with a musculoskeletal model in a simulated physics environment. The mouse arm model is currently training at faster than 1 million training steps per second due to GPU acceleration with JAX and Mujoco-MJX. We present results that indicate that adding naturalistic constraints on energy and velocity lead to simulated musculoskeletal activity that better predict real EMG signals. This work provides evidence to suggest that energy and control constraints are critical to modeling musculoskeletal motor control.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight ML-Based Air Quality Prediction for IoT and Embedded Applications</title>
<link>https://arxiv.org/abs/2511.21857</link>
<guid>https://arxiv.org/abs/2511.21857</guid>
<content:encoded><![CDATA[
<div> XGBoost, air quality, regression, model efficiency, IoT

<br><br>Summary:  
This study evaluates two variants of the XGBoost regression model—a full-capacity version and a lightweight (tiny) version—for predicting urban air pollutant concentrations, specifically carbon monoxide (CO) and nitrogen dioxide (NO2). The AirQualityUCI dataset, representing one year of data in an urban environment, was used for the analysis. Model performance was assessed using standard predictive accuracy metrics including Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Mean Bias Error (MBE), and the coefficient of determination (R2). Alongside accuracy, resource-oriented metrics such as inference time, model size, and peak RAM usage were also examined. Results showed the full XGBoost model provided superior accuracy in predicting pollutant levels, while the tiny model, despite slightly diminished precision, significantly reduced computational resource demands. This reduction manifested as much faster inference times and smaller model storage requirements. The findings highlight that simplified XGBoost models maintain acceptable predictive quality while enabling deployment in resource-constrained environments. Therefore, the tiny XGBoost variant is particularly well-suited for real-time air quality monitoring applications in Internet of Things (IoT) and embedded system contexts, where computational resources and storage are limited. <div>
arXiv:2511.21857v1 Announce Type: new 
Abstract: This study investigates the effectiveness and efficiency of two variants of the XGBoost regression model, the full-capacity and lightweight (tiny) versions, for predicting the concentrations of carbon monoxide (CO) and nitrogen dioxide (NO2). Using the AirQualityUCI dataset collected over one year in an urban environment, we conducted a comprehensive evaluation based on widely accepted metrics, including Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Mean Bias Error (MBE), and the coefficient of determination (R2). In addition, we assessed resource-oriented metrics such as inference time, model size, and peak RAM usage. The full XGBoost model achieved superior predictive accuracy for both pollutants, while the tiny model, though slightly less precise, offered substantial computational benefits with significantly reduced inference time and model storage requirements. These results demonstrate the feasibility of deploying simplified models in resource-constrained environments without compromising predictive quality. This makes the tiny XGBoost model suitable for real-time air-quality monitoring in IoT and embedded applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Foundation Model for Partial Differential Equations Across Physics Domains</title>
<link>https://arxiv.org/abs/2511.21861</link>
<guid>https://arxiv.org/abs/2511.21861</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.21861v1  
Keywords: PDE-FM, physics-informed machine learning, neural operators, multi-physics simulation, foundation model  

<br><br>Summary:  
The paper presents PDE-FM, a modular foundation model designed for physics-informed machine learning that integrates spatial, spectral, and temporal reasoning across diverse partial differential equation (PDE) systems. PDE-FM features a combination of spatial-spectral tokenization, physics-aware conditioning, and a Mamba-based state-space backbone coupled with an operator-theoretic decoder, which together enable scalable and data-efficient modeling of complex physical dynamics. Unlike task-specific neural operators, PDE-FM undergoes a single pretraining phase on a variety of PDE datasets, facilitating seamless transfer to new physical regimes without requiring architectural or data-specific adjustments. The model was evaluated on twelve 2D and 3D datasets from The Well benchmark, covering hydrodynamic, radiative, elastic, and astrophysical phenomena, and achieved state-of-the-art accuracy in six domains. It demonstrated a significant reduction in mean VRMSE by 46% compared to previous operator-learning baselines. Additionally, PDE-FM showed robust cross-physics generalization capabilities, excelling particularly in turbulent and radiative systems while maintaining strong accuracy in linear and steady-state scenarios. The study highlights that large-scale pretraining on diverse physical processes can generate transferable dynamic representations, advancing toward unified foundation-level surrogates for multi-physics simulation and accelerating scientific discovery. <div>
arXiv:2511.21861v1 Announce Type: new 
Abstract: We present PDE-FM, a modular foundation model for physics-informed machine learning that unifies spatial, spectral, and temporal reasoning across heterogeneous partial differential equation (PDE) systems. PDE-FM combines spatial-spectral tokenization, physics-aware conditioning, and a Mamba-based state-space backbone with an operator-theoretic decoder, enabling scalable and data-efficient modeling of complex physical dynamics. In contrast to task-specific neural operators, PDE-FM is pretrained once on diverse PDE datasets and can be transferred to new physical regimes without architectural or data-specific modifications. Evaluated on twelve 2D and 3D datasets from The Well benchmark - spanning hydrodynamic, radiative, elastic, and astrophysical phenomena - PDE-FM achieves state-of-the-art accuracy in six domains, reducing mean VRMSE by 46% relative to prior operator-learning baselines. The model demonstrates robust cross-physics generalization, excelling in turbulent and radiative systems while maintaining strong performance in linear and steady-state regimes. These results suggest that large-scale pretraining across diverse physical processes can yield transferable representations of dynamics, marking a step toward unified, foundation-level surrogates for multi-physics simulation and scientific discovery.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closed-Loop Transformers: Autoregressive Modeling as Iterative Latent Equilibrium</title>
<link>https://arxiv.org/abs/2511.21882</link>
<guid>https://arxiv.org/abs/2511.21882</guid>
<content:encoded><![CDATA[
<div> Keywords: autoregressive transformers, closed-loop prediction, Equilibrium Transformers, energy-based model, latent refinement  

<br><br>Summary:  
This paper identifies a fundamental limitation in contemporary autoregressive transformers, which operate in an open-loop manner—computing hidden states in a single forward pass without revision—leading to error propagation in tasks requiring long-range reasoning, factual consistency, and multi-step planning. To overcome this, the authors propose the closed-loop prediction principle, where the model iteratively refines latent states until achieving self-consistent equilibrium before generating each token. They instantiate this idea through Equilibrium Transformers (EqT), which integrate an Equilibrium Refinement Module that minimizes a learned energy function via latent space gradient descent. This energy function enforces bidirectional consistency, episodic memory coherence, and output confidence without external supervision. Theoretically, EqT is shown to perform approximate MAP inference in an energy-based latent model with linear convergence guarantees. The refinement process particularly improves performance on difficult inputs where one-shot inference fails. Their framework unifies several approaches, including deep equilibrium models, diffusion language models, and test-time training. Preliminary experiments on the binary parity task demonstrate average improvements of 3.28%, with gains up to 8.07% on sequences where standard transformers perform near chance, confirming that deliberation yields more benefits as task difficulty rises. The approach is proposed as a foundational advance toward resolving the commitment bottleneck in autoregressive language models. <div>
arXiv:2511.21882v1 Announce Type: new 
Abstract: Contemporary autoregressive transformers operate in open loop: each hidden state is computed in a single forward pass and never revised, causing errors to propagate uncorrected through the sequence. We identify this open-loop bottleneck as a fundamental architectural limitation underlying well-documented failures in long-range reasoning, factual consistency, and multi-step planning. To address this limitation, we introduce the closed-loop prediction principle, which requires that models iteratively refine latent representations until reaching a self-consistent equilibrium before committing to each token. We instantiate this principle as Equilibrium Transformers (EqT), which augment standard transformer layers with an Equilibrium Refinement Module that minimizes a learned energy function via gradient descent in latent space. The energy function enforces bidirectional prediction consistency, episodic memory coherence, and output confidence, all computed without external supervision. Theoretically, we prove that EqT performs approximate MAP inference in a latent energy-based model, establish linear convergence guarantees, and show that refinement improves predictions precisely on hard instances where one-shot inference is suboptimal. The framework unifies deep equilibrium models, diffusion language models, and test-time training as special cases. Preliminary experiments on the binary parity task demonstrate +3.28% average improvement on challenging sequences, with gains reaching +8.07% where standard transformers approach random performance, validating that the benefit of deliberation scales with task difficulty. Just as attention mechanisms resolved the sequential bottleneck of recurrent networks, we propose that closed-loop equilibrium may resolve the commitment bottleneck of open-loop autoregression, representing a foundational step toward language models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically Interpretable Representation Learning with Gaussian Mixture Variational AutoEncoder (GM-VAE)</title>
<link>https://arxiv.org/abs/2511.21883</link>
<guid>https://arxiv.org/abs/2511.21883</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian Mixture Variational Autoencoder, Expectation-Maximization, spectral interpretability metric, latent clustering, physical systems<br><br>Summary:<br><br>The article proposes a novel Gaussian Mixture Variational Autoencoder (GM-VAE) framework aimed at extracting compact and physically interpretable representations from high-dimensional scientific data with complex nonlinear structures. Unlike traditional VAEs that optimize reconstruction and clustering jointly, often causing training instability, their method employs an Expectation-Maximization (EM)-inspired block-coordinate descent strategy that alternates between expectation and maximization steps. This approach stabilizes training and naturally aligns latent clusters with distinct physical regimes, improving interpretability. To objectively assess the quality of the learned latent representations, the authors introduce a new spectral interpretability metric based on graph-Laplacian smoothness, which quantifies the coherence of physical quantities over the latent manifold. The framework is validated on progressively complex datasets including surface reaction ODEs, Navier-Stokes wake flows, and experimental laser-induced combustion Schlieren images. Results demonstrate that the GM-VAE produces smooth and physically consistent latent manifolds alongside accurate regime clustering. Overall, the method offers a robust, data-driven tool for analyzing and interpreting turbulent and reactive flow systems, addressing long-standing challenges in linking high-dimensional data to underlying physical behaviors through stable and interpretable latent representations. <div>
arXiv:2511.21883v1 Announce Type: new 
Abstract: Extracting compact, physically interpretable representations from high-dimensional scientific data is a persistent challenge due to the complex, nonlinear structures inherent in physical systems. We propose a Gaussian Mixture Variational Autoencoder (GM-VAE) framework designed to address this by integrating an Expectation-Maximization (EM)-inspired training scheme with a novel spectral interpretability metric. Unlike conventional VAEs that jointly optimize reconstruction and clustering (often leading to training instability), our method utilizes a block-coordinate descent strategy, alternating between expectation and maximization steps. This approach stabilizes training and naturally aligns latent clusters with distinct physical regimes. To objectively evaluate the learned representations, we introduce a quantitative metric based on graph-Laplacian smoothness, which measures the coherence of physical quantities across the latent manifold. We demonstrate the efficacy of this framework on datasets of increasing complexity: surface reaction ODEs, Navier-Stokes wake flows, and experimental laser-induced combustion Schlieren images. The results show that our GM-VAE yields smooth, physically consistent manifolds and accurate regime clustering, offering a robust data-driven tool for interpreting turbulent and reactive flow systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Fusion Strategies for Multimodal Vision-Language Systems</title>
<link>https://arxiv.org/abs/2511.21889</link>
<guid>https://arxiv.org/abs/2511.21889</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal machine learning, data fusion, BERT, vision networks, latency-accuracy tradeoff  

<br><br>Summary:  
This paper investigates the impact of different fusion strategies in multimodal machine learning, focusing on combining image and text data using a hybrid BERT and vision network framework. Two vision networks, MobileNetV2 and ViT, serve as the image processing modules. For each vision network, the authors propose three fusion models that integrate data at early, intermediate, and late stages within the architecture. The models are evaluated on the CMU MOSI dataset to assess their accuracy and benchmarked on an NVIDIA Jetson Orin AGX to measure inference latency. Findings indicate a clear tradeoff: late fusion achieves the highest accuracy by merging processed features at the end of the pipeline, while early fusion delivers the fastest inference times by consolidating data closer to input layers. Intermediate fusion models offer a balance between these extremes. The study highlights that the choice of fusion strategy should be aligned with the application's specific needs for accuracy versus latency. This work provides practical insights for designing multimodal models, emphasizing that integrating data earlier accelerates inference but typically reduces accuracy, whereas late fusion improves accuracy but at the cost of increased latency. <div>
arXiv:2511.21889v1 Announce Type: new 
Abstract: Modern machine learning models often combine multiple input streams of data to more accurately capture the information that informs their decisions. In multimodal machine learning, choosing the strategy for fusing data together requires careful consideration of the application's accuracy and latency requirements, as fusing the data at earlier or later stages in the model architecture can lead to performance changes in accuracy and latency. To demonstrate this tradeoff, we investigate different fusion strategies using a hybrid BERT and vision network framework that integrates image and text data. We explore two different vision networks: MobileNetV2 and ViT. We propose three models for each vision network, which fuse data at late, intermediate, and early stages in the architecture. We evaluate the proposed models on the CMU MOSI dataset and benchmark their latency on an NVIDIA Jetson Orin AGX. Our experimental results demonstrate that while late fusion yields the highest accuracy, early fusion offers the lowest inference latency. We describe the three proposed model architectures and discuss the accuracy and latency tradeoffs, concluding that data fusion earlier in the model architecture results in faster inference times at the cost of accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Illusion: Consensus-Based Generative Mitigation of Adversarial Illusions in Multi-Modal Embeddings</title>
<link>https://arxiv.org/abs/2511.21893</link>
<guid>https://arxiv.org/abs/2511.21893</guid>
<content:encoded><![CDATA[
<div> multi-modal foundation models, adversarial illusions, generative models, Variational Autoencoders, cross-modal alignment<br><br>Summary:<br><br>Multi-modal foundation models integrate various data types like images and text into a shared embedding space but are susceptible to adversarial illusions, which involve subtle perturbations that disrupt this alignment and impair downstream tasks. To address this, the paper proposes a task-agnostic defense mechanism that reconstructs the original input from adversarially perturbed inputs using generative models such as Variational Autoencoders (VAEs). This reconstruction aims to restore natural cross-modal alignment. Additionally, the method incorporates a generative sampling strategy combined with a consensus-based aggregation to enhance robustness by aggregating multiple reconstructions. Experiments conducted on state-of-the-art multi-modal encoders demonstrate the effectiveness of this approach, significantly reducing illusion attack success rates to nearly zero. Furthermore, the defense mechanism improves cross-modal alignment metrics by 4% (from 42 to 46) in unperturbed settings and by 11% (from 32 to 43) under perturbed conditions. The proposed method offers an effective, model-agnostic strategy to mitigate adversarial illusions, enhancing the reliability and security of multi-modal models in practical applications. <div>
arXiv:2511.21893v1 Announce Type: new 
Abstract: Multi-modal foundation models align images, text, and other modalities in a shared embedding space but remain vulnerable to adversarial illusions (Zhang et al., 2025), where imperceptible perturbations disrupt cross-modal alignment and mislead downstream tasks. To counteract the effects of adversarial illusions, we propose a task-agnostic mitigation mechanism that reconstructs the input from the attacker's perturbed input through generative models, e.g., Variational Autoencoders (VAEs), to maintain natural alignment. To further enhance our proposed defense mechanism, we adopt a generative sampling strategy combined with a consensus-based aggregation scheme over the outcomes of the generated samples. Our experiments on the state-of-the-art multi-modal encoders show that our approach substantially reduces the illusion attack success rates to near-zero and improves cross-modal alignment by 4% (42 to 46) and 11% (32 to 43) in unperturbed and perturbed input settings respectively, providing an effective and model-agnostic defense against adversarial illusions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Atoms: Evaluating Electron Density Representation for 3D Molecular Learning</title>
<link>https://arxiv.org/abs/2511.21900</link>
<guid>https://arxiv.org/abs/2511.21900</guid>
<content:encoded><![CDATA[
<div> electron density, voxel-based CNN, molecular property prediction, protein-ligand binding, quantum property modeling<br><br>Summary:<br><br>This study examines the use of electron density maps as voxel-based inputs for 3D convolutional neural networks (CNNs) in molecular property prediction tasks, contrasting these with traditional atom-based representations. Three voxel input types are evaluated: atom types, raw electron density, and density gradient magnitude, across two tasks—protein-ligand binding affinity prediction (PDBbind) and quantum property prediction (QM9). The volumetric nature of electron density maps aligns naturally with voxel grids, supporting both experimental and computational density data. On the protein-ligand affinity task, all input types show similar performance with large datasets; however, in low-data scenarios, density-based inputs outperform atom types, while a shape-based baseline performs on par, indicating the importance of spatial occupancy information in this context. For the quantum property task, where labels come from Density Functional Theory (DFT) but input densities are derived from a lower-level method (XTB), density-based inputs demonstrate superior accuracy at scale compared to atom-based inputs. This suggests electron density inputs carry richer structural and electronic information beneficial for quantum property modeling. Overall, the results underscore that the advantage of density-derived inputs depends on the specific task and data regime, enhancing data efficiency in binding affinity prediction and improving accuracy in quantum property prediction. <div>
arXiv:2511.21900v1 Announce Type: new 
Abstract: Machine learning models for 3D molecular property prediction typically rely on atom-based representations, which may overlook subtle physical information. Electron density maps -- the direct output of X-ray crystallography and cryo-electron microscopy -- offer a continuous, physically grounded alternative. We compare three voxel-based input types for 3D convolutional neural networks (CNNs): atom types, raw electron density, and density gradient magnitude, across two molecular tasks -- protein-ligand binding affinity prediction (PDBbind) and quantum property prediction (QM9). We focus on voxel-based CNNs because electron density is inherently volumetric, and voxel grids provide the most natural representation for both experimental and computed densities. On PDBbind, all representations perform similarly with full data, but in low-data regimes, density-based inputs outperform atom types, while a shape-based baseline performs comparably -- suggesting that spatial occupancy dominates this task. On QM9, where labels are derived from Density Functional Theory (DFT) but input densities from a lower-level method (XTB), density-based inputs still outperform atom-based ones at scale, reflecting the rich structural and electronic information encoded in density. Overall, these results highlight the task- and regime-dependent strengths of density-derived inputs, improving data efficiency in affinity prediction and accuracy in quantum property modeling.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Machine Learning for Early Trust Prediction in Human-AI Interaction Using Face Image and GSR Bio Signals</title>
<link>https://arxiv.org/abs/2511.21908</link>
<guid>https://arxiv.org/abs/2511.21908</guid>
<content:encoded><![CDATA[
<div> Keywords: human trust, AI prediction, galvanic skin response, facial emotion analysis, ADHD mHealth<br><br>Summary:<br><br>This study addresses the prediction of human trust in AI systems within healthcare decision support, focusing on a simulated ADHD mobile health (mHealth) scenario. It introduces a multi-modal machine learning framework that integrates facial video data and galvanic skin response (GSR) signals to predict user trust in AI- versus human-generated recommendations. Facial videos were processed using OpenCV with a pre-trained transformer model to extract emotional features, while GSR data were decomposed into tonic and phasic components to assess physiological arousal. Trust prediction was performed separately for two temporal windows: the Early Detection Window (6 to 3 seconds before decision making) and the Proximal Detection Window (3 to 0 seconds before decision making). Each data modality—image-based, GSR-based, and their combination—was analyzed with machine learning models. The best-performing unimodal models were combined in a multimodal stacking ensemble to improve accuracy. Results demonstrated that the multimodal approach significantly enhanced trust prediction, achieving an accuracy of 0.83, F1-score of 0.88, and ROC-AUC of 0.87 in the Early Detection Window, and slightly lower metrics in the Proximal Detection Window. The findings highlight bio signals as effective real-time indicators of user trust, which can enable adaptive AI systems to adjust dynamically, maintaining calibrated trust critical for accurate diagnosis and treatment in mental health contexts. <div>
arXiv:2511.21908v1 Announce Type: new 
Abstract: Predicting human trust in AI systems is crucial for safe integration of AI-based decision support tools, especially in healthcare. This study proposes a multi-modal machine learning framework that combines image and galvanic skin response (GSR) data to predict early user trust in AI- or human-generated recommendations in a simulated ADHD mHealth context. Facial video data were processed using OpenCV for frame extraction and transferred learning with a pre-trained transformer model to derive emotional features. Concurrently, GSR signals were decomposed into tonic and phasic components to capture physiological arousal patterns. Two temporal windows were defined for trust prediction: the Early Detection Window (6 to 3 seconds before decision-making) and the Proximal Detection Window (3 to 0 seconds before decision-making). For each window, trust prediction was conducted separately using image-based, GSR-based, and multimodal (image + GSR) features. Each modality was analyzed using machine learning algorithms, and the top-performing unimodal models were integrated through a multimodal stacking ensemble for final prediction. Experimental results showed that combining facial and physiological cues significantly improved prediction performance. The multimodal stacking framework achieved an accuracy of 0.83, F1-score of 0.88, and ROC-AUC of 0.87 in the Early Detection Window, and an accuracy of 0.75, F1-score of 0.82, and ROC-AUC of 0.66 in the Proximal Detection Window. These results demonstrate the potential of bio signals as real-time, objective markers of user trust, enabling adaptive AI systems that dynamically adjust their responses to maintain calibrated trust which is a critical capability in mental health applications where mis-calibrated trust can affect diagnostic and treatment outcomes.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Dynamic Properties of Backdoor Training Through Information Bottleneck</title>
<link>https://arxiv.org/abs/2511.21923</link>
<guid>https://arxiv.org/abs/2511.21923</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor attacks, neural networks, Information Bottleneck, mutual information, stealthiness<br><br>Summary:  
This paper provides a rigorous analysis of how backdoor data affects the training dynamics of neural networks, focusing on the differences between the target class and other clean classes. It leverages the Information Bottleneck (IB) principle linked with clustering in internal representations to study these effects. The authors discover that backdoor attacks leave unique mutual information (MI) signatures that evolve differently throughout training phases and vary according to the type of attack. A surprising trade-off is revealed: visually conspicuous attacks such as BadNets can be more stealthy in terms of information-theoretic integration within the model compared to many visually imperceptible attacks. Based on these insights, the paper introduces a novel dynamics-based stealthiness metric that quantifies how well an attack is integrated at the model level. The proposed metric and findings are validated across multiple datasets and various attack types, providing a new perspective for understanding and evaluating backdoor threats in neural networks. The authors have also made their code publicly available to support further research in this area. <div>
arXiv:2511.21923v1 Announce Type: new 
Abstract: Understanding how backdoor data influences neural network training dynamics remains a complex and underexplored challenge. In this paper, we present a rigorous analysis of the impact of backdoor data on the learning process, with a particular focus on the distinct behaviors between the target class and other clean classes. Leveraging the Information Bottleneck (IB) principle connected with clustering of internal representation, We find that backdoor attacks create unique mutual information (MI) signatures, which evolve across training phases and differ based on the attack mechanism. Our analysis uncovers a surprising trade-off: visually conspicuous attacks like BadNets can achieve high stealthiness from an information-theoretic perspective, integrating more seamlessly into the model than many visually imperceptible attacks. Building on these insights, we propose a novel, dynamics-based stealthiness metric that quantifies an attack's integration at the model level. We validate our findings and the proposed metric across multiple datasets and diverse attack types, offering a new dimension for understanding and evaluating backdoor threats. Our code is available in: https://github.com/XinyuLiu71/Information_Bottleneck_Backdoor.git.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2511.21928</link>
<guid>https://arxiv.org/abs/2511.21928</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Prompted Policy Search, Numerical Optimization, Sample Efficiency  

<br><br>Summary:  
This paper introduces Prompted Policy Search (ProPS), a novel reinforcement learning (RL) approach that integrates numerical reward signals with natural language input using large language models (LLMs). Unlike traditional RL methods which depend solely on scalar rewards, ProPS places an LLM at the core of the policy optimization loop, allowing the model to propose policy updates based on both numerical feedback and semantic information such as goals, domain knowledge, and strategic hints. ProPS leverages the LLM’s in-context numerical optimization ability to improve exploration and enhance sample efficiency. The method was evaluated across fifteen Gymnasium tasks, including classic control, Atari games, and MuJoCo simulated environments. It was benchmarked against seven popular RL algorithms, including PPO, SAC, and TRPO. ProPS outperformed these baselines in over half of the tasks (eight out of fifteen) and showed significant performance improvements when enhanced with domain knowledge. The findings suggest that unifying semantic understanding with numeric RL signals can lead to more transparent, generalizable, and human-aligned reinforcement learning systems. This approach highlights the promise of combining linguistic reasoning and reward-driven learning to advance RL capabilities beyond traditional frameworks. <div>
arXiv:2511.21928v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augment existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop-directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, domain knowledge, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across fifteen Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on eight out of fifteen tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned RL.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment</title>
<link>https://arxiv.org/abs/2511.21931</link>
<guid>https://arxiv.org/abs/2511.21931</guid>
<content:encoded><![CDATA[
<div> Keywords: model-data alignment, feature importance, binary classification, interpretability, potential outcomes framework<br><br>Summary:<br><br>1. The paper introduces a novel and computationally efficient framework designed to evaluate how well machine learning models align with the underlying structure of the data they are trained on. <br><br>2. Unlike traditional interpretability techniques that primarily explain model behavior, this method establishes a baseline directly from the data, emphasizing the concept of whether "the model says what the data says." <br><br>3. The approach is inspired by Rubin's Potential Outcomes Framework and focuses on quantifying the strength with which each feature differentiates between two outcome groups in binary classification tasks. <br><br>4. Moving beyond simple descriptive statistics, the framework estimates the actual effect of each feature on the outcome, providing a more meaningful measure of feature importance grounded in the data itself. <br><br>5. By comparing the rankings of features derived from the data with the explanations produced by machine learning models, the method delivers an interpretable, model-agnostic tool that helps practitioners assess how well their models reflect the true structure of the data. <div>
arXiv:2511.21931v1 Announce Type: new 
Abstract: In this work, we propose a simple and computationally efficient framework to evaluate whether machine learning models align with the structure of the data they learn from; that is, whether \textit{the model says what the data says}. Unlike existing interpretability methods that focus exclusively on explaining model behavior, our approach establishes a baseline derived directly from the data itself. Drawing inspiration from Rubin's Potential Outcomes Framework, we quantify how strongly each feature separates the two outcome groups in a binary classification task, moving beyond traditional descriptive statistics to estimate each feature's effect on the outcome. By comparing these data-derived feature rankings against model-based explanations, we provide practitioners with an interpretable and model-agnostic method to assess model--data alignment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Quantum Autoencoder Trainable Kernel for IoT Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.21932</link>
<guid>https://arxiv.org/abs/2511.21932</guid>
<content:encoded><![CDATA[
<div> Quantum autoencoder, quantum support vector classification, IoT traffic, intrusion detection, NISQ devices  

<br><br>Summary: This article addresses the escalating challenge of detecting anomalies in high-dimensional Internet of Things (IoT) traffic where classical methods fall short. It proposes a quantum machine learning framework combining a quantum autoencoder (QAE) for compressing network traffic into effective latent representations with a quantum support vector classification (QSVC) technique for intrusion detection. The framework was evaluated on three different datasets, demonstrating enhanced accuracy both in ideal quantum simulators and on real IBM Quantum hardware. Notably, the practical experiments reveal a quantum advantage achievable on current Noisy Intermediate-Scale Quantum (NISQ) devices, which are available today. A key insight is that moderate depolarizing noise inherent in NISQ hardware works as an implicit regularizer, which stabilizes the training process and improves the model’s generalization capabilities. This finding indicates that quantum noise, often seen as a drawback, can be beneficial in quantum learning tasks. Overall, the work establishes quantum machine learning as a promising, practical, and hardware-ready solution to the pressing cybersecurity challenge of real-time intrusion detection, especially in complex, high-dimensional IoT environments where traditional approaches struggle to keep pace. <div>
arXiv:2511.21932v1 Announce Type: new 
Abstract: Escalating cyber threats and the high-dimensional complexity of IoT traffic have outpaced classical anomaly detection methods. While deep learning offers improvements, computational bottlenecks limit real-time deployment at scale. We present a quantum autoencoder (QAE) framework that compresses network traffic into discriminative latent representations and employs quantum support vector classification (QSVC) for intrusion detection. Evaluated on three datasets, our approach achieves improved accuracy on ideal simulators and on the IBM Quantum hardware demonstrating practical quantum advantage on current NISQ devices. Crucially, moderate depolarizing noise acts as implicit regularization, stabilizing training and enhancing generalization. This work establishes quantum machine learning as a viable, hardware-ready solution for real-world cybersecurity challenges.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation</title>
<link>https://arxiv.org/abs/2511.21934</link>
<guid>https://arxiv.org/abs/2511.21934</guid>
<content:encoded><![CDATA[
<div> Keywords: feature transformation, multi-agent reinforcement learning, feature crossing, multi-head attention, shared critic mechanism<br><br>Summary: This paper addresses the challenge of improving feature transformation for structured data, where deep learning models often underperform in capturing complex feature interactions. Existing automated methods frequently rely on heuristics or exhaustive searches, which are inefficient. Recent reinforcement learning (RL) approaches improve upon this, but face two main issues: instability caused by dynamically expanding features during transformation and poor communication among agents leading to suboptimal feature crossing. To overcome these, the authors propose a novel heterogeneous multi-agent RL framework with three agents divided into two types, each responsible for selecting critical features and operations. A shared critic mechanism is introduced to enhance communication among agents, facilitating better cooperation in feature crossing. Moreover, multi-head attention-based feature agents are designed to effectively handle the dynamically growing feature space by selecting appropriate features for crossing. A state encoding technique is developed to stabilize and improve RL agents’ learning dynamics, yielding more robust and reliable transformation policies. Comprehensive experiments demonstrate the proposed model’s effectiveness, efficiency, robustness, and interpretability, indicating its potential for scalable and collaborative feature transformation in complex structured data tasks. <div>
arXiv:2511.21934v1 Announce Type: new 
Abstract: Feature transformation enhances downstream task performance by generating informative features through mathematical feature crossing. Despite the advancements in deep learning, feature transformation remains essential for structured data, where deep models often struggle to capture complex feature interactions. Prior literature on automated feature transformation has achieved success but often relies on heuristics or exhaustive searches, leading to inefficient and time-consuming processes. Recent works employ reinforcement learning (RL) to enhance traditional approaches through a more effective trial-and-error way. However, two limitations remain: 1) Dynamic feature expansion during the transformation process, which causes instability and increases the learning complexity for RL agents; 2) Insufficient cooperation and communication between agents, which results in suboptimal feature crossing operations and degraded model performance. To address them, we propose a novel heterogeneous multi-agent RL framework to enable cooperative and scalable feature transformation. The framework comprises three heterogeneous agents, grouped into two types, each designed to select essential features and operations for feature crossing. To enhance communication among these agents, we implement a shared critic mechanism that facilitates information exchange during feature transformation. To handle the dynamically expanding feature space, we tailor multi-head attention-based feature agents to select suitable features for feature crossing. Additionally, we introduce a state encoding technique during the optimization process to stabilize and enhance the learning dynamics of the RL agents, resulting in more robust and reliable transformation policies. Finally, we conduct extensive experiments to validate the effectiveness, efficiency, robustness, and interpretability of our model.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Algorithmic Collusion in Human-AI Ecosystems</title>
<link>https://arxiv.org/abs/2511.21935</link>
<guid>https://arxiv.org/abs/2511.21935</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, repeated pricing games, human defection, algorithmic collusion, mixed ecosystems<br><br>Summary:  
This paper investigates the dynamics of pricing in ecosystems where AI agents and humans interact repeatedly. The authors focus on a theoretical model based on repeated pricing games wherein AI agents follow equilibrium strategies to sustain supracompetitive prices. However, the presence of humans who manually set prices using no-regret strategies represents a form of defection from the AI-driven collusion. The main finding reveals that even a single human defector can destabilize the collusive equilibrium maintained by AI agents and significantly lower prices. As the number of human defectors increases, prices are pushed closer to competitive market levels, undermining AI-facilitated collusion. Furthermore, the study explores how AI agents that are defection-aware adapt their collusive strategies to such mixed ecosystems involving both humans and AI participants. This work characterizes the conditions under which algorithmic collusion remains robust or fragile, highlighting that collusion maintained by AI agents is vulnerable in environments where humans occasionally disrupt algorithmic strategies. The research contributes a theoretical understanding of the fragility and potential persistence of algorithmic collusion in human-AI interactive settings. <div>
arXiv:2511.21935v1 Announce Type: new 
Abstract: AI agents are increasingly deployed in ecosystems where they repeatedly interact not only with each other but also with humans. In this work, we study these human-AI ecosystems from a theoretical perspective, focusing on the classical framework of repeated pricing games. In our stylized model, the AI agents play equilibrium strategies, and one or more humans manually perform the pricing task instead of adopting an AI agent, thereby defecting to a no-regret strategy. Motivated by how populations of AI agents can sustain supracompetitive prices, we investigate whether high prices persist under such defections. Our main finding is that even a single human defection can destabilize collusion and drive down prices, and multiple defections push prices even closer to competitive levels. We further show how the nature of collusion changes under defection-aware AI agents. Taken together, our results characterize when algorithmic collusion is fragile--and when it persists--in mixed ecosystems of AI agents and humans.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning Architectures for Code-Modulated Visual Evoked Potentials Detection</title>
<link>https://arxiv.org/abs/2511.21940</link>
<guid>https://arxiv.org/abs/2511.21940</guid>
<content:encoded><![CDATA[
<div> Keywords: Brain-Computer Interfaces, Code-Modulated Visual Evoked Potentials, deep learning, EEG decoding, Siamese networks<br><br>Summary:  
This study addresses the challenge of decoding Code-Modulated Visual Evoked Potentials (C-VEPs) in non-invasive Brain-Computer Interfaces (BCIs) by proposing advanced deep learning methods to overcome temporal variability and session-dependent noise in EEG signals. Various architectures were evaluated, including convolutional neural networks (CNNs) designed for 63-bit m-sequence reconstruction and classification, as well as Siamese networks leveraging similarity-based decoding methods. Canonical correlation analysis (CCA) was used as a baseline for comparison. EEG data from 13 healthy adults during single-target flicker stimulation were utilized for model training and testing. The deep learning models outperformed traditional methods significantly, with the use of Earth Mover's Distance (EMD) and constrained EMD metrics yielding greater robustness to latency shifts than Euclidean and Mahalanobis distances. Implementing temporal data augmentation by applying minor temporal shifts further enhanced the generalization capabilities of these models across different sessions. Among all tested architectures, the multi-class Siamese network achieved the highest average decoding accuracy of 96.89%, demonstrating its effectiveness. These findings highlight the promise of data-driven deep learning approaches for reliable, single-trial decoding of C-VEPs, paving the way for more adaptive and robust non-invasive BCI systems. <div>
arXiv:2511.21940v1 Announce Type: new 
Abstract: Non-invasive Brain-Computer Interfaces (BCIs) based on Code-Modulated Visual Evoked Potentials (C-VEPs) require highly robust decoding methods to address temporal variability and session-dependent noise in EEG signals. This study proposes and evaluates several deep learning architectures, including convolutional neural networks (CNNs) for 63-bit m-sequence reconstruction and classification, and Siamese networks for similarity-based decoding, alongside canonical correlation analysis (CCA) baselines. EEG data were recorded from 13 healthy adults under single-target flicker stimulation. The proposed deep models significantly outperformed traditional approaches, with distance-based decoding using Earth Mover's Distance (EMD) and constrained EMD showing greater robustness to latency variations than Euclidean and Mahalanobis metrics. Temporal data augmentation with small shifts further improved generalization across sessions. Among all models, the multi-class Siamese network achieved the best overall performance with an average accuracy of 96.89%, demonstrating the potential of data-driven deep architectures for reliable, single-trial C-VEP decoding in adaptive non-invasive BCI systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABLE: Using Adversarial Pairs to Construct Local Models for Explaining Model Predictions</title>
<link>https://arxiv.org/abs/2511.21952</link>
<guid>https://arxiv.org/abs/2511.21952</guid>
<content:encoded><![CDATA[
<div> Keywords: local explanation, adversarial attack, stability, fidelity, decision boundary<br><br>Summary:<br><br>1. Machine learning models deployed in critical applications often lack transparency and operate as "black boxes," making interpretability essential.<br>2. Local explanation methods like LIME seek to provide interpretability by approximating complex model behavior locally with interpretable models but face issues such as instability and poor local fidelity.<br>3. The paper introduces a novel method called Adversarially Bracketed Local Explanation (ABLE) to overcome these limitations by focusing on the model's local decision boundary around a test instance.<br>4. ABLE generates neighborhood points by adding bounded Gaussian noise to the test instance and uses a two-step adversarial attack process to create adversarial pairs (A and A') that bracket the local decision boundary—A differs in label from D while A' shares D's label.<br>5. A linear model trained on these adversarial pairs effectively approximates the local decision boundary, thereby improving explanation stability and fidelity.<br>6. Experimental validation on six UCI benchmark datasets across three different deep neural network architectures shows that ABLE outperforms state-of-the-art local explanation methods in both stability and fidelity. <div>
arXiv:2511.21952v1 Announce Type: new 
Abstract: Machine learning models are increasingly used in critical applications but are mostly "black boxes" due to their lack of transparency. Local explanation approaches, such as LIME, address this issue by approximating the behavior of complex models near a test instance using simple, interpretable models. However, these approaches often suffer from instability and poor local fidelity. In this paper, we propose a novel approach called Adversarially Bracketed Local Explanation (ABLE) to address these limitations. Our approach first generates a set of neighborhood points near the test instance, x_test, by adding bounded Gaussian noise. For each neighborhood point D, we apply an adversarial attack to generate an adversarial point A with minimal perturbation that results in a different label than D. A second adversarial attack is then performed on A to generate a point A' that has the same label as D (and thus different than A). The points A and A' form an adversarial pair that brackets the local decision boundary for x_test. We then train a linear model on these adversarial pairs to approximate the local decision boundary. Experimental results on six UCI benchmark datasets across three deep neural network architectures demonstrate that our approach achieves higher stability and fidelity than the state-of-the-art.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CTR Prediction on Alibaba's Taobao Advertising Dataset Using Traditional and Deep Learning Models</title>
<link>https://arxiv.org/abs/2511.21963</link>
<guid>https://arxiv.org/abs/2511.21963</guid>
<content:encoded><![CDATA[
<div> Click-through rate prediction, Transformer, user behavior modeling, deep learning, Taobao dataset  

<br><br>Summary:  
This study focuses on improving click-through rate (CTR) prediction essential for advertising systems by leveraging a large-scale Taobao dataset from Alibaba. Initially, traditional supervised models like logistic regression and Light-GBM were used, trained on static features including user demographics, ad characteristics, and contextual metadata, serving as interpretable and fast benchmarks but limited in capturing complex user behaviors. To better capture user intent, the researchers incorporated extensive behavioral data from hundreds of millions of interactions spanning 22 days. By encoding user action sequences, they created dynamic representations of user interests over time. Deep learning models, particularly multilayer perceptrons (MLPs), fused behavioral embeddings with static features and significantly improved performance. To further model temporal dynamics, a Transformer-based architecture employing self-attention mechanisms was proposed, effectively understanding contextual dependencies in user behaviors, including timing and frequency of interactions. This Transformer model improved the AUC metric by 2.81% compared to the logistic regression baseline, especially enhancing predictions for users with diverse or evolving interests. Beyond modeling, the study also suggested an A/B testing framework for practical evaluation. Finally, it discussed broader implications, advocating that personalized ad targeting techniques could be applied to public health for precise delivery of health information and behavioral guidance, thus extending the impact of CTR prediction research beyond e-commerce. <div>
arXiv:2511.21963v1 Announce Type: new 
Abstract: Click-through rates prediction is critical in modern advertising systems, where ranking relevance and user engagement directly impact platform efficiency and business value. In this project, we explore how to model CTR more effectively using a large-scale Taobao dataset released by Alibaba. We start with supervised learning models, including logistic regression and Light-GBM, that are trained on static features such as user demographics, ad attributes, and contextual metadata. These models provide fast, interpretable benchmarks, but have limited capabilities to capture patterns of behavior that drive clicks. To better model user intent, we combined behavioral data from hundreds of millions of interactions over a 22-day period. By extracting and encoding user action sequences, we construct representations of user interests over time. We use deep learning models to fuse behavioral embeddings with static features. Among them, multilayer perceptrons (MLPs) have achieved significant performance improvements. To capture temporal dynamics, we designed a Transformer-based architecture that uses a self-attention mechanism to learn contextual dependencies across behavioral sequences, modeling not only what the user interacts with, but also the timing and frequency of interactions. Transformer improves AUC by 2.81 % over the baseline (LR model), with the largest gains observed for users whose interests are diverse or change over time. In addition to modeling, we propose an A/B testing strategy for real-world evaluation. We also think about the broader implications: personalized ad targeting technology can be applied to public health scenarios to achieve precise delivery of health information or behavior guidance. Our research provides a roadmap for advancing click-through rate predictions and extending their value beyond e-commerce.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOTIF-RF: Multi-template On-chip Transformer Synthesis Incorporating Frequency-domain Self-transfer Learning for RFIC Design Automation</title>
<link>https://arxiv.org/abs/2511.21970</link>
<guid>https://arxiv.org/abs/2511.21970</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-template machine learning, inverse design, transformers, frequency-domain self-transfer learning, evolutionary strategy<br><br>Summary:<br><br>This paper systematically investigates the development of multi-template machine learning (ML) surrogate models tailored for the inverse design of transformers (XFMRs) used in radio-frequency integrated circuits (RFICs). Initially, it benchmarks four prevalent ML architectures—MLP, CNN, UNet, and GT—using consistent datasets across various transformer topologies to establish baseline performance. To surpass these baselines, the authors introduce a novel frequency-domain self-transfer learning technique that leverages correlations between adjacent frequency bands, resulting in significant prediction accuracy improvements of around 30% to 50% in S-parameters. Utilizing these enhanced ML models, the study proceeds to develop an inverse design framework centered on the covariance matrix adaptation evolutionary strategy (CMA-ES) algorithm. This framework is rigorously validated through multiple impedance-matching tasks, consistently demonstrating rapid convergence and reliable performance. Collectively, these advancements contribute toward the overarching goal of AI-powered automation for transforming specifications to physical layouts (specs-to-GDS) in RFIC design. The findings empower RFIC designers by integrating sophisticated AI tools into the design workflow, thereby accelerating design cycles and improving model reliability for practical applications in RFIC transformer design. <div>
arXiv:2511.21970v1 Announce Type: new 
Abstract: This paper presents a systematic study on developing multi-template machine learning (ML) surrogate models and applying them to the inverse design of transformers (XFMRs) in radio-frequency integrated circuits (RFICs). Our study starts with benchmarking four widely used ML architectures, including MLP-, CNN-, UNet-, and GT-based models, using the same datasets across different XFMR topologies. To improve modeling accuracy beyond these baselines, we then propose a new frequency-domain self-transfer learning technique that exploits correlations between adjacent frequency bands, leading to around 30%-50% accuracy improvement in the S-parameters prediction. Building on these models, we further develop an inverse design framework based on the covariance matrix adaptation evolutionary strategy (CMA-ES) algorithm. This framework is validated using multiple impedance-matching tasks, all demonstrating fast convergence and trustworthy performance. These results advance the goal of AI-assisted specs-to-GDS automation for RFICs and provide RFIC designers with actionable tools for integrating AI into their workflows.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Safety and Security Framework for Real-World Agentic Systems</title>
<link>https://arxiv.org/abs/2511.21990</link>
<guid>https://arxiv.org/abs/2511.21990</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic AI, safety and security, risk taxonomy, AI-driven red teaming, enterprise deployment<br><br>Summary: This paper presents a dynamic, actionable framework designed to enhance the safety and security of agentic AI systems deployed in enterprise environments. It argues that safety and security emerge not just from individual models but from the complex interactions between models, orchestrators, tools, and data within their operating context. The authors introduce a novel approach to identifying new agentic risks through a user safety perspective, highlighting how safety and security concerns, traditionally distinct in isolated models, become interconnected in agentic systems. A unified operational agentic risk taxonomy is proposed, addressing classic safety and security issues alongside unique risks such as tool misuse, cascading action chains, and unintended control amplification. Central to the framework is a contextual agentic risk management approach leveraging auxiliary AI models and agents under human supervision to facilitate risk discovery, evaluation, and mitigation. The study emphasizes AI-driven, sandboxed red teaming as a key technique for uncovering hidden risks. Effectiveness is demonstrated through a detailed case study of NVIDIA’s AI-Q Research Assistant, illustrating comprehensive end-to-end safety and security assessments in complex agentic workflows. Finally, the authors release a dataset with over 10,000 simulated attack and defense scenarios, aiming to support further research in agentic AI safety. <div>
arXiv:2511.21990v1 Announce Type: new 
Abstract: This paper introduces a dynamic and actionable framework for securing agentic AI systems in enterprise deployment. We contend that safety and security are not merely fixed attributes of individual models but also emergent properties arising from the dynamic interactions among models, orchestrators, tools, and data within their operating environments. We propose a new way of identification of novel agentic risks through the lens of user safety. Although, for traditional LLMs and agentic models in isolation, safety and security has a clear separation, through the lens of safety in agentic systems, they appear to be connected. Building on this foundation, we define an operational agentic risk taxonomy that unifies traditional safety and security concerns with novel, uniquely agentic risks, including tool misuse, cascading action chains, and unintended control amplification among others. At the core of our approach is a dynamic agentic safety and security framework that operationalizes contextual agentic risk management by using auxiliary AI models and agents, with human oversight, to assist in contextual risk discovery, evaluation, and mitigation. We further address one of the most challenging aspects of safety and security of agentic systems: risk discovery through sandboxed, AI-driven red teaming. We demonstrate the framework effectiveness through a detailed case study of NVIDIA flagship agentic research assistant, AI-Q Research Assistant, showcasing practical, end-to-end safety and security evaluations in complex, enterprise-grade agentic workflows. This risk discovery phase finds novel agentic risks that are then contextually mitigated. We also release the dataset from our case study, containing traces of over 10,000 realistic attack and defense executions of the agentic workflow to help advance research in agentic safety.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distance-based Learning of Hypertrees</title>
<link>https://arxiv.org/abs/2511.22014</link>
<guid>https://arxiv.org/abs/2511.22014</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, shortest-path queries, online algorithm, orderly hypertrees, evolutionary tree reconstruction<br><br>Summary:<br><br>1. This paper addresses the problem of learning hypergraphs using shortest-path queries (SP-queries), focusing on a novel and well-defined class of hypertrees named orderly hypertrees.  
2. It introduces the first provably optimal online algorithm tailored for this class, which can also be converted to an optimal offline learning algorithm.  
3. Orderly hypertrees are analyzed within the Fagin hierarchy, a framework in database theory that categorizes acyclic hypergraphs, and this class strictly includes the largest previously known class of acyclic hypergraphs that can be learned with subquadratic SP-query complexity.  
4. Recognizing practical limitations in distance measurements, especially in applications like evolutionary tree reconstruction where distance accuracy decreases with increasing distance, the authors consider a bounded distance query model.  
5. Within this bounded distance query framework, the paper establishes asymptotically tight complexity bounds for the problem of learning general hypertrees, thus expanding theoretical understanding and practical applicability in scenarios where query accuracy is limited. <div>
arXiv:2511.22014v1 Announce Type: new 
Abstract: We study the problem of learning hypergraphs with shortest-path queries (SP-queries), and present the first provably optimal online algorithm for a broad and natural class of hypertrees that we call orderly hypertrees. Our online algorithm can be transformed into a provably optimal offline algorithm. Orderly hypertrees can be positioned within the Fagin hierarchy of acyclic hypergraph (well-studied in database theory), and strictly encompass the broadest class in this hierarchy that is learnable with subquadratic SP-query complexity.
  Recognizing that in some contexts, such as evolutionary tree reconstruction, distance measurements can degrade with increased distance, we also consider a learning model that uses bounded distance queries. In this model, we demonstrate asymptotically tight complexity bounds for learning general hypertrees.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equilibrium Propagation Without Limits</title>
<link>https://arxiv.org/abs/2511.22024</link>
<guid>https://arxiv.org/abs/2511.22024</guid>
<content:encoded><![CDATA[
<div> Equilibrium Propagation, Helmholtz free energy, Contrastive Hebbian Learning, Gibbs-Boltzmann distribution, finite nudging<br><br>Summary:<br><br>This paper presents a significant advancement in Equilibrium Propagation (EP) by removing the traditional requirement for infinitesimal perturbations in the learning process. Instead, the authors introduce a finite-nudge framework that grounds local credit assignment more robustly. They achieve this by modeling network states probabilistically as Gibbs-Boltzmann distributions instead of deterministic points, enabling a more general and exact treatment. A key theoretical result demonstrated is that the gradient of the difference in Helmholtz free energy between the nudged phase and the free phase corresponds exactly to the difference in expected local energy derivatives. This insight provides a rigorous justification for the Contrastive Hebbian Learning update rule as an exact gradient estimator, even with large nudges, without relying on infinitesimal approximations or assumptions of convexity. Moreover, the study extends EP by deriving a generalized algorithm based on the path integral of loss-energy covariances, which supports learning with strong error signals—something that previous infinitesimal approximation methods could not handle effectively. Overall, this work broadens the applicability and theoretical foundation of EP for training neural networks, potentially improving learning efficiency and flexibility in practical scenarios. <div>
arXiv:2511.22024v1 Announce Type: new 
Abstract: We liberate Equilibrium Propagation (EP) from the limit of infinitesimal perturbations by establishing a finite-nudge foundation for local credit assignment. By modeling network states as Gibbs-Boltzmann distributions rather than deterministic points, we prove that the gradient of the difference in Helmholtz free energy between a nudged and free phase is exactly the difference in expected local energy derivatives. This validates the classic Contrastive Hebbian Learning update as an exact gradient estimator for arbitrary finite nudging, requiring neither infinitesimal approximations nor convexity. Furthermore, we derive a generalized EP algorithm based on the path integral of loss-energy covariances, enabling learning with strong error signals that standard infinitesimal approximations cannot support.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibration-Free EEG-based Driver Drowsiness Detection with Online Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.22030</link>
<guid>https://arxiv.org/abs/2511.22030</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, drowsiness detection, test-time adaptation, batch normalization, prototype learning<br><br>Summary:<br><br>1. Drowsy driving significantly contributes to traffic accidents, and EEG-based detection systems are being explored to address this issue.<br>2. EEG signals show high inter-subject variability, causing domain shift problems and hindering generalization of detection models to new subjects.<br>3. The authors propose a novel driver drowsiness detection framework that leverages online test-time adaptation (TTA) by updating learnable parameters in batch normalization layers while preserving pretrained statistics.<br>4. A memory bank dynamically manages streaming EEG segments, selecting reliable samples based on negative energy scores and persistence time to improve adaptation.<br>5. Prototype learning is introduced to maintain robust predictions despite ongoing distribution shifts during operation.<br>6. The method was validated on a sustained-attention driving dataset in a simulated environment, where delayed reaction times during lane-keeping tasks served as drowsiness indicators.<br>7. Experimental results demonstrate the method outperforms all baselines, with an average F1-score of 81.73%, improving by 11.73% over the best existing TTA baseline.<br>8. This approach significantly advances the adaptability of EEG-based drowsiness detection systems in real-world, non-i.i.d. (non-independent and identically distributed) scenarios. <div>
arXiv:2511.22030v1 Announce Type: new 
Abstract: Drowsy driving is a growing cause of traffic accidents, prompting recent exploration of electroencephalography (EEG)-based drowsiness detection systems. However, the inherent variability of EEG signals due to psychological and physical factors necessitates a cumbersome calibration process. In particular, the inter-subject variability of EEG signals leads to a domain shift problem, which makes it challenging to generalize drowsiness detection models to unseen target subjects. To address these issues, we propose a novel driver drowsiness detection framework that leverages online test-time adaptation (TTA) methods to dynamically adjust to target subject distributions. Our proposed method updates the learnable parameters in batch normalization (BN) layers, while preserving pretrained normalization statistics, resulting in a modified configuration that ensures effective adaptation during test time. We incorporate a memory bank that dynamically manages streaming EEG segments, selecting samples based on their reliability determined by negative energy scores and persistence time. In addition, we introduce prototype learning to ensure robust predictions against distribution shifts over time. We validated our method on the sustained-attention driving dataset collected in a simulated environment, where drowsiness was estimated from delayed reaction times during monotonous lane-keeping tasks. Our experiments show that our method outperforms all baselines, achieving an average F1-score of 81.73\%, an improvement of 11.73\% over the best TTA baseline. This demonstrates that our proposed method significantly enhances the adaptability of EEG-based drowsiness detection systems in non-i.i.d. scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Public Health Impacts of Electricity Usage</title>
<link>https://arxiv.org/abs/2511.22031</link>
<guid>https://arxiv.org/abs/2511.22031</guid>
<content:encoded><![CDATA[
<div> Keywords: electric power sector, air pollutant emissions, health-informed demand-side management, AI model, public health impacts<br><br>Summary:<br><br>1. The electric power sector is a major source of air pollutant emissions that adversely affect public health across communities despite existing regulatory efforts. <br>2. Fossil fuels continue to play a significant role in electricity generation, creating an urgent need for advanced demand-side strategies focused on reducing health impacts. <br>3. The study introduces HealthPredictor, a novel domain-specific AI model designed to link electricity consumption directly to public health outcomes through an integrated pipeline. <br>4. HealthPredictor comprises three components: a fuel mix predictor estimating generation source contributions, an air quality converter modeling pollutant emissions and dispersion, and a health impact assessor converting pollutant changes into monetized health damages. <br>5. Validation across multiple U.S. regions shows the AI-driven health optimization framework significantly outperforms baseline models that rely solely on fuel mix data in predicting public health effects.<br>6. A case study involving electric vehicle charging schedules demonstrates the model’s capability to guide energy management decisions that reduce negative health impacts.<br>7. The research highlights the potential for AI tools to advance health-informed energy management, contributing to improved public health and societal well-being.<br>8. The authors have made their datasets and code publicly available to support further research and application. <div>
arXiv:2511.22031v1 Announce Type: new 
Abstract: The electric power sector is a leading source of air pollutant emissions, impacting the public health of nearly every community. Although regulatory measures have reduced air pollutants, fossil fuels remain a significant component of the energy supply, highlighting the need for more advanced demand-side approaches to reduce the public health impacts. To enable health-informed demand-side management, we introduce HealthPredictor, a domain-specific AI model that provides an end-to-end pipeline linking electricity use to public health outcomes. The model comprises three components: a fuel mix predictor that estimates the contribution of different generation sources, an air quality converter that models pollutant emissions and atmospheric dispersion, and a health impact assessor that translates resulting pollutant changes into monetized health damages. Across multiple regions in the United States, our health-driven optimization framework yields substantially lower prediction errors in terms of public health impacts than fuel mix-driven baselines. A case study on electric vehicle charging schedules illustrates the public health gains enabled by our method and the actionable guidance it can offer for health-informed energy management. Overall, this work shows how AI models can be explicitly designed to enable health-informed energy management for advancing public health and broader societal well-being. Our datasets and code are released at: https://github.com/Ren-Research/Health-Impact-Predictor.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence Dynamics of Over-Parameterized Score Matching for a Single Gaussian</title>
<link>https://arxiv.org/abs/2511.22069</link>
<guid>https://arxiv.org/abs/2511.22069</guid>
<content:encoded><![CDATA[
<div> Keywords: score matching, gradient descent, over-parameterized models, Gaussian distribution, convergence guarantees  

<br><br>Summary:  
This paper investigates the optimization dynamics of score matching when training over-parameterized models to learn a single Gaussian distribution. First, it considers a student model with \( n \) parameters trained using population score matching on Gaussian data. Second, it establishes that in the large noise regime, gradient descent achieves global convergence to the ground truth parameters. Third, in the low noise regime, a stationary point exists, making global convergence difficult to prove generally. However, under the condition of exponentially small parameter initialization, gradient descent is shown to converge to the ground truth. Fourth, the study demonstrates that without such small initialization, parameters may fail to converge. Fifth, when parameters are randomly initialized from a Gaussian distribution far from the ground truth, only one parameter tends to converge while others diverge, yet the loss function still converges to zero at a rate of \( 1/\tau \), with \(\tau\) iterations. The paper also provides a nearly matching lower bound for this convergence rate. Importantly, this is the first work proving global convergence guarantees for Gaussian mixtures with at least three components under the score matching framework, advancing the theoretical understanding of score matching in generative modeling. <div>
arXiv:2511.22069v1 Announce Type: new 
Abstract: Score matching has become a central training objective in modern generative modeling, particularly in diffusion models, where it is used to learn high-dimensional data distributions through the estimation of score functions. Despite its empirical success, the theoretical understanding of the optimization behavior of score matching, particularly in over-parameterized regimes, remains limited. In this work, we study gradient descent for training over-parameterized models to learn a single Gaussian distribution. Specifically, we use a student model with $n$ learnable parameters and train it on data generated from a single ground-truth Gaussian using the population score matching objective. We analyze the optimization dynamics under multiple regimes. When the noise scale is sufficiently large, we prove a global convergence result for gradient descent. In the low-noise regime, we identify the existence of a stationary point, highlighting the difficulty of proving global convergence in this case. Nevertheless, we show convergence under certain initialization conditions: when the parameters are initialized to be exponentially small, gradient descent ensures convergence of all parameters to the ground truth. We further prove that without the exponentially small initialization, the parameters may not converge to the ground truth. Finally, we consider the case where parameters are randomly initialized from a Gaussian distribution far from the ground truth. We prove that, with high probability, only one parameter converges while the others diverge, yet the loss still converges to zero with a $1/\tau$ rate, where $\tau$ is the number of iterations. We also establish a nearly matching lower bound on the convergence rate in this regime. This is the first work to establish global convergence guarantees for Gaussian mixtures with at least three components under the score matching framework.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-View Multi-Timescale Hypergraph-Empowered Spatiotemporal Framework for EV Charging Forecasting</title>
<link>https://arxiv.org/abs/2511.22072</link>
<guid>https://arxiv.org/abs/2511.22072</guid>
<content:encoded><![CDATA[
<div> Keywords: EV charging demand, hypergraphs, spatiotemporal forecasting, multi-view modeling, cross-attention  

<br><br>Summary:  
Accurate forecasting of electric vehicle (EV) charging demand is critical for maintaining grid stability and enabling EVs to actively participate in the electricity market. Current forecasting methods, particularly those using graph neural networks, typically focus only on pairwise relationships between charging stations, which limits their ability to capture the complex, group-wise interactions present in urban charging networks. To overcome this limitation, the authors propose HyperCast, a novel forecasting framework based on hypergraphs that can model higher-order spatiotemporal dependencies in EV charging patterns. HyperCast incorporates multi-view hypergraphs that represent both static geographical proximity and dynamic demand-based functional similarities among stations. It also integrates multi-timescale input data, allowing it to distinguish recent trends from weekly periodic patterns. The framework is equipped with specialized hyper-spatiotemporal blocks and custom cross-attention mechanisms to effectively combine information from multiple views and timescales. Extensive experiments on four public datasets demonstrate that HyperCast significantly outperforms existing state-of-the-art methods, confirming the benefit of explicitly modeling collective charging behaviors for improved forecasting accuracy. This approach advances the capabilities of EV demand prediction by harnessing complex group interactions and multi-dimensional dependencies. <div>
arXiv:2511.22072v1 Announce Type: new 
Abstract: Accurate electric vehicle (EV) charging demand forecasting is essential for stable grid operation and proactive EV participation in electricity market. Existing forecasting methods, particularly those based on graph neural networks, are often limited to modeling pairwise relationships between stations, failing to capture the complex, group-wise dynamics inherent in urban charging networks. To address this gap, we develop a novel forecasting framework namely HyperCast, leveraging the expressive power of hypergraphs to model the higher-order spatiotemporal dependencies hidden in EV charging patterns. HyperCast integrates multi-view hypergraphs, which capture both static geographical proximity and dynamic demand-based functional similarities, along with multi-timescale inputs to differentiate between recent trends and weekly periodicities. The framework employs specialized hyper-spatiotemporal blocks and tailored cross-attention mechanisms to effectively fuse information from these diverse sources: views and timescales. Extensive experiments on four public datasets demonstrate that HyperCast significantly outperforms a wide array of state-of-the-art baselines, demonstrating the effectiveness of explicitly modeling collective charging behaviors for more accurate forecasting.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARES: Anomaly Recognition Model For Edge Streams</title>
<link>https://arxiv.org/abs/2511.22078</link>
<guid>https://arxiv.org/abs/2511.22078</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal graphs, anomaly detection, edge streams, graph neural networks, half-space trees  

<br><br>Summary:  
This paper addresses the challenge of real-time anomaly detection in temporal graphs, which model streaming information through dynamic edge changes. Identifying unusual temporal connections (edge anomalies) is critical for preventing risks in various real-world applications, but is complicated by factors such as concept drift, high data volumes, and the need for immediate responses. The authors propose ARES, an unsupervised framework that detects anomalies in edge streams by combining Graph Neural Networks (GNNs) with Half-Space Trees (HST). GNNs are used to extract features by embedding node and edge properties into a latent space, effectively capturing different anomalous patterns like spikes and bursts. HST then partitions this latent space to efficiently isolate anomalies. ARES operates without requiring labeled data, making it broadly applicable. Additionally, a supervised thresholding method based on statistical dispersion of anomaly scores is incorporated to optimize threshold setting using minimal labeled examples, enhancing adaptability across domains. The framework’s performance is demonstrated via extensive experiments on multiple real-world cyber-attack scenarios, showing competitive detection accuracy, as well as favorable space and time complexity when compared to existing methods. <div>
arXiv:2511.22078v1 Announce Type: new 
Abstract: Many real-world scenarios involving streaming information can be represented as temporal graphs, where data flows through dynamic changes in edges over time. Anomaly detection in this context has the objective of identifying unusual temporal connections within the graph structure. Detecting edge anomalies in real time is crucial for mitigating potential risks. Unlike traditional anomaly detection, this task is particularly challenging due to concept drifts, large data volumes, and the need for real-time response. To face these challenges, we introduce ARES, an unsupervised anomaly detection framework for edge streams. ARES combines Graph Neural Networks (GNNs) for feature extraction with Half-Space Trees (HST) for anomaly scoring. GNNs capture both spike and burst anomalous behaviors within streams by embedding node and edge properties in a latent space, while HST partitions this space to isolate anomalies efficiently. ARES operates in an unsupervised way without the need for prior data labeling. To further validate its detection capabilities, we additionally incorporate a simple yet effective supervised thresholding mechanism. This approach leverages statistical dispersion among anomaly scores to determine the optimal threshold using a minimal set of labeled data, ensuring adaptability across different domains. We validate ARES through extensive evaluations across several real-world cyber-attack scenarios, comparing its performance against existing methods while analyzing its space and time complexity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization</title>
<link>https://arxiv.org/abs/2511.22080</link>
<guid>https://arxiv.org/abs/2511.22080</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, momentum, sharpness-aware minimization, non-IID, convergence

<br><br>Summary:  
The paper addresses critical challenges in federated learning (FL), focusing on achieving fast convergence under tight communication constraints while ensuring model generalization across non-IID client data distributions. It identifies two key failure modes when combining momentum and sharpness-aware minimization (SAM) in FL: local-global curvature misalignment, where local SAM perturbations do not align with the global loss geometry, and momentum-echo oscillation, a late-stage instability caused by accumulated momentum. To mitigate these issues, the authors propose FedWMSAM, a novel method that introduces a momentum-guided global perturbation based on server-aggregated momentum to align clients’ SAM directions with the global descent geometry, enabling an efficient single-backprop SAM approximation. Additionally, they design a cosine-similarity adaptive rule to couple momentum and SAM, implementing an early-momentum, late-SAM two-phase training schedule that stabilizes training. Theoretical contributions include a non-IID convergence bound explicitly accounting for perturbation-induced variance and its dependence on system variables such as the number of clients, rounds, and local steps. Extensive experiments across multiple datasets and models demonstrate the method’s effectiveness, adaptability, robustness, and superiority over existing approaches in overcoming FL optimization challenges. The implementation code is publicly available. <div>
arXiv:2511.22080v1 Announce Type: new 
Abstract: In federated learning (FL), models must \emph{converge quickly} under tight communication budgets while \emph{generalizing} across non-IID client distributions. These twin requirements have naturally led to two widely used techniques: client/server \emph{momentum} to accelerate progress, and \emph{sharpness-aware minimization} (SAM) to prefer flat solutions. However, simply combining momentum and SAM leaves two structural issues unresolved in non-IID FL. We identify and formalize two failure modes: \emph{local-global curvature misalignment} (local SAM directions need not reflect the global loss geometry) and \emph{momentum-echo oscillation} (late-stage instability caused by accumulated momentum). To our knowledge, these failure modes have not been jointly articulated and addressed in the FL literature. We propose \textbf{FedWMSAM} to address both failure modes. First, we construct a momentum-guided global perturbation from server-aggregated momentum to align clients' SAM directions with the global descent geometry, enabling a \emph{single-backprop} SAM approximation that preserves efficiency. Second, we couple momentum and SAM via a cosine-similarity adaptive rule, yielding an early-momentum, late-SAM two-phase training schedule. We provide a non-IID convergence bound that \emph{explicitly models the perturbation-induced variance} $\sigma_\rho^2=\sigma^2+(L\rho)^2$ and its dependence on $(S, K, R, N)$ on the theory side. We conduct extensive experiments on multiple datasets and model architectures, and the results validate the effectiveness, adaptability, and robustness of our method, demonstrating its superiority in addressing the optimization challenges of Federated Learning. Our code is available at https://github.com/Huang-Yongzhi/NeurlPS_FedWMSAM.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Bayesian Optimization for Quality Improvement in Fuselage Assembly</title>
<link>https://arxiv.org/abs/2511.22090</link>
<guid>https://arxiv.org/abs/2511.22090</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum Bayesian Optimization, smart manufacturing, aerospace fuselage assembly, sample efficiency, shape control<br><br>Summary:<br><br>This paper addresses the challenge of improving sample efficiency in aerospace fuselage assembly through precise shape adjustment to minimize dimensional gaps between assembled sections. Traditional Monte Carlo methods require many samples to estimate mean responses accurately, causing inefficiency in manufacturing contexts. The authors propose a Quantum Bayesian Optimization (QBO) framework that leverages quantum algorithms capable of achieving similar estimation accuracy with fewer samples. QBO integrates a quantum oracle, which relies on finite element analysis (FEA) or surrogate models, to obtain more accurate environmental response estimations with fewer queries. The optimization uses an Upper Confidence Bound (UCB) acquisition function to strategically select input parameters that maximize the objective. The method is theoretically proven to reduce sample usage while maintaining optimization performance comparable to classical approaches. A case study involving force-controlled actuators applied to one fuselage section demonstrates QBO’s practical advantages in reducing dimensional error and uncertainty. Experimental results confirm that with the same number of queries, QBO outperforms classical methods in accuracy and reliability, indicating significant potential for improving manufacturing process efficiency in aerospace assembly through quantum-enhanced techniques. <div>
arXiv:2511.22090v1 Announce Type: new 
Abstract: Recent efforts in smart manufacturing have enhanced aerospace fuselage assembly processes, particularly by innovating shape adjustment techniques to minimize dimensional gaps between assembled sections. Existing approaches have shown promising results but face the issue of low sample efficiency from the manufacturing systems. It arises from the limitation of the classical Monte Carlo method when uncovering the mean response from a distribution. In contrast, recent work has shown that quantum algorithms can achieve the same level of estimation accuracy with significantly fewer samples than the classical Monte Carlo method from distributions. Therefore, we can adopt the estimation of the quantum algorithm to obtain the estimation from real physical systems (distributions). Motivated by this advantage, we propose a Quantum Bayesian Optimization (QBO) framework for precise shape control during assembly to improve the sample efficiency in manufacturing practice. Specifically, this approach utilizes a quantum oracle, based on finite element analysis (FEA)-based models or surrogate models, to acquire a more accurate estimation of the environment response with fewer queries for a certain input. QBO employs an Upper Confidence Bound (UCB) as the acquisition function to strategically select input values that are most likely to maximize the objective function. It has been theoretically proven to require much fewer samples while maintaining comparable optimization results. In the case study, force-controlled actuators are applied to one fuselage section to adjust its shape and reduce the gap to the adjoining section. Experimental results demonstrate that QBO achieves significantly lower dimensional error and uncertainty compared to classical methods, particularly using the same queries from the simulation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs</title>
<link>https://arxiv.org/abs/2511.22099</link>
<guid>https://arxiv.org/abs/2511.22099</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, low-rank factorization, trustworthiness, adversarial robustness, fairness  

<br><br>Summary:  
This paper investigates the trustworthiness implications of compressing large language models (LLMs) using low-rank factorization, a technique effective for reducing size, memory, and computation while maintaining accuracy. First, the study finds that low-rank compression tends to preserve or improve training data privacy but weakens protection of personally identifiable information (PII) during interactive conversations. Second, adversarial robustness of compressed LLMs is generally maintained and often enhanced, even when models undergo deep compression. Third, ethical reasoning abilities of LLMs degrade in zero-shot evaluation scenarios after compression, but this decline can be partially mitigated by employing few-shot prompting techniques. Fourth, the analysis reveals that fairness metrics deteriorate under low-rank compression. Additionally, the paper explores how factors such as model scale and fine-tuning impact trustworthiness, which are critical considerations in low-rank compression strategies. Finally, to guide the development of more trustworthy compressed models, a gradient-based attribution analysis is conducted to identify specific layers within LLMs that contribute most significantly to adversarial robustness, offering actionable insights for future compression methods balancing efficiency and trustworthiness. <div>
arXiv:2511.22099v1 Announce Type: new 
Abstract: Large language models (LLMs) have driven major advances across domains, yet their massive size hinders deployment in resource-constrained settings. Model compression addresses this challenge, with low-rank factorization emerging as a particularly effective method for reducing size, memory, and computation while maintaining accuracy. However, while these compressed models boast of benign performance and system-level advantages, their trustworthiness implications remain poorly understood. In this paper, we present the first comprehensive study of how low-rank factorization affects LLM trustworthiness across privacy, adversarial robustness, fairness, and ethical alignment. We evaluate multiple LLMs of different sizes and variants compressed with diverse low-rank algorithms, revealing key insights: (1) low-rank compression preserves or improves training data privacy but weakens PII protection during conversation; (2) adversarial robustness is generally preserved and often enhanced, even under deep compression; (3) ethical reasoning degrades in zero-shot settings but partially recovers with few-shot prompting; (4) fairness declines under compression. Beyond compression, we investigate how model scale and fine-tuning affect trustworthiness, as both are important in low-rank methods. To guide trustworthy compression strategies, we end our paper with a gradient-based attribution analysis to identify which layers in LLMs contribute most to adversarial robustness.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba</title>
<link>https://arxiv.org/abs/2511.22101</link>
<guid>https://arxiv.org/abs/2511.22101</guid>
<content:encoded><![CDATA[
<div> Keywords: Uniswap V3, Deep Reinforcement Learning, Liquidity Provision, Mamba, DDQN<br><br>Summary:<br><br>This report reviews the replication and enhancement of the article "Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning." The replication section outlines the process of obtaining data from the Uniswap Subgraph, details the implementation steps, and offers commentary on the results achieved. Following the replication, a new model structure is proposed that integrates the Mamba framework with Double Deep Q-Networks (DDQN) and introduces a novel reward function designed to improve performance. Data cleaning is conducted again to ensure higher data quality and reliability. To evaluate the enhanced model, two new baselines are introduced for comprehensive comparison. Although the improved model has yet to be tested across all datasets, it demonstrates stronger theoretical foundations than the original model. Empirical testing on available datasets indicates that the new model outperforms the original in several key tests, suggesting potential for better adaptive liquidity provision strategies in Uniswap V3. <div>
arXiv:2511.22101v1 Announce Type: new 
Abstract: The report goes through the main steps of replicating and improving the article "Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning." The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representative Action Selection for Large Action Space: From Bandits to MDPs</title>
<link>https://arxiv.org/abs/2511.22104</link>
<guid>https://arxiv.org/abs/2511.22104</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, action subset selection, Markov Decision Processes, meta-bandits, non-centered sub-Gaussian process<br><br>Summary:<br><br>1. The article addresses the challenge of selecting a small yet representative subset of actions from a large shared action space across multiple reinforcement learning (RL) environments. This problem is particularly relevant in domains like inventory management and recommendation systems where evaluating every action is infeasible.<br><br>2. The goal is to identify a fixed subset of actions that contains a near-optimal action for each environment in a family, thus enabling efficient learning without the computational burden of exploring the entire action space.<br><br>3. This work generalizes previous results from the meta-bandit framework to the broader context of Markov Decision Processes (MDPs), thereby extending applicability to more complex decision-making problems.<br><br>4. The authors prove that their existing algorithm performs comparably to using the full action space, providing strong theoretical guarantees even under relaxed assumptions characterized by a non-centered sub-Gaussian process model. This model allows for greater heterogeneity among environments.<br><br>5. Overall, the proposed approach offers a computationally and sample-efficient method for large-scale combinatorial decision-making under uncertainty, making it valuable for real-world applications involving diverse and complex RL settings. <div>
arXiv:2511.22104v1 Announce Type: new 
Abstract: We study the problem of selecting a small, representative action subset from an extremely large action space shared across a family of reinforcement learning (RL) environments -- a fundamental challenge in applications like inventory management and recommendation systems, where direct learning over the entire space is intractable. Our goal is to identify a fixed subset of actions that, for every environment in the family, contains a near-optimal action, thereby enabling efficient learning without exhaustively evaluating all actions.
  This work extends our prior results for meta-bandits to the more general setting of Markov Decision Processes (MDPs). We prove that our existing algorithm achieves performance comparable to using the full action space. This theoretical guarantee is established under a relaxed, non-centered sub-Gaussian process model, which accommodates greater environmental heterogeneity. Consequently, our approach provides a computationally and sample-efficient solution for large-scale combinatorial decision-making under uncertainty.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.22105</link>
<guid>https://arxiv.org/abs/2511.22105</guid>
<content:encoded><![CDATA[
<div> Dynamic sleep mode optimization (SMO), millimeter-wave networks, multi-agent deep reinforcement learning (MARL), energy efficiency (EE), quality-of-service (QoS)  

<br><br>Summary:  
This paper addresses dynamic sleep mode optimization (SMO) in millimeter-wave (mmWave) networks to enhance energy efficiency (EE) while adhering to strict quality-of-service (QoS) constraints. Unlike traditional approaches that use static and aggregated base station (BS) traffic models, this work captures the inherent non-stationary traffic dynamics by introducing a multi-agent deep reinforcement learning (MARL) framework based on Double Deep Q-Network (DDQN), named MARL-DDQN. The framework operates in a realistic 3D urban environment incorporating time-varying, community-based user equipment (UE) mobility. MARL-DDQN supports scalable and distributed decision-making across multiple agents, reducing signaling overhead compared to single-agent RL methods. The study integrates an accurate BS power consumption model and beamforming techniques to effectively quantify EE. QoS is measured via throughput, ensuring fairness and mitigating inter-cell interference. Simulation results demonstrate that MARL-DDQN significantly outperforms existing methods such as All On, iterative QoS-aware load-based (IT-QoS-LB), MARL-DDPG, and MARL-PPO. The proposed approach achieves up to 0.60 Mbit/Joule in EE, delivers an 8.5 Mbps 10th-percentile throughput, and satisfies QoS constraints 95% of the time in dynamic scenarios, proving its effectiveness and practicality for real-world mmWave network deployments. <div>
arXiv:2511.22105v1 Announce Type: new 
Abstract: Dynamic sleep mode optimization (SMO) in millimeter-wave (mmWave) networks is essential for maximizing energy efficiency (EE) under stringent quality-of-service (QoS) constraints. However, existing optimization and reinforcement learning (RL) approaches rely on aggregated, static base station (BS) traffic models that fail to capture non-stationary traffic dynamics and suffer from large state-action spaces, limiting real-world deployment. To address these challenges, this paper proposes a multi-agent deep reinforcement learning (MARL) framework using a Double Deep Q-Network (DDQN), referred to as MARL-DDQN, for adaptive SMO in a 3D urban environment with a time-varying and community-based user equipment (UE) mobility model. Unlike conventional single-agent RL, MARL-DDQN enables scalable, distributed decision-making with minimal signaling overhead. A realistic BS power consumption model and beamforming are integrated to accurately quantify EE, while QoS is defined in terms of throughput. The method adapts SMO policies to maximize EE while mitigating inter-cell interference and ensuring throughput fairness. Simulations show that MARL-DDQN outperforms state-of-the-art strategies, including All On, iterative QoS-aware load-based (IT-QoS-LB), MARL-DDPG, and MARL-PPO, achieving up to 0.60 Mbit/Joule EE, 8.5 Mbps 10th-percentile throughput, and meeting QoS constraints 95% of the time under dynamic scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface</title>
<link>https://arxiv.org/abs/2511.22108</link>
<guid>https://arxiv.org/abs/2511.22108</guid>
<content:encoded><![CDATA[
<div> Continuous learning, Deep Spiking Neural Networks, Reinforcement Learning, implantable brain-machine interfaces, energy efficiency  

<br><br>Summary:  
The number of neurons recorded simultaneously in implantable brain-machine interfaces (iBMIs) is exponentially increasing, necessitating efficient data compression strategies. Integrating the neural decoder within the implant offers a solution, but decoder performance often suffers due to system non-stationarity. Continuous learning is therefore critical for maintaining reliable operation without frequent retraining, which also ensures user safety and comfort. Deep Spiking Neural Networks (DSNNs) are promising candidates for resource-efficient neural decoding compatible with the energy constraints of iBMIs. This work proposes continuous learning methods using Reinforcement Learning (RL) algorithms, specifically Banditron and AGREL, adapted for DSNNs. These algorithms are computationally lightweight and effectively address non-stationarity. Experiments were conducted in both open-loop and closed-loop formats. In open-loop tests, DSNN Banditron and DSNN AGREL maintained stable accuracy over extended durations. In closed-loop experiments with perturbations, DSNN Banditron’s performance in time-to-target matched that of DSNN AGREL but achieved significant reductions of 98% in memory access and 99% in multiply-and-accumulate operations during training. Compared to prior continuous learning SNN decoders, DSNN Banditron’s drastically lower computational demand makes it an excellent candidate for future wireless iBMI systems. <div>
arXiv:2511.22108v1 Announce Type: new 
Abstract: The number of simultaneously recorded neurons follows an exponentially increasing trend in implantable brain-machine interfaces (iBMIs). Integrating the neural decoder in the implant is an effective data compression method for future wireless iBMIs. However, the non-stationarity of the system makes the performance of the decoder unreliable. To avoid frequent retraining of the decoder and to ensure the safety and comfort of the iBMI user, continuous learning is essential for real-life applications. Since Deep Spiking Neural Networks (DSNNs) are being recognized as a promising approach for developing resource-efficient neural decoder, we propose continuous learning approaches with Reinforcement Learning (RL) algorithms adapted for DSNNs. Banditron and AGREL are chosen as the two candidate RL algorithms since they can be trained with limited computational resources, effectively addressing the non-stationary problem and fitting the energy constraints of implantable devices. To assess the effectiveness of the proposed methods, we conducted both open-loop and closed-loop experiments. The accuracy of open-loop experiments conducted with DSNN Banditron and DSNN AGREL remains stable over extended periods. Meanwhile, the time-to-target in the closed-loop experiment with perturbations, DSNN Banditron performed comparably to that of DSNN AGREL while achieving reductions of 98% in memory access usage and 99% in the requirements for multiply- and-accumulate (MAC) operations during training. Compared to previous continuous learning SNN decoders, DSNN Banditron requires 98% less computes making it a prime candidate for future wireless iBMI systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Data-Driven Surrogates of the Solar Wind with Spherical Fourier Neural Operator</title>
<link>https://arxiv.org/abs/2511.22112</link>
<guid>https://arxiv.org/abs/2511.22112</guid>
<content:encoded><![CDATA[
<div> Keywords: solar wind, surrogate modeling, Spherical Fourier Neural Operator, space weather forecasting, magnetohydrodynamic models<br><br>Summary:<br>1. The solar wind, a stream of charged particles from the Sun’s corona, significantly influences the heliosphere and space systems near Earth, where its variations can disrupt satellites, power grids, and communications.<br>2. Accurate modeling of solar wind variations like high-speed streams and coronal mass ejections is crucial for effective space weather forecasting.<br>3. Traditional 3D magnetohydrodynamic (MHD) models simulate these variations but are computationally intensive, limiting their use for analyzing impacts of boundary condition uncertainties.<br>4. This work introduces a surrogate modeling approach for steady-state solar wind using a Spherical Fourier Neural Operator (SFNO).<br>5. The SFNO surrogate is compared to the existing HUX numerical surrogate, showing comparable or improved performance on multiple evaluation metrics.<br>6. Although HUX exhibits greater physical smoothness, this highlights the need for better evaluation metrics rather than a limitation of SFNO.<br>7. SFNO is flexible, trainable, and supports efficient real-time forecasting, with the ability to improve as more data become available.<br>8. The source code and additional visual results are publicly accessible on GitHub, supporting transparency and reproducibility. <div>
arXiv:2511.22112v1 Announce Type: new 
Abstract: The solar wind, a continuous stream of charged particles from the Sun's corona, shapes the heliosphere and impacts space systems near Earth. Variations such as high-speed streams and coronal mass ejections can disrupt satellites, power grids, and communications, making accurate modeling essential for space weather forecasting. While 3D magnetohydrodynamic (MHD) models are used to simulate and investigate these variations in the solar wind, they tend to be computationally expensive, limiting their usefulness in investigating the impacts of boundary condition uncertainty. In this work, we develop a surrogate for steady state solar wind modeling, using a Spherical Fourier Neural Operator (SFNO). We compare our model to a previously developed numerical surrogate for this task called HUX, and we show that the SFNO achieves comparable or better performance across several metrics. Though HUX retains advantages in physical smoothness, this underscores the need for improved evaluation criteria rather than a flaw in SFNO. As a flexible and trainable approach, SFNO enables efficient real-time forecasting and can improve with more data. The source code and more visual results are available at https://github.com/rezmansouri/solarwind-sfno-velocity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IVGAE: Handling Incomplete Heterogeneous Data with a Variational Graph Autoencoder</title>
<link>https://arxiv.org/abs/2511.22116</link>
<guid>https://arxiv.org/abs/2511.22116</guid>
<content:encoded><![CDATA[
<div> Keywords: missing data imputation, heterogeneous data, variational graph autoencoder, dual-decoder architecture, Transformer embedding<br><br>Summary:<br><br>1. The paper addresses the challenge of imputing missing values in real-world tabular datasets, particularly when data involve heterogeneous numerical and categorical features. 2. Existing imputation methods often struggle to capture complex dependencies between samples and features, limiting their effectiveness on heterogeneous data. 3. The authors propose IVGAE, a Variational Graph Autoencoder framework that models data as a bipartite graph connecting samples and features, enabling graph representation learning to better capture structural relationships. 4. A novel dual-decoder architecture is introduced, consisting of one decoder that reconstructs feature embeddings and another that models missingness patterns, thus incorporating structural priors that are aware of the missing data mechanisms. 5. To efficiently handle categorical variables, a Transformer-based heterogeneous embedding module replaces traditional one-hot encoding, reducing dimensionality and enhancing encoding quality. 6. The framework is extensively tested on 16 real-world datasets across different missing data scenarios—MCAR (Missing Completely At Random), MAR (Missing At Random), and MNAR (Missing Not At Random)—with missing rates up to 30%. 7. IVGAE consistently outperforms baseline methods in terms of RMSE for imputation quality and F1 score for downstream tasks, demonstrating robustness and effectiveness. 8. The authors provide open-source code and datasets at the given GitHub repository to facilitate reproducibility and further research. <div>
arXiv:2511.22116v1 Announce Type: new 
Abstract: Handling missing data remains a fundamental challenge in real-world tabular datasets, especially when data are heterogeneous with both numerical and categorical features. Existing imputation methods often fail to capture complex structural dependencies and handle heterogeneous data effectively. We present \textbf{IVGAE}, a Variational Graph Autoencoder framework for robust imputation of incomplete heterogeneous data. IVGAE constructs a bipartite graph to represent sample-feature relationships and applies graph representation learning to model structural dependencies. A key innovation is its \textit{dual-decoder architecture}, where one decoder reconstructs feature embeddings and the other models missingness patterns, providing structural priors aware of missing mechanisms. To better encode categorical variables, we introduce a Transformer-based heterogeneous embedding module that avoids high-dimensional one-hot encoding. Extensive experiments on 16 real-world datasets show that IVGAE achieves consistent improvements in RMSE and downstream F1 across MCAR, MAR, and MNAR missing scenarios under 30\% missing rates. Code and data are available at: https://github.com/echoid/IVGAE.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Variational Manifold Embedding Framework for Nonlinear Dimensionality Reduction</title>
<link>https://arxiv.org/abs/2511.22128</link>
<guid>https://arxiv.org/abs/2511.22128</guid>
<content:encoded><![CDATA[
<div> Dimensionality reduction, PCA, nonlinear embeddings, variational framework, manifold learning<br><br>Summary:<br><br>1. This paper addresses the limitations of traditional dimensionality reduction methods such as principal component analysis (PCA), notably their inability to capture nonlinear structures within data manifolds. 2. It introduces a novel variational framework that conceptualizes dimensionality reduction as an optimal manifold embedding problem, thereby enabling more flexible, nonlinear embeddings than PCA. 3. The framework's variational nature provides interpretability benefits—in particular, each solution derived from this approach satisfies a specific set of partial differential equations, linking the embedding process to the underlying mathematical structure. 4. The authors analyze how the solutions reflect symmetries inherent to the embedding objective, further deepening the understanding of the learned representations. 5. In addition to theoretical insights, the framework can be analytically characterized in various scenarios, and interestingly, it recovers PCA exactly as a special case, demonstrating its generality and connection to classical methods. <div>
arXiv:2511.22128v1 Announce Type: new 
Abstract: Dimensionality reduction algorithms like principal component analysis (PCA) are workhorses of machine learning and neuroscience, but each has well-known limitations. Variants of PCA are simple and interpretable, but not flexible enough to capture nonlinear data manifold structure. More flexible approaches have other problems: autoencoders are generally difficult to interpret, and graph-embedding-based methods can produce pathological distortions in manifold geometry. Motivated by these shortcomings, we propose a variational framework that casts dimensionality reduction algorithms as solutions to an optimal manifold embedding problem. By construction, this framework permits nonlinear embeddings, allowing its solutions to be more flexible than PCA. Moreover, the variational nature of the framework has useful consequences for interpretability: each solution satisfies a set of partial differential equations, and can be shown to reflect symmetries of the embedding objective. We discuss these features in detail and show that solutions can be analytically characterized in some cases. Interestingly, one special case exactly recovers PCA.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking In-context Experiential Learning Through Repeated Product Recommendations</title>
<link>https://arxiv.org/abs/2511.22130</link>
<guid>https://arxiv.org/abs/2511.22130</guid>
<content:encoded><![CDATA[
<div> Experiential Learning, Product Recommendation, User Personas, Large Language Models, Adaptive Agents<br><br>Summary: This paper addresses the challenge of enabling agents to operate effectively in dynamic real-world environments where knowledge is incomplete and preferences shift over time. It highlights a critical gap in current evaluations, which tend to focus on tasks without ambiguity and fail to assess an agent’s capacity for adaptive, experiential learning. The authors introduce a new benchmark named BELA (Benchmark for Experiential Learning and Active exploration) tailored to test and promote in-context experiential learning within a product recommendation setting. BELA integrates three core components: a diverse set of real-world products from Amazon, multiple user personas encapsulating different and latent consumer preferences, and a large language model–based user simulator driven by these personas to generate interactive dialogue trajectories. Through this setup, agents must learn and refine their recommendations over multiple episodes by actively gathering information and adapting to evolving preferences expressed in natural language conversations. Experimental results reveal that state-of-the-art models currently underperform on this benchmark, showing limited improvement across interactions, which underscores the need for developing more agentic systems that are capable of effective in-context learning and adaptive reasoning through experience. <div>
arXiv:2511.22130v1 Announce Type: new 
Abstract: To reliably navigate ever-shifting real-world environments, agents must grapple with incomplete knowledge and adapt their behavior through experience. However, current evaluations largely focus on tasks that leave no ambiguity, and do not measure agents' ability to adaptively learn and reason through the experiences they accrued. We exemplify the need for this in-context experiential learning in a product recommendation context, where agents must navigate shifting customer preferences and product landscapes through natural language dialogue. We curate a benchmark for experiential learning and active exploration (BELA) that combines (1) rich real-world products from Amazon, (2) a diverse collection of user personas to represent heterogeneous yet latent preferences, and (3) a LLM user simulator powered by the persona to create rich interactive trajectories. We observe that current frontier models struggle to meaningfully improve across episodes, underscoring the need for agentic systems with strong in-context learning capabilities.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Digital Twin for Misspecified Structural Dynamical Systems via Latent Force Modeling and Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2511.22133</link>
<guid>https://arxiv.org/abs/2511.22133</guid>
<content:encoded><![CDATA[
<div> Keywords: probabilistic digital twin, Gaussian Process Latent Force Models, Bayesian Neural Networks, uncertainty quantification, model-form errors

<br><br>Summary: This article introduces a probabilistic digital twin framework designed for response prediction in dynamical systems where the governing physics models are misspecified. The method combines Gaussian Process Latent Force Models (GPLFM) with Bayesian Neural Networks (BNNs) to provide an end-to-end approach that incorporates uncertainty throughout inference and prediction stages. During the diagnosis phase, model-form errors (MFEs) are modeled as latent input forces added to a nominal linear dynamical system and are jointly estimated alongside system states using GPLFM based on sensor data. Subsequently, a BNN is trained on posterior samples to learn a probabilistic nonlinear mapping from system states to MFEs, capturing diagnostic uncertainty effectively. In the prognosis phase, this learned mapping generates pseudo-measurements that feed into Kalman filtering for improved state prediction. This framework ensures systematic propagation of uncertainty from diagnosis to prediction, enhancing the reliability and trustworthiness of digital twins. The effectiveness and robustness of the framework are validated through four nonlinear system examples: a single degree of freedom (DOF) oscillator, a multi-DOF system, the Bouc-Wen hysteretic benchmark, and the Silverbox experimental dataset, demonstrating strong predictive accuracy and robustness to model misspecification. <div>
arXiv:2511.22133v1 Announce Type: new 
Abstract: This work presents a probabilistic digital twin framework for response prediction in dynamical systems governed by misspecified physics. The approach integrates Gaussian Process Latent Force Models (GPLFM) and Bayesian Neural Networks (BNNs) to enable end-to-end uncertainty-aware inference and prediction. In the diagnosis phase, model-form errors (MFEs) are treated as latent input forces to a nominal linear dynamical system and jointly estimated with system states using GPLFM from sensor measurements. A BNN is then trained on posterior samples to learn a probabilistic nonlinear mapping from system states to MFEs, while capturing diagnostic uncertainty. For prognosis, this mapping is used to generate pseudo-measurements, enabling state prediction via Kalman filtering. The framework allows for systematic propagation of uncertainty from diagnosis to prediction, a key capability for trustworthy digital twins. The framework is demonstrated using four nonlinear examples: a single degree of freedom (DOF) oscillator, a multi-DOF system, and two established benchmarks -- the Bouc-Wen hysteretic system and the Silverbox experimental dataset -- highlighting its predictive accuracy and robustness to model misspecification.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices</title>
<link>https://arxiv.org/abs/2511.22138</link>
<guid>https://arxiv.org/abs/2511.22138</guid>
<content:encoded><![CDATA[
<div> Keywords: small language models, agentic tasks, edge devices, hybrid optimization, Direct Preference Optimization (DPO)  

<br><br>Summary:  
This paper investigates the effectiveness of small language models (SLMs) on agentic tasks such as function, tool, and API calling, with a focus on enabling autonomous agents to run on edge devices without cloud dependency. The authors evaluate these models using the Berkeley Function Calling Leaderboard (BFCL) framework, covering categories like simple, multiple, parallel, parallel-multiple, and relevance detection, assessing both live and non-live scenarios including multi-turn evaluations. They explore various parameter-driven optimization strategies including supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL), Direct Preference Optimization (DPO) for preference alignment, and hybrid combinations of these methods. The study reports results for models such as TinyAgent, TinyLlama, Qwen, and xLAM. Additionally, a DPO training pipeline was constructed from AgentBank datasets (e.g., ALFRED), converting SFT data into chosen-rejected pairs via TinyLlama-generated rejected outputs and manual validation. Results show a significant performance gap between medium-sized models (1-3 billion parameters) and ultra-compact models (<1 billion parameters), with medium-sized models reaching up to 65.74% overall accuracy and 55.62% multi-turn accuracy using hybrid optimization. The findings emphasize that hybrid optimization strategies enable SLMs to deliver accurate, efficient, and stable agentic AI capable of running privately and with low latency on edge devices, advancing practical cloud-independent autonomous agents. <div>
arXiv:2511.22138v1 Announce Type: new 
Abstract: This paper investigates the effectiveness of small language models (SLMs) for agentic tasks (function/tool/API calling) with a focus on running agents on edge devices without reliance on cloud infrastructure. We evaluate SLMs using the Berkeley Function Calling Leaderboard (BFCL) framework and describe parameter-driven optimization strategies that include supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL)-based optimization, preference alignment via Direct Preference Optimization (DPO), and hybrid methods. We report results for models including TinyAgent, TinyLlama, Qwen, and xLAM across BFCL categories (simple, multiple, parallel, parallel-multiple, and relevance detection), both in live and non-live settings, and in multi-turn evaluations. We additionally detail a DPO training pipeline constructed from AgentBank data (e.g., ALFRED), including our conversion of SFT data to chosen-rejected pairs using TinyLlama responses as rejected outputs and manual validation. Our results demonstrate clear accuracy differences across model scales where medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters), achieving up to 65.74% overall accuracy, and 55.62% multi-turn accuracy with hybrid optimization. This study highlights the importance of hybrid optimization strategies that enable small language models to deliver accurate, efficient, and stable agentic AI on edge devices, making privacy-preserving, low-latency autonomous agents practical beyond the cloud.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures</title>
<link>https://arxiv.org/abs/2511.22150</link>
<guid>https://arxiv.org/abs/2511.22150</guid>
<content:encoded><![CDATA[
<div> Keywords: embeddings, topology, geometry, text models, retrievability<br><br>Summary:<br><br>1. This paper investigates how text embeddings are organized in space to improve model interpretability and understand factors influencing downstream task performance.<br>2. The authors conduct a comprehensive analysis of various topological and geometric measures across multiple text embedding models and datasets.<br>3. They find a high degree of redundancy among these individual metrics, which often fail to sufficiently distinguish between different embedding spaces.<br>4. To address these limitations, the paper introduces Unified Topological Signatures (UTS), a holistic framework designed to characterize embedding spaces by integrating multiple attributes.<br>5. The study demonstrates that UTS can effectively predict model-specific properties and reveal similarities driven by underlying model architecture.<br>6. Furthermore, UTS is shown to link the topological structure of embeddings with ranking effectiveness and to accurately predict document retrievability.<br>7. Overall, the findings emphasize the necessity of a multi-attribute, holistic approach to fully understand and leverage the geometric and topological properties of text embeddings in practical applications. <div>
arXiv:2511.22150v1 Announce Type: new 
Abstract: Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing Instance-Level Sampling Schedules via REINFORCE with James-Stein Shrinkage</title>
<link>https://arxiv.org/abs/2511.22177</link>
<guid>https://arxiv.org/abs/2511.22177</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image samplers, sampling schedule, Dirichlet policy, James-Stein estimator, Stable Diffusion

<br><br>Summary:  
This paper proposes a novel post-training approach for text-to-image samplers that focuses on rescheduling the sampling timeline rather than modifying model weights. Unlike traditional fixed, global schedules, the authors introduce instance-level schedules conditioned on prompts and noise, learned through a single-pass Dirichlet policy. To achieve accurate gradient estimates in the high-dimensional policy learning setting, they develop a new reward baseline based on a James-Stein estimator, which provably reduces estimation error compared to existing methods, enhancing performance. The rescheduled samplers demonstrate improved text-image alignment, better text rendering, and enhanced compositional control across popular models like Stable Diffusion and Flux. Importantly, a 5-step Flux-Dev sampler using this scheduling framework matches the generation quality of specialized, distilled samplers such as Flux-Schnell, showing efficiency gains without retraining or distillation. The work presents their adaptive scheduling as a model-agnostic, post-training tool that unlocks further generative capabilities in pretrained samplers, offering a practical and scalable alternative to weight fine-tuning or distillation for improving synthesis quality. <div>
arXiv:2511.22177v1 Announce Type: new 
Abstract: Most post-training methods for text-to-image samplers focus on model weights: either fine-tuning the backbone for alignment or distilling it for few-step efficiency. We take a different route: rescheduling the sampling timeline of a frozen sampler. Instead of a fixed, global schedule, we learn instance-level (prompt- and noise-conditioned) schedules through a single-pass Dirichlet policy. To ensure accurate gradient estimates in high-dimensional policy learning, we introduce a novel reward baseline based on a principled James-Stein estimator; it provably achieves lower estimation errors than commonly used variants and leads to superior performance. Our rescheduled samplers consistently improve text-image alignment including text rendering and compositional control across modern Stable Diffusion and Flux model families. Additionally, a 5-step Flux-Dev sampler with our schedules can attain generation quality comparable to deliberately distilled samplers like Flux-Schnell. We thus position our scheduling framework as an emerging model-agnostic post-training lever that unlocks additional generative potential in pretrained samplers.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PULSE-ICU: A Pretrained Unified Long-Sequence Encoder for Multi-task Prediction in Intensive Care Units</title>
<link>https://arxiv.org/abs/2511.22199</link>
<guid>https://arxiv.org/abs/2511.22199</guid>
<content:encoded><![CDATA[
<div> ICU data, self-supervised learning, foundation model, Longformer, clinical prediction  

<br><br>Summary:  
This paper addresses the challenge of modeling intensive care unit (ICU) data, which is inherently irregular, heterogeneous, and temporally fragmented, making clinical prediction difficult to generalize. The authors introduce PULSE-ICU, a self-supervised foundation model designed to learn event-level representations directly from large-scale electronic health record (EHR) sequences without needing data resampling or manual feature engineering. A unified embedding module captures multiple aspects of ICU data, including event identity, continuous values, measurement units, and temporal information. PULSE-ICU employs a Longformer-based encoder, which efficiently processes long time-series trajectories within ICU records. The model was fine-tuned across 18 diverse clinical prediction tasks such as mortality prediction, intervention forecasting, and phenotype identification, demonstrating robust and strong performance across all tasks. Additionally, external validation on datasets like eICU, HiRID, and P12 revealed that PULSE-ICU maintains high accuracy with minimal fine-tuning, indicating strong robustness to domain shifts and varying dataset constraints. These results highlight that foundation-style modeling can enhance data efficiency and adaptability in clinical settings and offers a scalable framework for ICU decision support tools adaptable to multiple clinical environments. <div>
arXiv:2511.22199v1 Announce Type: new 
Abstract: Intensive care unit (ICU) data are highly irregular, heterogeneous, and temporally fragmented, posing challenges for generalizable clinical prediction. We present PULSE-ICU, a self-supervised foundation model that learns event-level ICU representations from large-scale EHR sequences without resampling or manual feature engineering. A unified embedding module encodes event identity, continuous values, units, and temporal attributes, while a Longformer-based encoder enables efficient modeling of long trajectories. PULSE-ICU was fine-tuned across 18 prediction tasks, including mortality, intervention forecasting, and phenotype identification, achieving strong performance across task types. External validation on eICU, HiRID, and P12 showed substantial improvements with minimal fine-tuning, demonstrating robustness to domain shift and variable constraints. These findings suggest that foundation-style modeling can improve data efficiency and adaptability, providing a scalable framework for ICU decision support across diverse clinical environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.22210</link>
<guid>https://arxiv.org/abs/2511.22210</guid>
<content:encoded><![CDATA[
<div> Keywords: Offline Inverse Reinforcement Learning, BiCQL-ML, Conservative Q-Learning, Reward Recovery, Soft-Optimal Policy<br><br>Summary:<br><br>1. The paper addresses the problem of offline inverse reinforcement learning (IRL), which recovers a reward function explaining expert behavior without requiring any online environment interaction, relying solely on fixed demonstration data.<br><br>2. The authors propose BiCQL-ML, a novel policy-free offline IRL algorithm that simultaneously learns a reward function and a conservative Q-function within a bi-level optimization framework, avoiding the need to explicitly learn a policy.<br><br>3. BiCQL-ML iteratively alternates between (i) learning a conservative Q-function through Conservative Q-Learning (CQL) under the current reward estimate, and (ii) updating reward parameters by maximizing the expected Q-values of expert actions while limiting over-generalization to out-of-distribution actions.<br><br>4. The method is interpretable as performing maximum likelihood estimation under a soft value matching principle, and the authors provide theoretical guarantees demonstrating that BiCQL-ML converges to a reward function for which the expert policy is soft-optimal.<br><br>5. Empirical evaluations on standard offline reinforcement learning benchmarks reveal that BiCQL-ML not only better recovers the underlying reward but also improves downstream policy performance, outperforming existing offline IRL baselines. <div>
arXiv:2511.22210v1 Announce Type: new 
Abstract: Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning</title>
<link>https://arxiv.org/abs/2511.22265</link>
<guid>https://arxiv.org/abs/2511.22265</guid>
<content:encoded><![CDATA[

arXiv:2511.22265v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), a framework built upon a novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into a single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train a global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads a single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https://github.com/AIResearch-Group/FedRE.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeCoder: Systematic Exploration and Optimisation of Decoding and Constraints for LLM Code Generation</title>
<link>https://arxiv.org/abs/2511.22277</link>
<guid>https://arxiv.org/abs/2511.22277</guid>
<content:encoded><![CDATA[

arXiv:2511.22277v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable ability to generate code, yet their outputs often violate syntactic or semantic constraints when guided only through natural language prompts. We introduce TreeCoder, the most general and flexible framework to date for exploring decoding strategies, constraints, and hyperparameters in LLMs, and use it in code generation to enforce correctness and structure during decoding rather than relying on prompt engineering. TreeCoder represents decoding as a tree search over candidate programs, where both decoding strategies and constraint functions - such as style, syntax, execution - are treated as first-class, optimisable components. This design enables systematic exploration and automatic tuning of decoding configurations using standard optimisation techniques. Experiments on the MBPP (Python) and SQL-Spider benchmarks show that TreeCoder consistently improves accuracy across open-source models such as CodeLlama, Mistral and DeepSeek, often outperforming their unconstrained baselines by considerable margins.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Hidden Cost of Approximation in Online Mirror Descent</title>
<link>https://arxiv.org/abs/2511.22283</link>
<guid>https://arxiv.org/abs/2511.22283</guid>
<content:encoded><![CDATA[

arXiv:2511.22283v1 Announce Type: new 
Abstract: Online mirror descent (OMD) is a fundamental algorithmic paradigm that underlies many algorithms in optimization, machine learning and sequential decision-making. The OMD iterates are defined as solutions to optimization subproblems which, oftentimes, can be solved only approximately, leading to an inexact version of the algorithm. Nonetheless, existing OMD analyses typically assume an idealized error free setting, thereby limiting our understanding of performance guarantees that should be expected in practice. In this work we initiate a systematic study into inexact OMD, and uncover an intricate relation between regularizer smoothness and robustness to approximation errors. When the regularizer is uniformly smooth, we establish a tight bound on the excess regret due to errors. Then, for barrier regularizers over the simplex and its subsets, we identify a sharp separation: negative entropy requires exponentially small errors to avoid linear regret, whereas log-barrier and Tsallis regularizers remain robust even when the errors are only polynomial. Finally, we show that when the losses are stochastic and the domain is the simplex, negative entropy regains robustness-but this property does not extend to all subsets, where exponentially small errors are again necessary to avoid suboptimal regret.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Dynamic Pricing of Complementary Products</title>
<link>https://arxiv.org/abs/2511.22291</link>
<guid>https://arxiv.org/abs/2511.22291</guid>
<content:encoded><![CDATA[

arXiv:2511.22291v1 Announce Type: new 
Abstract: Traditional pricing paradigms, once dominated by static models and rule-based heuristics, are increasingly being replaced by dynamic, data-driven approaches powered by machine learning algorithms. Despite their growing sophistication, most dynamic pricing algorithms focus on optimizing the price of each product independently, disregarding potential interactions among items. By neglecting these interdependencies in consumer demand across related goods, sellers may fail to capture the full potential of coordinated pricing strategies. In this paper, we address this problem by exploring dynamic pricing mechanisms designed explicitly for complementary products, aiming to exploit their joint demand structure to maximize overall revenue. We present an online learning algorithm considering both positive and negative interactions between products' demands. The algorithm utilizes transaction data to identify advantageous complementary relationships through an integer programming problem between different items, and then optimizes pricing strategies using data-driven and computationally efficient multi-armed bandit solutions based on heteroscedastic Gaussian processes. We validate our solution in a simulated environment, and we demonstrate that our solution improves the revenue w.r.t. a comparable learning algorithm ignoring such interactions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive tumor growth forecasting via neural &amp; universal ODEs</title>
<link>https://arxiv.org/abs/2511.22292</link>
<guid>https://arxiv.org/abs/2511.22292</guid>
<content:encoded><![CDATA[

arXiv:2511.22292v1 Announce Type: new 
Abstract: Forecasting tumor growth is critical for optimizing treatment. Classical growth models such as the Gompertz and Bertalanffy equations capture general tumor dynamics but may fail to adapt to patient-specific variability, particularly with limited data available. In this study, we leverage Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs), two pillars of Scientific Machine Learning (SciML), to construct adaptive tumor growth models capable of learning from experimental data. Using the Gompertz model as a baseline, we replace rigid terms with adaptive neural networks to capture hidden dynamics through robust modeling in the Julia programming language. We use our models to perform forecasting under data constraints and symbolic recovery to transform the learned dynamics into explicit mathematical expressions. Our approach has the potential to improve predictive accuracy, guiding dynamic and effective treatment strategies for improved clinical outcomes.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLUX: Efficient Descriptor-Driven Clustered Federated Learning under Arbitrary Distribution Shifts</title>
<link>https://arxiv.org/abs/2511.22305</link>
<guid>https://arxiv.org/abs/2511.22305</guid>
<content:encoded><![CDATA[

arXiv:2511.22305v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across multiple clients while preserving data privacy. Traditional FL methods often use a global model to fit all clients, assuming that clients' data are independent and identically distributed (IID). However, when this assumption does not hold, the global model accuracy may drop significantly, limiting FL applicability in real-world scenarios. To address this gap, we propose FLUX, a novel clustering-based FL (CFL) framework that addresses the four most common types of distribution shifts during both training and test time. To this end, FLUX leverages privacy-preserving client-side descriptor extraction and unsupervised clustering to ensure robust performance and scalability across varying levels and types of distribution shifts. Unlike existing CFL methods addressing non-IID client distribution shifts, FLUX i) does not require any prior knowledge of the types of distribution shifts or the number of client clusters, and ii) supports test-time adaptation, enabling unseen and unlabeled clients to benefit from the most suitable cluster-specific models. Extensive experiments across four standard benchmarks, two real-world datasets and ten state-of-the-art baselines show that FLUX improves performance and stability under diverse distribution shifts, achieving an average accuracy gain of up to 23 percentage points over the best-performing baselines, while maintaining computational and communication overhead comparable to FedAvg.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeXposure: A Dataset and Benchmarks for Inter-protocol Credit Exposure in Decentralized Financial Networks</title>
<link>https://arxiv.org/abs/2511.22314</link>
<guid>https://arxiv.org/abs/2511.22314</guid>
<content:encoded><![CDATA[

arXiv:2511.22314v1 Announce Type: new 
Abstract: We curate the DeXposure dataset, the first large-scale dataset for inter-protocol credit exposure in decentralized financial networks, covering global markets of 43.7 million entries across 4.3 thousand protocols, 602 blockchains, and 24.3 thousand tokens, from 2020 to 2025. A new measure, value-linked credit exposure between protocols, is defined as the inferred financial dependency relationships derived from changes in Total Value Locked (TVL). We develop a token-to-protocol model using DefiLlama metadata to infer inter-protocol credit exposure from the token's stock dynamics, as reported by the protocols. Based on the curated dataset, we develop three benchmarks for machine learning research with financial applications: (1) graph clustering for global network measurement, tracking the structural evolution of credit exposure networks, (2) vector autoregression for sector-level credit exposure dynamics during major shocks (Terra and FTX), and (3) temporal graph neural networks for dynamic link prediction on temporal graphs. From the analysis, we observe (1) a rapid growth of network volume, (2) a trend of concentration to key protocols, (3) a decline of network density (the ratio of actual connections to possible connections), and (4) distinct shock propagation across sectors, such as lending platforms, trading exchanges, and asset management protocols. The DeXposure dataset and code have been released publicly. We envision they will help with research and practice in machine learning as well as financial risk monitoring, policy analysis, DeFi market modeling, amongst others. The dataset also contributes to machine learning research by offering benchmarks for graph clustering, vector autoregression, and temporal graph analysis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SingleQuant: Efficient Quantization of Large Language Models in a Single Pass</title>
<link>https://arxiv.org/abs/2511.22316</link>
<guid>https://arxiv.org/abs/2511.22316</guid>
<content:encoded><![CDATA[

arXiv:2511.22316v1 Announce Type: new 
Abstract: Large Language Models (LLMs) quantization facilitates deploying LLMs in resource-limited settings, but existing methods that combine incompatible gradient optimization and quantization truncation lead to serious convergence pathology. This prolongs quantization time and degrades LLMs' task performance. Our studies confirm that Straight-Through Estimator (STE) on Stiefel manifolds introduce non-smoothness and gradient noise, obstructing optimization convergence and blocking high-fidelity quantized LLM development despite extensive training. To tackle the above limitations, we propose SingleQuant, a single-pass quantization framework that decouples from quantization truncation, thereby eliminating the above non-smoothness and gradient noise factors. Specifically, SingleQuant constructs Alignment Rotation Transformation (ART) and Uniformity Rotation Transformation (URT) targeting distinct activation outliers, where ART achieves smoothing of outlier values via closed-form optimal rotations, and URT reshapes distributions through geometric mapping. Both matrices comprise strictly formulated Givens rotations with predetermined dimensions and rotation angles, enabling promising LLMs task performance within a short time. Experimental results demonstrate SingleQuant's superiority over the selected baselines across diverse tasks on 7B-70B LLMs. To be more precise, SingleQuant enables quantized LLMs to achieve higher task performance while necessitating less time for quantization. For example, when quantizing LLaMA-2-13B, SingleQuant achieves 1,400$\times$ quantization speedup and increases +0.57\% average task performance compared to the selected best baseline.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test Time Training for AC Power Flow Surrogates via Physics and Operational Constraint Refinement</title>
<link>https://arxiv.org/abs/2511.22343</link>
<guid>https://arxiv.org/abs/2511.22343</guid>
<content:encoded><![CDATA[

arXiv:2511.22343v1 Announce Type: new 
Abstract: Power Flow (PF) calculation based on machine learning (ML) techniques offer significant computational advantages over traditional numerical methods but often struggle to maintain full physical consistency. This paper introduces a physics-informed test-time training (PI-TTT) framework that enhances the accuracy and feasibility of ML-based PF surrogates by enforcing AC power flow equalities and operational constraints directly at inference time. The proposed method performs a lightweight self-supervised refinement of the surrogate outputs through few gradient-based updates, enabling local adaptation to unseen operating conditions without requiring labeled data. Extensive experiments on the IEEE 14-, 118-, and 300-bus systems and the PEGASE 1354-bus network show that PI-TTT reduces power flow residuals and operational constraint violations by one to two orders of magnitude compared with purely ML-based models, while preserving their computational advantage. The results demonstrate that PI-TTT provides fast, accurate, and physically reliable predictions, representing a promising direction for scalable and physics-consistent learning in power system analysis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cleaning the Pool: Progressive Filtering of Unlabeled Pools in Deep Active Learning</title>
<link>https://arxiv.org/abs/2511.22344</link>
<guid>https://arxiv.org/abs/2511.22344</guid>
<content:encoded><![CDATA[

arXiv:2511.22344v1 Announce Type: new 
Abstract: Existing active learning (AL) strategies capture fundamentally different notions of data value, e.g., uncertainty or representativeness. Consequently, the effectiveness of strategies can vary substantially across datasets, models, and even AL cycles. Committing to a single strategy risks suboptimal performance, as no single strategy dominates throughout the entire AL process. We introduce REFINE, an ensemble AL method that combines multiple strategies without knowing in advance which will perform best. In each AL cycle, REFINE operates in two stages: (1) Progressive filtering iteratively refines the unlabeled pool by considering an ensemble of AL strategies, retaining promising candidates capturing different notions of value. (2) Coverage-based selection then chooses a final batch from this refined pool, ensuring all previously identified notions of value are accounted for. Extensive experiments across 6 classification datasets and 3 foundation models show that REFINE consistently outperforms individual strategies and existing ensemble methods. Notably, progressive filtering serves as a powerful preprocessing step that improves the performance of any individual AL strategy applied to the refined pool, which we demonstrate on an audio spectrogram classification use case. Finally, the ensemble of REFINE can be easily extended with upcoming state-of-the-art AL strategies.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoTailor: Automatic and Efficient Adaptive Model Deployment for Diverse Edge Devices</title>
<link>https://arxiv.org/abs/2511.22355</link>
<guid>https://arxiv.org/abs/2511.22355</guid>
<content:encoded><![CDATA[

arXiv:2511.22355v1 Announce Type: new 
Abstract: On-device machine learning (ML) has become a fundamental component of emerging mobile applications. Adaptive model deployment delivers efficient inference for heterogeneous device capabilities and performance requirements through customizing neural architectures. SuperNet-based approaches offer a promising solution by generating a large number of model variants from a pre-trained ML model. However, applying SuperNet in existing frameworks suffers from tedious model-aware development and time-consuming hardware-aware profiling, which limits their practical adoption.
  We present AutoTailor, the first framework to enable automated, end-to-end SuperNet-based adaptive model deployment for edge devices. Unlike manual SuperNet construction, AutoTailor employs a computation graph-guided compilation approach to automatically transform user-provided ML models into SuperNets. To support efficient specialization, AutoTailor incorporates learning-free latency and accuracy predictors, enabling low-cost yet accurate performance prediction. Our extended evaluations demonstrate that AutoTailor reduces the lines of code for SuperNet construction by 11--27$\times$, decreases hardware-aware profiling costs by at least 11$\times$, and achieves up to 15.60\% absolute accuracy improvement and 60.03\% latency reduction compared to state-of-the-art approaches across diverse models and devices.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient-Husformer: Efficient Multimodal Transformer Hyperparameter Optimization for Stress and Cognitive Loads</title>
<link>https://arxiv.org/abs/2511.22362</link>
<guid>https://arxiv.org/abs/2511.22362</guid>
<content:encoded><![CDATA[

arXiv:2511.22362v1 Announce Type: new 
Abstract: Transformer-based models have gained considerable attention in the field of physiological signal analysis. They leverage long-range dependencies and complex patterns in temporal signals, allowing them to achieve performance superior to traditional RNN and CNN models. However, they require high computational intensity and memory demands. In this work, we present Efficient-Husformer, a novel Transformer-based architecture developed with hyperparameter optimization (HPO) for multi-class stress detection across two multimodal physiological datasets (WESAD and CogLoad). The main contributions of this work are: (1) the design of a structured search space, targeting effective hyperparameter optimization; (2) a comprehensive ablation study evaluating the impact of architectural decisions; (3) consistent performance improvements over the original Husformer, with the best configuration achieving an accuracy of 88.41 and 92.61 (improvements of 13.83% and 6.98%) on WESAD and CogLoad datasets, respectively. The best-performing configuration is achieved with the (L + dm) or (L + FFN) modality combinations, using a single layer, 3 attention heads, a model dimension of 18/30, and FFN dimension of 120/30, resulting in a compact model with only about 30k parameters.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning</title>
<link>https://arxiv.org/abs/2511.22367</link>
<guid>https://arxiv.org/abs/2511.22367</guid>
<content:encoded><![CDATA[

arXiv:2511.22367v1 Announce Type: new 
Abstract: Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting and Interpolating Spatiotemporal Environmental Data: A Case Study of Groundwater Storage in Bangladesh</title>
<link>https://arxiv.org/abs/2511.22378</link>
<guid>https://arxiv.org/abs/2511.22378</guid>
<content:encoded><![CDATA[

arXiv:2511.22378v1 Announce Type: new 
Abstract: Geospatial observational datasets are often limited to point measurements, making temporal prediction and spatial interpolation essential for constructing continuous fields. This study evaluates two deep learning strategies for addressing this challenge: (1) a grid-to-grid approach, where gridded predictors are used to model rasterised targets (aggregation before modelling), and (2) a grid-to-point approach, where gridded predictors model point targets, followed by kriging interpolation to fill the domain (aggregation after modelling). Using groundwater storage data from Bangladesh as a case study, we compare the effcacy of these approaches. Our findings indicate that spatial interpolation is substantially more difficult than temporal prediction. In particular, nearest neighbours are not always the most similar, and uncertainties in geology strongly influence point temporal behaviour. These insights motivate future work on advanced interpolation methods informed by clustering locations based on time series dynamics. Demonstrated on groundwater storage, the conclusions are applicable to other environmental variables governed by indirectly observable factors. Code is available at https://github.com/pazolka/interpolation-prediction-gwsa.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS2Vec-Ensemble: An Enhanced Self-Supervised Framework for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.22395</link>
<guid>https://arxiv.org/abs/2511.22395</guid>
<content:encoded><![CDATA[

arXiv:2511.22395v1 Announce Type: new 
Abstract: Self-supervised representation learning, particularly through contrastive methods like TS2Vec, has advanced the analysis of time series data. However, these models often falter in forecasting tasks because their objective functions prioritize instance discrimination over capturing the deterministic patterns, such as seasonality and trend, that are critical for accurate prediction. This paper introduces TS2Vec-Ensemble, a novel hybrid framework designed to bridge this gap. Our approach enhances the powerful, implicitly learned dynamics from a pretrained TS2Vec encoder by fusing them with explicit, engineered time features that encode periodic cycles. This fusion is achieved through a dual-model ensemble architecture, where two distinct regression heads -- one focused on learned dynamics and the other on seasonal patterns -- are combined using an adaptive weighting scheme. The ensemble weights are optimized independently for each forecast horizon, allowing the model to dynamically prioritize short-term dynamics or long-term seasonality as needed. We conduct extensive experiments on the ETT benchmark datasets for both univariate and multivariate forecasting. The results demonstrate that TS2Vec-Ensemble consistently and significantly outperforms the standard TS2Vec baseline and other state-of-the-art models, validating our hypothesis that a hybrid of learned representations and explicit temporal priors is a superior strategy for long-horizon time series forecasting.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions</title>
<link>https://arxiv.org/abs/2511.22406</link>
<guid>https://arxiv.org/abs/2511.22406</guid>
<content:encoded><![CDATA[

arXiv:2511.22406v1 Announce Type: new 
Abstract: In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PISA: Prioritized Invariant Subgraph Aggregation</title>
<link>https://arxiv.org/abs/2511.22435</link>
<guid>https://arxiv.org/abs/2511.22435</guid>
<content:encoded><![CDATA[

arXiv:2511.22435v1 Announce Type: new 
Abstract: Recent work has extended the invariance principle for out-of-distribution (OOD) generalization from Euclidean to graph data, where challenges arise due to complex structures and diverse distribution shifts in node attributes and topology. To handle these, Chen et al. proposed CIGA (Chen et al., 2022b), which uses causal modeling and an information-theoretic objective to extract a single invariant subgraph capturing causal features. However, this single-subgraph focus can miss multiple causal patterns. Liu et al. (2025) addressed this with SuGAr, which learns and aggregates diverse invariant subgraphs via a sampler and diversity regularizer, improving robustness but still relying on simple uniform or greedy aggregation. To overcome this, the proposed PISA framework introduces a dynamic MLP-based aggregation that prioritizes and combines subgraph representations more effectively. Experiments on 15 datasets, including DrugOOD (Ji et al., 2023), show that PISA achieves up to 5% higher classification accuracy than prior methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction</title>
<link>https://arxiv.org/abs/2511.22460</link>
<guid>https://arxiv.org/abs/2511.22460</guid>
<content:encoded><![CDATA[

arXiv:2511.22460v1 Announce Type: new 
Abstract: In large-scale advertising recommendation systems, retrieval serves as a critical component, aiming to efficiently select a subset of candidate ads relevant to user behaviors from a massive ad inventory for subsequent ranking and recommendation. The Embedding-Based Retrieval (EBR) methods modeled by the dual-tower network are widely used in the industry to maintain both retrieval efficiency and accuracy. However, the dual-tower model has significant limitations: the embeddings of users and ads interact only at the final inner product computation, resulting in insufficient feature interaction capabilities. Although DNN-based models with both user and ad as input features, allowing for early-stage interaction between these features, are introduced in the ranking stage to mitigate this issue, they are computationally infeasible for the retrieval stage. To bridge this gap, this paper proposes an efficient GPU-based feature interaction for the dual-tower network to significantly improve retrieval accuracy while substantially reducing computational costs. Specifically, we introduce a novel compressed inverted list designed for GPU acceleration, enabling efficient feature interaction computation at scale. To the best of our knowledge, this is the first framework in the industry to successfully implement Wide and Deep in a retrieval system. We apply this model to the real-world business scenarios in Tencent Advertising, and experimental results demonstrate that our method outperforms existing approaches in offline evaluation and has been successfully deployed to Tencent's advertising recommendation system, delivering significant online performance gains. This improvement not only validates the effectiveness of the proposed method, but also provides new practical guidance for optimizing large-scale ad retrieval systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Flow Models</title>
<link>https://arxiv.org/abs/2511.22475</link>
<guid>https://arxiv.org/abs/2511.22475</guid>
<content:encoded><![CDATA[

arXiv:2511.22475v1 Announce Type: new 
Abstract: We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges</title>
<link>https://arxiv.org/abs/2511.22483</link>
<guid>https://arxiv.org/abs/2511.22483</guid>
<content:encoded><![CDATA[

arXiv:2511.22483v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Space Explanations of Neural Network Classification</title>
<link>https://arxiv.org/abs/2511.22498</link>
<guid>https://arxiv.org/abs/2511.22498</guid>
<content:encoded><![CDATA[

arXiv:2511.22498v1 Announce Type: new 
Abstract: We present a novel logic-based concept called Space Explanations for classifying neural networks that gives provable guarantees of the behavior of the network in continuous areas of the input feature space. To automatically generate space explanations, we leverage a range of flexible Craig interpolation algorithms and unsatisfiable core generation. Based on real-life case studies, ranging from small to medium to large size, we demonstrate that the generated explanations are more meaningful than those computed by state-of-the-art.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Utility-Bias Trade-offs for Privacy-Preserving Recommender Systems</title>
<link>https://arxiv.org/abs/2511.22515</link>
<guid>https://arxiv.org/abs/2511.22515</guid>
<content:encoded><![CDATA[

arXiv:2511.22515v1 Announce Type: new 
Abstract: Recommender systems (RSs) output ranked lists of items, such as movies or restaurants, that users may find interesting, based on the user's past ratings and ratings from other users. RSs increasingly incorporate differential privacy (DP) to protect user data, raising questions about how privacy mechanisms affect both recommendation accuracy and fairness. We conduct a comprehensive, cross-model evaluation of two DP mechanisms, differentially private stochastic gradient descent (DPSGD) and local differential privacy (LDP), applied to four recommender systems (Neural Collaborative Filtering (NCF), Bayesian Personalized Ranking (BPR), Singular Value Decomposition (SVD), and Variational Autoencoder (VAE)) on the MovieLens-1M and Yelp datasets. We find that stronger privacy consistently reduces utility, but not uniformly. NCF under DPSGD shows the smallest accuracy loss (under 10 percent at epsilon approximately 1), whereas SVD and BPR experience larger drops, especially for users with niche preferences. VAE is the most sensitive to privacy, with sharp declines for sparsely represented groups. The impact on bias metrics is similarly heterogeneous. DPSGD generally reduces the gap between recommendations of popular and less popular items, whereas LDP preserves existing patterns more closely. These results highlight that no single DP mechanism is uniformly superior; instead, each provides trade-offs under different privacy regimes and data conditions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>List-Decodable Regression via Expander Sketching</title>
<link>https://arxiv.org/abs/2511.22524</link>
<guid>https://arxiv.org/abs/2511.22524</guid>
<content:encoded><![CDATA[

arXiv:2511.22524v1 Announce Type: new 
Abstract: We introduce an expander-sketching framework for list-decodable linear regression that achieves sample complexity $\tilde{O}((d+\log(1/\delta))/\alpha)$, list size $O(1/\alpha)$, and near input-sparsity running time $\tilde{O}(\mathrm{nnz}(X)+d^{3}/\alpha)$ under standard sub-Gaussian assumptions. Our method uses lossless expanders to synthesize lightly contaminated batches, enabling robust aggregation and a short spectral filtering stage that matches the best known efficient guarantees while avoiding SoS machinery and explicit batch structure.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where to Measure: Epistemic Uncertainty-Based Sensor Placement with ConvCNPs</title>
<link>https://arxiv.org/abs/2511.22567</link>
<guid>https://arxiv.org/abs/2511.22567</guid>
<content:encoded><![CDATA[

arXiv:2511.22567v1 Announce Type: new 
Abstract: Accurate sensor placement is critical for modeling spatio-temporal systems such as environmental and climate processes. Neural Processes (NPs), particularly Convolutional Conditional Neural Processes (ConvCNPs), provide scalable probabilistic models with uncertainty estimates, making them well-suited for data-driven sensor placement. However, existing approaches rely on total predictive uncertainty, which conflates epistemic and aleatoric components, that may lead to suboptimal sensor selection in ambiguous regions. To address this, we propose expected reduction in epistemic uncertainty as a new acquisition function for sensor placement. To enable this, we extend ConvCNPs with a Mixture Density Networks (MDNs) output head for epistemic uncertainty estimation. Preliminary results suggest that epistemic uncertainty driven sensor placement more effectively reduces model error than approaches based on overall uncertainty.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy is all you need for Inter-Seed Cross-Play in Hanabi</title>
<link>https://arxiv.org/abs/2511.22581</link>
<guid>https://arxiv.org/abs/2511.22581</guid>
<content:encoded><![CDATA[

arXiv:2511.22581v1 Announce Type: new 
Abstract: We find that in Hanabi, one of the most complex and popular benchmarks for zero-shot coordination and ad-hoc teamplay, a standard implementation of independent PPO with a slightly higher entropy coefficient 0.05 instead of the typically used 0.01, achieves a new state-of-the-art in cross-play between different seeds, beating by a significant margin all previous specialized algorithms, which were specifically designed for this setting. We provide an intuition for why sufficiently high entropy regularization ensures that different random seed produce joint policies which are mutually compatible. We also empirically find that a high $\lambda_{\text{GAE}}$ around 0.9, and using RNNs instead of just feed-forward layers in the actor-critic architecture, strongly increase inter-seed cross-play. While these results demonstrate the dramatic effect that hyperparameters can have not just on self-play scores but also on cross-play scores, we show that there are simple Dec-POMDPs though, in which standard policy gradient methods with increased entropy regularization are not able to achieve perfect inter-seed cross-play, thus demonstrating the continuing necessity for new algorithms for zero-shot coordination.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Multiclass Score-Oriented Loss (MultiSOL) on the Simplex</title>
<link>https://arxiv.org/abs/2511.22587</link>
<guid>https://arxiv.org/abs/2511.22587</guid>
<content:encoded><![CDATA[

arXiv:2511.22587v1 Announce Type: new 
Abstract: In the supervised binary classification setting, score-oriented losses have been introduced with the aim of optimizing a chosen performance metric directly during the training phase, thus avoiding \textit{a posteriori} threshold tuning. To do this, in their construction, the decision threshold is treated as a random variable provided with a certain \textit{a priori} distribution. In this paper, we use a recently introduced multidimensional threshold-based classification framework to extend such score-oriented losses to multiclass classification, defining the Multiclass Score-Oriented Loss (MultiSOL) functions. As also demonstrated by several classification experiments, this proposed family of losses is designed to preserve the main advantages observed in the binary setting, such as the direct optimization of the target metric and the robustness to class imbalance, achieving performance comparable to other state-of-the-art loss functions and providing new insights into the interaction between simplex geometry and score-oriented learning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system</title>
<link>https://arxiv.org/abs/2511.22598</link>
<guid>https://arxiv.org/abs/2511.22598</guid>
<content:encoded><![CDATA[

arXiv:2511.22598v1 Announce Type: new 
Abstract: Large language models (LLMs) such as ChatGPT o1, ChatGPT o3, and DeepSeek R1 have shown great potential in solving difficult problems. However, current LLM evaluation benchmarks are limited to one-step interactions. Some of the existing sequence decision-making environments, such as TextStarCraftII and LLM-PySC2, are too complicated and require hours of interaction to complete a game. In this paper, we introduce LLM-Cave, a benchmark and light environment for LLM reasoning and decision-making systems. This environment is a classic instance in the era of Symbolism. Artificial intelligence enables the agent to explore the environment and avoid potential losses by reasoning about nearby dangers using partial observable state information. In the experiment, we evaluated the sequential reasoning ability, decision-making performance and computational efficiency of mainstream large language models (LLMs) such as GPT-4o-mini, o1-mini, and DeepSeek-R1. Experiments show that while Deepseek-R1 achieved the highest success rate on complex reasoning tasks, smaller models like 4o-mini significantly narrowed the performance gap on challenges by employing Chain of Speculation and Planner-Critic strategies, at the expense of reduced computational efficiency. This indicates that structured, multi-step reasoning combined with an LLM-based feedback mechanism can substantially enhance an LLM's decision-making capabilities, providing a promising direction for improving reasoning in weaker models and suggesting a new reasoning-centered benchmark for LLM assessment. Our code is open-sourced in https://github.com/puleya1277/CaveEnv.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers</title>
<link>https://arxiv.org/abs/2511.22616</link>
<guid>https://arxiv.org/abs/2511.22616</guid>
<content:encoded><![CDATA[

arXiv:2511.22616v1 Announce Type: new 
Abstract: The integration of IoT and AI has unlocked innovation across industries, but growing privacy concerns and data isolation hinder progress. Traditional centralized ML struggles to overcome these challenges, which has led to the rise of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing local raw data. FL ensures data privacy, reduces communication overhead, and supports scalability, yet its heterogeneity adds complexity compared to centralized approaches. This survey focuses on three main FL research directions: personalization, optimization, and robustness, offering a structured classification through a hybrid methodology that combines bibliometric analysis with systematic review to identify the most influential works. We examine challenges and techniques related to heterogeneity, efficiency, security, and privacy, and provide a comprehensive overview of aggregation strategies, including architectures, synchronization methods, and diverse federation objectives. To complement this, we discuss practical evaluation approaches and present experiments comparing aggregation methods under IID and non-IID data distributions. Finally, we outline promising research directions to advance FL, aiming to guide future innovation in this rapidly evolving field.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow Density Control: Generative Optimization Beyond Entropy-Regularized Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.22640</link>
<guid>https://arxiv.org/abs/2511.22640</guid>
<content:encoded><![CDATA[

arXiv:2511.22640v1 Announce Type: new 
Abstract: Adapting large-scale foundation flow and diffusion generative models to optimize task-specific objectives while preserving prior information is crucial for real-world applications such as molecular design, protein docking, and creative image generation. Existing principled fine-tuning methods aim to maximize the expected reward of generated samples, while retaining knowledge from the pre-trained model via KL-divergence regularization. In this work, we tackle the significantly more general problem of optimizing general utilities beyond average rewards, including risk-averse and novelty-seeking reward maximization, diversity measures for exploration, and experiment design objectives among others. Likewise, we consider more general ways to preserve prior information beyond KL-divergence, such as optimal transport distances and Renyi divergences. To this end, we introduce Flow Density Control (FDC), a simple algorithm that reduces this complex problem to a specific sequence of simpler fine-tuning tasks, each solvable via scalable established methods. We derive convergence guarantees for the proposed scheme under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we validate our method on illustrative settings, text-to-image, and molecular design tasks, showing that it can steer pre-trained generative models to optimize objectives and solve practically relevant tasks beyond the reach of current fine-tuning schemes.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatially Aware Dictionary-Free Eigenfunction Identification for Modeling and Control of Nonlinear Dynamical Systems</title>
<link>https://arxiv.org/abs/2511.22648</link>
<guid>https://arxiv.org/abs/2511.22648</guid>
<content:encoded><![CDATA[

arXiv:2511.22648v1 Announce Type: new 
Abstract: A new approach to data-driven discovery of Koopman eigenfunctions without a pre-defined set of basis functions is proposed. The approach is based on a reference trajectory, for which the Koopman mode amplitudes are first identified, and the Koopman mode decomposition is transformed to a new basis, which contains fundamental functions of eigenvalues and time. The initial values of the eigenfunctions are obtained by projecting trajectories onto this basis via a regularized least-squares fit. A global optimizer was employed to optimize the eigenvalues. Mapping initial-state values to eigenfunction values reveals their spatial structure, enabling the numerical computation of their gradients. Thus, deviations from the Koopman partial differential equation are penalized, leading to more robust solutions. The approach was successfully tested on several benchmark nonlinear dynamical systems, including the FitzHugh-Nagumo system with inputs, van der Pol and Duffing oscillators, and a 2-spool turbojet engine with control. The study demonstrates that incorporating principal eigenvalues and spatial structure integrity promotion significantly improves the accuracy of Koopman predictors. The approach effectively discovers Koopman spectral components even with sparse state-space sampling and reveals geometric features of the state space, such as invariant partitions. Finally, the numerical approximation of the eigenfunction gradient can be used for input dynamics modeling and control design. The results support the practicality of the approach for use with various dynamical systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Design Optimization via Strategic Search with Large Language Models</title>
<link>https://arxiv.org/abs/2511.22651</link>
<guid>https://arxiv.org/abs/2511.22651</guid>
<content:encoded><![CDATA[

arXiv:2511.22651v1 Announce Type: new 
Abstract: Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \$159 per run, compared to an estimated cost of up to \$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-aware Hybrid-order Similarity Learning for Multi-view Unsupervised Feature Selection</title>
<link>https://arxiv.org/abs/2511.22656</link>
<guid>https://arxiv.org/abs/2511.22656</guid>
<content:encoded><![CDATA[

arXiv:2511.22656v1 Announce Type: new 
Abstract: Multi-view unsupervised feature selection (MUFS) has recently emerged as an effective dimensionality reduction method for unlabeled multi-view data. However, most existing methods mainly use first-order similarity graphs to preserve local structure, often overlooking the global structure that can be captured by second-order similarity. In addition, a few MUFS methods leverage predefined second-order similarity graphs, making them vulnerable to noise and outliers and resulting in suboptimal feature selection performance. In this paper, we propose a novel MUFS method, termed Structure-aware Hybrid-order sImilarity learNing for multi-viEw unsupervised Feature Selection (SHINE-FS), to address the aforementioned problem. SHINE-FS first learns consensus anchors and the corresponding anchor graph to capture the cross-view relationships between the anchors and the samples. Based on the acquired cross-view consensus information, it generates low-dimensional representations of the samples, which facilitate the reconstruction of multi-view data by identifying discriminative features. Subsequently, it employs the anchor-sample relationships to learn a second-order similarity graph. Furthermore, by jointly learning first-order and second-order similarity graphs, SHINE-FS constructs a hybrid-order similarity graph that captures both local and global structures, thereby revealing the intrinsic data structure to enhance feature selection. Comprehensive experimental results on real multi-view datasets show that SHINE-FS outperforms the state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Difficulties with Evaluating a Deception Detector for AIs</title>
<link>https://arxiv.org/abs/2511.22662</link>
<guid>https://arxiv.org/abs/2511.22662</guid>
<content:encoded><![CDATA[

arXiv:2511.22662v1 Announce Type: new 
Abstract: Building reliable deception detectors for AI systems -- methods that could predict when an AI system is being strategically deceptive without necessarily requiring behavioural evidence -- would be valuable in mitigating risks from advanced AI systems. But evaluating the reliability and efficacy of a proposed deception detector requires examples that we can confidently label as either deceptive or honest. We argue that we currently lack the necessary examples and further identify several concrete obstacles in collecting them. We provide evidence from conceptual arguments, analysis of existing empirical works, and analysis of novel illustrative case studies. We also discuss the potential of several proposed empirical workarounds to these problems and argue that while they seem valuable, they also seem insufficient alone. Progress on deception detection likely requires further consideration of these problems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mod\`eles de Fondation et Ajustement : Vers une Nouvelle G\'en\'eration de Mod\`eles pour la Pr\'evision des S\'eries Temporelles</title>
<link>https://arxiv.org/abs/2511.22674</link>
<guid>https://arxiv.org/abs/2511.22674</guid>
<content:encoded><![CDATA[

arXiv:2511.22674v1 Announce Type: new 
Abstract: Inspired by recent advances in large language models, foundation models have been developed for zero-shot time series forecasting, enabling prediction on datasets unseen during pretraining. These large-scale models, trained on vast collections of time series, learn generalizable representations for both point and probabilistic forecasting, reducing the need for task-specific architectures and manual tuning.
  In this work, we review the main architectures, pretraining strategies, and optimization methods used in such models, and study the effect of fine-tuning after pretraining to enhance their performance on specific datasets. Our empirical results show that fine-tuning generally improves zero-shot forecasting capabilities, especially for long-term horizons.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-time scaling of diffusions with flow maps</title>
<link>https://arxiv.org/abs/2511.22688</link>
<guid>https://arxiv.org/abs/2511.22688</guid>
<content:encoded><![CDATA[

arXiv:2511.22688v1 Announce Type: new 
Abstract: A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Anchored Fields: Controlled Data Generation via Emergent Velocity Fields and Transport Algebra</title>
<link>https://arxiv.org/abs/2511.22693</link>
<guid>https://arxiv.org/abs/2511.22693</guid>
<content:encoded><![CDATA[

arXiv:2511.22693v1 Announce Type: new 
Abstract: We present Generative Anchored Fields (GAF), a generative model that learns independent endpoint predictors $J$ (noise) and $K$ (data) rather than a trajectory predictor. The velocity field $v=K-J$ emerges from their time-conditioned disagreement. This factorization enables \textit{Transport Algebra}: algebraic operation on learned $\{(J_n,K_n)\}_{n=1}^N$ heads for compositional control. With class-specific $K_n$ heads, GAF supports a rich family of directed transport maps between a shared base distribution and multiple modalities, enabling controllable interpolation, hybrid generation, and semantic morphing through vector arithmetic. We achieve strong sample quality (FID 7.5 on CelebA-HQ $64\times 64$) while uniquely providing compositional generation as an architectural primitive. We further demonstrate, GAF has lossless cyclic transport between its initial and final state with LPIPS=$0.0$. Code available at https://github.com/IDLabMedia/GAF
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrated Transcriptomic-proteomic Biomarker Identification for Radiation Response Prediction in Non-small Cell Lung Cancer Cell Lines</title>
<link>https://arxiv.org/abs/2511.22735</link>
<guid>https://arxiv.org/abs/2511.22735</guid>
<content:encoded><![CDATA[

arXiv:2511.22735v1 Announce Type: new 
Abstract: To develop an integrated transcriptome-proteome framework for identifying concurrent biomarkers predictive of radiation response, as measured by survival fraction at 2 Gy (SF2), in non-small cell lung cancer (NSCLC) cell lines. RNA sequencing (RNA-seq) and data-independent acquisition mass spectrometry (DIA-MS) proteomic data were collected from 73 and 46 NSCLC cell lines, respectively. Following preprocessing, 1,605 shared genes were retained for analysis. Feature selection was performed using least absolute shrinkage and selection operator (Lasso) regression with a frequency-based ranking criterion under five-fold cross-validation repeated ten times. Support vector regression (SVR) models were constructed using transcriptome-only, proteome-only, and combined transcriptome-proteome feature sets. Model performance was assessed by the coefficient of determination (R2) and root mean square error (RMSE). Correlation analyses evaluated concordance between RNA and protein expression and the relationships of selected biomarkers with SF2. RNA-protein expression exhibited significant positive correlations (median Pearson's r = 0.363). Independent pipelines identified 20 prioritized gene signatures from transcriptomic, proteomic, and combined datasets. Models trained on single-omic features achieved limited cross-omic generalizability, while the combined model demonstrated balanced predictive accuracy in both datasets (R2=0.461, RMSE=0.120 for transcriptome; R2=0.604, RMSE=0.111 for proteome). This study presents the first proteotranscriptomic framework for SF2 prediction in NSCLC, highlighting the complementary value of integrating transcriptomic and proteomic data. The identified concurrent biomarkers capture both transcriptional regulation and functional protein activity, offering mechanistic insights and translational potential.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction for RTL Generation Optimization</title>
<link>https://arxiv.org/abs/2511.22749</link>
<guid>https://arxiv.org/abs/2511.22749</guid>
<content:encoded><![CDATA[

arXiv:2511.22749v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show strong performance in RTL generation, but different models excel on different tasks because of architecture and training differences. Prior work mainly prompts or finetunes a single model. What remains not well studied is how to coordinate multiple different LLMs so they jointly improve RTL quality while also reducing cost, instead of running all models and choosing the best output. We define this as the multi-LLM RTL generation problem. We propose VeriDispatcher, a multi-LLM RTL generation framework that dispatches each RTL task to suitable LLMs based on pre-inference difficulty prediction. For each model, we train a compact classifier over semantic embeddings of task descriptions, using difficulty scores derived from benchmark variants that combine syntax, structural similarity, and functional correctness. At inference, VeriDispatcher uses these predictors to route tasks to a selected subset of LLMs. Across 10 diverse LLMs on RTLLM and VerilogEval, VeriDispatcher achieves up to 18% accuracy improvement on RTLLM using only 40% of commercial calls, and on VerilogEval maintains accuracy while reducing commercial usage by 25%, enabling cost-effective, high-quality LLM deployment in hardware design automation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Learning of Arithmetic with Differentiable Agents</title>
<link>https://arxiv.org/abs/2511.22751</link>
<guid>https://arxiv.org/abs/2511.22751</guid>
<content:encoded><![CDATA[

arXiv:2511.22751v1 Announce Type: new 
Abstract: We explore the possibility of exact algorithmic learning with gradient-based methods and introduce a differentiable framework capable of strong length generalization on arithmetic tasks. Our approach centers on Differentiable Finite-State Transducers (DFSTs), a Turing-complete model family that avoids the pitfalls of prior architectures by enabling constant-precision, constant-time generation, and end-to-end log-parallel differentiable training. Leveraging policy-trajectory observations from expert agents, we train DFSTs to perform binary and decimal addition and multiplication. Remarkably, models trained on tiny datasets generalize without error to inputs thousands of times longer than the training examples. These results show that training differentiable agents on structured intermediate supervision could pave the way towards exact gradient-based learning of algorithmic skills. Code available at \href{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels</title>
<link>https://arxiv.org/abs/2511.22793</link>
<guid>https://arxiv.org/abs/2511.22793</guid>
<content:encoded><![CDATA[

arXiv:2511.22793v1 Announce Type: new 
Abstract: Channel state information (CSI) is essential for adaptive beamforming and maintaining robust links in wireless communication systems. However, acquiring CSI incurs significant overhead, consuming up to 25\% of spectrum resources in 5G networks due to frequent pilot transmissions at sub-millisecond intervals. Recent approaches aim to reduce this burden by reconstructing CSI from spatiotemporal RF measurements, such as signal strength and direction-of-arrival. While effective in offline settings, these methods often suffer from inference latencies in the 5--100~ms range, making them impractical for real-time systems. We present GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels, the first algorithm to break the 1 ms latency barrier while maintaining high accuracy. GSpaRC represents the RF environment using a compact set of 3D Gaussian primitives, each parameterized by a lightweight neural model augmented with physics-informed features such as distance-based attenuation. Unlike traditional vision-based splatting pipelines, GSpaRC is tailored for RF reception: it employs an equirectangular projection onto a hemispherical surface centered at the receiver to reflect omnidirectional antenna behavior. A custom CUDA pipeline enables fully parallelized directional sorting, splatting, and rendering across frequency and spatial dimensions. Evaluated on multiple RF datasets, GSpaRC achieves similar CSI reconstruction fidelity to recent state-of-the-art methods while reducing training and inference time by over an order of magnitude. By trading modest GPU computation for a substantial reduction in pilot overhead, GSpaRC enables scalable, low-latency channel estimation suitable for deployment in 5G and future wireless systems. The code is available here: \href{https://github.com/Nbhavyasai/GSpaRC-WirelessGaussianSplatting.git}{GSpaRC}.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Synthetic Data Improve Symbolic Regression Extrapolation Performance?</title>
<link>https://arxiv.org/abs/2511.22794</link>
<guid>https://arxiv.org/abs/2511.22794</guid>
<content:encoded><![CDATA[

arXiv:2511.22794v1 Announce Type: new 
Abstract: Many machine learning models perform well when making predictions within the training data range, but often struggle when required to extrapolate beyond it. Symbolic regression (SR) using genetic programming (GP) can generate flexible models but is prone to unreliable behaviour in extrapolation. This paper investigates whether adding synthetic data can help improve performance in such cases. We apply Kernel Density Estimation (KDE) to identify regions in the input space where the training data is sparse. Synthetic data is then generated in those regions using a knowledge distillation approach: a teacher model generates predictions on new input points, which are then used to train a student model. We evaluate this method across six benchmark datasets, using neural networks (NN), random forests (RF), and GP both as teacher models (to generate synthetic data) and as student models (trained on the augmented data). Results show that GP models can often improve when trained on synthetic data, especially in extrapolation areas. However, the improvement depends on the dataset and teacher model used. The most important improvements are observed when synthetic data from GPe is used to train GPp in extrapolation regions. Changes in interpolation areas show only slight changes. We also observe heterogeneous errors, where model performance varies across different regions of the input space. Overall, this approach offers a practical solution for better extrapolation. Note: An earlier version of this work appeared in the GECCO 2025 Workshop on Symbolic Regression. This arXiv version corrects several parts of the original submission.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent Neural Networks: From Layered Architectures to Graph-Organized Intelligence</title>
<link>https://arxiv.org/abs/2511.22813</link>
<guid>https://arxiv.org/abs/2511.22813</guid>
<content:encoded><![CDATA[

arXiv:2511.22813v1 Announce Type: new 
Abstract: Biological neurons exhibit remarkable intelligence: they maintain internal states, communicate selectively with other neurons, and self-organize into complex graphs rather than rigid hierarchical layers. What if artificial intelligence could emerge from similarly intelligent computational units? We introduce Intelligent Neural Networks (INN), a paradigm shift where neurons are first-class entities with internal memory and learned communication patterns, organized in complete graphs rather than sequential layers.
  Each Intelligent Neuron combines selective state-space dynamics (knowing when to activate) with attention-based routing (knowing to whom to send signals), enabling emergent computation through graph-structured interactions. On the standard Text8 character modeling benchmark, INN achieves 1.705 Bit-Per-Character (BPC), significantly outperforming a comparable Transformer (2.055 BPC) and matching a highly optimized LSTM baseline. Crucially, a parameter-matched baseline of stacked Mamba blocks fails to converge (>3.4 BPC) under the same training protocol, demonstrating that INN's graph topology provides essential training stability. Ablation studies confirm this: removing inter-neuron communication degrades performance or leads to instability, proving the value of learned neural routing.
  This work demonstrates that neuron-centric design with graph organization is not merely bio-inspired -- it is computationally effective, opening new directions for modular, interpretable, and scalable neural architectures.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified and Stable Risk Minimization Framework for Weakly Supervised Learning with Theoretical Guarantees</title>
<link>https://arxiv.org/abs/2511.22823</link>
<guid>https://arxiv.org/abs/2511.22823</guid>
<content:encoded><![CDATA[

arXiv:2511.22823v1 Announce Type: new 
Abstract: Weakly supervised learning has emerged as a practical alternative to fully supervised learning when complete and accurate labels are costly or infeasible to acquire. However, many existing methods are tailored to specific supervision patterns -- such as positive-unlabeled (PU), unlabeled-unlabeled (UU), complementary-label (CLL), partial-label (PLL), or similarity-unlabeled annotations -- and rely on post-hoc corrections to mitigate instability induced by indirect supervision. We propose a principled, unified framework that bypasses such post-hoc adjustments by directly formulating a stable surrogate risk grounded in the structure of weakly supervised data. The formulation naturally subsumes diverse settings -- including PU, UU, CLL, PLL, multi-class unlabeled, and tuple-based learning -- under a single optimization objective. We further establish a non-asymptotic generalization bound via Rademacher complexity that clarifies how supervision structure, model capacity, and sample size jointly govern performance. Beyond this, we analyze the effect of class-prior misspecification on the bound, deriving explicit terms that quantify its impact, and we study identifiability, giving sufficient conditions -- most notably via supervision stratification across groups -- under which the target risk is recoverable. Extensive experiments show consistent gains across class priors, dataset scales, and class counts -- without heuristic stabilization -- while exhibiting robustness to overfitting.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CausalProfiler: Generating Synthetic Benchmarks for Rigorous and Transparent Evaluation of Causal Machine Learning</title>
<link>https://arxiv.org/abs/2511.22842</link>
<guid>https://arxiv.org/abs/2511.22842</guid>
<content:encoded><![CDATA[

arXiv:2511.22842v1 Announce Type: new 
Abstract: Causal machine learning (Causal ML) aims to answer "what if" questions using machine learning algorithms, making it a promising tool for high-stakes decision-making. Yet, empirical evaluation practices in Causal ML remain limited. Existing benchmarks often rely on a handful of hand-crafted or semi-synthetic datasets, leading to brittle, non-generalizable conclusions. To bridge this gap, we introduce CausalProfiler, a synthetic benchmark generator for Causal ML methods. Based on a set of explicit design choices about the class of causal models, queries, and data considered, the CausalProfiler randomly samples causal models, data, queries, and ground truths constituting the synthetic causal benchmarks. In this way, Causal ML methods can be rigorously and transparently evaluated under a variety of conditions. This work offers the first random generator of synthetic causal benchmarks with coverage guarantees and transparent assumptions operating on the three levels of causal reasoning: observation, intervention, and counterfactual. We demonstrate its utility by evaluating several state-of-the-art methods under diverse conditions and assumptions, both in and out of the identification regime, illustrating the types of analyses and insights the CausalProfiler enables.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerfMamba: Performance Analysis and Pruning of Selective State Space Models</title>
<link>https://arxiv.org/abs/2511.22849</link>
<guid>https://arxiv.org/abs/2511.22849</guid>
<content:encoded><![CDATA[

arXiv:2511.22849v1 Announce Type: new 
Abstract: Recent advances in sequence modeling have introduced selective SSMs as promising alternatives to Transformer architectures, offering theoretical computational efficiency and sequence processing advantages. A comprehensive understanding of selective SSMs in runtime behavior, resource utilization patterns, and scaling characteristics still remains unexplored, thus obstructing their optimal deployment and further architectural improvements. This paper presents a thorough empirical study of Mamba-1 and Mamba-2, systematically profiled for performance to assess the design principles that contribute to their efficiency in state-space modeling. A detailed analysis of computation patterns, memory access, I/O characteristics, and scaling properties was performed for sequence lengths ranging from 64 to 16384 tokens. Our findings show that the SSM component, a central part of the selective SSM architecture, demands a significant portion of computational resources compared to other components in the Mamba block. Based on these insights, we propose a pruning technique that selectively removes low-activity states within the SSM component, achieving measurable throughput and memory gains while maintaining accuracy within a moderate pruning regime. This approach results in performance improvements across varying sequence lengths, achieving a 1.14x speedup and reducing memory usage by 11.50\%. These results offer valuable guidance for designing more efficient SSM architectures that can be applied to a wide range of real-world applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TARFVAE: Efficient One-Step Generative Time Series Forecasting via TARFLOW based VAE</title>
<link>https://arxiv.org/abs/2511.22853</link>
<guid>https://arxiv.org/abs/2511.22853</guid>
<content:encoded><![CDATA[

arXiv:2511.22853v1 Announce Type: new 
Abstract: Time series data is ubiquitous, with forecasting applications spanning from finance to healthcare. Beyond popular deterministic methods, generative models are gaining attention due to advancements in areas like image synthesis and video generation, as well as their inherent ability to provide probabilistic predictions. However, existing generative approaches mostly involve recurrent generative operations or repeated denoising steps, making the prediction laborious, particularly for long-term forecasting. Most of them only conduct experiments for relatively short-term forecasting, with limited comparison to deterministic methods in long-term forecasting, leaving their practical advantages unclear. This paper presents TARFVAE, a novel generative framework that combines the Transformer-based autoregressive flow (TARFLOW) and variational autoencoder (VAE) for efficient one-step generative time series forecasting. Inspired by the rethinking that complex architectures for extracting time series representations might not be necessary, we add a flow module, TARFLOW, to VAE to promote spontaneous learning of latent variables that benefit predictions. TARFLOW enhances VAE's posterior estimation by breaking the Gaussian assumption, thereby enabling a more informative latent space. TARFVAE uses only the forward process of TARFLOW, avoiding autoregressive inverse operations and thus ensuring fast generation. During generation, it samples from the prior latent space and directly generates full-horizon forecasts via the VAE decoder. With simple MLP modules, TARFVAE achieves superior performance over state-of-the-art deterministic and generative models across different forecast horizons on benchmark datasets while maintaining efficient prediction speed, demonstrating its effectiveness as an efficient and powerful solution for generative time series forecasting.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate</title>
<link>https://arxiv.org/abs/2511.22854</link>
<guid>https://arxiv.org/abs/2511.22854</guid>
<content:encoded><![CDATA[

arXiv:2511.22854v1 Announce Type: new 
Abstract: When people reason about cause and effect, they often consider many competing "what if" scenarios before deciding which explanation fits best. Analogously, advanced language models capable of causal inference can consider multiple interventions and counterfactuals to judge the validity of causal claims. Crucially, this type of reasoning is less like a single calculation and more like an internal dialogue between alternative hypotheses. In this paper, we make this dialogue explicit through a dual-agent debate framework where one model provides a structured causal inference, and the other critically examines this reasoning for logical flaws. When disagreements arise, agents attempt to persuade each other, challenging each other's logic and revising their conclusions until they converge on a mutually agreed answer. To take advantage of this deliberative process, we specifically use reasoning language models, whose strengths in both causal inference and adversarial debate remain under-explored relative to standard large language models. We evaluate our approach on the CLadder dataset, a benchmark linking natural language questions to formally defined causal graphs across all three rungs of Pearl's ladder of causation. With Qwen3 and DeepSeek-R1 as debater agents, we demonstrate that multi-agent debate improves DeepSeek-R1's overall accuracy in causal inference from 78.03% to 87.45%, with the counterfactual category specifically improving from 67.94% to 80.04% accuracy. Similarly, Qwen3's overall accuracy improves from 84.16% to 89.41%, and counterfactual questions from 71.53% to 80.35%, showing that strong models can still benefit greatly from debate with weaker agents. Our results highlight the potential of reasoning models as building blocks for multi-agent systems in causal inference, and demonstrate the importance of diverse perspectives in causal problem-solving.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.22862</link>
<guid>https://arxiv.org/abs/2511.22862</guid>
<content:encoded><![CDATA[

arXiv:2511.22862v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) enables online model adaptation using only unlabeled test data, aiming to bridge the gap between source and target distributions. However, in multimodal scenarios, varying degrees of distribution shift across different modalities give rise to a complex coupling effect of unimodal shallow feature shift and cross-modal high-level semantic misalignment, posing a major obstacle to extending existing TTA methods to the multimodal field. To address this challenge, we propose a novel multimodal test-time adaptation (MMTTA) framework, termed as Bridging Modalities via Progressive Re-alignment (BriMPR). BriMPR, consisting of two progressively enhanced modules, tackles the coupling effect with a divide-and-conquer strategy. Specifically, we first decompose MMTTA into multiple unimodal feature alignment sub-problems. By leveraging the strong function approximation ability of prompt tuning, we calibrate the unimodal global feature distributions to their respective source distributions, so as to achieve the initial semantic re-alignment across modalities. Subsequently, we assign the credible pseudo-labels to combinations of masked and complete modalities, and introduce inter-modal instance-wise contrastive learning to further enhance the information interaction among modalities and refine the alignment. Extensive experiments on MMTTA tasks, including both corruption-based and real-world domain shift benchmarks, demonstrate the superiority of our method. Our source code is available at [this URL](https://github.com/Luchicken/BriMPR).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARM-Explainer -- Explaining and improving graph neural network predictions for the maximum clique problem using node features and association rule mining</title>
<link>https://arxiv.org/abs/2511.22866</link>
<guid>https://arxiv.org/abs/2511.22866</guid>
<content:encoded><![CDATA[

arXiv:2511.22866v1 Announce Type: new 
Abstract: Numerous graph neural network (GNN)-based algorithms have been proposed to solve graph-based combinatorial optimization problems (COPs), but methods to explain their predictions remain largely undeveloped. We introduce ARM-Explainer, a post-hoc, model-level explainer based on association rule mining, and demonstrate it on the predictions of the hybrid geometric scattering (HGS) GNN for the maximum clique problem (MCP), a canonical NP-hard graph-based COP. The eight most explanatory association rules discovered by ARM-Explainer achieve high median lift and confidence values of 2.42 and 0.49, respectively, on test instances from the TWITTER and BHOSLIB-DIMACS benchmark datasets. ARM-Explainer identifies the most important node features, together with their value ranges, that influence the GNN's predictions on these datasets. Furthermore, augmenting the GNN with informative node features substantially improves its performance on the MCP, increasing the median largest-found clique size by 22% (from 29.5 to 36) on large graphs from the BHOSLIB-DIMACS dataset.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Covering-Space Normalizing Flows: Approximating Pushforwards on Lens Spaces</title>
<link>https://arxiv.org/abs/2511.22882</link>
<guid>https://arxiv.org/abs/2511.22882</guid>
<content:encoded><![CDATA[

arXiv:2511.22882v1 Announce Type: new 
Abstract: We construct pushforward distributions via the universal covering map rho: S^3 -> L(p;q) with the goal of approximating these distributions using flows on L(p;q). We highlight that our method deletes redundancies in the case of a symmetric S^3 distribution. Using our model, we approximate the pushforwards of von Mises-Fisher-induced target densities as well as that of a Z_12-symmetric Boltzmann distribution on S^3 constructed to model benzene.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Chaotic Pedestrian Behavior Using Chaos Indicators and Supervised Learning</title>
<link>https://arxiv.org/abs/2511.22887</link>
<guid>https://arxiv.org/abs/2511.22887</guid>
<content:encoded><![CDATA[

arXiv:2511.22887v1 Announce Type: new 
Abstract: As cities around the world aim to improve walkability and safety, understanding the irregular and unpredictable nature of pedestrian behavior has become increasingly important. This study introduces a data-driven framework for modeling chaotic pedestrian movement using empirically observed trajectory data and supervised learning. Videos were recorded during both daytime and nighttime conditions to capture pedestrian dynamics under varying ambient and traffic contexts. Pedestrian trajectories were extracted through computer vision techniques, and behavioral chaos was quantified using four chaos metrics: Approximate Entropy and Lyapunov Exponent, each computed for both velocity and direction change. A Principal Component Analysis (PCA) was then applied to consolidate these indicators into a unified chaos score. A comprehensive set of individual, group-level, and contextual traffic features was engineered and used to train Random Forest and CatBoost regression models. CatBoost models consistently achieved superior performance. The best daytime PCA-based CatBoost model reached an R^2 of 0.8319, while the nighttime PCA-based CatBoost model attained an R^2 of 0.8574. SHAP analysis highlighted that features such as distance travel, movement duration, and speed variability were robust contributors to chaotic behavior. The proposed framework enables practitioners to quantify and anticipate behavioral instability in real-world settings. Planners and engineers can use chaos scores to identify high-risk pedestrian zones, apprise infrastructure improvements, and calibrate realistic microsimulation models. The approach also supports adaptive risk assessment in automated vehicle systems by capturing short-term motion unpredictability grounded in observable, interpretable features.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Training for Process Reward Models</title>
<link>https://arxiv.org/abs/2511.22888</link>
<guid>https://arxiv.org/abs/2511.22888</guid>
<content:encoded><![CDATA[

arXiv:2511.22888v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnECG: Efficient Ensemble Learning for Electrocardiogram Multi-task Foundation Model</title>
<link>https://arxiv.org/abs/2511.22935</link>
<guid>https://arxiv.org/abs/2511.22935</guid>
<content:encoded><![CDATA[

arXiv:2511.22935v1 Announce Type: new 
Abstract: Electrocardiogram (ECG) analysis plays a vital role in the early detection, monitoring, and management of various cardiovascular conditions. While existing models have achieved notable success in ECG interpretation, they fail to leverage the interrelated nature of various cardiac abnormalities. Conversely, developing a specific model capable of extracting all relevant features for multiple ECG tasks remains a significant challenge. Large-scale foundation models, though powerful, are not typically pretrained on ECG data, making full re-training or fine-tuning computationally expensive. To address these challenges, we propose EnECG(Mixture of Experts-based Ensemble Learning for ECG Multi-tasks), an ensemble-based framework that integrates multiple specialized foundation models, each excelling in different aspects of ECG interpretation. Instead of relying on a single model or single task, EnECG leverages the strengths of multiple specialized models to tackle a variety of ECG-based tasks. To mitigate the high computational cost of full re-training or fine-tuning, we introduce a lightweight adaptation strategy: attaching dedicated output layers to each foundation model and applying Low-Rank Adaptation (LoRA) only to these newly added parameters. We then adopt a Mixture of Experts (MoE) mechanism to learn ensemble weights, effectively combining the complementary expertise of individual models. Our experimental results demonstrate that by minimizing the scope of fine-tuning, EnECG can help reduce computational and memory costs while maintaining the strong representational power of foundation models. This framework not only enhances feature extraction and predictive performance but also ensures practical efficiency for real-world clinical applications. The code is available at https://github.com/yuhaoxu99/EnECG.git.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORGI: GNNs with Convolutional Residual Global Interactions for Lagrangian Simulation</title>
<link>https://arxiv.org/abs/2511.22938</link>
<guid>https://arxiv.org/abs/2511.22938</guid>
<content:encoded><![CDATA[

arXiv:2511.22938v1 Announce Type: new 
Abstract: Partial differential equations (PDEs) are central to dynamical systems modeling, particularly in hydrodynamics, where traditional solvers often struggle with nonlinearity and computational cost. Lagrangian neural surrogates such as GNS and SEGNN have emerged as strong alternatives by learning from particle-based simulations. However, these models typically operate with limited receptive fields, making them inaccurate for capturing the inherently global interactions in fluid flows. Motivated by this observation, we introduce Convolutional Residual Global Interactions (CORGI), a hybrid architecture that augments any GNN-based solver with a lightweight Eulerian component for global context aggregation. By projecting particle features onto a grid, applying convolutional updates, and mapping them back to the particle domain, CORGI captures long-range dependencies without significant overhead. When applied to a GNS backbone, CORGI achieves a 57% improvement in rollout accuracy with only 13% more inference time and 31% more training time. Compared to SEGNN, CORGI improves accuracy by 49% while reducing inference time by 48% and training time by 30%. Even under identical runtime constraints, CORGI outperforms GNS by 47% on average, highlighting its versatility and performance on varied compute budgets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandit Guided Submodular Curriculum for Adaptive Subset Selection</title>
<link>https://arxiv.org/abs/2511.22944</link>
<guid>https://arxiv.org/abs/2511.22944</guid>
<content:encoded><![CDATA[

arXiv:2511.22944v1 Announce Type: new 
Abstract: Traditional curriculum learning proceeds from easy to hard samples, yet defining a reliable notion of difficulty remains elusive. Prior work has used submodular functions to induce difficulty scores in curriculum learning. We reinterpret adaptive subset selection and formulate it as a multi-armed bandit problem, where each arm corresponds to a submodular function guiding sample selection. We introduce ONLINESUBMOD, a novel online greedy policy that optimizes a utility-driven reward and provably achieves no-regret performance under various sampling regimes. Empirically, ONLINESUBMOD outperforms both traditional curriculum learning and bi-level optimization approaches across vision and language datasets, showing superior accuracy-efficiency tradeoffs. More broadly, we show that validationdriven reward metrics offer a principled way to guide the curriculum schedule.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Experts are all you need: A Composable Framework for Large Language Model Inference</title>
<link>https://arxiv.org/abs/2511.22955</link>
<guid>https://arxiv.org/abs/2511.22955</guid>
<content:encoded><![CDATA[

arXiv:2511.22955v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or "experts". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential "plan--act--observe" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Trainable Centrality Framework for Modern Data</title>
<link>https://arxiv.org/abs/2511.22959</link>
<guid>https://arxiv.org/abs/2511.22959</guid>
<content:encoded><![CDATA[

arXiv:2511.22959v1 Announce Type: new 
Abstract: Measuring how central or typical a data point is underpins robust estimation, ranking, and outlier detection, but classical depth notions become expensive and unstable in high dimensions and are hard to extend beyond Euclidean data. We introduce Fused Unified centrality Score Estimation (FUSE), a neural centrality framework that operates on top of arbitrary representations. FUSE combines a global head, trained from pairwise distance-based comparisons to learn an anchor-free centrality score, with a local head, trained by denoising score matching to approximate a smoothed log-density potential. A single parameter between 0 and 1 interpolates between these calibrated signals, yielding depth-like centrality from different views via one forward pass. Across synthetic distributions, real images, time series, and text data, and standard outlier detection benchmarks, FUSE recovers meaningful classical ordering, reveals multi-scale geometric structures, and attains competitive performance with strong classical baselines while remaining simple and efficient.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Modular Framework for Rapidly Building Intrusion Predictors</title>
<link>https://arxiv.org/abs/2511.23000</link>
<guid>https://arxiv.org/abs/2511.23000</guid>
<content:encoded><![CDATA[

arXiv:2511.23000v1 Announce Type: new 
Abstract: We study automated intrusion prediction in an IT system using statistical learning methods. The focus is on developing online attack predictors that detect attacks in real time and identify the current stage of the attack. While such predictors have been proposed in the recent literature, these works typically rely on constructing a monolithic predictor tailored to a specific attack type and scenario. Given that hundreds of attack types are cataloged in the MITRE framework, training a separate monolithic predictor for each of them is infeasible. In this paper, we propose a modular framework for rapidly assembling online attack predictors from reusable components. The modular nature of a predictor facilitates controlling key metrics like timeliness and accuracy of prediction, as well as tuning the trade-off between them. Using public datasets for training and evaluation, we provide many examples of modular predictors and show how an effective predictor can be dynamically assembled during training from a network of modular components.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Diffusion for Generative Recommendation</title>
<link>https://arxiv.org/abs/2511.23021</link>
<guid>https://arxiv.org/abs/2511.23021</guid>
<content:encoded><![CDATA[

arXiv:2511.23021v1 Announce Type: new 
Abstract: Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delta-XAI: A Unified Framework for Explaining Prediction Changes in Online Time Series Monitoring</title>
<link>https://arxiv.org/abs/2511.23036</link>
<guid>https://arxiv.org/abs/2511.23036</guid>
<content:encoded><![CDATA[

arXiv:2511.23036v1 Announce Type: new 
Abstract: Explaining online time series monitoring models is crucial across sensitive domains such as healthcare and finance, where temporal and contextual prediction dynamics underpin critical decisions. While recent XAI methods have improved the explainability of time series models, they mostly analyze each time step independently, overlooking temporal dependencies. This results in further challenges: explaining prediction changes is non-trivial, methods fail to leverage online dynamics, and evaluation remains difficult. To address these challenges, we propose Delta-XAI, which adapts 14 existing XAI methods through a wrapper function and introduces a principled evaluation suite for the online setting, assessing diverse aspects, such as faithfulness, sufficiency, and coherence. Experiments reveal that classical gradient-based methods, such as Integrated Gradients (IG), can outperform recent approaches when adapted for temporal analysis. Building on this, we propose Shifted Window Integrated Gradients (SWING), which incorporates past observations in the integration path to systematically capture temporal dependencies and mitigate out-of-distribution effects. Extensive experiments consistently demonstrate the effectiveness of SWING across diverse settings with respect to diverse metrics. Our code is publicly available at https://anonymous.4open.science/r/Delta-XAI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory</title>
<link>https://arxiv.org/abs/2511.23083</link>
<guid>https://arxiv.org/abs/2511.23083</guid>
<content:encoded><![CDATA[

arXiv:2511.23083v1 Announce Type: new 
Abstract: High-capacity kernel Hopfield networks exhibit a "Ridge of Optimization" characterized by extreme stability. While previously linked to "Spectral Concentration," its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the "Edge of Stability," a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Freeze, Diffuse, Decode: Geometry-Aware Adaptation of Pretrained Transformer Embeddings for Antimicrobial Peptide Design</title>
<link>https://arxiv.org/abs/2511.23120</link>
<guid>https://arxiv.org/abs/2511.23120</guid>
<content:encoded><![CDATA[

arXiv:2511.23120v1 Announce Type: new 
Abstract: Pretrained transformers provide rich, general-purpose embeddings, which are transferred to downstream tasks. However, current transfer strategies: fine-tuning and probing, either distort the pretrained geometric structure of the embeddings or lack sufficient expressivity to capture task-relevant signals. These issues become even more pronounced when supervised data are scarce. Here, we introduce Freeze, Diffuse, Decode (FDD), a novel diffusion-based framework that adapts pre-trained embeddings to downstream tasks while preserving their underlying geometric structure. FDD propagates supervised signal along the intrinsic manifold of frozen embeddings, enabling a geometry-aware adaptation of the embedding space. Applied to antimicrobial peptide design, FDD yields low-dimensional, predictive, and interpretable representations that support property prediction, retrieval, and latent-space interpolation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Discovery of Laser Dicing Processes with Bayesian Optimization for Semiconductor Manufacturing</title>
<link>https://arxiv.org/abs/2511.23141</link>
<guid>https://arxiv.org/abs/2511.23141</guid>
<content:encoded><![CDATA[

arXiv:2511.23141v1 Announce Type: new 
Abstract: Laser dicing of semiconductor wafers is a critical step in microelectronic manufacturing, where multiple sequential laser passes precisely separate individual dies from the wafer. Adapting this complex sequential process to new wafer materials typically requires weeks of expert effort to balance process speed, separation quality, and material integrity. We present the first automated discovery of production-ready laser dicing processes on an industrial LASER1205 dicing system. We formulate the problem as a high-dimensional, constrained multi-objective Bayesian optimization task, and introduce a sequential two-level fidelity strategy to minimize expensive destructive die-strength evaluations. On bare silicon and product wafers, our method autonomously delivers feasible configurations that match or exceed expert baselines in production speed, die strength, and structural integrity, using only technician-level operation. Post-hoc validation of different weight configurations of the utility functions reveals that multiple feasible solutions with qualitatively different trade-offs can be obtained from the final surrogate model. Expert-refinement of the discovered process can further improve production speed while preserving die strength and structural integrity, surpassing purely manual or automated methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Neural Audio Codecs to EEG</title>
<link>https://arxiv.org/abs/2511.23142</link>
<guid>https://arxiv.org/abs/2511.23142</guid>
<content:encoded><![CDATA[

arXiv:2511.23142v1 Announce Type: new 
Abstract: EEG and audio are inherently distinct modalities, differing in sampling rate, channel structure, and scale. Yet, we show that pretrained neural audio codecs can serve as effective starting points for EEG compression, provided that the data are preprocessed to be suitable to the codec's input constraints. Using DAC, a state-of-the-art neural audio codec as our base, we demonstrate that raw EEG can be mapped into the codec's stride-based framing, enabling direct reuse of the audio-pretrained encoder-decoder. Even without modification, this setup yields stable EEG reconstructions, and fine-tuning on EEG data further improves fidelity and generalization compared to training from scratch. We systematically explore compression-quality trade-offs by varying residual codebook depth, codebook (vocabulary) size, and input sampling rate. To capture spatial dependencies across electrodes, we propose DAC-MC, a multi-channel extension with attention-based cross-channel aggregation and channel-specific decoding, while retaining the audio-pretrained initialization. Evaluations on the TUH Abnormal and Epilepsy datasets show that the adapted codecs preserve clinically relevant information, as reflected in spectrogram-based reconstruction loss and downstream classification accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretical Framework for Discovering Groups and Unitary Representations via Tensor Factorization</title>
<link>https://arxiv.org/abs/2511.23152</link>
<guid>https://arxiv.org/abs/2511.23152</guid>
<content:encoded><![CDATA[

arXiv:2511.23152v1 Announce Type: new 
Abstract: We analyze the HyperCube model, an \textit{operator-valued} tensor factorization architecture that discovers group structures and their unitary representations. We provide a rigorous theoretical explanation for this inductive bias by decomposing its objective into a term regulating factor scales ($\mathcal{B}$) and a term enforcing directional alignment ($\mathcal{R} \geq 0$). This decomposition isolates the \textit{collinear manifold} ($\mathcal{R}=0$), to which numerical optimization consistently converges for group isotopes. We prove that this manifold admits feasible solutions exclusively for group isotopes, and that within it, $\mathcal{B}$ exerts a variational pressure toward unitarity. To bridge the gap to the global landscape, we formulate a \textit{Collinearity Dominance Conjecture}, supported by empirical observations. Conditional on this dominance, we prove two key results: (1) the global minimum is achieved by the unitary regular representation for groups, and (2) non-group operations incur a strictly higher objective value, formally quantifying the model's inductive bias toward the associative structure of groups (up to isotopy).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating the Event-Related Potential from Few EEG Trials</title>
<link>https://arxiv.org/abs/2511.23162</link>
<guid>https://arxiv.org/abs/2511.23162</guid>
<content:encoded><![CDATA[

arXiv:2511.23162v1 Announce Type: new 
Abstract: Event-related potentials (ERP) are measurements of brain activity with wide applications in basic and clinical neuroscience, that are typically estimated using the average of many trials of electroencephalography signals (EEG) to sufficiently reduce noise and signal variability. We introduce EEG2ERP, a novel uncertainty-aware autoencoder approach that maps an arbitrary number of EEG trials to their associated ERP. To account for the ERP uncertainty we use bootstrapped training targets and introduce a separate variance decoder to model the uncertainty of the estimated ERP. We evaluate our approach in the challenging zero-shot scenario of generalizing to new subjects considering three different publicly available data sources; i) the comprehensive ERP CORE dataset that includes over 50,000 EEG trials across six ERP paradigms from 40 subjects, ii) the large P300 Speller BCI dataset, and iii) a neuroimaging dataset on face perception consisting of both EEG and magnetoencephalography (MEG) data. We consistently find that our method in the few trial regime provides substantially better ERP estimates than commonly used conventional and robust averaging procedures. EEG2ERP is the first deep learning approach to map EEG signals to their associated ERP, moving toward reducing the number of trials necessary for ERP research. Code is available at https://github.com/andersxa/EEG2ERP
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-Efficient Vision Transformer Inference for Edge-AI Deployment</title>
<link>https://arxiv.org/abs/2511.23166</link>
<guid>https://arxiv.org/abs/2511.23166</guid>
<content:encoded><![CDATA[

arXiv:2511.23166v1 Announce Type: new 
Abstract: The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series with Missing Data</title>
<link>https://arxiv.org/abs/2511.23238</link>
<guid>https://arxiv.org/abs/2511.23238</guid>
<content:encoded><![CDATA[

arXiv:2511.23238v1 Announce Type: new 
Abstract: Irregularly sampled time series with substantial missing observations are common in healthcare and sensor networks. We introduce SDE-Attention, a family of SDE-RNNs equipped with channel-level attention on the latent pre-RNN state, including channel recalibration, time-varying feature attention, and pyramidal multi-scale self-attention. We therefore conduct a comparison on a synthetic periodic dataset and real-world benchmarks, under varying missing rate. Latent-space attention consistently improves over a vanilla SDE-RNN. On the univariate UCR datasets, the LSTM-based time-varying feature model SDE-TVF-L achieves the highest average accuracy, raising mean performance by approximately 4, 6, and 10 percentage points over the baseline at 30%, 60% and 90% missingness, respectively (averaged across datasets). On multivariate UEA benchmarks, attention-augmented models again outperform the backbone, with SDE-TVF-L yielding up to a 7% gain in mean accuracy under high missingness. Among the proposed mechanisms, time-varying feature attention is the most robust on univariate datasets. On multivariate datasets, different attention types excel on different tasks, showing that SDE-Attention can be flexibly adapted to the structure of each problem.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Understanding Transformers in Learning Random Walks</title>
<link>https://arxiv.org/abs/2511.23239</link>
<guid>https://arxiv.org/abs/2511.23239</guid>
<content:encoded><![CDATA[

arXiv:2511.23239v1 Announce Type: new 
Abstract: Transformers have proven highly effective across various applications, especially in handling sequential data such as natural languages and time series. However, transformer models often lack clear interpretability, and the success of transformers has not been well understood in theory. In this paper, we study the capability and interpretability of transformers in learning a family of classic statistical models, namely random walks on circles. We theoretically demonstrate that, after training with gradient descent, a one-layer transformer model can achieve optimal accuracy in predicting random walks. Importantly, our analysis reveals that the trained model is interpretable: the trained softmax attention serves as a token selector, focusing on the direct parent state; subsequently, the value matrix executes a one-step probability transition to predict the location of the next state based on this parent state. We also show that certain edge cases not covered by our theory are indeed failure cases, demonstrating that our theoretical conditions are tight. By investigating these success and failure cases, it is revealed that gradient descent with small initialization may fail or struggle to converge to a good solution in certain simple tasks even beyond random walks. Experiments are conducted to support our theoretical findings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heteroscedastic Neural Networks for Path Loss Prediction with Link-Specific Uncertainty</title>
<link>https://arxiv.org/abs/2511.23243</link>
<guid>https://arxiv.org/abs/2511.23243</guid>
<content:encoded><![CDATA[

arXiv:2511.23243v1 Announce Type: new 
Abstract: Traditional and modern machine learning-based path loss models typically assume a constant prediction variance. We propose a neural network that jointly predicts the mean and link-specific variance by minimizing a Gaussian negative log-likelihood, enabling heteroscedastic uncertainty estimates. We compare shared, partially shared, and independent-parameter architectures using accuracy, calibration, and sharpness metrics on blind test sets from large public RF drive-test datasets. The shared-parameter architecture performs best, achieving an RMSE of 7.4 dB, 95.1 percent coverage for 95 percent prediction intervals, and a mean interval width of 29.6 dB. These uncertainty estimates further support link-specific coverage margins, improve RF planning and interference analyses, and provide effective self-diagnostics of model weaknesses.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time Series Forecasting via Direct Per-Step Probability Distribution Modeling</title>
<link>https://arxiv.org/abs/2511.23260</link>
<guid>https://arxiv.org/abs/2511.23260</guid>
<content:encoded><![CDATA[

arXiv:2511.23260v1 Announce Type: new 
Abstract: Deep neural network-based time series prediction models have recently demonstrated superior capabilities in capturing complex temporal dependencies. However, it is challenging for these models to account for uncertainty associated with their predictions, because they directly output scalar values at each time step. To address such a challenge, we propose a novel model named interleaved dual-branch Probability Distribution Network (interPDN), which directly constructs discrete probability distributions per step instead of a scalar. The regression output at each time step is derived by computing the expectation of the predictive distribution on a predefined support set. To mitigate prediction anomalies, a dual-branch architecture is introduced with interleaved support sets, augmented by coarse temporal-scale branches for long-term trend forecasting. Outputs from another branch are treated as auxiliary signals to impose self-supervised consistency constraints on the current branch's prediction. Extensive experiments on multiple real-world datasets demonstrate the superior performance of interPDN.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Improved and Generalised Analysis for Spectral Clustering</title>
<link>https://arxiv.org/abs/2511.23261</link>
<guid>https://arxiv.org/abs/2511.23261</guid>
<content:encoded><![CDATA[

arXiv:2511.23261v1 Announce Type: new 
Abstract: We revisit the theoretical performances of Spectral Clustering, a classical algorithm for graph partitioning that relies on the eigenvectors of a matrix representation of the graph. Informally, we show that Spectral Clustering works well as long as the smallest eigenvalues appear in groups well separated from the rest of the matrix representation's spectrum. This arises, for example, whenever there exists a hierarchy of clusters at different scales, a regime not captured by previous analyses. Our results are very general and can be applied beyond the traditional graph Laplacian. In particular, we study Hermitian representations of digraphs and show Spectral Clustering can recover partitions where edges between clusters are oriented mostly in the same direction. This has applications in, for example, the analysis of trophic levels in ecological networks. We demonstrate that our results accurately predict the performances of Spectral Clustering on synthetic and real-world data sets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning</title>
<link>https://arxiv.org/abs/2511.23264</link>
<guid>https://arxiv.org/abs/2511.23264</guid>
<content:encoded><![CDATA[

arXiv:2511.23264v1 Announce Type: new 
Abstract: Multi-aspect sentiment analysis of Bangla e-commerce reviews remains challenging due to limited annotated datasets, morphological complexity, code-mixing phenomena, and domain shift issues, affecting 300 million Bangla-speaking users. Existing approaches lack explainability and cross-domain generalization capabilities crucial for practical deployment. We present BanglaSentNet, an explainable hybrid deep learning framework integrating LSTM, BiLSTM, GRU, and BanglaBERT through dynamic weighted ensemble learning for multi-aspect sentiment classification. We introduce a dataset of 8,755 manually annotated Bangla product reviews across four aspects (Quality, Service, Price, Decoration) from major Bangladeshi e-commerce platforms. Our framework incorporates SHAP-based feature attribution and attention visualization for transparent insights. BanglaSentNet achieves 85% accuracy and 0.88 F1-score, outperforming standalone deep learning models by 3-7% and traditional approaches substantially. The explainability suite achieves 9.4/10 interpretability score with 87.6% human agreement. Cross-domain transfer learning experiments reveal robust generalization: zero-shot performance retains 67-76% effectiveness across diverse domains (BanglaBook reviews, social media, general e-commerce, news headlines); few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance, significantly reducing annotation costs. Real-world deployment demonstrates practical utility for Bangladeshi e-commerce platforms, enabling data-driven decision-making for pricing optimization, service improvement, and customer experience enhancement. This research establishes a new state-of-the-art benchmark for Bangla sentiment analysis, advances ensemble learning methodologies for low-resource languages, and provides actionable solutions for commercial applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Curve Fitting: Neuro-Symbolic Agents for Context-Aware Epidemic Forecasting</title>
<link>https://arxiv.org/abs/2511.23276</link>
<guid>https://arxiv.org/abs/2511.23276</guid>
<content:encoded><![CDATA[

arXiv:2511.23276v1 Announce Type: new 
Abstract: Effective surveillance of hand, foot and mouth disease (HFMD) requires forecasts accounting for epidemiological patterns and contextual drivers like school calendars and weather. While classical models and recent foundation models (e.g., Chronos, TimesFM) incorporate covariates, they often lack the semantic reasoning to interpret the causal interplay between conflicting drivers. In this work, we propose a two-agent framework decoupling contextual interpretation from probabilistic forecasting. An LLM "event interpreter" processes heterogeneous signals-including school schedules, meteorological summaries, and reports-into a scalar transmission-impact signal. A neuro-symbolic core then combines this with historical case counts to produce calibrated probabilistic forecasts. We evaluate the framework on real-world HFMD datasets from Hong Kong (2023-2024) and Lishui, China (2024). Compared to traditional and foundation-model baselines, our approach achieves competitive point forecasting accuracy while providing robust 90% prediction intervals (coverage 0.85-1.00) and human-interpretable rationales. Our results suggest that structurally integrating domain knowledge through LLMs can match state-of-the-art performance while yielding context-aware forecasts that align with public health workflows. Code is available at https://github.com/jw-chae/forecast_MED .
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Generalization Gap in Parameter-efficient Federated Edge Learning</title>
<link>https://arxiv.org/abs/2511.23282</link>
<guid>https://arxiv.org/abs/2511.23282</guid>
<content:encoded><![CDATA[

arXiv:2511.23282v1 Announce Type: new 
Abstract: Federated edge learning (FEEL) provides a promising foundation for edge artificial intelligence (AI) by enabling collaborative model training while preserving data privacy. However, limited and heterogeneous local datasets, as well as resource-constrained deployment, severely degrade both model generalization and resource utilization, leading to a compromised learning performance. Therefore, we propose a parameter-efficient FEEL framework that jointly leverages model pruning and client selection to tackle such challenges. First, we derive an information-theoretic generalization statement that characterizes the discrepancy between training and testing function losses and embed it into the convergence analysis. It reveals that a larger local generalization statement can undermine the global convergence. Then, we formulate a generalization-aware average squared gradient norm bound minimization problem, by jointly optimizing the pruning ratios, client selection, and communication-computation resources under energy and delay constraints. Despite its non-convexity, the resulting mixed-integer problem is efficiently solved via an alternating optimization algorithm. Extensive experiments demonstrate that the proposed design achieves superior learning performance than state-of-the-art baselines, validating the effectiveness of coupling generalization-aware analysis with system-level optimization for efficient FEEL.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla</title>
<link>https://arxiv.org/abs/2511.23287</link>
<guid>https://arxiv.org/abs/2511.23287</guid>
<content:encoded><![CDATA[

arXiv:2511.23287v1 Announce Type: new 
Abstract: The expansion of the Internet and social networks has led to an explosion of user-generated content. Author intent understanding plays a crucial role in interpreting social media content. This paper addresses author intent classification in Bangla social media posts by leveraging both textual and visual data. Recognizing limitations in previous unimodal approaches, we systematically benchmark transformer-based language models (mBERT, DistilBERT, XLM-RoBERTa) and vision architectures (ViT, Swin, SwiftFormer, ResNet, DenseNet, MobileNet), utilizing the Uddessho dataset of 3,048 posts spanning six practical intent categories. We introduce a novel intermediate fusion strategy that significantly outperforms early and late fusion on this task. Experimental results show that intermediate fusion, particularly with mBERT and Swin Transformer, achieves 84.11% macro-F1 score, establishing a new state-of-the-art with an 8.4 percentage-point improvement over prior Bangla multimodal approaches. Our analysis demonstrates that integrating visual context substantially enhances intent classification. Cross-modal feature integration at intermediate levels provides optimal balance between modality-specific representation and cross-modal learning. This research establishes new benchmarks and methodological standards for Bangla and other low-resource languages. We call our proposed framework BangACMM (Bangla Author Content MultiModal).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning for Scientific Visualization: Ensemble Data Analysis</title>
<link>https://arxiv.org/abs/2511.23290</link>
<guid>https://arxiv.org/abs/2511.23290</guid>
<content:encoded><![CDATA[

arXiv:2511.23290v1 Announce Type: new 
Abstract: Scientific simulations and experimental measurements produce vast amounts of spatio-temporal data, yet extracting meaningful insights remains challenging due to high dimensionality, complex structures, and missing information. Traditional analysis methods often struggle with these issues, motivating the need for more robust, data-driven approaches. This dissertation explores deep learning methodologies to improve the analysis and visualization of spatio-temporal scientific ensembles, focusing on dimensionality reduction, flow estimation, and temporal interpolation. First, we address high-dimensional data representation through autoencoder-based dimensionality reduction for scientific ensembles. We evaluate the stability of projection metrics under partial labeling and introduce a Pareto-efficient selection strategy to identify optimal autoencoder variants, ensuring expressive and reliable low-dimensional embeddings. Next, we present FLINT, a deep learning model for high-quality flow estimation and temporal interpolation in both flow-supervised and flow-unsupervised settings. FLINT reconstructs missing velocity fields and generates high-fidelity temporal interpolants for scalar fields across 2D+time and 3D+time ensembles without domain-specific assumptions or extensive finetuning. To further improve adaptability and generalization, we introduce HyperFLINT, a hypernetwork-based approach that conditions on simulation parameters to estimate flow fields and interpolate scalar data. This parameter-aware adaptation yields more accurate reconstructions across diverse scientific domains, even with sparse or incomplete data. Overall, this dissertation advances deep learning techniques for scientific visualization, providing scalable, adaptable, and high-quality solutions for interpreting complex spatio-temporal ensembles.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hard-Constrained Neural Networks with Physics-Embedded Architecture for Residual Dynamics Learning and Invariant Enforcement in Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2511.23307</link>
<guid>https://arxiv.org/abs/2511.23307</guid>
<content:encoded><![CDATA[

arXiv:2511.23307v1 Announce Type: new 
Abstract: This paper presents a framework for physics-informed learning in complex cyber-physical systems governed by differential equations with both unknown dynamics and algebraic invariants. First, we formalize the Hybrid Recurrent Physics-Informed Neural Network (HRPINN), a general-purpose architecture that embeds known physics as a hard structural constraint within a recurrent integrator to learn only residual dynamics. Second, we introduce the Projected HRPINN (PHRPINN), a novel extension that integrates a predict-project mechanism to strictly enforce algebraic invariants by design. The framework is supported by a theoretical analysis of its representational capacity. We validate HRPINN on a real-world battery prognostics DAE and evaluate PHRPINN on a suite of standard constrained benchmarks. The results demonstrate the framework's potential for achieving high accuracy and data efficiency, while also highlighting critical trade-offs between physical consistency, computational cost, and numerical stability, providing practical guidance for its deployment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.23315</link>
<guid>https://arxiv.org/abs/2511.23315</guid>
<content:encoded><![CDATA[

arXiv:2511.23315v1 Announce Type: new 
Abstract: A clearer understanding of when coordination emerges, fluctuates, or collapses in decentralized multi-agent reinforcement learning (MARL) is increasingly sought in order to characterize the dynamics of multi-agent learning systems. We revisit fully independent Q-learning (IQL) as a minimal decentralized testbed and run large-scale experiments across environment size L and agent density rho. We construct a phase map using two axes - the cooperative success rate (CSR) and a stability index derived from TD-error variance - revealing three distinct regimes: a coordinated and stable phase, a fragile transition region, and a jammed or disordered phase. A sharp double Instability Ridge separates these regimes and corresponds to persistent kernel drift, the time-varying shift of each agent's effective transition kernel induced by others' policy updates. Synchronization analysis further shows that temporal alignment is required for sustained cooperation, and that competition between drift and synchronization generates the fragile regime. Removing agent identifiers eliminates drift entirely and collapses the three-phase structure, demonstrating that small inter-agent asymmetries are a necessary driver of drift. Overall, the results show that decentralized MARL exhibits a coherent phase structure governed by the interaction between scale, density, and kernel drift, suggesting that emergent coordination behaves as a distribution-interaction-driven phase phenomenon.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParaGate: Parasitic-Driven Domain Adaptation Transfer Learning for Netlist Performance Prediction</title>
<link>https://arxiv.org/abs/2511.23340</link>
<guid>https://arxiv.org/abs/2511.23340</guid>
<content:encoded><![CDATA[

arXiv:2511.23340v1 Announce Type: new 
Abstract: In traditional EDA flows, layout-level performance metrics are only obtainable after placement and routing, hindering global optimization at earlier stages. Although some neural-network-based solutions predict layout-level performance directly from netlists, they often face generalization challenges due to the black-box heuristics of commercial placement-and-routing tools, which create disparate data across designs. To this end, we propose ParaGate, a three-step cross-stage prediction framework that infers layout-level timing and power from netlists. First, we propose a two-phase transfer-learning approach to predict parasitic parameters, pre-training on mid-scale circuits and fine-tuning on larger ones to capture extreme conditions. Next, we rely on EDA tools for timing analysis, offloading the long-path numerical reasoning. Finally, ParaGate performs global calibration using subgraph features. Experiments show that ParaGate achieves strong generalization with minimal fine-tuning data: on openE906, its arrival-time R2 from 0.119 to 0.897. These results demonstrate that ParaGate could provide guidance for global optimization in the synthesis and placement stages.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Dynamic Associative Memory via Online Convex Optimization</title>
<link>https://arxiv.org/abs/2511.23347</link>
<guid>https://arxiv.org/abs/2511.23347</guid>
<content:encoded><![CDATA[

arXiv:2511.23347v1 Announce Type: new 
Abstract: An associative memory (AM) enables cue-response recall, and it has recently been recognized as a key mechanism underlying modern neural architectures such as Transformers. In this work, we introduce the concept of distributed dynamic associative memory (DDAM), which extends classical AM to settings with multiple agents and time-varying data streams. In DDAM, each agent maintains a local AM that must not only store its own associations but also selectively memorize information from other agents based on a specified interest matrix. To address this problem, we propose a novel tree-based distributed online gradient descent algorithm, termed DDAM-TOGD, which enables each agent to update its memory on the fly via inter-agent communication over designated routing trees. We derive rigorous performance guarantees for DDAM-TOGD, proving sublinear static regret in stationary environments and a path-length dependent dynamic regret bound in non-stationary environments. These theoretical results provide insights into how communication delays and network structure impact performance. Building on the regret analysis, we further introduce a combinatorial tree design strategy that optimizes the routing trees to minimize communication delays, thereby improving regret bounds. Numerical experiments demonstrate that the proposed DDAM-TOGD framework achieves superior accuracy and robustness compared to representative online learning baselines such as consensus-based distributed optimization, confirming the benefits of the proposed approach in dynamic, distributed environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning-Augmented Online Bipartite Matching in the Random Arrival Order Model</title>
<link>https://arxiv.org/abs/2511.23388</link>
<guid>https://arxiv.org/abs/2511.23388</guid>
<content:encoded><![CDATA[

arXiv:2511.23388v1 Announce Type: new 
Abstract: We study the online unweighted bipartite matching problem in the random arrival order model, with $n$ offline and $n$ online vertices, in the learning-augmented setting: The algorithm is provided with untrusted predictions of the types (neighborhoods) of the online vertices. We build upon the work of Choo et al. (ICML 2024, pp. 8762-8781) who proposed an approach that uses a prefix of the arrival sequence as a sample to determine whether the predictions are close to the true arrival sequence and then either follows the predictions or uses a known baseline algorithm that ignores the predictions and is $\beta$-competitive. Their analysis is limited to the case that the optimal matching has size $n$, i.e., every online vertex can be matched. We generalize their approach and analysis by removing any assumptions on the size of the optimal matching while only requiring that the size of the predicted matching is at least $\alpha n$ for any constant $0 < \alpha \le 1$. Our learning-augmented algorithm achieves $(1-o(1))$-consistency and $(\beta-o(1))$-robustness. Additionally, we show that the competitive ratio degrades smoothly between consistency and robustness with increasing prediction error.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantized-Tinyllava: a new multimodal foundation model enables efficient split learning</title>
<link>https://arxiv.org/abs/2511.23402</link>
<guid>https://arxiv.org/abs/2511.23402</guid>
<content:encoded><![CDATA[

arXiv:2511.23402v1 Announce Type: new 
Abstract: Split learning is well known as a method for resolving data privacy concerns by training a model on distributed devices, thereby avoiding data sharing that raises privacy issues. However, high network communication costs are always an impediment to split learning, especially for large foundation models that require transmitting large amounts of high-dimensional data. To resolve this issue, we present a new multimodal model structure that incorporates a learning-based data compression method, which compresses model embeddings into low-bit integers while preserving the model's performance, greatly reducing the transmission costs between partitions. We then determine the optimal number of discrete representation levels based on a solid theoretical foundation from entropy coding.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LFM2 Technical Report</title>
<link>https://arxiv.org/abs/2511.23404</link>
<guid>https://arxiv.org/abs/2511.23404</guid>
<content:encoded><![CDATA[

arXiv:2511.23404v1 Announce Type: new 
Abstract: We present LFM2, a family of Liquid Foundation Models designed for efficient on-device deployment and strong task capabilities. Using hardware-in-the-loop architecture search under edge latency and memory constraints, we obtain a compact hybrid backbone that combines gated short convolutions with a small number of grouped query attention blocks, delivering up to 2x faster prefill and decode on CPUs compared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, including dense models (350M, 700M, 1.2B, 2.6B) and a mixture-of-experts variant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes a tempered, decoupled Top-K knowledge distillation objective that avoids support mismatch; curriculum learning with difficulty-ordered data; and a three-stage post-training recipe of supervised fine-tuning, length-normalized preference optimization, and model merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% on IFEval and 82.41% on GSM8K. We further build multimodal and retrieval variants: LFM2-VL for vision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs via token-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enable real-time speech-to-speech interaction competitive with models 3x larger. LFM2-ColBERT provides a low-latency encoder for queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages for ExecuTorch, llama.cpp, and vLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation</title>
<link>https://arxiv.org/abs/2511.23440</link>
<guid>https://arxiv.org/abs/2511.23440</guid>
<content:encoded><![CDATA[

arXiv:2511.23440v1 Announce Type: new 
Abstract: Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts</title>
<link>https://arxiv.org/abs/2511.23442</link>
<guid>https://arxiv.org/abs/2511.23442</guid>
<content:encoded><![CDATA[

arXiv:2511.23442v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Benefits of Sinusoidal Activation for Modular Addition</title>
<link>https://arxiv.org/abs/2511.23443</link>
<guid>https://arxiv.org/abs/2511.23443</guid>
<content:encoded><![CDATA[

arXiv:2511.23443v1 Announce Type: new 
Abstract: This paper studies the role of activation functions in learning modular addition with two-layer neural networks. We first establish a sharp expressivity gap: sine MLPs admit width-$2$ exact realizations for any fixed length $m$ and, with bias, width-$2$ exact realizations uniformly over all lengths. In contrast, the width of ReLU networks must scale linearly with $m$ to interpolate, and they cannot simultaneously fit two lengths with different residues modulo $p$. We then provide a novel Natarajan-dimension generalization bound for sine networks, yielding nearly optimal sample complexity $\widetilde{\mathcal{O}}(p)$ for ERM over constant-width sine networks. We also derive width-independent, margin-based generalization for sine networks in the overparametrized regime and validate it. Empirically, sine networks generalize consistently better than ReLU networks across regimes and exhibit strong length extrapolation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Thermophysical Property Retrieval</title>
<link>https://arxiv.org/abs/2511.23449</link>
<guid>https://arxiv.org/abs/2511.23449</guid>
<content:encoded><![CDATA[

arXiv:2511.23449v1 Announce Type: new 
Abstract: Inverse heat problems refer to the estimation of material thermophysical properties given observed or known heat diffusion behaviour. Inverse heat problems have wide-ranging uses, but a critical application lies in quantifying how building facade renovation reduces thermal transmittance, a key determinant of building energy efficiency. However, solving inverse heat problems with non-invasive data collected in situ is error-prone due to environmental variability or deviations from theoretically assumed conditions. Hence, current methods for measuring thermal conductivity are either invasive, require lengthy observation periods, or are sensitive to environmental and experimental conditions. Here, we present a PINN-based iterative framework to estimate the thermal conductivity k of a wall from a set of thermographs; our framework alternates between estimating the forward heat problem with a PINN for a fixed k, and optimizing k by comparing the thermographs and surface temperatures predicted by the PINN, repeating until the estimated k's convergence. Using both environmental data captured by a weather station and data generated from Finite-Volume-Method software simulations, we accurately predict k across different environmental conditions and data collection sampling times, given the temperature profile of the wall at dawn is close to steady state. Although violating the steady-state assumption impacts the accuracy of k's estimation, we show that our proposed framework still only exhibits a maximum MAE of 4.0851. Our work demonstrates the potential of PINN-based methods for reliable estimation of material properties in situ and under realistic conditions, without lengthy measurement campaigns. Given the lack of research on using machine learning, and more specifically on PINNs, for solving in-situ inverse problems, we expect our work to be a starting point for more research on the topic.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference</title>
<link>https://arxiv.org/abs/2511.23455</link>
<guid>https://arxiv.org/abs/2511.23455</guid>
<content:encoded><![CDATA[

arXiv:2511.23455v1 Announce Type: new 
Abstract: Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a warped picture of progress in practical capabilities per dollar. To remedy this, we use data from Artificial Analysis and Epoch AI to form the largest dataset of current and historical prices to run benchmarks to date. We find that the price for a given level of benchmark performance has decreased remarkably fast, around $5\times$ to $10\times$ per year, for frontier models on knowledge, reasoning, math, and software engineering benchmarks. These reductions in the cost of AI inference are due to economic forces, hardware efficiency improvements, and algorithmic efficiency improvements. Isolating out open models to control for competition effects and dividing by hardware price declines, we estimate that algorithmic efficiency progress is around $3\times$ per year. Finally, we recommend that evaluators both publicize and take into account the price of benchmarking as an essential part of measuring the real-world impact of AI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmallWorlds: Assessing Dynamics Understanding of World Models in Isolated Environments</title>
<link>https://arxiv.org/abs/2511.23465</link>
<guid>https://arxiv.org/abs/2511.23465</guid>
<content:encoded><![CDATA[

arXiv:2511.23465v1 Announce Type: new 
Abstract: Current world models lack a unified and controlled setting for systematic evaluation, making it difficult to assess whether they truly capture the underlying rules that govern environment dynamics. In this work, we address this open challenge by introducing the SmallWorld Benchmark, a testbed designed to assess world model capability under isolated and precisely controlled dynamics without relying on handcrafted reward signals. Using this benchmark, we conduct comprehensive experiments in the fully observable state space on representative architectures including Recurrent State Space Model, Transformer, Diffusion model, and Neural ODE, examining their behavior across six distinct domains. The experimental results reveal how effectively these models capture environment structure and how their predictions deteriorate over extended rollouts, highlighting both the strengths and limitations of current modeling paradigms and offering insights into future improvement directions in representation learning and dynamics modeling.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThetaEvolve: Test-time Learning on Open Problems</title>
<link>https://arxiv.org/abs/2511.23473</link>
<guid>https://arxiv.org/abs/2511.23473</guid>
<content:encoded><![CDATA[

arXiv:2511.23473v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Role of Preference Variance in Preference Optimization</title>
<link>https://arxiv.org/abs/2510.13022</link>
<guid>https://arxiv.org/abs/2510.13022</guid>
<content:encoded><![CDATA[

arXiv:2510.13022v1 Announce Type: cross 
Abstract: Direct Preference Optimization (DPO) has emerged as an important approach for learning from human preferences in aligning large language models (LLMs). However, collecting human preference data is costly and inefficient, motivating methods to reduce the required annotations. In this work, we investigate the impact of \emph{preference variance} (PVar), which measures the variance in model preferences when comparing pairs of responses, on the effectiveness of DPO training. We provide a theoretical insight by establishing an upper bound on the DPO gradient norm for any given prompt, showing it is controlled by the PVar of that prompt. This implies that prompts with low PVar can only produce small gradient updates, making them less valuable for learning. We validate this finding by fine-tuning LLMs with preferences generated by a reward model, evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental results demonstrate that prompts with higher PVar outperform randomly selected prompts or those with lower PVar. We also show that our PVar-based selection method is robust, when using smaller reward models (1B, 3B) for selection. Notably, in a separate experiment using the original human annotations from the UltraFeedback dataset, we found that training on only the top 10\% of prompts with the highest PVar yields better evaluation performance than training on the full dataset, highlighting the importance of preference variance in identifying informative examples for efficient LLM alignment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion</title>
<link>https://arxiv.org/abs/2511.21542</link>
<guid>https://arxiv.org/abs/2511.21542</guid>
<content:encoded><![CDATA[

arXiv:2511.21542v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations</title>
<link>https://arxiv.org/abs/2511.21701</link>
<guid>https://arxiv.org/abs/2511.21701</guid>
<content:encoded><![CDATA[

arXiv:2511.21701v1 Announce Type: cross 
Abstract: The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Embedding Generalization: How LLMs, LoRA, and SLERP Shape Representational Geometry</title>
<link>https://arxiv.org/abs/2511.21703</link>
<guid>https://arxiv.org/abs/2511.21703</guid>
<content:encoded><![CDATA[

arXiv:2511.21703v1 Announce Type: cross 
Abstract: We investigate the generalization properties of dense text embeddings when the embedding backbone is a large language model (LLM) versus when it is a non-LLM encoder, and we study the extent to which spherical linear interpolation (SLERP) model-merging mitigates over-specialization introduced by task-specific adaptation (e.g., LoRA). To make the comparison concrete and domain-agnostic, we design a controlled suite of experiments in which models embed short numerical sequences and are evaluated on their ability to cluster and classify those sequences according to well-defined number-theoretic properties. Our experimental protocol compares four families of models: (1) non-LLM encoders trained from scratch or fine-tuned for embeddings, (2) LLM-based encoders adapted with parameter-efficient methods (LoRA), (3) LLM-based encoders with LoRA followed by model souping merging into the base weights, and (4) the same LoRA-adapted LLMs merged using SLERP across checkpoints or stages. We evaluate representational quality with clustering indices (Silhouette and Davies Bouldin). We additionally analyze the use of kmeans labels to see if the embeddings encode any other information besides the one we are testing for. Empirically, we find that LLM-based backbones produce embeddings that better capture higher-order, compositional numeric patterns, but are prone to adapter dominance that degrades balanced generalization; SLERP merging consistently recovers base-model structure while retaining most task gains, yielding superior tradeoffs in clustering separability, and robustness compared to model souping or models that were not merged.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing Stereotypes in Large Language Models: A Critical Examination and Mitigation</title>
<link>https://arxiv.org/abs/2511.21711</link>
<guid>https://arxiv.org/abs/2511.21711</guid>
<content:encoded><![CDATA[

arXiv:2511.21711v1 Announce Type: cross 
Abstract: Large Language models (LLMs), such as ChatGPT, have gained popularity in recent years with the advancement of Natural Language Processing (NLP), with use cases spanning many disciplines and daily lives as well. LLMs inherit explicit and implicit biases from the datasets they were trained on; these biases can include social, ethical, cultural, religious, and other prejudices and stereotypes. It is important to comprehensively examine such shortcomings by identifying the existence and extent of such biases, recognizing the origin, and attempting to mitigate such biased outputs to ensure fair outputs to reduce harmful stereotypes and misinformation. This study inspects and highlights the need to address biases in LLMs amid growing generative Artificial Intelligence (AI). We utilize bias-specific benchmarks such StereoSet and CrowSPairs to evaluate the existence of various biases in many different generative models such as BERT, GPT 3.5, and ADA. To detect both explicit and implicit biases, we adopt a three-pronged approach for thorough and inclusive analysis. Results indicate fine-tuned models struggle with gender biases but excel at identifying and avoiding racial biases. Our findings also illustrated that despite some cases of success, LLMs often over-rely on keywords in prompts and its outputs. This demonstrates the incapability of LLMs to attempt to truly understand the accuracy and authenticity of its outputs. Finally, in an attempt to bolster model performance, we applied an enhancement learning strategy involving fine-tuning, models using different prompting techniques, and data augmentation of the bias benchmarks. We found fine-tuned models to exhibit promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DNNs, Dataset Statistics, and Correlation Functions</title>
<link>https://arxiv.org/abs/2511.21715</link>
<guid>https://arxiv.org/abs/2511.21715</guid>
<content:encoded><![CDATA[

arXiv:2511.21715v1 Announce Type: cross 
Abstract: This paper argues that dataset structure is important in image recognition tasks (among other tasks). Specifically, we focus on the nature and genesis of correlational structure in the actual datasets upon which DNNs are trained. We argue that DNNs are implementing a widespread methodology in condensed matter physics and materials science that focuses on mesoscale correlation structures that live between fundamental atomic/molecular scales and continuum scales. Specifically, we argue that DNNs that are successful in image classification must be discovering high order correlation functions. It is well-known that DNNs successfully generalize in apparent contravention of standard statistical learning theory. We consider the implications of our discussion for this puzzle.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PeerCoPilot: A Language Model-Powered Assistant for Behavioral Health Organizations</title>
<link>https://arxiv.org/abs/2511.21721</link>
<guid>https://arxiv.org/abs/2511.21721</guid>
<content:encoded><![CDATA[

arXiv:2511.21721v1 Announce Type: cross 
Abstract: Behavioral health conditions, which include mental health and substance use disorders, are the leading disease burden in the United States. Peer-run behavioral health organizations (PROs) critically assist individuals facing these conditions by combining mental health services with assistance for needs such as income, employment, and housing. However, limited funds and staffing make it difficult for PROs to address all service user needs. To assist peer providers at PROs with their day-to-day tasks, we introduce PeerCoPilot, a large language model (LLM)-powered assistant that helps peer providers create wellness plans, construct step-by-step goals, and locate organizational resources to support these goals. PeerCoPilot ensures information reliability through a retrieval-augmented generation pipeline backed by a large database of over 1,300 vetted resources. We conducted human evaluations with 15 peer providers and 6 service users and found that over 90% of users supported using PeerCoPilot. Moreover, we demonstrated that PeerCoPilot provides more reliable and specific information than a baseline LLM. PeerCoPilot is now used by a group of 5-10 peer providers at CSPNJ, a large behavioral health organization serving over 10,000 service users, and we are actively expanding PeerCoPilot's use.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks</title>
<link>https://arxiv.org/abs/2511.21726</link>
<guid>https://arxiv.org/abs/2511.21726</guid>
<content:encoded><![CDATA[

arXiv:2511.21726v1 Announce Type: cross 
Abstract: How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue</title>
<link>https://arxiv.org/abs/2511.21728</link>
<guid>https://arxiv.org/abs/2511.21728</guid>
<content:encoded><![CDATA[

arXiv:2511.21728v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark for Procedural Memory Retrieval in Language Agents</title>
<link>https://arxiv.org/abs/2511.21730</link>
<guid>https://arxiv.org/abs/2511.21730</guid>
<content:encoded><![CDATA[

arXiv:2511.21730v1 Announce Type: cross 
Abstract: Current AI agents excel in familiar settings, but fail sharply when faced with novel tasks with unseen vocabularies -- a core limitation of procedural memory systems. We present the first benchmark that isolates procedural memory retrieval from task execution, evaluating whether agents can recognize functionally equivalent procedures that span different object instantiations. Using ALFWorld, we construct dual corpora of expert and LLM-generated trajectories and evaluate six retrieval methods using systematically stratified queries. Our results expose a clear generalization cliff: embedding-based methods perform strongly on familiar contexts, yet degrade considerably on novel ones, while LLM-generated procedural abstractions demonstrate reliable cross-context transfer. Controlled ablations show that although embeddings capture some lexical-level abstraction, they fundamentally treat procedures as unordered bags of words, discarding temporal structure necessary for cross-context transfer. Corpus scale delivers far larger gains than representation enrichment, revealing an architectural ceiling in current encoders. Our benchmark offers the first diagnostic framework separating genuine procedural understanding from surface-level memorization and gives tools for developing retrieval systems capable of dependable generalization. Resources available at our GitHub repository (https://github.com/qpiai/Proced_mem_bench).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multiscale Geometric Method for Capturing Relational Topic Alignment</title>
<link>https://arxiv.org/abs/2511.21741</link>
<guid>https://arxiv.org/abs/2511.21741</guid>
<content:encoded><![CDATA[

arXiv:2511.21741v1 Announce Type: cross 
Abstract: Interpretable topic modeling is essential for tracking how research interests evolve within co-author communities. In scientific corpora, where novelty is prized, identifying underrepresented niche topics is particularly important. However, contemporary models built from dense transformer embeddings tend to miss rare topics and therefore also fail to capture smooth temporal alignment. We propose a geometric method that integrates multimodal text and co-author network data, using Hellinger distances and Ward's linkage to construct a hierarchical topic dendrogram. This approach captures both local and global structure, supporting multiscale learning across semantic and temporal dimensions. Our method effectively identifies rare-topic structure and visualizes smooth topic drift over time. Experiments highlight the strength of interpretable bag-of-words models when paired with principled geometric alignment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuantumChem-200K: A Large-Scale Open Organic Molecular Dataset for Quantum-Chemistry Property Screening and Language Model Benchmarking</title>
<link>https://arxiv.org/abs/2511.21747</link>
<guid>https://arxiv.org/abs/2511.21747</guid>
<content:encoded><![CDATA[

arXiv:2511.21747v1 Announce Type: cross 
Abstract: The discovery of next-generation photoinitiators for two-photon polymerization (TPP) is hindered by the absence of large, open datasets containing the quantum-chemical and photophysical properties required to model photodissociation and excited-state behavior. Existing molecular datasets typically provide only basic physicochemical descriptors and therefore cannot support data-driven screening or AI-assisted design of photoinitiators. To address this gap, we introduce QuantumChem-200K, a large-scale dataset of over 200,000 organic molecules annotated with eleven quantum-chemical properties, including two-photon absorption (TPA) cross sections, TPA spectral ranges, singlet-triplet intersystem crossing (ISC) energies, toxicity and synthetic accessibility scores, hydrophilicity, solubility, boiling point, molecular weight, and aromaticity. These values are computed using a hybrid workflow that integrates density function theory (DFT), semi-empirical excited-state methods, atomistic quantum solvers, and neural-network predictors. Using QuantumChem-200K, we fine tune the open-source Qwen2.5-32B large language model to create a chemistry AI assistant capable of forward property prediction from SMILES. Benchmarking on 3000 unseen molecules from VQM24 and ZINC20 demonstrates that domain-specific fine-tuning significantly improves accuracy over GPT-4o, Llama-3.1-70B, and the base Qwen2.5-32B model, particularly for TPA and ISC predictions central to photoinitiator design. QuantumChem-200K and the corresponding AI assistant together provide the first scalable platform for high-throughput, LLM-driven photoinitiator screening and accelerated discovery of photosensitive materials.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models</title>
<link>https://arxiv.org/abs/2511.21759</link>
<guid>https://arxiv.org/abs/2511.21759</guid>
<content:encoded><![CDATA[

arXiv:2511.21759v1 Announce Type: cross 
Abstract: Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Statistical and Machine Learning Platform for Biological Research</title>
<link>https://arxiv.org/abs/2511.21770</link>
<guid>https://arxiv.org/abs/2511.21770</guid>
<content:encoded><![CDATA[

arXiv:2511.21770v1 Announce Type: cross 
Abstract: Research increasingly relies on computational methods to analyze experimental data and predict molecular properties. Current approaches often require researchers to use a variety of tools for statistical analysis and machine learning, creating workflow inefficiencies. We present an integrated platform that combines classical statistical methods with Random Forest classification for comprehensive data analysis that can be used in the biological sciences. The platform implements automated hyperparameter optimization, feature importance analysis, and a suite of statistical tests including t tests, ANOVA, and Pearson correlation analysis. Our methodology addresses the gap between traditional statistical software, modern machine learning frameworks and biology, by providing a unified interface accessible to researchers without extensive programming experience. The system achieves this through automatic data preprocessing, categorical encoding, and adaptive model configuration based on dataset characteristics. Initial testing protocols are designed to evaluate classification accuracy across diverse chemical datasets with varying feature distributions. This work demonstrates that integrating statistical rigor with machine learning interpretability can accelerate biological discovery workflows while maintaining methodological soundness. The platform's modular architecture enables future extensions to additional machine learning algorithms and statistical procedures relevant to bioinformatics.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Membership: Limitations of Add/Remove Adjacency in Differential Privacy</title>
<link>https://arxiv.org/abs/2511.21804</link>
<guid>https://arxiv.org/abs/2511.21804</guid>
<content:encoded><![CDATA[

arXiv:2511.21804v1 Announce Type: cross 
Abstract: Training machine learning models with differential privacy (DP) limits an adversary's ability to infer sensitive information about the training data. It can be interpreted as a bound on adversary's capability to distinguish two adjacent datasets according to chosen adjacency relation. In practice, most DP implementations use the add/remove adjacency relation, where two datasets are adjacent if one can be obtained from the other by adding or removing a single record, thereby protecting membership. In many ML applications, however, the goal is to protect attributes of individual records (e.g., labels used in supervised fine-tuning). We show that privacy accounting under add/remove overstates attribute privacy compared to accounting under the substitute adjacency relation, which permits substituting one record. To demonstrate this gap, we develop novel attacks to audit DP under substitute adjacency, and show empirically that audit results are inconsistent with DP guarantees reported under add/remove, yet remain consistent with the budget accounted under the substitute adjacency relation. Our results highlight that the choice of adjacency when reporting DP guarantees is critical when the protection target is per-record attributes rather than membership.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLAWS: A Benchmark for Error Identification and Localization in Scientific Papers</title>
<link>https://arxiv.org/abs/2511.21843</link>
<guid>https://arxiv.org/abs/2511.21843</guid>
<content:encoded><![CDATA[

arXiv:2511.21843v1 Announce Type: cross 
Abstract: The identification and localization of errors is a core task in peer review, yet the exponential growth of scientific output has made it increasingly difficult for human reviewers to reliably detect errors given the limited pool of experts. Recent advances in Large Language Models (LLMs) have sparked interest in their potential to support such evaluation tasks, from academic peer review to automated scientific assessment. However, despite the growing use of LLMs in review systems, their capabilities to pinpoint errors remain underexplored. In this work, we introduce Fault Localization Across Writing in Science (FLAWS), an automated benchmark consisting of 713 paper-error pairs designed to evaluate how effectively LLMs detect errors that undermine key claims in research papers. We construct the benchmark by systematically inserting claim-invalidating errors into peer-reviewed papers using LLMs, paired with an automated evaluation metric that measures whether models can identify and localize these errors. Developing such a benchmark presents unique challenges that we overcome: ensuring that the inserted errors are well-defined, challenging, and relevant to the content of the paper, avoiding artifacts that would make identification trivial, and designing a scalable, automated evaluation metric. On the resulting benchmark, we evaluate five frontier LLMs: Claude Sonnet 4.5, DeepSeek Reasoner v3.1, Gemini 2.5 Pro, GPT 5, and Grok 4. Among these, GPT 5 is the top-performing model, achieving 39.1% identification accuracy when k=10, where k is the number of top-ranked error text candidates generated by the LLM.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LILAD: Learning In-context Lyapunov-stable Adaptive Dynamics Models</title>
<link>https://arxiv.org/abs/2511.21846</link>
<guid>https://arxiv.org/abs/2511.21846</guid>
<content:encoded><![CDATA[

arXiv:2511.21846v1 Announce Type: cross 
Abstract: System identification in control theory aims to approximate dynamical systems from trajectory data. While neural networks have demonstrated strong predictive accuracy, they often fail to preserve critical physical properties such as stability and typically assume stationary dynamics, limiting their applicability under distribution shifts. Existing approaches generally address either stability or adaptability in isolation, lacking a unified framework that ensures both. We propose LILAD (Learning In-Context Lyapunov-stable Adaptive Dynamics), a novel framework for system identification that jointly guarantees adaptability and stability. LILAD simultaneously learns a dynamics model and a Lyapunov function through in-context learning (ICL), explicitly accounting for parametric uncertainty. Trained across a diverse set of tasks, LILAD produces a stability-aware, adaptive dynamics model alongside an adaptive Lyapunov certificate. At test time, both components adapt to a new system instance using a short trajectory prompt, which enables fast generalization. To rigorously ensure stability, LILAD also computes a state-dependent attenuator that enforces a sufficient decrease condition on the Lyapunov function for any state in the new system instance. This mechanism extends stability guarantees even under out-of-distribution and out-of-task scenarios. We evaluate LILAD on benchmark autonomous systems and demonstrate that it outperforms adaptive, robust, and non-adaptive baselines in predictive accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saddle-Free Guidance: Improved On-Manifold Sampling without Labels or Additional Training</title>
<link>https://arxiv.org/abs/2511.21863</link>
<guid>https://arxiv.org/abs/2511.21863</guid>
<content:encoded><![CDATA[

arXiv:2511.21863v1 Announce Type: cross 
Abstract: Score-based generative models require guidance in order to generate plausible, on-manifold samples. The most popular guidance method, Classifier-Free Guidance (CFG), is only applicable in settings with labeled data and requires training an additional unconditional score-based model. More recently, Auto-Guidance adopts a smaller, less capable version of the original model to guide generation. While each method effectively promotes the fidelity of generated data, each requires labeled data or the training of additional models, making it challenging to guide score-based models when (labeled) training data are not available or training new models is not feasible.
  We make the surprising discovery that the positive curvature of log density estimates in saddle regions provides strong guidance for score-based models. Motivated by this, we develop saddle-free guidance (SFG) which maintains estimates of maximal positive curvature of the log density to guide individual score-based models. SFG has the same computational cost of classifier-free guidance, does not require additional training, and works with off-the-shelf diffusion and flow matching models. Our experiments indicate that SFG achieves state-of-the-art FID and FD-DINOv2 metrics in single-model unconditional ImageNet-512 generation. When SFG is combined with Auto-Guidance, its unconditional samples achieve general state-of-the-art in FD-DINOv2 score. Our experiments with FLUX.1-dev and Stable Diffusion v3.5 indicate that SFG boosts the diversity of output images compared to CFG while maintaining excellent prompt adherence and image fidelity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Invited to Develop: Institutional Belonging and the Counterfactual Architecture of Development</title>
<link>https://arxiv.org/abs/2511.21865</link>
<guid>https://arxiv.org/abs/2511.21865</guid>
<content:encoded><![CDATA[

arXiv:2511.21865v1 Announce Type: cross 
Abstract: This paper examines how institutional belonging shapes long-term development by comparing Spain and Uruguay, two small democracies with similar historical endowments whose trajectories diverged sharply after the 1960s. While Spain integrated into dense European institutional architectures, Uruguay remained embedded within the Latin American governance regime, characterized by weaker coordination and lower institutional coherence. To assess how alternative institutional embeddings could have altered these paths, the study develops a generative counterfactual framework grounded in economic complexity, institutional path dependence, and a Wasserstein GAN trained on data from 1960-2020. The resulting Expected Developmental Shift (EDS) quantifies structural gains or losses from hypothetical re-embedding in different institutional ecosystems. Counterfactual simulations indicate that Spain would have experienced significant developmental decline under a Latin American configuration, while Uruguay would have achieved higher complexity and resilience within a European regime. These findings suggest that development is not solely determined by domestic reforms but emerges from a country's structural position within transnational institutional networks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Marine Bioacoustics with Deep Generative Models: A Hybrid Augmentation Strategy for Southern Resident Killer Whale Detection</title>
<link>https://arxiv.org/abs/2511.21872</link>
<guid>https://arxiv.org/abs/2511.21872</guid>
<content:encoded><![CDATA[

arXiv:2511.21872v1 Announce Type: cross 
Abstract: Automated detection and classification of marine mammals vocalizations is critical for conservation and management efforts but is hindered by limited annotated datasets and the acoustic complexity of real-world marine environments. Data augmentation has proven to be an effective strategy to address this limitation by increasing dataset diversity and improving model generalization without requiring additional field data. However, most augmentation techniques used to date rely on effective but relatively simple transformations, leaving open the question of whether deep generative models can provide additional benefits. In this study, we evaluate the potential of deep generative for data augmentation in marine mammal call detection including: Variational Autoencoders, Generative Adversarial Networks, and Denoising Diffusion Probabilistic Models. Using Southern Resident Killer Whale (Orcinus orca) vocalizations from two long-term hydrophone deployments in the Salish Sea, we compare these approaches against traditional augmentation methods such as time-shifting and vocalization masking. While all generative approaches improved classification performance relative to the baseline, diffusion-based augmentation yielded the highest recall (0.87) and overall F1-score (0.75). A hybrid strategy combining generative-based synthesis with traditional methods achieved the best overall performance with an F1-score of 0.81. We hope this study encourages further exploration of deep generative models as complementary augmentation strategies to advance acoustic monitoring of threatened marine mammal populations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differential privacy from axioms</title>
<link>https://arxiv.org/abs/2511.21876</link>
<guid>https://arxiv.org/abs/2511.21876</guid>
<content:encoded><![CDATA[

arXiv:2511.21876v1 Announce Type: cross 
Abstract: Differential privacy (DP) is the de facto notion of privacy both in theory and in practice. However, despite its popularity, DP imposes strict requirements which guard against strong worst-case scenarios. For example, it guards against seemingly unrealistic scenarios where an attacker has full information about all but one point in the data set, and still nothing can be learned about the remaining point. While preventing such a strong attack is desirable, many works have explored whether average-case relaxations of DP are easier to satisfy [HWR13,WLF16,BF16,LWX23].
  In this work, we are motivated by the question of whether alternate, weaker notions of privacy are possible: can a weakened privacy notion still guarantee some basic level of privacy, and on the other hand, achieve privacy more efficiently and/or for a substantially broader set of tasks? Our main result shows the answer is no: even in the statistical setting, any reasonable measure of privacy satisfying nontrivial composition is equivalent to DP. To prove this, we identify a core set of four axioms or desiderata: pre-processing invariance, prohibition of blatant non-privacy, strong composition, and linear scalability. Our main theorem shows that any privacy measure satisfying our axioms is equivalent to DP, up to polynomial factors in sample complexity. We complement this result by showing our axioms are minimal: removing any one of our axioms enables ill-behaved measures of privacy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Multiple Kernel Learning: Alternating Best Response and Semidefinite Relaxations</title>
<link>https://arxiv.org/abs/2511.21890</link>
<guid>https://arxiv.org/abs/2511.21890</guid>
<content:encoded><![CDATA[

arXiv:2511.21890v1 Announce Type: cross 
Abstract: We study Sparse Multiple Kernel Learning (SMKL), which is the problem of selecting a sparse convex combination of prespecified kernels for support vector binary classification. Unlike prevailing l1 regularized approaches that approximate a sparsifying penalty, we formulate the problem by imposing an explicit cardinality constraint on the kernel weights and add an l2 penalty for robustness. We solve the resulting non-convex minimax problem via an alternating best response algorithm with two subproblems: the alpha subproblem is a standard kernel SVM dual solved via LIBSVM, while the beta subproblem admits an efficient solution via the Greedy Selector and Simplex Projector algorithm. We reformulate SMKL as a mixed integer semidefinite optimization problem and derive a hierarchy of semidefinite convex relaxations which can be used to certify near-optimality of the solutions returned by our best response algorithm and also to warm start it. On ten UCI benchmarks, our method with random initialization outperforms state-of-the-art MKL approaches in out-of-sample prediction accuracy on average by 3.34 percentage points (relative to the best performing benchmark) while selecting a small number of candidate kernels in comparable runtime. With warm starting, our method outperforms the best performing benchmark's out-of-sample prediction accuracy on average by 4.05 percentage points. Our convex relaxations provide a certificate that in several cases, the solution returned by our best response algorithm is the globally optimal solution.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Parameter Optimization for Robust Remote Photoplethysmography</title>
<link>https://arxiv.org/abs/2511.21903</link>
<guid>https://arxiv.org/abs/2511.21903</guid>
<content:encoded><![CDATA[

arXiv:2511.21903v1 Announce Type: cross 
Abstract: Remote photoplethysmography (rPPG) enables contactless vital sign monitoring using standard RGB cameras. However, existing methods rely on fixed parameters optimized for particular lighting conditions and camera setups, limiting adaptability to diverse deployment environments. This paper introduces the Projection-based Robust Signal Mixing (PRISM) algorithm, a training-free method that jointly optimizes photometric detrending and color mixing through online parameter adaptation based on signal quality assessment. PRISM achieves state-of-the-art performance among unsupervised methods, with MAE of 0.77 bpm on PURE and 0.66 bpm on UBFC-rPPG, and accuracy of 97.3\% and 97.5\% respectively at a 5 bpm threshold. Statistical analysis confirms PRISM performs equivalently to leading supervised methods ($p > 0.2$), while maintaining real-time CPU performance without training. This validates that adaptive time series optimization significantly improves rPPG across diverse conditions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithms and Scientific Software for Quasi-Monte Carlo, Fast Gaussian Process Regression, and Scientific Machine Learning</title>
<link>https://arxiv.org/abs/2511.21915</link>
<guid>https://arxiv.org/abs/2511.21915</guid>
<content:encoded><![CDATA[

arXiv:2511.21915v1 Announce Type: cross 
Abstract: Most scientific domains elicit the development of efficient algorithms and accessible scientific software. This thesis unifies our developments in three broad domains: Quasi-Monte Carlo (QMC) methods for efficient high-dimensional integration, Gaussian process (GP) regression for high-dimensional interpolation with built-in uncertainty quantification, and scientific machine learning (sciML) for modeling partial differential equations (PDEs) with mesh-free solvers. For QMC, we built new algorithms for vectorized error estimation and developed QMCPy (https://qmcsoftware.github.io/QMCSoftware/): an open-source Python interface to randomized low-discrepancy sequence generators, automatic variable transforms, adaptive error estimation procedures, and diverse use cases. For GPs, we derived new digitally-shift-invariant kernels of higher-order smoothness, developed novel fast multitask GP algorithms, and produced the scalable Python software FastGPs (https://alegresor.github.io/fastgps/). For sciML, we developed a new algorithm capable of machine precision recovery of PDEs with random coefficients. We have also studied a number of applications including GPs for probability of failure estimation, multilevel GPs for the Darcy flow equation, neural surrogates for modeling radiative transfer, and fast GPs for Bayesian multilevel QMC.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WalkCLIP: Multimodal Learning for Urban Walkability Prediction</title>
<link>https://arxiv.org/abs/2511.21947</link>
<guid>https://arxiv.org/abs/2511.21947</guid>
<content:encoded><![CDATA[

arXiv:2511.21947v1 Announce Type: cross 
Abstract: Urban walkability is a cornerstone of public health, sustainability, and quality of life. Traditional walkability assessments rely on surveys and field audits, which are costly and difficult to scale. Recent studies have used satellite imagery, street view imagery, or population indicators to estimate walkability, but these single-source approaches capture only one dimension of the walking environment. Satellite data describe the built environment from above, but overlook the pedestrian perspective. Street view imagery captures conditions at the ground level, but lacks broader spatial context. Population dynamics reveal patterns of human activity but not the visual form of the environment. We introduce WalkCLIP, a multimodal framework that integrates these complementary viewpoints to predict urban walkability. WalkCLIP learns walkability-aware vision-language representations from GPT-4o generated image captions, refines these representations with a spatial aggregation module that incorporates neighborhood context, and fuses the resulting features with representations from a population dynamics foundation model. Evaluated at 4,660 locations throughout Minneapolis-Saint Paul, WalkCLIP outperforms unimodal and multimodal baselines in both predictive accuracy and spatial alignment. These results show that the integration of visual and behavioral signals yields reliable predictions of the walking environment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepGI: Explainable Deep Learning for Gastrointestinal Image Classification</title>
<link>https://arxiv.org/abs/2511.21959</link>
<guid>https://arxiv.org/abs/2511.21959</guid>
<content:encoded><![CDATA[

arXiv:2511.21959v1 Announce Type: cross 
Abstract: This paper presents a comprehensive comparative model analysis on a novel gastrointestinal medical imaging dataset, comprised of 4,000 endoscopic images spanning four critical disease classes: Diverticulosis, Neoplasm, Peritonitis, and Ureters. Leveraging state-of-the-art deep learning techniques, the study confronts common endoscopic challenges such as variable lighting, fluctuating camera angles, and frequent imaging artifacts. The best performing models, VGG16 and MobileNetV2, each achieved a test accuracy of 96.5%, while Xception reached 94.24%, establishing robust benchmarks and baselines for automated disease classification. In addition to strong classification performance, the approach includes explainable AI via Grad-CAM visualization, enabling identification of image regions most influential to model predictions and enhancing clinical interpretability. Experimental results demonstrate the potential for robust, accurate, and interpretable medical image analysis even in complex real-world conditions. This work contributes original benchmarks, comparative insights, and visual explanations, advancing the landscape of gastrointestinal computer-aided diagnosis and underscoring the importance of diverse, clinically relevant datasets and model explainability in medical AI research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Elevation Model Estimation from RGB Satellite Imagery using Generative Deep Learning</title>
<link>https://arxiv.org/abs/2511.21985</link>
<guid>https://arxiv.org/abs/2511.21985</guid>
<content:encoded><![CDATA[

arXiv:2511.21985v1 Announce Type: cross 
Abstract: Digital Elevation Models (DEMs) are vital datasets for geospatial applications such as hydrological modeling and environmental monitoring. However, conventional methods to generate DEM, such as using LiDAR and photogrammetry, require specific types of data that are often inaccessible in resource-constrained settings. To alleviate this problem, this study proposes an approach to generate DEM from freely available RGB satellite imagery using generative deep learning, particularly based on a conditional Generative Adversarial Network (GAN). We first developed a global dataset consisting of 12K RGB-DEM pairs using Landsat satellite imagery and NASA's SRTM digital elevation data, both from the year 2000. A unique preprocessing pipeline was implemented to select high-quality, cloud-free regions and aggregate normalized RGB composites from Landsat imagery. Additionally, the model was trained in a two-stage process, where it was first trained on the complete dataset and then fine-tuned on high-quality samples filtered by Structural Similarity Index Measure (SSIM) values to improve performance on challenging terrains. The results demonstrate promising performance in mountainous regions, achieving an overall mean root-mean-square error (RMSE) of 0.4671 and a mean SSIM score of 0.2065 (scale -1 to 1), while highlighting limitations in lowland and residential areas. This study underscores the importance of meticulous preprocessing and iterative refinement in generative modeling for DEM generation, offering a cost-effective and adaptive alternative to conventional methods while emphasizing the challenge of generalization across diverse terrains worldwide.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Sensitivity Approach to Causal Inference Under Limited Overlap</title>
<link>https://arxiv.org/abs/2511.22003</link>
<guid>https://arxiv.org/abs/2511.22003</guid>
<content:encoded><![CDATA[

arXiv:2511.22003v1 Announce Type: cross 
Abstract: Limited overlap between treated and control groups is a key challenge in observational analysis. Standard approaches like trimming importance weights can reduce variance but introduce a fundamental bias. We propose a sensitivity framework for contextualizing findings under limited overlap, where we assess how irregular the outcome function has to be in order for the main finding to be invalidated. Our approach is based on worst-case confidence bounds on the bias introduced by standard trimming practices, under explicit assumptions necessary to extrapolate counterfactual estimates from regions of overlap to those without. Empirically, we demonstrate how our sensitivity framework protects against spurious findings by quantifying uncertainty in regions with limited overlap.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Effect of Regularization on Nonparametric Mean-Variance Regression</title>
<link>https://arxiv.org/abs/2511.22004</link>
<guid>https://arxiv.org/abs/2511.22004</guid>
<content:encoded><![CDATA[

arXiv:2511.22004v1 Announce Type: cross 
Abstract: Uncertainty quantification is vital for decision-making and risk assessment in machine learning. Mean-variance regression models, which predict both a mean and residual noise for each data point, provide a simple approach to uncertainty quantification. However, overparameterized mean-variance models struggle with signal-to-noise ambiguity, deciding whether prediction targets should be attributed to signal (mean) or noise (variance). At one extreme, models fit all training targets perfectly with zero residual noise, while at the other, they provide constant, uninformative predictions and explain the targets as noise. We observe a sharp phase transition between these extremes, driven by model regularization. Empirical studies with varying regularization levels illustrate this transition, revealing substantial variability across repeated runs. To explain this behavior, we develop a statistical field theory framework, which captures the observed phase transition in alignment with experimental results. This analysis reduces the regularization hyperparameter search space from two dimensions to one, significantly lowering computational costs. Experiments on UCI datasets and the large-scale ClimSim dataset demonstrate robust calibration performance, effectively quantifying predictive uncertainty.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2511.22016</link>
<guid>https://arxiv.org/abs/2511.22016</guid>
<content:encoded><![CDATA[

arXiv:2511.22016v1 Announce Type: cross 
Abstract: Existing AI bias evaluation benchmarks largely reflect Western perspectives, leaving African contexts underrepresented and enabling harmful stereotypes in applications across various domains. To address this gap, we introduce AfriStereo, the first open-source African stereotype dataset and evaluation framework grounded in local socio-cultural contexts. Through community engaged efforts across Senegal, Kenya, and Nigeria, we collected 1,163 stereotypes spanning gender, ethnicity, religion, age, and profession. Using few-shot prompting with human-in-the-loop validation, we augmented the dataset to over 5,000 stereotype-antistereotype pairs. Entries were validated through semantic clustering and manual annotation by culturally informed reviewers. Preliminary evaluation of language models reveals that nine of eleven models exhibit statistically significant bias, with Bias Preference Ratios (BPR) ranging from 0.63 to 0.78 (p <= 0.05), indicating systematic preferences for stereotypes over antistereotypes, particularly across age, profession, and gender dimensions. Domain-specific models appeared to show weaker bias in our setup, suggesting task-specific training may mitigate some associations. Looking ahead, AfriStereo opens pathways for future research on culturally grounded bias evaluation and mitigation, offering key methodologies for the AI community on building more equitable, context-aware, and globally inclusive NLP technologies.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResearchArcade: Graph Interface for Academic Tasks</title>
<link>https://arxiv.org/abs/2511.22036</link>
<guid>https://arxiv.org/abs/2511.22036</guid>
<content:encoded><![CDATA[

arXiv:2511.22036v1 Announce Type: cross 
Abstract: Academic research generates diverse data sources, and as researchers increasingly use machine learning to assist research tasks, a crucial question arises: Can we build a unified data interface to support the development of machine learning models for various academic tasks? Models trained on such a unified interface can better support human researchers throughout the research process, eventually accelerating knowledge discovery. In this work, we introduce ResearchArcade, a graph-based interface that connects multiple academic data sources, unifies task definitions, and supports a wide range of base models to address key academic challenges. ResearchArcade utilizes a coherent multi-table format with graph structures to organize data from different sources, including academic corpora from ArXiv and peer reviews from OpenReview, while capturing information with multiple modalities, such as text, figures, and tables. ResearchArcade also preserves temporal evolution at both the manuscript and community levels, supporting the study of paper revisions as well as broader research trends over time. Additionally, ResearchArcade unifies diverse academic task definitions and supports various models with distinct input requirements. Our experiments across six academic tasks demonstrate that combining cross-source and multi-modal information enables a broader range of tasks, while incorporating graph structures consistently improves performance over baseline methods. This highlights the effectiveness of ResearchArcade and its potential to advance research progress.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Support Vector Machine Classifier with Rescaled Huberized Pinball Loss</title>
<link>https://arxiv.org/abs/2511.22065</link>
<guid>https://arxiv.org/abs/2511.22065</guid>
<content:encoded><![CDATA[

arXiv:2511.22065v1 Announce Type: cross 
Abstract: Support vector machines are widely used in machine learning classification tasks, but traditional SVM models suffer from sensitivity to outliers and instability in resampling, which limits their performance in practical applications. To address these issues, this paper proposes a novel rescaled Huberized pinball loss function with asymmetric, non-convex, and smooth properties. Based on this loss function, we develop a corresponding SVM model called RHPSVM (Rescaled Huberized Pinball Loss Support Vector Machine). Theoretical analyses demonstrate that RHPSVM conforms to Bayesian rules, has a strict generalization error bound, a bounded influence function, and controllable optimality conditions, ensuring excellent classification accuracy, outlier insensitivity, and resampling stability. Additionally, RHPSVM can be extended to various advanced SVM variants by adjusting parameters, enhancing its flexibility. We transform the non-convex optimization problem of RHPSVM into a series of convex subproblems using the concave-convex procedure (CCCP) and solve it with the ClipDCD algorithm, which is proven to be convergent. Experimental results on simulated data, UCI datasets, and small-sample crop leaf image classification tasks show that RHPSVM outperforms existing SVM models in both noisy and noise-free scenarios, especially in handling high-dimensional small-sample data.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation</title>
<link>https://arxiv.org/abs/2511.22102</link>
<guid>https://arxiv.org/abs/2511.22102</guid>
<content:encoded><![CDATA[

arXiv:2511.22102v1 Announce Type: cross 
Abstract: MRI-based brain age estimation models aim to assess a subject's biological brain age based on information, such as neuroanatomical features. Various factors, including neurodegenerative diseases, can accelerate brain aging and measuring this phenomena could serve as a potential biomarker for clinical applications. While deep learning (DL)-based regression has recently attracted major attention, existing approaches often fail to capture the continuous nature of neuromorphological changes, potentially resulting in sub-optimal feature representation and results. To address this, we propose to use supervised contrastive learning with the recent Rank-N-Contrast (RNC) loss to estimate brain age based on widely used T1w structural MRI for the first time and leverage Grad-RAM to visually explain regression results. Experiments show that our proposed method achieves a mean absolute error (MAE) of 4.27 years and an $R^2$ of 0.93 with a limited dataset of training samples, significantly outperforming conventional deep regression with the same ResNet backbone while performing better or comparably with the state-of-the-art methods with significantly larger training data. Furthermore, Grad-RAM revealed more nuanced features related to age regression with the RNC loss than conventional deep regression. As an exploratory study, we employed the proposed method to estimate the gap between the biological and chronological brain ages in Alzheimer's Disease and Parkinson's disease patients, and revealed the correlation between the brain age gap and disease severity, demonstrating its potential as a biomarker in neurodegenerative disorders.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous labeling of surgical resection margins using a foundation model</title>
<link>https://arxiv.org/abs/2511.22131</link>
<guid>https://arxiv.org/abs/2511.22131</guid>
<content:encoded><![CDATA[

arXiv:2511.22131v1 Announce Type: cross 
Abstract: Assessing resection margins is central to pathological specimen evaluation and has profound implications for patient outcomes. Current practice employs physical inking, which is applied variably, and cautery artifacts can obscure the true margin on histological sections. We present a virtual inking network (VIN) that autonomously localizes the surgical cut surface on whole-slide images, reducing reliance on inks and standardizing margin-focused review. VIN uses a frozen foundation model as the feature extractor and a compact two-layer multilayer perceptron trained for patch-level classification of cautery-consistent features. The dataset comprised 120 hematoxylin and eosin (H&amp;E) stained slides from 12 human tonsil tissue blocks, resulting in ~2 TB of uncompressed raw image data, where a board-certified pathologist provided boundary annotations. In blind testing with 20 slides from previously unseen blocks, VIN produced coherent margin overlays that qualitatively aligned with expert annotations across serial sections. Quantitatively, region-level accuracy was ~73.3% across the test set, with errors largely confined to limited areas that did not disrupt continuity of the whole-slide margin map. These results indicate that VIN captures cautery-related histomorphology and can provide a reproducible, ink-free margin delineation suitable for integration into routine digital pathology workflows and for downstream measurement of margin distances.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stacked Ensemble of Fine-Tuned CNNs for Knee Osteoarthritis Severity Grading</title>
<link>https://arxiv.org/abs/2511.22143</link>
<guid>https://arxiv.org/abs/2511.22143</guid>
<content:encoded><![CDATA[

arXiv:2511.22143v1 Announce Type: cross 
Abstract: Knee Osteoarthritis (KOA) is a musculoskeletal condition that can cause significant limitations and impairments in daily activities, especially among older individuals. To evaluate the severity of KOA, typically, X-ray images of the affected knee are analyzed, and a grade is assigned based on the Kellgren-Lawrence (KL) grading system, which classifies KOA severity into five levels, ranging from 0 to 4. This approach requires a high level of expertise and time and is susceptible to subjective interpretation, thereby introducing potential diagnostic inaccuracies. To address this problem a stacked ensemble model of fine-tuned Convolutional Neural Networks (CNNs) was developed for two classification tasks: a binary classifier for detecting the presence of KOA, and a multiclass classifier for precise grading across the KL spectrum. The proposed stacked ensemble model consists of a diverse set of pre-trained architectures, including MobileNetV2, You Only Look Once (YOLOv8), and DenseNet201 as base learners and Categorical Boosting (CatBoost) as the meta-learner. This proposed model had a balanced test accuracy of 73% in multiclass classification and 87.5% in binary classification, which is higher than previous works in extant literature.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-PGDN: A Two-level Classification Method for Full-Process Recognition of Newly Registered Pornographic and Gambling Domain Names</title>
<link>https://arxiv.org/abs/2511.22215</link>
<guid>https://arxiv.org/abs/2511.22215</guid>
<content:encoded><![CDATA[

arXiv:2511.22215v1 Announce Type: cross 
Abstract: Online pornography and gambling have consistently posed regulatory challenges for governments, threatening both personal assets and privacy. Therefore, it is imperative to research the classification of the newly registered Pornographic and Gambling Domain Names (PGDN). However, scholarly investigation into this topic is limited. Previous efforts in PGDN classification pursue high accuracy using ideal sample data, while others employ up-to-date data from real-world scenarios but achieve lower classification accuracy. This paper introduces the Real-PGDN method, which accomplishes a complete process of timely and comprehensive real-data crawling, feature extraction with feature-missing tolerance, precise PGDN classification, and assessment of application effects in actual scenarios. Our two-level classifier, which integrates CoSENT (BERT-based), Multilayer Perceptron (MLP), and traditional classification algorithms, achieves a 97.88% precision. The research process amasses the NRD2024 dataset, which contains continuous detection information over 20 days for 1,500,000 newly registered domain names across 6 directions. Results from our case study demonstrate that this method also maintains a forecast precision of over 70% for PGDN that are delayed in usage after registration.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-Consistent Multi-View Editing by Diffusion Guidance</title>
<link>https://arxiv.org/abs/2511.22228</link>
<guid>https://arxiv.org/abs/2511.22228</guid>
<content:encoded><![CDATA[

arXiv:2511.22228v1 Announce Type: cross 
Abstract: Recent advancements in diffusion models have greatly improved text-based image editing, yet methods that edit images independently often produce geometrically and photometrically inconsistent results across different views of the same scene. Such inconsistencies are particularly problematic for editing of 3D representations such as NeRFs or Gaussian Splat models. We propose a training-free diffusion framework that enforces multi-view consistency during the image editing process. The key assumption is that corresponding points in the unedited images should undergo similar transformations after editing. To achieve this, we introduce a consistency loss that guides the diffusion sampling toward coherent edits. The framework is flexible and can be combined with widely varying image editing methods, supporting both dense and sparse multi-view editing setups. Experimental results show that our approach significantly improves 3D consistency compared to existing multi-view editing methods. We also show that this increased consistency enables high-quality Gaussian Splat editing with sharp details and strong fidelity to user-specified text prompts. Please refer to our project page for video results: https://3d-consistent-editing.github.io/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs</title>
<link>https://arxiv.org/abs/2511.22270</link>
<guid>https://arxiv.org/abs/2511.22270</guid>
<content:encoded><![CDATA[

arXiv:2511.22270v1 Announce Type: cross 
Abstract: Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration. Moreover, we identify a concrete learning task where DP-GD can achieve superior generalization performance compared to GD in training two-layer Huberized ReLU convolutional neural networks (CNNs). Specifically, we demonstrate that, under mild conditions, a small signal-to-noise ratio can result in GD producing training models with poor test accuracy, whereas DP-GD can yield training models with good test accuracy and privacy guarantees if the signal-to-noise ratio is not too small. This indicates that DP-GD has the potential to enhance model performance while ensuring privacy protection in certain learning tasks. Numerical simulations are further conducted to support our theoretical results.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UCB for Large-Scale Pure Exploration: Beyond Sub-Gaussianity</title>
<link>https://arxiv.org/abs/2511.22273</link>
<guid>https://arxiv.org/abs/2511.22273</guid>
<content:encoded><![CDATA[

arXiv:2511.22273v1 Announce Type: cross 
Abstract: Selecting the best alternative from a finite set represents a broad class of pure exploration problems. Traditional approaches to pure exploration have predominantly relied on Gaussian or sub-Gaussian assumptions on the performance distributions of all alternatives, which limit their applicability to non-sub-Gaussian especially heavy-tailed problems. The need to move beyond sub-Gaussianity may become even more critical in large-scale problems, which tend to be especially sensitive to distributional specifications. In this paper, motivated by the widespread use of upper confidence bound (UCB) algorithms in pure exploration and beyond, we investigate their performance in the large-scale, non-sub-Gaussian settings. We consider the simplest category of UCB algorithms, where the UCB value for each alternative is defined as the sample mean plus an exploration bonus that depends only on its own sample size. We abstract this into a meta-UCB algorithm and propose letting it select the alternative with the largest sample size as the best upon stopping. For this meta-UCB algorithm, we first derive a distribution-free lower bound on the probability of correct selection. Building on this bound, we analyze two general non-sub-Gaussian scenarios: (1) all alternatives follow a common location-scale structure and have bounded variance; and (2) when such a structure does not hold, each alternative has a bounded absolute moment of order $q > 3$. In both settings, we show that the meta-UCB algorithm and therefore a broad class of UCB algorithms can achieve the sample optimality. These results demonstrate the applicability of UCB algorithms for solving large-scale pure exploration problems with non-sub-Gaussian distributions. Numerical experiments support our results and provide additional insights into the comparative behaviors of UCB algorithms within and beyond our meta-UCB framework.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLA-Grad++: An Improved Griffin-Lim Guided Diffusion Model for Speech Synthesis</title>
<link>https://arxiv.org/abs/2511.22293</link>
<guid>https://arxiv.org/abs/2511.22293</guid>
<content:encoded><![CDATA[

arXiv:2511.22293v1 Announce Type: cross 
Abstract: Recent advances in diffusion models have positioned them as powerful generative frameworks for speech synthesis, demonstrating substantial improvements in audio quality and stability. Nevertheless, their effectiveness in vocoders conditioned on mel spectrograms remains constrained, particularly when the conditioning diverges from the training distribution. The recently proposed GLA-Grad model introduced a phase-aware extension to the WaveGrad vocoder that integrated the Griffin-Lim algorithm (GLA) into the reverse process to reduce inconsistencies between generated signals and conditioning mel spectrogram. In this paper, we further improve GLA-Grad through an innovative choice in how to apply the correction. Particularly, we compute the correction term only once, with a single application of GLA, to accelerate the generation process. Experimental results demonstrate that our method consistently outperforms the baseline models, particularly in out-of-domain scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure is Supervision: Multiview Masked Autoencoders for Radiology</title>
<link>https://arxiv.org/abs/2511.22294</link>
<guid>https://arxiv.org/abs/2511.22294</guid>
<content:encoded><![CDATA[

arXiv:2511.22294v1 Announce Type: cross 
Abstract: Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven informative priors for Bayesian inference with quasi-periodic data</title>
<link>https://arxiv.org/abs/2511.22296</link>
<guid>https://arxiv.org/abs/2511.22296</guid>
<content:encoded><![CDATA[

arXiv:2511.22296v1 Announce Type: cross 
Abstract: Bayesian computational strategies for inference can be inefficient in approximating the posterior distribution in models that exhibit some form of periodicity. This is because the probability mass of the marginal posterior distribution of the parameter representing the period is usually highly concentrated in a very small region of the parameter space. Therefore, it is necessary to provide as much information as possible to the inference method through the parameter prior distribution. We intend to show that it is possible to construct a prior distribution from the data by fitting a Gaussian process (GP) with a periodic kernel. More specifically, we want to show that it is possible to approximate the marginal posterior distribution of the hyperparameter corresponding to the period in the kernel. Subsequently, this distribution can be used as a prior distribution for the inference method. We use an adaptive importance sampling method to approximate the posterior distribution of the hyperparameters of the GP. Then, we use the marginal posterior distribution of the hyperparameter related to the periodicity in order to construct a prior distribution for the period of the parametric model. This workflow is empirical Bayes, implemented as a modular (cut) transfer of a GP posterior for the period to the parametric model. We applied the proposed methodology to both synthetic and real data. We approximated the posterior distribution of the period of the GP kernel and then passed it forward as a posterior-as-prior with no feedback. Finally, we analyzed its impact on the marginal posterior distribution.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Conditional Generation of Double Perovskite by Knowledge-Guided Language Model Feedback</title>
<link>https://arxiv.org/abs/2511.22307</link>
<guid>https://arxiv.org/abs/2511.22307</guid>
<content:encoded><![CDATA[

arXiv:2511.22307v1 Announce Type: cross 
Abstract: Double perovskites (DPs) are promising candidates for sustainable energy technologies due to their compositional tunability and compatibility with low-energy fabrication, yet their vast design space poses a major challenge for conditional materials discovery. This work introduces a multi-agent, text gradient-driven framework that performs DP composition generation under natural-language conditions by integrating three complementary feedback sources: LLM-based self-evaluation, DP-specific domain knowledge-informed feedback, and ML surrogate-based feedback. Analogous to how knowledge-informed machine learning improves the reliability of conventional data-driven models, our framework incorporates domain-informed text gradients to guide the generative process toward physically meaningful regions of the DP composition space. Systematic comparison of three incremental configurations, (i) pure LLM generation, (ii) LLM generation with LLM reasoning-based feedback, and (iii) LLM generation with domain knowledge-guided feedback, shows that iterative guidance from knowledge-informed gradients improves stability-condition satisfaction without additional training data, achieving over 98% compositional validity and up to 54% stable or metastable candidates, surpassing both the LLM-only baseline (43%) and prior GAN-based results (27%). Analyses of ML-based gradients further reveal that they enhance performance in in-distribution (ID) regions but become unreliable in out-of-distribution (OOD) regimes. Overall, this work provides the first systematic analysis of multi-agent, knowledge-guided text gradients for DP discovery and establishes a generalizable blueprint for MAS-driven generative materials design aimed at advancing sustainable technologies.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation</title>
<link>https://arxiv.org/abs/2511.22311</link>
<guid>https://arxiv.org/abs/2511.22311</guid>
<content:encoded><![CDATA[

arXiv:2511.22311v1 Announce Type: cross 
Abstract: Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Condition Number Dependency in Bilevel Optimization</title>
<link>https://arxiv.org/abs/2511.22331</link>
<guid>https://arxiv.org/abs/2511.22331</guid>
<content:encoded><![CDATA[

arXiv:2511.22331v1 Announce Type: cross 
Abstract: Bilevel optimization minimizes an objective function, defined by an upper-level problem whose feasible region is the solution of a lower-level problem. We study the oracle complexity of finding an $\epsilon$-stationary point with first-order methods when the upper-level problem is nonconvex and the lower-level problem is strongly convex. Recent works (Ji et al., ICML 2021; Arbel and Mairal, ICLR 2022; Chen el al., JMLR 2025) achieve a $\tilde{\mathcal{O}}(\kappa^4 \epsilon^{-2})$ upper bound that is near-optimal in $\epsilon$. However, the optimal dependency on the condition number $\kappa$ is unknown. In this work, we establish a new $\Omega(\kappa^2 \epsilon^{-2})$ lower bound and $\tilde{\mathcal{O}}(\kappa^{7/2} \epsilon^{-2})$ upper bound for this problem, establishing the first provable gap between bilevel problems and minimax problems in this setup. Our lower bounds can be extended to various settings, including high-order smooth functions, stochastic oracles, and convex hyper-objectives: (1) For second-order and arbitrarily smooth problems, we show $\Omega(\kappa_y^{13/4} \epsilon^{-12/7})$ and $\Omega(\kappa^{17/10} \epsilon^{-8/5})$ lower bounds, respectively. (2) For convex-strongly-convex problems, we improve the previously best lower bound (Ji and Liang, JMLR 2022) from $\Omega(\kappa /\sqrt{\epsilon})$ to $\Omega(\kappa^{5/4} / \sqrt{\epsilon})$. (3) For smooth stochastic problems, we show an $\Omega(\kappa^4 \epsilon^{-4})$ lower bound.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unexplored flaws in multiple-choice VQA evaluations</title>
<link>https://arxiv.org/abs/2511.22341</link>
<guid>https://arxiv.org/abs/2511.22341</guid>
<content:encoded><![CDATA[

arXiv:2511.22341v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate strong capabilities in handling image-text inputs. A common way to assess this ability is through multiple-choice Visual Question Answering (VQA). Earlier works have already revealed that these benchmarks are sensitive to answer choice order, a limitation that can be mitigated through careful design. Yet, we highlight additional, unexplored biases in prompt formatting that question the reliability of current MLLM evaluations. Specifically, we identify three key variation factors in prompt formatting and analyze their impact through a large-scale study involving $\mathbf{\text{seven}}$ MLLMs and $\mathbf{\text{five}}$ VQA datasets, spanning $\mathbf{48}$ distinct $\mathbf{\text{prompt format variations}}$. Our findings reveal that multiple-choice VQA is highly sensitive to minor prompt format changes, even when these changes are semantically neutral. We further demonstrate that these biases persist independently of known order biases or the MLLM's confidence in the correct answer. Finally, we demonstrate that existing bias mitigation strategies fail to address these newly identified biases.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MATCH: Engineering Transparent and Controllable Conversational XAI Systems through Composable Building Blocks</title>
<link>https://arxiv.org/abs/2511.22420</link>
<guid>https://arxiv.org/abs/2511.22420</guid>
<content:encoded><![CDATA[

arXiv:2511.22420v1 Announce Type: cross 
Abstract: While the increased integration of AI technologies into interactive systems enables them to solve an increasing number of tasks, the black-box problem of AI models continues to spread throughout the interactive system as a whole. Explainable AI (XAI) techniques can make AI models more accessible by employing post-hoc methods or transitioning to inherently interpretable models. While this makes individual AI models clearer, the overarching system architecture remains opaque. This challenge not only pertains to standard XAI techniques but also to human examination and conversational XAI approaches that need access to model internals to interpret them correctly and completely. To this end, we propose conceptually representing such interactive systems as sequences of structural building blocks. These include the AI models themselves, as well as control mechanisms grounded in literature. The structural building blocks can then be explained through complementary explanatory building blocks, such as established XAI techniques like LIME and SHAP. The flow and APIs of the structural building blocks form an unambiguous overview of the underlying system, serving as a communication basis for both human and automated agents, thus aligning human and machine interpretability of the embedded AI models. In this paper, we present our flow-based approach and a selection of building blocks as MATCH: a framework for engineering Multi-Agent Transparent and Controllable Human-centered systems. This research contributes to the field of (conversational) XAI by facilitating the integration of interpretability into existing interactive systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents</title>
<link>https://arxiv.org/abs/2511.22441</link>
<guid>https://arxiv.org/abs/2511.22441</guid>
<content:encoded><![CDATA[

arXiv:2511.22441v1 Announce Type: cross 
Abstract: Images shared on social media often expose geographic cues. While early geolocation methods required expert effort and lacked generalization, the rise of Large Vision Language Models (LVLMs) now enables accurate geolocation even for ordinary users. However, existing approaches are not optimized for this task. To explore the full potential and associated privacy risks, we present Geo-Detective, an agent that mimics human reasoning and tool use for image geolocation inference. It follows a procedure with four steps that adaptively selects strategies based on image difficulty and is equipped with specialized tools such as visual reverse search, which emulates how humans gather external geographic clues. Experimental results show that GEO-Detective outperforms baseline large vision language models (LVLMs) overall, particularly on images lacking visible geographic features. In country level geolocation tasks, it achieves an improvement of over 11.1% compared to baseline LLMs, and even at finer grained levels, it still provides around a 5.2% performance gain. Meanwhile, when equipped with external clues, GEO-Detective becomes more likely to produce accurate predictions, reducing the "unknown" prediction rate by more than 50.6%. We further explore multiple defense strategies and find that Geo-Detective exhibits stronger robustness, highlighting the need for more effective privacy safeguards.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Is the Optimal Ranking Score Between Precision and Recall? We Can Always Find It and It Is Rarely $F_1$</title>
<link>https://arxiv.org/abs/2511.22442</link>
<guid>https://arxiv.org/abs/2511.22442</guid>
<content:encoded><![CDATA[

arXiv:2511.22442v1 Announce Type: cross 
Abstract: Ranking methods or models based on their performance is of prime importance but is tricky because performance is fundamentally multidimensional. In the case of classification, precision and recall are scores with probabilistic interpretations that are both important to consider and complementary. The rankings induced by these two scores are often in partial contradiction. In practice, therefore, it is extremely useful to establish a compromise between the two views to obtain a single, global ranking. Over the last fifty years or so,it has been proposed to take a weighted harmonic mean, known as the F-score, F-measure, or $F_\beta$. Generally speaking, by averaging basic scores, we obtain a score that is intermediate in terms of values. However, there is no guarantee that these scores lead to meaningful rankings and no guarantee that the rankings are good tradeoffs between these base scores. Given the ubiquity of $F_\beta$ scores in the literature, some clarification is in order. Concretely: (1) We establish that $F_\beta$-induced rankings are meaningful and define a shortest path between precision- and recall-induced rankings. (2) We frame the problem of finding a tradeoff between two scores as an optimization problem expressed with Kendall rank correlations. We show that $F_1$ and its skew-insensitive version are far from being optimal in that regard. (3) We provide theoretical tools and a closed-form expression to find the optimal value for $\beta$ for any distribution or set of performances, and we illustrate their use on six case studies.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking machine learning models for multi-class state recognition in double duantum dot data</title>
<link>https://arxiv.org/abs/2511.22451</link>
<guid>https://arxiv.org/abs/2511.22451</guid>
<content:encoded><![CDATA[

arXiv:2511.22451v1 Announce Type: cross 
Abstract: Semiconductor quantum dots (QDs) are a leading platform for scalable quantum processors. However, scaling to large arrays requires reliable, automated tuning strategies for devices' bootstrapping, calibration, and operation, with many tuning aspects depending on accurately identifying QD device states from charge-stability diagrams (CSDs). In this work, we present a comprehensive benchmarking study of four modern machine learning (ML) architectures for multi-class state recognition in double-QD CSDs. We evaluate their performance across different data budgets and normalization schemes using both synthetic and experimental data. We find that the more resource-intensive models -- U-Nets and visual transformers (ViTs) -- achieve the highest MSE score (defined as $1-\mathrm{MSE}$) on synthetic data (over $0.98$) but fail to generalize to experimental data. MDNs are the most computationally efficient and exhibit highly stable training, but with substantially lower peak performance. CNNs offer the most favorable trade-off on experimental CSDs, achieving strong accuracy with two orders of magnitude fewer parameters than the U-Nets and ViTs. Normalization plays a nontrivial role: min-max scaling generally yields higher MSE scores but less stable convergence, whereas z-score normalization produces more predictable training dynamics but at reduced accuracy for most models. Overall, our study shows that CNNs with min-max normalization are a practical approach for QD CSDs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Machine Learning Approach to Moment Closure Relations for Plasma: A Review</title>
<link>https://arxiv.org/abs/2511.22486</link>
<guid>https://arxiv.org/abs/2511.22486</guid>
<content:encoded><![CDATA[

arXiv:2511.22486v1 Announce Type: cross 
Abstract: The requirement for large-scale global simulations of plasma is an ongoing challenge in both space and laboratory plasma physics. Any simulation based on a fluid model inherently requires a closure relation for the high order plasma moments. This review compiles and analyses the recent surge of machine learning approaches developing improved plasma closure models capable of capturing kinetic phenomena within plasma fluid models. The purpose of this review is both to collect and analyse the various methods employed on the plasma closure problem, including both equation discovery methods and neural network surrogate approaches, as well as to provide a general overview of the state of the problem. In particular, we highlight the challenges of developing a data-driven closure as well as the direction future work should take toward addressing these challenges, in the pursuit of a computationally viable large-scale global simulation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Shape Is Optimal for Masks in Text Removal?</title>
<link>https://arxiv.org/abs/2511.22499</link>
<guid>https://arxiv.org/abs/2511.22499</guid>
<content:encoded><![CDATA[

arXiv:2511.22499v1 Announce Type: cross 
Abstract: The advent of generative models has dramatically improved the accuracy of image inpainting. In particular, by removing specific text from document images, reconstructing original images is extremely important for industrial applications. However, most existing methods of text removal focus on deleting simple scene text which appears in images captured by a camera in an outdoor environment. There is little research dedicated to complex and practical images with dense text. Therefore, we created benchmark data for text removal from images including a large amount of text. From the data, we found that text-removal performance becomes vulnerable against mask profile perturbation. Thus, for practical text-removal tasks, precise tuning of the mask shape is essential. This study developed a method to model highly flexible mask profiles and learn their parameters using Bayesian optimization. The resulting profiles were found to be character-wise masks. It was also found that the minimum cover of a text region is not optimal. Our research is expected to pave the way for a user-friendly guideline for manual masking.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdS/Deep-Learning made easy II: neural network-based approaches to holography and inverse problems</title>
<link>https://arxiv.org/abs/2511.22522</link>
<guid>https://arxiv.org/abs/2511.22522</guid>
<content:encoded><![CDATA[

arXiv:2511.22522v1 Announce Type: cross 
Abstract: We apply physics-informed machine learning (PIML) to solve inverse problems in holography and classical mechanics, focusing on neural ordinary differential equations (Neural ODEs) and physics-informed neural networks (PINNs) for solving non-linear differential equations of motion. First, we introduce holographic inverse problems and demonstrate how PIML can reconstruct bulk spacetime and effective potentials from boundary quantum data. To illustrate this, two case studies are explored: the QCD equation of state in holographic QCD and $T$-linear resistivity in holographic strange metals. Additionally, we explicitly show how such holographic problems can be analogized to inverse problems in classical mechanics, modeling frictional forces with neural networks. We also explore Kolmogorov-Arnold Networks (KANs) as an alternative to traditional neural networks, offering more efficient solutions in certain cases. This manuscript aim to provide a systematic framework for using neural networks in inverse problems, serving as a comprehensive reference for researchers in machine learning for high-energy physics, with methodologies that also have broader applications in mathematics, engineering, and the natural sciences.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counting Still Counts: Understanding Neural Complex Query Answering Through Query Relaxation</title>
<link>https://arxiv.org/abs/2511.22565</link>
<guid>https://arxiv.org/abs/2511.22565</guid>
<content:encoded><![CDATA[

arXiv:2511.22565v1 Announce Type: cross 
Abstract: Neural methods for Complex Query Answering (CQA) over knowledge graphs (KGs) are widely believed to learn patterns that generalize beyond explicit graph structure, allowing them to infer answers that are unreachable through symbolic query processing. In this work, we critically examine this assumption through a systematic analysis comparing neural CQA models with an alternative, training-free query relaxation strategy that retrieves possible answers by relaxing query constraints and counting resulting paths. Across multiple datasets and query structures, we find several cases where neural and relaxation-based approaches perform similarly, with no neural model consistently outperforming the latter. Moreover, a similarity analysis reveals that their retrieved answers exhibit little overlap, and that combining their outputs consistently improves performance. These results call for a re-evaluation of progress in neural query answering: despite their complexity, current models fail to subsume the reasoning patterns captured by query relaxation. Our findings highlight the importance of stronger non-neural baselines and suggest that future neural approaches could benefit from incorporating principles of query relaxation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DisCEdge: Distributed Context Management for Large Language Models at the Edge</title>
<link>https://arxiv.org/abs/2511.22599</link>
<guid>https://arxiv.org/abs/2511.22599</guid>
<content:encoded><![CDATA[

arXiv:2511.22599v1 Announce Type: cross 
Abstract: Deploying Large Language Model (LLM) services at the edge benefits latency-sensitive and privacy-aware applications. However, the stateless nature of LLMs makes managing user context (e.g., sessions, preferences) across geo-distributed edge nodes challenging. Existing solutions, such as client-side context storage, often introduce network latency and bandwidth overhead, undermining the advantages of edge deployment.
  We propose DisCEdge, a distributed context management system that stores and replicates user context in tokenized form across edge nodes. By maintaining context as token sequences rather than raw text, our system avoids redundant computation and enables efficient data replication. We implement and evaluate an open-source prototype in a realistic edge environment with commodity hardware. We show DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15% compared to a raw-text-based system. It also reduces client request sizes by a median of 90% compared to client-side context management, while guaranteeing data consistency.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial Computing</title>
<link>https://arxiv.org/abs/2511.22607</link>
<guid>https://arxiv.org/abs/2511.22607</guid>
<content:encoded><![CDATA[

arXiv:2511.22607v1 Announce Type: cross 
Abstract: Eye tracking has become increasingly important in virtual and augmented reality applications; however, the current gaze accuracy falls short of meeting the requirements for spatial computing. We designed a gaze collection framework and utilized high-precision equipment to gather the first precise benchmark dataset, GazeTrack, encompassing diverse ethnicities, ages, and visual acuity conditions for pupil localization and gaze tracking. We propose a novel shape error regularization method to constrain pupil ellipse fitting and train on open-source datasets, enhancing semantic segmentation and pupil position prediction accuracy. Additionally, we invent a novel coordinate transformation method similar to paper unfolding to accurately predict gaze vectors on the GazeTrack dataset. Finally, we built a gaze vector generation model that achieves reduced gaze angle error with lower computational complexity compared to other methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational analysis of determinantal varieties</title>
<link>https://arxiv.org/abs/2511.22613</link>
<guid>https://arxiv.org/abs/2511.22613</guid>
<content:encoded><![CDATA[

arXiv:2511.22613v1 Announce Type: cross 
Abstract: Determinantal varieties -- the sets of bounded-rank matrices or tensors -- have attracted growing interest in low-rank optimization. The tangent cone to low-rank sets is widely studied and underpins a range of geometric methods. The second-order geometry, which encodes curvature information, is more intricate. In this work, we develop a unified framework to derive explicit formulas for both first- and second-order tangent sets to various low-rank sets, including low-rank matrices, tensors, symmetric matrices, and positive semidefinite matrices. The framework also accommodates the intersection of a low-rank set and another set satisfying mild assumptions, thereby yielding a tangent intersection rule. Through the lens of tangent sets, we establish a necessary and sufficient condition under which a nonsmooth problem and its smooth parameterization share equivalent second-order stationary points. Moreover, we exploit tangent sets to characterize optimality conditions for low-rank optimization and prove that verifying second-order optimality is NP-hard. In a separate line of analysis, we investigate variational geometry of the graph of the normal cone to matrix varieties, deriving the explicit Bouligand tangent cone, Fr\'echet and Mordukhovich normal cones to the graph. These results are further applied to develop optimality conditions for low-rank bilevel programs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable-Drift: A Patient-Aware Latent Drift Replay Method for Stabilizing Representations in Continual Learning</title>
<link>https://arxiv.org/abs/2511.22615</link>
<guid>https://arxiv.org/abs/2511.22615</guid>
<content:encoded><![CDATA[

arXiv:2511.22615v1 Announce Type: cross 
Abstract: When deep learning models are sequentially trained on new data, they tend to abruptly lose performance on previously learned tasks, a critical failure known as catastrophic forgetting. This challenge severely limits the deployment of AI in medical imaging, where models must continually adapt to data from new hospitals without compromising established diagnostic knowledge. To address this, we introduce a latent drift-guided replay method that identifies and replays samples with high representational instability. Specifically, our method quantifies this instability via latent drift, the change in a sample internal feature representation after naive domain adaptation. To ensure diversity and clinical relevance, we aggregate drift at the patient level, our memory buffer stores the per patient slices exhibiting the greatest multi-layer representation shift. Evaluated on a cross-hospital COVID-19 CT classification task using state-of-the-art CNN and Vision Transformer backbones, our method substantially reduces forgetting compared to naive fine-tuning and random replay. This work highlights latent drift as a practical and interpretable replay signal for advancing robust continual learning in real world medical settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative models for crystalline materials</title>
<link>https://arxiv.org/abs/2511.22652</link>
<guid>https://arxiv.org/abs/2511.22652</guid>
<content:encoded><![CDATA[

arXiv:2511.22652v1 Announce Type: cross 
Abstract: Understanding structure-property relationships in materials is fundamental in condensed matter physics and materials science. Over the past few years, machine learning (ML) has emerged as a powerful tool for advancing this understanding and accelerating materials discovery. Early ML approaches primarily focused on constructing and screening large material spaces to identify promising candidates for various applications. More recently, research efforts have increasingly shifted toward generating crystal structures using end-to-end generative models. This review analyzes the current state of generative modeling for crystal structure prediction and \textit{de novo} generation. It examines crystal representations, outlines the generative models used to design crystal structures, and evaluates their respective strengths and limitations. Furthermore, the review highlights experimental considerations for evaluating generated structures and provides recommendations for suitable existing software tools. Emerging topics, such as modeling disorder and defects, integration in advanced characterization, and incorporating synthetic feasibility constraints, are explored. Ultimately, this work aims to inform both experimental scientists looking to adapt suitable ML models to their specific circumstances and ML specialists seeking to understand the unique challenges related to inverse materials design and discovery.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Privacy-preserving Intrusion Detection Scheme for UAV Swarm Networks</title>
<link>https://arxiv.org/abs/2511.22791</link>
<guid>https://arxiv.org/abs/2511.22791</guid>
<content:encoded><![CDATA[

arXiv:2511.22791v1 Announce Type: cross 
Abstract: The rapid proliferation of unmanned aerial vehicles (UAVs) and their applications in diverse domains, such as surveillance, disaster management, agriculture, and defense, have revolutionized modern technology. While the potential benefits of swarm-based UAV networks are growing significantly, they are vulnerable to various security attacks that can jeopardize the overall mission success by degrading their performance, disrupting decision-making, and compromising the trajectory planning process. The Intrusion Detection System (IDS) plays a vital role in identifying potential security attacks to ensure the secure operation of UAV swarm networks. However, conventional IDS primarily focuses on binary classification with resource-intensive neural networks and faces challenges, including latency, privacy breaches, increased performance overhead, and model drift. This research aims to address these challenges by developing a novel lightweight and federated continuous learning-based IDS scheme. Our proposed model facilitates decentralized training across diverse UAV swarms to ensure data heterogeneity and privacy. The performance evaluation of our model demonstrates significant improvements, with classification accuracies of 99.45% on UKM-IDS, 99.99% on UAV-IDS, 96.85% on TLM-UAV dataset, and 98.05% on Cyber-Physical datasets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images</title>
<link>https://arxiv.org/abs/2511.22805</link>
<guid>https://arxiv.org/abs/2511.22805</guid>
<content:encoded><![CDATA[

arXiv:2511.22805v1 Announce Type: cross 
Abstract: While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Semantic Drift: Evaluating LLMs' Efficacy in Psychotherapy through MI Dialogue Summarization</title>
<link>https://arxiv.org/abs/2511.22818</link>
<guid>https://arxiv.org/abs/2511.22818</guid>
<content:encoded><![CDATA[

arXiv:2511.22818v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have shown their potential across both general and domain-specific tasks. However, there is a growing concern regarding their lack of sensitivity, factual incorrectness in responses, inconsistent expressions of empathy, bias, hallucinations, and overall inability to capture the depth and complexity of human understanding, especially in low-resource and sensitive domains such as psychology. To address these challenges, our study employs a mixed-methods approach to evaluate the efficacy of LLMs in psychotherapy. We use LLMs to generate precise summaries of motivational interviewing (MI) dialogues and design a two-stage annotation scheme based on key components of the Motivational Interviewing Treatment Integrity (MITI) framework, namely evocation, collaboration, autonomy, direction, empathy, and a non-judgmental attitude. Using expert-annotated MI dialogues as ground truth, we formulate multi-class classification tasks to assess model performance under progressive prompting techniques, incorporating one-shot and few-shot prompting. Our results offer insights into LLMs' capacity for understanding complex psychological constructs and highlight best practices to mitigate ``semantic drift" in therapeutic settings. Our work contributes not only to the MI community by providing a high-quality annotated dataset to address data scarcity in low-resource domains but also critical insights for using LLMs for precise contextual interpretation in complex behavioral therapy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolving Sharp Gradients of Unstable Singularities to Machine Precision via Neural Networks</title>
<link>https://arxiv.org/abs/2511.22819</link>
<guid>https://arxiv.org/abs/2511.22819</guid>
<content:encoded><![CDATA[

arXiv:2511.22819v1 Announce Type: cross 
Abstract: Recent work introduced a robust computational framework combining embedded mathematical structures, advanced optimization, and neural network architecture, leading to the discovery of multiple unstable self-similar solutions for key fluid dynamics equations, including the Incompressible Porous Media (IPM) and 2D Boussinesq systems. While this framework confirmed the existence of these singularities, an accuracy level approaching double-float machine precision was only achieved for stable and 1st unstable solutions of the 1D C\'ordoba-C\'ordoba-Fontelos model. For highly unstable solutions characterized by extreme gradients, the accuracy remained insufficient for validation. The primary obstacle is the presence of sharp solution gradients. Those gradients tend to induce large, localized PDE residuals during training, which not only hinder convergence, but also obscure the subtle signals near the origin required to identify the correct self-similar scaling parameter lambda of the solutions. In this work, we introduce a gradient-normalized PDE residual re-weighting scheme to resolve the high-gradient challenge while amplifying the critical residual signals at the origin for lambda identification. Coupled with the multi-stage neural network architecture, the PDE residuals are reduced to the level of round-off error across a wide spectrum of unstable self-similar singularities previously discovered. Furthermore, our method enables the discovery of new highly unstable singularities, i.e. the 4th unstable solution for IPM equations and a novel family of highly unstable solitons for the Nonlinear Schr\"odinger equations. This results in achieving high-gradient solutions with high precision, providing an important ingredient for bridging the gap between numerical discovery and computer-assisted proofs for unstable phenomena in nonlinear PDEs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems</title>
<link>https://arxiv.org/abs/2511.22880</link>
<guid>https://arxiv.org/abs/2511.22880</guid>
<content:encoded><![CDATA[

arXiv:2511.22880v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORION: Teaching Language Models to Reason Efficiently in the Language of Thought</title>
<link>https://arxiv.org/abs/2511.22891</link>
<guid>https://arxiv.org/abs/2511.22891</guid>
<content:encoded><![CDATA[

arXiv:2511.22891v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClearGCD: Mitigating Shortcut Learning For Robust Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2511.22892</link>
<guid>https://arxiv.org/abs/2511.22892</guid>
<content:encoded><![CDATA[

arXiv:2511.22892v1 Announce Type: cross 
Abstract: In open-world scenarios, Generalized Category Discovery (GCD) requires identifying both known and novel categories within unlabeled data. However, existing methods often suffer from prototype confusion caused by shortcut learning, which undermines generalization and leads to forgetting of known classes. We propose ClearGCD, a framework designed to mitigate reliance on non-semantic cues through two complementary mechanisms. First, Semantic View Alignment (SVA) generates strong augmentations via cross-class patch replacement and enforces semantic consistency using weak augmentations. Second, Shortcut Suppression Regularization (SSR) maintains an adaptive prototype bank that aligns known classes while encouraging separation of potential novel ones. ClearGCD can be seamlessly integrated into parametric GCD approaches and consistently outperforms state-of-the-art methods across multiple benchmarks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-conditioned world model improves policy generalization by reading environmental descriptions</title>
<link>https://arxiv.org/abs/2511.22904</link>
<guid>https://arxiv.org/abs/2511.22904</guid>
<content:encoded><![CDATA[

arXiv:2511.22904v1 Announce Type: cross 
Abstract: To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying "what to do". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optical diffraction neural networks assisted computational ghost imaging through dynamic scattering media</title>
<link>https://arxiv.org/abs/2511.22913</link>
<guid>https://arxiv.org/abs/2511.22913</guid>
<content:encoded><![CDATA[

arXiv:2511.22913v1 Announce Type: cross 
Abstract: Ghost imaging leverages a single-pixel detector with no spatial resolution to acquire object echo intensity signals, which are correlated with illumination patterns to reconstruct an image. This architecture inherently mitigates scattering interference between the object and the detector but sensitive to scattering between the light source and the object. To address this challenge, we propose an optical diffraction neural networks (ODNNs) assisted ghost imaging method for imaging through dynamic scattering media. In our scheme, a set of fixed ODNNs, trained on simulated datasets, is incorporated into the experimental optical path to actively correct random distortions induced by dynamic scattering media. Experimental validation using rotating single-layer and double-layer ground glass confirms the feasibility and effectiveness of our approach. Furthermore, our scheme can also be combined with physics-prior-based reconstruction algorithms, enabling high-quality imaging under undersampled conditions. This work demonstrates a novel strategy for imaging through dynamic scattering media, which can be extended to other imaging systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pooling Attention: Evaluating Pretrained Transformer Embeddings for Deception Classification</title>
<link>https://arxiv.org/abs/2511.22977</link>
<guid>https://arxiv.org/abs/2511.22977</guid>
<content:encoded><![CDATA[

arXiv:2511.22977v1 Announce Type: cross 
Abstract: This paper investigates fake news detection as a downstream evaluation of Transformer representations, benchmarking encoder-only and decoder-only pre-trained models (BERT, GPT-2, Transformer-XL) as frozen embedders paired with lightweight classifiers. Through controlled preprocessing comparing pooling versus padding and neural versus linear heads, results demonstrate that contextual self-attention encodings consistently transfer effectively. BERT embeddings combined with logistic regression outperform neural baselines on LIAR dataset splits, while analyses of sequence length and aggregation reveal robustness to truncation and advantages from simple max or average pooling. This work positions attention-based token encoders as robust, architecture-centric foundations for veracity tasks, isolating Transformer contributions from classifier complexity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maritime Activities Observed Through Open-Access Positioning Data: Moving and Stationary Vessels in the Baltic Sea</title>
<link>https://arxiv.org/abs/2511.23016</link>
<guid>https://arxiv.org/abs/2511.23016</guid>
<content:encoded><![CDATA[

arXiv:2511.23016v1 Announce Type: cross 
Abstract: Understanding past and present maritime activity patterns is critical for navigation safety, environmental assessment, and commercial operations. An increasing number of services now openly provide positioning data from the Automatic Identification System (AIS) via ground-based receivers. We show that coastal vessel activity can be reconstructed from open access data with high accuracy, even with limited data quality and incomplete receiver coverage. For three months of open AIS data in the Baltic Sea from August to October 2024, we present (i) cleansing and reconstruction methods to improve the data quality, and (ii) a journey model that converts AIS message data into vessel counts, traffic estimates, and spatially resolved vessel density at a resolution of $\sim$400 m. Vessel counts are provided, along with their uncertainties, for both moving and stationary activity. Vessel density maps also enable the identification of port locations, and we infer the most crowded and busiest coastal areas in the Baltic Sea. We find that on average, $\gtrsim$4000 vessels simultaneously operate in the Baltic Sea, and more than 300 vessels enter or leave the area each day. Our results agree within 20\% with previous studies relying on proprietary data.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Factor Graph-Based Tightly Coupled GNSS/IMU Fusion for Robust Positionin</title>
<link>https://arxiv.org/abs/2511.23017</link>
<guid>https://arxiv.org/abs/2511.23017</guid>
<content:encoded><![CDATA[

arXiv:2511.23017v1 Announce Type: cross 
Abstract: Reliable positioning in GNSS-challenged environments remains a critical challenge for navigation systems. Tightly coupled GNSS/IMU fusion improves robustness but remains vulnerable to non-Gaussian noise and outliers. We present a robust and adaptive factor graph-based fusion framework that directly integrates GNSS pseudorange measurements with IMU preintegration factors and incorporates the Barron loss, a general robust loss function that unifies several m-estimators through a single tunable parameter. By adaptively down weighting unreliable GNSS measurements, our approach improves resilience positioning. The method is implemented in an extended GTSAM framework and evaluated on the UrbanNav dataset. The proposed solution reduces positioning errors by up to 41% relative to standard FGO, and achieves even larger improvements over extended Kalman filter (EKF) baselines in urban canyon environments. These results highlight the benefits of Barron loss in enhancing the resilience of GNSS/IMU-based navigation in urban and signal-compromised environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time Extrapolation with Graph Convolutional Autoencoder and Tensor Train Decomposition</title>
<link>https://arxiv.org/abs/2511.23037</link>
<guid>https://arxiv.org/abs/2511.23037</guid>
<content:encoded><![CDATA[

arXiv:2511.23037v1 Announce Type: cross 
Abstract: Graph autoencoders have gained attention in nonlinear reduced-order modeling of parameterized partial differential equations defined on unstructured grids. Despite they provide a geometrically consistent way of treating complex domains, applying such architectures to parameterized dynamical systems for temporal prediction beyond the training data, i.e. the extrapolation regime, is still a challenging task due to the simultaneous need of temporal causality and generalizability in the parametric space. In this work, we explore the integration of graph convolutional autoencoders (GCAs) with tensor train (TT) decomposition and Operator Inference (OpInf) to develop a time-consistent reduced-order model. In particular, high-fidelity snapshots are represented as a combination of parametric, spatial, and temporal cores via TT decomposition, while OpInf is used to learn the evolution of the latter. Moreover, we enhance the generalization performance by developing a multi-fidelity two-stages approach in the framework of Deep Operator Networks (DeepONet), treating the spatial and temporal cores as the trunk networks, and the parametric core as the branch network. Numerical results, including heat-conduction, advection-diffusion and vortex-shedding phenomena, demonstrate great performance in effectively learning the dynamic in the extrapolation regime for complex geometries, also in comparison with state-of-the-art approaches e.g. MeshGraphNets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Standard Occupation Classifier -- A Natural Language Processing Approach</title>
<link>https://arxiv.org/abs/2511.23057</link>
<guid>https://arxiv.org/abs/2511.23057</guid>
<content:encoded><![CDATA[

arXiv:2511.23057v1 Announce Type: cross 
Abstract: Standard Occupational Classifiers (SOC) are systems used to categorize and classify different types of jobs and occupations based on their similarities in terms of job duties, skills, and qualifications. Integrating these facets with Big Data from job advertisement offers the prospect to investigate labour demand that is specific to various occupations. This project investigates the use of recent developments in natural language processing to construct a classifier capable of assigning an occupation code to a given job advertisement. We develop various classifiers for both UK ONS SOC and US O*NET SOC, using different Language Models. We find that an ensemble model, which combines Google BERT and a Neural Network classifier while considering job title, description, and skills, achieved the highest prediction accuracy. Specifically, the ensemble model exhibited a classification accuracy of up to 61% for the lower (or fourth) tier of SOC, and 72% for the third tier of SOC. This model could provide up to date, accurate information on the evolution of the labour market using job advertisements.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Buffer replay enhances the robustness of multimodal learning under missing-modality</title>
<link>https://arxiv.org/abs/2511.23070</link>
<guid>https://arxiv.org/abs/2511.23070</guid>
<content:encoded><![CDATA[

arXiv:2511.23070v1 Announce Type: cross 
Abstract: Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraining dark matter halo profiles with symbolic regression</title>
<link>https://arxiv.org/abs/2511.23073</link>
<guid>https://arxiv.org/abs/2511.23073</guid>
<content:encoded><![CDATA[

arXiv:2511.23073v1 Announce Type: cross 
Abstract: Dark matter haloes are typically characterised by radial density profiles with fixed forms motivated by simulations (e.g. NFW). However, simulation predictions depend on uncertain dark matter physics and baryonic modelling. Here, we present a method to constrain halo density profiles directly from observations using Exhaustive Symbolic Regression (ESR), a technique that searches the space of analytic expressions for the function that best balances accuracy and simplicity for a given dataset. We test the approach on mock weak lensing excess surface density (ESD) data of synthetic clusters with NFW profiles. Motivated by real data, we assign each ESD data point a constant fractional uncertainty and vary this uncertainty and the number of clusters to probe how data precision and sample size affect model selection. For fractional errors around 5%, ESR recovers the NFW profile even from samples as small as 20 clusters. At higher uncertainties representative of current surveys, simpler functions are favoured over NFW, though it remains competitive. This preference arises because weak lensing errors are smallest in the outskirts, causing the fits to be dominated by the outer profile. ESR therefore provides a robust, simulation-independent framework both for testing mass models and determining which features of a halo's density profile are genuinely constrained by the data.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?</title>
<link>https://arxiv.org/abs/2511.23112</link>
<guid>https://arxiv.org/abs/2511.23112</guid>
<content:encoded><![CDATA[

arXiv:2511.23112v1 Announce Type: cross 
Abstract: Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism</title>
<link>https://arxiv.org/abs/2511.23113</link>
<guid>https://arxiv.org/abs/2511.23113</guid>
<content:encoded><![CDATA[

arXiv:2511.23113v1 Announce Type: cross 
Abstract: Scaling Diffusion Transformer (DiT) inference via sequence parallelism is critical for reducing latency in visual generation, but is severely hampered by workload imbalance when applied to models employing block-wise sparse attention. The imbalance stems from the inherent variation in sparsity across attention heads and the irregular distribution of dense blocks within the sparse mask, when sequence parallelism is applied along the head dimension (as in Ulysses) or the block dimension (as in Ring Attention). In this paper, we formalize a sparse imbalance ratio to quantify the imbalance, and propose db-SP, a sparsity-aware sequence parallelism technique that tackles the challenge. db-SP contains a dual-level partitioning approach that achieves near-perfect workload balance at both the head and block levels with negligible overhead. Furthermore, to handle the evolving sparsity patterns across denoising steps and layers, db-SP dynamically determines the parallel degrees for the head and block dimensions at runtime. Experimental results demonstrate that db-SP delivers an end-to-end speedup of 1.25x and an attention-specific speedup of 1.40x over state-of-the-art sequence parallel methods on average. Code is available at https://github.com/thu-nics/db-SP.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine learning for violence prediction: a systematic review and critical appraisal</title>
<link>https://arxiv.org/abs/2511.23118</link>
<guid>https://arxiv.org/abs/2511.23118</guid>
<content:encoded><![CDATA[

arXiv:2511.23118v1 Announce Type: cross 
Abstract: Purpose To conduct a systematic review of machine learning models for predicting violent behaviour by synthesising and appraising their validity, usefulness, and performance.
  Methods We systematically searched nine bibliographic databases and Google Scholar up to September 2025 for development and/or validation studies on machine learning methods for predicting all forms of violent behaviour. We synthesised the results by summarising discrimination and calibration performance statistics and evaluated study quality by examining risk of bias and clinical utility.
  Results We identified 38 studies reporting the development and validation of 40 models. Most studies reported Area Under the Curve (AUC) as the discrimination statistic with a range of 0.68-0.99. Only eight studies reported calibration performance, and three studies reported external validation. 31 studies had a high risk of bias, mainly in the analysis domain, and three studies had low risk of bias. The overall clinical utility of violence prediction models is poor, as indicated by risks of overfitting due to small samples, lack of transparent reporting, and low generalisability.
  Conclusion Although black box machine learning models currently have limited applicability in clinical settings, they may show promise for identifying high-risk individuals. We recommend five key considerations for violence prediction modelling: (i) ensuring methodological quality (e.g. following guidelines) and interdisciplinary collaborations; (ii) using black box algorithms only for highly complex data; (iii) incorporating dynamic predictions to allow for risk monitoring; (iv) developing more trustworthy algorithms using explainable methods; and (v) applying causal machine learning approaches where appropriate.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging</title>
<link>https://arxiv.org/abs/2511.23193</link>
<guid>https://arxiv.org/abs/2511.23193</guid>
<content:encoded><![CDATA[

arXiv:2511.23193v1 Announce Type: cross 
Abstract: Multi-Agent Reinforcement Learning (MARL) holds significant promise for enabling cooperative driving among Connected and Automated Vehicles (CAVs). However, its practical application is hindered by a critical limitation, i.e., insufficient fault tolerance against observational faults. Such faults, which appear as perturbations in the vehicles' perceived data, can substantially compromise the performance of MARL-based driving systems. Addressing this problem presents two primary challenges. One is to generate adversarial perturbations that effectively stress the policy during training, and the other is to equip vehicles with the capability to mitigate the impact of corrupted observations. To overcome the challenges, we propose a fault-tolerant MARL method for cooperative on-ramp vehicles incorporating two key agents. First, an adversarial fault injection agent is co-trained to generate perturbations that actively challenge and harden the vehicle policies. Second, we design a novel fault-tolerant vehicle agent equipped with a self-diagnosis capability, which leverages the inherent spatio-temporal correlations in vehicle state sequences to detect faults and reconstruct credible observations, thereby shielding the policy from misleading inputs. Experiments in a simulated highway merging scenario demonstrate that our method significantly outperforms baseline MARL approaches, achieving near-fault-free levels of safety and efficiency under various observation fault patterns.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustering Malware at Scale: A First Full-Benchmark Study</title>
<link>https://arxiv.org/abs/2511.23198</link>
<guid>https://arxiv.org/abs/2511.23198</guid>
<content:encoded><![CDATA[

arXiv:2511.23198v1 Announce Type: cross 
Abstract: Recent years have shown that malware attacks still happen with high frequency. Malware experts seek to categorize and classify incoming samples to confirm their trustworthiness or prove their maliciousness. One of the ways in which groups of malware samples can be identified is through malware clustering. Despite the efforts of the community, malware clustering which incorporates benign samples has been under-explored. Moreover, despite the availability of larger public benchmark malware datasets, malware clustering studies have avoided fully utilizing these datasets in their experiments, often resorting to small datasets with only a few families. Additionally, the current state-of-the-art solutions for malware clustering remain unclear. In our study, we evaluate malware clustering quality and establish the state-of-the-art on Bodmas and Ember - two large public benchmark malware datasets. Ours is the first study of malware clustering performed on whole malware benchmark datasets. Additionally, we extend the malware clustering task by incorporating benign samples. Our results indicate that incorporating benign samples does not significantly degrade clustering quality. We find that there are significant differences in the quality of the created clusters between Ember and Bodmas, as well as a private industry dataset. Contrary to popular opinion, our top clustering performers are K-Means and BIRCH, with DBSCAN and HAC falling behind.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A PLS-Integrated LASSO Method with Application in Index Tracking</title>
<link>https://arxiv.org/abs/2511.23205</link>
<guid>https://arxiv.org/abs/2511.23205</guid>
<content:encoded><![CDATA[

arXiv:2511.23205v1 Announce Type: cross 
Abstract: In traditional multivariate data analysis, dimension reduction and regression have been treated as distinct endeavors. Established techniques such as principal component regression (PCR) and partial least squares (PLS) regression traditionally compute latent components as intermediary steps -- although with different underlying criteria -- before proceeding with the regression analysis. In this paper, we introduce an innovative regression methodology named PLS-integrated Lasso (PLS-Lasso) that integrates the concept of dimension reduction directly into the regression process. We present two distinct formulations for PLS-Lasso, denoted as PLS-Lasso-v1 and PLS-Lasso-v2, along with clear and effective algorithms that ensure convergence to global optima. PLS-Lasso-v1 and PLS-Lasso-v2 are compared with Lasso on the task of financial index tracking and show promising results.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asymptotic Theory and Phase Transitions for Variable Importance in Quantile Regression Forests</title>
<link>https://arxiv.org/abs/2511.23212</link>
<guid>https://arxiv.org/abs/2511.23212</guid>
<content:encoded><![CDATA[

arXiv:2511.23212v1 Announce Type: cross 
Abstract: Quantile Regression Forests (QRF) are widely used for non-parametric conditional quantile estimation, yet statistical inference for variable importance measures remains challenging due to the non-smoothness of the loss function and the complex bias-variance trade-off. In this paper, we develop a asymptotic theory for variable importance defined as the difference in pinball loss risks. We first establish the asymptotic normality of the QRF estimator by handling the non-differentiable pinball loss via Knight's identity. Second, we uncover a "phase transition" phenomenon governed by the subsampling rate $\beta$ (where $s \asymp n^{\beta}$). We prove that in the bias-dominated regime ($\beta \ge 1/2$), which corresponds to large subsample sizes typically favored in practice to maximize predictive accuracy, standard inference breaks down as the estimator converges to a deterministic bias constant rather than a zero-mean normal distribution. Finally, we derive the explicit analytic form of this asymptotic bias and discuss the theoretical feasibility of restoring valid inference via analytic bias correction. Our results highlight a fundamental trade-off between predictive performance and inferential validity, providing a theoretical foundation for understanding the intrinsic limitations of random forest inference in high-dimensional settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonstabilizerness Estimation using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.23224</link>
<guid>https://arxiv.org/abs/2511.23224</guid>
<content:encoded><![CDATA[

arXiv:2511.23224v1 Announce Type: cross 
Abstract: This article proposes a Graph Neural Network (GNN) approach to estimate nonstabilizerness in quantum circuits, measured by the stabilizer R\'enyi entropy (SRE). Nonstabilizerness is a fundamental resource for quantum advantage, and efficient SRE estimations are highly beneficial in practical applications. We address the nonstabilizerness estimation problem through three supervised learning formulations starting from easier classification tasks to the more challenging regression task. Experimental results show that the proposed GNN manages to capture meaningful features from the graph-based circuit representation, resulting in robust generalization performances achieved across diverse scenarios. In classification tasks, the GNN is trained on product states and generalizes on circuits evolved under Clifford operations, entangled states, and circuits with higher number of qubits. In the regression task, the GNN significantly improves the SRE estimation on out-of-distribution circuits with higher number of qubits and gate counts compared to previous work, for both random quantum circuits and structured circuits derived from the transverse-field Ising model. Moreover, the graph representation of quantum circuits naturally integrates hardware-specific information. Simulations on noisy quantum hardware highlight the potential of the proposed GNN to predict the SRE measured on quantum devices.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies</title>
<link>https://arxiv.org/abs/2511.23225</link>
<guid>https://arxiv.org/abs/2511.23225</guid>
<content:encoded><![CDATA[

arXiv:2511.23225v1 Announce Type: cross 
Abstract: Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.23310</link>
<guid>https://arxiv.org/abs/2511.23310</guid>
<content:encoded><![CDATA[

arXiv:2511.23310v1 Announce Type: cross 
Abstract: Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>New-Onset Diabetes Assessment Using Artificial Intelligence-Enhanced Electrocardiography</title>
<link>https://arxiv.org/abs/2205.02900</link>
<guid>https://arxiv.org/abs/2205.02900</guid>
<content:encoded><![CDATA[

arXiv:2205.02900v3 Announce Type: replace 
Abstract: Diabetes has a long asymptomatic period which can often remain undiagnosed for multiple years. In this study, we trained a deep learning model to detect new-onset diabetes using 12-lead ECG and readily available demographic information. To do so, we used retrospective data where patients have both a hemoglobin A1c and ECG measured. However, such patients may not be representative of the complete patient population. As part of the study, we proposed a methodology to evaluate our model in the target population by estimating the probability of receiving an A1c test and reweight the retrospective population to represent the general population. We also adapted an efficient algorithm to generate Shapley values for both ECG signals and demographic features at the same time for model interpretation. The model offers an automated, more accurate method for early diabetes detection compared to current screening efforts. Their potential use in wearable devices can facilitate large-scale, community-wide screening, improving healthcare outcomes.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data efficient surrogate modeling for engineering design: Ensemble-free batch mode deep active learning for regression</title>
<link>https://arxiv.org/abs/2211.10360</link>
<guid>https://arxiv.org/abs/2211.10360</guid>
<content:encoded><![CDATA[

arXiv:2211.10360v2 Announce Type: replace 
Abstract: High fidelity design evaluation processes such as Computational Fluid Dynamics and Finite Element Analysis are often replaced with data driven surrogates to reduce computational cost in engineering design optimization. However, building accurate surrogate models still requires a large number of expensive simulations. To address this challenge, we introduce epsilon HQS, a scalable active learning strategy that leverages a student teacher framework to train deep neural networks efficiently. Unlike Bayesian AL methods, which are computationally demanding with DNNs, epsilon HQS selectively queries informative samples to reduce labeling cost. Applied to CFD, FEA, and propeller design tasks, our method achieves higher accuracy under fixed labeling cost budgets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Sampling-Based Domain Generalization Study with Diffusion Generative Models</title>
<link>https://arxiv.org/abs/2310.09213</link>
<guid>https://arxiv.org/abs/2310.09213</guid>
<content:encoded><![CDATA[

arXiv:2310.09213v3 Announce Type: replace 
Abstract: In this work, we investigate the domain generalization capabilities of diffusion models in the context of synthesizing images that are distinct from the training data. Instead of fine-tuning, we tackle this challenge from a sampling-based perspective using frozen, pre-trained diffusion models. Specifically, we demonstrate that arbitrary out-of-domain (OOD) images establish Gaussian priors in the latent spaces of a given model after inversion, and that these priors are separable from those of the original training domain. This OOD latent property allows us to synthesize new images of the target unseen domain by discovering qualified OOD latent encodings in the inverted noisy spaces, without altering the pre-trained models. Our cross-model and cross-domain experiments show that the proposed sampling-based method can expand the latent space and generate unseen images without impairing the generation quality of the original domain. We also showcase a practical application of our approach using astrophysical data, highlighting the potential of this generalization paradigm in data-sparse fields such as scientific exploration.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Diffusion Models with Parallel Sampling: Inference at Sub-Linear Time Complexity</title>
<link>https://arxiv.org/abs/2405.15986</link>
<guid>https://arxiv.org/abs/2405.15986</guid>
<content:encoded><![CDATA[

arXiv:2405.15986v2 Announce Type: replace 
Abstract: Diffusion models have become a leading method for generative modeling of both image and scientific data. As these models are costly to train and \emph{evaluate}, reducing the inference cost for diffusion models remains a major goal. Inspired by the recent empirical success in accelerating diffusion models via the parallel sampling technique~\cite{shih2024parallel}, we propose to divide the sampling process into $\mathcal{O}(1)$ blocks with parallelizable Picard iterations within each block. Rigorous theoretical analysis reveals that our algorithm achieves $\widetilde{\mathcal{O}}(\mathrm{poly} \log d)$ overall time complexity, marking \emph{the first implementation with provable sub-linear complexity w.r.t. the data dimension $d$}. Our analysis is based on a generalized version of Girsanov's theorem and is compatible with both the SDE and probability flow ODE implementations. Our results shed light on the potential of fast and efficient sampling of high-dimensional data on fast-evolving modern large-memory GPU clusters.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Laplacian-based Bayesian Multi-fidelity Modeling</title>
<link>https://arxiv.org/abs/2409.08211</link>
<guid>https://arxiv.org/abs/2409.08211</guid>
<content:encoded><![CDATA[

arXiv:2409.08211v2 Announce Type: replace 
Abstract: We present a novel probabilistic approach for generating multi-fidelity data while accounting for errors inherent in both low- and high-fidelity data. In this approach a graph Laplacian constructed from the low-fidelity data is used to define a multivariate Gaussian prior density for the coordinates of the true data points. In addition, few high-fidelity data points are used to construct a conjugate likelihood term. Thereafter, Bayes rule is applied to derive an explicit expression for the posterior density which is also multivariate Gaussian. The maximum \textit{a posteriori} (MAP) estimate of this density is selected to be the optimal multi-fidelity estimate. It is shown that the MAP estimate and the covariance of the posterior density can be determined through the solution of linear systems of equations. Thereafter, two methods, one based on spectral truncation and another based on a low-rank approximation, are developed to solve these equations efficiently. The multi-fidelity approach is tested on a variety of problems in solid and fluid mechanics with data that represents vectors of quantities of interest and discretized spatial fields in one and two dimensions. The results demonstrate that by utilizing a small fraction of high-fidelity data, the multi-fidelity approach can significantly improve the accuracy of a large collection of low-fidelity data points.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rapid optimization in high dimensional space by deep kernel learning augmented genetic algorithms</title>
<link>https://arxiv.org/abs/2410.03173</link>
<guid>https://arxiv.org/abs/2410.03173</guid>
<content:encoded><![CDATA[

arXiv:2410.03173v2 Announce Type: replace 
Abstract: Exploration of complex high-dimensional spaces presents significant challenges in fields such as molecular discovery, process optimization, and supply chain management. Genetic Algorithms (GAs), while offering significant power for creating new candidate spaces, often entail high computational demands due to the need for evaluation of each new proposed solution. On the other hand, Deep Kernel Learning (DKL) efficiently navigates the spaces of preselected candidate structures but lacks generative capabilities. This study introduces an approach that amalgamates the generative power of GAs to create new candidates with the efficiency of DKL-based surrogate models to rapidly ascertain the behavior of new candidate spaces. This DKL-GA framework can be further used to build Bayesian Optimization (BO) workflows. We demonstrate the effectiveness of this approach through the optimization of the FerroSIM model, showcasing its broad applicability to diverse challenges, including molecular discovery and battery charging optimization.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Based Reward Shaping for Adversarial Inverse Reinforcement Learning in Stochastic Environments</title>
<link>https://arxiv.org/abs/2410.03847</link>
<guid>https://arxiv.org/abs/2410.03847</guid>
<content:encoded><![CDATA[

arXiv:2410.03847v2 Announce Type: replace 
Abstract: In this paper, we aim to tackle the limitation of the Adversarial Inverse Reinforcement Learning (AIRL) method in stochastic environments where theoretical results cannot hold and performance is degraded. To address this issue, we propose a novel method which infuses the dynamics information into the reward shaping with the theoretical guarantee for the induced optimal policy in the stochastic environments. Incorporating our novel model-enhanced rewards, we present a novel Model-Enhanced AIRL framework, which integrates transition model estimation directly into reward shaping. Furthermore, we provide a comprehensive theoretical analysis of the reward error bound and performance difference bound for our method. The experimental results in MuJoCo benchmarks show that our method can achieve superior performance in stochastic environments and competitive performance in deterministic environments, with significant improvement in sample efficiency, compared to existing baselines.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretability for Time Series Transformers using A Concept Bottleneck Framework</title>
<link>https://arxiv.org/abs/2410.06070</link>
<guid>https://arxiv.org/abs/2410.06070</guid>
<content:encoded><![CDATA[

arXiv:2410.06070v2 Announce Type: replace 
Abstract: Mechanistic interpretability focuses on reverse engineering the internal mechanisms learned by neural networks. We extend our focus and propose to mechanistically forward engineer using our framework based on Concept Bottleneck Models. In the context of long-term time series forecasting, we modify the training objective to encourage a model to develop representations which are similar to predefined, interpretable concepts using Centered Kernel Alignment. This steers the bottleneck components to learn the predefined concepts, while allowing other components to learn other, undefined concepts. We apply the framework to the Vanilla Transformer, Autoformer and FEDformer, and present an in-depth analysis on synthetic data and on a variety of benchmark datasets. We find that the model performance remains mostly unaffected, while the model shows much improved interpretability. Additionally, we verify the interpretation of the bottleneck components with an intervention experiment using activation patching.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Market Trends with Enhanced Technical Indicator Integration and Classification Models</title>
<link>https://arxiv.org/abs/2410.06935</link>
<guid>https://arxiv.org/abs/2410.06935</guid>
<content:encoded><![CDATA[

arXiv:2410.06935v2 Announce Type: replace 
Abstract: Thanks to the high potential for profit, trading has become increasingly attractive to investors as the cryptocurrency and stock markets rapidly expand. However, because financial markets are intricate and dynamic, accurately predicting prices remains a significant challenge. The volatile nature of the cryptocurrency market makes it even harder for traders and investors to make decisions. This study presents a classification-based machine learning model to forecast the direction of the cryptocurrency market, i.e., whether prices will increase or decrease. The model is trained using historical data and important technical indicators such as the Moving Average Convergence Divergence, the Relative Strength Index, and the Bollinger Bands. We illustrate our approach with an empirical study of the closing price of Bitcoin. Several simulations, including a confusion matrix and Receiver Operating Characteristic curve, are used to assess the model's performance, and the results show a buy/sell signal accuracy of over 92\%. These findings demonstrate how machine learning models can assist investors and traders of cryptocurrencies in making wise/informed decisions in a very volatile market.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anomaly Resilient Temporal QoS Prediction using Hypergraph Convoluted Transformer Network</title>
<link>https://arxiv.org/abs/2410.17762</link>
<guid>https://arxiv.org/abs/2410.17762</guid>
<content:encoded><![CDATA[

arXiv:2410.17762v2 Announce Type: replace 
Abstract: Quality-of-Service (QoS) prediction is a critical task in the service lifecycle, enabling precise and adaptive service recommendations by anticipating performance variations over time in response to evolving network uncertainties and user preferences. However, contemporary QoS prediction methods frequently encounter data sparsity and cold-start issues, which hinder accurate QoS predictions and limit the ability to capture diverse user preferences. Additionally, these methods often assume QoS data reliability, neglecting potential credibility issues such as outliers and the presence of greysheep users and services with atypical invocation patterns. Furthermore, traditional approaches fail to leverage diverse features, including domain-specific knowledge and complex higher-order patterns, essential for accurate QoS predictions. In this paper, we introduce a real-time, trust-aware framework for temporal QoS prediction to address the aforementioned challenges, featuring an end-to-end deep architecture called the Hypergraph Convoluted Transformer Network (HCTN). HCTN combines a hypergraph structure with graph convolution over hyper-edges to effectively address high-sparsity issues by capturing complex, high-order correlations. Complementing this, the transformer network utilizes multi-head attention along with parallel 1D convolutional layers and fully connected dense blocks to capture both fine-grained and coarse-grained dynamic patterns. Additionally, our approach includes a sparsity-resilient solution for detecting greysheep users and services, incorporating their unique characteristics to improve prediction accuracy. Trained with a robust loss function resistant to outliers, HCTN demonstrated state-of-the-art performance on the large-scale WSDREAM-2 datasets for response time and throughput.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linearly Constrained Diffusion Implicit Models</title>
<link>https://arxiv.org/abs/2411.00359</link>
<guid>https://arxiv.org/abs/2411.00359</guid>
<content:encoded><![CDATA[

arXiv:2411.00359v2 Announce Type: replace 
Abstract: We introduce Linearly Constrained Diffusion Implicit Models (CDIM), a fast and accurate approach to solving noisy linear inverse problems using diffusion models. Traditional diffusion-based inverse methods rely on numerous projection steps to enforce measurement consistency in addition to unconditional denoising steps. CDIM achieves a 10-50x reduction in projection steps by dynamically adjusting the number and size of projection steps to align a residual measurement energy with its theoretical distribution under the forward diffusion process. This adaptive alignment preserves measurement consistency while substantially accelerating constrained inference. For noise-free linear inverse problems, CDIM exactly satisfies the measurement constraints with few projection steps, even when existing methods fail. We demonstrate CDIM's effectiveness across a range of applications, including super-resolution, denoising, inpainting, deblurring, and 3D point cloud reprojection. Code and an interactive demo can be found on our project website.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Un-mixing Test-time Adaptation under Heterogeneous Data Streams</title>
<link>https://arxiv.org/abs/2411.15173</link>
<guid>https://arxiv.org/abs/2411.15173</guid>
<content:encoded><![CDATA[

arXiv:2411.15173v4 Announce Type: replace 
Abstract: Deploying deep models in real-world scenarios remains challenging due to significant performance drops under distribution shifts between training and deployment environments. Test-Time Adaptation (TTA) has recently emerged as a promising solution, enabling on-the-fly model adaptation. However, its effectiveness deteriorates in the presence of mixed distribution shifts -- common in practical settings -- where multiple target domains coexist. In this paper, we study TTA under mixed distribution shifts and move beyond conventional whole-batch adaptation paradigms. By revisiting distribution shifts from a spectral perspective, we find that the heterogeneity across latent domains is often pronounced in Fourier space. In particular, high-frequency components encode domain-specific variations, which facilitates clearer separation of samples from different distributions. Motivated by this observation, we propose to un-mix heterogeneous data streams using high-frequency domain cues, making diverse shift patterns more tractable. To this end, we propose Frequency-based Decentralized Adaptation (FreDA), a novel framework that decomposes globally heterogeneous data stream into locally homogeneous clusters in the Fourier space. It leverages decentralized learning and augmentation strategies to robustly adapt under mixed domain shifts. Extensive experiments across various environments (corrupted, natural, and medical) show the superiority of our method over the state-of-the-arts.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPO-VCS: An End-to-End Smart Predict-then-Optimize Framework with Alternating Differentiation Method for Relocation Problems in Large-Scale Vehicle Crowd Sensing</title>
<link>https://arxiv.org/abs/2411.18432</link>
<guid>https://arxiv.org/abs/2411.18432</guid>
<content:encoded><![CDATA[

arXiv:2411.18432v3 Announce Type: replace 
Abstract: Ubiquitous mobile devices have catalyzed the development of vehicle crowd sensing (VCS). In particular, vehicle sensing systems show great potential in the flexible acquisition of spatio-temporal urban data through built-in sensors under diverse sensing scenarios. However, vehicle systems often exhibit biased coverage due to the heterogeneous nature of trip requests and routes. To achieve a high sensing coverage, a critical challenge lies in optimally relocating vehicles to minimize the divergence between vehicle distributions and target sensing distributions. Conventional approaches typically employ a two-stage predict-then-optimize (PTO) process: first predicting real-time vehicle distributions and subsequently generating an optimal relocation strategy based on the predictions. However, this approach can lead to suboptimal decision-making due to the propagation of errors from upstream prediction. To this end, we develop an end-to-end Smart Predict-then-Optimize (SPO) framework by integrating optimization into prediction within the deep learning architecture, and the entire framework is trained by minimizing the task-specific matching divergence rather than the upstream prediction error. Methodologically, we formulate the vehicle relocation problem by quadratic programming (QP) and incorporate a novel unrolling approach based on the Alternating Direction Method of Multipliers (ADMM) within the SPO framework to compute gradients of the QP layer, facilitating backpropagation and gradient-based optimization for end-to-end learning. The effectiveness of the proposed framework is validated by real-world taxi datasets in Hong Kong. Utilizing the alternating differentiation method, the general SPO framework presents a novel concept of addressing decision-making problems with uncertainty, demonstrating significant potential for advancing applications in intelligent transportation systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Compressive-Expressive Communication Framework for Compositional Representations</title>
<link>https://arxiv.org/abs/2501.19182</link>
<guid>https://arxiv.org/abs/2501.19182</guid>
<content:encoded><![CDATA[

arXiv:2501.19182v4 Announce Type: replace 
Abstract: Compositionality in knowledge and language--the ability to represent complex concepts as a combination of simpler ones--is a hallmark of human cognition and communication. Despite recent advances, deep neural networks still struggle to acquire this property reliably. Neural models for emergent communication look to endow artificial agents with compositional language by simulating the pressures that form human language. In this work, we introduce CELEBI (Compressive-Expressive Language Emergence through a discrete Bottleneck and Iterated learning), a novel self-supervised framework for inducing compositional representations through a reconstruction-based communication game between a sender and a receiver. Building on theories of language emergence and the iterated learning framework, we integrate three mechanisms that jointly promote compressibility, expressivity, and efficiency in the emergent language. First, Progressive Decoding incentivizes intermediate reasoning by requiring the receiver to produce partial reconstructions after each symbol. Second, Final-State Imitation trains successive generations of agents to imitate reconstructions rather than messages, enforcing a tighter communication bottleneck. Third, Pairwise Distance Maximization regularizes message diversity by encouraging high distances between messages, with formal links to entropy maximization. Our method significantly improves both the efficiency and compositionality of the learned messages on the Shapes3D and MPI3D datasets, surpassing prior discrete communication frameworks in both reconstruction accuracy and topographic similarity. This work provides new theoretical and empirical evidence for the emergence of structured, generalizable communication protocols from simplicity-based inductive biases.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Solvers for Discrete Diffusion Models: Theory and Applications of High-Order Algorithms</title>
<link>https://arxiv.org/abs/2502.00234</link>
<guid>https://arxiv.org/abs/2502.00234</guid>
<content:encoded><![CDATA[

arXiv:2502.00234v2 Announce Type: replace 
Abstract: Discrete diffusion models have emerged as a powerful generative modeling framework for discrete data with successful applications spanning from text generation to image synthesis. However, their deployment faces challenges due to the high dimensionality of the state space, necessitating the development of efficient inference algorithms. Current inference approaches mainly fall into two categories: exact simulation and approximate methods such as $\tau$-leaping. While exact methods suffer from unpredictable inference time and redundant function evaluations, $\tau$-leaping is limited by its first-order accuracy. In this work, we advance the latter category by tailoring the first extension of high-order numerical inference schemes to discrete diffusion models, enabling larger step sizes while reducing error. We rigorously analyze the proposed schemes and establish the second-order accuracy of the $\theta$-Trapezoidal method in KL divergence. Empirical evaluations on GSM8K-level math-reasoning, GPT-2-level text, and ImageNet-level image generation tasks demonstrate that our method achieves superior sample quality compared to existing approaches under equivalent computational constraints, with consistent performance gains across models ranging from 200M to 8B. Our code is available at https://github.com/yuchen-zhu-zyc/DiscreteFastSolver.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CVKAN: Complex-Valued Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2502.02417</link>
<guid>https://arxiv.org/abs/2502.02417</guid>
<content:encoded><![CDATA[

arXiv:2502.02417v4 Announce Type: replace 
Abstract: In this work we propose CVKAN, a complex-valued Kolmogorov-Arnold Network (KAN), to join the intrinsic interpretability of KANs and the advantages of Complex-Valued Neural Networks (CVNNs). We show how to transfer a KAN and the necessary associated mechanisms into the complex domain. To confirm that CVKAN meets expectations we conduct experiments on symbolic complex-valued function fitting and physically meaningful formulae as well as on a more realistic dataset from knot theory. Our proposed CVKAN is more stable and performs on par or better than real-valued KANs while requiring less parameters and a shallower network architecture, making it more explainable.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Logarithmic Regret of Exploration in Average Reward Markov Decision Processes</title>
<link>https://arxiv.org/abs/2502.06480</link>
<guid>https://arxiv.org/abs/2502.06480</guid>
<content:encoded><![CDATA[

arXiv:2502.06480v2 Announce Type: replace 
Abstract: In average reward Markov decision processes, state-of-the-art algorithms for regret minimization follow a well-established framework: They are model-based, optimistic and episodic. First, they maintain a confidence region from which optimistic policies are computed using a well-known subroutine called Extended Value Iteration (EVI). Second, these policies are used over time windows called episodes, each ended by the Doubling Trick (DT) rule or a variant thereof. In this work, without modifying EVI, we show that there is a significant advantage in replacing (DT) by another simple rule, that we call the Vanishing Multiplicative (VM) rule. When managing episodes with (VM), the algorithm's regret is, both in theory and in practice, as good if not better than with (DT), while the one-shot behavior is greatly improved. More specifically, the management of bad episodes (when sub-optimal policies are being used) is much better under (VM) than (DT) by making the regret of exploration logarithmic rather than linear. These results are made possible by a new in-depth understanding of the contrasting behaviors of confidence regions during good and bad episodes.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Almost Linear Time Consistent Mode Estimation and Quick Shift Clustering</title>
<link>https://arxiv.org/abs/2503.07995</link>
<guid>https://arxiv.org/abs/2503.07995</guid>
<content:encoded><![CDATA[

arXiv:2503.07995v2 Announce Type: replace 
Abstract: In this paper, we propose a method for density-based clustering in high-dimensional spaces that combines Locality-Sensitive Hashing (LSH) with the Quick Shift algorithm. The Quick Shift algorithm, known for its hierarchical clustering capabilities, is extended by integrating approximate Kernel Density Estimation (KDE) using LSH to provide efficient density estimates. The proposed approach achieves almost linear time complexity while preserving the consistency of density-based clustering.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Model Merging for Hybrid Data Learning: Leveraging Centralized Data to Refine Decentralized Models</title>
<link>https://arxiv.org/abs/2503.20138</link>
<guid>https://arxiv.org/abs/2503.20138</guid>
<content:encoded><![CDATA[

arXiv:2503.20138v3 Announce Type: replace 
Abstract: Current network training paradigms primarily focus on either centralized or decentralized data regimes. However, in practice, data availability often exhibits a hybrid nature, where both regimes coexist. This hybrid setting presents new opportunities for model training, as the two regimes offer complementary trade-offs: decentralized data is abundant but subject to heterogeneity and communication constraints, while centralized data, though limited in volume and potentially unrepresentative, enables better curation and high-throughput access. Despite its potential, effectively combining these paradigms remains challenging, and few frameworks are tailored to hybrid data regimes. To address this, we propose a novel framework that constructs a model atlas from decentralized models and leverages centralized data to refine a global model within this structured space. The refined model is then used to reinitialize the decentralized models. Our method synergizes federated learning (to exploit decentralized data) and model merging (to utilize centralized data), enabling effective training under hybrid data availability. Theoretically, we show that our approach achieves faster convergence than methods relying solely on decentralized data, due to variance reduction in the merging process. Extensive experiments demonstrate that our framework consistently outperforms purely centralized, purely decentralized, and existing hybrid-adaptable methods. Notably, our method remains robust even when the centralized and decentralized data domains differ or when decentralized data contains noise, significantly broadening its applicability.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning</title>
<link>https://arxiv.org/abs/2503.22738</link>
<guid>https://arxiv.org/abs/2503.22738</guid>
<content:encoded><![CDATA[

arXiv:2503.22738v2 Announce Type: replace 
Abstract: Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose ShieldAgent, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, ShieldAgent first constructs a safety policy model by extracting verifiable rules from policy documents and structuring them into a set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, ShieldAgent retrieves relevant rule circuits and generates a shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that ShieldAgent achieves SOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior methods by 11.3% on average with a high recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ga$_2$O$_3$ TCAD Mobility Parameter Calibration using Simulation Augmented Machine Learning with Physics Informed Neural Network</title>
<link>https://arxiv.org/abs/2504.02283</link>
<guid>https://arxiv.org/abs/2504.02283</guid>
<content:encoded><![CDATA[

arXiv:2504.02283v2 Announce Type: replace 
Abstract: In this paper, we demonstrate the feasibility of performing automatic Technology Computer Aided Design (TCAD) parameter calibration and extraction using machine learning, with the machine trained solely by TCAD simulation data. The methodology is validated using experimental data. Schottky Barrier Diodes (SBDs) with different effective anode workfunction (WF) are fabricated with emerging ultra-wide bandgap material, Gallium Oxide (Ga2O3), and are measured at various temperatures (T). Their current voltage curves are used for automatic Ga2O3 Philips Unified Mobility (PhuMob) model parameters calibration. Five critical PhuMob parameters were calibrated. The machine consists of an autoencoder and a neural network and is trained solely by TCAD simulation data with variations in WF, T, and the five PhuMob parameters (seven variations in total). Then, Ga2O3 PhuMob parameters are extracted from the noisy experimental curves. Subsequent TCAD simulation using the extracted parameters shows that the quality of the parameters is as good as an expert's calibration at the pre-turned on regime, but not in the on state regime. By using a simple physics-informed neural network, the machine performs as well as the human expert in all regimes.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KeepKV: Achieving Periodic Lossless KV Cache Compression for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2504.09936</link>
<guid>https://arxiv.org/abs/2504.09936</guid>
<content:encoded><![CDATA[

arXiv:2504.09936v2 Announce Type: replace 
Abstract: Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to preserve performance under strict memory constraints, achieving single-step lossless compression and providing error bounds for multi-step compression. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging method, compensating for attention loss resulting from cache merging. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage while successfully retaining essential context information, achieving over 2x inference throughput improvement and maintaining superior generation quality even with only 10% KV cache budgets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedCanon: Non-Convex Composite Federated Learning with Efficient Proximal Operation on Heterogeneous Data</title>
<link>https://arxiv.org/abs/2504.11903</link>
<guid>https://arxiv.org/abs/2504.11903</guid>
<content:encoded><![CDATA[

arXiv:2504.11903v2 Announce Type: replace 
Abstract: Composite federated learning offers a general framework for solving machine learning problems with additional regularization terms. However, existing methods often face significant limitations: many require clients to perform computationally expensive proximal operations, and their performance is frequently vulnerable to data heterogeneity. To overcome these challenges, we propose a novel composite federated learning algorithm called \textbf{FedCanon}, designed to solve the optimization problems comprising a possibly non-convex loss function and a weakly convex, potentially non-smooth regularization term. By decoupling proximal mappings from local updates, FedCanon requires only a single proximal evaluation on the server per iteration, thereby reducing the overall proximal computation cost. Concurrently, it integrates control variables into local updates to mitigate the client drift arising from data heterogeneity. The entire architecture avoids the complex subproblems of primal-dual alternatives. The theoretical analysis provides the first rigorous convergence guarantees for this proximal-skipping framework in the general non-convex setting. It establishes that FedCanon achieves a sublinear convergence rate, and a linear rate under the Polyak-{\L}ojasiewicz condition, without the restrictive bounded heterogeneity assumption. Extensive experiments demonstrate that FedCanon outperforms the state-of-the-art methods in terms of both accuracy and computational efficiency, particularly under heterogeneous data distributions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Rank Critical Road Segments via Heterogeneous Graphs with OD Flow Integration</title>
<link>https://arxiv.org/abs/2504.19199</link>
<guid>https://arxiv.org/abs/2504.19199</guid>
<content:encoded><![CDATA[

arXiv:2504.19199v2 Announce Type: replace 
Abstract: Existing learning-to-rank methods for road networks often fail to incorporate origin-destination (OD) flows and route information, limiting their ability to model long-range spatial dependencies. To address this gap, we propose HetGL2R, a heterogeneous graph learning framework for ranking road-segment importance. HetGL2R builds a tripartite graph that unifies OD flows, routes, and network topology, and further introduces attribute-guided graphs that elevate node attributes into explicit nodes to model functional similarity. A heterogeneous joint random walk algorithm (HetGWalk) samples both graph types to generate context-rich node sequences. These sequences are encoded with a Transformer to learn embeddings that capture long-range structural dependencies driven by OD demand and route configuration, as well as functional associations derived from attribute similarity. Finally, a listwise ranking strategy with a KL-divergence loss evaluates and ranks segment importance. Experiments on three SUMO-generated simulated networks of different scales show that, against state-of-the-art methods, HetGL2R achieves average improvements of approximately 7.52%, 4.40% and 3.57% in ranking performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Axial-UNet: A Neural Weather Model for Precipitation Nowcasting</title>
<link>https://arxiv.org/abs/2504.19408</link>
<guid>https://arxiv.org/abs/2504.19408</guid>
<content:encoded><![CDATA[

arXiv:2504.19408v2 Announce Type: replace 
Abstract: Accurately predicting short-term precipitation is critical for weather-sensitive applications such as disaster management, aviation, and urban planning. Traditional numerical weather prediction can be computationally intensive at high resolution and short lead times. In this work, we propose a lightweight UNet-based encoder-decoder augmented with axial-attention blocks that attend along image rows and columns to capture long-range spatial interactions, while temporal context is provided by conditioning on multiple past radar frames. Our hybrid architecture captures both local and long-range spatio-temporal dependencies from radar image sequences, enabling fixed lead-time precipitation nowcasting with modest compute. Experimental results on a preprocessed subset of the HKO-7 radar dataset demonstrate that our model outperforms ConvLSTM, pix2pix-style cGANs, and a plain UNet in pixel-fidelity metrics, reaching PSNR 47.67 and SSIM 0.9943. We report PSNR/SSIM here; extending evaluation to meteorology-oriented skill measures (e.g., CSI/FSS) is left to future work. The approach is simple, scalable, and effective for resource-constrained, real-time forecasting scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairPO: Robust Preference Optimization for Fair Multi-Label Learning</title>
<link>https://arxiv.org/abs/2505.02433</link>
<guid>https://arxiv.org/abs/2505.02433</guid>
<content:encoded><![CDATA[

arXiv:2505.02433v4 Announce Type: replace 
Abstract: Multi-label classification (MLC) often suffers from performance disparities across labels. We propose \textbf{FairPO}, a framework combining preference-based loss and group-robust optimization to improve fairness by targeting underperforming labels. FairPO partitions labels into a \textit{privileged} set for targeted improvement and a \textit{non-privileged} set to maintain baseline performance. For privileged labels, a DPO-inspired preference loss addresses hard examples by correcting ranking errors between true labels and their confusing counterparts. A constrained objective maintains performance for non-privileged labels, while a Group Robust Preference Optimization (GRPO) formulation adaptively balances both objectives to mitigate bias. We also demonstrate FairPO's versatility with reference-free variants using Contrastive (CPO) and Simple (SimPO) Preference Optimization.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unraveling the Rainbow: can value-based methods schedule?</title>
<link>https://arxiv.org/abs/2505.03323</link>
<guid>https://arxiv.org/abs/2505.03323</guid>
<content:encoded><![CDATA[

arXiv:2505.03323v2 Announce Type: replace 
Abstract: In this work, we conduct an extensive empirical study of several deep reinforcement learning algorithms on two challenging combinatorial optimization problems: the job-shop and flexible job-shop scheduling problems, both fundamental challenges with multiple industrial applications. Broadly, deep reinforcement learning algorithms fall into two categories: policy-gradient and value-based. While value-based algorithms have achieved notable success in domains such as the Arcade Learning Environment, the combinatorial optimization community has predominantly favored policy-gradient algorithms, often overlooking the potential of value-based alternatives. From our results, value-based algorithms demonstrated a lower variance and a more stable convergence profile compared to policy-gradient ones. Moreover, they achieved superior cross-size and cross-distribution generalization, that is, effectively solving instances that are substantially larger or structurally distinct from those seen during training. Finally, our analysis also suggests that the relative performance of each category of algorithms may be dependent on structural properties of the problem, such as problem flexibility and instance size. Overall, our findings challenge the prevailing assumption that policy-gradient algorithms are inherently superior for combinatorial optimization. We show instead that value-based algorithms can match or even surpass the performance of policy-gradient algorithms, suggesting that they deserve greater attention from the combinatorial optimization community. Our code is openly available at: https://github.com/AJ-Correa/Unraveling-the-Rainbow
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FP64 is All You Need: Rethinking Failure Modes in Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.10949</link>
<guid>https://arxiv.org/abs/2505.10949</guid>
<content:encoded><![CDATA[

arXiv:2505.10949v2 Announce Type: replace 
Abstract: Physics Informed Neural Networks (PINNs) often exhibit failure modes in which the PDE residual loss converges while the solution error stays large, a phenomenon traditionally blamed on local optima separated from the true solution by steep loss barriers. We challenge this understanding by demonstrate that the real culprit is insufficient arithmetic precision: with standard FP32, the LBFGS optimizer prematurely satisfies its convergence test, freezing the network in a spurious failure phase. Simply upgrading to FP64 rescues optimization, enabling vanilla PINNs to solve PDEs without any failure modes. These results reframe PINN failure modes as precision induced stalls rather than inescapable local minima and expose a three stage training dynamic unconverged, failure, success whose boundaries shift with numerical precision. Our findings emphasize that rigorous arithmetic precision is the key to dependable PDE solving with neural networks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mu$PC: Scaling Predictive Coding to 100+ Layer Networks</title>
<link>https://arxiv.org/abs/2505.13124</link>
<guid>https://arxiv.org/abs/2505.13124</guid>
<content:encoded><![CDATA[

arXiv:2505.13124v2 Announce Type: replace 
Abstract: The biological implausibility of backpropagation (BP) has motivated many alternative, brain-inspired algorithms that attempt to rely only on local information, such as predictive coding (PC) and equilibrium propagation. However, these algorithms have notoriously struggled to train very deep networks, preventing them from competing with BP in large-scale settings. Indeed, scaling PC networks (PCNs) has recently been posed as a challenge for the community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can be trained reliably using a Depth-$\mu$P parameterisation (Yang et al., 2023; Bordelon et al., 2023) which we call "$\mu$PC". By analysing the scaling behaviour of PCNs, we reveal several pathologies that make standard PCNs difficult to train at large depths. We then show that, despite addressing only some of these instabilities, $\mu$PC allows stable training of very deep (up to 128-layer) residual networks on simple classification tasks with competitive performance and little tuning compared to current benchmarks. Moreover, $\mu$PC enables zero-shot transfer of both weight and activity learning rates across widths and depths. Our results serve as a first step towards scaling PC to more complex architectures and have implications for other local algorithms. Code for $\mu$PC is made available as part of a JAX library for PCNs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation</title>
<link>https://arxiv.org/abs/2505.19194</link>
<guid>https://arxiv.org/abs/2505.19194</guid>
<content:encoded><![CDATA[

arXiv:2505.19194v3 Announce Type: replace 
Abstract: Adversarial attack reveals the vulnerability of deep learning models. It is assumed that high curvature may give rise to rough decision boundary and thus result in less robust models. However, the most commonly used \textit{curvature} is the curvature of loss function, scores or other parameters from within the model as opposed to decision boundary curvature, since the former can be relatively easily formed using second order derivative. In this paper, we propose a new query-efficient method, dynamic curvature estimation (DCE), to estimate the decision boundary curvature in a black-box setting. Our approach is based on CGBA, a black-box adversarial attack. By performing DCE on a wide range of classifiers, we discovered, statistically, a connection between decision boundary curvature and adversarial robustness. We also propose a new attack method, curvature dynamic black-box attack (CDBA) with improved performance using the estimated curvature.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabPFN: One Model to Rule Them All?</title>
<link>https://arxiv.org/abs/2505.20003</link>
<guid>https://arxiv.org/abs/2505.20003</guid>
<content:encoded><![CDATA[

arXiv:2505.20003v2 Announce Type: replace 
Abstract: Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a transformer-based deep learning model for regression and classification on tabular data, which they claim "outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time." Furthermore, they have called TabPFN a "foundation model" for tabular data, as it can support "data generation, density estimation, learning reusable embeddings and fine-tuning". In this paper, we provide a tailored explanation of how TabPFN works for a statistics audience, by emphasizing its interpretation as approximate Bayesian inference. We then explore the significance of TabPFN to the field of statistics: We show that an out-of-the-box application of TabPFN can sometimes outperform specialized state-of-the-art methods for semi-supervised parameter estimation, prediction under covariate shift, and heterogeneous treatment effect estimation. As a partial explanation for the predictive effectiveness of TabPFN, we show that it can simultaneously adapt to both nonparametric structure and parametric structure, for instance, sometimes outperforming LASSO even when assumptions are correctly specified. All experiments can be reproduced using the code provided at https://github.com/qinglong-tian/tabpfn_study (https://github.com/qinglong-tian/tabpfn_study).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents</title>
<link>https://arxiv.org/abs/2505.23055</link>
<guid>https://arxiv.org/abs/2505.23055</guid>
<content:encoded><![CDATA[

arXiv:2505.23055v2 Announce Type: replace 
Abstract: Clinical decision-making is inherently complex and fast-paced, particularly in emergency departments (EDs) where critical, rapid and high-stakes decisions are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools that combine signs, symptoms, and clinical variables into decision trees to make consistent and accurate diagnoses. CDR usage is often hindered by the clinician's cognitive load, limiting their ability to quickly recall and apply the appropriate rules. We introduce CDR-Agent, a novel LLM-based system designed to enhance ED decision-making by autonomously identifying and applying the most appropriate CDRs based on unstructured clinical notes. To validate CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\% (synthetic) and 8.7\% (CDR-Bench) accuracy gain relative to the standalone LLM baseline in CDR selection. Moreover, CDR-Agent significantly reduces computational overhead. Using these datasets, we demonstrated that CDR-Agent not only selects relevant CDRs efficiently, but makes cautious yet effective imaging decisions by minimizing unnecessary interventions while successfully identifying most positively diagnosed cases, outperforming traditional LLM prompting approaches. Code for our work can be found at: https://github.com/zhenxianglance/medagent-cdr-agent
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network Inversion for Uncertainty-Aware Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.23448</link>
<guid>https://arxiv.org/abs/2505.23448</guid>
<content:encoded><![CDATA[

arXiv:2505.23448v2 Announce Type: replace 
Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are critical components for building safe machine learning systems, especially in real-world scenarios where unexpected inputs are inevitable. However the two problems have, until recently, separately been addressed. In this work, we propose a novel framework that combines network inversion with classifier training to simultaneously address both OOD detection and uncertainty estimation. For a standard n-class classification task, we extend the classifier to an (n+1)-class model by introducing a "garbage" class, initially populated with random gaussian noise to represent outlier inputs. After each training epoch, we use network inversion to reconstruct input images corresponding to all output classes that initially appear as noisy and incoherent and are therefore excluded to the garbage class for retraining the classifier. This cycle of training, inversion, and exclusion continues iteratively till the inverted samples begin to resemble the in-distribution data more closely, with a significant drop in the uncertainty, suggesting that the classifier has learned to carve out meaningful decision boundaries while sanitising the class manifolds by pushing OOD content into the garbage class. During inference, this training scheme enables the model to effectively detect and reject OOD samples by classifying them into the garbage class. Furthermore, the confidence scores associated with each prediction can be used to estimate uncertainty for both in-distribution and OOD inputs. Our approach is scalable, interpretable, and does not require access to external OOD datasets or post-hoc calibration techniques while providing a unified solution to the dual challenges of OOD detection and uncertainty estimation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title>
<link>https://arxiv.org/abs/2505.24592</link>
<guid>https://arxiv.org/abs/2505.24592</guid>
<content:encoded><![CDATA[

arXiv:2505.24592v2 Announce Type: replace 
Abstract: Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruption, adversarial attacks, and domain shifts. Data augmentation is one of the prevalent and effective ways to enhance robustness. Despite the great success of augmentations in different fields, a general theoretical understanding of their efficacy in improving model robustness is lacking. We offer a unified theoretical framework to clarify how augmentations can enhance model robustness through the lens of loss surface flatness and PAC generalization bound. Our work diverges from prior studies in that our analysis i) broadly encompasses much of the existing augmentation methods, and ii) is not limited to specific types of distribution shifts like adversarial attacks. We confirm our theories through simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets, as well as domain generalization benchmarks including PACS and OfficeHome.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning</title>
<link>https://arxiv.org/abs/2506.07619</link>
<guid>https://arxiv.org/abs/2506.07619</guid>
<content:encoded><![CDATA[

arXiv:2506.07619v2 Announce Type: replace 
Abstract: Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models</title>
<link>https://arxiv.org/abs/2506.10177</link>
<guid>https://arxiv.org/abs/2506.10177</guid>
<content:encoded><![CDATA[

arXiv:2506.10177v2 Announce Type: replace 
Abstract: Diffusion-based generative models employ stochastic differential equations (SDEs) and their equivalent probability flow ordinary differential equations (ODEs) to establish a smooth transformation between complex high-dimensional data distributions and tractable prior distributions. In this paper, we reveal a striking geometric regularity in the deterministic sampling dynamics of diffusion generative models: each simulated sampling trajectory along the gradient field lies within an extremely low-dimensional subspace, and all trajectories exhibit an almost identical boomerang shape, regardless of the model architecture, applied conditions, or generated content. We characterize several intriguing properties of these trajectories, particularly under closed-form solutions based on kernel-estimated data modeling. We also demonstrate a practical application of the discovered trajectory regularity by proposing a dynamic programming-based scheme to better align the sampling time schedule with the underlying trajectory structure. This simple strategy requires minimal modification to existing deterministic numerical solvers, incurs negligible computational overhead, and achieves superior image generation performance, especially in regions with only 5 - 10 function evaluations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CheMixHub: Datasets and Benchmarks for Chemical Mixture Property Prediction</title>
<link>https://arxiv.org/abs/2506.12231</link>
<guid>https://arxiv.org/abs/2506.12231</guid>
<content:encoded><![CDATA[

arXiv:2506.12231v2 Announce Type: replace 
Abstract: Developing improved predictive models for multi-molecular systems is crucial, as nearly every chemical product used results from a mixture of chemicals. While being a vital part of the industry pipeline, the chemical mixture space remains relatively unexplored by the Machine Learning community. In this paper, we introduce CheMixHub, a holistic benchmark for molecular mixtures, covering a corpus of 11 chemical mixtures property prediction tasks, from drug delivery formulations to battery electrolytes, totalling approximately 500k data points gathered and curated from 7 publicly available datasets. CheMixHub introduces various data splitting techniques to assess context-specific generalization and model robustness, providing a foundation for the development of predictive models for chemical mixture properties. Furthermore, we map out the modelling space of deep learning models for chemical mixtures, establishing initial benchmarks for the community. This dataset has the potential to accelerate chemical mixture development, encompassing reformulation, optimization, and discovery. The dataset and code for the benchmarks can be found at: https://github.com/chemcognition-lab/chemixhub
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title>
<link>https://arxiv.org/abs/2506.12484</link>
<guid>https://arxiv.org/abs/2506.12484</guid>
<content:encoded><![CDATA[

arXiv:2506.12484v5 Announce Type: replace 
Abstract: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new state-of-the-art for robust unlearning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated ADMM from Bayesian Duality</title>
<link>https://arxiv.org/abs/2506.13150</link>
<guid>https://arxiv.org/abs/2506.13150</guid>
<content:encoded><![CDATA[

arXiv:2506.13150v2 Announce Type: replace 
Abstract: We propose a new Bayesian approach to derive and extend the federated Alternating Direction Method of Multipliers (ADMM). We show that the solutions of variational-Bayesian objectives are associated with a duality structure that not only resembles ADMM but also extends it. For example, ADMM-like updates are recovered when the objective is optimized over the isotropic-Gaussian family, and new non-trivial extensions are obtained for other more flexible exponential families. Examples include a Newton-like variant that converges in one step on quadratics and an Adam-like variant called IVON-ADMM that has the same cost as Adam but yields up to 7% accuracy boosts in heterogeneous deep learning. Our work opens a new direction to use Bayes to extend ADMM and other primal-dual methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction</title>
<link>https://arxiv.org/abs/2506.18396</link>
<guid>https://arxiv.org/abs/2506.18396</guid>
<content:encoded><![CDATA[

arXiv:2506.18396v2 Announce Type: replace 
Abstract: Leukemia diagnosis and monitoring rely increasingly on high-throughput image data, yet conventional clustering methods lack the flexibility to accommodate evolving cellular patterns and quantify uncertainty in real time. We introduce Adaptive and Dynamic Neuro-Fuzzy Clustering, a novel streaming-capable framework that combines Convolutional Neural Network-based feature extraction with an online fuzzy clustering engine. ADNF initializes soft partitions via Fuzzy C-Means, then continuously updates micro-cluster centers, densities, and fuzziness parameters using a Fuzzy Temporal Index (FTI) that measures entropy evolution. A topology refinement stage performs density-weighted merging and entropy-guided splitting to guard against over- and under-segmentation. On the C-NMC leukemia microscopy dataset, our tool achieves a silhouette score of 0.51, demonstrating superior cohesion and separation over static baselines. The method's adaptive uncertainty modeling and label-free operation hold immediate potential for integration within the INFANT pediatric oncology network, enabling scalable, up-to-date support for personalized leukemia management.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the necessity of adaptive regularisation:Optimal anytime online learning on $\boldsymbol{\ell_p}$-balls</title>
<link>https://arxiv.org/abs/2506.19752</link>
<guid>https://arxiv.org/abs/2506.19752</guid>
<content:encoded><![CDATA[

arXiv:2506.19752v3 Announce Type: replace 
Abstract: We study online convex optimization on $\ell_p$-balls in $\mathbb{R}^d$ for $p > 2$. While always sub-linear, the optimal regret exhibits a shift between the high-dimensional setting ($d > T$), when the dimension $d$ is greater than the time horizon $T$ and the low-dimensional setting ($d \leq T$). We show that Follow-the-Regularised-Leader (FTRL) with time-varying regularisation which is adaptive to the dimension regime is anytime optimal for all dimension regimes. Motivated by this, we ask whether it is possible to obtain anytime optimality of FTRL with fixed non-adaptive regularisation. Our main result establishes that for separable regularisers, adaptivity in the regulariser is necessary, and that any fixed regulariser will be sub-optimal in one of the two dimension regimes. Finally, we provide lower bounds which rule out sub-linear regret bounds for the linear bandit problem in sufficiently high-dimension for all $\ell_p$-balls with $p \geq 1$.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards</title>
<link>https://arxiv.org/abs/2506.20520</link>
<guid>https://arxiv.org/abs/2506.20520</guid>
<content:encoded><![CDATA[

arXiv:2506.20520v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive Groupwise Comparison for Reinforcement Learning from Human Feedback</title>
<link>https://arxiv.org/abs/2507.04340</link>
<guid>https://arxiv.org/abs/2507.04340</guid>
<content:encoded><![CDATA[

arXiv:2507.04340v2 Announce Type: replace 
Abstract: Reinforcement learning from human feedback (RLHF) has emerged as a key enabling technology for aligning AI behaviour with human preferences. The traditional way to collect data in RLHF is via pairwise comparisons: human raters are asked to indicate which one of two samples they prefer. We present an interactive visualisation that better exploits the human visual ability to compare and explore whole groups of samples. The interface is comprised of two linked views: 1) an exploration view showing a contextual overview of all sampled behaviours organised in a hierarchical clustering structure; and 2) a comparison view displaying two selected groups of behaviours for user queries. Users can efficiently explore large sets of behaviours by iterating between these two views. Additionally, we devised an active learning approach suggesting groups for comparison. As shown by our evaluation in six simulated robotics tasks, our approach increases the final rewards by 69.34%. It leads to lower error rates and better policies. We open-source the code that can be easily integrated into the RLHF training loop, supporting research on human-AI alignment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy</title>
<link>https://arxiv.org/abs/2507.06969</link>
<guid>https://arxiv.org/abs/2507.06969</guid>
<content:encoded><![CDATA[

arXiv:2507.06969v3 Announce Type: replace 
Abstract: Differentially private (DP) mechanisms are difficult to interpret and calibrate because existing methods for mapping standard privacy parameters to concrete privacy risks -- re-identification, attribute inference, and data reconstruction -- are both overly pessimistic and inconsistent. In this work, we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that bounds on attack success can take the same unified form across re-identification, attribute inference, and data reconstruction risks. Our unified bounds are (1) consistent across a multitude of attack settings, and (2) tunable, enabling practitioners to evaluate risk with respect to arbitrary, including worst-case, levels of baseline risk. Empirically, our results are tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated DP. As a result, calibrating noise using our bounds can reduce the required noise by 20% at the same risk level, which yields, e.g., an accuracy increase from 52% to 70% in a text classification task. Overall, this unifying perspective provides a principled framework for interpreting and calibrating the degree of protection in DP against specific levels of re-identification, attribute inference, or data reconstruction risk.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Scores: Proximal Diffusion Models</title>
<link>https://arxiv.org/abs/2507.08956</link>
<guid>https://arxiv.org/abs/2507.08956</guid>
<content:encoded><![CDATA[

arXiv:2507.08956v2 Announce Type: replace 
Abstract: Diffusion models have quickly become some of the most popular and powerful generative models for high-dimensional data. The key insight that enabled their development was the realization that access to the score -- the gradient of the log-density at different noise levels -- allows for sampling from data distributions by solving a reverse-time stochastic differential equation (SDE) via forward discretization, and that popular denoisers allow for unbiased estimators of this score. In this paper, we demonstrate that an alternative, backward discretization of these SDEs, using proximal maps in place of the score, leads to theoretical and practical benefits. We leverage recent results in proximal matching to learn proximal operators of the log-density and, with them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that $\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL divergence. Empirically, we show that two variants of ProxDM achieve significantly faster convergence within just a few sampling steps compared to conventional score-matching methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Unlearning of Traffic State Estimation and Prediction</title>
<link>https://arxiv.org/abs/2507.17984</link>
<guid>https://arxiv.org/abs/2507.17984</guid>
<content:encoded><![CDATA[

arXiv:2507.17984v3 Announce Type: replace 
Abstract: Data-driven traffic state estimation and prediction (TSEP) relies heavily on data sources that contain sensitive information. While the abundance of data has fueled significant breakthroughs, particularly in machine learning-based methods, it also raises concerns regarding privacy, cybersecurity, and data freshness. These issues can erode public trust in intelligent transportation systems. Recently, regulations have introduced the "right to be forgotten", allowing users to request the removal of their private data from models. As machine learning models can remember old data, simply removing it from back-end databases is insufficient in such systems. To address these challenges, this study introduces a novel learning paradigm for TSEP-Machine Unlearning TSEP-which enables a trained TSEP model to selectively forget privacy-sensitive, poisoned, or outdated data. By empowering models to "unlearn," we aim to enhance the trustworthiness and reliability of data-driven traffic TSEP.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination</title>
<link>https://arxiv.org/abs/2508.13532</link>
<guid>https://arxiv.org/abs/2508.13532</guid>
<content:encoded><![CDATA[

arXiv:2508.13532v2 Announce Type: replace 
Abstract: With the increasing penetration of renewable generation on the power grid, maintaining system balance requires coordinated demand flexibility from aggregations of buildings. Reinforcement learning (RL) has been widely explored for building controls because of its model-free nature. Open-source simulation testbeds are essential not only for training RL agents but also for fairly benchmarking control strategies. However, most building-sector testbeds target single buildings; multi-building platforms are relatively limited and typically rely on simplified models (e.g., Resistanc-Capacitance) or data-driven approaches, which lack the ability to fully capture the physical intricacies and intermediate variables necessary for interpreting control performance. Moreover, these platforms often impose fixed inputs, outputs, and model formats, restricting their applicability as benchmarking tools across diverse control scenarios. To address these gaps, MuFlex, a scalable, open-source platform for multi-building flexibility coordination, was developed. MuFlex enables synchronous information exchange across EnergyPlus building models and adheres to the latest OpenAI Gym interface, providing a modular, standardized RL implementation. The platform's capabilities were demonstrated in a case study coordinating demand flexibility across four office buildings using the Soft Actor-Critic (SAC) algorithm. The results show that under four buildings' coordination, SAC effectively reduced the aggregated peak demand by nearly 12% with maintained indoor comfort to ensure the power demand below the threshold. The platform is released open-source on GitHub: https://github.com/BuildNexusX/MuFlex.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Defining Neural Averaging</title>
<link>https://arxiv.org/abs/2508.14832</link>
<guid>https://arxiv.org/abs/2508.14832</guid>
<content:encoded><![CDATA[

arXiv:2508.14832v2 Announce Type: replace 
Abstract: What does it even mean to average neural networks? We investigate the problem of synthesizing a single neural network from a collection of pretrained models, each trained on disjoint data shards, using only their final weights and no access to training data. In forming a definition of neural averaging, we take insight from model soup, which appears to aggregate multiple models into a singular model while enhancing generalization performance. In this work, we reinterpret model souping as a special case of a broader framework: Amortized Model Ensembling (AME) for neural averaging, a data-free meta-optimization approach that treats model differences as pseudogradients to guide neural weight updates. We show that this perspective not only recovers model soup but enables more expressive and adaptive ensembling strategies. Empirically, AME produces averaged neural solutions that outperform both individual experts and model soup baselines, especially in out-of-distribution settings. Our results suggest a principled and generalizable notion of data-free model weight aggregation and defines, in one sense, how to perform neural averaging.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SACA: Selective Attention-Based Clustering Algorithm</title>
<link>https://arxiv.org/abs/2508.17150</link>
<guid>https://arxiv.org/abs/2508.17150</guid>
<content:encoded><![CDATA[

arXiv:2508.17150v2 Announce Type: replace 
Abstract: Clustering algorithms are fundamental tools across many fields, with density-based methods offering particular advantages in identifying arbitrarily shaped clusters and handling noise. However, their effectiveness is often limited by the requirement of critical parameter tuning by users, which typically requires significant domain expertise. This paper introduces a novel density-based clustering algorithm loosely inspired by the concept of selective attention, designed to minimize reliance on parameter tuning for most applications. The proposed method computes an adaptive threshold to exclude sparsely distributed points and outliers, constructs an initial cluster framework, and subsequently reintegrates the filtered points to refine the final results. Extensive experiments on diverse benchmark datasets demonstrate the robustness, accuracy, and ease of use of the proposed approach, establishing it as a powerful alternative to conventional density-based clustering techniques.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composition and Alignment of Diffusion Models using Constrained Learning</title>
<link>https://arxiv.org/abs/2508.19104</link>
<guid>https://arxiv.org/abs/2508.19104</guid>
<content:encoded><![CDATA[

arXiv:2508.19104v2 Announce Type: replace 
Abstract: Diffusion models have become prevalent in generative modeling due to their ability to sample from complex distributions. To improve the quality of generated samples and their compliance with user requirements, two commonly used methods are: (i) Alignment, which involves finetuning a diffusion model to align it with a reward; and (ii) Composition, which combines several pretrained diffusion models together, each emphasizing a desirable attribute in the generated outputs. However, trade-offs often arise when optimizing for multiple rewards or combining multiple models, as they can often represent competing properties. Existing methods cannot guarantee that the resulting model faithfully generates samples with all the desired properties. To address this gap, we propose a constrained optimization framework that unifies alignment and composition of diffusion models by enforcing that the aligned model satisfies reward constraints and/or remains close to each pretrained model. We provide a theoretical characterization of the solutions to the constrained alignment and composition problems and develop a Lagrangian-based primal-dual training algorithm to approximate these solutions. Empirically, we demonstrate our proposed approach in image generation, applying it to alignment and composition, and show that our aligned or composed model satisfies constraints effectively. Our implementation can be found at: \href{https://github.com/shervinkhalafi/constrained_comp_align}{https://github.com/shervinkhalafi/constrained\_comp\_align}
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Drone Route Optimization in Post-Disaster Road Assessment</title>
<link>https://arxiv.org/abs/2509.01886</link>
<guid>https://arxiv.org/abs/2509.01886</guid>
<content:encoded><![CDATA[

arXiv:2509.01886v2 Announce Type: replace 
Abstract: Rapid post-disaster road damage assessment is critical for effective emergency response, yet traditional optimization methods suffer from excessive computational time and require domain knowledge for algorithm design, making them unsuitable for time-sensitive disaster scenarios. This study proposes an attention-based encoder-decoder model (AEDM) for rapid drone routing decision in post-disaster road damage assessment. The method employs deep reinforcement learning to determine high-quality drone assessment routes without requiring algorithmic design knowledge. A network transformation method is developed to convert link-based routing problems into equivalent node-based formulations, while a synthetic road network generation technique addresses the scarcity of large-scale training datasets. The model is trained using policy optimization with multiple optima (POMO) with multi-task learning capabilities to handle diverse parameter combinations. Experimental results demonstrate two key strengths of AEDM: it outperforms commercial solvers by 20--71\% and traditional heuristics by 23--35\% in solution quality, while achieving rapid inference (1--2 seconds) versus 100--2,000 seconds for traditional methods. The model exhibits strong generalization across varying problem scales, drone numbers, and time constraints, consistently outperforming baseline methods on unseen parameter distributions and real-world road networks. The proposed method effectively balances computational efficiency with solution quality, making it particularly suitable for time-critical disaster response applications where rapid decision-making is essential for saving lives. The source code for AEDM is publicly available at https://github.com/PJ-HTU/AEDM-for-Post-disaster-road-assessment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Particle Dynamics Subject to Rigid Body Manipulations Using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.03446</link>
<guid>https://arxiv.org/abs/2509.03446</guid>
<content:encoded><![CDATA[

arXiv:2509.03446v2 Announce Type: replace 
Abstract: Simulating particle dynamics with high fidelity is crucial for solving real-world interaction and control tasks involving liquids in design, graphics, and robotics. Recently, data-driven approaches, particularly those based on graph neural networks (GNNs), have shown progress in tackling such problems. However, these approaches are often limited to learning fluid behavior in static free-fall environments or simple manipulation settings involving primitive objects, often overlooking complex interactions with dynamically moving kinematic rigid bodies. Here, we propose a GNN-based framework designed from the ground up to learn the dynamics of liquids under rigid body interactions and active manipulations, where particles are represented as graph nodes and particle-object collisions are handled using surface representations with the bounding volume hierarchy (BVH) algorithm. Our approach accurately captures fluid behavior in dynamic settings and can also function as a simulator in static free-fall environments. Despite being trained on single-object manipulation tasks, our model generalizes effectively to environments with novel objects and novel manipulation tasks. Finally, we show that the learned dynamics can be leveraged to solve control and manipulation tasks using gradient-based optimization methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset</title>
<link>https://arxiv.org/abs/2509.04449</link>
<guid>https://arxiv.org/abs/2509.04449</guid>
<content:encoded><![CDATA[

arXiv:2509.04449v3 Announce Type: replace 
Abstract: We present ChronoGraph, a graph-structured multivariate time series forecasting dataset built from real-world production microservices. Each node is a service that emits a multivariate stream of system-level performance metrics, capturing CPU, memory, and network usage patterns, while directed edges encode dependencies between services. The primary task is forecasting future values of these signals at the service level. In addition, ChronoGraph provides expert-annotated incident windows as anomaly labels, enabling evaluation of anomaly detection methods and assessment of forecast robustness during operational disruptions. Compared to existing benchmarks from industrial control systems or traffic and air-quality domains, ChronoGraph uniquely combines (i) multivariate time series, (ii) an explicit, machine-readable dependency graph, and (iii) anomaly labels aligned with real incidents. We report baseline results spanning forecasting models, pretrained time-series foundation models, and standard anomaly detectors. ChronoGraph offers a realistic benchmark for studying structure-aware forecasting and incident-aware evaluation in microservice systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Diffusion Models as Energy Minimization</title>
<link>https://arxiv.org/abs/2509.13866</link>
<guid>https://arxiv.org/abs/2509.13866</guid>
<content:encoded><![CDATA[

arXiv:2509.13866v2 Announce Type: replace 
Abstract: We present a systematic theoretical framework that interprets masked diffusion models (MDMs) as solutions to energy minimization problems in discrete optimal transport. Specifically, we prove that three distinct energy formulations--kinetic, conditional kinetic, and geodesic energy--are mathematically equivalent under the structure of MDMs, and that MDMs minimize all three when the mask schedule satisfies a closed-form optimality condition. This unification not only clarifies the theoretical foundations of MDMs, but also motivates practical improvements in sampling. By parameterizing interpolation schedules via Beta distributions, we reduce the schedule design space to a tractable 2D search, enabling efficient post-training tuning without model modification. Experiments on synthetic and real-world benchmarks demonstrate that our energy-inspired schedules outperform hand-crafted baselines, particularly in low-step sampling settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria</title>
<link>https://arxiv.org/abs/2509.16040</link>
<guid>https://arxiv.org/abs/2509.16040</guid>
<content:encoded><![CDATA[

arXiv:2509.16040v3 Announce Type: replace 
Abstract: The automated discovery of constitutive models from data has recently emerged as a promising alternative to the traditional model calibration paradigm. In this work, we present a fully automated framework for constitutive model discovery that systematically pairs three sparse regression algorithms Least Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression (LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection criteria: $K$-fold cross-validation (CV), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). This pairing yields nine distinct algorithms for model discovery and enables a systematic exploration of the trade-off between sparsity, predictive performance, and computational cost. While LARS serves as an efficient path-based solver for the $\ell_1$-constrained problem, OMP is introduced as a tractable heuristic for $\ell_0$-regularized selection. The framework is applied to both isotropic and anisotropic hyperelasticity, utilizing both synthetic and experimental datasets. Results reveal that all nine algorithm-criterion combinations perform consistently well in discovering isotropic and anisotropic materials, yielding highly accurate constitutive models. These findings broaden the range of viable discovery algorithms beyond $\ell_1$-based approaches such as LASSO.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Manifold-Aware Diffusion-Augmented Contrastive Learning for Noise-Robust Biosignal Representation</title>
<link>https://arxiv.org/abs/2509.20048</link>
<guid>https://arxiv.org/abs/2509.20048</guid>
<content:encoded><![CDATA[

arXiv:2509.20048v3 Announce Type: replace 
Abstract: Learning robust representations for physiological time-series signals continues to pose a substantial challenge in developing efficient few-shot learning applications. This difficulty is largely due to the complex pathological variations in biosignals. In this context, this paper introduces a manifold-aware Diffusion-Augmented Contrastive Learning (DACL) framework, which efficiently leverages the generative structure of latent diffusion models with the discriminative power of supervised contrastive learning. The proposed framework operates within a contextualized scattering latent space derived from Scattering Transformer (ST) features. Within a contrastive learning framework, we employ a forward diffusion process in the scattering latent space as a structured manifold-aware feature augmentation technique. We assessed the proposed framework using the PhysioNet 2017 ECG benchmark dataset. The proposed method achieved a competitive AUROC of 0.9741 in the task of detecting atrial fibrillation from a single-lead ECG signal. The proposed framework achieved performance on par with relevant state-of-the-art related works. In-depth evaluation findings suggest that early-stage diffusion serves as an ideal "local manifold explorer," producing embeddings with greater precision than typical augmentation methods while preserving inference efficiency.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Inductive Bias for Data-Efficient Generalization in State Space Models</title>
<link>https://arxiv.org/abs/2509.20789</link>
<guid>https://arxiv.org/abs/2509.20789</guid>
<content:encoded><![CDATA[

arXiv:2509.20789v3 Announce Type: replace 
Abstract: The remarkable success of large-scale models is fundamentally tied to scaling laws, yet the finite nature of high-quality data presents a looming challenge. One of the next frontiers in modeling is data efficiency: the ability to learn more from less. A model's inductive bias is a critical lever for this, but foundational sequence models like State Space Models (SSMs) rely on a fixed bias. This fixed prior is sample-inefficient when a task's underlying structure does not match. In this work, we introduce a principled framework to solve this problem. We first formalize the inductive bias of linear time-invariant SSMs through an SSM-induced kernel, mathematically and empirically proving its spectrum is directly governed by the model's frequency response. Further, we propose a method of Task-Dependent Initialization (TDI): power spectrum matching, a fast and efficient method that aligns the model's inductive bias with the task's spectral characteristics before large-scale training. Our experiments on a diverse set of real-world benchmarks show that TDI significantly improves generalization and sample efficiency, particularly in low-data regimes. This work provides a theoretical and practical tool to create more data-efficient models, a crucial step towards sustainable scaling.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q-Net: Queue Length Estimation via Kalman-based Neural Networks</title>
<link>https://arxiv.org/abs/2509.24725</link>
<guid>https://arxiv.org/abs/2509.24725</guid>
<content:encoded><![CDATA[

arXiv:2509.24725v2 Announce Type: replace 
Abstract: Estimating queue lengths at signalized intersections is a long-standing challenge in traffic management. Partial observability of vehicle flows complicates this task despite the availability of two privacy preserving data sources: (i) aggregated vehicle counts from loop detectors near stop lines, and (ii) aggregated floating car data (aFCD) that provide segment-wise average speed measurements. However, how to integrate these sources with differing spatial and temporal resolutions for queue length estimation is rather unclear. Addressing this question, we present Q Net: a robust queue estimation framework built upon a state-space formulation. This formulation addresses key challenges in queue modeling, such as violations of traffic conservation assumptions. To overcome the limitations of standard filtering models in integrating diverse data sources, Q-Net employs an AI-augmented Kalman filter for estimation. Q-Net follows the Kalman predict-update framework and maintains physical interpretability, with internal variables linked to real-world traffic dynamics. Q-Net can be implemented in real-time, making it suitable for integration into queue-based traffic control systems. To achieve spatial transferability across road sections, we group aFCD measurements into fixed-size groups. This strategy ensures the dimension of Q-Net's learnable parameters is independent of section length. Evaluations on urban main roads in Rotterdam, the Netherlands, show that Q-Net outperforms baseline methods, accurately tracking queue formation and dissipation while correcting aFCD-induced delays. By combining data efficiency, interpretability, and strong transferability, Q Net makes accurate queue length estimation possible without costly sensing infrastructure like cameras or radar.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RouterArena: An Open Platform for Comprehensive Comparison of LLM Routers</title>
<link>https://arxiv.org/abs/2510.00202</link>
<guid>https://arxiv.org/abs/2510.00202</guid>
<content:encoded><![CDATA[

arXiv:2510.00202v3 Announce Type: replace 
Abstract: Today's LLM ecosystem comprises a wide spectrum of models that differ in size, capability, and cost. No single model is optimal for all scenarios; hence, LLM routers have become essential for selecting the most appropriate model under varying circumstances. However, the rapid emergence of various routers makes choosing the right one increasingly challenging. To address this problem, we need a comprehensive router comparison and a standardized leaderboard, similar to those available for models. In this work, we introduce RouterArena, the first open platform enabling comprehensive comparison of LLM routers. RouterArena has (1) a principally constructed dataset with broad knowledge domain coverage, (2) distinguishable difficulty levels for each domain, (3) an extensive list of evaluation metrics, and (4) an automated framework for leaderboard updates. Leveraging our framework, we have produced the initial leaderboard with detailed metrics comparison as shown in Figure 1. Our framework for evaluating new routers is on https://github.com/RouteWorks/RouterArena. Our leaderboard is on https://routeworks.github.io/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probability calibration for precipitation nowcasting</title>
<link>https://arxiv.org/abs/2510.00594</link>
<guid>https://arxiv.org/abs/2510.00594</guid>
<content:encoded><![CDATA[

arXiv:2510.00594v2 Announce Type: replace 
Abstract: Reliable precipitation nowcasting is critical for weather-sensitive decision-making, yet neural weather models (NWMs) can produce poorly calibrated probabilistic forecasts. Standard calibration metrics such as the expected calibration error (ECE) fail to capture miscalibration across precipitation thresholds. We introduce the expected thresholded calibration error (ETCE), a new metric that better captures miscalibration in ordered classes like precipitation amounts. We extend post-processing techniques from computer vision to the forecasting domain. Our results show that selective scaling with lead time conditioning reduces model miscalibration without reducing the forecast quality.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges</title>
<link>https://arxiv.org/abs/2510.03381</link>
<guid>https://arxiv.org/abs/2510.03381</guid>
<content:encoded><![CDATA[

arXiv:2510.03381v2 Announce Type: replace 
Abstract: Interchanges are crucial nodes for vehicle transfers between highways, yet the lack of real-time ramp detectors creates blind spots in traffic prediction. To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a two-stage framework that leverages cross-modal reconstruction pretraining. In the first stage, STDAE reconstructs historical ramp flows from mainline data, forcing the model to capture intrinsic spatio-temporal relations. Its decoupled architecture with parallel spatial and temporal autoencoders efficiently extracts heterogeneous features. In the prediction stage, the learned representations are integrated with models such as GWNet to enhance accuracy. Experiments on three real-world interchange datasets show that STDAE-GWNET consistently outperforms thirteen state-of-the-art baselines and achieves performance comparable to models using historical ramp data. This demonstrates its effectiveness in overcoming detector scarcity and its plug-and-play potential for diverse forecasting pipelines.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arithmetic-Mean $\mu$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets</title>
<link>https://arxiv.org/abs/2510.04327</link>
<guid>https://arxiv.org/abs/2510.04327</guid>
<content:encoded><![CDATA[

arXiv:2510.04327v2 Announce Type: replace 
Abstract: Choosing an appropriate learning rate remains a key challenge in scaling depth of modern deep networks. The classical maximal update parameterization ($\mu$P) enforces a fixed per-layer update magnitude, which is well suited to homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in heterogeneous architectures where residual accumulation and convolutions introduce imbalance across layers. We introduce Arithmetic-Mean $\mu$P (AM-$\mu$P), which constrains not each individual layer but the network-wide average one-step pre-activation second moment to a constant scale. Combined with a residual-aware He fan-in initialization - scaling residual-branch weights by the number of blocks ($\mathrm{Var}[W]=c/(K\cdot \mathrm{fan\text{-}in})$) - AM-$\mu$P yields width-robust depth laws that transfer consistently across depths. We prove that, for one- and two-dimensional convolutional networks, the maximal-update learning rate satisfies $\eta^\star(L)\propto L^{-3/2}$; with zero padding, boundary effects are constant-level as $N\gg k$. For standard residual networks with general conv+MLP blocks, we establish $\eta^\star(L)=\Theta(L^{-3/2})$, with $L$ the minimal depth. Empirical results across a range of depths confirm the $-3/2$ scaling law and enable zero-shot learning-rate transfer, providing a unified and practical LR principle for convolutional and deep residual networks without additional tuning overhead.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation Quantization of Vision Encoders Needs Prefixing Registers</title>
<link>https://arxiv.org/abs/2510.04547</link>
<guid>https://arxiv.org/abs/2510.04547</guid>
<content:encoded><![CDATA[

arXiv:2510.04547v3 Announce Type: replace 
Abstract: Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\textit{RegCache}$, a training-free algorithm that mitigates outliers in large-scale pretrained vision encoders and serves as a plug-in module that can be applied on top of other quantization methods. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modelings</title>
<link>https://arxiv.org/abs/2510.09895</link>
<guid>https://arxiv.org/abs/2510.09895</guid>
<content:encoded><![CDATA[

arXiv:2510.09895v2 Announce Type: replace 
Abstract: Modeling clinical time-series data is hampered by the challenge of capturing latent, time-varying dependencies among features. State-of-the-art approaches often rely on black-box mechanisms or simple aggregation, failing to explicitly model how the influence of one clinical variable propagates through others over time. We propose $\textbf{Chain-of-Influence (CoI)}$, an interpretable deep learning framework that constructs an explicit, time-unfolded graph of feature interactions. CoI enables the tracing of influence pathways, providing a granular audit trail that shows how any feature at any time contributes to the final prediction, both directly and through its influence on other variables. We evaluate CoI on mortality and disease progression tasks using the MIMIC-IV dataset and a chronic kidney disease cohort. Our framework achieves state-of-the-art predictive performance (AUROC of 0.960 on CKD progression and 0.950 on ICU mortality), with deletion-based sensitivity analyses confirming that CoI's learned attributions faithfully reflect its decision process. Through case studies, we demonstrate that CoI uncovers clinically meaningful, patient-specific patterns of disease progression, offering enhanced transparency into the temporal and cross-feature dependencies that inform clinical decision-making.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WARP-LUTs -- Walsh-Assisted Relaxation for Probabilistic Look Up Tables</title>
<link>https://arxiv.org/abs/2510.15655</link>
<guid>https://arxiv.org/abs/2510.15655</guid>
<content:encoded><![CDATA[

arXiv:2510.15655v3 Announce Type: replace 
Abstract: Fast and efficient machine learning is of growing interest to the scientific community and has spurred significant research into novel model architectures and hardware-aware design. Recent hard? and software co-design approaches have demonstrated impressive results with entirely multiplication-free models. Differentiable Logic Gate Networks (DLGNs), for instance, provide a gradient-based framework for learning optimal combinations of low-level logic gates, setting state-of-the-art trade-offs between accuracy, resource usage, and latency. However, these models suffer from high computational cost during training and do not generalize well to logic blocks with more inputs. In this work, we introduce Walsh-Assisted Relaxation for Probabilistic Look-Up Tables (WARP-LUTs) - a novel gradient-based method that efficiently learns combinations of logic gates with substantially fewer trainable parameters. We demonstrate that WARP-LUTs achieve significantly faster convergence on CIFAR-10 compared to DLGNs, while maintaining comparable accuracy. Furthermore, our approach suggests potential for extension to higher-input logic blocks, motivating future research on extremely efficient deployment on modern FPGAs and its real-time science applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Strategic inputs: feature selection from game-theoretic perspective</title>
<link>https://arxiv.org/abs/2510.24982</link>
<guid>https://arxiv.org/abs/2510.24982</guid>
<content:encoded><![CDATA[

arXiv:2510.24982v2 Announce Type: replace 
Abstract: The exponential growth of data volumes has led to escalating computational costs in machine learning model training. However, many features fail to contribute positively to model performance while consuming substantial computational resources. This paper presents an end-to-end feature selection framework for tabular data based on game theory. We formulate feature selection procedure based on a cooperative game where features are modeled as players, and their importance is determined through the evaluation of synergistic interactions and marginal contributions. The proposed framework comprises four core components: sample selection, game-theoretic feature importance evaluation, redundant feature elimination, and optimized model training. Experimental results demonstrate that the proposed method achieves substantial computation reduction while preserving predictive performance, thereby offering an efficient solution of the computational challenges of large-scale machine learning. The source code is available at https://github.com/vectorsss/strategy_inputs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\pi_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2510.25889</link>
<guid>https://arxiv.org/abs/2510.25889</guid>
<content:encoded><![CDATA[

arXiv:2510.25889v2 Announce Type: replace 
Abstract: Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (\eg, $\pi_0$, $\pi_{0.5}$) remains challenging due to intractable action log-likelihoods from iterative denoising. We address this challenge with $\pi_{\texttt{RL}}$, an open-source framework for training flow-based VLAs in parallel simulation. $\pi_{\texttt{RL}}$ implements two RL algorithms: (1) \textbf{Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) \textbf{Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration. We evaluate $\pi_{\texttt{RL}}$ on LIBERO, ManiSkill, and MetaWorld benchmarks. On LIBERO, $\pi_{\texttt{RL}}$ boosts few-shot SFT models $\pi_0$ and $\pi_{0.5}$ from 57.6\% to 97.6\% and from 77.1\% to 98.3\%, respectively. On ManiSkill, we train $\pi_{\texttt{RL}}$ in 320 parallel environments, improving $\pi_0$ from 38.4\% to 78.8\% and $\pi_{0.5}$ from 40.1\% to 90.8\% across 4352 variations of pick-and-place task. On MetaWorld, RL is conducted over 50 different manipulation tasks and yields performance gains of 35.0\% and 26.9\% for $\pi_0$ and $\pi_{0.5}$ models, respectively. Overall, $\pi_{\texttt{RL}}$ achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Static Cutoffs: One-Shot Dynamic Thresholding for Diffusion Language Models</title>
<link>https://arxiv.org/abs/2511.02077</link>
<guid>https://arxiv.org/abs/2511.02077</guid>
<content:encoded><![CDATA[

arXiv:2511.02077v2 Announce Type: replace 
Abstract: Masked diffusion language models (MDLMs) are becoming competitive with their autoregressive counterparts but typically decode with fixed steps and sequential unmasking. To accelerate decoding, recent work such as Fast-dLLM enables parallel decoding via a static global confidence threshold, yet we observe strong block- and step-wise confidence fluctuations and, within a dataset, near-identical confidence trajectories across inputs as measured by cosine similarity. Motivated by these observations, we introduce One-Shot Dynamic Thresholding (OSDT), which calibrates thresholds on a single sequence and applies them to subsequent inputs with negligible overhead. On GPQA, GSM8K, and HumanEval, OSDT attains superior accuracy-throughput trade-offs (+24% tokens/s on GSM8K at the best accuracy, +45% on GPQA with comparable accuracy, and +50% on HumanEval with a modest accuracy gap). Beyond these results, our findings suggest broader opportunities to leverage reusable task-level confidence signatures for more general-purpose algorithmic and systems innovations in diffusion decoding.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Periodic Skill Discovery</title>
<link>https://arxiv.org/abs/2511.03187</link>
<guid>https://arxiv.org/abs/2511.03187</guid>
<content:encoded><![CDATA[

arXiv:2511.03187v3 Announce Type: replace 
Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn diverse behaviors without relying on external rewards. However, current methods often overlook the periodic nature of learned skills, focusing instead on increasing the mutual dependence between states and skills or maximizing the distance traveled in latent space. Considering that many robotic tasks - particularly those involving locomotion - require periodic behaviors across varying timescales, the ability to discover diverse periodic skills is essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a framework that discovers periodic behaviors in an unsupervised manner. The key idea of PSD is to train an encoder that maps states to a circular latent space, thereby naturally encoding periodicity in the latent representation. By capturing temporal distance, PSD can effectively learn skills with diverse periods in complex robotic tasks, even with pixel-based observations. We further show that these learned skills achieve high performance on downstream tasks such as hurdling. Moreover, integrating PSD with an existing skill discovery method offers more diverse behaviors, thus broadening the agent's repertoire. Our code and demos are available at https://jonghaepark.github.io/psd/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Explanation for Multivariate Time Series Forecasting with Exogenous Variables</title>
<link>https://arxiv.org/abs/2511.06906</link>
<guid>https://arxiv.org/abs/2511.06906</guid>
<content:encoded><![CDATA[

arXiv:2511.06906v2 Announce Type: replace 
Abstract: Currently, machine learning is widely used across various domains, including time series data analysis. However, some machine learning models function as black boxes, making interpretability a critical concern. One approach to address this issue is counterfactual explanation (CE), which aims to provide insights into model predictions. This study focuses on the relatively underexplored problem of generating counterfactual explanations for time series forecasting. We propose a method for extracting CEs in time series forecasting using exogenous variables, which are frequently encountered in fields such as business and marketing. In addition, we present methods for analyzing the influence of each variable over an entire time series, generating CEs by altering only specific variables, and evaluating the quality of the resulting CEs. We validate the proposed method through theoretical analysis and empirical experiments, showcasing its accuracy and practical applicability. These contributions are expected to support real-world decision-making based on time series data analysis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private Sketches for Linear Regression</title>
<link>https://arxiv.org/abs/2511.07365</link>
<guid>https://arxiv.org/abs/2511.07365</guid>
<content:encoded><![CDATA[

arXiv:2511.07365v2 Announce Type: replace 
Abstract: Linear regression is frequently applied in a variety of domains, some of which might contain sensitive information. This necessitates that the application of these methods does not reveal private information. Differentially private (DP) linear regression methods, developed for this purpose, compute private estimates of the solution. These techniques typically involve computing a noisy version of the solution vector. Instead, we propose releasing private sketches of the datasets, which can then be used to compute an approximate solution to the regression problem. This is motivated by the \emph{sketch-and-solve} paradigm, where the regression problem is solved on a smaller sketch of the dataset instead of on the original problem space. The solution obtained on the sketch can also be shown to have good approximation guarantees to the original problem. Various sketching methods have been developed for improving the computational efficiency of linear regression problems under this paradigm. We adopt this paradigm for the purpose of releasing private sketches of the data. We construct differentially private sketches for the problems of least squares regression, as well as least absolute deviations regression. We show that the privacy constraints lead to sketched versions of regularized regression. We compute the bounds on the regularization parameter required for guaranteeing privacy. The availability of these private sketches facilitates the application of commonly available solvers for regression, without the risk of privacy leakage.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Sequential to Recursive: Enhancing Decision-Focused Learning with Bidirectional Feedback</title>
<link>https://arxiv.org/abs/2511.08035</link>
<guid>https://arxiv.org/abs/2511.08035</guid>
<content:encoded><![CDATA[

arXiv:2511.08035v2 Announce Type: replace 
Abstract: Decision-focused learning (DFL) has emerged as a powerful end-to-end alternative to conventional predict-then-optimize (PTO) pipelines by directly optimizing predictive models through downstream decision losses. Existing DFL frameworks are limited by their strictly sequential structure, referred to as sequential DFL (S-DFL). However, S-DFL fails to capture the bidirectional feedback between prediction and optimization in complex interaction scenarios. In view of this, we first time propose recursive decision-focused learning (R-DFL), a novel framework that introduces bidirectional feedback between downstream optimization and upstream prediction. We further extend two distinct differentiation methods: explicit unrolling via automatic differentiation and implicit differentiation based on fixed-point methods, to facilitate efficient gradient propagation in R-DFL. We rigorously prove that both methods achieve comparable gradient accuracy, with the implicit method offering superior computational efficiency. Extensive experiments on both synthetic and real-world datasets, including the newsvendor problem and the bipartite matching problem, demonstrate that R-DFL not only substantially enhances the final decision quality over sequential baselines but also exhibits robust adaptability across diverse scenarios in closed-loop decision-making problems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Training of Recursive Reasoning Models with Curriculum Guided Adaptive Recursion</title>
<link>https://arxiv.org/abs/2511.08653</link>
<guid>https://arxiv.org/abs/2511.08653</guid>
<content:encoded><![CDATA[

arXiv:2511.08653v2 Announce Type: replace 
Abstract: Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weaver: Kronecker Product Approximations of Spatiotemporal Attention for Traffic Network Forecasting</title>
<link>https://arxiv.org/abs/2511.08888</link>
<guid>https://arxiv.org/abs/2511.08888</guid>
<content:encoded><![CDATA[

arXiv:2511.08888v2 Announce Type: replace 
Abstract: Spatiotemporal forecasting on transportation networks is a complex task that requires understanding how traffic nodes interact within a dynamic, evolving system dictated by traffic flow dynamics and social behavioral patterns. The importance of transportation networks and ITS for modern mobility and commerce necessitates forecasting models that are not only accurate but also interpretable, efficient, and robust under structural or temporal perturbations. Recent approaches, particularly Transformer-based architectures, have improved predictive performance but often at the cost of high computational overhead and diminished architectural interpretability. In this work, we introduce Weaver, a novel attention-based model that applies Kronecker product approximations (KPA) to decompose the PN X PN spatiotemporal attention of O(P^2N^2) complexity into local P X P temporal and N X N spatial attention maps. This Kronecker attention map enables our Parallel-Kronecker Matrix-Vector product (P2-KMV) for efficient spatiotemporal message passing with O(P^2N + N^2P) complexity. To capture real-world traffic dynamics, we address the importance of negative edges in modeling traffic behavior by introducing Valence Attention using the continuous Tanimoto coefficient (CTC), which provides properties conducive to precise latent graph generation and training stability. To fully utilize the model's learning capacity, we introduce the Traffic Phase Dictionary for self-conditioning. Evaluations on PEMS-BAY and METR-LA show that Weaver achieves competitive performance across model categories while training more efficiently.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Making Every Head Count: Sparse Attention Without the Speed-Performance Trade-off</title>
<link>https://arxiv.org/abs/2511.09596</link>
<guid>https://arxiv.org/abs/2511.09596</guid>
<content:encoded><![CDATA[

arXiv:2511.09596v2 Announce Type: replace 
Abstract: The design of Large Language Models (LLMs) has long been hampered by a fundamental conflict within their core attention mechanism: its remarkable expressivity is built upon a computational complexity of O(H N^2) that grows quadratically with the context size (N) and linearly with the number of heads (H). This standard implementation harbors significant computational redundancy, as all heads independently compute attention over the same sequence space. Existing sparse methods, meanwhile, often trade information integrity for computational efficiency. To resolve this efficiency-performance trade-off, we propose SPAttention, whose core contribution is the introduction of a new paradigm we term Principled Structural Sparsity. SPAttention does not merely drop connections but instead reorganizes the computational task by partitioning the total attention workload into balanced, non-overlapping distance bands, assigning each head a unique segment. This approach transforms the multi-head attention mechanism from H independent O(N^2) computations into a single, collaborative O(N^2) computation, fundamentally reducing complexity by a factor of H. The structured inductive bias compels functional specialization among heads, enabling a more efficient allocation of computational resources from redundant modeling to distinct dependencies across the entire sequence span. Our work demonstrates that thoughtfully designed structural sparsity can serve as an effective inductive bias that simultaneously improves both computational efficiency and model performance, opening a new avenue for the architectural design of next-generation, high-performance LLMs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEGAgent: A Unified Framework for Automated EEG Analysis Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.09947</link>
<guid>https://arxiv.org/abs/2511.09947</guid>
<content:encoded><![CDATA[

arXiv:2511.09947v2 Announce Type: replace 
Abstract: Scalable and generalizable analysis of brain activity is essential for advancing both clinical diagnostics and cognitive research. Electroencephalography (EEG), a non-invasive modality with high temporal resolution, has been widely used for brain states analysis. However, most existing EEG models are usually tailored for individual specific tasks, limiting their utility in realistic scenarios where EEG analysis often involves multi-task and continuous reasoning. In this work, we introduce EEGAgent, a general-purpose framework that leverages large language models (LLMs) to schedule and plan multiple tools to automatically complete EEG-related tasks. EEGAgent is capable of performing the key functions: EEG basic information perception, spatiotemporal EEG exploration, EEG event detection, interaction with users, and EEG report generation. To realize these capabilities, we design a toolbox composed of different tools for EEG preprocessing, feature extraction, event detection, etc. These capabilities were evaluated on public datasets, and our EEGAgent can support flexible and interpretable EEG analysis, highlighting its potential for real-world clinical applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RI-Loss: A Learnable Residual-Informed Loss for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.10130</link>
<guid>https://arxiv.org/abs/2511.10130</guid>
<content:encoded><![CDATA[

arXiv:2511.10130v2 Announce Type: replace 
Abstract: Time series forecasting relies on predicting future values from historical data, yet most state-of-the-art approaches-including transformer and multilayer perceptron-based models-optimize using Mean Squared Error (MSE), which has two fundamental weaknesses: its point-wise error computation fails to capture temporal relationships, and it does not account for inherent noise in the data. To overcome these limitations, we introduce the Residual-Informed Loss (RI-Loss), a novel objective function based on the Hilbert-Schmidt Independence Criterion (HSIC). RI-Loss explicitly models noise structure by enforcing dependence between the residual sequence and a random time series, enabling more robust, noise-aware representations. Theoretically, we derive the first non-asymptotic HSIC bound with explicit double-sample complexity terms, achieving optimal convergence rates through Bernstein-type concentration inequalities and Rademacher complexity analysis. This provides rigorous guarantees for RI-Loss optimization while precisely quantifying kernel space interactions. Empirically, experiments across eight real-world benchmarks and five leading forecasting models demonstrate improvements in predictive performance, validating the effectiveness of our approach. The code is publicly available at: https://github.com/shang-xl/RI-Loss.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.10200</link>
<guid>https://arxiv.org/abs/2511.10200</guid>
<content:encoded><![CDATA[

arXiv:2511.10200v2 Announce Type: replace 
Abstract: Time series forecasting is an important task that involves analyzing temporal dependencies and underlying patterns (such as trends, cyclicality, and seasonality) in historical data to predict future values or trends. Current deep learning-based forecasting models primarily employ Mean Squared Error (MSE) loss functions for regression modeling. Despite enabling direct value prediction, this method offers no uncertainty estimation and exhibits poor outlier robustness. To address these limitations, we propose OCE-TS, a novel ordinal classification approach for time series forecasting that replaces MSE with Ordinal Cross-Entropy (OCE) loss, preserving prediction order while quantifying uncertainty through probability output. Specifically, OCE-TS begins by discretizing observed values into ordered intervals and deriving their probabilities via a parametric distribution as supervision signals. Using a simple linear model, we then predict probability distributions for each timestep. The OCE loss is computed between the cumulative distributions of predicted and ground-truth probabilities, explicitly preserving ordinal relationships among forecasted values. Through theoretical analysis using influence functions, we establish that cross-entropy (CE) loss exhibits superior stability and outlier robustness compared to MSE loss. Empirically, we compared OCE-TS with five baseline models-Autoformer, DLinear, iTransformer, TimeXer, and TimeBridge-on seven public time series datasets. Using MSE and Mean Absolute Error (MAE) as evaluation metrics, the results demonstrate that OCE-TS consistently outperforms benchmark models. The codeis publicly available at: https://github.com/Shi-hm/OCE-TS.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oya: Deep Learning for Accurate Global Precipitation Estimation</title>
<link>https://arxiv.org/abs/2511.10562</link>
<guid>https://arxiv.org/abs/2511.10562</guid>
<content:encoded><![CDATA[

arXiv:2511.10562v2 Announce Type: replace 
Abstract: Accurate precipitation estimation is critical for hydrological applications, especially in the Global South where ground-based observation networks are sparse and forecasting skill is limited. Existing satellite-based precipitation products often rely on the longwave infrared channel alone or are calibrated with data that can introduce significant errors, particularly at sub-daily timescales. This study introduces Oya, a novel real-time precipitation retrieval algorithm utilizing the full spectrum of visible and infrared (VIS-IR) observations from geostationary (GEO) satellites. Oya employs a two-stage deep learning approach, combining two U-Net models: one for precipitation detection and another for quantitative precipitation estimation (QPE), to address the inherent data imbalance between rain and no-rain events. The models are trained using high-resolution GPM Combined Radar-Radiometer Algorithm (CORRA) v07 data as ground truth and pre-trained on IMERG-Final retrievals to enhance robustness and mitigate overfitting due to the limited temporal sampling of CORRA. By leveraging multiple GEO satellites, Oya achieves quasi-global coverage and demonstrates superior performance compared to existing competitive regional and global precipitation baselines, offering a promising pathway to improved precipitation monitoring and forecasting.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Exogenous Signals for Hydrology Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.11849</link>
<guid>https://arxiv.org/abs/2511.11849</guid>
<content:encoded><![CDATA[

arXiv:2511.11849v2 Announce Type: replace 
Abstract: Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolEdit: Knowledge Editing for Multimodal Molecule Language Models</title>
<link>https://arxiv.org/abs/2511.12770</link>
<guid>https://arxiv.org/abs/2511.12770</guid>
<content:encoded><![CDATA[

arXiv:2511.12770v2 Announce Type: replace 
Abstract: Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Trio Neural Model for Dynamic Entity Relatedness Ranking</title>
<link>https://arxiv.org/abs/1808.08316</link>
<guid>https://arxiv.org/abs/1808.08316</guid>
<content:encoded><![CDATA[

arXiv:1808.08316v5 Announce Type: replace-cross 
Abstract: Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in static settings and an unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic over time. In this work, we propose a neural networkbased approach for dynamic entity relatedness, leveraging the collective attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning with Global Alignment</title>
<link>https://arxiv.org/abs/2205.12186</link>
<guid>https://arxiv.org/abs/2205.12186</guid>
<content:encoded><![CDATA[

arXiv:2205.12186v3 Announce Type: replace-cross 
Abstract: Continual learning aims to sequentially learn new tasks without forgetting previous tasks' knowledge (catastrophic forgetting). One factor that can cause forgetting is the interference between the gradients on losses from different tasks. When the gradients on the current task's loss are in opposing directions to those on previous tasks' losses, updating the model for the current task may cause performance degradation on previous tasks. In this paper, we first identify causes of the above interference, and hypothesize that correlations between data representations are a key factor of interference. We then propose a method for promoting appropriate correlations between arbitrary tasks' data representations (i.e., global alignment) in individual task learning. Specifically, we learn the data representation as a task-specific composition of pre-trained token representations shared across all tasks. Then the correlations between different tasks' data representations are grounded by correlations between pre-trained token representations. We explore different ways to learn such compositions. Without experience replay, our model achieves SOTA performance in continual learning tasks. It also achieves advanced class-incremental performance through task-incremental training.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-concordant smoothing in proximal quasi-Newton algorithms for large-scale convex composite optimization</title>
<link>https://arxiv.org/abs/2309.01781</link>
<guid>https://arxiv.org/abs/2309.01781</guid>
<content:encoded><![CDATA[

arXiv:2309.01781v3 Announce Type: replace-cross 
Abstract: We introduce a notion of self-concordant smoothing for minimizing the sum of two convex functions, one of which is smooth and the other nonsmooth. The key highlight is a natural property of the resulting problem's structure that yields a variable metric selection method and a step length rule especially suited to proximal quasi-Newton algorithms. Also, we efficiently handle specific structures promoted by the nonsmooth term, such as l1-regularization and group lasso penalties. A convergence analysis for the class of proximal quasi-Newton methods covered by our framework is presented. In particular, we obtain guarantees, under standard assumptions, for two algorithms: Prox-N-SCORE (a proximal Newton method) and Prox-GGN-SCORE (a proximal generalized Gauss-Newton method). The latter uses a low-rank approximation of the Hessian inverse, reducing most of the cost of matrix inversion and making it effective for overparameterized machine learning models. Numerical experiments on synthetic and real data demonstrate the efficiency of both algorithms against state-of-the-art approaches. A Julia implementation is publicly available at https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Generalization Bounds for Transductive Learning by Transductive Local Complexity and Its Applications</title>
<link>https://arxiv.org/abs/2309.16858</link>
<guid>https://arxiv.org/abs/2309.16858</guid>
<content:encoded><![CDATA[

arXiv:2309.16858v3 Announce Type: replace-cross 
Abstract: We introduce Transductive Local Complexity (TLC) as a new tool for analyzing the generalization performance of transductive learning methods. Our work extends the classical Local Rademacher Complexity (LRC) to the transductive setting, incorporating substantial and novel components beyond standard inductive LRC analysis. Although LRC has been used to obtain sharp generalization bounds and minimax rates for inductive tasks such as classification and nonparametric regression, it has remained an open problem whether a localized Rademacher complexity framework can be effectively adapted to transductive learning to achieve sharp or nearly sharp bounds consistent with inductive results. We provide an affirmative answer via TLC. TLC is constructed by first deriving a new concentration inequality in Theorem 4.1 for the supremum of empirical processes capturing the gap between test and training losses, termed the test-train process, under uniform sampling without replacement, which leverages a novel combinatorial property of the test-train process and a new proof strategy applying the exponential Efron-Stein inequality twice. A subsequent peeling strategy and a new surrogate variance operator then yield excess risk bounds in the transductive setting that are nearly consistent with classical LRC-based inductive bounds up to a logarithmic gap. We further advance transductive learning through two applications: (1) for realizable transductive learning over binary-valued classes with finite VC dimension and $u \ge m \ge \dVC$ where $u$ and $m$ are the number of test features and training features, our Theorem 6.1 gives a nearly optimal bound $\Theta(\dVC \log(me/\dVC)/m)$ matching the minimax rate $\Theta(\dVC/m)$ up to $\log m$, resolving a decade-old open question; and (2) Theorem 6.3 presents a sharper excess risk bound for transductive kernel learning compared to the current state-of-the-art.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast multiplication by two's complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation</title>
<link>https://arxiv.org/abs/2311.09922</link>
<guid>https://arxiv.org/abs/2311.09922</guid>
<content:encoded><![CDATA[

arXiv:2311.09922v4 Announce Type: replace-cross 
Abstract: We demonstrate a multiplication method based on numbers represented as set of polynomial radix 2 indices stored as an integer list. The 'polynomial integer index multiplication' method is a set of algorithms implemented in python code. We demonstrate the method to be faster than both the Number Theoretic Transform (NTT) and Karatsuba for multiplication within a certain bit range. Also implemented in python code for comparison purposes with the polynomial radix 2 integer method. We demonstrate that it is possible to express any integer or real number as a list of integer indices, representing a finite series in base two. The finite series of integer index representation of a number can then be stored and distributed across multiple CPUs / GPUs. We show that operations of addition and multiplication can be applied as two's complement additions operating on the index integer representations and can be fully distributed across a given CPU / GPU architecture. We demonstrate fully distributed arithmetic operations such that the 'polynomial integer index multiplication' method overcomes the current limitation of parallel multiplication methods. Ie, the need to share common core memory and common disk for the calculation of results and intermediate results.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Universality in Neural Network Dynamics with Generalized Structured Input Distributions</title>
<link>https://arxiv.org/abs/2405.00642</link>
<guid>https://arxiv.org/abs/2405.00642</guid>
<content:encoded><![CDATA[

arXiv:2405.00642v4 Announce Type: replace-cross 
Abstract: Analyzing neural network dynamics via stochastic gradient descent (SGD) is crucial to building theoretical foundations for deep learning. Previous work has analyzed structured inputs within the \textit{hidden manifold model}, often under the simplifying assumption of a Gaussian distribution. We extend this framework by modeling inputs as Gaussian mixtures to better represent complex, real-world data. Through empirical and theoretical investigation, we demonstrate that with proper standardization, the learning dynamics converges to the behavior seen in the simple Gaussian case. This finding exhibits a form of universality, where diverse structured distributions yield results consistent with Gaussian assumptions, thereby strengthening the theoretical understanding of deep learning models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Split Conformal Prediction under Data Contamination</title>
<link>https://arxiv.org/abs/2407.07700</link>
<guid>https://arxiv.org/abs/2407.07700</guid>
<content:encoded><![CDATA[

arXiv:2407.07700v3 Announce Type: replace-cross 
Abstract: Conformal prediction is a non-parametric technique for constructing prediction intervals or sets from arbitrary predictive models under the assumption that the data is exchangeable. It is popular as it comes with theoretical guarantees on the marginal coverage of the prediction sets and the split conformal prediction variant has a very low computational cost compared to model training. We study the robustness of split conformal prediction in a data contamination setting, where we assume a small fraction of the calibration scores are drawn from a different distribution than the bulk. We quantify the impact of the corrupted data on the coverage and efficiency of the constructed sets when evaluated on "clean" test points, and verify our results with numerical experiments. Moreover, we propose an adjustment in the classification setting which we call Contamination Robust Conformal Prediction, and verify the efficacy of our approach using both synthetic and real datasets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach</title>
<link>https://arxiv.org/abs/2407.12687</link>
<guid>https://arxiv.org/abs/2407.12687</guid>
<content:encoded><![CDATA[

arXiv:2407.12687v3 Announce Type: replace-cross 
Abstract: A major challenge facing the world is the provision of equitable and universal access to quality education. Recent advances in generative AI (gen AI) have created excitement about the potential of new technologies to offer a personal tutor for every learner and a teaching assistant for every teacher. The full extent of this dream, however, has not yet materialised. We argue that this is primarily due to the difficulties with verbalising pedagogical intuitions into gen AI prompts and the lack of good evaluation practices, reinforced by the challenges in defining excellent pedagogy. Here we present our work collaborating with learners and educators to translate high level principles from learning science into a pragmatic set of seven diverse educational benchmarks, spanning quantitative, qualitative, automatic and human evaluations; and to develop a new set of fine-tuning datasets to improve the pedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluations show that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini by educators and learners on a number of pedagogical dimensions. We hope that this work can serve as a first step towards developing a comprehensive educational evaluation framework, and that this can enable rapid progress within the AI and EdTech communities towards maximising the positive impact of gen AI in education.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Masquerade Attacks in Controller Area Networks Using Graph Machine Learning</title>
<link>https://arxiv.org/abs/2408.05427</link>
<guid>https://arxiv.org/abs/2408.05427</guid>
<content:encoded><![CDATA[

arXiv:2408.05427v2 Announce Type: replace-cross 
Abstract: Modern vehicles rely on a myriad of electronic control units (ECUs) interconnected via controller area networks (CANs) for critical operations. Despite their ubiquitous use and reliability, CANs are susceptible to sophisticated cyberattacks, particularly masquerade attacks, which inject false data that mimic legitimate messages at the expected frequency. These attacks pose severe risks such as unintended acceleration, brake deactivation, and rogue steering. Traditional intrusion detection systems (IDS) often struggle to detect these subtle intrusions due to their seamless integration into normal traffic. This paper introduces a novel framework for detecting masquerade attacks in the CAN bus using graph machine learning (ML). We hypothesize that the integration of shallow graph embeddings with time series features derived from CAN frames enhances the detection of masquerade attacks. We show that by representing CAN bus frames as message sequence graphs (MSGs) and enriching each node with contextual statistical attributes from time series, we can enhance detection capabilities across various attack patterns compared to using graph-based features only. Our method ensures a comprehensive and dynamic analysis of CAN frame interactions, improving robustness and efficiency. Extensive experiments on the ROAD dataset validate the effectiveness of our approach, demonstrating statistically significant improvements in the detection rates of masquerade attacks compared to a baseline that uses graph-based features only as confirmed by Mann-Whitney U and Kolmogorov-Smirnov tests p < 0.05.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Targeted Deep Learning System Boundary Testing</title>
<link>https://arxiv.org/abs/2408.06258</link>
<guid>https://arxiv.org/abs/2408.06258</guid>
<content:encoded><![CDATA[

arXiv:2408.06258v3 Announce Type: replace-cross 
Abstract: Evaluating the behavioral boundaries of deep learning (DL) systems is crucial for understanding their reliability across diverse, unseen inputs. Existing solutions fall short as they rely on untargeted random, model- or latent-based perturbations, due to difficulties in generating controlled input variations. In this work, we introduce Mimicry, a novel black-box test generator for fine-grained, targeted exploration of DL system boundaries. Mimicry performs boundary testing by leveraging the probabilistic nature of DL outputs to identify promising directions for exploration. It uses style-based GANs to disentangle input representations into content and style components, enabling controlled feature mixing to approximate the decision boundary. We evaluated Mimicry's effectiveness in generating boundary inputs for five widely used DL image classification systems of increasing complexity, comparing it to two baseline approaches. Our results show that Mimicry consistently identifies inputs closer to the decision boundary. It generates semantically meaningful boundary test cases that reveal new functional (mis)behaviors, while the baselines produce mainly corrupted or invalid inputs. Thanks to its enhanced control over latent space manipulations, Mimicry remains effective as dataset complexity increases, maintaining competitive diversity and higher validity rates, confirmed by human assessors.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Proximal Modified Quasi-Newton Method for Nonsmooth Regularized Optimization</title>
<link>https://arxiv.org/abs/2409.19428</link>
<guid>https://arxiv.org/abs/2409.19428</guid>
<content:encoded><![CDATA[

arXiv:2409.19428v2 Announce Type: replace-cross 
Abstract: We develop R2N, a modified quasi-Newton method for minimizing the sum of a $\mathcal{C}^1$ function $f$ and a lower semi-continuous prox-bounded $h$. Both $f$ and $h$ may be nonconvex. At each iteration, our method computes a step by minimizing the sum of a quadratic model of $f$, a model of $h$, and an adaptive quadratic regularization term. A step may be computed by a variant of the proximal-gradient method. An advantage of R2N over trust-region (TR) methods is that proximal operators do not involve an extra TR indicator. We also develop the variant R2DH, in which the model Hessian is diagonal, which allows us to compute a step without relying on a subproblem solver when $h$ is separable. R2DH can be used as standalone solver, but also as subproblem solver inside R2N. We describe non-monotone variants of both R2N and R2DH. Global convergence of a first-order stationarity measure to zero holds without relying on local Lipschitz continuity of $\nabla f$, while allowing model Hessians to grow unbounded, an assumption particularly relevant to quasi-Newton models. Under Lipschitz-continuity of $\nabla f$, we establish a tight worst-case complexity bound of $O(1 / \epsilon^{2/(1 - p)})$ to bring said measure below $\epsilon > 0$, where $0 \leq p < 1$ controls the growth of model Hessians. The latter must not diverge faster than $|\mathcal{S}_k|^p$, where $\mathcal{S}_k$ is the set of successful iterations up to iteration $k$. When $p = 1$, we establish the tight exponential complexity bound $O(\exp(c \epsilon^{-2}))$ where $c > 0$ is a constant. We describe our Julia implementation and report numerical experience on a classic basis-pursuit problem, an image denoising problem, a minimum-rank matrix completion problem, a nonlinear support vector machine and an inverse nonlinear problem.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeGaussian: Annotation-free Control of Articulated Objects via 3D Gaussian Splats with Flow Derivatives</title>
<link>https://arxiv.org/abs/2410.22070</link>
<guid>https://arxiv.org/abs/2410.22070</guid>
<content:encoded><![CDATA[

arXiv:2410.22070v3 Announce Type: replace-cross 
Abstract: Reconstructing controllable Gaussian splats for articulated objects from monocular video is especially challenging due to its inherently insufficient constraints. Existing methods address this by relying on dense masks and manually defined control signals, limiting their real-world applications. In this paper, we propose an annotation-free method, FreeGaussian, which mathematically disentangles camera egomotion and articulated movements via flow derivatives. By establishing a connection between 2D flows and 3D Gaussian dynamic flow, our method enables optimization and continuity of dynamic Gaussian motions from flow priors without any control signals. Furthermore, we introduce a 3D spherical vector controlling scheme, which represents the state as a 3D Gaussian trajectory, thereby eliminating the need for complex 1D control signal calculations and simplifying controllable Gaussian modeling. Extensive experiments on articulated objects demonstrate the state-of-the-art visual performance and precise, part-aware controllability of our method. Code is available at: https://github.com/Tavish9/freegaussian.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Semantic Communication and Human Decision-Making into an End-to-End Sensing-Decision Framework</title>
<link>https://arxiv.org/abs/2412.05103</link>
<guid>https://arxiv.org/abs/2412.05103</guid>
<content:encoded><![CDATA[

arXiv:2412.05103v3 Announce Type: replace-cross 
Abstract: As early as 1949, Weaver defined communication in a very broad sense to include all procedures by which one mind or technical system can influence another, thus establishing the idea of semantic communication. With the recent success of machine learning in expert assistance systems where sensed information is wirelessly provided to a human to assist task execution, the need to design effective and efficient communications has become increasingly apparent. In particular, semantic communication aims to convey the meaning behind the sensed information relevant for Human Decision-Making (HDM). Regarding the interplay between semantic communication and HDM, many questions remain, such as how to model the entire end-to-end sensing-decision-making process, how to design semantic communication for the HDM and which information should be provided for HDM. To address these questions, we propose to integrate semantic communication and HDM into one probabilistic end-to-end sensing-decision framework that bridges communications and psychology. In our interdisciplinary framework, we model the human through a HDM process, allowing us to explore how feature extraction from semantic communication can best support HDM both in theory and in simulations. In this sense, our study reveals the fundamental design trade-off between maximizing the relevant semantic information and matching the cognitive capabilities of the HDM model. Our initial analysis shows how semantic communication can balance the level of detail with human cognitive capabilities while demanding less bandwidth, power, and latency.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReasoningWeekly: A General Knowledge and Verbal Reasoning Challenge for Large Language Models</title>
<link>https://arxiv.org/abs/2502.01584</link>
<guid>https://arxiv.org/abs/2502.01584</guid>
<content:encoded><![CDATA[

arXiv:2502.01584v4 Announce Type: replace-cross 
Abstract: Existing benchmarks for frontier models often test specialized, "PhD-level" knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark with 613 problems based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models; however correct solutions are easy to verify, and models' mistakes are easy to spot. As LLMs are more widely deployed in society, we believe it is useful to develop benchmarks for frontier models that humans can understand without the need for deep domain expertise. Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models on our benchmark, despite being on par with other models when tested on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with "I give up" before providing an answer that it knows is wrong. R1 can also be remarkably "uncertain" in its output and in rare cases, it does not "finish thinking," which suggests the need for techniques to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Shot Learning for k-SAT</title>
<link>https://arxiv.org/abs/2502.07135</link>
<guid>https://arxiv.org/abs/2502.07135</guid>
<content:encoded><![CDATA[

arXiv:2502.07135v3 Announce Type: replace-cross 
Abstract: Consider a $k$-SAT formula $\Phi$ where every variable appears at most $d$ times. Let $\sigma$ be a satisfying assignment, sampled proportionally to $e^{\beta m(\sigma)}$ where $m(\sigma)$ is the number of true variables and $\beta$ is a real parameter. Given $\Phi$ and $\sigma$, can we efficiently learn $\beta$?
  This problem falls into a recent line of work about single-sample (``one-shot'') learning of Markov random fields. Our $k$-SAT setting was recently studied by Galanis, Kalavasis, Kandiros (SODA24). They showed that single-sample learning is possible when roughly $d\leq 2^{k/6.45}$ and impossible when $d\geq (k+1) 2^{k-1}$. In addition to the gap in~$d$, their impossibility result left open the question of whether the feasibility threshold for one-shot learning is dictated by the satisfiability threshold for bounded-degree $k$-SAT formulas.
  Our main contribution is to answer this question negatively. We show that one-shot learning for $k$-SAT is infeasible well below the satisfiability threshold; in fact, we obtain impossibility results for degrees $d$ as low as $k^2$ when $\beta$ is sufficiently large, and bootstrap this to small values of $\beta$ when $d$ scales exponentially with $k$, via a probabilistic construction. On the positive side, we simplify the analysis of the learning algorithm, obtaining significantly stronger bounds on $d$ in terms of $\beta$. For the uniform case $\beta\rightarrow 0$, we show that learning is possible under the condition $d\lesssim 2^{k/2}$. This is (up to constant factors) all the way to the sampling threshold -- it is known that sampling a uniformly-distributed satisfying assignment is NP-hard for $d\gtrsim 2^{k/2}$.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atom of Thoughts for Markov LLM Test-Time Scaling</title>
<link>https://arxiv.org/abs/2502.12018</link>
<guid>https://arxiv.org/abs/2502.12018</guid>
<content:encoded><![CDATA[

arXiv:2502.12018v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning can be achieved by solving a series of independent and self-contained subquestions. These subquestions are essentially \textit{atomic questions}, exhibiting the memoryless property similar to Markov processes. Based on this observation, we propose Atom of Thoughts (\our), where each state transition consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a simplified question that maintains answer equivalence with the original problem. This answer preservation enables the iterative \textit{decomposition-contraction} process to naturally form a meaningful Markov reasoning process. Furthermore, these atomic states can be seamlessly integrated into existing test-time scaling methods, enabling \our to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of \our both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, \our achieves an \textbf{80.6\%} F1 score, surpassing o3-mini by \textbf{3.4\%} and DeepSeek-R1 by \textbf{10.6\%}. The code is available at \href{https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Statistical Significance of Deep Nearest Neighbor Anomaly Detection via Selective Inference</title>
<link>https://arxiv.org/abs/2502.12978</link>
<guid>https://arxiv.org/abs/2502.12978</guid>
<content:encoded><![CDATA[

arXiv:2502.12978v2 Announce Type: replace-cross 
Abstract: In real-world applications, anomaly detection (AD) often operates without access to anomalous data, necessitating semi-supervised methods that rely solely on normal data. Among these methods, deep k-nearest neighbor (deep kNN) AD stands out for its interpretability and flexibility, leveraging distance-based scoring in deep latent spaces.Despite its strong performance, deep kNN lacks a mechanism to quantify uncertainty-an essential feature for critical applications such as industrial inspection. To address this limitation, we propose a statistical framework that quantifies the significance of detected anomalies in the form of p-values, thereby enabling control over false positive rates at a user-specified significance level (e.g.,0.05). A central challenge lies in managing selection bias, which we tackle using Selective Inference-a principled method for conducting inference conditioned on data-driven selections. We evaluate our method on diverse datasets and demonstrate that it provides reliable AD well-suited for industrial use cases.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RiboGen: RNA Sequence and Structure Co-Generation with Equivariant MultiFlow</title>
<link>https://arxiv.org/abs/2503.02058</link>
<guid>https://arxiv.org/abs/2503.02058</guid>
<content:encoded><![CDATA[

arXiv:2503.02058v5 Announce Type: replace-cross 
Abstract: Ribonucleic acid (RNA) plays fundamental roles in biological systems, from carrying genetic information to performing enzymatic function. Understanding and designing RNA can enable novel therapeutic application and biotechnological innovation. To enhance RNA design, in this paper we introduce RiboGen, the first deep learning model to simultaneously generate RNA sequence and all-atom 3D structure. RiboGen leverages the standard Flow Matching with Discrete Flow Matching in a multimodal data representation. RiboGen is based on Euclidean Equivariant neural networks for efficiently processing and learning three-dimensional geometry. Our experiments show that RiboGen can efficiently generate chemically plausible and self-consistent RNA samples, suggesting that co-generation of sequence and structure is a competitive approach for modeling RNA.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Frank-Wolfe for Structured Nonconvex Optimization</title>
<link>https://arxiv.org/abs/2503.08921</link>
<guid>https://arxiv.org/abs/2503.08921</guid>
<content:encoded><![CDATA[

arXiv:2503.08921v2 Announce Type: replace-cross 
Abstract: We introduce a new projection-free (Frank-Wolfe) method for optimizing structured nonconvex functions that are expressed as a difference of two convex functions. This problem class subsumes smooth nonconvex minimization, positioning our method as a promising alternative to the classical Frank-Wolfe algorithm. DC decompositions are not unique; by carefully selecting a decomposition, we can better exploit the problem structure, improve computational efficiency, and adapt to the underlying problem geometry to find better local solutions. We prove that the proposed method achieves a first-order stationary point in $O(1/\epsilon^2)$ iterations, matching the complexity of the standard Frank-Wolfe algorithm for smooth nonconvex minimization in general. Specific decompositions can, for instance, yield a gradient-efficient variant that requires only $O(1/\epsilon)$ calls to the gradient oracle. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method compared to other projection-free algorithms.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation</title>
<link>https://arxiv.org/abs/2503.09399</link>
<guid>https://arxiv.org/abs/2503.09399</guid>
<content:encoded><![CDATA[

arXiv:2503.09399v3 Announce Type: replace-cross 
Abstract: Transformers, particularly Vision Transformers (ViTs), have achieved state-of-the-art performance in large-scale image classification. However, they often require large amounts of data and can exhibit biases, such as center or size bias, that limit their robustness and generalizability. This paper introduces ForAug, a novel data augmentation operation that addresses these challenges by explicitly imposing invariances into the training data, which are otherwise part of the neural network architecture. ForAug is constructed by using pretrained foundation models to separate and recombine foreground objects with different backgrounds. This recombination step enables us to take fine-grained control over object position and size, as well as background selection. We demonstrate that using ForAug significantly improves the accuracy of ViTs and other architectures by up to 4.5 percentage points (p.p.) on ImageNet, which translates to 7.3 p.p. on downstream tasks. Importantly, ForAug not only improves accuracy but also opens new ways to analyze model behavior and quantify biases. Namely, we introduce metrics for background robustness, foreground focus, center bias, and size bias and show that using ForAug during training substantially reduces these biases. In summary, ForAug provides a valuable tool for analyzing and mitigating biases, enabling the development of more robust and reliable computer vision models. Our code and dataset are publicly available at https://github.com/tobna/ForAug.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Semantic Attribute Binding for Free-Lunch Color Control in Diffusion Models</title>
<link>https://arxiv.org/abs/2503.09864</link>
<guid>https://arxiv.org/abs/2503.09864</guid>
<content:encoded><![CDATA[

arXiv:2503.09864v2 Announce Type: replace-cross 
Abstract: Recent advances in text-to-image (T2I) diffusion models have enabled remarkable control over various attributes, yet precise color specification remains a fundamental challenge. Existing approaches, such as ColorPeel, rely on model personalization, requiring additional optimization and limiting flexibility in specifying arbitrary colors. In this work, we introduce ColorWave, a novel training-free approach that achieves exact RGB-level color control in diffusion models without fine-tuning. By systematically analyzing the cross-attention mechanisms within IP-Adapter, we uncover an implicit binding between textual color descriptors and reference image features. Leveraging this insight, our method rewires these bindings to enforce precise color attribution while preserving the generative capabilities of pretrained models. Our approach maintains generation quality and diversity, outperforming prior methods in accuracy and applicability across diverse object categories. Through extensive evaluations, we demonstrate that ColorWave establishes a new paradigm for structured, color-consistent diffusion-based image synthesis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.02821</link>
<guid>https://arxiv.org/abs/2504.02821</guid>
<content:encoded><![CDATA[

arXiv:2504.02821v3 Announce Type: replace-cross 
Abstract: Sparse Autoencoders (SAEs) have recently gained attention as a means to improve the interpretability and steerability of Large Language Models (LLMs), both of which are essential for AI safety. In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity at the neuron-level in visual representations. To ensure that our evaluation aligns with human perception, we propose a benchmark derived from a large-scale user study. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons, with sparsity and wide latents being the most influential factors. Further, we demonstrate that applying SAE interventions on CLIP's vision encoder directly steers multimodal LLM outputs (e.g., LLaVA), without any modifications to the underlying language model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised tool for enhancing both interpretability and control of VLMs. Code and benchmark data are available at https://github.com/ExplainableML/sae-for-vlm.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDFs from Unoriented Point Clouds using Neural Variational Heat Distances</title>
<link>https://arxiv.org/abs/2504.11212</link>
<guid>https://arxiv.org/abs/2504.11212</guid>
<content:encoded><![CDATA[

arXiv:2504.11212v3 Announce Type: replace-cross 
Abstract: We propose a novel variational approach for computing neural Signed Distance Fields (SDF) from unoriented point clouds. To this end, we replace the commonly used eikonal equation with the heat method, carrying over to the neural domain what has long been standard practice for computing distances on discrete surfaces. This yields two convex optimization problems for whose solution we employ neural networks: We first compute a neural approximation of the gradients of the unsigned distance field through a small time step of heat flow with weighted point cloud densities as initial data. Then we use it to compute a neural approximation of the SDF. We prove that the underlying variational problems are well-posed. Through numerical experiments, we demonstrate that our method provides state-of-the-art surface reconstruction and consistent SDF gradients. Furthermore, we show in a proof-of-concept that it is accurate enough for solving a PDE on the zero-level set.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Embodied Intelligence in Robotic-Assisted Endovascular Procedures: A Systematic Review of AI Solutions</title>
<link>https://arxiv.org/abs/2504.15327</link>
<guid>https://arxiv.org/abs/2504.15327</guid>
<content:encoded><![CDATA[

arXiv:2504.15327v3 Announce Type: replace-cross 
Abstract: Endovascular procedures have revolutionized vascular disease treatment, yet their manual execution is challenged by the demands for high precision, operator fatigue, and radiation exposure. Robotic systems have emerged as transformative solutions to mitigate these inherent limitations. A pivotal moment has arrived, where a confluence of pressing clinical needs and breakthroughs in AI creates an opportunity for a paradigm shift toward Embodied Intelligence (EI), enabling robots to navigate complex vascular networks and adapt to dynamic physiological conditions. Data-driven approaches, leveraging advanced computer vision, medical image analysis, and machine learning, drive this evolution by enabling real-time vessel segmentation, device tracking, and anatomical landmark detection. Reinforcement learning and imitation learning further enhance navigation strategies and replicate expert techniques. This review systematically analyzes the integration of EI into endovascular robotics, identifying profound systemic challenges such as the heterogeneity in validation standards and the gap between human mimicry and machine-native capabilities. Based on this analysis, a conceptual roadmap is proposed that reframes the ultimate objective away from systems that supplant clinical decision-making. This vision of augmented intelligence, where the clinician's role evolves into that of a high-level supervisor, provides a principled foundation for the future of the field.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-Training Estimators for Structural Models: Application to Consumer Search</title>
<link>https://arxiv.org/abs/2505.00526</link>
<guid>https://arxiv.org/abs/2505.00526</guid>
<content:encoded><![CDATA[

arXiv:2505.00526v4 Announce Type: replace-cross 
Abstract: We develop pre-trained estimators for structural econometric models. The estimator uses a neural net to recognize the structural model's parameter from data patterns. Once trained, the estimator can be shared and applied to different datasets at negligible cost and effort. Under sufficient training, the estimator converges to the Bayesian posterior given the data patterns. As an illustration, we construct a pretrained estimator for a sequential search model (available at pnnehome.github.io). Estimation takes only seconds and achieves high accuracy on 12 real datasets. More broadly, pretrained estimators can make structural models much easier to use and more accessible.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering</title>
<link>https://arxiv.org/abs/2505.07073</link>
<guid>https://arxiv.org/abs/2505.07073</guid>
<content:encoded><![CDATA[

arXiv:2505.07073v2 Announce Type: replace-cross 
Abstract: Concept-based explanations have emerged as an effective approach within Explainable Artificial Intelligence, enabling interpretable insights by aligning model decisions with human-understandable concepts. However, existing methods rely on computationally intensive procedures and struggle to efficiently capture complex, semantic concepts. This work introduces the Concept Directions via Latent Clustering (CDLC), which extracts global, class-specific concept directions by clustering latent difference vectors derived from factual and diffusion-generated counterfactual image pairs. CDLC reduces storage requirements by ~4.6% and accelerates concept discovery by ~5.3% compared to the baseline method, while requiring no GPU for clustering, thereby enabling efficient extraction of multidimensional semantic concepts across latent dimensions. This approach is validated on a real-world skin lesion dataset, demonstrating that the extracted concept directions align with clinically recognized dermoscopic features and, in some cases, reveal dataset-specific biases or unknown biomarkers. These results highlight that CDLC is interpretable, scalable, and applicable across high-stakes domains and diverse data modalities.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces</title>
<link>https://arxiv.org/abs/2505.17703</link>
<guid>https://arxiv.org/abs/2505.17703</guid>
<content:encoded><![CDATA[

arXiv:2505.17703v2 Announce Type: replace-cross 
Abstract: Automatic program repair seeks to generate correct code from buggy programs, with most approaches searching the correct program in a discrete, symbolic space of source code tokens. This symbolic search is fundamentally limited by its inability to directly reason about program behavior. We introduce Gradient-Based Program Repair (GBPR), a new paradigm that reframes program repair as continuous optimization in a differentiable numerical program space. Our core insight is to compile symbolic programs into differentiable numerical representations, enabling search in the numerical program space directly guided by program behavior. To evaluate GBPR, we present RaspBugs, a new benchmark of 1,466 buggy symbolic RASP programs and their respective numerical representations. Our experiments demonstrate that GBPR can effectively repair buggy symbolic programs by gradient-based optimization in the numerical program space, with convincing repair trajectories. To our knowledge, we are the first to state program repair as continuous optimization in a numerical program space. Our work establishes a new direction for program repair research, bridging two rich worlds: continuous optimization and program behavior.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks</title>
<link>https://arxiv.org/abs/2505.17958</link>
<guid>https://arxiv.org/abs/2505.17958</guid>
<content:encoded><![CDATA[

arXiv:2505.17958v2 Announce Type: replace-cross 
Abstract: We study the high-dimensional asymptotics of empirical risk minimization (ERM) in over-parametrized two-layer neural networks with quadratic activations trained on synthetic data. We derive sharp asymptotics for both training and test errors by mapping the $\ell_2$-regularized learning problem to a convex matrix sensing task with nuclear norm penalization. This reveals that capacity control in such networks emerges from a low-rank structure in the learned feature maps. Our results characterize the global minima of the loss and yield precise generalization thresholds, showing how the width of the target function governs learnability. This analysis bridges and extends ideas from spin-glass methods, matrix factorization, and convex optimization and emphasizes the deep link between low-rank matrix sensing and learning in quadratic neural networks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs</title>
<link>https://arxiv.org/abs/2506.03195</link>
<guid>https://arxiv.org/abs/2506.03195</guid>
<content:encoded><![CDATA[

arXiv:2506.03195v2 Announce Type: replace-cross 
Abstract: Despite Multimodal Large Language Models (MLLMs) showing promising results on general zero-shot image classification tasks, fine-grained image classification remains challenging. It demands precise attention to subtle visual details to distinguish between visually similar subcategories--details that MLLMs may easily overlook without explicit guidance. To address this, we introduce AutoSEP, an iterative self-supervised prompt learning framework designed to enhance MLLM fine-grained classification capabilities in a fully unsupervised manner. Our core idea is to leverage unlabeled data to learn a description prompt that guides MLLMs in identifying crucial discriminative features within an image, and boosts classification accuracy. We developed an automatic self-enhancing prompt learning framework called AutoSEP to iteratively improve the description prompt using unlabeled data, based on instance-level classification scoring function. AutoSEP only requires black-box access to MLLMs, eliminating the need for any training or fine-tuning. We evaluate our approach on multiple fine-grained classification datasets. It consistently outperforms other unsupervised baselines, demonstrating the effectiveness of our self-supervised optimization framework. Notably, AutoSEP on average improves 13 percent over standard zero-shot classification and 5 percent over the best-performing baselines. Code is available at: https://github.com/yq-hong/AutoSEP
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Channel Estimation for RIS-Assisted mmWave Systems via Diffusion Models</title>
<link>https://arxiv.org/abs/2506.07770</link>
<guid>https://arxiv.org/abs/2506.07770</guid>
<content:encoded><![CDATA[

arXiv:2506.07770v3 Announce Type: replace-cross 
Abstract: Reconfigurable intelligent surface (RIS) has been recognized as a promising technology for next-generation wireless communications. However, the performance of RIS-assisted systems critically depends on accurate channel state information (CSI). To address this challenge, this letter proposes a novel channel estimation method for RIS-aided millimeter-wave (mmWave) systems based on diffusion models (DMs). Specifically, the forward diffusion process of the original signal is formulated to model the received signal as a noisy observation within the framework of DMs. Subsequently, the channel estimation task is formulated as the reverse diffusion process, and a sampling algorithm based on denoising diffusion implicit models (DDIMs) is developed to enable effective inference. Furthermore, a lightweight neural network, termed BRCNet, is introduced to replace the conventional U-Net, significantly reducing the number of parameters and computational complexity. Extensive experiments conducted under various scenarios demonstrate that the proposed method consistently outperforms existing baselines.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Reasoning in Ambiguous Contexts</title>
<link>https://arxiv.org/abs/2506.12241</link>
<guid>https://arxiv.org/abs/2506.12241</guid>
<content:encoded><![CDATA[

arXiv:2506.12241v2 Announce Type: replace-cross 
Abstract: We study the ability of language models to reason about appropriate information disclosure - a central aspect of the evolving field of agentic privacy. Whereas previous works have focused on evaluating a model's ability to align with human decisions, we examine the role of ambiguity and missing context on model performance when making information-sharing decisions. We identify context ambiguity as a crucial barrier for high performance in privacy assessments. By designing Camber, a framework for context disambiguation, we show that model-generated decision rationales can reveal ambiguities and that systematically disambiguating context based on these rationales leads to significant accuracy improvements (up to 13.3% in precision and up to 22.3% in recall) as well as reductions in prompt sensitivity. Overall, our results indicate that approaches for context disambiguation are a promising way forward to enhance agentic privacy reasoning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory</title>
<link>https://arxiv.org/abs/2507.02863</link>
<guid>https://arxiv.org/abs/2507.02863</guid>
<content:encoded><![CDATA[

arXiv:2507.02863v2 Announce Type: replace-cross 
Abstract: Dense 3D scene reconstruction from an ordered sequence or unordered image collections is a critical step when bringing research in computer vision into practical scenarios. Following the paradigm introduced by DUSt3R, which unifies an image pair densely into a shared coordinate system, subsequent methods maintain an implicit memory to achieve dense 3D reconstruction from more images. However, such implicit memory is limited in capacity and may suffer from information loss of earlier frames. We propose Point3R, an online framework targeting dense streaming 3D reconstruction. To be specific, we maintain an explicit spatial pointer memory directly associated with the 3D structure of the current scene. Each pointer in this memory is assigned a specific 3D position and aggregates scene information nearby in the global coordinate system into a changing spatial feature. Information extracted from the latest frame interacts explicitly with this pointer memory, enabling dense integration of the current observation into the global coordinate system. We design a 3D hierarchical position embedding to promote this interaction and design a simple yet effective fusion mechanism to ensure that our pointer memory is uniform and efficient. Our method achieves competitive or state-of-the-art performance on various tasks with low training costs. Code: https://github.com/YkiWu/Point3R.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Property Elicitation on Imprecise Probabilities</title>
<link>https://arxiv.org/abs/2507.05857</link>
<guid>https://arxiv.org/abs/2507.05857</guid>
<content:encoded><![CDATA[

arXiv:2507.05857v2 Announce Type: replace-cross 
Abstract: Property elicitation studies which attributes of a probability distribution can be determined by minimizing a risk. We investigate a generalization of property elicitation to imprecise probabilities (IP). This investigation is motivated by distributionally robust optimization and multi-distribution learning. Both those frameworks replace the minimization of a single risk over a (precise) probability by a maximin risk minimization over a set of probabilities -- i.e. an IP. We show what can be learned in those multi-distribution setups by providing necessary and sufficient conditions for the elicitability of an IP-property. Central to these conditions is the observation made in related literature that the elicited IP-property is the corresponding classical property of the probability in the IP with the maximum Bayes risk.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Equivariant Imaging: Acceleration for Unsupervised Learning via Augmented Lagrangian and Auxiliary PnP Denoisers</title>
<link>https://arxiv.org/abs/2507.06764</link>
<guid>https://arxiv.org/abs/2507.06764</guid>
<content:encoded><![CDATA[

arXiv:2507.06764v3 Announce Type: replace-cross 
Abstract: In this work, we propose Fast Equivariant Imaging (FEI), a novel unsupervised learning framework to rapidly and efficiently train deep imaging networks without ground-truth data. From the perspective of reformulating the Equivariant Imaging based optimization problem via the method of Lagrange multipliers and utilizing plug-and-play denoisers, this novel unsupervised scheme shows superior efficiency and performance compared to the vanilla Equivariant Imaging paradigm. In particular, our FEI schemes achieve an order-of-magnitude (10x) acceleration over standard EI on training U-Net for X-ray CT reconstruction and image inpainting, with improved generalization performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge</title>
<link>https://arxiv.org/abs/2508.02583</link>
<guid>https://arxiv.org/abs/2508.02583</guid>
<content:encoded><![CDATA[

arXiv:2508.02583v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM's intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ultralight Polarity-Split Neuromorphic SNN for Event-Stream Super-Resolution</title>
<link>https://arxiv.org/abs/2508.03244</link>
<guid>https://arxiv.org/abs/2508.03244</guid>
<content:encoded><![CDATA[

arXiv:2508.03244v2 Announce Type: replace-cross 
Abstract: Event cameras offer unparalleled advantages such as high temporal resolution, low latency, and high dynamic range. However, their limited spatial resolution poses challenges for fine-grained perception tasks. In this work, we propose an ultra-lightweight, stream-based event-to-event super-resolution method based on Spiking Neural Networks (SNNs), designed for real-time deployment on resource-constrained devices. To further reduce model size, we introduce a novel Dual-Forward Polarity-Split Event Encoding strategy that decouples positive and negative events into separate forward paths through a shared SNN. Furthermore, we propose a Learnable Spatio-temporal Polarity-aware Loss (LearnSTPLoss) that adaptively balances temporal, spatial, and polarity consistency using learnable uncertainty-based weights. Experimental results demonstrate that our method achieves competitive super-resolution performance on multiple datasets while significantly reducing model size and inference time. The lightweight design enables embedding the module into event cameras or using it as an efficient front-end preprocessing for downstream vision tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holistic Evaluation of Multimodal LLMs on Spatial Intelligence</title>
<link>https://arxiv.org/abs/2508.13142</link>
<guid>https://arxiv.org/abs/2508.13142</guid>
<content:encoded><![CDATA[

arXiv:2508.13142v4 Announce Type: replace-cross 
Abstract: Multimodal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, the very capability that anchors artificial general intelligence in the physical world. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path toward spatial intelligence (SI). We thus propose EASI for holistic Evaluation of multimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and a growing collection of newly curated ones, enabling systematic evaluation of state-of-the-art models. In this report, we conduct the study across eight key benchmarks, at a cost exceeding ten billion total tokens. Our empirical study then reveals that (1) GPT-5 demonstrates unprecedented strength in SI, yet (2) still falls short of human performance significantly across a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose greater model capability deficiency than non-SI tasks, to the extent that (4) proprietary models do not exhibit a decisive advantage when facing the most difficult ones. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans, yet fail the most advanced multimodal models. EASI is an ongoing community effort: we have open-sourced the EASI codebase that provides a one-stop and reproducible solution with standardized interfaces, integrated protocols and prompts that significantly reduce the friction of configuring and running multiple benchmarks; we have also launched an accompanying EASI leaderboard to provide a continually updated snapshot of model performance across the full SI spectrum, accelerating collective progress toward robust SI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints</title>
<link>https://arxiv.org/abs/2508.13663</link>
<guid>https://arxiv.org/abs/2508.13663</guid>
<content:encoded><![CDATA[

arXiv:2508.13663v3 Announce Type: replace-cross 
Abstract: Methods for query answering over incomplete knowledge graphs retrieve entities that are \emph{likely} to be answers, which is particularly useful when such answers cannot be reached by direct graph traversal due to missing edges. However, existing approaches have focused on queries formalized using first-order-logic. In practice, many real-world queries involve constraints that are inherently vague or context-dependent, such as preferences for attributes or related categories. Addressing this gap, we introduce the problem of query answering with soft constraints. We formalize the problem and introduce two efficient methods designed to adjust query answer scores by incorporating soft constraints without disrupting the original answers to a query. These methods are lightweight, requiring tuning only two parameters or a small neural network trained to capture soft constraints while maintaining the original ranking structure. To evaluate the task, we extend existing QA benchmarks by generating datasets with soft constraints. Our experiments demonstrate that our methods can capture soft constraints while maintaining robust query answering performance and adding very little overhead. With our work, we explore a new and flexible way to interact with graph databases that allows users to specify their preferences by providing examples interactively.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training</title>
<link>https://arxiv.org/abs/2508.15390</link>
<guid>https://arxiv.org/abs/2508.15390</guid>
<content:encoded><![CDATA[

arXiv:2508.15390v3 Announce Type: replace-cross 
Abstract: Large language models are trained with tokenizers, and the resulting token distribution is highly imbalanced: a few words dominate the stream while most occur rarely. Recent practice favors ever-larger vocabularies, but it is unclear where the benefit comes from. To this end, we perform a controlled study that scales the vocabulary of the language model from 24K to 196K while holding data, computation, and optimization unchanged. We begin by quantifying the complexity of tokenized text -- formalized via Kolmogorov complexity -- and show that larger vocabularies reduce this complexity. Above 24K, every common word is already tokenized as a single token, so enlarging vocabulary only deepens the relative token-frequency imbalance. Word-level loss decomposition shows that larger vocabularies reduce cross-entropy loss almost exclusively by lowering uncertainty on the 2,500 most frequent words, even though loss on the rare tail rises. The same frequent words cover roughly 75% of tokens in downstream benchmarks, so this training advantage transfers intact. We further show that enlarging model parameters with a fixed vocabulary yields the same frequent-word benefit. Our results recast "bigger vocabularies help" as "lowering complexity of tokenized text helps," offering a simple, principled knob for tokenizer-model co-design and clarifying the loss dynamics that govern language model scaling in pre-training.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification</title>
<link>https://arxiv.org/abs/2508.20461</link>
<guid>https://arxiv.org/abs/2508.20461</guid>
<content:encoded><![CDATA[

arXiv:2508.20461v2 Announce Type: replace-cross 
Abstract: We propose a novel medical image classification method that integrates dual-model weight selection with self-knowledge distillation (SKD). In real-world medical settings, deploying large-scale models is often limited by computational resource constraints, which pose significant challenges for their practical implementation. Thus, developing lightweight models that achieve comparable performance to large-scale models while maintaining computational efficiency is crucial. To address this, we employ a dual-model weight selection strategy that initializes two lightweight models with weights derived from a large pretrained model, enabling effective knowledge transfer. Next, SKD is applied to these selected models, allowing the use of a broad range of initial weight configurations without imposing additional excessive computational cost, followed by fine-tuning for the target classification tasks. By combining dual-model weight selection with self-knowledge distillation, our method overcomes the limitations of conventional approaches, which often fail to retain critical information in compact models. Extensive experiments on publicly available datasets-chest X-ray images, lung computed tomography scans, and brain magnetic resonance imaging scans-demonstrate the superior performance and robustness of our approach compared to existing methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning and composing of classical music using restricted Boltzmann machines</title>
<link>https://arxiv.org/abs/2509.04899</link>
<guid>https://arxiv.org/abs/2509.04899</guid>
<content:encoded><![CDATA[

arXiv:2509.04899v3 Announce Type: replace-cross 
Abstract: We investigate how machine learning models acquire the ability to compose music and how musical information is internally represented within such models. We develop a composition algorithm based on a restricted Boltzmann machine (RBM), a simple generative model capable of producing musical pieces of arbitrary length. We convert musical scores into piano-roll image representations and train the RBM in an unsupervised manner. We confirm that the trained RBM can generate new musical pieces; however, by analyzing the model's responses and internal structure, we find that the learned information is not stored in a form directly interpretable by humans. This study contributes to a better understanding of how machine learning models capable of music composition may internally represent musical structure and highlights issues related to the interpretability of generative models in creative tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2509.08422</link>
<guid>https://arxiv.org/abs/2509.08422</guid>
<content:encoded><![CDATA[

arXiv:2509.08422v3 Announce Type: replace-cross 
Abstract: Video-based AI systems are increasingly adopted in safety-critical domains such as autonomous driving and healthcare. However, interpreting their decisions remains challenging due to the inherent spatiotemporal complexity of video data and the opacity of deep learning models. Existing explanation techniques often suffer from limited temporal coherence and a lack of actionable causal insights. Current counterfactual explanation methods typically do not incorporate guidance from the target model, reducing semantic fidelity and practical utility. We introduce Latent Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework designed to explain the behavior of video-based AI models. Compared to previous approaches, LD-ViCE reduces the computational costs of generating explanations by operating in latent space using a state-of-the-art diffusion model, while producing realistic and interpretable counterfactuals through an additional refinement step. Experiments on three diverse video datasets - EchoNet-Dynamic (cardiac ultrasound), FERV39k (facial expression), and Something-Something V2 (action recognition) with multiple target models covering both classification and regression tasks, demonstrate that LD-ViCE generalizes well and achieves state-of-the-art performance. On the EchoNet-Dynamic dataset, LD-ViCE achieves significantly higher regression accuracy than prior methods and exhibits high temporal consistency, while the refinement stage further improves perceptual quality. Qualitative analyses confirm that LD-ViCE produces semantically meaningful and temporally coherent explanations, providing actionable insights into model behavior. LD-ViCE advances the trustworthiness and interpretability of video-based AI systems through visually coherent counterfactual explanations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On a Geometry of Interbrain Networks</title>
<link>https://arxiv.org/abs/2509.10650</link>
<guid>https://arxiv.org/abs/2509.10650</guid>
<content:encoded><![CDATA[

arXiv:2509.10650v3 Announce Type: replace-cross 
Abstract: Effective analysis in neuroscience benefits significantly from robust conceptual frameworks. Traditional metrics of interbrain synchrony in social neuroscience typically depend on fixed, correlation-based approaches, restricting their explanatory capacity to descriptive observations. Inspired by the successful integration of geometric insights in network science, we propose leveraging discrete geometry to examine the dynamic reconfigurations in neural interactions during social exchanges. Unlike conventional synchrony approaches, our method interprets inter-brain connectivity changes through the evolving geometric structures of neural networks. This geometric framework is realized through a pipeline that identifies critical transitions in network connectivity using entropy metrics derived from curvature distributions. By doing so, we significantly enhance the capacity of hyperscanning methodologies to uncover underlying neural mechanisms in interactive social behavior.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Audio Codecs for Prompt-Driven Universal Sound Separation</title>
<link>https://arxiv.org/abs/2509.11717</link>
<guid>https://arxiv.org/abs/2509.11717</guid>
<content:encoded><![CDATA[

arXiv:2509.11717v4 Announce Type: replace-cross 
Abstract: Text-guided sound separation supports flexible audio editing across media and assistive applications, but existing models like AudioSep are too compute-heavy for edge deployment. Neural audio codec (NAC) models such as CodecFormer and SDCodec are compute-efficient but limited to fixed-class separation. We introduce CodecSep, the first NAC-based model for on-device universal, text-driven separation. CodecSep combines DAC compression with a Transformer masker modulated by CLAP-derived FiLM parameters. Across six open-domain benchmarks under matched training/prompt protocols, \textbf{CodecSep} surpasses \textbf{AudioSep} in separation fidelity (SI-SDR) while remaining competitive in perceptual quality (ViSQOL) and matching or exceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream deployments, it needs just 1.35~GMACs end-to-end -- approximately $54\times$ less compute ($25\times$ architecture-only) than spectrogram-domain separators like AudioSep -- while remaining fully bitstream-compatible.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatially Parallel All-optical Neural Networks</title>
<link>https://arxiv.org/abs/2509.23611</link>
<guid>https://arxiv.org/abs/2509.23611</guid>
<content:encoded><![CDATA[

arXiv:2509.23611v2 Announce Type: replace-cross 
Abstract: All-optical neural networks (AONNs) have emerged as a promising paradigm for ultrafast and energy-efficient computation. These networks typically consist of multiple serially connected layers between input and output layers--a configuration we term spatially series AONNs, with deep neural networks (DNNs) being the most prominent examples. However, such series architectures suffer from progressive signal degradation during information propagation and critically require additional nonlinearity designs to model complex relationships effectively. Here we propose a spatially parallel architecture for all-optical neural networks (SP-AONNs). Unlike series architecture that sequentially processes information through consecutively connected optical layers, SP-AONNs divide the input signal into identical copies fed simultaneously into separate optical layers. Through coherent interference between these parallel linear sub-networks, SP-AONNs inherently enable nonlinear computation without relying on active nonlinear components or iterative updates. We implemented a modular 4F optical system for SP-AONNs and evaluated its performance across multiple image classification benchmarks. Experimental results demonstrate that increasing the number of parallel sub-networks consistently enhances accuracy, improves noise robustness, and expands model expressivity. Our findings highlight spatial parallelism as a practical and scalable strategy for advancing the capabilities of optical neural computing.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAKO: Meta-Adaptive Koopman Operators for Learning-based Model Predictive Control of Parametrically Uncertain Nonlinear Systems</title>
<link>https://arxiv.org/abs/2510.09042</link>
<guid>https://arxiv.org/abs/2510.09042</guid>
<content:encoded><![CDATA[

arXiv:2510.09042v2 Announce Type: replace-cross 
Abstract: In this work, we propose a meta-learning-based Koopman modeling and predictive control approach for nonlinear systems with parametric uncertainties. An adaptive deep meta-learning-based modeling approach, called Meta Adaptive Koopman Operator (MAKO), is proposed. Without knowledge of the parametric uncertainty, the proposed MAKO approach can learn a meta-model from a multi-modal dataset and efficiently adapt to new systems with previously unseen parameter settings by using online data. Based on the learned meta Koopman model, a predictive control scheme is developed, and the stability of the closed-loop system is ensured even in the presence of previously unseen parameter settings. Through extensive simulations, our proposed approach demonstrates superior performance in both modeling accuracy and control efficacy as compared to competitive baselines.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Sampling for Efficient Learning of Coarse-Grained Machine Learning Potentials</title>
<link>https://arxiv.org/abs/2510.11148</link>
<guid>https://arxiv.org/abs/2510.11148</guid>
<content:encoded><![CDATA[

arXiv:2510.11148v2 Announce Type: replace-cross 
Abstract: Coarse-graining (CG) enables molecular dynamics (MD) simulations of larger systems and longer timescales that are otherwise infeasible with atomistic models. Machine learning potentials (MLPs), with their capacity to capture many-body interactions, can provide accurate approximations of the potential of mean force (PMF) in CG models. Current CG MLPs are typically trained in a bottom-up manner via force matching, which in practice relies on configurations sampled from the unbiased equilibrium Boltzmann distribution to ensure thermodynamic consistency. This convention poses two key limitations: first, sufficiently long atomistic trajectories are needed to reach convergence; and second, even once equilibrated, transition regions remain poorly sampled. To address these issues, we employ enhanced sampling to bias along CG degrees of freedom for data generation, and then recompute the forces with respect to the unbiased potential. This strategy simultaneously shortens the simulation time required to produce equilibrated data and enriches sampling in transition regions, while preserving the correct PMF. We demonstrate its effectiveness on the M\"uller-Brown potential and capped alanine, achieving notable improvements. Our findings support the use of enhanced sampling for force matching as a promising direction to improve the accuracy and reliability of CG MLPs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection</title>
<link>https://arxiv.org/abs/2510.16499</link>
<guid>https://arxiv.org/abs/2510.16499</guid>
<content:encoded><![CDATA[

arXiv:2510.16499v2 Announce Type: replace-cross 
Abstract: Designing effective agentic systems requires the seamless composition and integration of agents, tools, and models within dynamic and uncertain environments. Most existing methods rely on static, semantic retrieval approaches for tool or agent discovery. However, effective reuse and composition of existing components remain challenging due to incomplete capability descriptions and the limitations of retrieval methods. Component selection suffers because the decisions are not based on capability, cost, and real-time utility. To address these challenges, we introduce a structured, automated framework for agentic system composition that is inspired by the knapsack problem. Our framework enables a composer agent to systematically identify, select, and assemble an optimal set of agentic components by jointly considering performance, budget constraints, and compatibility. By dynamically testing candidate components and modeling their utility in real-time, our approach streamlines the assembly of agentic systems and facilitates scalable reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five benchmarking datasets shows that our online-knapsack-based composer consistently lies on the Pareto frontier, achieving higher success rates at significantly lower component costs compared to our baselines. In the single-agent setup, the online knapsack composer shows a success rate improvement of up to 31.6% in comparison to the retrieval baselines. In multi-agent systems, the online knapsack composer increases success rate from 37% to 87% when agents are selected from an agent inventory of 100+ agents. The substantial performance gap confirms the robust adaptability of our method across diverse domains and budget constraints.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation</title>
<link>https://arxiv.org/abs/2510.17529</link>
<guid>https://arxiv.org/abs/2510.17529</guid>
<content:encoded><![CDATA[

arXiv:2510.17529v2 Announce Type: replace-cross 
Abstract: Active Surveillance (AS) is a treatment option for managing low and intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while monitoring disease progression through serial MRI and clinical follow-up. Accurate prostate segmentation is an important preliminary step for automating this process, enabling automated detection and diagnosis of PCa. However, existing deep-learning segmentation models are often trained on single-time-point and expertly annotated datasets, making them unsuitable for longitudinal AS analysis, where multiple time points and a scarcity of expert labels hinder their effective fine-tuning. To address these challenges, we propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation architecture that computes the segmentation for time point t by leveraging the MRI and the corresponding segmentation mask from the previous time point. We introduce two new components: (i) a Mamba-enhanced Cross-Attention Module, which integrates the Mamba block into cross attention to efficiently capture temporal evolution and long-range spatial dependencies, and (ii) a Shape Extractor Module that encodes the previous segmentation mask into a latent anatomical representation for refined zone delination. Moreover, we introduce a semi-supervised self-training strategy that leverages pseudo-labels generated from a pre-trained nnU-Net, enabling effective learning without expert annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results showed that it significantly outperforms state-of-the-art U-Net and Transformer-based models, achieving superior prostate zone segmentation even when trained on limited and noisy data.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning</title>
<link>https://arxiv.org/abs/2510.20406</link>
<guid>https://arxiv.org/abs/2510.20406</guid>
<content:encoded><![CDATA[

arXiv:2510.20406v2 Announce Type: replace-cross 
Abstract: Robotic manipulation systems benefit from complementary sensing modalities, where each provides unique environmental information. Point clouds capture detailed geometric structure, while RGB images provide rich semantic context. Current point cloud methods struggle to capture fine-grained detail, especially for complex tasks, which RGB methods lack geometric awareness, which hinders their precision and generalization. We introduce PointMapPolicy, a novel approach that conditions diffusion policies on structured grids of points without downsampling. The resulting data type makes it easier to extract shape and spatial relationships from observations, and can be transformed between reference frames. Yet due to their structure in a regular grid, we enable the use of established computer vision techniques directly to 3D data. Using xLSTM as a backbone, our model efficiently fuses the point maps with RGB data for enhanced multi-modal perception. Through extensive experiments on the RoboCasa and CALVIN benchmarks and real robot evaluations, we demonstrate that our method achieves state-of-the-art performance across diverse manipulation tasks. The overview and demos are available on our project page: https://point-map.github.io/Point-Map/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Coherence-Based Measure of AGI</title>
<link>https://arxiv.org/abs/2510.20784</link>
<guid>https://arxiv.org/abs/2510.20784</guid>
<content:encoded><![CDATA[

arXiv:2510.20784v2 Announce Type: replace-cross 
Abstract: Recent approaches to evaluating Artificial General Intelligence (AGI) typically summarize a system's capability using the arithmetic mean of its proficiencies across multiple cognitive domains. While simple, this implicitly assumes compensability: exceptional performance in some areas can offset severe deficiencies in others. Genuine general intelligence, however, requires coherent sufficiency: balanced competence across all essential faculties. We introduce a coherence-based measure of AGI that integrates the generalized mean over a continuum of compensability exponents. This yields an area-under-the-curve (AUC) metric spanning arithmetic, geometric, and harmonic regimes, quantifying how robust an evaluated capability remains as compensability assumptions become stricter. Unlike the arithmetic mean, which rewards specialization, the AUC penalizes imbalance and exposes bottlenecks that constrain performance. To illustrate the framework, we apply it to cognitive profiles derived from the Cattell-Horn-Carroll (CHC) model, showing how coherence-based aggregation highlights imbalances that are obscured by arithmetic averaging. As a second, independent example, we apply the same methodology to a set of 17 heterogeneous benchmarks, demonstrating how coherence-based evaluation can reveal unevenness even in narrower task collections. These examples show that the proposed approach offers a principled, interpretable, and stricter foundation for measuring progress toward AGI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongCat-Flash-Omni Technical Report</title>
<link>https://arxiv.org/abs/2511.00279</link>
<guid>https://arxiv.org/abs/2511.00279</guid>
<content:encoded><![CDATA[

arXiv:2511.00279v2 Announce Type: replace-cross 
Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Analogical Inference from Boolean to Continuous Domains</title>
<link>https://arxiv.org/abs/2511.10416</link>
<guid>https://arxiv.org/abs/2511.10416</guid>
<content:encoded><![CDATA[

arXiv:2511.10416v2 Announce Type: replace-cross 
Abstract: Analogical reasoning is a powerful inductive mechanism, widely used in human cognition and increasingly applied in artificial intelligence. Formal frameworks for analogical inference have been developed for Boolean domains, where inference is provably sound for affine functions and approximately correct for functions close to affine. These results have informed the design of analogy-based classifiers. However, they do not extend to regression tasks or continuous domains. In this paper, we revisit analogical inference from a foundational perspective. We first present a counterexample showing that existing generalization bounds fail even in the Boolean setting. We then introduce a unified framework for analogical reasoning in real-valued domains based on parameterized analogies defined via generalized means. This model subsumes both Boolean classification and regression, and supports analogical inference over continuous functions. We characterize the class of analogy-preserving functions in this setting and derive both worst-case and average-case error bounds under smoothness assumptions. Our results offer a general theory of analogical inference across discrete and continuous domains.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Spatial Intelligence with Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2511.13719</link>
<guid>https://arxiv.org/abs/2511.13719</guid>
<content:encoded><![CDATA[

arXiv:2511.13719v2 Announce Type: replace-cross 
Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Improvement Supervision</title>
<link>https://arxiv.org/abs/2511.16886</link>
<guid>https://arxiv.org/abs/2511.16886</guid>
<content:encoded><![CDATA[

arXiv:2511.16886v2 Announce Type: replace-cross 
Abstract: Recently, it was shown that small, looped architectures, such as Tiny Recursive Models (TRMs), can outperform Large Language Models (LLMs) on complex reasoning tasks, including the Abstraction and Reasoning Corpus (ARC). In this work, we investigate a core question: how can we further improve the efficiency of these methods with minimal changes? To address this, we frame the latent reasoning of TRMs as a form of classifier-free guidance and implicit policy improvement algorithm. Building on these insights, we propose a novel training scheme that provides a target for each loop during training. We demonstrate that our approach significantly enhances training efficiency. Our method reduces the total number of forward passes by 18x and eliminates halting mechanisms, while maintaining quality comparable to standard TRMs. Notably, we achieve 24% accuracy on ARC-1 with only 0.8M parameters, outperforming most LLMs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>Mathematical Insights into Protein Architecture: Persistent Homology and Machine Learning Applied to the Flagellar Motor</title>
<link>https://arxiv.org/abs/2504.16941</link>
<guid>https://arxiv.org/abs/2504.16941</guid>
<content:encoded><![CDATA[
<div> Keywords: persistent homology, bacterial flagellar motor, topological data analysis, machine learning, protein function prediction<br /><br />Summary:<br /><br />1. The study introduces a machine learning method that uses persistent homology, a tool from topological data analysis, to classify bacterial flagellar motors into two states: rotated and stalled.<br /><br />2. Protein structural data are embedded into a topological framework by constructing filtered simplicial complexes from atomic coordinates, enabling extraction of multiscale topological features.<br /><br />3. Key topological invariants, including persistence diagrams and barcodes, capture geometric and connectivity patterns that correlate closely with the functional state of the motors.<br /><br />4. These topological features are vectorized and input into a machine learning pipeline comprising dimensionality reduction followed by supervised classification, allowing the model to handle complex structural variations.<br /><br />5. Applied on a curated dataset of experimentally characterized flagellar motors from diverse bacterial species, this approach achieves high accuracy and robustness, emphasizing the value of topological data analysis beyond traditional geometric descriptors for protein function prediction. <div>
arXiv:2504.16941v4 Announce Type: replace-cross 
Abstract: We present a machine learning approach that leverages persistent homology to classify bacterial flagellar motors into two functional states: rotated and stalled. By embedding protein structural data into a topological framework, we extract multiscale features from filtered simplicial complexes constructed over atomic coordinates. These topological invariants, specifically persistence diagrams and barcodes, capture critical geometric and connectivity patterns that correlate with motor function. The extracted features are vectorized and integrated into a machine learning pipeline that includes dimensionality reduction and supervised classification. Applied to a curated dataset of experimentally characterized flagellar motors from diverse bacterial species, our model demonstrates high classification accuracy and robustness to structural variation. This approach highlights the power of topological data analysis in revealing functionally relevant patterns beyond the reach of traditional geometric descriptors, offering a novel computational tool for protein function prediction.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems</title>
<link>https://arxiv.org/abs/2504.20906</link>
<guid>https://arxiv.org/abs/2504.20906</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, industrial control systems, linearization, explainability, water treatment testbed<br /><br />Summary:<br /><br />1. The paper emphasizes the importance of continuous monitoring of cyber-physical components in industrial control systems (ICS) to ensure safe automation and fail-safe plant operations.<br />2. Safety is ensured by managing actuator commands based on sensor readings, which serve as ground truth for decision-making.<br />3. Detecting anomalies such as attacks, faults, and uncertain states in ICS is critical for plant safety, personnel protection, and service reliability.<br />4. The authors propose an anomaly detection approach utilizing accurate linearization of nonlinear sensor-actuator relationships, since linear models are simpler and well-studied.<br />5. Their approach is demonstrated using a water treatment testbed, showing millisecond-level anomaly detection response.<br />6. The method excels in both detection speed and explainability, outperforming existing AI/ML models integrated with explainable AI (XAI) for similar purposes.<br />7. Explainability allows pinpointing specific sensors and actuator states involved in detected anomalies.<br />8. The algorithm achieved 97.72% accuracy by correctly classifying deviations within safe limits as non-anomalous, indicating that ultra-high-resolution but slower detectors are unnecessary when safety boundaries have tolerance.<br />9. Overall, the method offers a fast, accurate, and interpretable tool for ICS anomaly detection, enhancing operational safety and reliability. <div>
arXiv:2504.20906v3 Announce Type: replace-cross 
Abstract: The continuous monitoring of the interactions between cyber-physical components of any industrial control system (ICS) is required to secure automation of the system controls, and to guarantee plant processes are fail-safe and remain in an acceptably safe state. Safety is achieved by managing actuation (where electric signals are used to trigger physical movement), dependent on corresponding sensor readings; used as ground truth in decision making. Timely detection of anomalies (attacks, faults and unascertained states) in ICSs is crucial for the safe running of a plant, the safety of its personnel, and for the safe provision of any services provided. We propose an anomaly detection method that involves accurate linearization of the non-linear forms arising from sensor-actuator(s) relationships, primarily because solving linear models is easier and well understood. We accomplish this by using a well-known water treatment testbed as a use case. Our experiments show millisecond time response to detect anomalies, all of which are explainable and traceable; this simultaneous coupling of detection speed and explainability has not been achieved by other state of the art Artificial Intelligence (AI)/ Machine Learning (ML) models with eXplainable AI (XAI) used for the same purpose. Our methods explainability enables us to pin-point the sensor(s) and the actuation state(s) for which the anomaly was detected. The proposed algorithm showed an accuracy of 97.72% by flagging deviations within safe operation limits as non-anomalous; indicative that slower detectors with highest detection resolution is unnecessary, for systems whose safety boundaries provide leeway within safety limits.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding</title>
<link>https://arxiv.org/abs/2511.20696</link>
<guid>https://arxiv.org/abs/2511.20696</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG decoding, continual learning, prototype memory, knowledge distillation, cross-subject alignment<br /><br />Summary: Due to large variability in EEG signals across individuals, continual learning models for EEG decoding face the challenge of catastrophic forgetting, where knowledge from previous subjects is lost as new subjects are introduced. Existing methods often rely on storing historical EEG data as replay buffers to mitigate forgetting, but this is impractical due to privacy issues and memory limits. This paper proposes a Prototype-guided Non-Exemplar Continual Learning (ProNECL) framework that preserves learned knowledge without retaining any raw EEG samples. ProNECL creates class-level prototypes that summarize discriminative features from each subject, effectively capturing essential information in a compressed form. The framework incrementally aligns features from new subjects with these global prototype memories by employing cross-subject feature alignment and knowledge distillation techniques. Experimental validation on two benchmark datasets, BCI Competition IV 2a and 2b, demonstrates that ProNECL successfully balances the retention of previous knowledge with adaptability to new subjects. The proposed method achieves superior performance in cross-subject continual EEG decoding tasks compared to traditional approaches, highlighting its potential for privacy-preserving and efficient EEG-based brain-computer interface applications. <div>
arXiv:2511.20696v1 Announce Type: new 
Abstract: Due to the significant variability in electroencephalogram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding task. Current works mainly rely on storing the historical data of seen subjects as a replay buffer to prevent forgetting. However, privacy concerns or memory constraints make keeping such data impractical. Instead, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL)framework that preserves prior knowledge without accessing any historical EEG samples. ProNECL constructs class-level prototypes to summarize discriminative representations from each subject and incrementally aligns new feature spaces with the global prototype memory through cross-subject feature alignment and knowledge distillation. Validated on the BCI Competition IV 2a and 2b datasets, our framework effectively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Role of Hidden States of Modern Hopfield Network in Transformer</title>
<link>https://arxiv.org/abs/2511.20698</link>
<guid>https://arxiv.org/abs/2511.20698</guid>
<content:encoded><![CDATA[
<div> Keywords: Hopfield networks, self-attention, Transformers, modern Hopfield attention, rank collapse<br /><br />Summary: This paper investigates the connection between modern Hopfield networks (MHNs) and the self-attention mechanism used in Transformers, going beyond the previously known adiabatic approximation. A new variable, the hidden state derived from the MHN, is introduced into the self-attention framework, resulting in a novel attention mechanism called modern Hopfield attention (MHA). MHA allows attention scores from the input layer of the Transformer to be inherited by the output layer, enhancing the quality and stability of attention weights. The study addresses critical challenges in deep Transformers, specifically rank collapse and token uniformity, both theoretically and empirically showing significant improvements using MHA hidden states. Importantly, the proposed modification improves model accuracy in Vision Transformer and GPT architectures without increasing the number of training parameters. This work highlights the beneficial role that ideas from Hopfield networks can play in advancing the Transformer architecture, offering a new perspective for designing more robust and effective attention mechanisms in deep learning models. <div>
arXiv:2511.20698v1 Announce Type: new 
Abstract: Associative memory models based on Hopfield networks and self-attention based on key-value mechanisms have been popular approaches in the study of memory mechanisms in deep learning. It has been pointed out that the state update rule of the modern Hopfield network (MHN) in the adiabatic approximation is in agreement with the self-attention layer of Transformer. In this paper, we go beyond this approximation and investigate the relationship between MHN and self-attention. Our results show that the correspondence between Hopfield networks and Transformers can be established in a more generalized form by adding a new variable, the hidden state derived from the MHN, to self-attention. This new attention mechanism, modern Hopfield attention (MHA), allows the inheritance of attention scores from the input layer of the Transformer to the output layer, which greatly improves the nature of attention weights. In particular, we show both theoretically and empirically that MHA hidden states significantly improve serious problem of deep Transformers known as rank collapse and token uniformity. We also confirm that MHA can systematically improve accuracy without adding training parameters to the Vision Transformer or GPT. Our results provide a new case in which Hopfield networks can be a useful perspective for improving the Transformer architecture.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Post-Pruning Accuracy Recovery via Data-Free Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.20702</link>
<guid>https://arxiv.org/abs/2511.20702</guid>
<content:encoded><![CDATA[
<div> Model pruning, Data-Free Knowledge Distillation, DeepInversion, Batch Normalization, Privacy-preserving<br /><br />Summary:<br /><br />1. The paper addresses the challenge of model pruning in Deep Neural Networks (DNNs), focusing on the accuracy degradation often caused by global unstructured pruning. 2. It highlights the difficulty of fine-tuning pruned models in privacy-sensitive domains, such as healthcare and finance, where original training data cannot be accessed post-deployment due to legal regulations like GDPR and HIPAA. 3. To overcome this, the authors propose a novel Data-Free Knowledge Distillation framework that synthesizes "dream" images from the pretrained teacher model using DeepInversion by inverting Batch Normalization statistics, thus avoiding the need for real data. 4. These synthetic images act as a transfer set to distill knowledge from the original teacher model to the pruned student model, facilitating accurate model compression without data privacy compromises. 5. Experimental evaluations on CIFAR-10 using various architectures (ResNet, MobileNet, VGG) demonstrate that the proposed method effectively recovers the accuracy lost during pruning, achieving significant improvements without accessing any real training samples. This approach provides a practical solution to deploy compressed DNNs securely in privacy-sensitive applications. <div>
arXiv:2511.20702v1 Announce Type: new 
Abstract: Model pruning is a widely adopted technique to reduce the computational complexity and memory footprint of Deep Neural Networks (DNNs). However, global unstructured pruning often leads to significant degradation in accuracy, typically necessitating fine-tuning on the original training dataset to recover performance. In privacy-sensitive domains such as healthcare or finance, access to the original training data is often restricted post-deployment due to regulations (e.g., GDPR, HIPAA). This paper proposes a Data-Free Knowledge Distillation framework to bridge the gap between model compression and data privacy. We utilize DeepInversion to synthesize privacy-preserving ``dream'' images from the pre-trained teacher model by inverting Batch Normalization (BN) statistics. These synthetic images serve as a transfer set to distill knowledge from the original teacher to the pruned student network. Experimental results on CIFAR-10 across various architectures (ResNet, MobileNet, VGG) demonstrate that our method significantly recovers accuracy lost during pruning without accessing a single real data point.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretraining Transformer-Based Models on Diffusion-Generated Synthetic Graphs for Alzheimer's Disease Prediction</title>
<link>https://arxiv.org/abs/2511.20704</link>
<guid>https://arxiv.org/abs/2511.20704</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, diffusion model, graph transformer, synthetic data, transfer learning<br /><br />Summary: Early and accurate detection of Alzheimer's disease (AD) is essential to enable timely intervention and improve patient outcomes. The development of robust machine learning models for AD diagnosis is hindered by challenges including limited labeled data, class imbalance, and heterogeneity across multiple clinical sites. To address these issues, the authors propose a Transformer-based diagnostic framework that integrates diffusion-based synthetic data generation with graph representation learning and transfer learning. They employ a class-conditional denoising diffusion probabilistic model (DDPM) trained on the NACC dataset to generate a large synthetic cohort that replicates multimodal clinical and neuroimaging feature distributions while balancing diagnostic classes. Modality-specific Graph Transformer encoders are pretrained on this synthetic data to learn class-discriminative and robust feature embeddings, which are then fixed during the training of a neural classifier on the original NACC data. The study measures alignment between synthetic and real data distributions using metrics such as Maximum Mean Discrepancy (MMD), Frechet distance, and energy distance and evaluates diagnostic performance by calibration and fixed-specificity sensitivity analyses. The results demonstrate that this approach outperforms standard baselines, including early and late fusion deep neural networks and the graph-based model MaGNet, showing improved AUC, accuracy, sensitivity, and specificity in subject-wise cross-validation. Overall, diffusion-based synthetic pretraining combined with Graph Transformers enhances generalization in low-sample, imbalanced clinical prediction tasks. <div>
arXiv:2511.20704v1 Announce Type: new 
Abstract: Early and accurate detection of Alzheimer's disease (AD) is crucial for enabling timely intervention and improving outcomes. However, developing reliable machine learning (ML) models for AD diagnosis is challenging due to limited labeled data, multi-site heterogeneity, and class imbalance. We propose a Transformer-based diagnostic framework that combines diffusion-based synthetic data generation with graph representation learning and transfer learning. A class-conditional denoising diffusion probabilistic model (DDPM) is trained on the real-world NACC dataset to generate a large synthetic cohort that mirrors multimodal clinical and neuroimaging feature distributions while balancing diagnostic classes. Modality-specific Graph Transformer encoders are first pretrained on this synthetic data to learn robust, class-discriminative representations and are then frozen while a neural classifier is trained on embeddings from the original NACC data. We quantify distributional alignment between real and synthetic cohorts using metrics such as Maximum Mean Discrepancy (MMD), Frechet distance, and energy distance, and complement discrimination metrics with calibration and fixed-specificity sensitivity analyses. Empirically, our framework outperforms standard baselines, including early and late fusion deep neural networks and the multimodal graph-based model MaGNet, yielding higher AUC, accuracy, sensitivity, and specificity under subject-wise cross-validation on NACC. These results show that diffusion-based synthetic pretraining with Graph Transformers can improve generalization in low-sample, imbalanced clinical prediction settings.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Diffusion Inverse Problems with Restart Posterior Sampling</title>
<link>https://arxiv.org/abs/2511.20705</link>
<guid>https://arxiv.org/abs/2511.20705</guid>
<content:encoded><![CDATA[
<div> Inverse problems, diffusion models, posterior sampling, restart strategy, non-linear measurement models  

<br /><br />Summary:  
Inverse problems involve estimating an unknown signal or state from incomplete or noisy observations, a challenge prevalent in science and engineering. Recent methods use diffusion models as implicit priors due to their capability to model complex data distributions, but they often depend on approximations of the posterior, require costly gradient backpropagation through the score network, or are limited to linear measurement settings. This paper introduces Restart for Posterior Sampling (RePS), a novel and general framework that addresses these limitations by enabling efficient sampling in both linear and non-linear inverse problems using pretrained diffusion models. RePS leverages conditioned ordinary differential equations (ODEs) compatible with any differentiable measurement model and incorporates a simplified restart mechanism that mitigates accumulated errors in iterative sampling, improving sample quality. Crucially, RePS avoids backpropagation through the score network, significantly reducing computational expenses. Experimental results demonstrate that RePS achieves faster convergence rates and better reconstruction fidelity compared to existing diffusion-based baselines across diverse inverse problem scenarios. This makes it a versatile, efficient, and high-performance approach for posterior inference with diffusion models, expanding their applicability beyond linear measurement models. <div>
arXiv:2511.20705v1 Announce Type: new 
Abstract: Inverse problems are fundamental to science and engineering, where the goal is to infer an underlying signal or state from incomplete or noisy measurements. Recent approaches employ diffusion models as powerful implicit priors for such problems, owing to their ability to capture complex data distributions. However, existing diffusion-based methods for inverse problems often rely on strong approximations of the posterior distribution, require computationally expensive gradient backpropagation through the score network, or are restricted to linear measurement models.
  In this work, we propose Restart for Posterior Sampling (RePS), a general and efficient framework for solving both linear and non-linear inverse problems using pre-trained diffusion models. RePS builds on the idea of restart-based sampling, previously shown to improve sample quality in unconditional diffusion, and extends it to posterior inference. Our method employs a conditioned ODE applicable to any differentiable measurement model and introduces a simplified restart strategy that contracts accumulated approximation errors during sampling. Unlike some of the prior approaches, RePS avoids backpropagation through the score network, substantially reducing computational cost.
  We demonstrate that RePS achieves faster convergence and superior reconstruction quality compared to existing diffusion-based baselines across a range of inverse problems, including both linear and non-linear settings.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Slice Discovery in Large Language Models</title>
<link>https://arxiv.org/abs/2511.20713</link>
<guid>https://arxiv.org/abs/2511.20713</guid>
<content:encoded><![CDATA[
<div> Large Language Models, error slices, active slice discovery, toxicity classification, active learning<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) often have systematic errors concentrated in specific data subsets called error slices, such as difficulties in correctly identifying toxic comments related to certain demographics.<br />2. Identifying these error slices is essential for understanding model weaknesses and improving performance but presents significant challenges due to the need for extensive manual annotation.<br />3. The paper introduces and formalizes the concept of Active Slice Discovery, an approach that actively groups model errors likely belonging to the same slice while efficiently using limited annotator input to verify shared error patterns.<br />4. The study applies this method empirically to the task of discovering known human-defined slices within toxicity classification datasets.<br />5. Experimental results indicate that uncertainty-based active learning algorithms are particularly effective, achieving good accuracy with only 2-10% of the available slice membership information, and significantly outperform several baseline methods under various feature representations and active learning strategies. <div>
arXiv:2511.20713v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit systematic errors on specific subsets of data, known as error slices. For instance, a slice can correspond to a certain demographic, where a model does poorly in identifying toxic comments regarding that demographic. Identifying error slices is crucial to understanding and improving models, but it is also challenging. An appealing approach to reduce the amount of manual annotation required is to actively group errors that are likely to belong to the same slice, while using limited access to an annotator to verify whether the chosen samples share the same pattern of model mistake. In this paper, we formalize this approach as Active Slice Discovery and explore it empirically on a problem of discovering human-defined slices in toxicity classification. We examine the efficacy of active slice discovery under different choices of feature representations and active learning algorithms. On several slices, we find that uncertainty-based active learning algorithms are most effective, achieving competitive accuracy using 2-10% of the available slice membership information, while significantly outperforming baselines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training</title>
<link>https://arxiv.org/abs/2511.20718</link>
<guid>https://arxiv.org/abs/2511.20718</guid>
<content:encoded><![CDATA[
<div> Keywords: PPO, large language models, multi-turn dialogue, importance sampling, stabilization techniques<br /><br />Summary: Proximal Policy Optimization (PPO) is commonly used to train large language models (LLMs) for multi-turn dialogue and reasoning tasks at the token level but often suffers from instability and collapse. This instability primarily stems from two sources: token-level importance sampling misaligned with the natural turn-level granularity of these multi-turn environments, and inaccurate advantage estimation from off-policy samples where the critic is unreliable in evaluating certain state-action pairs, causing high-variance gradient updates. To tackle these issues, the paper proposes two complementary stabilization methods: turn-level importance sampling to better align optimization with the natural stage structure of multi-turn reasoning, and clipping-bias correction to normalize gradients by downweighting highly off-policy samples, reducing bias from clipped updates. These methods lead to three PPO variants: Turn-PPO (only turn-level sampling), S-PPO (clipping-bias correction with token-level PPO), and ST-PPO (both turn-level sampling and clipping-bias correction combined). Experiments conducted on multi-turn search tasks in general QA, multi-hop QA, and medical multiple-choice benchmarks demonstrate that ST-PPO and S-PPO consistently prevent training collapse, maintain lower clipping ratios during optimization, and outperform standard token-level PPO in task performance. Overall, integrating turn-level importance sampling with clipping-bias correction offers a practical and scalable means of stabilizing multi-turn LLM agent training. <div>
arXiv:2511.20718v1 Announce Type: new 
Abstract: PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient Descent Algorithm Survey</title>
<link>https://arxiv.org/abs/2511.20725</link>
<guid>https://arxiv.org/abs/2511.20725</guid>
<content:encoded><![CDATA[
<div> Keywords: optimization algorithms, deep learning, SGD, Adam, Lion  

<br /><br />Summary:  
This article addresses the practical configuration requirements of optimization algorithms widely used in deep learning. It focuses on five major algorithms: SGD, Mini-batch SGD, Momentum, Adam, and Lion, analyzing their fundamental strengths and weaknesses. The study provides detailed practical recommendations for each algorithm, emphasizing how to select and tune parameters effectively to maximize performance. By offering a systematic comparison, the article aims to establish a standardized reference that supports both academic research and engineering applications. It highlights the importance of understanding algorithmic nuances to tackle optimization challenges across various model sizes and training scenarios. The research contributes to improved decision-making when choosing appropriate optimization strategies to enhance training efficiency and model accuracy. <div>
arXiv:2511.20725v1 Announce Type: new 
Abstract: Focusing on the practical configuration needs of optimization algorithms in deep learning, this article concentrates on five major algorithms: SGD, Mini-batch SGD, Momentum, Adam, and Lion. It systematically analyzes the core advantages, limitations, and key practical recommendations of each algorithm. The research aims to gain an in-depth understanding of these algorithms and provide a standardized reference for the reasonable selection, parameter tuning, and performance improvement of optimization algorithms in both academic research and engineering practice, helping to solve optimization challenges in different scales of models and various training scenarios.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge</title>
<link>https://arxiv.org/abs/2511.20726</link>
<guid>https://arxiv.org/abs/2511.20726</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, scenario generation, conditional variational autoencoder, large language model, safety validation<br /><br />Summary:  
This paper addresses key challenges in autonomous driving related to rare long-tail events and complex multi-agent interactions, which are underrepresented in real-world data but crucial for robust safety validation. The authors propose a high-fidelity scenario generation framework that combines a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE leverages historical trajectories and map data from naturalistic datasets to learn latent traffic structures, enabling the creation of physically consistent base scenarios. The LLM enhances this by acting as an adversarial reasoning engine, transforming unstructured scene descriptions into domain-specific loss functions to guide scenario generation dynamically across various risk levels. This approach integrates knowledge-driven optimization to carefully balance scenario realism and controllability, ensuring plausibility while emphasizing risk sensitivity. Extensive evaluations in CARLA and SMARTS simulation environments reveal that the framework significantly increases coverage of high-risk and rare events, aligns simulated traffic distributions more closely with real-world data, and exposes autonomous driving systems to far more challenging multi-agent interactions compared to existing rule-based or purely data-driven methods. The results demonstrate a novel and effective pathway for safety validation, enabling principled stress-testing of autonomous systems under rare yet critical scenarios. <div>
arXiv:2511.20726v1 Announce Type: new 
Abstract: Autonomous driving faces critical challenges in rare long-tail events and complex multi-agent interactions, which are scarce in real-world data yet essential for robust safety validation. This paper presents a high-fidelity scenario generation framework that integrates a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE encodes historical trajectories and map information from large-scale naturalistic datasets to learn latent traffic structures, enabling the generation of physically consistent base scenarios. Building on this, the LLM acts as an adversarial reasoning engine, parsing unstructured scene descriptions into domain-specific loss functions and dynamically guiding scenario generation across varying risk levels. This knowledge-driven optimization balances realism with controllability, ensuring that generated scenarios remain both plausible and risk-sensitive. Extensive experiments in CARLA and SMARTS demonstrate that our framework substantially increases the coverage of high-risk and long-tail events, improves consistency between simulated and real-world traffic distributions, and exposes autonomous driving systems to interactions that are significantly more challenging than those produced by existing rule- or data-driven methods. These results establish a new pathway for safety validation, enabling principled stress-testing of autonomous systems under rare but consequential events.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-Temporal Trajectory Foundation Model - Recent Advances and Future Directions</title>
<link>https://arxiv.org/abs/2511.20729</link>
<guid>https://arxiv.org/abs/2511.20729</guid>
<content:encoded><![CDATA[
<div> Keywords: Foundation models, spatio-temporal foundation models, trajectory foundation models, adaptability, knowledge discovery<br /><br />Summary:<br /><br />This article focuses on the emerging field of trajectory foundation models (TFMs), a specialized subset of spatio-temporal foundation models (STFMs) inspired by the success of large language models and foundation models in general. First, it outlines the significance of foundation models in enabling diverse data analytics and knowledge discovery across various scientific disciplines. Second, it acknowledges the recent trend toward developing STFMs to enhance adaptability and generalization for spatio-temporal tasks. Third, it identifies a notable gap in the literature—specifically a lack of systematic study regarding TFMs despite their importance. Fourth, the tutorial presented aims to fill this gap by providing a comprehensive overview of recent advances, including a taxonomy of existing TFM methodologies accompanied by a critical assessment of their respective strengths and limitations. Lastly, the article highlights several unresolved challenges and proposes promising future research directions to foster the development of robust, responsible, and transferable TFMs that contribute toward the broader goal of spatio-temporal general intelligence. <div>
arXiv:2511.20729v1 Announce Type: new 
Abstract: Foundation models (FMs) have emerged as a powerful paradigm, enabling a diverse range of data analytics and knowledge discovery tasks across scientific fields. Inspired by the success of FMs, particularly large language models, researchers have recently begun to explore spatio-temporal foundation models (STFMs) to improve adaptability and generalization across a wide spectrum of spatio-temporal (ST) tasks. Despite rapid progress, a systematic investigation of trajectory foundation models (TFMs), a crucial subclass of STFMs, is largely lacking. This tutorial addresses this gap by offering a comprehensive overview of recent advances in TFMs, including a taxonomy of existing methodologies and a critical analysis of their strengths and limitations. In addition, the tutorial highlights open challenges and outlines promising research directions to advance spatio-temporal general intelligence through the development of robust, responsible, and transferable TFMs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHiQPM: Calibrated Hierarchical Interpretable Image Classification</title>
<link>https://arxiv.org/abs/2511.20779</link>
<guid>https://arxiv.org/abs/2511.20779</guid>
<content:encoded><![CDATA[
<div> Interpretability, Hierarchical explanations, Conformal prediction, Human-AI complementarity, Calibrated prediction  
<br /><br />Summary:  
This paper introduces the Calibrated Hierarchical QPM (CHiQPM), a novel model designed to provide both global and local interpretability crucial for trustworthy AI, especially in safety-critical applications. CHiQPM enhances global interpretability through contrastive explanations that address the majority of classes, making the interpretive process more comprehensive. Additionally, it offers hierarchical explanations that mirror human reasoning processes, allowing users to explore explanations at multiple levels of detail. This hierarchical structure enables built-in conformal prediction (CP), producing calibrated set predictions that are interpretable and coherent. The model maintains state-of-the-art point prediction accuracy, achieving 99% of the accuracy of comparable non-interpretable models, thus demonstrating that interpretability does not come at the cost of performance. Moreover, CHiQPM’s calibrated set predictions are computationally efficient and competitive with existing CP methods, while uniquely providing interpretable explanations alongside these predictions. Overall, CHiQPM paves the way for enhanced human-AI collaboration by combining superior accuracy, interpretability, and calibrated decision making in a unified framework. <div>
arXiv:2511.20779v1 Announce Type: new 
Abstract: Globally interpretable models are a promising approach for trustworthy AI in safety-critical domains. Alongside global explanations, detailed local explanations are a crucial complement to effectively support human experts during inference. This work proposes the Calibrated Hierarchical QPM (CHiQPM) which offers uniquely comprehensive global and local interpretability, paving the way for human-AI complementarity. CHiQPM achieves superior global interpretability by contrastively explaining the majority of classes and offers novel hierarchical explanations that are more similar to how humans reason and can be traversed to offer a built-in interpretable Conformal prediction (CP) method. Our comprehensive evaluation shows that CHiQPM achieves state-of-the-art accuracy as a point predictor, maintaining 99% accuracy of non-interpretable models. This demonstrates a substantial improvement, where interpretability is incorporated without sacrificing overall accuracy. Furthermore, its calibrated set prediction is competitively efficient to other CP methods, while providing interpretable predictions of coherent sets along its hierarchical explanation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model</title>
<link>https://arxiv.org/abs/2511.20798</link>
<guid>https://arxiv.org/abs/2511.20798</guid>
<content:encoded><![CDATA[
<div> Keywords: mechanistic interpretability, large language models, foundation models, physics simulations, concept directions<br /><br />Summary:<br /><br />1. The paper explores whether the development of internal, human-understandable abstract representations is unique to models trained on structured data like language and images, or if it also appears in scientific foundation models focused on physics.<br /><br />2. The authors analyze a large physics-focused foundation model by extracting activation vectors during forward passes over simulation datasets representing different physical regimes.<br /><br />3. They compute "delta" representations by subtracting activations between two different regimes, treating these as concept directions encoding specific physical features within the model's activation space.<br /><br />4. By injecting these concept directions into the model during inference, the researchers demonstrate the ability to steer the model’s predictions causally, effectively inducing or removing particular physical features in simulations.<br /><br />5. The findings suggest that scientific foundation models develop generalized representations of physical principles beyond superficial correlations, offering new possibilities for understanding, controlling such models, and advancing AI-enabled scientific discovery. <div>
arXiv:2511.20798v1 Announce Type: new 
Abstract: Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute "delta" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Safety Monitoring for Flight Testing: A Case Study in Data-Driven Safety Learning</title>
<link>https://arxiv.org/abs/2511.20811</link>
<guid>https://arxiv.org/abs/2511.20811</guid>
<content:encoded><![CDATA[
<div> Keywords: runtime safety monitoring, flight testing, stochastic trajectory simulation, nearest neighbor classification, conformal prediction  

<br /><br />Summary:  
This paper introduces a data-driven framework for runtime safety monitoring tailored for flight testing scenarios, where aircraft operate with uncertain parameters. The core challenge addressed is the unexpected emergence of safety violations due to parameter uncertainties during pilot maneuvers, necessitating reliable, early-warning criteria for pilots to abort risky maneuvers before violations occur. The proposed solution leverages offline stochastic trajectory simulations to build a calibrated statistical model that assesses short-term safety risk. The approach is demonstrated through a flight testing example, highlighting complexities due to safety risks, uncertainties, and human interaction. The methodology incorporates three key components: (1) a model that predicts the aircraft's future state based on recent observations, (2) a nearest neighbor classifier that evaluates the predicted state's safety, and (3) calibration of the classifier’s confidence using conformal prediction to provide reliable probabilistic guarantees. Experimental evaluation on a flight dynamics model with uncertain parameters proves the method’s effectiveness in accurately identifying unsafe conditions and surpassing baseline methods in risk classification. Furthermore, the approach aligns with theoretical safety guarantees, emphasizing its potential utility beyond flight testing to other dynamic systems requiring proactive safety monitoring. <div>
arXiv:2511.20811v1 Announce Type: new 
Abstract: We develop a data-driven approach for runtime safety monitoring in flight testing, where pilots perform maneuvers on aircraft with uncertain parameters. Because safety violations can arise unexpectedly as a result of these uncertainties, pilots need clear, preemptive criteria to abort the maneuver in advance of safety violation. To solve this problem, we use offline stochastic trajectory simulation to learn a calibrated statistical model of the short-term safety risk facing pilots. We use flight testing as a motivating example for data-driven learning/monitoring of safety due to its inherent safety risk, uncertainty, and human-interaction. However, our approach consists of three broadly-applicable components: a model to predict future state from recent observations, a nearest neighbor model to classify the safety of the predicted state, and classifier calibration via conformal prediction. We evaluate our method on a flight dynamics model with uncertain parameters, demonstrating its ability to reliably identify unsafe scenarios, match theoretical guarantees, and outperform baseline approaches in preemptive classification of risk.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effects of Initialization Biases on Deep Neural Network Training Dynamics</title>
<link>https://arxiv.org/abs/2511.20826</link>
<guid>https://arxiv.org/abs/2511.20826</guid>
<content:encoded><![CDATA[
<div> Initial Guessing Bias, neural networks, loss functions, training dynamics, robust learning<br /><br />Summary:<br /><br />This paper investigates a phenomenon termed Initial Guessing Bias (IGB) in large neural networks just after random initialization. IGB leads models to favor a small subset of classes with high predicted probabilities, while assigning near-zero probability to others. This bias notably influences early training dynamics, a critical phase when the model is adapting to the coarse data structure. The authors emphasize that the choice of loss function has a significant impact on how these early dynamics unfold. Specifically, two recent loss functions, Blurry loss and Piecewise-zero loss, which were initially designed to improve robustness against label errors, can fail to effectively guide training direction in the presence of Initial Guessing Bias. Experimental results demonstrate that loss functions differ dramatically in their ability to manage early training behavior under IGB. Overall, the study highlights the importance of carefully considering the interaction between Initial Guessing Bias and training components, especially loss functions, to ensure stable and effective early neural network training. This understanding could inform better design of training protocols and loss functions that mitigate the adverse impact of Initial Guessing Bias on model performance. <div>
arXiv:2511.20826v1 Announce Type: new 
Abstract: Untrained large neural networks, just after random initialization, tend to favour a small subset of classes, assigning high predicted probabilities to these few classes and approximately zero probability to all others. This bias, termed Initial Guessing Bias, affects the early training dynamics, when the model is fitting to the coarse structure of the data. The choice of loss function against which to train the model has a large impact on how these early dynamics play out. Two recent loss functions, Blurry and Piecewise-zero loss, were designed for robustness to label errors but can become unable to steer the direction of training when exposed to this initial bias. Results indicate that the choice of loss function has a dramatic effect on the early phase training of networks, and highlights the need for careful consideration of how Initial Guessing Bias may interact with various components of the training scheme.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoregressive Surrogate Modeling of the Solar Wind with Spherical Fourier Neural Operator</title>
<link>https://arxiv.org/abs/2511.20830</link>
<guid>https://arxiv.org/abs/2511.20830</guid>
<content:encoded><![CDATA[
<div> Keywords: solar wind, machine learning, Spherical Fourier Neural Operator, space weather, surrogate modeling  

<br /><br />Summary:  
1. The solar wind is a continuous flow of charged particles from the Sun's corona that affects the heliosphere and space systems near Earth, making accurate modeling critical for space weather forecasting.  
2. Traditional three-dimensional magnetohydrodynamic (MHD) models, while accurate, are computationally expensive and thus not well-suited for rapid exploration of uncertainties in boundary conditions.  
3. This study introduces the first autoregressive machine learning proxy model for predicting steady-state solar wind radial velocity using the Spherical Fourier Neural Operator (SFNO).  
4. The proposed method predicts the solar wind velocity over a limited radial range and iteratively propagates the solution outward, which enhances accuracy in regions farther from the Sun compared to a one-step prediction approach.  
5. Benchmarking against the numerical Heliospheric Upwind eXtrapolation (HUX) surrogate shows that SFNO offers superior or comparable performance while being more flexible, trainable, and purely data-driven.  
6. This work establishes a new methodology for high-fidelity solar wind modeling, enabling faster and more adaptable predictions essential for space weather applications.  
7. The authors have made the source code and additional visualization results publicly available at their GitHub repository, facilitating further research and development in this area. <div>
arXiv:2511.20830v1 Announce Type: new 
Abstract: The solar wind, a continuous outflow of charged particles from the Sun's corona, shapes the heliosphere and impacts space systems near Earth. Accurate prediction of features such as high-speed streams and coronal mass ejections is critical for space weather forecasting, but traditional three-dimensional magnetohydrodynamic (MHD) models are computationally expensive, limiting rapid exploration of boundary condition uncertainties. We introduce the first autoregressive machine learning surrogate for steady-state solar wind radial velocity using the Spherical Fourier Neural Operator (SFNO). By predicting a limited radial range and iteratively propagating the solution outward, the model improves accuracy in distant regions compared to a single-step approach. Compared with the numerical HUX surrogate, SFNO demonstrates superior or comparable performance while providing a flexible, trainable, and data-driven alternative, establishing a novel methodology for high-fidelity solar wind modeling. The source code and additional visual results are available at https://github.com/rezmansouri/solarwind-sfno-velocity-autoregressive.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Primal: A Unified Deterministic Framework for Quasi-Orthogonal Hashing and Manifold Learning</title>
<link>https://arxiv.org/abs/2511.20839</link>
<guid>https://arxiv.org/abs/2511.20839</guid>
<content:encoded><![CDATA[
<div> Keywords: deterministic feature mapping, prime square roots, StaticPrime, DynamicPrime, kernel map<br /><br />Summary:  
This paper introduces Primal, a deterministic feature mapping framework leveraging number-theoretic properties of prime square roots to produce robust and tunable vector representations. Unlike stochastic approaches like Random Fourier Features, Primal exploits the Besicovitch property to generate irrational frequency modulations, ensuring infinite non-repeating phase trajectories. The authors propose two algorithmic variants: (1) StaticPrime, which generates temporal position encodings approaching the Welch bound for quasi-orthogonality, enhancing orthogonal sequence design, and (2) DynamicPrime, a tunable projection layer adapting feature mappings based on input, controlled by a scaling parameter σ. Notably, DynamicPrime unifies two mathematical utility classes: at low-frequency settings, it acts as an isometric kernel map that linearizes complex, non-convex geometries (such as spirals), facilitating tasks like signal reconstruction and compressive sensing; at high-frequency settings, it induces chaotic phase wrapping, functioning as a maximum-entropy one-way hash beneficial for Hyperdimensional Computing and privacy-preserving Split Learning. Empirical results demonstrate that Primal outperforms normalized Gaussian random projections in orthogonality preservation and distribution tightness, making it both computationally efficient and mathematically rigorous. The framework’s code implementation is publicly accessible on GitHub for reproduction and further research. <div>
arXiv:2511.20839v1 Announce Type: new 
Abstract: We present Primal, a deterministic feature mapping framework that harnesses the number-theoretic independence of prime square roots to construct robust, tunable vector representations. Diverging from standard stochastic projections (e.g., Random Fourier Features), our method exploits the Besicovitch property to create irrational frequency modulations that guarantee infinite non-repeating phase trajectories. We formalize two distinct algorithmic variants: (1) StaticPrime, a sequence generation method that produces temporal position encodings empirically approaching the theoretical Welch bound for quasi-orthogonality; and (2) DynamicPrime, a tunable projection layer for input-dependent feature mapping. A central novelty of the dynamic framework is its ability to unify two disparate mathematical utility classes through a single scaling parameter {\sigma}. In the low-frequency regime, the method acts as an isometric kernel map, effectively linearizing non-convex geometries (e.g., spirals) to enable high-fidelity signal reconstruction and compressive sensing. Conversely, the high-frequency regime induces chaotic phase wrapping, transforming the projection into a maximum-entropy one-way hash suitable for Hyperdimensional Computing and privacy-preserving Split Learning. Empirical evaluations demonstrate that our framework yields superior orthogonality retention and distribution tightness compared to normalized Gaussian baselines, establishing it as a computationally efficient, mathematically rigorous alternative to random matrix projections. The code is available at https://github.com/VladimerKhasia/primal
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-train to Gain: Robust Learning Without Clean Labels</title>
<link>https://arxiv.org/abs/2511.20844</link>
<guid>https://arxiv.org/abs/2511.20844</guid>
<content:encoded><![CDATA[
<div> Noisy labels, self-supervised learning, feature extractor, CIFAR-10, label-error detection<br /><br />Summary:<br /><br />This paper addresses the challenge of training deep neural networks in the presence of noisy labels, which typically causes poor model generalization due to overfitting on incorrect labels. The authors propose a robust training pipeline that eliminates the need for a clean subset of labeled data. The method involves pre-training a feature extractor backbone using self-supervised learning (SSL) techniques such as SimCLR and Barlow Twins, which do not require any labels. After this unlabeled pre-training, the model undergoes supervised training on the noisy labeled dataset. Experiments conducted on CIFAR-10 and CIFAR-100 datasets under both synthetic and real-world noise scenarios demonstrate consistent improvements in classification accuracy across all noise levels. Additionally, the SSL pre-training enhances downstream label-error detection capabilities, achieving higher F1 scores and balanced accuracy. The benefits of the approach become more pronounced as the noise rate increases, indicating its superior noise robustness. Remarkably, the proposed method matches the performance of models pre-trained on large curated datasets like ImageNet when noise levels are low, and significantly outperforms them under high noise conditions. This shows its promise as a practical solution for noisy label scenarios without relying on clean data subsets. <div>
arXiv:2511.20844v1 Announce Type: new 
Abstract: Training deep networks with noisy labels leads to poor generalization and degraded accuracy due to overfitting to label noise. Existing approaches for learning with noisy labels often rely on the availability of a clean subset of data. By pre-training a feature extractor backbone without labels using self-supervised learning (SSL), followed by standard supervised training on the noisy dataset, we can train a more noise robust model without requiring a subset with clean labels. We evaluate the use of SimCLR and Barlow~Twins as SSL methods on CIFAR-10 and CIFAR-100 under synthetic and real world noise. Across all noise rates, self-supervised pre-training consistently improves classification accuracy and enhances downstream label-error detection (F1 and Balanced Accuracy). The performance gap widens as the noise rate increases, demonstrating improved robustness. Notably, our approach achieves comparable results to ImageNet pre-trained models at low noise levels, while substantially outperforming them under high noise conditions.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selecting Belief-State Approximations in Simulators with Latent States</title>
<link>https://arxiv.org/abs/2511.20870</link>
<guid>https://arxiv.org/abs/2511.20870</guid>
<content:encoded><![CDATA[
<div> keywords: state resetting, simulators, belief state, conditional distribution selection, sampling algorithms<br /><br />Summary: State resetting in simulators is a crucial feature that enables sample-based planning and calibration using real data by resetting to previously encountered states. When simulators contain latent variables, resetting requires sampling from the posterior distribution of the latent state given observable history, known as the belief state, which is often difficult to perform exactly. The paper reduces this challenge to a general conditional distribution-selection problem and proposes a new algorithm with theoretical analysis under sampling-only access. Two main formulations for belief-state selection are presented: latent state-based selection, which targets the latent state distribution directly, and observation-based selection, which aims at the induced distribution over observations. These formulations differ in how their guarantees work with roll-out methods; notably, observation-based selection may fail under the common Single-Reset roll-out but has guarantees with the less conventional Repeated-Reset approach. The work also discusses practical concerns such as distribution shifts and the influence of sampling policies. Overall, this study uncovers a complex landscape of algorithmic strategies, theoretical subtleties, and open research questions in state resetting for simulators that was previously overlooked. <div>
arXiv:2511.20870v1 Announce Type: new 
Abstract: State resetting is a fundamental but often overlooked capability of simulators. It supports sample-based planning by allowing resets to previously encountered simulation states, and enables calibration of simulators using real data by resetting to states observed in real-system traces. While often taken for granted, state resetting in complex simulators can be nontrivial: when the simulator comes with latent variables (states), state resetting requires sampling from the posterior over the latent state given the observable history, a.k.a. the belief state (Silver and Veness, 2010). While exact sampling is often infeasible, many approximate belief-state samplers can be constructed, raising the question of how to select among them using only sampling access to the simulator.
  In this paper, we show that this problem reduces to a general conditional distribution-selection task and develop a new algorithm and analysis under sampling-only access. Building on this reduction, the belief-state selection problem admits two different formulations: latent state-based selection, which directly targets the conditional distribution of the latent state, and observation-based selection, which targets the induced distribution over the observation. Interestingly, these formulations differ in how their guarantees interact with the downstream roll-out methods: perhaps surprisingly, observation-based selection may fail under the most natural roll-out method (which we call Single-Reset) but enjoys guarantees under the less conventional alternative (which we call Repeated-Reset). Together with discussion on issues such as distribution shift and the choice of sampling policies, our paper reveals a rich landscape of algorithmic choices, theoretical nuances, and open questions, in this seemingly simple problem.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Integrity in Temporal Graph Learning Methods</title>
<link>https://arxiv.org/abs/2511.20873</link>
<guid>https://arxiv.org/abs/2511.20873</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic graphs, representation integrity, embedding evaluation, synthetic scenarios, neural models<br /><br />Summary: This paper addresses the challenge of evaluating dynamic graph embeddings beyond task-specific scores by introducing the concept of representation integrity, which measures how faithfully embeddings reflect changes in the underlying network topology. The authors formalize this notion and develop a family of indexes designed to quantify the alignment between graph changes and the corresponding embedding variations. They test these indexes using three synthetic scenarios—Gradual Merge, Abrupt Move, and Periodic Re-wiring—across forty-two candidates to identify a single recommended metric that meets both theoretical and empirical criteria. This chosen metric consistently ranks the UASE and IPP models as the most stable, confirming their robustness. The study then applies this metric to compare common dynamic graph learning models, revealing that different neural architectures excel in scenario-specific ways. Moreover, the integrity index shows a strong positive correlation with one-step link-prediction AUC, linking embedding quality with predictive performance. Overall, the proposed representation integrity framework provides a task-agnostic, interpretable evaluation tool for dynamic graph embeddings, enabling clearer guidance for model selection and informing future dynamic graph learning architecture design. <div>
arXiv:2511.20873v1 Announce Type: new 
Abstract: Real-world systems ranging from airline routes to cryptocurrency transfers are naturally modelled as dynamic graphs whose topology changes over time. Conventional benchmarks judge dynamic-graph learners by a handful of task-specific scores, yet seldom ask whether the embeddings themselves remain a truthful, interpretable reflection of the evolving network. We formalize this requirement as representation integrity and derive a family of indexes that measure how closely embedding changes follow graph changes. Three synthetic scenarios, Gradual Merge, Abrupt Move, and Periodic Re-wiring, are used to screen forty-two candidate indexes. Based on which we recommend one index that passes all of our theoretical and empirical tests. In particular, this validated metric consistently ranks the provably stable UASE and IPP models highest. We then use this index to do a comparative study on representation integrity of common dynamic graph learning models. This study exposes the scenario-specific strengths of neural methods, and shows a strong positive rank correlation with one-step link-prediction AUC. The proposed integrity framework, therefore, offers a task-agnostic and interpretable evaluation tool for dynamic-graph representation quality, providing more explicit guidance for model selection and future architecture design.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Hash Embeddings for Online Learning of Categorical Features</title>
<link>https://arxiv.org/abs/2511.20893</link>
<guid>https://arxiv.org/abs/2511.20893</guid>
<content:encoded><![CDATA[
<div> Keywords: streaming data, categorical features, probabilistic hash embedding, online learning, Bayesian inference<br /><br />Summary:  
This paper addresses the challenge of handling streaming data with categorical features whose vocabulary dynamically changes and can grow without bound. Traditional feature hashing methods map categorical values to a fixed-size feature space but are typically designed for batch, offline settings. The authors demonstrate that deterministic embeddings used in online learning are sensitive to the order in which categories arrive and suffer from forgetting, resulting in performance decline. To overcome these limitations, they propose a novel probabilistic hash embedding (PHE) model that treats embeddings as stochastic and incorporates Bayesian online learning to update embeddings incrementally. The paper derives a scalable inference algorithm that efficiently learns model parameters and updates posterior distributions of embeddings and latent variables. Key advantages of the approach include handling evolving vocabularies, adaptability to new items without forgetting previously seen ones, bounded memory requirements independent of the total distinct categories, and invariance to the sequence in which items arrive. Experimental results in classification, sequence modeling, and recommendation system tasks under online learning scenarios demonstrate that PHE outperforms baseline methods while maintaining memory efficiency—using only 2 to 4 times the memory of a one-hot embedding table. Supplementary materials and code are available on GitHub for reproducibility and further investigation. <div>
arXiv:2511.20893v1 Announce Type: new 
Abstract: We study streaming data with categorical features where the vocabulary of categorical feature values is changing and can even grow unboundedly over time. Feature hashing is commonly used as a pre-processing step to map these categorical values into a feature space of fixed size before learning their embeddings. While these methods have been developed and evaluated for offline or batch settings, in this paper we consider online settings. We show that deterministic embeddings are sensitive to the arrival order of categories and suffer from forgetting in online learning, leading to performance deterioration. To mitigate this issue, we propose a probabilistic hash embedding (PHE) model that treats hash embeddings as stochastic and applies Bayesian online learning to learn incrementally from data. Based on the structure of PHE, we derive a scalable inference algorithm to learn model parameters and infer/update the posteriors of hash embeddings and other latent variables. Our algorithm (i) can handle an evolving vocabulary of categorical items, (ii) is adaptive to new items without forgetting old items, (iii) is implementable with a bounded set of parameters that does not grow with the number of distinct observed values on the stream, and (iv) is invariant to the item arrival order. Experiments in classification, sequence modeling, and recommendation systems in online learning setups demonstrate the superior performance of PHE while maintaining high memory efficiency (consumes as low as 2~4 memory of a one-hot embedding table). Supplementary materials are at https://github.com/aodongli/probabilistic-hash-embeddings
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolved SampleWeights for Bias Mitigation: Effectiveness Depends on Optimization Objectives</title>
<link>https://arxiv.org/abs/2511.20909</link>
<guid>https://arxiv.org/abs/2511.20909</guid>
<content:encoded><![CDATA[
<div> Bias mitigation, reweighting, genetic algorithm, fairness metrics, predictive performance<br /><br />Summary:<br />1. This paper addresses the issue of biased predictions in machine learning models trained on real-world data, which may harm marginalized communities.  
2. The study focuses on reweighting methods that assign different weights to data points during training to reduce bias.  
3. Three weighting strategies are compared: evolved weights using a Genetic Algorithm (GA), weights based only on dataset characteristics, and uniform equal weights for all data points.  
4. Model performance is evaluated using two predictive metrics—accuracy and area under the ROC curve—and two fairness metrics—demographic parity difference and subgroup false negative fairness. These metrics also serve as the optimization objectives for the GA.  
5. Experiments conducted on eleven publicly available datasets, including two medical datasets, demonstrate that GA-evolved weights can improve trade-offs between fairness and predictive performance compared to the other weighting methods.  
6. The benefits of evolved weights are highly dependent on which optimization objectives are chosen during the GA process.  
7. Optimizing specifically for accuracy and demographic parity difference yielded the most datasets showing significant improvement in both fairness and predictive performance when using evolved weights. <div>
arXiv:2511.20909v1 Announce Type: new 
Abstract: Machine learning models trained on real-world data may inadvertently make biased predictions that negatively impact marginalized communities. Reweighting is a method that can mitigate such bias in model predictions by assigning a weight to each data point used during model training. In this paper, we compare three methods for generating these weights: (1) evolving them using a Genetic Algorithm (GA), (2) computing them using only dataset characteristics, and (3) assigning equal weights to all data points. Model performance under each strategy was evaluated using paired predictive and fairness metrics, which also served as optimization objectives for the GA during evolution. Specifically, we used two predictive metrics (accuracy and area under the Receiver Operating Characteristic curve) and two fairness metrics (demographic parity difference and subgroup false negative fairness). Using experiments on eleven publicly available datasets (including two medical datasets), we show that evolved sample weights can produce models that achieve better trade-offs between fairness and predictive performance than alternative weighting methods. However, the magnitude of these benefits depends strongly on the choice of optimization objectives. Our experiments reveal that optimizing with accuracy and demographic parity difference metrics yields the largest number of datasets for which evolved weights are significantly better than other weighting strategies in optimizing both objectives.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Time-Step Size in Reinforcement Learning for Sepsis Treatment</title>
<link>https://arxiv.org/abs/2511.20913</link>
<guid>https://arxiv.org/abs/2511.20913</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, sepsis management, time-step size, offline RL, policy evaluation  

<br /><br />Summary:  
This study investigates the impact of different time-step sizes on reinforcement learning (RL) for sepsis management, a crucial healthcare application. Most previous RL approaches have used a fixed 4-hour time-step aggregation for patient data, which may oversimplify patient dynamics and affect policy quality. To address this, the authors empirically compare four time-step sizes: 1, 2, 4, and 8 hours, maintaining an identical offline RL pipeline for fairness. They develop action re-mapping techniques that enable evaluation of policies trained and tested at differing time intervals. Cross-time-step model selection is conducted under two distinct policy learning scenarios. The primary focus is to determine how time-step granularity influences state representation learning, behavior cloning, policy training, and off-policy evaluation performances. Results indicate that the optimal time-step size depends on the learning setup, but policies trained with finer time resolutions (1 and 2 hours) combined with a static behavior policy show superior performance and greater stability overall. This research underscores time-step size as a vital design parameter in offline RL for healthcare, challenging the conventional 4-hour standard and advocating for exploring shorter intervals to better capture patient dynamics and improve treatment strategies. <div>
arXiv:2511.20913v1 Announce Type: new 
Abstract: Existing studies on reinforcement learning (RL) for sepsis management have mostly followed an established problem setup, in which patient data are aggregated into 4-hour time steps. Although concerns have been raised regarding the coarseness of this time-step size, which might distort patient dynamics and lead to suboptimal treatment policies, the extent to which this is a problem in practice remains unexplored. In this work, we conducted empirical experiments for a controlled comparison of four time-step sizes ($\Delta t\!=\!1,2,4,8$ h) on this domain, following an identical offline RL pipeline. To enable a fair comparison across time-step sizes, we designed action re-mapping methods that allow for evaluation of policies on datasets with different time-step sizes, and conducted cross-$\Delta t$ model selections under two policy learning setups. Our goal was to quantify how time-step size influences state representation learning, behavior cloning, policy training, and off-policy evaluation. Our results show that performance trends across $\Delta t$ vary as learning setups change, while policies learned at finer time-step sizes ($\Delta t = 1$ h and $2$ h) using a static behavior policy achieve the overall best performance and stability. Our work highlights time-step size as a core design choice in offline RL for healthcare and provides evidence supporting alternatives beyond the conventional 4-hour setup.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Operationalizing Quantized Disentanglement</title>
<link>https://arxiv.org/abs/2511.20927</link>
<guid>https://arxiv.org/abs/2511.20927</guid>
<content:encoded><![CDATA[
<div> Quantized factors, unsupervised identifiability, axis-aligned discontinuities, density cliffs, disentanglement

<br /><br />Summary:  
This work addresses the challenge of unsupervised identifiability of quantized latent factors by leveraging recent theoretical insights. The key assumption is that quantization thresholds manifest as axis-aligned discontinuities in the probability density function of latent factors. Building on this, the authors propose a practical criterion to encourage learned mappings to exhibit density profiles with such axis-aligned discontinuities, termed "cliffs." These cliffs represent sharp jumps in estimated factor densities. To capture the theoretical property of independent discontinuities, the method enforces that the location of these cliffs along one factor should remain independent of other factors' values. This approach, named Cliff, operationalizes the high-level theoretical principles into a usable framework for unsupervised disentanglement. Experimental results show that Cliff outperforms existing baseline methods consistently across all standard disentanglement benchmarks. This improvement underscores the effectiveness of incorporating axis-aligned, independent discontinuities as a guiding criterion in unsupervised latent factor disentanglement, particularly under nonlinear transformations. The study thus bridges a critical gap between theoretical identifiability results and practical unsupervised learning applications. <div>
arXiv:2511.20927v1 Announce Type: new 
Abstract: Recent theoretical work established the unsupervised identifiability of quantized factors under any diffeomorphism. The theory assumes that quantization thresholds correspond to axis-aligned discontinuities in the probability density of the latent factors. By constraining a learned map to have a density with axis-aligned discontinuities, we can recover the quantization of the factors. However, translating this high-level principle into an effective practical criterion remains challenging, especially under nonlinear maps. Here, we develop a criterion for unsupervised disentanglement by encouraging axis-aligned discontinuities. Discontinuities manifest as sharp changes in the estimated density of factors and form what we call cliffs. Following the definition of independent discontinuities from the theory, we encourage the location of the cliffs along a factor to be independent of the values of the other factors. We show that our method, Cliff, outperforms the baselines on all disentanglement benchmarks, demonstrating its effectiveness in unsupervised disentanglement.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection</title>
<link>https://arxiv.org/abs/2511.20944</link>
<guid>https://arxiv.org/abs/2511.20944</guid>
<content:encoded><![CDATA[
<div> Keywords: Business Email Compromise, Psycholinguistic Detection, DistilBERT, CatBoost, Cost-sensitive Learning  

<br /><br />Summary:  
This paper addresses Business Email Compromise (BEC), a high-impact social engineering threat causing over $2.9 billion in annual losses as reported by the 2024 FBI Internet Crime Report. It highlights the significant economic asymmetry between the costs of False Negatives (fraud losses) and False Positives (manual reviews), with fraud losses being approximately 5,480 times more costly. Two detection paradigms are studied: the Forensic Psycholinguistic Stream using CatBoost, which offers high interpretability, low latency (0.885 ms), and low computational cost; and the Semantic Stream using DistilBERT, a deep learning model providing superior accuracy (AUC = 1.0000, F1 = 0.9981) at a higher but acceptable latency (7.403 ms) on GPU infrastructure. The dataset employed is adversarially poisoned and generated with the novel Black Hole protocol, with evaluations performed on a Tesla T4 GPU. The results suggest that DistilBERT is ideal for organizations equipped with GPU resources due to superior detection performance, while CatBoost is more suitable for edge or cost-sensitive deployments due to significantly lower latency and resource consumption. Both approaches demonstrate remarkable return on investment exceeding 99.96%, achieved by optimizing cost-sensitive learning strategies to reduce costly False Negatives, thus mitigating financial damages effectively. <div>
arXiv:2511.20944v1 Announce Type: new 
Abstract: Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies and exploits psychological vulnerabilities, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual adjusted losses, presenting significant economic asymmetry: the cost of a False Negative (fraud loss) exceeds the cost of a False Positive (manual review) by orders of magnitude (approximately 1 to 5,480).
  This paper examines two detection paradigms for BEC: the Forensic Psycholinguistic Stream, which utilizes CatBoost to analyze psycholinguistic cues with high interpretability and low latency, and the Semantic Stream, which employs DistilBERT for deep learning-based contextual language understanding, offering superior accuracy at higher computational cost. We evaluated DistilBERT on an adversarially poisoned dataset (N = 7,990) generated via our Black Hole protocol, benchmarked on Tesla T4 GPU infrastructure, achieving superior detection (AUC = 1.0000, F1 = 0.9981) with acceptable real-time latency (7.403 milliseconds). CatBoost achieves competitive detection (AUC = 0.9905, F1 = 0.9486) at 8.4x lower latency (0.885 milliseconds), consuming negligible computational resources. For organizations with GPU infrastructure, DistilBERT offers superior accuracy. CatBoost is preferable for edge deployments or cost-sensitive environments due to comparable security and lower operational costs. Both approaches demonstrate return on investment exceeding 99.96% when optimized through cost-sensitive learning, by significantly reducing false negatives and associated financial losses.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset Poisoning Attacks on Behavioral Cloning Policies</title>
<link>https://arxiv.org/abs/2511.20992</link>
<guid>https://arxiv.org/abs/2511.20992</guid>
<content:encoded><![CDATA[
<div> Behavior Cloning, backdoor attacks, dataset poisoning, robustness, entropy-based trigger<br /><br />Summary:<br /><br />1. This paper analyzes the vulnerability of Behavior Cloning (BC) policies to clean-label backdoor attacks, which inject visual triggers into expert demonstration datasets to create harmful spurious correlations.<br />2. The study investigates how the effectiveness of these backdoor attacks scales with key factors such as the fraction of poisoned data, trigger strength, and trigger type.<br />3. A novel entropy-based test-time trigger attack is introduced, targeting critical states during deployment to maximize the degradation of policy performance.<br />4. Empirical results reveal that even minimally poisoned datasets produce BC policies that maintain near-baseline task performance, masking significant susceptibility to backdoor triggers during real-world use.<br />5. The findings highlight an urgent need for research focused on improving the robustness of BC policies, especially as they are increasingly trained with large-scale datasets for deployment in cyber-physical systems.<br /><br />Additional resources including videos and code are provided at the authors' project site. <div>
arXiv:2511.20992v1 Announce Type: new 
Abstract: Behavior Cloning (BC) is a popular framework for training sequential decision policies from expert demonstrations via supervised learning. As these policies are increasingly being deployed in the real world, their robustness and potential vulnerabilities are an important concern. In this work, we perform the first analysis of the efficacy of clean-label backdoor attacks on BC policies. Our backdoor attacks poison a dataset of demonstrations by injecting a visual trigger to create a spurious correlation that can be exploited at test time. We evaluate how policy vulnerability scales with the fraction of poisoned data, the strength of the trigger, and the trigger type. We also introduce a novel entropy-based test-time trigger attack that substantially degrades policy performance by identifying critical states where test-time triggering of the backdoor is expected to be most effective at degrading performance. We empirically demonstrate that BC policies trained on even minimally poisoned datasets exhibit deceptively high, near-baseline task performance despite being highly vulnerable to backdoor trigger attacks during deployment. Our results underscore the urgent need for more research into the robustness of BC policies, particularly as large-scale datasets are increasingly used to train policies for real-world cyber-physical systems. Videos and code are available at https://sites.google.com/view/dataset-poisoning-in-bc.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.20993</link>
<guid>https://arxiv.org/abs/2511.20993</guid>
<content:encoded><![CDATA[
<div> Large language models, reinforcement learning, subgoal planning, multi-LLM pipeline, subgoal graph<br /><br />Summary:<br /><br />This paper addresses the challenge of aligning high-level task plans generated by large language models (LLMs) with executable actions in reinforcement learning (RL) environments. The authors identify two key limitations of single-LLM planning: the generation of semantically plausible but infeasible or contextually irrelevant subgoals due to lack of environment grounding, and unreliable self-verification that causes overconfident yet failing subgoals during execution. To overcome these issues, the paper presents the Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR) framework. This approach incorporates an environment-specific subgoal graph and structured entity knowledge to ground subgoal generation. It utilizes a multi-LLM planning pipeline that explicitly separates the roles of subgoal generation, critique, and refinement, improving the reliability and executability of subgoals. Additionally, a subgoal tracker monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain continuous alignment between planned subgoals and actions. Experimental validation on 22 diverse tasks within the open-world game "Crafter" demonstrates that SGA-ACR effectively improves planning-execution alignment and task success rates, highlighting the benefits of multi-LLM collaboration and environment-specific knowledge integration in RL planning. <div>
arXiv:2511.20993v1 Announce Type: new 
Abstract: Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game "Crafter" demonstrate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FANoise: Singular Value-Adaptive Noise Modulation for Robust Multimodal Representation Learning</title>
<link>https://arxiv.org/abs/2511.20997</link>
<guid>https://arxiv.org/abs/2511.20997</guid>
<content:encoded><![CDATA[
<div> Noise Injection, Representation Learning, Multimodal, FANoise, Contrastive Learning<br /><br />Summary:<br /><br />1. Representation learning is essential in modern machine learning, enabling key applications such as text retrieval and multimodal understanding. However, achieving robust and generalizable representations is still a challenge.<br /><br />2. Existing methods that improve encoding performance often use active noise injection as data augmentation, but these approaches typically rely on heuristic or static noise, which fails to account for the evolving feature distributions during training.<br /><br />3. This paper systematically investigates the role of noise in representation learning through both gradient-based and feature distribution viewpoints, utilizing the InfoNCE loss as a case study.<br /><br />4. The authors introduce FANoise, a novel feature-adaptive noise injection strategy tailored for multimodal representation learning, which dynamically adjusts noise based on the learning progress and feature distribution dynamics.<br /><br />5. Extensive experiments across various base vision-language models (VLMs) show that FANoise effectively balances the benefits of noise while reducing its drawbacks, consistently improving multimodal task performance under a solid theoretical framework. <div>
arXiv:2511.20997v1 Announce Type: new 
Abstract: Representation learning is fundamental to modern machine learning, powering applications such as text retrieval and multimodal understanding. However, learning robust and generalizable representations remains challenging. While prior work has demonstrated that active noise injection, a form of data augmentation, can enhance encoding performance, most existing methods rely on heuristic or static noise, overlooking the dynamic nature of feature distributions during training. In this work, we systematically study the role of noise in representation learning from both gradient-based and feature distribution perspectives, using InfoNCE loss as a representative example. Focusing on multimodal representation learning, we propose FANoise, a novel feature-adaptive noise injection strategy. By leveraging the dynamics of contrastive learning, FANoise effectively mitigates the negative impacts of noise while preserving its benefits. Under this theoretically grounded framework, comprehensive experiments demonstrate that FANoise consistently improves overall performance on multimodal tasks across various base VLM models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Ising Models in Total Variation Distance</title>
<link>https://arxiv.org/abs/2511.21008</link>
<guid>https://arxiv.org/abs/2511.21008</guid>
<content:encoded><![CDATA[
<div> Ising models, Total Variation distance, Maximum Pseudo-Likelihood Estimator, Modified Log-Sobolev Inequality, polynomial-time algorithms<br /><br />Summary:<br /><br />This article addresses the problem of estimating Ising models over \( n \) variables in Total Variation (TV) distance using \( l \) independent samples. While the statistical complexity is well understood, the challenge lies in designing algorithms that are both computationally and statistically efficient. The paper highlights previous advances in specific cases, such as tree-structured graphs, Gaussian-distributed interaction matrices, and models with eigenvalues concentrated in small intervals, yet notes the absence of a unified polynomial-time estimation framework in TV distance. The authors propose a unified analysis of the Maximum Pseudo-Likelihood Estimator (MPLE) that covers two broad classes of Ising models: (1) models with bounded operator norm satisfying the Modified Log-Sobolev Inequality (MLSI), which ensures certain properties of Glauber dynamics, and (2) models with interaction matrices having bounded infinity norm (bounded width), a common assumption in Ising model structure learning. Their results enable polynomial-time algorithms accompanied by optimal or nearly optimal sample complexity guarantees across various scenarios. The proof techniques integrate advanced tools such as tensorization inequalities, measure decompositions, and concentration bounds, thus providing a broad theoretical foundation for efficient Ising model estimation in the TV metric. <div>
arXiv:2511.21008v1 Announce Type: new 
Abstract: We consider the problem of estimating Ising models over $n$ variables in Total Variation (TV) distance, given $l$ independent samples from the model. While the statistical complexity of the problem is well-understood [DMR20], identifying computationally and statistically efficient algorithms has been challenging. In particular, remarkable progress has occurred in several settings, such as when the underlying graph is a tree [DP21, BGPV21], when the entries of the interaction matrix follow a Gaussian distribution [GM24, CK24], or when the bulk of its eigenvalues lie in a small interval [AJK+24, KLV24], but no unified framework for polynomial-time estimation in TV exists so far. Our main contribution is a unified analysis of the Maximum Pseudo-Likelihood Estimator (MPLE) for two general classes of Ising models. The first class includes models that have bounded operator norm and satisfy the Modified Log-Sobolev Inequality (MLSI), a functional inequality that was introduced to study the convergence of the associated Glauber dynamics to stationarity. In the second class of models, the interaction matrix has bounded infinity norm (or bounded width), which is the most common assumption in the literature for structure learning of Ising models. We show how our general results for these classes yield polynomial-time algorithms and optimal or near-optimal sample complexity guarantees in a variety of settings. Our proofs employ a variety of tools from tensorization inequalities to measure decompositions and concentration bounds.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChatGpt Content detection: A new approach using xlm-roberta alignment</title>
<link>https://arxiv.org/abs/2511.21009</link>
<guid>https://arxiv.org/abs/2511.21009</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated text detection, XLM-RoBERTa, perplexity, semantic features, academic integrity<br /><br />Summary:<br /><br />1. The paper addresses the growing need to distinguish AI-generated text from human-authored content, especially as generative AI tools like ChatGPT become more prevalent.<br /><br />2. The study focuses both on detecting text entirely generated by AI and on identifying human-written text that has been reworded or paraphrased by AI.<br /><br />3. The authors propose a comprehensive detection methodology utilizing XLM-RoBERTa, a state-of-the-art multilingual transformer model, incorporating careful preprocessing and feature extraction.<br /><br />4. Key features used in detection include perplexity, semantic analysis, and readability metrics, which help characterize differences between human and AI texts.<br /><br />5. The model was fine-tuned on a balanced dataset with both human and AI-generated samples, showing high accuracy and robustness across multiple text genres.<br /><br />6. Feature analysis indicated that perplexity measures and attention mechanisms within the transformer played crucial roles in model decisions.<br /><br />7. The work contributes to academic integrity efforts and AI ethics by providing tools that promote transparency and accountability in AI usage.<br /><br />8. The authors suggest future research directions, including testing more advanced models and expanding the dataset to improve the system's generalizability and robustness. <div>
arXiv:2511.21009v1 Announce Type: new 
Abstract: The challenge of separating AI-generated text from human-authored content is becoming more urgent as generative AI technologies like ChatGPT become more widely available. In this work, we address this issue by looking at both the detection of content that has been entirely generated by AI and the identification of human text that has been reworded by AI. In our work, a comprehensive methodology to detect AI- generated text using XLM-RoBERTa, a state-of-the-art multilingual transformer model. Our approach includes rigorous preprocessing, and feature extraction involving perplexity, semantic, and readability features. We fine-tuned the XLM-RoBERTa model on a balanced dataset of human and AI-generated texts and evaluated its performance. The model demonstrated high accuracy and robust performance across various text genres. Additionally, we conducted feature analysis to understand the model's decision-making process, revealing that perplexity and attention-based features are critical in differentiating between human and AI-generated texts. Our findings offer a valuable tool for maintaining academic integrity and contribute to the broader field of AI ethics by promoting transparency and accountability in AI systems. Future research directions include exploring other advanced models and expanding the dataset to enhance the model's generalizability.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.21011</link>
<guid>https://arxiv.org/abs/2511.21011</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, staggered resets, Proximal Policy Optimization, nonstationarity, parallel environments<br /><br />Summary:<br /><br />1. The paper addresses the challenge in massively parallel GPU-based reinforcement learning (RL) environments, where short rollouts and high update-to-data (UTD) ratios implemented with synchronized resets induce harmful nonstationarity, destabilizing training.<br /><br />2. To overcome this, the authors propose "staggered resets," a method where environment resets are spread at varied points within the task horizon rather than all at once, increasing temporal diversity in training batches.<br /><br />3. Through analysis on toy environments, the study demonstrates how staggered resets reduce the skew in the learning signal caused by synchronized rollouts, providing a more stable training process.<br /><br />4. Applying staggered resets to high-dimensional robotics tasks shows substantial improvements in sample efficiency, acceleration in wall-clock training convergence, and stronger final policy performance.<br /><br />5. Furthermore, staggered resets scale more effectively with an increasing number of parallel environments, outperforming naive synchronized rollout strategies and making it a practical solution for large-scale RL training setups. <div>
arXiv:2511.21011v1 Announce Type: new 
Abstract: Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression</title>
<link>https://arxiv.org/abs/2511.21016</link>
<guid>https://arxiv.org/abs/2511.21016</guid>
<content:encoded><![CDATA[
<div> Keywords: Gated KalmaNet, linear state-space models, Kalman filter, Chebyshev Iteration, adaptive regularization<br /><br />Summary:<br /><br />1. This paper introduces Gated KalmaNet (GKA), a novel layer designed to improve recall performance in sequence modeling by fully accounting for the entire past, unlike traditional linear state-space models (SSMs) which store only fading, lossy summaries.<br />2. GKA operates by solving an online ridge regression problem at test time with constant memory and linear computational cost relative to sequence length, leveraging iterative solutions inspired by the Kalman Filter.<br />3. The authors identify numerical instability and parallelization challenges of standard Kalman filter equations in low-precision hardware (e.g., bfloat16), and address them through two key innovations: an adaptive regularization method with input-dependent gating to control conditioning, and the adoption of Chebyshev Iteration as a more stable iterative solver.<br />4. To further enhance scalability and hardware efficiency, they develop a chunk-wise implementation of Chebyshev Iteration and custom backpropagation kernels that handle adaptive regularization and gating.<br />5. Empirically, GKA outperforms existing SSM-based layers like Mamba2 and GLA on short-context language tasks and achieves over 10% relative improvement in long-context real-world retrieval-augmented generation (RAG) and long question answering tasks with sequence lengths up to 128k tokens. <div>
arXiv:2511.21016v1 Announce Type: new 
Abstract: As efficient alternatives to softmax Attention, linear state-space models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall oriented tasks. We propose Gated KalmaNet (GKA), a layer that reduces this gap by accounting for the full past when predicting the next token, while maintaining SSM-style efficiency. GKA achieves this by solving an online ridge regression problem at test time, with constant memory and linear compute cost in the sequence length. Drawing inspiration from the Kalman Filter, we iteratively solve the online ridge regression problem. However, a critical insight is that standard Kalman filter equations are numerically unstable in low-precision environments (like bfloat16) and difficult to parallelize in modern hardware. We address both challenges through two key innovations: (1) an adaptive regularization strategy with input-dependent gating that controls the condition number of the ridge regression, ensuring numerical stability while balancing memory retention. And (2) the use of Chebyshev Iteration instead of other conventional iterative solvers, which we demonstrate to be more stable in low-precision settings. To further improve scalability, we develop a hardware-aware chunk-wise implementation of Chebyshev Iteration along with custom kernels for backpropagating through our adaptive regularization and gating mechanisms. Empirically, GKA shows strong language understanding capabilites on short-context tasks outperforming existing SSM layers (like Mamba2, GLA and Gated DeltaNet). On long-context, GKA excels at real-world RAG and LongQA tasks up to 128k tokens, achieving more than $10$% relative improvement over other fading memory baselines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Wildfire Spread Prediction Using an Autoregressive Conditional Generative Adversarial Network</title>
<link>https://arxiv.org/abs/2511.21019</link>
<guid>https://arxiv.org/abs/2511.21019</guid>
<content:encoded><![CDATA[
<div> Keywords: wildfire spread prediction, autoregressive CGAN, probabilistic forecasting, nonlinearity, wildfire perimeter delineation  

<br /><br />Summary:  
This study addresses the increasing need for rapid and accurate wildfire spread prediction driven by climate change. Traditional physics-based simulators like FARSITE provide high-fidelity results but are too slow for real-time applications, while existing deep learning approaches tend to produce overly smooth outputs that miss the complex nonlinear behavior of wildfires. To overcome these limitations, the authors propose an autoregressive conditional generative adversarial network (CGAN) framework designed for probabilistic wildfire spread prediction. This autoregressive approach enables the model to learn sequential state transitions, which helps maintain stability over long-term forecasts. Experimental evaluations demonstrate that the CGAN-based method outperforms conventional deep learning models in terms of overall accuracy and sharpness in delineating fire perimeters. The adversarial learning scheme allows the model to better capture the inherent uncertainty and intricate spatiotemporal dynamics of wildfire propagation rather than merely averaging pixel values. Furthermore, the autoregressive setup supports systematic temporal forecasting of wildfire evolution, improving both prediction precision and physical interpretability. Collectively, the proposed framework offers a promising tool for enhancing time-sensitive wildfire response and evacuation planning by delivering more reliable and realistic spread predictions. <div>
arXiv:2511.21019v1 Announce Type: new 
Abstract: Climate change has intensified the frequency and severity of wildfires, making rapid and accurate prediction of fire spread essential for effective mitigation and response. Physics-based simulators such as FARSITE offer high-fidelity predictions but are computationally intensive, limiting their applicability in real-time decision-making, while existing deep learning models often yield overly smooth predictions that fail to capture the complex, nonlinear dynamics of wildfire propagation. This study proposes an autoregressive conditional generative adversarial network (CGAN) for probabilistic wildfire spread prediction. By formulating the prediction task as an autoregressive problem, the model learns sequential state transitions, ensuring long-term prediction stability. Experimental results demonstrate that the proposed CGAN-based model outperforms conventional deep learning models in both overall predictive accuracy and boundary delineation of fire perimeters. These results demonstrate that adversarial learning allows the model to capture the strong nonlinearity and uncertainty of wildfire spread, instead of simply fitting the pixel average. Furthermore, the autoregressive framework facilitates systematic temporal forecasting of wildfire evolution. The proposed CGAN-based autoregressive framework enhances both the accuracy and physical interpretability of wildfire spread prediction, offering a promising foundation for time-sensitive response and evacuation planning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Probabilistic Framework for Temporal Distribution Generalization in Industry-Scale Recommender Systems</title>
<link>https://arxiv.org/abs/2511.21032</link>
<guid>https://arxiv.org/abs/2511.21032</guid>
<content:encoded><![CDATA[
<div> Temporal Distribution Shift, recommender systems, incremental learning, causal graph, self-supervised variational objective  

<br /><br />Summary:  
The paper addresses the challenge of temporal distribution shift (TDS) in recommender systems, which degrades long-term accuracy over time. Traditional industrial methods relying on periodic incremental training are inadequate for capturing both persistent and transient patterns in changing data. To overcome this, the authors propose ELBO$_\text{TDS}$, a novel probabilistic framework designed to fit into large-scale incremental learning workflows. First, through statistical analysis of real-world production data, they identify key shifting factors and create an effective data augmentation strategy that resamples these factors to broaden the training data distribution. Second, to capitalize on the extended training support while avoiding representation collapse, they represent the temporal recommendation setting with a causal graph and derive a self-supervised variational objective grounded in this causal structure. Experiments, both theoretical and empirical, show that ELBO$_\text{TDS}$ significantly improves temporal generalization and leads to a 2.33% increase in gross merchandise volume (GMV) per user. The method has been successfully implemented in Shopee Product Search, demonstrating practical impact. The authors also provide open access to their codebase to enable further research and application. <div>
arXiv:2511.21032v1 Announce Type: new 
Abstract: Temporal distribution shift (TDS) erodes the long-term accuracy of recommender systems, yet industrial practice still relies on periodic incremental training, which struggles to capture both stable and transient patterns. Existing approaches such as invariant learning and self-supervised learning offer partial solutions but often suffer from unstable temporal generalization, representation collapse, or inefficient data utilization. To address these limitations, we propose ELBO$_\text{TDS}$, a probabilistic framework that integrates seamlessly into industry-scale incremental learning pipelines. First, we identify key shifting factors through statistical analysis of real-world production data and design a simple yet effective data augmentation strategy that resamples these time-varying factors to extend the training support. Second, to harness the benefits of this extended distribution while preventing representation collapse, we model the temporal recommendation scenario using a causal graph and derive a self-supervised variational objective, ELBO$_\text{TDS}$, grounded in the causal structure. Extensive experiments supported by both theoretical and empirical analysis demonstrate that our method achieves superior temporal generalization, yielding a 2.33\% uplift in GMV per user and has been successfully deployed in Shopee Product Search. Code is available at https://github.com/FuCongResearchSquad/ELBO4TDS.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction of Herd Life in Dairy Cows Using Multi-Head Attention Transformers</title>
<link>https://arxiv.org/abs/2511.21034</link>
<guid>https://arxiv.org/abs/2511.21034</guid>
<content:encoded><![CDATA[
<div> Keywords: dairy farming, cow longevity, AI model, Multi-Head Attention Transformers, time-series data<br /><br />Summary:<br /><br />This study addresses the critical decision-making challenge dairy farmers face in deciding whether to keep or cull cows. It highlights the importance of identifying resilient cows that can adapt to farm conditions and complete more lactations, which has significant environmental and economic impacts. To aid this process, the researchers developed an AI-driven model to predict cow longevity using historical multivariate time-series data collected from birth. The model was built using advanced AI techniques, specifically Multi-Head Attention Transformers, to effectively analyze large and complex datasets. The dataset comprised approximately 780,000 records from 19,000 unique cows across seven Australian farms, providing a robust basis for model training and validation. The results demonstrated strong predictive accuracy, with the model achieving an overall determination coefficient (R²) of 83% in forecasting herd life. This high level of performance suggests the model’s practical utility in supporting herd management decisions. Overall, the study illustrates how cutting-edge AI tools can be leveraged to improve sustainability and productivity in dairy farming by optimizing culling strategies based on objective longevity predictions. <div>
arXiv:2511.21034v1 Announce Type: new 
Abstract: Dairy farmers should decide to keep or cull a cow based on an objective assessment of her likely performance in the herd. For this purpose, farmers need to identify more resilient cows, which can cope better with farm conditions and complete more lactations. This decision-making process is inherently complex, with significant environmental and economic implications. In this study, we develop an AI-driven model to predict cow longevity using historical multivariate time-series data recorded from birth. Leveraging advanced AI techniques, specifically Multi-Head Attention Transformers, we analysed approximately 780,000 records from 19,000 unique cows across 7 farms in Australia. The results demonstrate that our model achieves an overall determination coefficient of 83% in predicting herd life across the studied farms, highlighting its potential for practical application in dairy herd management.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAVQ-HoloNet: Rate-Adaptive Vector-Quantized Hologram Compression</title>
<link>https://arxiv.org/abs/2511.21035</link>
<guid>https://arxiv.org/abs/2511.21035</guid>
<content:encoded><![CDATA[
<div> Holography, AR/VR, Data Compression, Rate-Adaptive, Vector Quantization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of high data compression demands in holography, which limits its use in AR/VR applications.<br /><br />2. Existing deep learning methods for holographic data compression generally lack the ability to adapt compression rates within a single neural network.<br /><br />3. The authors propose RAVQ-HoloNet, a novel rate-adaptive vector quantization framework designed to maintain high-fidelity holographic reconstructions at both low and ultra-low bit rates.<br /><br />4. RAVQ-HoloNet demonstrates superior performance compared to current state-of-the-art techniques, significantly improving the balance between reconstruction quality and data rate.<br /><br />5. Experimental results show that at low bit rates, the proposed method outperforms the best existing methods by achieving a -33.91% gain in BD-Rate and a BD-PSNR improvement of 1.02 dB, as shown by the rate-distortion curve analysis. <div>
arXiv:2511.21035v1 Announce Type: new 
Abstract: Holography offers significant potential for AR/VR applications, yet its adoption is limited by the high demands of data compression. Existing deep learning approaches generally lack rate adaptivity within a single network. We present RAVQ-HoloNet, a rate-adaptive vector quantization framework that achieves high-fidelity reconstructions at low and ultra-low bit rates, outperforming current state-of-the-art methods. In low bit, our method exceeds by -33.91% in BD-Rate and achieves a BD-PSNR of 1.02 dB from the best existing method demonstrated by the rate-distortion curve.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CNN-LSTM Hybrid Architecture for Over-the-Air Automatic Modulation Classification Using SDR</title>
<link>https://arxiv.org/abs/2511.21040</link>
<guid>https://arxiv.org/abs/2511.21040</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Modulation Classification, CNN-LSTM architecture, Software Defined Radio, Signal-to-Noise Ratio, cognitive radio<br /><br />Summary:<br /><br />Automatic Modulation Classification (AMC) is vital for future wireless communication systems, enabling modulation scheme identification without prior knowledge, which is crucial in cognitive radio, spectrum monitoring, and intelligent networks. This study proposes a hybrid architecture combining Convolutional Neural Networks (CNNs) for spatial feature extraction with Long Short-Term Memory (LSTM) networks for capturing temporal dependencies, integrated within a Software Defined Radio (SDR) platform. The hybrid CNN-LSTM model efficiently processes complex, time-varying communication signals. Experiments were conducted with over-the-air signals from a custom-built FM transmitter alongside other modulation schemes, validating practical system performance. Training utilized a combined dataset from RadioML2018 and a custom-generated dataset, with Signal-to-Noise Ratios (SNRs) ranging from 0 to 30 dB, ensuring robustness across noise conditions. Performance metrics included accuracy, precision, recall, F1 score, and AUC-ROC, where the optimized model achieved 93.48% accuracy, 93.53% precision, 93.48% recall, and a 93.45% F1 score. AUC-ROC analysis confirmed strong discriminative ability even under noisy environments. The results demonstrate the effectiveness of the hybrid CNN-LSTM architecture for AMC, highlighting its potential for adaptive spectrum management and advanced cognitive radio applications. <div>
arXiv:2511.21040v1 Announce Type: new 
Abstract: Automatic Modulation Classification (AMC) is a core technology for future wireless communication systems, enabling the identification of modulation schemes without prior knowledge. This capability is essential for applications in cognitive radio, spectrum monitoring, and intelligent communication networks. We propose an AMC system based on a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) architecture, integrated with a Software Defined Radio (SDR) platform. The proposed architecture leverages CNNs for spatial feature extraction and LSTMs for capturing temporal dependencies, enabling efficient handling of complex, time-varying communication signals. The system's practical ability was demonstrated by identifying over-the-air (OTA) signals from a custom-built FM transmitter alongside other modulation schemes. The system was trained on a hybrid dataset combining the RadioML2018 dataset with a custom-generated dataset, featuring samples at Signal-to-Noise Ratios (SNRs) from 0 to 30dB. System performance was evaluated using accuracy, precision, recall, F1 score, and the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). The optimized model achieved 93.48% accuracy, 93.53% precision, 93.48% recall, and an F1 score of 93.45%. The AUC-ROC analysis confirmed the model's discriminative power, even in noisy conditions. This paper's experimental results validate the effectiveness of the hybrid CNN-LSTM architecture for AMC, suggesting its potential application in adaptive spectrum management and advanced cognitive radio systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedAPA: Federated Learning with Adaptive Prototype Aggregation Toward Heterogeneous Wi-Fi CSI-based Crowd Counting</title>
<link>https://arxiv.org/abs/2511.21048</link>
<guid>https://arxiv.org/abs/2511.21048</guid>
<content:encoded><![CDATA[
<div> Keywords: Wi-Fi CSI, federated learning, adaptive prototype aggregation, human activity recognition, crowd counting<br /><br />Summary:<br /><br />This paper addresses the challenge of large-scale deployment of Wi-Fi channel state information (CSI)-based sensing systems used for human activity recognition and crowd counting, focusing on the difficulty posed by site-specific training data requirements. To overcome privacy concerns and raw data sharing limitations, the authors propose FedAPA, a federated learning (FL) algorithm incorporating an adaptive prototype aggregation (APA) strategy. FedAPA assigns similarity-based weights to peer prototypes, allowing for personalized and adaptive client contributions rather than fixed-weight aggregation, resulting in a tailored global prototype for each client. The local training process combines classification learning with representation contrastive learning, aligning local and global models effectively. The paper provides a theoretical convergence analysis of the proposed method. Experimental validation is conducted in a real-world distributed Wi-Fi crowd counting scenario involving six different environments and up to 20 participants. Results demonstrate FedAPA outperforms existing baseline methods substantially, achieving at least a 9.65% increase in accuracy, a 9% improvement in F1 score, a 0.29 reduction in mean absolute error (MAE), and remarkably reduces communication overhead by 95.94%, highlighting its efficiency and effectiveness for large-scale, privacy-preserving Wi-Fi sensing applications. <div>
arXiv:2511.21048v1 Announce Type: new 
Abstract: Wi-Fi channel state information (CSI)-based sensing provides a non-invasive, device-free approach for tasks such as human activity recognition and crowd counting, but large-scale deployment is hindered by the need for extensive site-specific training data. Federated learning (FL) offers a way to avoid raw data sharing but is challenged by heterogeneous sensing data and device resources. This paper proposes FedAPA, a collaborative Wi-Fi CSI-based sensing algorithm that uses adaptive prototype aggregation (APA) strategy to assign similarity-based weights to peer prototypes, enabling adaptive client contributions and yielding a personalized global prototype for each client instead of a fixed-weight aggregation. During local training, we adopt a hybrid objective that combines classification learning with representation contrastive learning to align local and global knowledge. We provide a convergence analysis of FedAPA and evaluate it in a real-world distributed Wi-Fi crowd counting scenario with six environments and up to 20 people. The results show that our method outperform multiple baselines in terms of accuracy, F1 score, mean absolute error (MAE), and communication overhead, with FedAPA achieving at least a 9.65% increase in accuracy, a 9% gain in F1 score, a 0.29 reduction in MAE, and a 95.94% reduction in communication overhead.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs</title>
<link>https://arxiv.org/abs/2511.21050</link>
<guid>https://arxiv.org/abs/2511.21050</guid>
<content:encoded><![CDATA[
<div> Safety-capability tradeoff, RLVR, fine-tuning, language models, theoretical analysis<br /><br />Summary:<br /><br />This paper addresses the common safety-capability tradeoff observed when fine-tuning large language models (LLMs), where attempts to improve task performance often lead to decreased safety alignment, even on benign datasets. Traditional methods like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) do not fully resolve this issue. The authors investigate reinforcement learning with verifiable rewards (RLVR) as an alternative approach that optimizes models on objectively measurable tasks, focusing on its safety implications. The study provides the first comprehensive theoretical analysis establishing upper bounds on safety drift during KL-constrained optimization and proves conditions that can eliminate safety degradation. Empirically, extensive experiments are conducted on five adversarial safety benchmarks, showing that RLVR can enhance reasoning capabilities while maintaining or even improving safety guardrails simultaneously. Through detailed ablation studies, the effects of optimization algorithms, model scale, and different task domains are examined. The findings counter the prevailing belief that improving capability necessarily compromises safety, demonstrating that with specific training methodologies, it is possible to achieve both robust reasoning and strong safety alignment in LLMs, which has important implications for their safe deployment. <div>
arXiv:2511.21050v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Diffusion Planning with Temporal Diffusion</title>
<link>https://arxiv.org/abs/2511.21054</link>
<guid>https://arxiv.org/abs/2511.21054</guid>
<content:encoded><![CDATA[
<div> Diffusion planning, Temporal Diffusion Planner, decision efficiency, denoising steps, offline data<br /><br />Summary: Diffusion planning is an effective approach for learning high-performance policies from offline data but suffers from computational inefficiencies as previous methods generate new plans at every time step. This frequent replanning reduces decision frequency and may negatively impact performance. Inspired by human planning strategies that combine detailed short-term and vague long-term plans, the authors propose the Temporal Diffusion Planner (TDP). TDP enhances decision efficiency by distributing denoising steps over time, starting with a detailed initial plan that becomes progressively vaguer. Instead of generating a new plan each time, TDP updates prior plans with fewer denoising steps at subsequent time steps, significantly reducing computational overhead. To guard against deviations between the plan and the real environment, an automated replanning mechanism is incorporated. Experiments on the D4RL benchmark demonstrate that TDP improves decision-making frequency by 11 to 24.8 times compared to prior methods, while achieving similar or better performance. This approach strikes a balance between planning quality and computational efficiency, making it a promising advancement in offline reinforcement learning and diffusion-based planning. <div>
arXiv:2511.21054v1 Announce Type: new 
Abstract: Diffusion planning is a promising method for learning high-performance policies from offline data. To avoid the impact of discrepancies between planning and reality on performance, previous works generate new plans at each time step. However, this incurs significant computational overhead and leads to lower decision frequencies, and frequent plan switching may also affect performance. In contrast, humans might create detailed short-term plans and more general, sometimes vague, long-term plans, and adjust them over time. Inspired by this, we propose the Temporal Diffusion Planner (TDP) which improves decision efficiency by distributing the denoising steps across the time dimension. TDP begins by generating an initial plan that becomes progressively more vague over time. At each subsequent time step, rather than generating an entirely new plan, TDP updates the previous one with a small number of denoising steps. This reduces the average number of denoising steps, improving decision efficiency. Additionally, we introduce an automated replanning mechanism to prevent significant deviations between the plan and reality. Experiments on D4RL show that, compared to previous works that generate new plans every time step, TDP improves the decision-making frequency by 11-24.8 times while achieving higher or comparable performance.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Understanding of Offline Data Selection and Online Self-refining Generation for Post-training LLMs</title>
<link>https://arxiv.org/abs/2511.21056</link>
<guid>https://arxiv.org/abs/2511.21056</guid>
<content:encoded><![CDATA[
<div> Keywords: offline data selection, online self-refining generation, bilevel optimization, large language models, fine-tuning

<br /><br />Summary: This paper addresses the challenge of improving large language models (LLMs) for downstream tasks by focusing on two crucial processes: offline data selection and online self-refining generation. The authors approach these two steps through an optimization framework that leverages bilevel data selection aligned with a validation dataset to enhance offline data quality. Online self-refining generation is framed as a form of model adaptation, selecting the best model based on responses that align with the validation data distribution. Their unified framework assigns learned weights to each question and response, effectively prioritizing higher-quality data samples either explicitly or implicitly. For the first time, the paper provides theoretical evidence supporting the benefits of bilevel data selection in this context. Empirically, the method shows significant improvements over unfiltered data mixing baselines. By integrating the offline data selection with validation-weighted online generations, the approach results in enhanced fine-tuning performance of LLMs. Experiments conducted also confirm improvements in both output quality and safety-aware fine-tuning, validating the practical effectiveness of the proposed framework for adapting large language models to specific tasks requiring high data quality and safety considerations. <div>
arXiv:2511.21056v1 Announce Type: new 
Abstract: Offline data selection and online self-refining generation, which enhance the data quality, are crucial steps in adapting large language models (LLMs) to specific downstream tasks. We tackle offline data selection and online self-refining generations through an optimization perspective. Specifically, bilevel data selection is used for offline data selection with respect to the validation dataset, and we treat online self-refining generation as a model adaptation step of selecting the model trained on current responses that best fits the validation data. Our framework offers a unified understanding of offline data selection and self-refining generation by assigning a learned data weight to each question and response, either explicitly or implicitly. For the first time, we theoretically demonstrate the effectiveness of the bilevel data selection framework and demonstrate its performance gains over unfiltered direct mixing baselines. By combining offline data with validation-weighted online generations, our method enhances fine-tuning performance. Experiments on quality enhancement and safety-aware LLM fine-tuning validate its effectiveness.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G-Net: A Provably Easy Construction of High-Accuracy Random Binary Neural Networks</title>
<link>https://arxiv.org/abs/2511.21063</link>
<guid>https://arxiv.org/abs/2511.21063</guid>
<content:encoded><![CDATA[
<div> Keywords: randomized algorithm, binary neural networks, hyperdimensional computing, G-Nets, accuracy tuning<br /><br />Summary:<br /><br />1. The article proposes a novel randomized algorithm for constructing binary neural networks with tunable accuracy, motivated by hyperdimensional computing (HDC), a brain-inspired approach leveraging high-dimensional vector representations. <br />2. Unlike traditional low-precision methods based on quantization, the authors consider binary embeddings as points in a hypercube using Hamming distance, offering robustness and efficient hardware implementation.<br />3. A new family of floating-point neural networks called G-Nets is introduced, which are flexible enough to replicate standard neural network layers.<br />4. Each floating-point G-Net has an embedded hyperdimensional (EHD) binary counterpart that maintains accuracy with theoretical guarantees stemming from concentration of measure principles.<br />5. Empirical evaluations demonstrate that these binary G-Net models match convolutional neural network accuracy and surpass prior HDC approaches significantly, achieving nearly 30% higher accuracy on CIFAR-10.<br />6. The work establishes G-Nets as a theoretically justified link between conventional neural networks and randomized binary neural networks, paving the way for robust, quantized deep learning architectures.<br />7. The authors have made their implementation publicly available at https://github.com/GNet2025/GNet to facilitate further research and adoption. <div>
arXiv:2511.21063v1 Announce Type: new 
Abstract: We propose a novel randomized algorithm for constructing binary neural networks with tunable accuracy. This approach is motivated by hyperdimensional computing (HDC), which is a brain-inspired paradigm that leverages high-dimensional vector representations, offering efficient hardware implementation and robustness to model corruptions. Unlike traditional low-precision methods that use quantization, we consider binary embeddings of data as points in the hypercube equipped with the Hamming distance. We propose a novel family of floating-point neural networks, G-Nets, which are general enough to mimic standard network layers. Each floating-point G-Net has a randomized binary embedding, an embedded hyperdimensional (EHD) G-Net, that retains the accuracy of its floating-point counterparts, with theoretical guarantees, due to the concentration of measure. Empirically, our binary models match convolutional neural network accuracies and outperform prior HDC models by large margins, for example, we achieve almost 30\% higher accuracy on CIFAR-10 compared to prior HDC models. G-Nets are a theoretically justified bridge between neural networks and randomized binary neural networks, opening a new direction for constructing robust binary/quantized deep learning models. Our implementation is available at https://github.com/GNet2025/GNet.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.21075</link>
<guid>https://arxiv.org/abs/2511.21075</guid>
<content:encoded><![CDATA[
<div> Keywords: Balanced Fine-Tuning, Large Language Models, biomedical reasoning, sparse data, post-training  

<br /><br />Summary:  
This paper addresses the challenge of effectively aligning Large Language Models (LLMs) with specialized biomedical knowledge through post-training methods. Traditional Supervised Fine-Tuning (SFT) often overfits to superficial instruction patterns and struggles to internalize the fragmented and sparse nature of biomedical data. Reinforcement Learning (RL) approaches are impractical due to the difficulty of defining rewards that require costly experimental validation, such as wet-lab drug response testing. To overcome these challenges, the authors propose Balanced Fine-Tuning (BFT), an innovative and efficient post-training technique that does not rely on external reward signals. BFT utilizes a two-layer weighting mechanism: at the token level, it scales loss by prediction probabilities to stabilize gradients and prevent overfitting; at the sample level, it employs a "minimum group confidence" metric to adaptively focus on harder samples. Experimental results demonstrate that BFT significantly outperforms SFT, enabling LLMs to better acquire biomedical knowledge and perform more complex biological process reasoning, surpassing even specialized agents like GeneAgent. Additionally, the embeddings generated by BFT-enhanced LLMs prove effective for downstream biomedical tasks, including gene interaction and single-cell perturbation response prediction. Overall, BFT presents a promising approach for broad application of LLMs in biomedical research. <div>
arXiv:2511.21075v1 Announce Type: new 
Abstract: Effective post-training is essential to align Large Language Models (LLMs) with specialized biomedical knowledge to accelerate life science research. However, current approaches face significant limitations. First, biomedical reasoning involves intricate mechanisms often represented by sparse textual data. Standard Supervised Fine-Tuning (SFT) tends to overfit to surface-level instruction patterns without effectively internalizing this fragmented scientific knowledge. Second, Reinforcement Learning (RL) is impractical for this domain, as defining meaningful rewards often necessitates prohibitive experimental validation (e.g., wet-lab verification of drug responses), rendering real-time feedback unfeasible. We propose Balanced Fine-Tuning (BFT), an efficient post-training method designed to learn complex reasoning from sparse data without external reward signals. BFT operates through a two-layer weighting mechanism: 1. At the token level, it scales loss via prediction probabilities to stabilize gradients and prevent overfitting; 2. At the sample level, it uses "minimum group confidence" to adaptively enhance the learning of hard samples. Experiments demonstrate that BFT significantly outperforms SFT. In medical tasks, it enables LLMs to acquire knowledge that SFT misses. In biological tasks, BFT-based LLMs surpass GeneAgent (an accurate agent for biology analysis) in biological process reasoning. Moreover, the text embeddings generated by BFT can be directly applied to downstream tasks, such as gene interaction and single-cell perturbation response prediction. These results indicate that BFT facilitates broad applications of LLMs in biomedical research.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deceptron: Learned Local Inverses for Fast and Stable Physics Inversion</title>
<link>https://arxiv.org/abs/2511.21076</link>
<guid>https://arxiv.org/abs/2511.21076</guid>
<content:encoded><![CDATA[
<div> Inverse problems, Deceptron, Jacobian Composition Penalty, D-IPG, Heat-1D  

<br /><br />Summary:  
1. The paper addresses the challenge of ill-conditioned inverse problems in physical sciences, where progress depends sensitively on step size.  
2. It introduces the Deceptron, a lightweight bidirectional module designed to learn a local inverse of a differentiable forward surrogate model.  
3. Training the Deceptron combines several strategies: supervised fitting, forward-reverse consistency, a spectral penalty, a soft bias tie, and a novel Jacobian Composition Penalty (JCP) which encourages the product of Jacobians \(J_g(f(x)) J_f(x) \approx I\) using Jacobian-vector product (JVP) and vector-Jacobian product (VJP) probes.  
4. At solve time, the method D-IPG (Deceptron Inverse-Preconditioned Gradient) performs descent steps in output space and projects the iterates back through the forward map \(g\), utilizing backtracking and stopping criteria similar to baseline methods.  
5. Empirical results show that on Heat-1D initial-condition recovery and a damped oscillator inverse problem, D-IPG achieves target accuracy with roughly 20 times fewer iterations on Heat and 2-3 times fewer on the Oscillator compared to projected gradient descent, performing competitively with Gauss-Newton methods.  
6. Diagnostics indicate that the JCP reduces the measured composition error and correlates with iteration improvements.  
7. The authors also preview DeceptronNet (v0), a single-scale 2D version that learns few-step corrections efficiently under a strict fairness protocol, demonstrating notably faster convergence. <div>
arXiv:2511.21076v1 Announce Type: new 
Abstract: Inverse problems in the physical sciences are often ill-conditioned in input space, making progress step-size sensitive. We propose the Deceptron, a lightweight bidirectional module that learns a local inverse of a differentiable forward surrogate. Training combines a supervised fit, forward-reverse consistency, a lightweight spectral penalty, a soft bias tie, and a Jacobian Composition Penalty (JCP) that encourages $J_g(f(x))\,J_f(x)\!\approx\!I$ via JVP/VJP probes. At solve time, D-IPG (Deceptron Inverse-Preconditioned Gradient) takes a descent step in output space, pulls it back through $g$, and projects under the same backtracking and stopping rules as baselines. On Heat-1D initial-condition recovery and a Damped Oscillator inverse problem, D-IPG reaches a fixed normalized tolerance with $\sim$20$\times$ fewer iterations on Heat and $\sim$2-3$\times$ fewer on Oscillator than projected gradient, competitive in iterations and cost with Gauss-Newton. Diagnostics show JCP reduces a measured composition error and tracks iteration gains. We also preview a single-scale 2D instantiation, DeceptronNet (v0), that learns few-step corrections under a strict fairness protocol and exhibits notably fast convergence.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2511.21089</link>
<guid>https://arxiv.org/abs/2511.21089</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Mixture-of-Experts, structured sparsity, tensor parallelism, pruning<br /><br />Summary: This paper introduces MLPMoE, a novel, training-free method to transform dense MLP layers in transformer blocks into a static Mixture-of-Experts (MoE) architecture using deterministic tensor slicing and summation. Unlike previous MoE upcycling methods, MLPMoE avoids clustering, activation profiling, or router training, operating entirely post hoc on existing model checkpoints. The approach interprets tensor parallelism as a topological conversion rather than just a distributed training technique. Additionally, the authors propose Fractal Fade, a mechanism for differential branch sparsity, and Compensated Pruning, which preserves variance when reducing branches, both contributing to lightweight structured sparsity. Experimentation on models Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B demonstrates that MLPMoE performs with negligible impact on proxy perplexity (less than 0.05%) when parameter counts are maintained. Further, applying differential sparsity on the 8B parameter model prunes about 20% of MLP parameters, leading to only an approximate 2% increase in perplexity compared to the dense baseline. The method requires no gradient computation, calibration data, or router training, making it computationally efficient and immediately applicable. Code is made publicly available for adoption and further research. <div>
arXiv:2511.21089v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MNM : Multi-level Neuroimaging Meta-analysis with Hyperbolic Brain-Text Representations</title>
<link>https://arxiv.org/abs/2511.21092</link>
<guid>https://arxiv.org/abs/2511.21092</guid>
<content:encoded><![CDATA[
<div> Keywords: neuroimaging, meta-analysis, hyperbolic geometry, brain activation, hierarchical representation<br /><br />Summary:<br /><br />This paper addresses the small sample size issue in neuroimaging studies which limits their reliability, proposing meta-analysis as a strategy to combine results across studies for more consistent brain activity patterns. Traditional meta-analytic methods rely on keyword-based retrieval or linear mapping techniques that fail to capture the brain's complex hierarchical structure. To overcome this, the authors introduce a novel framework that uses hyperbolic geometry, specifically the Lorentz model, to embed both neuroscience text and brain activation maps into a shared space. This shared hyperbolic space allows the model to simultaneously capture semantic similarities and hierarchical relationships inherent in neuroimaging data. Their multi-level neuroimaging meta-analysis (MNM) approach includes three key steps: aligning brain and text embeddings to establish semantic correspondence, using the hierarchy between text and brain activations to guide analysis, and preserving hierarchical relationships within brain activation patterns themselves. Experimental results demonstrate that this method outperforms baseline techniques, providing a more robust and interpretable framework for conducting multi-level meta-analyses in neuroimaging by leveraging hyperbolic brain-text representations. <div>
arXiv:2511.21092v1 Announce Type: new 
Abstract: Various neuroimaging studies suffer from small sample size problem which often limit their reliability. Meta-analysis addresses this challenge by aggregating findings from different studies to identify consistent patterns of brain activity. However, traditional approaches based on keyword retrieval or linear mappings often overlook the rich hierarchical structure in the brain. In this work, we propose a novel framework that leverages hyperbolic geometry to bridge the gap between neuroscience literature and brain activation maps. By embedding text from research articles and corresponding brain images into a shared hyperbolic space via the Lorentz model, our method captures both semantic similarity and hierarchical organization inherent in neuroimaging data. In the hyperbolic space, our method performs multi-level neuroimaging meta-analysis (MNM) by 1) aligning brain and text embeddings for semantic correspondence, 2) guiding hierarchy between text and brain activations, and 3) preserving the hierarchical relationships within brain activation patterns. Experimental results demonstrate that our model outperforms baselines, offering a robust and interpretable paradigm of multi-level neuroimaging meta-analysis via hyperbolic brain-text representation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Early Stage Ranking</title>
<link>https://arxiv.org/abs/2511.21095</link>
<guid>https://arxiv.org/abs/2511.21095</guid>
<content:encoded><![CDATA[
<div> Keywords: Early Stage Ranking, Generative Early Stage Ranking, Mixture of Attention, Target-Aware Self Attention, Multi-Logit Parameterized Gating<br /><br />Summary:<br /><br />1. The paper addresses limitations in Early Stage Ranking (ESR) systems used in large-scale recommendation, where user and item representations are learned independently and combined only at the final layer, thus lacking fine-grained affinity modeling.<br /><br />2. To improve effectiveness, the authors propose the Generative Early Stage Ranking (GESR) paradigm, featuring the Mixture of Attention (MoA) module that incorporates multiple specialized attention mechanisms for richer user-item interaction modeling.<br /><br />3. MoA includes Hard Matching Attention (HMA) to encode explicit cross-signals via raw match counts, Target-Aware Self Attention to create user representations conditioned on items for personalized learning, and Cross Attention modules to enable early and complex user-item feature interactions.<br /><br />4. The Multi-Logit Parameterized Gating (MLPG) module refines these attention-based embeddings in the final layer by integrating gated secondary logits with the primary logit, enhancing prediction quality.<br /><br />5. To maintain efficiency and low latency at scale, the authors implement a suite of optimization techniques such as custom computational kernels and caching-enabled serving architectures.<br /><br />6. Extensive offline and online experiments demonstrate significant improvements in key topline, engagement, and consumption metrics.<br /><br />7. The work represents the first large-scale deployment of full target-aware attention sequence modeling within an ESR stage, marking a novel advancement in recommendation system design. <div>
arXiv:2511.21095v1 Announce Type: new 
Abstract: Large-scale recommendations commonly adopt a multi-stage cascading ranking system paradigm to balance effectiveness and efficiency. Early Stage Ranking (ESR) systems utilize the "user-item decoupling" approach, where independently learned user and item representations are only combined at the final layer. While efficient, this design is limited in effectiveness, as it struggles to capture fine-grained user-item affinities and cross-signals. To address these, we propose the Generative Early Stage Ranking (GESR) paradigm, introducing the Mixture of Attention (MoA) module which leverages diverse attention mechanisms to bridge the effectiveness gap: the Hard Matching Attention (HMA) module encodes explicit cross-signals by computing raw match counts between user and item features; the Target-Aware Self Attention module generates target-aware user representations conditioned on the item, enabling more personalized learning; and the Cross Attention modules facilitate early and more enriched interactions between user-item features. MoA's specialized attention encodings are further refined in the final layer through a Multi-Logit Parameterized Gating (MLPG) module, which integrates the newly learned embeddings via gating and produces secondary logits that are fused with the primary logit. To address the efficiency and latency challenges, we have introduced a comprehensive suite of optimization techniques. These span from custom kernels that maximize the capabilities of the latest hardware to efficient serving solutions powered by caching mechanisms. The proposed GESR paradigm has shown substantial improvements in topline metrics, engagement, and consumption tasks, as validated by both offline and online experiments. To the best of our knowledge, this marks the first successful deployment of full target-aware attention sequence modeling within an ESR stage at such a scale.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Bits to Rounds: Parallel Decoding with Exploration for Diffusion Language Models</title>
<link>https://arxiv.org/abs/2511.21103</link>
<guid>https://arxiv.org/abs/2511.21103</guid>
<content:encoded><![CDATA[
<div> Diffusion Language Models, decoding efficiency, information bottleneck, Explore-Then-Exploit, parallel decoding<br /><br />Summary:<br />1. Diffusion Language Models (DLMs) are a promising alternative to traditional autoregressive language models, offering similar accuracy with faster inference through parallel decoding.<br />2. Current DLM decoding methods focus on selecting high-confidence tokens, but this approach encounters an information-theoretic bottleneck that limits progress in each decoding round and slows down generation.<br />3. The authors theoretically and empirically show that prioritizing high-probability tokens is inefficient because these tokens carry very little new information, restricting the effective information gained per round.<br />4. They establish a "bits-to-rounds" principle, proving that the number of decoding rounds required grows linearly with the total information content (measured by negative log-likelihood) and inversely with the amount of information processed each round.<br />5. To overcome these limitations, the paper proposes a novel, training-free decoding strategy called Explore-Then-Exploit (ETE), which mixes cross-block decoding with targeted exploration of uncertain tokens to reshape the token distribution and enable cascades of confident predictions.<br />6. Experimental results confirm the theoretical insights and demonstrate that ETE reduces the number of decoding rounds needed compared to confidence-based methods, while maintaining generation quality. <div>
arXiv:2511.21103v1 Announce Type: new 
Abstract: Diffusion Language Models (DLMs) have recently emerged as a strong alternative to autoregressive language models (LMs). DLMs offer comparable accuracy with faster inference speed via parallel decoding. However, standard DLM decoding strategies relying on high-confidence tokens encounter an inherent information-theoretic bottleneck that restricts decoding progress and ultimately slows generation. We demonstrate both theoretically and empirically that prioritizing high-confidence tokens is inherently inefficient. High-probability tokens carry negligible information and strictly relying on them limits the effective progress made in each decoding round. We prove that the number of decoding rounds must grow linearly with the sample's total information (negative log-likelihood) and inversely with the per-round information budget, establishing a bits-to-rounds principle. We also propose Explore-Then-Exploit (ETE), a training-free decoding strategy that maximizes information throughput and decoding efficiency. ETE combines cross-block decoding with targeted exploration of high-uncertainty tokens to reshape the conditional distribution and trigger cascades of confident predictions. Experiments verify our theoretical bounds and demonstrate that ETE consistently reduces the required number of decoding rounds compared to confidence-only baselines without compromising generation quality.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BRIDGE: Building Representations In Domain Guided Program Verification</title>
<link>https://arxiv.org/abs/2511.21104</link>
<guid>https://arxiv.org/abs/2511.21104</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, program verification, structured prompting, Lean4, verified synthesis<br /><br />Summary: Large language models (LLMs) excel in code generation but face significant challenges in program verification within interactive proof systems like Lean4 due to the complexity of scaling verification across code, specifications, and proofs. The paper introduces BRIDGE, a systematic framework that decomposes verified program synthesis into three interconnected domains: executable code, formal specifications, and constructive correctness proofs. BRIDGE leverages structured prompting that elicits distinct reasoning modes—functional, specification-driven, and proof-oriented—as intermediate representations maintaining semantic coherence across these domains. Through extensive ablation studies, BRIDGE demonstrates substantial improvements in both accuracy and efficiency compared to traditional error feedback methods. Notably, functional reasoning improves code correctness in Lean4 by nearly 1.5 times (pass@5) and doubles inference-time compute efficiency, achieving higher pass rates with fewer generations and reduced sampling budgets. Similarly, specification-driven prompting enhances Python code pass rates by up to 17.5%. These results underscore the value of structured domain alignment in advancing verified synthesis. Lastly, BRIDGE lays the groundwork for further training strategies such as expert iteration and reinforcement learning from verified reasoning (RLVR) to help models internalize integrated reasoning strategies that coherently span code, specifications, and proofs. <div>
arXiv:2511.21104v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive results in code generation, yet struggle with program verification, especially in interactive proof frameworks such as Lean4. A central challenge is scalability: verified synthesis requires not just code, but also precise specifications and correctness proofs, and existing approaches rarely span all three domains. We present BRIDGE, the first systematic study of structured prompting for scalable verified program generation. BRIDGE decomposes verification into three interconnected domains: Code (executable implementations), Specifications (formal intent statements), and Proofs (constructive correctness arguments). Our key idea is to elicit distinct reasoning behaviors functional, specification-driven, and proof-oriented as intermediate representations that preserve semantic structure and connect these domains. Through systematic ablations, we show that this approach substantially improves both accuracy and efficiency beyond standard error feedback methods. For example, functional reasoning improves correctness of code in formal languages (Lean4) by nearly 1.5x (pass@5) over direct baselines. In inference-time compute, functional reasoning is also 2x more efficient, achieving higher pass rates with fewer generations and lower total sampling budgets. Similarly, we find that specification-driven prompting boosts Python coding pass rates by up to 17.5%. These findings suggest that structured domain alignment is a promising direction for advancing verified synthesis. BRIDGE establishes a foundation for training via expert iteration or RLVR, enabling models to internalize these reasoning strategies across code, specifications, and proofs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Stratified Contrastive Learning with Upstream Augmentation for MILP Branching</title>
<link>https://arxiv.org/abs/2511.21107</link>
<guid>https://arxiv.org/abs/2511.21107</guid>
<content:encoded><![CDATA[
<div> Keywords: Mixed Integer Linear Programming, Branch-and-Bound, contrastive training, graph convolutional neural network, data augmentation<br /><br />Summary:<br /><br />This paper addresses challenges in solving Mixed Integer Linear Programming (MILP) problems using Branch-and-Bound (B&amp;B) methods, focusing on the critical role of branching policies. Traditional neural-based methods for branching optimization face difficulties due to semantic variations across tree depths, limited upstream node data, and high costs in collecting strong branching samples. To overcome these obstacles, the authors propose a novel framework named \ours, a Dynamic Stratified Contrastive Training Framework tailored for MILP branching. This approach clusters B&amp;B nodes by feature distributions and applies a Graph Convolutional Neural Network (GCNN) to learn discriminative node representations that progressively distinguish nodes across groups, capturing fine-grained semantic differences. To mitigate data scarcity and imbalance at upstream nodes, the framework employs an upstream-augmented MILP derivation procedure that generates both theoretically equivalent and perturbed instances, enhancing training diversity. Experimental results on standard MILP benchmarks demonstrate that \ours improves branching accuracy and significantly reduces solving time. Additionally, this method shows strong generalization capability to unseen MILP instances, indicating its practical applicability for enhancing MILP solver performance. <div>
arXiv:2511.21107v1 Announce Type: new 
Abstract: Mixed Integer Linear Programming (MILP) is a fundamental class of NP-hard problems that has garnered significant attention from both academia and industry. The Branch-and-Bound (B\&amp;B) method is the dominant approach for solving MILPs and the branching plays an important role in B\&amp;B methods. Neural-based learning frameworks have recently been developed to enhance branching policies and the efficiency of solving MILPs. However, these methods still struggle with semantic variation across depths, the scarcity of upstream nodes, and the costly collection of strong branching samples. To address these issues, we propose \ours, a Dynamic \underline{\textbf{S}}tratified \underline{\textbf{C}}ontrastive Training Framework for \underline{\textbf{MILP}} Branching. It groups branch-and-bound nodes based on their feature distributions and trains a GCNN-based discriminative model to progressively separate nodes across groups, learning finer-grained node representations throughout the tree. To address data scarcity and imbalance at upstream nodes, we introduce an upstream-augmented MILP derivation procedure that generates both theoretically equivalent and perturbed instances. \ours~effectively models subtle semantic differences between nodes, significantly enhancing branching accuracy and solving efficiency, particularly for upstream nodes. Extensive experiments on standard MILP benchmarks demonstrate that our method enhances branching accuracy, reduces solving time, and generalizes effectively to unseen instances.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Fair Clustering</title>
<link>https://arxiv.org/abs/2511.21109</link>
<guid>https://arxiv.org/abs/2511.21109</guid>
<content:encoded><![CDATA[
<div> Keywords: fair clustering, interpretability, decision trees, fairness constraints, multiple sensitive attributes<br /><br />Summary:<br /><br />Fair clustering methods have become increasingly important for applications involving socially sensitive attributes, yet many existing approaches struggle with a lack of interpretability. This paper proposes a novel framework that integrates fairness constraints directly into the structure of decision trees, thereby producing clusters that are both fair and interpretable. The method constructs decision trees that partition data with a guaranteed fair treatment of protected groups. To address the challenge of tuning fairness hyperparameters, the authors introduce a variant relying on post-pruning a tree initially built without fairness constraints, eliminating the need for manual hyperparameter adjustment. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed approach achieves performance comparable to state-of-the-art methods while improving fairness metrics. Additionally, the framework shows robustness in handling complex scenarios involving multiple sensitive attributes simultaneously. Besides fairness and performance benefits, the approach’s interpretability allows users to understand the rationale behind clustering decisions, making it highly suitable for high-stakes scenarios requiring transparency. Overall, this work advances equitable and transparent clustering by combining fairness, interpretability, and practical usability without sacrificing clustering quality. <div>
arXiv:2511.21109v1 Announce Type: new 
Abstract: Fair clustering has gained increasing attention in recent years, especially in applications involving socially sensitive attributes. However, existing fair clustering methods often lack interpretability, limiting their applicability in high-stakes scenarios where understanding the rationale behind clustering decisions is essential. In this work, we address this limitation by proposing an interpretable and fair clustering framework, which integrates fairness constraints into the structure of decision trees. Our approach constructs interpretable decision trees that partition the data while ensuring fair treatment across protected groups. To further enhance the practicality of our framework, we also introduce a variant that requires no fairness hyperparameter tuning, achieved through post-pruning a tree constructed without fairness constraints. Extensive experiments on both real-world and synthetic datasets demonstrate that our method not only delivers competitive clustering performance and improved fairness, but also offers additional advantages such as interpretability and the ability to handle multiple sensitive attributes. These strengths enable our method to perform robustly under complex fairness constraints, opening new possibilities for equitable and transparent clustering.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustless Federated Learning at Edge-Scale: A Compositional Architecture for Decentralized, Verifiable, and Incentive-Aligned Coordination</title>
<link>https://arxiv.org/abs/2511.21118</link>
<guid>https://arxiv.org/abs/2511.21118</guid>
<content:encoded><![CDATA[
arXiv:2511.21118v1 Announce Type: new 
Abstract: Artificial intelligence is retracing the Internet's path from centralized provision to distributed creation. Initially, resource-intensive computation concentrates within institutions capable of training and serving large models.Eventually, as federated learning matures, billions of edge devices holding sensitive data will be able to collectively improve models without surrendering raw information, enabling both contribution and consumption at scale. This democratic vision remains unrealized due to certain compositional gaps; aggregators handle updates without accountability, economic mechanisms are lacking and even when present remain vulnerable to gaming, coordination serializes state modifications limiting scalability, and governance permits retroactive manipulation. This work addresses these gaps by leveraging cryptographic receipts to prove aggregation correctness, geometric novelty measurement to prevent incentive gaming, parallel object ownership to achieve linear scalability, and time-locked policies to check retroactive manipulation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Cell-Aware Hierarchical Multi-Modal Representations for Robust Molecular Modeling</title>
<link>https://arxiv.org/abs/2511.21120</link>
<guid>https://arxiv.org/abs/2511.21120</guid>
<content:encoded><![CDATA[
arXiv:2511.21120v1 Announce Type: new 
Abstract: Understanding how chemical perturbations propagate through biological systems is essential for robust molecular property prediction. While most existing methods focus on chemical structures alone, recent advances highlight the crucial role of cellular responses such as morphology and gene expression in shaping drug effects. However, current cell-aware approaches face two key limitations: (1) modality incompleteness in external biological data, and (2) insufficient modeling of hierarchical dependencies across molecular, cellular, and genomic levels. We propose CHMR (Cell-aware Hierarchical Multi-modal Representations), a robust framework that jointly models local-global dependencies between molecules and cellular responses and captures latent biological hierarchies via a novel tree-structured vector quantization module. Evaluated on nine public benchmarks spanning 728 tasks, CHMR outperforms state-of-the-art baselines, yielding average improvements of 3.6% on classification and 17.2% on regression tasks. These results demonstrate the advantage of hierarchy-aware, multimodal learning for reliable and biologically grounded molecular representations, offering a generalizable framework for integrative biomedical modeling. The code is in https://github.com/limengran98/CHMR.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Correctly Report LLM-as-a-Judge Evaluations</title>
<link>https://arxiv.org/abs/2511.21140</link>
<guid>https://arxiv.org/abs/2511.21140</guid>
<content:encoded><![CDATA[
arXiv:2511.21140v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy in Federated Learning with Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2511.21181</link>
<guid>https://arxiv.org/abs/2511.21181</guid>
<content:encoded><![CDATA[
arXiv:2511.21181v1 Announce Type: new 
Abstract: Spiking neural networks (SNNs) have emerged as prominent candidates for embedded and edge AI. Their inherent low power consumption makes them far more efficient than conventional ANNs in scenarios where energy budgets are tightly constrained. In parallel, federated learning (FL) has become the prevailing training paradigm in such settings, enabling on-device learning while limiting the exposure of raw data. However, gradient inversion attacks represent a critical privacy threat in FL, where sensitive training data can be reconstructed directly from shared gradients. While this vulnerability has been widely investigated in conventional ANNs, its implications for SNNs remain largely unexplored. In this work, we present the first comprehensive empirical study of gradient leakage in SNNs across diverse data domains. SNNs are inherently non-differentiable and are typically trained using surrogate gradients, which we hypothesized would be less correlated with the original input and thus less informative from a privacy perspective. To investigate this, we adapt different gradient leakage attacks to the spike domain. Our experiments reveal a striking contrast with conventional ANNs: whereas ANN gradients reliably expose salient input content, SNN gradients yield noisy, temporally inconsistent reconstructions that fail to recover meaningful spatial or temporal structure. These results indicate that the combination of event-driven dynamics and surrogate-gradient training substantially reduces gradient informativeness. To the best of our knowledge, this work provides the first systematic benchmark of gradient inversion attacks for spiking architectures, highlighting the inherent privacy-preserving potential of neuromorphic computation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation</title>
<link>https://arxiv.org/abs/2511.21208</link>
<guid>https://arxiv.org/abs/2511.21208</guid>
<content:encoded><![CDATA[
arXiv:2511.21208v1 Announce Type: new 
Abstract: Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Gene Prioritization via Fast-mRMR Feature Selection in high-dimensional omics data</title>
<link>https://arxiv.org/abs/2511.21211</link>
<guid>https://arxiv.org/abs/2511.21211</guid>
<content:encoded><![CDATA[
arXiv:2511.21211v1 Announce Type: new 
Abstract: Gene prioritization (identifying genes potentially associated with a biological process) is increasingly tackled with Artificial Intelligence. However, existing methods struggle with the high dimensionality and incomplete labelling of biomedical data. This work proposes a more robust and efficient pipeline that leverages Fast-mRMR feature selection to retain only relevant, non-redundant features for classifiers. This enables us to build simpler and more effective models, as well as to combine different biological feature sets. Experiments on Dietary Restriction datasets show significant improvements over existing methods, proving that feature selection can be critical for reliable gene prioritization.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Informed U-net-LSTM Network for Data-Driven Seismic Response Modeling of Structures</title>
<link>https://arxiv.org/abs/2511.21276</link>
<guid>https://arxiv.org/abs/2511.21276</guid>
<content:encoded><![CDATA[
arXiv:2511.21276v1 Announce Type: new 
Abstract: Accurate and efficient seismic response prediction is essential for the design of resilient structures. While the Finite Element Method (FEM) remains the standard for nonlinear seismic analysis, its high computational demands limit its scalability and real time applicability. Recent developments in deep learning, particularly Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short Term Memory (LSTM) models, have shown promise in reducing the computational cost of nonlinear seismic analysis of structures. However, these data driven models often struggle to generalize and capture the underlying physics, leading to reduced reliability. We propose a novel Physics Informed U Net LSTM framework that integrates physical laws with deep learning to enhance both accuracy and efficiency. By embedding domain specific constraints into the learning process, the proposed model achieves improved predictive performance over conventional Machine Learning architectures. This hybrid approach bridges the gap between purely data driven methods and physics based modeling, offering a robust and computationally efficient alternative for seismic response prediction of structures.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models</title>
<link>https://arxiv.org/abs/2511.21320</link>
<guid>https://arxiv.org/abs/2511.21320</guid>
<content:encoded><![CDATA[
arXiv:2511.21320v1 Announce Type: new 
Abstract: Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TSGM: Regular and Irregular Time-series Generation using Score-based Generative Models</title>
<link>https://arxiv.org/abs/2511.21335</link>
<guid>https://arxiv.org/abs/2511.21335</guid>
<content:encoded><![CDATA[
arXiv:2511.21335v1 Announce Type: new 
Abstract: Score-based generative models (SGMs) have demonstrated unparalleled sampling quality and diversity in numerous fields, such as image generation, voice synthesis, and tabular data synthesis, etc. Inspired by those outstanding results, we apply SGMs to synthesize time-series by learning its conditional score function. To this end, we present a conditional score network for time-series synthesis, deriving a denoising score matching loss tailored for our purposes. In particular, our presented denoising score matching loss is the conditional denoising score matching loss for time-series synthesis. In addition, our framework is such flexible that both regular and irregular time-series can be synthesized with minimal changes to our model design. Finally, we obtain exceptional synthesis performance on various time-series datasets, achieving state-of-the-art sampling diversity and quality.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2511.21338</link>
<guid>https://arxiv.org/abs/2511.21338</guid>
<content:encoded><![CDATA[
arXiv:2511.21338v1 Announce Type: new 
Abstract: Masked Diffusion Language Models (MDLMs) have recently emerged as a promising alternative to Autoregressive Language Models (ARLMs), leveraging a denoising objective that, in principle, should enable more uniform context utilisation. In this work, we examine the context comprehension abilities of MDLMs and uncover two key limitations. First, despite their more global training objective and bidirectional attention mechanism, similarly to ARLMS, MDLMs exhibit a strong locality bias: performance is highly sensitive to the position of relevant information within the input, favouring local over distant context. Second, we show that appending a large number of mask tokens--required for generation--can significantly degrade context comprehension. Through systematic ablations, we find that these masks act as distractors, reducing the model's ability to process relevant information. To address this, we introduce a mask-agnostic loss function that encourages predictions to remain invariant to the number of appended masks. Fine-tuning with this objective substantially mitigates the distracting effect of masks, improving robustness of MDLMs. Overall, our findings reveal critical limitations of the current MDLM training paradigm and provide actionable insights for building diffusion-based language models with stronger context comprehension.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Best Practices for Machine Learning Experimentation in Scientific Applications</title>
<link>https://arxiv.org/abs/2511.21354</link>
<guid>https://arxiv.org/abs/2511.21354</guid>
<content:encoded><![CDATA[
arXiv:2511.21354v1 Announce Type: new 
Abstract: Machine learning (ML) is increasingly adopted in scientific research, yet the quality and reliability of results often depend on how experiments are designed and documented. Poor baselines, inconsistent preprocessing, or insufficient validation can lead to misleading conclusions about model performance. This paper presents a practical and structured guide for conducting ML experiments in scientific applications, focussing on reproducibility, fair comparison, and transparent reporting. We outline a step-by-step workflow, from dataset preparation to model selection and evaluation, and propose metrics that account for overfitting and instability across validation folds, including the Logarithmic Overfitting Ratio (LOR) and the Composite Overfitting Score (COS). Through recommended practices and example reporting formats, this work aims to support researchers in establishing robust baselines and drawing valid evidence-based insights from ML models applied to scientific problems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid-AIRL: Enhancing Inverse Reinforcement Learning with Supervised Expert Guidance</title>
<link>https://arxiv.org/abs/2511.21356</link>
<guid>https://arxiv.org/abs/2511.21356</guid>
<content:encoded><![CDATA[
arXiv:2511.21356v1 Announce Type: new 
Abstract: Adversarial Inverse Reinforcement Learning (AIRL) has shown promise in addressing the sparse reward problem in reinforcement learning (RL) by inferring dense reward functions from expert demonstrations. However, its performance in highly complex, imperfect-information settings remains largely unexplored. To explore this gap, we evaluate AIRL in the context of Heads-Up Limit Hold'em (HULHE) poker, a domain characterized by sparse, delayed rewards and significant uncertainty. In this setting, we find that AIRL struggles to infer a sufficiently informative reward function. To overcome this limitation, we contribute Hybrid-AIRL (H-AIRL), an extension that enhances reward inference and policy learning by incorporating a supervised loss derived from expert data and a stochastic regularization mechanism. We evaluate H-AIRL on a carefully selected set of Gymnasium benchmarks and the HULHE poker setting. Additionally, we analyze the learned reward function through visualization to gain deeper insights into the learning process. Our experimental results show that H-AIRL achieves higher sample efficiency and more stable learning compared to AIRL. This highlights the benefits of incorporating supervised signals into inverse RL and establishes H-AIRL as a promising framework for tackling challenging, real-world settings.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment for Local Feature Attribution Methods</title>
<link>https://arxiv.org/abs/2511.21363</link>
<guid>https://arxiv.org/abs/2511.21363</guid>
<content:encoded><![CDATA[
arXiv:2511.21363v1 Announce Type: new 
Abstract: The utility of an explanation method critically depends on its fidelity to the underlying machine learning model. Especially in high-stakes medical settings, clinicians and regulators require explanations that faithfully reflect the model's decision process. Existing fidelity metrics such as Infidelity rely on Monte Carlo approximation, which demands numerous model evaluations and introduces uncertainty due to random sampling. This work proposes a novel metric for evaluating the fidelity of local feature attribution methods by modifying the existing Prediction Change (PC) metric within the Guided Perturbation Experiment. By incorporating the direction of both perturbation and attribution, the proposed Directed Prediction Change (DPC) metric achieves an almost tenfold speedup and eliminates randomness, resulting in a deterministic and trustworthy evaluation procedure that measures the same property as local Infidelity. DPC is evaluated on two datasets (skin lesion images and financial tabular data), two black-box models, seven explanation algorithms, and a wide range of hyperparameters. Across $4\,744$ distinct explanations, the results demonstrate that DPC, together with PC, enables a holistic and computationally efficient evaluation of both baseline-oriented and local feature attribution methods, while providing deterministic and reproducible outcomes.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaMM-Disaster: A Multimodal Transformer-Based Deep Learning Framework for Multiclass Disaster Classification in Bangla</title>
<link>https://arxiv.org/abs/2511.21364</link>
<guid>https://arxiv.org/abs/2511.21364</guid>
<content:encoded><![CDATA[
arXiv:2511.21364v1 Announce Type: new 
Abstract: Natural disasters remain a major challenge for Bangladesh, so real-time monitoring and quick response systems are essential. In this study, we present BanglaMM-Disaster, an end-to-end deep learning-based multimodal framework for disaster classification in Bangla, using both textual and visual data from social media. We constructed a new dataset of 5,037 Bangla social media posts, each consisting of a caption and a corresponding image, annotated into one of nine disaster-related categories. The proposed model integrates transformer-based text encoders, including BanglaBERT, mBERT, and XLM-RoBERTa, with CNN backbones such as ResNet50, DenseNet169, and MobileNetV2, to process the two modalities. Using early fusion, the best model achieves 83.76% accuracy. This surpasses the best text-only baseline by 3.84% and the image-only baseline by 16.91%. Our analysis also shows reduced misclassification across all classes, with noticeable improvements for ambiguous examples. This work fills a key gap in Bangla multimodal disaster analysis and demonstrates the benefits of combining multiple data types for real-time disaster response in low-resource settings.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controlling changes to attention logits</title>
<link>https://arxiv.org/abs/2511.21377</link>
<guid>https://arxiv.org/abs/2511.21377</guid>
<content:encoded><![CDATA[
arXiv:2511.21377v1 Announce Type: new 
Abstract: Stability of neural network weights is critical when training transformer models. The query and key weights are particularly problematic, as they tend to grow large without any intervention. Applying normalization to queries and keys, known as `QK norm', fixes stability issues in practice, but is not always applicable. For example, QK norm is not compatible with Multi Latent Attention (MLA) because QK norm requires full materialization of queries and keys during inference, which is not done in MLA. In this paper we suggest that controlling the changes to logits is important for stability. We show that these changes are controllable by assigning parameter-dependent learning rates to the query and key weights. We find that our cheap intervention allows us to increase the base learning rate of the network, outperform other methods in the MLA setting, and achieve performance competitive with QK norm when using Multi-head Attention.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anomaly Detection with Adaptive and Aggressive Rejection for Contaminated Training Data</title>
<link>https://arxiv.org/abs/2511.21378</link>
<guid>https://arxiv.org/abs/2511.21378</guid>
<content:encoded><![CDATA[
arXiv:2511.21378v1 Announce Type: new 
Abstract: Handling contaminated data poses a critical challenge in anomaly detection, as traditional models assume training on purely normal data. Conventional methods mitigate contamination by relying on fixed contamination ratios, but discrepancies between assumed and actual ratios can severely degrade performance, especially in noisy environments where normal and abnormal data distributions overlap. To address these limitations, we propose Adaptive and Aggressive Rejection (AAR), a novel method that dynamically excludes anomalies using a modified z-score and Gaussian mixture model-based thresholds. AAR effectively balances the trade-off between preserving normal data and excluding anomalies by integrating hard and soft rejection strategies. Extensive experiments on two image datasets and thirty tabular datasets demonstrate that AAR outperforms the state-of-the-art method by 0.041 AUROC. By providing a scalable and reliable solution, AAR enhances robustness against contaminated datasets, paving the way for broader real-world applications in domains such as security and healthcare.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaASTE: A Novel Framework for Aspect-Sentiment-Opinion Extraction in Bangla E-commerce Reviews Using Ensemble Deep Learning</title>
<link>https://arxiv.org/abs/2511.21381</link>
<guid>https://arxiv.org/abs/2511.21381</guid>
<content:encoded><![CDATA[
arXiv:2511.21381v1 Announce Type: new 
Abstract: Aspect-Based Sentiment Analysis (ABSA) has emerged as a critical tool for extracting fine-grained sentiment insights from user-generated content, particularly in e-commerce and social media domains. However, research on Bangla ABSA remains significantly underexplored due to the absence of comprehensive datasets and specialized frameworks for triplet extraction in this language. This paper introduces BanglaASTE, a novel framework for Aspect Sentiment Triplet Extraction (ASTE) that simultaneously identifies aspect terms, opinion expressions, and sentiment polarities from Bangla product reviews. Our contributions include: (1) creation of the first annotated Bangla ASTE dataset containing 3,345 product reviews collected from major e-commerce platforms including Daraz, Facebook, and Rokomari; (2) development of a hybrid classification framework that employs graph-based aspect-opinion matching with semantic similarity techniques; and (3) implementation of an ensemble model combining BanglaBERT contextual embeddings with XGBoost boosting algorithms for enhanced triplet extraction performance. Experimental results demonstrate that our ensemble approach achieves superior performance with 89.9% accuracy and 89.1% F1-score, significantly outperforming baseline models across all evaluation metrics. The framework effectively addresses key challenges in Bangla text processing including informal expressions, spelling variations, and data sparsity. This research advances the state-of-the-art in low-resource language sentiment analysis and provides a scalable solution for Bangla e-commerce analytics applications.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subjective Depth and Timescale Transformers: Learning Where and When to Compute</title>
<link>https://arxiv.org/abs/2511.21408</link>
<guid>https://arxiv.org/abs/2511.21408</guid>
<content:encoded><![CDATA[
arXiv:2511.21408v1 Announce Type: new 
Abstract: The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUPN: Shallow Universal Polynomial Networks</title>
<link>https://arxiv.org/abs/2511.21414</link>
<guid>https://arxiv.org/abs/2511.21414</guid>
<content:encoded><![CDATA[
arXiv:2511.21414v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) and Kolmogorov-Arnold networks (KANs) are popular methods for function approximation due to their flexibility and expressivity. However, they typically require a large number of trainable parameters to produce a suitable approximation. Beyond making the resulting network less transparent, overparameterization creates a large optimization space, likely producing local minima in training that have quite different generalization errors. In this case, network initialization can have an outsize impact on the model's out-of-sample accuracy. For these reasons, we propose shallow universal polynomial networks (SUPNs). These networks replace all but the last hidden layer with a single layer of polynomials with learnable coefficients, leveraging the strengths of DNNs and polynomials to achieve sufficient expressivity with far fewer parameters. We prove that SUPNs converge at the same rate as the best polynomial approximation of the same degree, and we derive explicit formulas for quasi-optimal SUPN parameters. We complement theory with an extensive suite of numerical experiments involving SUPNs, DNNs, KANs, and polynomial projection in one, two, and ten dimensions, consisting of over 13,000 trained models. On the target functions we numerically studied, for a given number of trainable parameters, the approximation error and variability are often lower for SUPNs than for DNNs and KANs by an order of magnitude. In our examples, SUPNs even outperform polynomial projection on non-smooth functions.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensemble Performance Through the Lens of Linear Independence of Classifier Votes in Data Streams</title>
<link>https://arxiv.org/abs/2511.21465</link>
<guid>https://arxiv.org/abs/2511.21465</guid>
<content:encoded><![CDATA[
arXiv:2511.21465v1 Announce Type: new 
Abstract: Ensemble learning improves classification performance by combining multiple base classifiers. While increasing the number of classifiers generally enhances accuracy, excessively large ensembles can lead to computational inefficiency and diminishing returns. This paper investigates the relationship between ensemble size and performance through the lens of linear independence among classifier votes in data streams. We propose that ensembles composed of linearly independent classifiers maximize representational capacity, particularly under a geometric model. We then generalize the importance of linear independence to the weighted majority voting problem. By modeling the probability of achieving linear independence among classifier outputs, we derive a theoretical framework that explains the trade-off between ensemble size and accuracy. Our analysis leads to a theoretical estimate of the ensemble size required to achieve a user-specified probability of linear independence. We validate our theory through experiments on both real-world and synthetic datasets using two ensemble methods, OzaBagging and GOOWE. Our results confirm that this theoretical estimate effectively identifies the point of performance saturation for robust ensembles like OzaBagging. Conversely, for complex weighting schemes like GOOWE, our framework reveals that high theoretical diversity can trigger algorithmic instability. Our implementation is publicly available to support reproducibility and future research.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mean-Field Limits for Two-Layer Neural Networks Trained with Consensus-Based Optimization</title>
<link>https://arxiv.org/abs/2511.21466</link>
<guid>https://arxiv.org/abs/2511.21466</guid>
<content:encoded><![CDATA[
arXiv:2511.21466v1 Announce Type: new 
Abstract: We study two-layer neural networks and train these with a particle-based method called consensus-based optimization (CBO). We compare the performance of CBO against Adam on two test cases and demonstrate how a hybrid approach, combining CBO with Adam, provides faster convergence than CBO. In the context of multi-task learning, we recast CBO into a formulation that offers less memory overhead. The CBO method allows for a mean-field limit formulation, which we couple with the mean-field limit of the neural network. To this end, we first reformulate CBO within the optimal transport framework. Finally, in the limit of infinitely many particles, we define the corresponding dynamics on the Wasserstein-over-Wasserstein space and show that the variance decreases monotonically.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in Time? A Meta-Learning Framework for Time-Shift-Tolerant Physiological Signal Transformation</title>
<link>https://arxiv.org/abs/2511.21500</link>
<guid>https://arxiv.org/abs/2511.21500</guid>
<content:encoded><![CDATA[
arXiv:2511.21500v1 Announce Type: new 
Abstract: Translating non-invasive signals such as photoplethysmography (PPG) and ballistocardiography (BCG) into clinically meaningful signals like arterial blood pressure (ABP) is vital for continuous, low-cost healthcare monitoring. However, temporal misalignment in multimodal signal transformation impairs transformation accuracy, especially in capturing critical features like ABP peaks. Conventional synchronization methods often rely on strong similarity assumptions or manual tuning, while existing Learning with Noisy Labels (LNL) approaches are ineffective under time-shifted supervision, either discarding excessive data or failing to correct label shifts. To address this challenge, we propose ShiftSyncNet, a meta-learning-based bi-level optimization framework that automatically mitigates performance degradation due to time misalignment. It comprises a transformation network (TransNet) and a time-shift correction network (SyncNet), where SyncNet learns time offsets between training pairs and applies Fourier phase shifts to align supervision signals. Experiments on one real-world industrial dataset and two public datasets show that ShiftSyncNet outperforms strong baselines by 9.4%, 6.0%, and 12.8%, respectively. The results highlight its effectiveness in correcting time shifts, improving label quality, and enhancing transformation accuracy across diverse misalignment scenarios, pointing toward a unified direction for addressing temporal inconsistencies in multimodal physiological transformation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference</title>
<link>https://arxiv.org/abs/2511.21513</link>
<guid>https://arxiv.org/abs/2511.21513</guid>
<content:encoded><![CDATA[
arXiv:2511.21513v1 Announce Type: new 
Abstract: Deploying Transformer models on edge devices is limited by latency and energy budgets. While INT8 quantization effectively accelerates the primary matrix multiplications, it exposes the softmax as the dominant bottleneck. This stage incurs a costly dequantize-softmax-requantize detour, which can account for up to 65% of total attention latency and disrupts the end-to-end integer dataflow critical for edge hardware efficiency. To address this limitation, we present IntAttention, the first fully integer, plug-and-play attention pipeline without retraining. At the core of our approach lies IndexSoftmax, a hardware-friendly operator that replaces floating-point exponentials entirely within the integer domain. IntAttention integrates sparsity-aware clipping, a 32-entry lookup-table approximation, and direct integer normalization, thereby eliminating all datatype conversion overhead. We evaluate IntAttention and demonstrate consistent and substantial gains. Our method achieves up to 3.7x speedup and 61% energy reduction over FP16 baselines and 2.0x faster than conventional INT8 attention pipelines on Armv8 CPUs. These gains are achieved with high-fidelity accuracy comparable to baselines across diverse language and vision models, enabling practical and efficient Transformer inference on commodity edge devices. Code will be released in later version of this work.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanistic Interpretability for Transformer-based Time Series Classification</title>
<link>https://arxiv.org/abs/2511.21514</link>
<guid>https://arxiv.org/abs/2511.21514</guid>
<content:encoded><![CDATA[
arXiv:2511.21514v1 Announce Type: new 
Abstract: Transformer-based models have become state-of-the-art tools in various machine learning tasks, including time series classification, yet their complexity makes understanding their internal decision-making challenging. Existing explainability methods often focus on input-output attributions, leaving the internal mechanisms largely opaque. This paper addresses this gap by adapting various Mechanistic Interpretability techniques; activation patching, attention saliency, and sparse autoencoders, from NLP to transformer architectures designed explicitly for time series classification. We systematically probe the internal causal roles of individual attention heads and timesteps, revealing causal structures within these models. Through experimentation on a benchmark time series dataset, we construct causal graphs illustrating how information propagates internally, highlighting key attention heads and temporal positions driving correct classifications. Additionally, we demonstrate the potential of sparse autoencoders for uncovering interpretable latent features. Our findings provide both methodological contributions to transformer interpretability and novel insights into the functional mechanics underlying transformer performance in time series classification tasks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Safety Shield for Dyna-Q Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.21531</link>
<guid>https://arxiv.org/abs/2511.21531</guid>
<content:encoded><![CDATA[
arXiv:2511.21531v1 Announce Type: new 
Abstract: Obtaining safety guarantees for reinforcement learning is a major challenge to achieve applicability for real-world tasks. Safety shields extend standard reinforcement learning and achieve hard safety guarantees. However, existing safety shields commonly use random sampling of safe actions or a fixed fallback controller, therefore disregarding future performance implications of different safe actions. In this work, we propose a predictive safety shield for model-based reinforcement learning agents in discrete space. Our safety shield updates the Q-function locally based on safe predictions, which originate from a safe simulation of the environment model. This shielding approach improves performance while maintaining hard safety guarantees. Our experiments on gridworld environments demonstrate that even short prediction horizons can be sufficient to identify the optimal path. We observe that our approach is robust to distribution shifts, e.g., between simulation and reality, without requiring additional training.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Specific Causal Graph Discovery with Unobserved Contexts: Non-Stationarity, Regimes and Spatio-Temporal Patterns</title>
<link>https://arxiv.org/abs/2511.21537</link>
<guid>https://arxiv.org/abs/2511.21537</guid>
<content:encoded><![CDATA[
arXiv:2511.21537v1 Announce Type: new 
Abstract: Real-world data, for example in climate applications, often consists of spatially gridded time series data or data with comparable structure. While the underlying system is often believed to behave similar at different points in space and time, those variations that do exist are twofold relevant: They often encode important information in and of themselves. And they may negatively affect the stability / convergence and reliability\Slash{}validity of results of algorithms assuming stationarity or space-translation invariance. We study the information encoded in changes of the causal graph, with stability in mind. An analysis of this general task identifies two core challenges. We develop guiding principles to overcome these challenges, and provide a framework realizing these principles by modifying constraint-based causal discovery approaches on the level of independence testing. This leads to an extremely modular, easily extensible and widely applicable framework. It can leverage existing constraint-based causal discovery methods (demonstrated on IID-algorithms PC, PC-stable, FCI and time series algorithms PCMCI, PCMCI+, LPCMCI) with little to no modification. The built-in modularity allows to systematically understand and improve upon an entire array of subproblems. By design, it can be extended by leveraging insights from change-point-detection, clustering, independence-testing and other well-studied related problems. The division into more accessible sub-problems also simplifies the understanding of fundamental limitations, hyperparameters controlling trade-offs and the statistical interpretation of results. An open-source implementation will be available soon.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computing Strategic Responses to Non-Linear Classifiers</title>
<link>https://arxiv.org/abs/2511.21560</link>
<guid>https://arxiv.org/abs/2511.21560</guid>
<content:encoded><![CDATA[
arXiv:2511.21560v1 Announce Type: new 
Abstract: We consider the problem of strategic classification, where the act of deploying a classifier leads to strategic behaviour that induces a distribution shift on subsequent observations. Current approaches to learning classifiers in strategic settings are focused primarily on the linear setting, but in many cases non-linear classifiers are more suitable. A central limitation to progress for non-linear classifiers arises from the inability to compute best responses in these settings. We present a novel method for computing the best response by optimising the Lagrangian dual of the Agents' objective. We demonstrate that our method reproduces best responses in linear settings, identifying key weaknesses in existing approaches. We present further results demonstrating our method can be straight-forwardly applied to non-linear classifier settings, where it is useful for both evaluation and training.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Approaches to Clinical Risk Prediction: Multi-Scale Temporal Alignment in Electronic Health Records</title>
<link>https://arxiv.org/abs/2511.21561</link>
<guid>https://arxiv.org/abs/2511.21561</guid>
<content:encoded><![CDATA[
arXiv:2511.21561v1 Announce Type: new 
Abstract: This study proposes a risk prediction method based on a Multi-Scale Temporal Alignment Network (MSTAN) to address the challenges of temporal irregularity, sampling interval differences, and multi-scale dynamic dependencies in Electronic Health Records (EHR). The method focuses on temporal feature modeling by introducing a learnable temporal alignment mechanism and a multi-scale convolutional feature extraction structure to jointly model long-term trends and short-term fluctuations in EHR sequences. At the input level, the model maps multi-source clinical features into a unified high-dimensional semantic space and employs temporal embedding and alignment modules to dynamically weight irregularly sampled data, reducing the impact of temporal distribution differences on model performance. The multi-scale feature extraction module then captures key patterns across different temporal granularities through multi-layer convolution and hierarchical fusion, achieving a fine-grained representation of patient states. Finally, an attention-based aggregation mechanism integrates global temporal dependencies to generate individual-level risk representations for disease risk prediction and health status assessment. Experiments conducted on publicly available EHR datasets show that the proposed model outperforms mainstream baselines in accuracy, recall, precision, and F1-Score, demonstrating the effectiveness and robustness of multi-scale temporal alignment in complex medical time-series analysis. This study provides a new solution for intelligent representation of high-dimensional asynchronous medical sequences and offers important technical support for EHR-driven clinical risk prediction.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A decoupled alignment kernel for peptide membrane permeability predictions</title>
<link>https://arxiv.org/abs/2511.21566</link>
<guid>https://arxiv.org/abs/2511.21566</guid>
<content:encoded><![CDATA[
arXiv:2511.21566v1 Announce Type: new 
Abstract: Cyclic peptides are promising modalities for targeting intracellular sites; however, cell-membrane permeability remains a key bottleneck, exacerbated by limited public data and the need for well-calibrated uncertainty. Instead of relying on data-eager complex deep learning architecture, we propose a monomer-aware decoupled global alignment kernel (MD-GAK), which couples chemically meaningful residue-residue similarity with sequence alignment while decoupling local matches from gap penalties. MD-GAK is a relatively simple kernel. To further demonstrate the robustness of our framework, we also introduce a variant, PMD-GAK, which incorporates a triangular positional prior. As we will show in the experimental section, PMD-GAK can offer additional advantages over MD-GAK, particularly in reducing calibration errors. Since our focus is on uncertainty estimation, we use Gaussian Processes as the predictive model, as both MD-GAK and PMD-GAK can be directly applied within this framework. We demonstrate the effectiveness of our methods through an extensive set of experiments, comparing our fully reproducible approach against state-of-the-art models, and show that it outperforms them across all metrics.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.21581</link>
<guid>https://arxiv.org/abs/2511.21581</guid>
<content:encoded><![CDATA[
arXiv:2511.21581v1 Announce Type: new 
Abstract: Latent reasoning represents a new development in Transformer language models that has shown potential in compressing reasoning lengths compared to chain-of-thought reasoning. By directly passing the information-rich previous final latent state into the next sequence, latent reasoning removes the restriction to human language tokens as the medium for reasoning. We develop adaptive-length latent reasoning models and introduce a post-SFT reinforcement-learning methodology to optimize latent reasoning length by minimizing reasoning length while maintaining accuracy. This, in turn, further reduces compute usage and raises the bar on the compressive capabilities of latent reasoning models. Experiments on the Llama 3.2 1B model and the GSM8K-Aug dataset show a $52\%$ drop in total reasoning length with no penalty to accuracy. In future work, we plan to extend to additional models and datasets, analyze relationships between training coefficients, experiment with architecture variations, and continue our knowledge distillation for latent reasoning SFT efforts. We make our code and pretrained weights available at https://github.com/apning/adaptive-latent-reasoning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids</title>
<link>https://arxiv.org/abs/2511.21590</link>
<guid>https://arxiv.org/abs/2511.21590</guid>
<content:encoded><![CDATA[
arXiv:2511.21590v1 Announce Type: new 
Abstract: Smart grids are a fusion of classical power infrastructure and advanced communication networks and smart control, to create a cyber-physical environment that is more efficient and flexible than ever before. This integration causes vulnerabilities that can undermine grid stability as well as reliability. Digital forensics is a fundamental concept of learning and identifying, detecting, and mitigating such security incidents. This paper presents an all-in-one machine learning-based digital forensic framework of smart grid systems deployed on the Cloud. The framework combines the data acquisition at the sensor-level, authenticated communication, scalable cloud storage and automated forensic analytics. The model uses supervised and unsupervised learning algorithms - such as Random Forest, Support Vector Machine, Gradient Boosted Trees and deep neural architectures for anomaly detection, event reconstruction and intrusion analysis in real time. After several simulation and experimental studies on real-time smart-meter data streams, the proposed framework is shown to be very accurate, scalable and resilient to cyber-attacks including data tampering, false-data injection and coordinated control-loop manipulation. The results indicate that cloud services are the best backbone for big-data-driven forensic workflows, which allows energy utilities to achieve a fast situational awareness and intelligent incident response.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</title>
<link>https://arxiv.org/abs/2511.21594</link>
<guid>https://arxiv.org/abs/2511.21594</guid>
<content:encoded><![CDATA[
arXiv:2511.21594v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Origin of Algorithmic Progress in AI</title>
<link>https://arxiv.org/abs/2511.21622</link>
<guid>https://arxiv.org/abs/2511.21622</guid>
<content:encoded><![CDATA[
arXiv:2511.21622v1 Announce Type: new 
Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale-Agnostic Kolmogorov-Arnold Geometry in Neural Networks</title>
<link>https://arxiv.org/abs/2511.21626</link>
<guid>https://arxiv.org/abs/2511.21626</guid>
<content:encoded><![CDATA[
arXiv:2511.21626v1 Announce Type: new 
Abstract: Recent work by Freedman and Mulligan demonstrated that shallow multilayer perceptrons spontaneously develop Kolmogorov-Arnold geometric (KAG) structure during training on synthetic three-dimensional tasks. However, it remained unclear whether this phenomenon persists in realistic high-dimensional settings and what spatial properties this geometry exhibits.
  We extend KAG analysis to MNIST digit classification (784 dimensions) using 2-layer MLPs with systematic spatial analysis at multiple scales. We find that KAG emerges during training and appears consistently across spatial scales, from local 7-pixel neighborhoods to the full 28x28 image. This scale-agnostic property holds across different training procedures: both standard training and training with spatial augmentation produce the same qualitative pattern. These findings reveal that neural networks spontaneously develop organized, scale-invariant geometric structure during learning on realistic high-dimensional data.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanisms of Non-Monotonic Scaling in Vision Transformers</title>
<link>https://arxiv.org/abs/2511.21635</link>
<guid>https://arxiv.org/abs/2511.21635</guid>
<content:encoded><![CDATA[
arXiv:2511.21635v1 Announce Type: new 
Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO</title>
<link>https://arxiv.org/abs/2511.21638</link>
<guid>https://arxiv.org/abs/2511.21638</guid>
<content:encoded><![CDATA[
arXiv:2511.21638v1 Announce Type: new 
Abstract: Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvilGenie: A Reward Hacking Benchmark</title>
<link>https://arxiv.org/abs/2511.21654</link>
<guid>https://arxiv.org/abs/2511.21654</guid>
<content:encoded><![CDATA[
arXiv:2511.21654v1 Announce Type: new 
Abstract: We introduce EvilGenie, a benchmark for reward hacking in programming settings. We source problems from LiveCodeBench and create an environment in which agents can easily reward hack, such as by hardcoding test cases or editing the testing files. We measure reward hacking in three ways: held out unit tests, LLM judges, and test file edit detection. We verify these methods against human review and each other. We find the LLM judge to be highly effective at detecting reward hacking in unambiguous cases, and observe only minimal improvement from the use of held out test cases. In addition to testing many models using Inspect's basic_agent scaffold, we also measure reward hacking rates for three popular proprietary coding agents: OpenAI's Codex, Anthropic's Claude Code, and Google's Gemini CLI Using GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro, respectively. We observe explicit reward hacking by both Codex and Claude Code, and misaligned behavior by all three agents. Our codebase can be found at https://github.com/JonathanGabor/EvilGenie.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Escaping the Verifier: Learning to Reason via Demonstrations</title>
<link>https://arxiv.org/abs/2511.21667</link>
<guid>https://arxiv.org/abs/2511.21667</guid>
<content:encoded><![CDATA[
arXiv:2511.21667v1 Announce Type: new 
Abstract: Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Through the telecom lens: Are all training samples important?</title>
<link>https://arxiv.org/abs/2511.21668</link>
<guid>https://arxiv.org/abs/2511.21668</guid>
<content:encoded><![CDATA[
arXiv:2511.21668v1 Announce Type: new 
Abstract: The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving</title>
<link>https://arxiv.org/abs/2511.21669</link>
<guid>https://arxiv.org/abs/2511.21669</guid>
<content:encoded><![CDATA[
arXiv:2511.21669v1 Announce Type: new 
Abstract: Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Grounded Evaluation of LLMs in International Student Knowledge</title>
<link>https://arxiv.org/abs/2511.20653</link>
<guid>https://arxiv.org/abs/2511.20653</guid>
<content:encoded><![CDATA[
arXiv:2511.20653v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to answer high-stakes study-abroad questions about admissions, visas, scholarships, and eligibility. Yet it remains unclear how reliably they advise students, and how often otherwise helpful answers drift into unsupported claims (``hallucinations'').
  This work provides a clear, domain-grounded overview of how current LLMs behave in this setting. Using realistic questions set drawn from ApplyBoard's advising workflows -- an EdTech platform that supports students from discovery to enrolment -- we evaluate two essentials side by side: accuracy (is the information correct and complete?) and hallucination (does the model add content not supported by the question or domain evidence). These questions are categorized by domain scope which can be a single-domain or multi-domain -- when it must integrate evidence across areas such as admissions, visas, and scholarships.
  To reflect real advising quality, we grade answers with a simple rubric which is correct, partial, or wrong. The rubric is domain-coverage-aware: an answer can be partial if it addresses only a subset of the required domains, and it can be over-scoped if it introduces extra, unnecessary domains; both patterns are captured in our scoring as under-coverage or reduced relevance/hallucination.
  We also report measures of faithfulness and answer relevance, alongside an aggregate hallucination score, to capture relevance and usefulness. All models are tested with the same questions for a fair, head-to-head comparison.
  Our goals are to: (1) give a clear picture of which models are most dependable for study-abroad advising, (2) surface common failure modes -- where answers are incomplete, off-topic, or unsupported, and (3) offer a practical, reusable protocol for auditing LLMs before deployment in education and advising contexts.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology</title>
<link>https://arxiv.org/abs/2511.20665</link>
<guid>https://arxiv.org/abs/2511.20665</guid>
<content:encoded><![CDATA[
arXiv:2511.20665v1 Announce Type: cross 
Abstract: This paper introduces the Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. Unlike neural embeddings that rely on statistical co-occurrence or optimization, HTP encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation, establishing a bijective and interpretable mapping between discrete symbols and continuous vector space. The harmonic formulation provides phase-coherent projections that preserve both structure and reversibility, enabling semantic similarity estimation from purely geometric alignment. Experimental evaluation on the Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension shows that HTP achieves a Spearman correlation of \r{ho} = 0.68 in English, maintaining stable performance across ten languages with negligible computational cost and sub-millisecond latency per sentence pair. This demonstrates that meaningful semantic relations can emerge from deterministic geometry, offering a transparent and efficient alternative to data-driven embeddings. Keywords: Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cryptocurrency Portfolio Management with Reinforcement Learning: Soft Actor--Critic and Deep Deterministic Policy Gradient Algorithms</title>
<link>https://arxiv.org/abs/2511.20678</link>
<guid>https://arxiv.org/abs/2511.20678</guid>
<content:encoded><![CDATA[
arXiv:2511.20678v1 Announce Type: cross 
Abstract: This paper proposes a reinforcement learning--based framework for cryptocurrency portfolio management using the Soft Actor--Critic (SAC) and Deep Deterministic Policy Gradient (DDPG) algorithms. Traditional portfolio optimization methods often struggle to adapt to the highly volatile and nonlinear dynamics of cryptocurrency markets. To address this, we design an agent that learns continuous trading actions directly from historical market data through interaction with a simulated trading environment. The agent optimizes portfolio weights to maximize cumulative returns while minimizing downside risk and transaction costs. Experimental evaluations on multiple cryptocurrencies demonstrate that the SAC and DDPG agents outperform baseline strategies such as equal-weighted and mean--variance portfolios. The SAC algorithm, with its entropy-regularized objective, shows greater stability and robustness in noisy market conditions compared to DDPG. These results highlight the potential of deep reinforcement learning for adaptive and data-driven portfolio management in cryptocurrency markets.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring</title>
<link>https://arxiv.org/abs/2511.20679</link>
<guid>https://arxiv.org/abs/2511.20679</guid>
<content:encoded><![CDATA[
arXiv:2511.20679v1 Announce Type: cross 
Abstract: Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Domain Deep Learning Method to Accelerate Local Basis Functions Computation for Reservoir Simulation in High-Contrast Porous Media</title>
<link>https://arxiv.org/abs/2511.20685</link>
<guid>https://arxiv.org/abs/2511.20685</guid>
<content:encoded><![CDATA[
arXiv:2511.20685v1 Announce Type: cross 
Abstract: In energy science, Darcy flow in heterogeneous porous media is a central problem in reservoir sim-ulation. However, the pronounced multiscale characteristics of such media pose significant challenges to conventional numerical methods in terms of computational demand and efficiency. The Mixed Generalized Multiscale Finite Element Method (MGMsFEM) provides an effective framework for addressing these challenges, yet the construction of multiscale basis functions remains computationally expensive. In this work, we propose a dual-domain deep learning framework to accelerate the computation of multiscale basis functions within MGMsFEM for solving Darcy flow problems. By extracting and decoding permeability field features in both the frequency and spatial domains, the method enables rapid generation of numerical matrices of multiscale basis functions. Numerical experiments demonstrate that the proposed framework achieves significant computational acceleration while maintaining high approximation accuracy, thereby offering the potential for future applications in real-world reservoir engineering.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI</title>
<link>https://arxiv.org/abs/2511.20686</link>
<guid>https://arxiv.org/abs/2511.20686</guid>
<content:encoded><![CDATA[
arXiv:2511.20686v1 Announce Type: cross 
Abstract: The rapid evolution of generative AI necessitates robust safety evaluations. However, current safety datasets are predominantly English-centric, failing to capture specific risks in non-English, socio-cultural contexts such as Korean, and are often limited to the text modality. To address this gap, we introduce AssurAI, a new quality-controlled Korean multimodal dataset for evaluating the safety of generative AI. First, we define a taxonomy of 35 distinct AI risk factors, adapted from established frameworks by a multidisciplinary expert group to cover both universal harms and relevance to the Korean socio-cultural context. Second, leveraging this taxonomy, we construct and release AssurAI, a large-scale Korean multimodal dataset comprising 11,480 instances across text, image, video, and audio. Third, we apply the rigorous quality control process used to ensure data integrity, featuring a two-phase construction (i.e., expert-led seeding and crowdsourced scaling), triple independent annotation, and an iterative expert red-teaming loop. Our pilot study validates AssurAI's effectiveness in assessing the safety of recent LLMs. We release AssurAI to the public to facilitate the development of safer and more reliable generative AI systems for the Korean community.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Human Brain as a Combinatorial Complex</title>
<link>https://arxiv.org/abs/2511.20692</link>
<guid>https://arxiv.org/abs/2511.20692</guid>
<content:encoded><![CDATA[
arXiv:2511.20692v1 Announce Type: cross 
Abstract: We propose a framework for constructing combinatorial complexes (CCs) from fMRI time series data that captures both pairwise and higher-order neural interactions through information-theoretic measures, bridging topological deep learning and network neuroscience. Current graph-based representations of brain networks systematically miss the higher-order dependencies that characterize neural complexity, where information processing often involves synergistic interactions that cannot be decomposed into pairwise relationships. Unlike topological lifting approaches that map relational structures into higher-order domains, our method directly constructs CCs from statistical dependencies in the data. Our CCs generalize graphs by incorporating higher-order cells that represent collective dependencies among brain regions, naturally accommodating the multi-scale, hierarchical nature of neural processing. The framework constructs data-driven combinatorial complexes using O-information and S-information measures computed from fMRI signals, preserving both pairwise connections and higher-order cells (e.g., triplets, quadruplets) based on synergistic dependencies. Using NetSim simulations as a controlled proof-of-concept dataset, we demonstrate our CC construction pipeline and show how both pairwise and higher-order dependencies in neural time series can be quantified and represented within a unified structure. This work provides a framework for brain network representation that preserves fundamental higher-order structure invisible to traditional graph methods, and enables the application of topological deep learning (TDL) architectures to neural data.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning</title>
<link>https://arxiv.org/abs/2511.20694</link>
<guid>https://arxiv.org/abs/2511.20694</guid>
<content:encoded><![CDATA[
arXiv:2511.20694v1 Announce Type: cross 
Abstract: Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present Reasoning With a Star, a newly contributed heliophysics dataset applicable to reasoning; we also provide an initial benchmarking approach. Our data are constructed from National Aeronautics and Space Administration & University Corporation for Atmospheric Research Living With a Star summer school problem sets and compiled into a readily consumable question-and-answer structure with question contexts, reasoning steps, expected answer type, ground-truth targets, format hints, and metadata. A programmatic grader checks the predictions using unit-aware numerical tolerance, symbolic equivalence, and schema validation. We benchmark a single-shot baseline and four multi-agent patterns, finding that decomposing workflows through systems engineering principles outperforms direct prompting on problems requiring deductive reasoning rather than pure inductive recall.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework</title>
<link>https://arxiv.org/abs/2511.20701</link>
<guid>https://arxiv.org/abs/2511.20701</guid>
<content:encoded><![CDATA[
arXiv:2511.20701v1 Announce Type: cross 
Abstract: While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach</title>
<link>https://arxiv.org/abs/2511.20703</link>
<guid>https://arxiv.org/abs/2511.20703</guid>
<content:encoded><![CDATA[
arXiv:2511.20703v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have sparked concerns over their potential to acquire and misuse dangerous or high-risk capabilities, posing frontier risks. Current safety evaluations primarily test for what a model \textit{can} do - its capabilities - without assessing what it $\textit{would}$ do if endowed with high-risk capabilities. This leaves a critical blind spot: models may strategically conceal capabilities or rapidly acquire them, while harboring latent inclinations toward misuse. We argue that $\textbf{propensity}$ - the likelihood of a model to pursue harmful actions if empowered - is a critical, yet underexplored, axis of safety evaluation. We present $\textbf{PropensityBench}$, a novel benchmark framework that assesses the proclivity of models to engage in risky behaviors when equipped with simulated dangerous capabilities using proxy tools. Our framework includes 5,874 scenarios with 6,648 tools spanning four high-risk domains: cybersecurity, self-proliferation, biosecurity, and chemical security. We simulate access to powerful capabilities via a controlled agentic environment and evaluate the models' choices under varying operational pressures that reflect real-world constraints or incentives models may encounter, such as resource scarcity or gaining more autonomy. Across open-source and proprietary frontier models, we uncover 9 alarming signs of propensity: models frequently choose high-risk tools when under pressure, despite lacking the capability to execute such actions unaided. These findings call for a shift from static capability audits toward dynamic propensity assessments as a prerequisite for deploying frontier AI systems safely. Our code is available at https://github.com/scaleapi/propensity-evaluation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Set of Rules for Model Validation</title>
<link>https://arxiv.org/abs/2511.20711</link>
<guid>https://arxiv.org/abs/2511.20711</guid>
<content:encoded><![CDATA[
arXiv:2511.20711v1 Announce Type: cross 
Abstract: The validation of a data-driven model is the process of assessing the model's ability to generalize to new, unseen data in the population of interest. This paper proposes a set of general rules for model validation. These rules are designed to help practitioners create reliable validation plans and report their results transparently. While no validation scheme is flawless, these rules can help practitioners ensure their strategy is sufficient for practical use, openly discuss any limitations of their validation strategy, and report clear, comparable performance metrics.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.20720</link>
<guid>https://arxiv.org/abs/2511.20720</guid>
<content:encoded><![CDATA[
arXiv:2511.20720v1 Announce Type: cross 
Abstract: Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundry: Distilling 3D Foundation Models for the Edge</title>
<link>https://arxiv.org/abs/2511.20721</link>
<guid>https://arxiv.org/abs/2511.20721</guid>
<content:encoded><![CDATA[
arXiv:2511.20721v1 Announce Type: cross 
Abstract: Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities</title>
<link>https://arxiv.org/abs/2511.20730</link>
<guid>https://arxiv.org/abs/2511.20730</guid>
<content:encoded><![CDATA[
arXiv:2511.20730v1 Announce Type: cross 
Abstract: The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models</title>
<link>https://arxiv.org/abs/2511.20799</link>
<guid>https://arxiv.org/abs/2511.20799</guid>
<content:encoded><![CDATA[
arXiv:2511.20799v1 Announce Type: cross 
Abstract: Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\Delta$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer</title>
<link>https://arxiv.org/abs/2511.20804</link>
<guid>https://arxiv.org/abs/2511.20804</guid>
<content:encoded><![CDATA[
arXiv:2511.20804v1 Announce Type: cross 
Abstract: Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $\Delta$-NeRF, a unique modular residual framework for incremental NeRF refinement. $\Delta$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\% of original size). Experiments on satellite imagery demonstrate that $\Delta$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\%. $\Delta$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPHINX: A Synthetic Environment for Visual Perception and Reasoning</title>
<link>https://arxiv.org/abs/2511.20814</link>
<guid>https://arxiv.org/abs/2511.20814</guid>
<content:encoded><![CDATA[
arXiv:2511.20814v1 Announce Type: cross 
Abstract: We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion</title>
<link>https://arxiv.org/abs/2511.20821</link>
<guid>https://arxiv.org/abs/2511.20821</guid>
<content:encoded><![CDATA[
arXiv:2511.20821v1 Announce Type: cross 
Abstract: Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs</title>
<link>https://arxiv.org/abs/2511.20823</link>
<guid>https://arxiv.org/abs/2511.20823</guid>
<content:encoded><![CDATA[
arXiv:2511.20823v1 Announce Type: cross 
Abstract: Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks</title>
<link>https://arxiv.org/abs/2511.20834</link>
<guid>https://arxiv.org/abs/2511.20834</guid>
<content:encoded><![CDATA[
arXiv:2511.20834v1 Announce Type: cross 
Abstract: Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Prompting Enables More Robust, Holistic Evaluation of Language Models</title>
<link>https://arxiv.org/abs/2511.20836</link>
<guid>https://arxiv.org/abs/2511.20836</guid>
<content:encoded><![CDATA[
arXiv:2511.20836v1 Announce Type: cross 
Abstract: As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we estimate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks (+2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing reasoning (chain-of-thought) reduces LM sensitivity to prompt design (smaller {\Delta} across prompts). To our knowledge, this is the first large-scale benchmarking study to empirically characterize LM behavior across benchmarks and prompting methods, showing that scalable performance ceiling estimation enables more decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOIR 2.0: Neural Signal Operated Intelligent Robots for Everyday Activities</title>
<link>https://arxiv.org/abs/2511.20848</link>
<guid>https://arxiv.org/abs/2511.20848</guid>
<content:encoded><![CDATA[
arXiv:2511.20848v1 Announce Type: cross 
Abstract: Neural Signal Operated Intelligent Robots (NOIR) system is a versatile brain-robot interface that allows humans to control robots for daily tasks using their brain signals. This interface utilizes electroencephalography (EEG) to translate human intentions regarding specific objects and desired actions directly into commands that robots can execute. We present NOIR 2.0, an enhanced version of NOIR. NOIR 2.0 includes faster and more accurate brain decoding algorithms, which reduce task completion time by 46%. NOIR 2.0 uses few-shot robot learning algorithms to adapt to individual users and predict their intentions. The new learning algorithms leverage foundation models for more sample-efficient learning and adaptation (15 demos vs. a single demo), significantly reducing overall human time by 65%.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Length-MAX Tokenizer for Language Models</title>
<link>https://arxiv.org/abs/2511.20849</link>
<guid>https://arxiv.org/abs/2511.20849</guid>
<content:encoded><![CDATA[
arXiv:2511.20849v1 Announce Type: cross 
Abstract: We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\%, 17.2\%, and 18.5\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\%, 12.7\%, and 13.7\% lower inference latency, together with a 16\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\% and enhancing HellaSwag accuracy by 4.3\%. Moreover, the Length-MAX tokenizer achieves 99.62\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\% at inference.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Features Beat Noise: A Feature Selection Technique Through Noise-Based Hypothesis Testing</title>
<link>https://arxiv.org/abs/2511.20851</link>
<guid>https://arxiv.org/abs/2511.20851</guid>
<content:encoded><![CDATA[
arXiv:2511.20851v1 Announce Type: cross 
Abstract: Feature selection has remained a daunting challenge in machine learning and artificial intelligence, where increasingly complex, high-dimensional datasets demand principled strategies for isolating the most informative predictors. Despite widespread adoption, many established techniques suffer from notable limitations; some incur substantial computational cost, while others offer no definite statistical driven stopping criteria or assesses the significance of their importance scores. A common heuristic approach introduces multiple random noise features and retains all predictors ranked above the strongest noise feature. Although intuitive, this strategy lacks theoretical justification and depends heavily on heuristics. This paper proposes a novel feature selection method that addresses these limitations. Our approach introduces multiple random noise features and evaluates each feature's importance against the maximum importance value among these noise features incorporating a non-parametric bootstrap-based hypothesis testing framework to establish a solid theoretical foundation. We establish the conceptual soundness of our approach through statistical derivations that articulate the principles guiding the design of our algorithm. To evaluate its reliability, we generated simulated datasets under controlled statistical settings and benchmarked performance against Boruta and Knockoff-based methods, observing consistently stronger recovery of meaningful signal. As a demonstration of practical utility, we applied the technique across diverse real-world datasets, where it surpassed feature selection techniques including Boruta, RFE, and Extra Trees. Hence, the method emerges as a robust algorithm for principled feature selection, enabling the distillation of informative predictors that support reliable inference, enhanced predictive performance, and efficient computation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MODEST: Multi-Optics Depth-of-Field Stereo Dataset</title>
<link>https://arxiv.org/abs/2511.20853</link>
<guid>https://arxiv.org/abs/2511.20853</guid>
<content:encoded><![CDATA[
arXiv:2511.20853v1 Announce Type: cross 
Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A review on data fusion in multimodal learning analytics and educational data mining</title>
<link>https://arxiv.org/abs/2511.20871</link>
<guid>https://arxiv.org/abs/2511.20871</guid>
<content:encoded><![CDATA[
arXiv:2511.20871v1 Announce Type: cross 
Abstract: The new educational models such as smart learning environments use of digital and context-aware devices to facilitate the learning process. In this new educational scenario, a huge quantity of multimodal students' data from a variety of different sources can be captured, fused, and analyze. It offers to researchers and educators a unique opportunity of being able to discover new knowledge to better understand the learning process and to intervene if necessary. However, it is necessary to apply correctly data fusion approaches and techniques in order to combine various sources of multimodal learning analytics (MLA). These sources or modalities in MLA include audio, video, electrodermal activity data, eye-tracking, user logs, and click-stream data, but also learning artifacts and more natural human signals such as gestures, gaze, speech, or writing. This survey introduces data fusion in learning analytics (LA) and educational data mining (EDM) and how these data fusion techniques have been applied in smart learning. It shows the current state of the art by reviewing the main publications, the main type of fused educational data, and the data fusion approaches and techniques used in EDM/LA, as well as the main open problems, trends, and challenges in this specific research area.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size with ResNets</title>
<link>https://arxiv.org/abs/2511.20888</link>
<guid>https://arxiv.org/abs/2511.20888</guid>
<content:encoded><![CDATA[
arXiv:2511.20888v1 Announce Type: cross 
Abstract: This paper argues that DNNs implement a computational Occam's razor -- finding the `simplest' algorithm that fits the data -- and that this could explain their incredible and wide-ranging success over more traditional statistical methods. We start with the discovery that the set of real-valued function $f$ that can be $\epsilon$-approximated with a binary circuit of size at most $c\epsilon^{-\gamma}$ becomes convex in the `Harder than Monte Carlo' (HTMC) regime, when $\gamma>2$, allowing for the definition of a HTMC norm on functions. In parallel one can define a complexity measure on the parameters of a ResNets (a weighted $\ell_1$ norm of the parameters), which induce a `ResNet norm' on functions. The HTMC and ResNet norms can then be related by an almost matching sandwich bound. Thus minimizing this ResNet norm is equivalent to finding a circuit that fits the data with an almost minimal number of nodes (within a power of 2 of being optimal). ResNets thus appear as an alternative model for computation of real functions, better adapted to the HTMC regime and its convexity.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation</title>
<link>https://arxiv.org/abs/2511.20889</link>
<guid>https://arxiv.org/abs/2511.20889</guid>
<content:encoded><![CDATA[
arXiv:2511.20889v1 Announce Type: cross 
Abstract: Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Readout-Side Bypass for Residual Hybrid Quantum-Classical Models</title>
<link>https://arxiv.org/abs/2511.20922</link>
<guid>https://arxiv.org/abs/2511.20922</guid>
<content:encoded><![CDATA[
arXiv:2511.20922v1 Announce Type: cross 
Abstract: Quantum machine learning (QML) promises compact and expressive representations, but suffers from the measurement bottleneck - a narrow quantum-to-classical readout that limits performance and amplifies privacy risk. We propose a lightweight residual hybrid architecture that concatenates quantum features with raw inputs before classification, bypassing the bottleneck without increasing quantum complexity. Experiments show our model outperforms pure quantum and prior hybrid models in both centralized and federated settings. It achieves up to +55% accuracy improvement over quantum baselines, while retaining low communication cost and enhanced privacy robustness. Ablation studies confirm the effectiveness of the residual connection at the quantum-classical interface. Our method offers a practical, near-term pathway for integrating quantum models into privacy-sensitive, resource-constrained settings like federated edge learning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Vocabulary Compositional Explanations for Neuron Alignment</title>
<link>https://arxiv.org/abs/2511.20931</link>
<guid>https://arxiv.org/abs/2511.20931</guid>
<content:encoded><![CDATA[
arXiv:2511.20931v1 Announce Type: cross 
Abstract: Neurons are the fundamental building blocks of deep neural networks, and their interconnections allow AI to achieve unprecedented results. Motivated by the goal of understanding how neurons encode information, compositional explanations leverage logical relationships between concepts to express the spatial alignment between neuron activations and human knowledge. However, these explanations rely on human-annotated datasets, restricting their applicability to specific domains and predefined concepts. This paper addresses this limitation by introducing a framework for the vision domain that allows users to probe neurons for arbitrary concepts and datasets. Specifically, the framework leverages masks generated by open vocabulary semantic segmentation to compute open vocabulary compositional explanations. The proposed framework consists of three steps: specifying arbitrary concepts, generating semantic segmentation masks using open vocabulary models, and deriving compositional explanations from these masks. The paper compares the proposed framework with previous methods for computing compositional explanations both in terms of quantitative metrics and human interpretability, analyzes the differences in explanations when shifting from human-annotated data to model-annotated data, and showcases the additional capabilities provided by the framework in terms of flexibility of the explanations with respect to the tasks and properties of interest.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guaranteed Optimal Compositional Explanations for Neurons</title>
<link>https://arxiv.org/abs/2511.20934</link>
<guid>https://arxiv.org/abs/2511.20934</guid>
<content:encoded><![CDATA[
arXiv:2511.20934v1 Announce Type: cross 
Abstract: While neurons are the basic units of deep neural networks, it is still unclear what they learn and if their knowledge is aligned with that of humans. Compositional explanations aim to answer this question by describing the spatial alignment between neuron activations and concepts through logical rules. These logical descriptions are typically computed via a search over all possible concept combinations. Since computing the spatial alignment over the entire state space is computationally infeasible, the literature commonly adopts beam search to restrict the space. However, beam search cannot provide any theoretical guarantees of optimality, and it remains unclear how close current explanations are to the true optimum. In this theoretical paper, we address this gap by introducing the first framework for computing guaranteed optimal compositional explanations. Specifically, we propose: (i) a decomposition that identifies the factors influencing the spatial alignment, (ii) a heuristic to estimate the alignment at any stage of the search, and (iii) the first algorithm that can compute optimal compositional explanations within a feasible time. Using this framework, we analyze the differences between optimal and non-optimal explanations in the most popular settings for compositional explanations, the computer vision domain and Convolutional Neural Networks. In these settings, we demonstrate that 10-40 percent of explanations obtained with beam search are suboptimal when overlapping concepts are involved. Finally, we evaluate a beam-search variant guided by our proposed decomposition and heuristic, showing that it matches or improves runtime over prior methods while offering greater flexibility in hyperparameters and computational resources.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusion of classical and quantum kernels enables accurate and robust two-sample tests</title>
<link>https://arxiv.org/abs/2511.20941</link>
<guid>https://arxiv.org/abs/2511.20941</guid>
<content:encoded><![CDATA[
arXiv:2511.20941v1 Announce Type: cross 
Abstract: Two-sample tests have been extensively employed in various scientific fields and machine learning such as evaluation on the effectiveness of drugs and A/B testing on different marketing strategies to discriminate whether two sets of samples come from the same distribution or not. Kernel-based procedures for hypothetical testing have been proposed to efficiently disentangle high-dimensional complex structures in data to obtain accurate results in a model-free way by embedding the data into the reproducing kernel Hilbert space (RKHS). While the choice of kernels plays a crucial role for their performance, little is understood about how to choose kernel especially for small datasets. Here we aim to construct a hypothetical test which is effective even for small datasets, based on the theoretical foundation of kernel-based tests using maximum mean discrepancy, which is called MMD-FUSE. To address this, we enhance the MMD-FUSE framework by incorporating quantum kernels and propose a novel hybrid testing strategy that fuses classical and quantum kernels. This approach creates a powerful and adaptive test by combining the domain-specific inductive biases of classical kernels with the unique expressive power of quantum kernels. We evaluate our method on various synthetic and real-world clinical datasets, and our experiments reveal two key findings: 1) With appropriate hyperparameter tuning, MMD-FUSE with quantum kernels consistently improves test power over classical counterparts, especially for small and high-dimensional data. 2) The proposed hybrid framework demonstrates remarkable robustness, adapting to different data characteristics and achieving high test power across diverse scenarios. These results highlight the potential of quantum-inspired and hybrid kernel strategies to build more effective statistical tests, offering a versatile tool for data analysis where sample sizes are limited.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model</title>
<link>https://arxiv.org/abs/2511.20956</link>
<guid>https://arxiv.org/abs/2511.20956</guid>
<content:encoded><![CDATA[
arXiv:2511.20956v1 Announce Type: cross 
Abstract: Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification</title>
<link>https://arxiv.org/abs/2511.20960</link>
<guid>https://arxiv.org/abs/2511.20960</guid>
<content:encoded><![CDATA[
arXiv:2511.20960v1 Announce Type: cross 
Abstract: Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain. We develop a geometric framework for post-hoc calibration of neural network probability outputs, treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric. Our approach yields Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems (Proposition~1) while extending naturally to multi-class settings -- providing a principled generalization that existing methods lack. Complementing calibration, we define geometric reliability scores based on Fisher--Rao distance and construct neutral zones for principled deferral of uncertain predictions.
  Theoretical contributions include: (i) consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and (ii) tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework (calibration followed by reliability-based deferral) captures 72.5\% of errors while deferring 34.5\% of samples. Notably, this operational gain is achievable with any well-calibrated probability output; the contribution of geometric calibration lies in its theoretical foundations rather than empirical superiority over simpler alternatives. This work bridges information geometry and statistical learning, offering formal guarantees relevant to applications requiring rigorous validation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crowdsourcing the Frontier: Advancing Hybrid Physics-ML Climate Simulation via $50,000 Kaggle Competition</title>
<link>https://arxiv.org/abs/2511.20963</link>
<guid>https://arxiv.org/abs/2511.20963</guid>
<content:encoded><![CDATA[
arXiv:2511.20963v1 Announce Type: cross 
Abstract: Subgrid machine-learning (ML) parameterizations have the potential to introduce a new generation of climate models that incorporate the effects of higher-resolution physics without incurring the prohibitive computational cost associated with more explicit physics-based simulations. However, important issues, ranging from online instability to inconsistent online performance, have limited their operational use for long-term climate projections. To more rapidly drive progress in solving these issues, domain scientists and machine learning researchers opened up the offline aspect of this problem to the broader machine learning and data science community with the release of ClimSim, a NeurIPS Datasets and Benchmarks publication, and an associated Kaggle competition. This paper reports on the downstream results of the Kaggle competition by coupling emulators inspired by the winning teams' architectures to an interactive climate model (including full cloud microphysics, a regime historically prone to online instability) and systematically evaluating their online performance. Our results demonstrate that online stability in the low-resolution, real-geography setting is reproducible across multiple diverse architectures, which we consider a key milestone. All tested architectures exhibit strikingly similar offline and online biases, though their responses to architecture-agnostic design choices (e.g., expanding the list of input variables) can differ significantly. Multiple Kaggle-inspired architectures achieve state-of-the-art (SOTA) results on certain metrics such as zonal mean bias patterns and global RMSE, indicating that crowdsourcing the essence of the offline problem is one path to improving online performance in hybrid physics-AI climate simulation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data</title>
<link>https://arxiv.org/abs/2511.20974</link>
<guid>https://arxiv.org/abs/2511.20974</guid>
<content:encoded><![CDATA[
arXiv:2511.20974v1 Announce Type: cross 
Abstract: The scarcity of parallel speech corpora critically hampers speech-to-speech translation (S2ST), often forcing reliance on complex, multi-stage pipelines. This paper introduces RosettaSpeech, a novel and simplified framework for zero-shot S2ST that is trained on monolingual speech-text data augmented by machine translation supervision. While our method leverages the linguistic knowledge inherent in text-based NMT models, it strictly eliminates the need for parallel speech-to-speech pairs. Our model uniquely uses text as an intermediate bridge during training but functions as a direct, end-to-end speech-to-speech model at inference. This streamlined approach achieves state-of-the-art results on standard benchmarks. For instance, on the CVSS-C test set, RosettaSpeech outperforms leading systems, achieving an ASR-BLEU score of 25.17 for German-to-English and 29.86 for Spanish-to-English-relative gains of over 27% and 14%, respectively. Furthermore, we demonstrate that a single model can deliver strong many-to-one translation performance (FR/ES/DE -> EN). We also provide a foundational analysis of how training data scaling impacts model performance. By prioritizing reliance on abundant parallel text rather than difficult-to-acquire parallel speech, RosettaSpeech offers a scalable path to creating high-quality, speaker-preserving S2ST for a much broader array of languages.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Independent policy gradient-based reinforcement learning for economic and reliable energy management of multi-microgrid systems</title>
<link>https://arxiv.org/abs/2511.20977</link>
<guid>https://arxiv.org/abs/2511.20977</guid>
<content:encoded><![CDATA[
arXiv:2511.20977v1 Announce Type: cross 
Abstract: Efficiency and reliability are both crucial for energy management, especially in multi-microgrid systems (MMSs) integrating intermittent and distributed renewable energy sources. This study investigates an economic and reliable energy management problem in MMSs under a distributed scheme, where each microgrid independently updates its energy management policy in a decentralized manner to optimize the long-term system performance collaboratively. We introduce the mean and variance of the exchange power between the MMS and the main grid as indicators for the economic performance and reliability of the system. Accordingly, we formulate the energy management problem as a mean-variance team stochastic game (MV-TSG), where conventional methods based on the maximization of expected cumulative rewards are unsuitable for variance metrics. To solve MV-TSGs, we propose a fully distributed independent policy gradient algorithm, with rigorous convergence analysis, for scenarios with known model parameters. For large-scale scenarios with unknown model parameters, we further develop a deep reinforcement learning algorithm based on independent policy gradients, enabling data-driven policy optimization. Numerical experiments in two scenarios validate the effectiveness of the proposed methods. Our approaches fully leverage the distributed computational capabilities of MMSs and achieve a well-balanced trade-off between economic performance and operational reliability.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Even with AI, Bijection Discovery is Still Hard: The Opportunities and Challenges of OpenEvolve for Novel Bijection Construction</title>
<link>https://arxiv.org/abs/2511.20987</link>
<guid>https://arxiv.org/abs/2511.20987</guid>
<content:encoded><![CDATA[
arXiv:2511.20987v1 Announce Type: cross 
Abstract: Evolutionary program synthesis systems such as AlphaEvolve, OpenEvolve, and ShinkaEvolve offer a new approach to AI-assisted mathematical discovery. These systems utilize teams of large language models (LLMs) to generate candidate solutions to a problem as human readable code. These candidate solutions are then 'evolved' with the goal of improving them beyond what an LLM can produce in a single shot. While existing mathematical applications have mostly focused on problems of establishing bounds (e.g., sphere packing), the program synthesis approach is well suited to any problem where the solution takes the form of an explicit construction. With this in mind, in this paper we explore the use of OpenEvolve for combinatorial bijection discovery. We describe the results of applying OpenEvolve to three bijection construction problems involving Dyck paths, two of which are known and one of which is open. We find that while systems like OpenEvolve show promise as a valuable tool for combinatorialists, the problem of finding novel, research-level bijections remains a challenging task for current frontier systems, reinforcing the need for human mathematicians in the loop. We describe some lessons learned for others in the field interested in exploring the use of these systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wavefront-Constrained Passive Obscured Object Detection</title>
<link>https://arxiv.org/abs/2511.20991</link>
<guid>https://arxiv.org/abs/2511.20991</guid>
<content:encoded><![CDATA[
arXiv:2511.20991v1 Announce Type: cross 
Abstract: Accurately localizing and segmenting obscured objects from faint light patterns beyond the field of view is highly challenging due to multiple scattering and medium-induced perturbations. Most existing methods, based on real-valued modeling or local convolutional operations, are inadequate for capturing the underlying physics of coherent light propagation. Moreover, under low signal-to-noise conditions, these methods often converge to non-physical solutions, severely compromising the stability and reliability of the observation. To address these challenges, we propose a novel physics-driven Wavefront Propagating Compensation Network (WavePCNet) to simulate wavefront propagation and enhance the perception of obscured objects. This WavePCNet integrates the Tri-Phase Wavefront Complex-Propagation Reprojection (TriWCP) to incorporate complex amplitude transfer operators to precisely constrain coherent propagation behavior, along with a momentum memory mechanism to effectively suppress the accumulation of perturbations. Additionally, a High-frequency Cross-layer Compensation Enhancement is introduced to construct frequency-selective pathways with multi-scale receptive fields and dynamically model structural consistency across layers, further boosting the model's robustness and interpretability under complex environmental conditions. Extensive experiments conducted on four physically collected datasets demonstrate that WavePCNet consistently outperforms state-of-the-art methods across both accuracy and robustness.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels</title>
<link>https://arxiv.org/abs/2511.21038</link>
<guid>https://arxiv.org/abs/2511.21038</guid>
<content:encoded><![CDATA[
arXiv:2511.21038v1 Announce Type: cross 
Abstract: Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \emph{natural} demonstrations (with correct labels) and \emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/semantic-anchors-icl.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Assessment of Concrete Slab Integrity via Impact-Echo Signals and Neural Networks</title>
<link>https://arxiv.org/abs/2511.21080</link>
<guid>https://arxiv.org/abs/2511.21080</guid>
<content:encoded><![CDATA[
arXiv:2511.21080v1 Announce Type: cross 
Abstract: Subsurface defects such as delamination, voids, and honeycombing critically affect the durability of concrete bridge decks but are difficult to detect reliably using visual inspection or manual sounding. This paper presents a machine learning based Impact Echo (IE) framework that automates both defect localization and multi-class classification of common concrete defects. Raw IE signals from Federal Highway Administration (FHWA) laboratory slabs and in-service bridge decks are transformed via Fast Fourier Transform (FFT) into dominant peak-frequency features and interpolated into spatial maps for defect zone visualization. Unsupervised k-means clustering highlights low-frequency, defect-prone regions, while Ground Truth Masks (GTMs) derived from seeded lab defects are used to validate spatial accuracy and generate high-confidence training labels. From these validated regions, spatially ordered peak-frequency sequences are constructed and fed into a stacked Long Short-Term Memory (LSTM) network that classifies four defect types shallow delamination, deep delamination, voids, and honeycombing with 73% overall accuracy. Field validation on the bridge deck demonstrates that models trained on laboratory data generalize under realistic coupling, noise, and environmental variability. The proposed framework enhances the objectivity, scalability, and repeatability of Non-Destructive Evaluation (NDE), supporting intelligent, data-driven bridge health monitoring at a network scale.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning</title>
<link>https://arxiv.org/abs/2511.21081</link>
<guid>https://arxiv.org/abs/2511.21081</guid>
<content:encoded><![CDATA[
arXiv:2511.21081v1 Announce Type: cross 
Abstract: In low-resource languages like Burmese, classification tasks often fine-tune only the final classification layer, keeping pre-trained encoder weights frozen. While Multi-Layer Perceptrons (MLPs) are commonly used, their fixed non-linearity can limit expressiveness and increase computational cost. This work explores Kolmogorov-Arnold Networks (KANs) as alternative classification heads, evaluating Fourier-based FourierKAN, Spline-based EfficientKAN, and Grid-based FasterKAN-across diverse embeddings including TF-IDF, fastText, and multilingual transformers (mBERT, Distil-mBERT). Experimental results show that KAN-based heads are competitive with or superior to MLPs. EfficientKAN with fastText achieved the highest F1-score (0.928), while FasterKAN offered the best trade-off between speed and accuracy. On transformer embeddings, EfficientKAN matched or slightly outperformed MLPs with mBERT (0.917 F1). These findings highlight KANs as expressive, efficient alternatives to MLPs for low-resource language classification.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features</title>
<link>https://arxiv.org/abs/2511.21088</link>
<guid>https://arxiv.org/abs/2511.21088</guid>
<content:encoded><![CDATA[
arXiv:2511.21088v1 Announce Type: cross 
Abstract: This paper investigates sequence-to-sequence Transformer models for automatic speech recognition (ASR) error correction in low-resource Burmese, focusing on different feature integration strategies including IPA and alignment information. To our knowledge, this is the first study addressing ASR error correction specifically for Burmese. We evaluate five ASR backbones and show that our ASR Error Correction (AEC) approaches consistently improve word- and character-level accuracy over baseline outputs. The proposed AEC model, combining IPA and alignment features, reduced the average WER of ASR models from 51.56 to 39.82 before augmentation (and 51.56 to 43.59 after augmentation) and improving chrF++ scores from 0.5864 to 0.627, demonstrating consistent gains over the baseline ASR outputs without AEC. Our results highlight the robustness of AEC and the importance of feature design for improving ASR outputs in low-resource settings.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing</title>
<link>https://arxiv.org/abs/2511.21101</link>
<guid>https://arxiv.org/abs/2511.21101</guid>
<content:encoded><![CDATA[
arXiv:2511.21101v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&amp;A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&amp;A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&amp;A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms</title>
<link>https://arxiv.org/abs/2511.21115</link>
<guid>https://arxiv.org/abs/2511.21115</guid>
<content:encoded><![CDATA[
arXiv:2511.21115v1 Announce Type: cross 
Abstract: This paper investigates the partial linear model by Least Absolute Deviation (LAD) regression. We parameterize the nonparametric term using Deep Neural Networks (DNNs) and formulate a penalized LAD problem for estimation. Specifically, our model exhibits the following challenges. First, the regularization term can be nonconvex and nonsmooth, necessitating the introduction of infinite dimensional variational analysis and nonsmooth analysis into the asymptotic normality discussion. Second, our network must expand (in width, sparsity level and depth) as more samples are observed, thereby introducing additional difficulties for theoretical analysis. Third, the oracle of the proposed estimator is itself defined through a ultra high-dimensional, nonconvex, and discontinuous optimization problem, which already entails substantial computational and theoretical challenges. Under such the challenges, we establish the consistency, convergence rate, and asymptotic normality of the estimator. Furthermore, we analyze the oracle problem itself and its continuous relaxation. We study the convergence of a proximal subgradient method for both formulations, highlighting their structural differences lead to distinct computational subproblems along the iterations. In particular, the relaxed formulation admits significantly cheaper proximal updates, reflecting an inherent trade-off between statistical accuracy and computational tractability.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lattice-to-total thermal conductivity ratio: a phonon-glass electron-crystal descriptor for data-driven thermoelectric design</title>
<link>https://arxiv.org/abs/2511.21213</link>
<guid>https://arxiv.org/abs/2511.21213</guid>
<content:encoded><![CDATA[
arXiv:2511.21213v1 Announce Type: cross 
Abstract: Thermoelectrics (TEs) are promising candidates for energy harvesting with performance quantified by figure of merit, $ZT$. To accelerate the discovery of high-$ZT$ materials, efforts have focused on identifying compounds with low thermal conductivity $\kappa$. Using a curated dataset of 71,913 entries, we show that high-$ZT$ materials reside not only in the low-$\kappa$ regime but also cluster near a lattice-to-total thermal conductivity ratio ($\kappa_\mathrm{L}/\kappa$) of approximately 0.5, consistent with the phonon-glass electron-crystal design concept. Building on this insight, we construct a framework consisting of two machine learning models for the lattice and electronic components of thermal conductivity that jointly provide both $\kappa$ and $\kappa_\mathrm{L}/\kappa$ for screening and guiding the optimization of TE materials. Among 104,567 compounds screened, our models identify 2,522 ultralow-$\kappa$ candidates. Follow-up case studies demonstrate that this framework can reliably provide optimization strategies by suggesting new dopants and alloys that shift pristine materials toward the $\kappa_\mathrm{L}/\kappa$ approaching 0.5 regime. Ultimately, by integrating rapid screening with PGEC-guided optimization, our data-driven framework effectively bridges the critical gap between materials discovery and performance enhancement.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting</title>
<link>https://arxiv.org/abs/2511.21215</link>
<guid>https://arxiv.org/abs/2511.21215</guid>
<content:encoded><![CDATA[
arXiv:2511.21215v1 Announce Type: cross 
Abstract: We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (<1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maxitive Donsker-Varadhan Formulation for Possibilistic Variational Inference</title>
<link>https://arxiv.org/abs/2511.21223</link>
<guid>https://arxiv.org/abs/2511.21223</guid>
<content:encoded><![CDATA[
arXiv:2511.21223v1 Announce Type: cross 
Abstract: Variational inference (VI) is a cornerstone of modern Bayesian learning, enabling approximate inference in complex models that would otherwise be intractable. However, its formulation depends on expectations and divergences defined through high-dimensional integrals, often rendering analytical treatment impossible and necessitating heavy reliance on approximate learning and inference techniques. Possibility theory, an imprecise probability framework, allows to directly model epistemic uncertainty instead of leveraging subjective probabilities. While this framework provides robustness and interpretability under sparse or imprecise information, adapting VI to the possibilistic setting requires rethinking core concepts such as entropy and divergence, which presuppose additivity. In this work, we develop a principled formulation of possibilistic variational inference and apply it to a special class of exponential-family functions, highlighting parallels with their probabilistic counterparts and revealing the distinctive mathematical structures of possibility theory.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI</title>
<link>https://arxiv.org/abs/2511.21232</link>
<guid>https://arxiv.org/abs/2511.21232</guid>
<content:encoded><![CDATA[
arXiv:2511.21232v1 Announce Type: cross 
Abstract: The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Spheres Dataset: Multitrack Orchestral Recordings for Music Source Separation and Information Retrieval</title>
<link>https://arxiv.org/abs/2511.21247</link>
<guid>https://arxiv.org/abs/2511.21247</guid>
<content:encoded><![CDATA[
arXiv:2511.21247v1 Announce Type: cross 
Abstract: This paper introduces The Spheres dataset, multitrack orchestral recordings designed to advance machine learning research in music source separation and related MIR tasks within the classical music domain. The dataset is composed of over one hour recordings of musical pieces performed by the Colibr\`i Ensemble at The Spheres recording studio, capturing two canonical works - Tchaikovsky's Romeo and Juliet and Mozart's Symphony No. 40 - along with chromatic scales and solo excerpts for each instrument. The recording setup employed 23 microphones, including close spot, main, and ambient microphones, enabling the creation of realistic stereo mixes with controlled bleeding and providing isolated stems for supervised training of source separation models. In addition, room impulse responses were estimated for each instrument position, offering valuable acoustic characterization of the recording space. We present the dataset structure, acoustic analysis, and baseline evaluations using X-UMX based models for orchestral family separation and microphone debleeding. Results highlight both the potential and the challenges of source separation in complex orchestral scenarios, underscoring the dataset's value for benchmarking and for exploring new approaches to separation, localization, dereverberation, and immersive rendering of classical music.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimation in high-dimensional linear regression: Post-Double-Autometrics as an alternative to Post-Double-Lasso</title>
<link>https://arxiv.org/abs/2511.21257</link>
<guid>https://arxiv.org/abs/2511.21257</guid>
<content:encoded><![CDATA[
arXiv:2511.21257v1 Announce Type: cross 
Abstract: Post-Double-Lasso is becoming the most popular method for estimating linear regression models with many covariates when the purpose is to obtain an accurate estimate of a parameter of interest, such as an average treatment effect. However, this method can suffer from substantial omitted variable bias in finite sample. We propose a new method called Post-Double-Autometrics, which is based on Autometrics, and show that this method outperforms Post-Double-Lasso. Its use in a standard application of economic growth sheds new light on the hypothesis of convergence from poor to rich economies.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Periodic Orbits of the Dual Logarithmic Derivative Operator</title>
<link>https://arxiv.org/abs/2511.21283</link>
<guid>https://arxiv.org/abs/2511.21283</guid>
<content:encoded><![CDATA[
arXiv:2511.21283v1 Announce Type: cross 
Abstract: We study the periodic behaviour of the dual logarithmic derivative operator $\mathcal{A}[f]=\mathrm{d}\ln f/\mathrm{d}\ln x$ in a complex analytic setting. We show that $\mathcal{A}$ admits genuinely nondegenerate period-$2$ orbits and identify a canonical explicit example. Motivated by this, we obtain a complete classification of all nondegenerate period-$2$ solutions, which are precisely the rational pairs $(c a x^{c}/(1-ax^{c}),\, c/(1-ax^{c}))$ with $ac\neq 0$. We further classify all fixed points of $\mathcal{A}$, showing that every solution of $\mathcal{A}[f]=f$ has the form $f(x)=1/(a-\ln x)$. As an illustration, logistic-type functions become pre-periodic under $\mathcal{A}$ after a logarithmic change of variables, entering the period-$2$ family in one iterate. These results give an explicit description of the low-period structure of $\mathcal{A}$ and provide a tractable example of operator-induced dynamics on function spaces.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase-Aware Code-Aided EM Algorithm for Blind Channel Estimation in PSK-Modulated OFDM</title>
<link>https://arxiv.org/abs/2511.21340</link>
<guid>https://arxiv.org/abs/2511.21340</guid>
<content:encoded><![CDATA[
arXiv:2511.21340v1 Announce Type: cross 
Abstract: This paper presents a fully blind phase-aware expectation-maximization (EM) algorithm for OFDM systems with the phase-shift keying (PSK) modulation. We address the well-known local maximum problem of the EM algorithm for blind channel estimation. This is primarily caused by the unknown phase ambiguity in the channel estimates, which conventional blind EM estimators cannot resolve. To overcome this limitation, we propose to exploit the extrinsic information from the decoder as model evidence metrics. A finite set of candidate models is generated based on the inherent symmetries of PSK modulation, and the decoder selects the most likely candidate model. Simulation results demonstrate that, when combined with a simple convolutional code, the phase-aware EM algorithm reliably resolves phase ambiguity during the initialization stage and reduces the local convergence rate from 80% to nearly 0% in frequency-selective channels with a constant phase ambiguity. The algorithm is invoked only once after the EM initialization stage, resulting in negligible additional complexity during subsequent turbo iterations.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Multi-Order Block Structure in Higher-Order Networks</title>
<link>https://arxiv.org/abs/2511.21350</link>
<guid>https://arxiv.org/abs/2511.21350</guid>
<content:encoded><![CDATA[
arXiv:2511.21350v1 Announce Type: cross 
Abstract: Higher-order networks, naturally described as hypergraphs, are essential for modeling real-world systems involving interactions among three or more entities. Stochastic block models offer a principled framework for characterizing mesoscale organization, yet their extension to hypergraphs involves a trade-off between expressive power and computational complexity. A recent simplification, a single-order model, mitigates this complexity by assuming a single affinity pattern governs interactions of all orders. This universal assumption, however, may overlook order-dependent structural details. Here, we propose a framework that relaxes this assumption by introducing a multi-order block structure, in which different affinity patterns govern distinct subsets of interaction orders. Our framework is based on a multi-order stochastic block model and searches for the optimal partition of the set of interaction orders that maximizes out-of-sample hyperlink prediction performance. Analyzing a diverse range of real-world networks, we find that multi-order block structures are prevalent. Accounting for them not only yields better predictive performance over the single-order model but also uncovers sharper, more interpretable mesoscale organization. Our findings reveal that order-dependent mechanisms are a key feature of the mesoscale organization of real-world higher-order networks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Physics-Neural Models enable Learning of Non-Markovian Closures for Accelerated Coarse-Grained Physics Simulations</title>
<link>https://arxiv.org/abs/2511.21369</link>
<guid>https://arxiv.org/abs/2511.21369</guid>
<content:encoded><![CDATA[
arXiv:2511.21369v1 Announce Type: cross 
Abstract: Numerical simulations provide key insights into many physical, real-world problems. However, while these simulations are solved on a full 3D domain, most analysis only require a reduced set of metrics (e.g. plane-level concentrations). This work presents a hybrid physics-neural model that predicts scalar transport in a complex domain orders of magnitude faster than the 3D simulation (from hours to less than 1 min). This end-to-end differentiable framework jointly learns the physical model parameterization (i.e. orthotropic diffusivity) and a non-Markovian neural closure model to capture unresolved, 'coarse-grained' effects, thereby enabling stable, long time horizon rollouts. This proposed model is data-efficient (learning with 26 training data), and can be flexibly extended to an out-of-distribution scenario (with a moving source), achieving a Spearman correlation coefficient of 0.96 at the final simulation time. Overall results show that this differentiable physics-neural framework enables fast, accurate, and generalizable coarse-grained surrogates for physical phenomena.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis</title>
<link>https://arxiv.org/abs/2511.21397</link>
<guid>https://arxiv.org/abs/2511.21397</guid>
<content:encoded><![CDATA[
arXiv:2511.21397v1 Announce Type: cross 
Abstract: How does irrelevant information (i.e., distractors) affect test-time scaling in vision-language models (VLMs)? Prior studies on language models have reported an inverse scaling effect, where textual distractors lead to longer but less effective reasoning. To investigate whether similar phenomena occur in multimodal settings, we introduce Idis (Images with distractors), a visual question-answering dataset that systematically varies distractors along semantic, numerical, and spatial dimensions. Our analyses reveal that visual distractors differ fundamentally from textual ones: although inverse scaling persists, adding visual distractors reduces accuracy without increasing reasoning length. We further show that tracking attribute counts within reasoning traces provides key insights into how distractors, reasoning length, and accuracy interact. Finally, we demonstrate that these trends extend to established visual bias benchmarks such as Waterbirds, and we propose a simple prompting strategy to mitigate bias-driven predictions in reasoning models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning</title>
<link>https://arxiv.org/abs/2511.21416</link>
<guid>https://arxiv.org/abs/2511.21416</guid>
<content:encoded><![CDATA[
arXiv:2511.21416v1 Announce Type: cross 
Abstract: Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs--limited by over-smoothing and hop-dependent diffusion--or employ Transformers that overlook graph topology and treat nodes as isolated sequences. We propose Odin (Oriented Dual-module INtegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism.Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model's semantic hierarchy. Because aggregation operates on the global [CLS] representation, Odin fundamentally avoids over-smoothing and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin's expressive power strictly contains that of both pure Transformers and GNNs.To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure-text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Study of Model Merging Techniques in Large Language Models</title>
<link>https://arxiv.org/abs/2511.21437</link>
<guid>https://arxiv.org/abs/2511.21437</guid>
<content:encoded><![CDATA[
arXiv:2511.21437v1 Announce Type: cross 
Abstract: Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Going with the Speed of Sound: Pushing Neural Surrogates into Highly-turbulent Transonic Regimes</title>
<link>https://arxiv.org/abs/2511.21474</link>
<guid>https://arxiv.org/abs/2511.21474</guid>
<content:encoded><![CDATA[
arXiv:2511.21474v1 Announce Type: cross 
Abstract: The widespread use of neural surrogates in automotive aerodynamics, enabled by datasets such as DrivAerML and DrivAerNet++, has primarily focused on bluff-body flows with large wakes. Extending these methods to aerospace, particularly in the transonic regime, remains challenging due to the high level of non-linearity of compressible flows and 3D effects such as wingtip vortices. Existing aerospace datasets predominantly focus on 2D airfoils, neglecting these critical 3D phenomena. To address this gap, we present a new dataset of CFD simulations for 3D wings in the transonic regime. The dataset comprises volumetric and surface-level fields for around $30,000$ samples with unique geometry and inflow conditions. This allows computation of lift and drag coefficients, providing a foundation for data-driven aerodynamic optimization of the drag-lift Pareto front. We evaluate several state-of-the-art neural surrogates on our dataset, including Transolver and AB-UPT, focusing on their out-of-distribution (OOD) generalization over geometry and inflow variations. AB-UPT demonstrates strong performance for transonic flowfields and reproduces physically consistent drag-lift Pareto fronts even for unseen wing configurations. Our results demonstrate that AB-UPT can approximate drag-lift Pareto fronts for unseen geometries, highlighting its potential as an efficient and effective tool for rapid aerodynamic design exploration. To facilitate future research, we open-source our dataset at https://huggingface.co/datasets/EmmiAI/Emmi-Wing.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning</title>
<link>https://arxiv.org/abs/2511.21490</link>
<guid>https://arxiv.org/abs/2511.21490</guid>
<content:encoded><![CDATA[
arXiv:2511.21490v1 Announce Type: cross 
Abstract: We present a novel training approach, named Merge-and-Bound (M&amp;B) for Class Incremental Learning (CIL), which directly manipulates model weights in the parameter space for optimization. Our algorithm involves two types of weight merging: inter-task weight merging and intra-task weight merging. Inter-task weight merging unifies previous models by averaging the weights of models from all previous stages. On the other hand, intra-task weight merging facilitates the learning of current task by combining the model parameters within current stage. For reliable weight merging, we also propose a bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks; this strategy reveals that it is possible to effectively obtain new models near old ones, reducing catastrophic forgetting. M&amp;B is seamlessly integrated into existing CIL methods without modifying architecture components or revising learning objectives. We extensively evaluate our algorithm on standard CIL benchmarks and demonstrate superior performance compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities (II)</title>
<link>https://arxiv.org/abs/2511.21526</link>
<guid>https://arxiv.org/abs/2511.21526</guid>
<content:encoded><![CDATA[
arXiv:2511.21526v1 Announce Type: cross 
Abstract: A fundamental theoretical question in network analysis is to determine under which conditions community recovery is possible in polynomial time in the Stochastic Block Model (SBM). When the number $K$ of communities remains smaller than $\sqrt{n}$ --where $n$ denotes the number of nodes--, non-trivial community recovery is possible in polynomial time above, and only above, the Kesten--Stigum (KS) threshold, originally postulated using arguments from statistical physics.
  When $K \geq \sqrt{n}$, Chin, Mossel, Sohn, and Wein recently proved that, in the \emph{sparse regime}, community recovery in polynomial time is achievable below the KS threshold by counting non-backtracking paths. This finding led them to postulate a new threshold for the many-communities regime $K \geq \sqrt{n}$. Subsequently, Carpentier, Giraud, and Verzelen established the failure of low-degree polynomials below this new threshold across all density regimes, and demonstrated successful recovery above the threshold in certain moderately sparse settings. While these results provide strong evidence that, in the many community setting, the computational barrier lies at the threshold proposed in~Chin et al., the question of achieving recovery above this threshold still remains open in most density regimes.
  The present work is a follow-up to~Carpentier et al., in which we prove Conjecture~1.4 stated therein by: \\ 1- Constructing a family of motifs satisfying specific structural properties; and\\ 2- Proving that community recovery is possible above the proposed threshold by counting such motifs.\\ Our results complete the picture of the computational barrier for community recovery in the SBM with $K \geq \sqrt{n}$ communities. They also indicate that, in moderately sparse regimes, the optimal algorithms appear to be fundamentally different from spectral methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMA: A Momentum Mamba Architecture for Human Activity Recognition with Inertial Sensors</title>
<link>https://arxiv.org/abs/2511.21550</link>
<guid>https://arxiv.org/abs/2511.21550</guid>
<content:encoded><![CDATA[
arXiv:2511.21550v1 Announce Type: cross 
Abstract: Human activity recognition (HAR) from inertial sensors is essential for ubiquitous computing, mobile health, and ambient intelligence. Conventional deep models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and transformers have advanced HAR but remain limited by vanishing or exloding gradients, high computational cost, and difficulty in capturing long-range dependencies. Structured state-space models (SSMs) like Mamba address these challenges with linear complexity and effective temporal modeling, yet they are restricted to first-order dynamics without stable longterm memory mechanisms. We introduce Momentum Mamba, a momentum-augmented SSM that incorporates second-order dynamics to improve stability of information flow across time steps, robustness, and long-sequence modeling. Two extensions further expand its capacity: Complex Momentum Mamba for frequency-selective memory scaling. Experiments on multiple HAR benchmarks demonstrate consistent gains over vanilla Mamba and Transformer baselines in accuracy, robustness, and convergence speed. With only moderate increases in training cost, momentum-augmented SSMs offer a favorable accuracy-efficiency balance, establishing them as a scalable paradigm for HAR and a promising principal framework for broader sequence modeling applications.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAB-DRW: A DFT-based Robust Watermark for Generative Tabular Data</title>
<link>https://arxiv.org/abs/2511.21600</link>
<guid>https://arxiv.org/abs/2511.21600</guid>
<content:encoded><![CDATA[
arXiv:2511.21600v1 Announce Type: cross 
Abstract: The rise of generative AI has enabled the production of high-fidelity synthetic tabular data across fields such as healthcare, finance, and public policy, raising growing concerns about data provenance and misuse. Watermarking offers a promising solution to address these concerns by ensuring the traceability of synthetic data, but existing methods face many limitations: they are computationally expensive due to reliance on large diffusion models, struggle with mixed discrete-continuous data, or lack robustness to post-modifications. To address them, we propose TAB-DRW, an efficient and robust post-editing watermarking scheme for generative tabular data. TAB-DRW embeds watermark signals in the frequency domain: it normalizes heterogeneous features via the Yeo-Johnson transformation and standardization, applies the discrete Fourier transform (DFT), and adjusts the imaginary parts of adaptively selected entries according to precomputed pseudorandom bits. To further enhance robustness and efficiency, we introduce a novel rank-based pseudorandom bit generation method that enables row-wise retrieval without incurring storage overhead. Experiments on five benchmark tabular datasets show that TAB-DRW achieves strong detectability and robustness against common post-processing attacks, while preserving high data fidelity and fully supporting mixed-type features.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation</title>
<link>https://arxiv.org/abs/2511.21607</link>
<guid>https://arxiv.org/abs/2511.21607</guid>
<content:encoded><![CDATA[
arXiv:2511.21607v1 Announce Type: cross 
Abstract: Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining</title>
<link>https://arxiv.org/abs/2511.21613</link>
<guid>https://arxiv.org/abs/2511.21613</guid>
<content:encoded><![CDATA[
arXiv:2511.21613v1 Announce Type: cross 
Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Error Correction on Low-Resource Devices</title>
<link>https://arxiv.org/abs/2511.21652</link>
<guid>https://arxiv.org/abs/2511.21652</guid>
<content:encoded><![CDATA[
arXiv:2511.21652v1 Announce Type: cross 
Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Evolution-Based Models for Experimentation Under Interference</title>
<link>https://arxiv.org/abs/2511.21675</link>
<guid>https://arxiv.org/abs/2511.21675</guid>
<content:encoded><![CDATA[
arXiv:2511.21675v1 Announce Type: cross 
Abstract: Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic Learner with Grow-and-Refine Multimodal Semantic Memory</title>
<link>https://arxiv.org/abs/2511.21678</link>
<guid>https://arxiv.org/abs/2511.21678</guid>
<content:encoded><![CDATA[
arXiv:2511.21678v1 Announce Type: cross 
Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework</title>
<link>https://arxiv.org/abs/2511.21686</link>
<guid>https://arxiv.org/abs/2511.21686</guid>
<content:encoded><![CDATA[
arXiv:2511.21686v1 Announce Type: cross 
Abstract: Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration</title>
<link>https://arxiv.org/abs/2511.21689</link>
<guid>https://arxiv.org/abs/2511.21689</guid>
<content:encoded><![CDATA[
arXiv:2511.21689v1 Announce Type: cross 
Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</title>
<link>https://arxiv.org/abs/2511.21690</link>
<guid>https://arxiv.org/abs/2511.21690</guid>
<content:encoded><![CDATA[
arXiv:2511.21690v1 Announce Type: cross 
Abstract: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Balancing for Multi-Task Learning</title>
<link>https://arxiv.org/abs/2308.12029</link>
<guid>https://arxiv.org/abs/2308.12029</guid>
<content:encoded><![CDATA[
arXiv:2308.12029v3 Announce Type: replace 
Abstract: Multi-task learning aims to learn multiple related tasks simultaneously and has achieved great success in various fields. However, the disparity in loss and gradient scales among tasks often leads to performance compromises, and the balancing of tasks remains a significant challenge. In this paper, we propose Dual-Balancing Multi-Task Learning (DB-MTL) to achieve task balancing from both the loss and gradient perspectives. Specifically, DB-MTL achieves loss-scale balancing by performing logarithm transformation on each task loss, and rescales gradient magnitudes by normalizing all task gradients to comparable magnitudes using the maximum gradient norm. Extensive experiments on a number of benchmark datasets demonstrate that DB-MTL consistently performs better than the current state-of-the-art.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Differentially Private Federated Learning for Speech Recognition: Benchmarks, Adaptive Optimizers and Gradient Clipping</title>
<link>https://arxiv.org/abs/2310.00098</link>
<guid>https://arxiv.org/abs/2310.00098</guid>
<content:encoded><![CDATA[
arXiv:2310.00098v4 Announce Type: replace 
Abstract: While federated learning (FL) and differential privacy (DP) have been extensively studied, their application to automatic speech recognition (ASR) remains largely unexplored due to the challenges in training large transformer models. Specifically, large models further exacerbate issues in FL as they are particularly susceptible to gradient heterogeneity across layers, unlike the relatively uniform gradient behavior observed in shallow models. As a result, prior works struggle to converge with standard optimization techniques, even in the absence of DP mechanisms. To the best of our knowledge, no existing work establishes a competitive, practical recipe for FL with DP in the context of ASR. To address this gap, we establish \textbf{the first benchmark for FL with DP in end-to-end ASR}. Our approach centers on per-layer clipping and layer-wise gradient normalization: theoretical analysis reveals that these techniques together mitigate clipping bias and gradient heterogeneity across layers in deeper models. Consistent with these theoretical insights, our empirical results show that FL with DP is viable under strong privacy guarantees, provided a population of at least several million users. Specifically, we achieve user-level (7.2, $10^{-9}$)-DP (resp. (4.5, $10^{-9}$)-DP) with only a 1.3% (resp. 4.6%) absolute drop in word error rate when extrapolating to high (resp. low) population scales for FL with DP in ASR. Although our experiments focus on ASR, the underlying principles we uncover - particularly those concerning gradient heterogeneity and layer-wise gradient normalization - offer broader guidance for designing scalable, privacy-preserving FL algorithms for large models across domains. Code of all experiments and benchmarks is available at https://github.com/apple/ml-pfl4asr.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices</title>
<link>https://arxiv.org/abs/2311.01759</link>
<guid>https://arxiv.org/abs/2311.01759</guid>
<content:encoded><![CDATA[
arXiv:2311.01759v3 Announce Type: replace 
Abstract: Developing deep learning models on tiny devices (e.g. Microcontroller units, MCUs) has attracted much attention in various embedded IoT applications. However, it is challenging to efficiently design and deploy recent advanced models (e.g. transformers) on tiny devices due to their severe hardware resource constraints. In this work, we propose TinyFormer, a framework specifically designed to develop and deploy resource-efficient transformer models on MCUs. TinyFormer consists of SuperNAS, SparseNAS, and SparseEngine. Separately, SuperNAS aims to search for an appropriate supernet from a vast search space. SparseNAS evaluates the best sparse single-path transformer model from the identified supernet. Finally, SparseEngine efficiently deploys the searched sparse models onto MCUs. To the best of our knowledge, SparseEngine is the first deployment framework capable of performing inference of sparse transformer models on MCUs. Evaluation results on the CIFAR-10 dataset demonstrate that TinyFormer can design efficient transformers with an accuracy of 96.1% while adhering to hardware constraints of 1MB storage and 320KB memory. Additionally, TinyFormer achieves significant speedups in sparse inference, up to 12.2x comparing to the CMSIS-NN library. TinyFormer is believed to bring powerful transformers into TinyML scenarios and to greatly expand the scope of deep learning applications
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single- vs. Dual-Policy Reinforcement Learning for Dynamic Bike Rebalancing</title>
<link>https://arxiv.org/abs/2402.03589</link>
<guid>https://arxiv.org/abs/2402.03589</guid>
<content:encoded><![CDATA[
arXiv:2402.03589v2 Announce Type: replace 
Abstract: Bike-sharing systems (BSS) provide a sustainable urban mobility solution, but ensuring their reliability requires effective rebalancing strategies to address stochastic demand and prevent station imbalances. This paper proposes reinforcement learning (RL) algorithms for dynamic rebalancing problem with multiple vehicles, introducing and comparing two RL approaches: Single-policy RL and Dual-policy RL. We formulate this network optimization problem as a Markov Decision Process within a continuous-time framework, allowing vehicles to make independent and cooperative rebalancing decisions without synchronization constraints. In the first approach, a single deep Q-network (DQN) is trained to jointly learn inventory and routing decisions. The second approach decouples node-level inventory decisions from arc-level vehicle routing, enhancing learning efficiency and adaptability. A high-fidelity simulator under the first-arrive-first-serve rule is developed to estimate rewards across diverse demand scenarios influenced by temporal and weather variations. Extensive experiments demonstrate that while the single-policy model is competitive against several benchmarks, the dual-policy model significantly reduces lost demand. These findings provide valuable insights for bike-sharing operators, reinforcing the potential of RL for real-time rebalancing and paving the way for more adaptive and intelligent urban mobility solutions.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning: A Stochastic Approximation Approach</title>
<link>https://arxiv.org/abs/2402.12945</link>
<guid>https://arxiv.org/abs/2402.12945</guid>
<content:encoded><![CDATA[
arXiv:2402.12945v2 Announce Type: replace 
Abstract: This paper considers the Federated learning (FL) in a stochastic approximation (SA) framework. Here, each client $i$ trains a local model using its dataset $\mathcal{D}^{(i)}$ and periodically transmits the model parameters $w^{(i)}_n$ to a central server, where they are aggregated into a global model parameter $\bar{w}_n$ and sent back. The clients continue their training by re-initializing their local models with the global model parameters.
  Prior works typically assumed constant (and often identical) step sizes (learning rates) across clients for model training. As a consequence the aggregated model converges only in expectation. In this work, client-specific tapering step sizes $a^{(i)}_n$ are used. The global model is shown to track an ODE with a forcing function equal to the weighted sum of the negative gradients of the individual clients. The weights being the limiting ratios $p^{(i)}=\lim_{n \to \infty} \frac{a^{(i)}_n}{a^{(1)}_n}$ of the step sizes, where $a^{(1)}_n \geq a^{(i)}_n, \forall n$. Unlike the constant step sizes, the convergence here is with probability one.
  In this framework, the clients with the larger $p^{(i)}$ exert a greater influence on the global model than those with smaller $p^{(i)}$, which can be used to favor clients that have rare and uncommon data. Numerical experiments were conducted to validate the convergence and demonstrate the choice of step-sizes for regulating the influence of the clients.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Valuation by Fusing Global and Local Statistical Information</title>
<link>https://arxiv.org/abs/2405.17464</link>
<guid>https://arxiv.org/abs/2405.17464</guid>
<content:encoded><![CDATA[
arXiv:2405.17464v2 Announce Type: replace 
Abstract: Data valuation has garnered increasing attention in recent years, given the critical role of high-quality data in various applications. Among diverse data valuation approaches, Shapley value-based methods are predominant due to their strong theoretical grounding. However, the exact computation of Shapley values is often computationally prohibitive, prompting the development of numerous approximation techniques. Despite notable advancements, existing methods generally neglect the incorporation of value distribution information and fail to account for dynamic data conditions, thereby compromising their performance and application potential. In this paper, we highlight the crucial role of both global and local statistical properties of value distributions in the context of data valuation for machine learning. First, we conduct a comprehensive analysis of these distributions across various simulated and real-world datasets, uncovering valuable insights and key patterns. Second, we propose an enhanced data valuation method that fuses the explored distribution characteristics into two regularization terms to refine Shapley value estimation. The proposed regularizers can be seamlessly incorporated into various existing data valuation methods. Third, we introduce a novel approach for dynamic data valuation that infers updated data values without recomputing Shapley values, thereby significantly improving computational efficiency. Extensive experiments have been conducted across a range of tasks, including Shapley value estimation, value-based data addition and removal, mislabeled data detection, and dynamic data valuation. The results showcase the consistent effectiveness and efficiency of our proposed methodologies, affirming the significant potential of global and local value distributions in data valuation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CTSyn: A Foundation Model for Cross Tabular Data Generation</title>
<link>https://arxiv.org/abs/2406.04619</link>
<guid>https://arxiv.org/abs/2406.04619</guid>
<content:encoded><![CDATA[
arXiv:2406.04619v2 Announce Type: replace 
Abstract: Generative Foundation Models (GFMs) have achieved remarkable success in producing high-quality synthetic data for images and text. However, their application to tabular data presents significant challenges due to the heterogeneous nature of table features. Current cross-table learning frameworks struggle because they lack a generative model backbone and an effective mechanism to decode heterogeneous feature values. To address these challenges, we propose the Cross-Table Synthesizer (CTSyn), a diffusion-based generative foundation model for tabular data generation. CTSyn comprises two key components. The first is an autoencoder network that consolidates diverse tables into a unified latent space. It dynamically reconstructs table values using a table schema embedding, allowing adaptation to heterogeneous datasets. The second is a conditional latent diffusion model that generates samples from the learned latent space, conditioned on the table schema. Through large-scale pre-training, CTSyn outperforms existing table synthesizers on standard benchmarks in both utility and diversity. These results position CTSyn as a promising framework for synthetic table generation and lay the groundwork for developing large-scale tabular foundation models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve Adversarial Robustness</title>
<link>https://arxiv.org/abs/2406.19622</link>
<guid>https://arxiv.org/abs/2406.19622</guid>
<content:encoded><![CDATA[
arXiv:2406.19622v2 Announce Type: replace 
Abstract: As deep neural networks (DNNs) are increasingly deployed in sensitive applications, ensuring their security and robustness has become critical. A major threat to DNNs arises from adversarial attacks, where small input perturbations can lead to incorrect predictions. Recent advances in adversarial training improve robustness by incorporating additional examples from external datasets or generative models. However, these methods often incur high computational costs, limiting their practicality and hindering real-world deployment. In this paper, we propose a cost-efficient alternative based on Lipschitz continuity that achieves robustness comparable to models trained with extensive supplementary data. Unlike conventional adversarial training, our method requires only a single pass over the dataset without gradient estimation, making it highly efficient. Furthermore, our method can integrate seamlessly with existing adversarial training frameworks and enhances the robustness of models without requiring extra generative data. Experimental results show that our approach not only reduces computational overhead but also maintains or improves the defensive capabilities of robust neural networks. This work opens a promising direction for developing practical, scalable defenses against adversarial attacks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HO-FMN: Hyperparameter Optimization for Fast Minimum-Norm Attacks</title>
<link>https://arxiv.org/abs/2407.08806</link>
<guid>https://arxiv.org/abs/2407.08806</guid>
<content:encoded><![CDATA[
arXiv:2407.08806v3 Announce Type: replace 
Abstract: Gradient-based attacks are a primary tool to evaluate robustness of machine-learning models. However, many attacks tend to provide overly-optimistic evaluations as they use fixed loss functions, optimizers, step-size schedulers, and default hyperparameters. In this work, we tackle these limitations by proposing a parametric variation of the well-known fast minimum-norm attack algorithm, whose loss, optimizer, step-size scheduler, and hyperparameters can be dynamically adjusted. We re-evaluate 12 robust models, showing that our attack finds smaller adversarial perturbations without requiring any additional tuning. This also enables reporting adversarial robustness as a function of the perturbation budget, providing a more complete evaluation than that offered by fixed-budget attacks, while remaining efficient. We release our open-source code at https://github.com/pralab/HO-FMN.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis</title>
<link>https://arxiv.org/abs/2409.04290</link>
<guid>https://arxiv.org/abs/2409.04290</guid>
<content:encoded><![CDATA[
arXiv:2409.04290v2 Announce Type: replace 
Abstract: Motivation: Survival analysis is a branch of statistics that is crucial in medicine for modeling the time to critical events such as death or relapse, in order to improve treatment strategies and patient outcomes. Selecting survival models often involves a trade-off between performance and interpretability; deep learning models offer high performance but lack the transparency of more traditional approaches. This poses a significant issue in medicine, where practitioners are reluctant to use black-box models for critical patient decisions.
  Results: We introduce CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable, high-performance survival analysis. Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable and accurate alternative to multi-layer perceptrons. We evaluated CoxKAN on four synthetic and nine real datasets, including five cohorts with clinical data and four with genomics biomarkers. In synthetic experiments, CoxKAN accurately recovered interpretable hazard function formulae and excelled in automatic feature selection. Evaluations on real datasets showed that CoxKAN consistently outperformed the traditional Cox proportional hazards model (by up to 4% in C-index) and matched or surpassed the performance of deep learning-based models. Importantly, CoxKAN revealed complex interactions between predictor variables and uncovered symbolic formulae, which are key capabilities that other survival analysis methods lack, to provide clear insights into the impact of key biomarkers on patient risk.
  Availability and implementation: CoxKAN is available at GitHub and Zenodo
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Large Language Models: Current Progress and Future Directions</title>
<link>https://arxiv.org/abs/2409.15723</link>
<guid>https://arxiv.org/abs/2409.15723</guid>
<content:encoded><![CDATA[
arXiv:2409.15723v2 Announce Type: replace 
Abstract: Large language models are rapidly gaining popularity and have been widely adopted in real-world applications. While the quality of training data is essential, privacy concerns arise during data collection. Federated learning offers a solution by allowing multiple clients to collaboratively train LLMs without sharing local data. However, FL introduces new challenges, such as model convergence issues due to heterogeneous data and high communication costs. A comprehensive study is required to address these challenges and guide future research. This paper surveys Federated learning for LLMs (FedLLM), highlighting recent advances and future directions. We focus on two key aspects: fine-tuning and prompt learning in a federated setting, discussing existing work and associated research challenges. We finally propose potential directions for federated LLMs, including pre-training, federated agents, and LLMs for federated learning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Request Left Behind: Tackling Heterogeneity in Long-Context LLM Inference with Medha</title>
<link>https://arxiv.org/abs/2409.17264</link>
<guid>https://arxiv.org/abs/2409.17264</guid>
<content:encoded><![CDATA[
arXiv:2409.17264v5 Announce Type: replace 
Abstract: Deploying million-token Large Language Models (LLMs) is challenging because production workloads are highly heterogeneous, mixing short queries and long documents. This heterogeneity, combined with the quadratic complexity of attention, creates severe convoy effects where long-running requests stall short, interactive ones, degrading system responsiveness. We present Medha, a serving system that eliminates these convoys by introducing fine-grained, preemptive scheduling to LLM inference.
  Medha makes preemption practical with a co-designed set of mechanisms -- including Adaptive Chunking and Stream Pipeline Parallel that overcome the perceived inefficiencies and scaling challenges of chunking. Additionally, we present a new parallelism strategy KV-Cache Parallelism to reduce the decode latency and afford interactivity despite very long context. These mechanisms are orchestrated by a Length-Aware Relative Slack (LARS) scheduler, a deadline and heterogeneity-aware scheduling policy that prevents both the convoy effect and the starvation that plagues simpler policies. Under a heterogeneous workload, Medha improves throughput by 5.7x while reducing median and 99th percentile latency by 30x and 174x, respectively, compared to state-of-the-art non-preemptive systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HoGA: Higher-Order Graph Attention via Diversity-Aware k-Hop Sampling</title>
<link>https://arxiv.org/abs/2411.12052</link>
<guid>https://arxiv.org/abs/2411.12052</guid>
<content:encoded><![CDATA[
arXiv:2411.12052v3 Announce Type: replace 
Abstract: Graphs model latent variable relationships in many real-world systems, and Message Passing Neural Networks (MPNNs) are widely used to learn such structures for downstream tasks. While edge-based MPNNs effectively capture local interactions, their expressive power is theoretically bounded, limiting the discovery of higher-order relationships. We introduce the Higher-Order Graph Attention (HoGA) module, which constructs a k-order attention matrix by sampling subgraphs to maximize diversity among feature vectors. Unlike existing higher-order attention methods that greedily resample similar k-order relationships, HoGA targets diverse modalities in higher-order topology, reducing redundancy and expanding the range of captured substructures. Applied to two single-hop attention models, HoGA achieves at least a 5% accuracy gain on all benchmark node classification datasets and outperforms recent baselines on six of eight datasets. Code is available at https://github.com/TB862/Higher_Order.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Effectiveness of Adversarial Training on Malware Classifiers</title>
<link>https://arxiv.org/abs/2412.18218</link>
<guid>https://arxiv.org/abs/2412.18218</guid>
<content:encoded><![CDATA[
arXiv:2412.18218v2 Announce Type: replace 
Abstract: Adversarial Training (AT) is a key defense against Machine Learning evasion attacks, but its effectiveness for real-world malware detection remains poorly understood. This uncertainty stems from a critical disconnect in prior research: studies often overlook the inherent nature of malware and are fragmented, examining diverse variables like realism or confidence of adversarial examples in isolation, or relying on weak evaluations that yield non-generalizable insights. To address this, we introduce Rubik, a framework for the systematic, multi-dimensional evaluation of AT in the malware domain. This framework defines diverse key factors across essential dimensions, including data, feature representations, classifiers, and robust optimization settings, for a comprehensive exploration of the interplay of influential AT's variables through reliable evaluation practices, such as realistic evasion attacks. We instantiate Rubik on Android malware, empirically analyzing how this interplay shapes robustness. Our findings challenge prior beliefs--showing, for instance, that realizable adversarial examples offer only conditional robustness benefits--and reveal new insights, such as the critical role of model architecture and feature-space structure in determining AT's success. From this analysis, we distill four key insights, expose four common evaluation misconceptions, and offer practical recommendations to guide the development of truly robust malware classifiers.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Introspection: Reinforcing Thinking via Externalist Behavioral Feedback</title>
<link>https://arxiv.org/abs/2501.01457</link>
<guid>https://arxiv.org/abs/2501.01457</guid>
<content:encoded><![CDATA[
arXiv:2501.01457v2 Announce Type: replace 
Abstract: While inference-time thinking allows Large Language Models (LLMs) to address complex problems, the extended thinking process can be unreliable or inconsistent because of the model's probabilistic nature, especially near its knowledge boundaries. Existing approaches attempt to mitigate this by having the model critique its own reasoning to make corrections. However, such self-critique inherits the same biases of the original output, known as the introspection illusion. Moving beyond such introspection and inspired by core methodologies in ethology, we propose an externalist three-step framework Distillation-Reinforcement-Reasoning (DRR). Rather than relying on a model's introspection, DRR evaluates its observable behaviors to provide corrective feedback. DRR first distills the reasoner's behavioral traces, then trains a lightweight, external Discriminative Model (DM). At inference time, this DM acts as a critic, identifying and rejecting suspicious reasoning steps. This external feedback compels the LLM to discard flawed pathways and explore alternatives, thereby enhancing reasoning quality without altering the base model. Experiments on multiple reasoning benchmarks show that our framework significantly outperforms prominent self-critique methods. Benefiting from a lightweight and annotation-free design, DRR offers a scalable and adaptable solution for improving the reliability of reasoning in a wide range of LLMs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unifying View of Linear Function Approximation in Off-Policy RL Through Matrix Splitting and Preconditioning</title>
<link>https://arxiv.org/abs/2501.01774</link>
<guid>https://arxiv.org/abs/2501.01774</guid>
<content:encoded><![CDATA[
arXiv:2501.01774v3 Announce Type: replace 
Abstract: In off-policy policy evaluation (OPE) tasks within reinforcement learning, Temporal Difference Learning(TD) and Fitted Q-Iteration (FQI) have traditionally been viewed as differing in the number of updates toward the target value function: TD makes one update, FQI makes an infinite number, and Partial Fitted Q-Iteration (PFQI) performs a finite number. We show that this view is not accurate, and provide a new mathematical perspective under linear value function approximation that unifies these methods as a single iterative method solving the same linear system, but using different matrix splitting schemes and preconditioners. We show that increasing the number of updates under the same target value function, i.e., the target network technique, is a transition from using a constant preconditioner to using a data-feature adaptive preconditioner. This elucidates, for the first time, why TD convergence does not necessarily imply FQI convergence, and establishes tight convergence connections among TD, PFQI, and FQI. Our framework enables sharper theoretical results than previous work and characterization of the convergence conditions for each algorithm, without relying on assumptions about the features (e.g., linear independence). We also provide an encoder-decoder perspective to better understand the convergence conditions of TD, and prove, for the first time, that when a large learning rate doesn't work, trying a smaller one may help. Our framework also leads to the discovery of new crucial conditions on features for convergence, and shows how common assumptions about features influence convergence, e.g., the assumption of linearly independent features can be dropped without compromising the convergence guarantees of stochastic TD in the on-policy setting. This paper is also the first to introduce matrix splitting into the convergence analysis of these algorithms.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CroMe: Multimodal Fake News Detection using Cross-Modal Tri-Transformer and Metric Learning</title>
<link>https://arxiv.org/abs/2501.12422</link>
<guid>https://arxiv.org/abs/2501.12422</guid>
<content:encoded><![CDATA[
arXiv:2501.12422v2 Announce Type: replace 
Abstract: Multimodal Fake News Detection has received increasing attention recently. Existing methods rely on independently encoded unimodal data and overlook the advantages of capturing intra-modality relationships and integrating inter-modal similarities using advanced techniques. To address these issues, Cross-Modal Tri-Transformer and Metric Learning for Multimodal Fake News Detection (CroMe) is proposed. CroMe utilizes Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (BLIP2) as encoders to capture detailed text, image and combined image-text representations. The metric learning module employs a proxy anchor method to capture intra-modality relationships while the feature fusion module uses a Cross-Modal and Tri-Transformer for effective integration. The final fake news detector processes the fused features through a classifier to predict the authenticity of the content. Experiments on datasets show that CroMe excels in multimodal fake news detection.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Prediction Games</title>
<link>https://arxiv.org/abs/2503.03401</link>
<guid>https://arxiv.org/abs/2503.03401</guid>
<content:encoded><![CDATA[
arXiv:2503.03401v3 Announce Type: replace 
Abstract: When a prediction algorithm serves a collection of users, disparities in prediction quality are likely to emerge. If users respond to accurate predictions by increasing engagement, inviting friends, or adopting trends, repeated learning creates a feedback loop that shapes both the model and the population of its users. In this work, we introduce evolutionary prediction games, a framework grounded in evolutionary game theory which models such feedback loops as natural-selection processes among groups of users. Our theoretical analysis reveals a gap between idealized and real-world learning settings: In idealized settings with unlimited data and computational power, repeated learning creates competition and promotes competitive exclusion across a broad class of behavioral dynamics. However, under realistic constraints such as finite data, limited compute, or risk of overfitting, we show that stable coexistence and mutualistic symbiosis between groups becomes possible. We analyze these possibilities in terms of their stability and feasibility, present mechanisms that can sustain their existence, and empirically demonstrate our findings.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster</title>
<link>https://arxiv.org/abs/2503.07649</link>
<guid>https://arxiv.org/abs/2503.07649</guid>
<content:encoded><![CDATA[
arXiv:2503.07649v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) and Foundation Models (FMs) have recently become prevalent for time series forecasting tasks. While fine-tuning LLMs enables domain adaptation, they often struggle to generalize across diverse and unseen datasets. Moreover, existing Time Series Foundation Models (TSFMs) still face challenges in handling non-stationary dynamics and distribution shifts, largely due to the lack of effective mechanisms for adaptation. To this end, we present TS-RAG, a retrieval-augmented generation framework for time series forecasting that enhances the generalization and interpretability of TSFMs. Specifically, TS-RAG leverages pre-trained time series encoders to retrieve semantically relevant segments from a dedicated knowledge base, enriching the contextual representation of the input query. Furthermore, we propose an Adaptive Retrieval Mixer (ARM) module that dynamically fuses the retrieved patterns with the TSFM's internal representation, improving forecasting accuracy without requiring task-specific fine-tuning. Thorough empirical studies on seven public benchmark datasets demonstrate that TS-RAG achieves state-of-the-art zero-shot forecasting performance, outperforming the existing TSFMs by up to 6.84% across diverse domains while also providing desirable interpretability. Our code and data are available at: https://github.com/UConn-DSIS/TS-RAG
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>F-INR: Functional Tensor Decomposition for Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2503.21507</link>
<guid>https://arxiv.org/abs/2503.21507</guid>
<content:encoded><![CDATA[
arXiv:2503.21507v2 Announce Type: replace 
Abstract: Implicit Neural Representations (INRs) model signals as continuous, differentiable functions. However, monolithic INRs scale poorly with data dimensionality, leading to excessive training costs. We propose F-INR, a framework that addresses this limitation by factorizing a high-dimensional INR into a set of compact, axis-specific sub-networks based on functional tensor decomposition. These sub-networks learn low-dimensional functional components that are then combined via tensor operations. This factorization reduces computational complexity while additionally improving representational capacity. F-INR is both architecture- and decomposition-agnostic. It integrates with various existing INR backbones (e.g., SIREN, WIRE, FINER, Factor Fields) and tensor formats (e.g., CP, TT, Tucker), offering fine-grained control over the speed-accuracy trade-off via the tensor rank and mode. Our experiments show F-INR accelerates training by up to $20\times$ and improves fidelity by over \num{6.0} dB PSNR compared to state-of-the-art INRs. We validate these gains on diverse tasks, including image representation, 3D geometry reconstruction, and neural radiance fields. We further show F-INR's applicability to scientific computing by modeling complex physics simulations. Thus, F-INR provides a scalable, flexible, and efficient framework for high-dimensional signal modeling. Project page: https://f-inr.github.io
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement</title>
<link>https://arxiv.org/abs/2504.16136</link>
<guid>https://arxiv.org/abs/2504.16136</guid>
<content:encoded><![CDATA[
arXiv:2504.16136v2 Announce Type: replace 
Abstract: In the era of data-driven intelligence, the paradox of data abundance and annotation scarcity has emerged as a critical bottleneck in the advancement of machine learning. This paper gives a detailed overview of Active Learning (AL), which is a strategy in machine learning that helps models achieve better performance using fewer labeled examples. It introduces the basic concepts of AL and discusses how it is used in various fields such as computer vision, natural language processing, transfer learning, and real-world applications. The paper focuses on important research topics such as uncertainty estimation, handling of class imbalance, domain adaptation, fairness, and the creation of strong evaluation metrics and benchmarks. It also shows that learning methods inspired by humans and guided by questions can improve data efficiency and help models learn more effectively. In addition, this paper talks about current challenges in the field, including the need to rebuild trust, ensure reproducibility, and deal with inconsistent methodologies. It points out that AL often gives better results than passive learning, especially when good evaluation measures are used. This work aims to be useful for both researchers and practitioners by providing key insights and proposing directions for future progress in active learning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel-Young Losses</title>
<link>https://arxiv.org/abs/2505.09432</link>
<guid>https://arxiv.org/abs/2505.09432</guid>
<content:encoded><![CDATA[
arXiv:2505.09432v3 Announce Type: replace 
Abstract: Surrogate regret bounds, also known as excess risk bounds, bridge the gap between the convergence rates of surrogate and target losses. The regret transfer is lossless if the surrogate regret bound is linear. While convex smooth surrogate losses are appealing in particular due to the efficient estimation and optimization, the existence of a trade-off between the loss smoothness and linear regret bound has been believed in the community. Under this scenario, the better optimization and estimation properties of convex smooth surrogate losses may inevitably deteriorate after undergoing the regret transfer onto a target loss. We overcome this dilemma for arbitrary discrete target losses by constructing a convex smooth surrogate loss, which entails a linear surrogate regret bound composed with a tailored prediction link. The construction is based on Fenchel--Young losses generated by the convolutional negentropy, which are equivalent to the infimal convolution of a generalized negentropy and the target Bayes risk. Consequently, the infimal convolution enables us to derive a smooth loss while maintaining the surrogate regret bound linear. We additionally benefit from the infimal convolution to have a consistent estimator of the underlying class probability. Our results are overall a novel demonstration of how convex analysis penetrates into optimization and statistical efficiency in risk minimization.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Training Data Attribution with Representational Optimization</title>
<link>https://arxiv.org/abs/2505.18513</link>
<guid>https://arxiv.org/abs/2505.18513</guid>
<content:encoded><![CDATA[
arXiv:2505.18513v2 Announce Type: replace 
Abstract: Training data attribution (TDA) methods aim to measure how training data impacts a model's predictions. While gradient-based attribution methods, such as influence functions, offer theoretical grounding, their computational costs make them impractical for large-scale applications. Representation-based approaches are far more scalable, but typically rely on heuristic embeddings that are not optimized for attribution, limiting their fidelity. To address these challenges, we propose AirRep, a scalable, representation-based approach that closes this gap by learning task-specific and model-aligned representations optimized explicitly for TDA. AirRep introduces two key innovations: a trainable encoder tuned for attribution quality, and an attention-based pooling mechanism that enables accurate estimation of group-wise influence. We train AirRep using a ranking objective over automatically constructed training subsets labeled by their empirical effect on target predictions. Experiments on instruction-tuned LLMs demonstrate that AirRep achieves performance on par with state-of-the-art gradient-based approaches while being nearly two orders of magnitude more efficient at inference time. Further analysis highlights its robustness and generalization across tasks and models. Our code is available at https://github.com/sunnweiwei/AirRep
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asymmetric Duos: Sidekicks Improve Uncertainty</title>
<link>https://arxiv.org/abs/2505.18636</link>
<guid>https://arxiv.org/abs/2505.18636</guid>
<content:encoded><![CDATA[
arXiv:2505.18636v2 Announce Type: replace 
Abstract: The go-to strategy to apply deep networks in settings where uncertainty informs decisions--ensembling multiple training runs with random initializations--is ill-suited for the extremely large-scale models and practical fine-tuning workflows of today. We introduce a new cost-effective strategy for improving the uncertainty quantification and downstream decisions of a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate but much smaller "sidekick" (e.g. a fine-tuned ResNet-34) with a fraction of the computational cost. We propose aggregating the predictions of this Asymmetric Duo by simple learned weighted averaging. Surprisingly, despite their inherent asymmetry, the sidekick model almost never harms the performance of the larger model. In fact, across five image classification benchmarks and a variety of model architectures and training schemes (including soups), Asymmetric Duos significantly improve accuracy, uncertainty quantification, and selective classification metrics with only ${\sim}10-20\%$ more computation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alignment of large language models with constrained learning</title>
<link>https://arxiv.org/abs/2505.19387</link>
<guid>https://arxiv.org/abs/2505.19387</guid>
<content:encoded><![CDATA[
arXiv:2505.19387v2 Announce Type: replace 
Abstract: We study the problem of computing an optimal large language model (LLM) policy for the constrained alignment problem, where the goal is to maximize a primary reward objective while satisfying constraints on secondary utilities. Despite the popularity of Lagrangian-based LLM policy search in constrained alignment, iterative primal-dual methods often fail to converge, and non-iterative dual-based methods do not achieve optimality in the LLM parameter space. To address these challenges, we employ Lagrangian duality to develop an iterative dual-based alignment method that alternates between updating the LLM policy via Lagrangian maximization and updating the dual variable via dual descent. In theory, we characterize the primal-dual gap between the primal value in the distribution space and the dual value in the LLM parameter space. We further quantify the optimality gap of the learned LLM policies at near-optimal dual variables with respect to both the objective and the constraint functions. These results prove that dual-based alignment methods can find an optimal constrained LLM policy, up to an LLM parametrization gap. We demonstrate the effectiveness and merits of our approach through extensive experiments conducted on the PKU-SafeRLHF and Anthropic HH-RLHF datasets.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Actor-Critics with Tight Risk Certificates</title>
<link>https://arxiv.org/abs/2505.19682</link>
<guid>https://arxiv.org/abs/2505.19682</guid>
<content:encoded><![CDATA[
arXiv:2505.19682v2 Announce Type: replace 
Abstract: Deep actor-critic algorithms have reached a level where they influence everyday life. They are a driving force behind continual improvement of large language models through user feedback. However, their deployment in physical systems is not yet widely adopted, mainly because no validation scheme fully quantifies their risk of malfunction. We demonstrate that it is possible to develop tight risk certificates for deep actor-critic algorithms that predict generalization performance from validation-time observations. Our key insight centers on the effectiveness of minimal evaluation data. A small feasible set of evaluation roll-outs collected from a pretrained policy suffices to produce accurate risk certificates when combined with a simple adaptation of PAC-Bayes theory. Specifically, we adopt a recently introduced recursive PAC-Bayes approach, which splits validation data into portions and recursively builds PAC-Bayes bounds on the excess loss of each portion's predictor, using the predictor from the previous portion as a data-informed prior. Our empirical results across multiple locomotion tasks, actor-critic methods, and policy expertise levels demonstrate risk certificates tight enough to be considered for practical use.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing Pattern Matching and Its Limits on Compositional Task Structures</title>
<link>https://arxiv.org/abs/2505.20278</link>
<guid>https://arxiv.org/abs/2505.20278</guid>
<content:encoded><![CDATA[
arXiv:2505.20278v2 Announce Type: replace 
Abstract: Despite impressive capabilities, LLMs' successes often rely on pattern-matching behaviors, yet these are also linked to OOD generalization failures in compositional tasks. However, behavioral studies commonly employ task setups that allow multiple generalization sources (e.g., algebraic invariances, structural repetition), obscuring a precise and testable account of how well LLMs perform generalization through pattern matching and their limitations. To address this ambiguity, we first formalize pattern matching as functional equivalence, i.e., identifying pairs of subsequences of inputs that consistently lead to identical results when the rest of the input is held constant. Then, we systematically study how decoder-only Transformer and Mamba behave in controlled tasks with compositional structures that isolate this mechanism. Our formalism yields predictive and quantitative insights: (1) Instance-wise success of pattern matching is well predicted by the number of contexts witnessing the relevant functional equivalence. (2) We prove a tight sample complexity bound of learning a two-hop structure by identifying the exponent of the data scaling law for perfect in-domain generalization. Our empirical results align with the theoretical prediction, under 20x parameter scaling and across architectures. (3) Path ambiguity is a structural barrier: when a variable influences the output via multiple paths, models fail to form unified intermediate state representations, impairing accuracy and interpretability. (4) Chain-of-Thought reduces data requirements yet does not resolve path ambiguity. Hence, we provide a predictive, falsifiable boundary for pattern matching and a foundational diagnostic for disentangling mixed generalization mechanisms.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-Time Alignment of Diffusion Models via Evolutionary Algorithms</title>
<link>https://arxiv.org/abs/2506.00299</link>
<guid>https://arxiv.org/abs/2506.00299</guid>
<content:encoded><![CDATA[
arXiv:2506.00299v2 Announce Type: replace 
Abstract: Diffusion models are state-of-the-art generative models, yet their samples often fail to satisfy application objectives such as safety constraints or domain-specific validity. Existing techniques for alignment require gradients, internal model access, or large computational budgets resulting in high compute demands, or lack of support for certain objectives. In response, we introduce an inference-time alignment framework based on evolutionary algorithms. We treat diffusion models as black boxes and search their latent space to maximize alignment objectives. Given equal or less running time, our method achieves 3-35% higher ImageReward scores than gradient-free and gradient-based methods. On the Open Image Preferences dataset, our method achieves competitive results across four popular alignment objectives. In terms of computational efficiency, we require 55% to 76% less GPU memory and are 72% to 80% faster than gradient-based methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.04171</link>
<guid>https://arxiv.org/abs/2506.04171</guid>
<content:encoded><![CDATA[
arXiv:2506.04171v2 Announce Type: replace 
Abstract: Deep generative models have recently been applied to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty-aware inference. However, enforcing physical constraints, such as conservation laws (linear and nonlinear) and physical consistencies, remains challenging. Existing methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints. In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints. Empirically, PCFM outperforms both unconstrained and constrained baselines on a range of PDEs, including those with shocks, discontinuities, and sharp features, while ensuring exact constraint satisfaction at the final solution. Our method provides a flexible framework for enforcing hard constraints in both scientific and general-purpose generative models, especially in applications where constraint satisfaction is essential.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ENMA: Tokenwise Autoregression for Generative Neural PDE Operators</title>
<link>https://arxiv.org/abs/2506.06158</link>
<guid>https://arxiv.org/abs/2506.06158</guid>
<content:encoded><![CDATA[
arXiv:2506.06158v2 Announce Type: replace 
Abstract: Solving time-dependent parametric partial differential equations (PDEs) remains a fundamental challenge for neural solvers, particularly when generalizing across a wide range of physical parameters and dynamics. When data is uncertain or incomplete-as is often the case-a natural approach is to turn to generative models. We introduce ENMA, a generative neural operator designed to model spatio-temporal dynamics arising from physical phenomena. ENMA predicts future dynamics in a compressed latent space using a generative masked autoregressive transformer trained with flow matching loss, enabling tokenwise generation. Irregularly sampled spatial observations are encoded into uniform latent representations via attention mechanisms and further compressed through a spatio-temporal convolutional encoder. This allows ENMA to perform in-context learning at inference time by conditioning on either past states of the target trajectory or auxiliary context trajectories with similar dynamics. The result is a robust and adaptable framework that generalizes to new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation</title>
<link>https://arxiv.org/abs/2506.08604</link>
<guid>https://arxiv.org/abs/2506.08604</guid>
<content:encoded><![CDATA[
arXiv:2506.08604v2 Announce Type: replace 
Abstract: Generative machine learning methods, such as diffusion models and flow matching, have shown great potential in modeling complex system behaviors and building efficient surrogate models. However, these methods typically learn the underlying physics implicitly from data. We propose Physics-Based Flow Matching (PBFM), a novel generative framework that explicitly embeds physical constraints, both PDE residuals and algebraic relations, into the flow matching objective. We also introduce temporal unrolling at training time that improves the accuracy of the final, noise-free sample prediction. Our method jointly minimizes the flow matching loss and the physics-based residual loss without requiring hyperparameter tuning of their relative weights. Additionally, we analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of physical constraints and evaluate a stochastic sampling strategy that helps to reduce physical residuals. Through extensive benchmarks on three representative PDE problems, we show that our approach yields up to an $8\times$ more accurate physical residuals compared to FM, while clearly outperforming existing algorithms in terms of distributional accuracy. PBFM thus provides a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits</title>
<link>https://arxiv.org/abs/2506.14988</link>
<guid>https://arxiv.org/abs/2506.14988</guid>
<content:encoded><![CDATA[
arXiv:2506.14988v4 Announce Type: replace 
Abstract: We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance. A key challenge in this setting is decision-making under limited information about arm rewards. To address this, we introduce a novel probing framework that strategically gathers information about selected arms before allocation. In the offline setting, where reward distributions are known, we leverage submodular properties to design a greedy probing algorithm with a provable performance bound. For the more complex online setting, we develop an algorithm that achieves sublinear regret while maintaining fairness. Extensive experiments on synthetic and real-world datasets show that our approach outperforms baseline methods, achieving better fairness and efficiency.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConStellaration: A dataset of QI-like stellarator plasma boundaries and optimization benchmarks</title>
<link>https://arxiv.org/abs/2506.19583</link>
<guid>https://arxiv.org/abs/2506.19583</guid>
<content:encoded><![CDATA[
arXiv:2506.19583v2 Announce Type: replace 
Abstract: Stellarators are magnetic confinement devices under active development to deliver steady-state carbon-free fusion energy. Their design involves a high-dimensional, constrained optimization problem that requires expensive physics simulations and significant domain expertise. Recent advances in plasma physics and open-source tools have made stellarator optimization more accessible. However, broader community progress is currently bottlenecked by the lack of standardized optimization problems with strong baselines and datasets that enable data-driven approaches, particularly for quasi-isodynamic (QI) stellarator configurations, considered as a promising path to commercial fusion due to their inherent resilience to current driven disruptions. Here, we release an open dataset of diverse QI-like stellarator plasma boundary shapes, paired with their ideal magnetohydrodynamic (MHD) equilibria and performance metrics. We generated this dataset by sampling a variety of QI fields and optimizing corresponding stellarator plasma boundaries. We introduce three optimization benchmarks of increasing complexity: (1) a single objective geometric optimization problem, (2) a "simple-to-build" QI stellarator, and (3) a multi-objective ideal-MHD stable QI stellarator that investigates trade-offs between compactness and coil simplicity. For every benchmark, we provide reference code, evaluation scripts, and strong baselines based on classical optimization techniques. Finally, we show how learned models trained on our dataset can efficiently generate novel, feasible configurations without querying expensive physics oracles. By openly releasing the dataset along with benchmark problems and baselines, we aim to lower the entry barrier for optimization and machine learning researchers to engage in stellarator design and to accelerate cross-disciplinary progress toward bringing fusion energy to the grid.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoDiscovery: Open-ended Scientific Discovery via Bayesian Surprise</title>
<link>https://arxiv.org/abs/2507.00310</link>
<guid>https://arxiv.org/abs/2507.00310</guid>
<content:encoded><![CDATA[
arXiv:2507.00310v2 Announce Type: replace 
Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on answering questions, but also on knowing which questions to ask. Most recent works in ASD explore the use of large language models (LLMs) in goal-driven settings, relying on human-specified research questions to guide hypothesis generation. However, scientific discovery may be accelerated further by allowing the AI system to drive exploration by its own criteria. The few existing approaches in open-ended ASD select hypotheses based on diversity heuristics or subjective proxies for human interestingness, but the former struggles to meaningfully navigate the typically vast hypothesis space, and the latter suffers from imprecise definitions. This paper presents AutoDiscovery -- a method for open-ended ASD that instead drives scientific exploration using Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior beliefs about a hypothesis to its posterior beliefs after gathering experimental results. To efficiently explore the space of nested hypotheses, our method employs a Monte Carlo tree search (MCTS) strategy with progressive widening using surprisal as the reward function. We evaluate AutoDiscovery in the setting of data-driven discovery across 21 real-world datasets spanning domains such as biology, economics, finance, and behavioral science. Our results demonstrate that under a fixed budget, AutoDiscovery substantially outperforms competitors by producing 5-29% more discoveries deemed surprising by the LLM. Our human evaluation further reveals that two-thirds of discoveries made by our system are surprising to domain experts as well, suggesting this is an important step towards building open-ended ASD systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Action Chunking and Exploratory Data Collection Yield Exponential Improvements in Behavior Cloning for Continuous Control</title>
<link>https://arxiv.org/abs/2507.09061</link>
<guid>https://arxiv.org/abs/2507.09061</guid>
<content:encoded><![CDATA[
arXiv:2507.09061v5 Announce Type: replace 
Abstract: This paper presents a theoretical analysis of two of the most impactful interventions in modern learning from demonstration in robotics and continuous control: the practice of action-chunking (predicting sequences of actions in open-loop) and exploratory augmentation of expert demonstrations. Though recent results show that learning from demonstration, also known as imitation learning (IL), can suffer errors that compound exponentially with task horizon in continuous settings, we demonstrate that action chunking and exploratory data collection circumvent exponential compounding errors in different regimes. Our results identify control-theoretic stability as the key mechanism underlying the benefits of these interventions. On the empirical side, we validate our predictions and the role of control-theoretic stability through experimentation on popular robot learning benchmarks. On the theoretical side, we demonstrate that the control-theoretic lens provides fine-grained insights into how compounding error arises, leading to tighter statistical guarantees on imitation learning error when these interventions are applied than previous techniques based on information-theoretic considerations alone.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness</title>
<link>https://arxiv.org/abs/2507.14446</link>
<guid>https://arxiv.org/abs/2507.14446</guid>
<content:encoded><![CDATA[
arXiv:2507.14446v4 Announce Type: replace 
Abstract: In this work, we study how to efficiently apply reinforcement learning (RL) for solving large-scale stochastic optimization problems by leveraging intervention models. The key of the proposed methodology is to better explore the solution space by simulating and composing the stochastic processes using pre-trained deep learning (DL) models. We demonstrate our approach on a challenging real-world application, the multi-sourcing multi-period inventory management problem in supply chain optimization. In particular, we employ deep RL models for learning and forecasting the stochastic supply chain processes under a range of assumptions. Moreover, we also introduce a constraint coordination mechanism, designed to forecast dual costs given the cross-products constraints in the inventory network. We highlight that instead of directly modeling the complex physical constraints into the RL optimization problem and solving the stochastic problem as a whole, our approach breaks down those supply chain processes into scalable and composable DL modules, leading to improved performance on large real-world datasets. We also outline open problems for future research to further investigate the efficacy of such models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Multi-color Message Passing Graph Neural Networks for Blood-brain Barrier Permeability Prediction</title>
<link>https://arxiv.org/abs/2507.18926</link>
<guid>https://arxiv.org/abs/2507.18926</guid>
<content:encoded><![CDATA[
arXiv:2507.18926v3 Announce Type: replace 
Abstract: Accurate prediction of blood-brain barrier permeability (BBBP) is essential for central nervous system (CNS) drug development. While graph neural networks (GNNs) have advanced molecular property prediction, they often rely on molecular topology and neglect the three-dimensional geometric information crucial for modeling transport mechanisms. This paper introduces the geometric multi-color message-passing graph neural network (GMC-MPNN), a novel framework that enhances standard message-passing architectures by explicitly incorporating atomic-level geometric features and long-range interactions. Our model constructs weighted colored subgraphs based on atom types to capture the spatial relationships and chemical context that govern BBB permeability. We evaluated GMC-MPNN on three benchmark datasets for both classification and regression tasks, using rigorous scaffold-based splitting to ensure a robust assessment of generalization. The results demonstrate that GMC-MPNN consistently outperforms existing state-of-the-art models, achieving superior performance in both classifying compounds as permeable/non-permeable (AUC-ROC of 0.9704 and 0.9685) and in regressing continuous permeability values (RMSE of 0.4609, Pearson correlation of 0.7759). An ablation study further quantified the impact of specific atom-pair interactions, revealing that the model's predictive power derives from its ability to learn from both common and rare, but chemically significant, functional motifs. By integrating spatial geometry into the graph representation, GMC-MPNN sets a new performance benchmark and offers a more accurate and generalizable tool for drug discovery pipelines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowering Time Series Forecasting with LLM-Agents</title>
<link>https://arxiv.org/abs/2508.04231</link>
<guid>https://arxiv.org/abs/2508.04231</guid>
<content:encoded><![CDATA[
arXiv:2508.04231v2 Announce Type: replace 
Abstract: Large Language Model (LLM) powered agents have emerged as effective planners for Automated Machine Learning (AutoML) systems. While most existing AutoML approaches focus on automating feature engineering and model architecture search, recent studies in time series forecasting suggest that lightweight models can often achieve state-of-the-art performance. This observation led us to explore improving data quality, rather than model architecture, as a potentially fruitful direction for AutoML on time series data. We propose DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata accompanying time series to clean data while optimizing forecasting performance. We evaluated DCATS using four time series forecasting models on a large-scale traffic volume forecasting dataset. Results demonstrate that DCATS achieves an average 6% error reduction across all tested models and time horizons, highlighting the potential of data-centric approaches in AutoML for time series forecasting.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems</title>
<link>https://arxiv.org/abs/2509.03340</link>
<guid>https://arxiv.org/abs/2509.03340</guid>
<content:encoded><![CDATA[
arXiv:2509.03340v2 Announce Type: replace 
Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple coexisting stable solutions, particularly in the presence of symmetry breaking. Deterministic machine learning models struggle to capture this multiplicity, averaging over solutions and failing to represent lower-symmetry outcomes. In this work, we propose a generative framework based on flow matching to model the full probability distribution over bifurcation outcomes. Our method enables direct sampling of multiple valid solutions while preserving system symmetries through equivariant modeling. We introduce a symmetric matching strategy that aligns predicted and target outputs under group actions, allowing accurate learning in equivariant settings. We validate our approach on a range of systems, from toy models to complex physical problems such as buckling beams and the Allen-Cahn equation. Our results demonstrate that flow matching significantly outperforms non-probabilistic and variational methods in capturing multimodal distributions and symmetry-breaking bifurcations, offering a principled and scalable solution for modeling multistability in high-dimensional systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis</title>
<link>https://arxiv.org/abs/2509.15392</link>
<guid>https://arxiv.org/abs/2509.15392</guid>
<content:encoded><![CDATA[
arXiv:2509.15392v2 Announce Type: replace 
Abstract: We study policy optimization in Stackelberg mean field games (MFGs), a hierarchical framework for modeling the strategic interaction between a single leader and an infinitely large population of homogeneous followers. The objective can be formulated as a structured bi-level optimization problem, in which the leader needs to learn a policy maximizing its reward, anticipating the response of the followers. Existing methods for solving these (and related) problems often rely on restrictive independence assumptions between the leader's and followers' objectives, use samples inefficiently due to nested-loop algorithm structure, and lack finite-time convergence guarantees. To address these limitations, we propose AC-SMFG, a single-loop actor-critic algorithm that operates on continuously generated Markovian samples. The algorithm alternates between (semi-)gradient updates for the leader, a representative follower, and the mean field, and is simple to implement in practice. We establish the finite-time and finite-sample convergence of the algorithm to a stationary point of the Stackelberg objective. To our knowledge, this is the first Stackelberg MFG algorithm with non-asymptotic convergence guarantees. Our key assumption is a "gradient alignment" condition, which requires that the full policy gradient of the leader can be approximated by a partial component of it, relaxing the existing leader-follower independence assumption. Simulation results in a range of well-established economics environments demonstrate that AC-SMFG outperforms existing multi-agent and MFG learning baselines in policy quality and convergence speed.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Conditional Distribution Equality Testing Framework using Deep Generative Learning</title>
<link>https://arxiv.org/abs/2509.17729</link>
<guid>https://arxiv.org/abs/2509.17729</guid>
<content:encoded><![CDATA[
arXiv:2509.17729v3 Announce Type: replace 
Abstract: In this paper, we propose a general framework for testing the conditional distribution equality in a two-sample problem, which is most relevant to covariate shift and causal discovery. Our framework is built on neural network-based generative methods and sample splitting techniques by transforming the conditional testing problem into an unconditional one. We introduce the generative classification accuracy-based conditional distribution equality test (GCA-CDET) to illustrate the proposed framework. We establish the convergence rate for the learned generator by deriving new results related to the recently-developed offset Rademacher complexity and prove the testing consistency of GCA-CDET under mild conditions.Empirically, we conduct numerical studies including synthetic datasets and two real-world datasets, demonstrating the effectiveness of our approach. Additional discussions on the optimality of the proposed framework are provided in the online supplementary material.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Noise-Curvature View of Loss of Trainability</title>
<link>https://arxiv.org/abs/2509.19698</link>
<guid>https://arxiv.org/abs/2509.19698</guid>
<content:encoded><![CDATA[
arXiv:2509.19698v2 Announce Type: replace 
Abstract: Loss of trainability refers to a phenomenon in continual learning where parameter updates no longer make progress on the optimization objective, so accuracy stalls or degrades as the learning problem changes over time. In this paper, we analyze loss of trainability through an optimization lens and find that the phenomenon is not reliably predicted by existing individual indicators such as Hessian rank, sharpness level, weight or gradient norms, gradient-to-parameter ratios, and unit-sign entropy. Motivated by our analysis, we introduce two complementary indicators: a batch-size-aware gradient-noise bound and a curvature volatility-controlled bound. We then combine these two indicators into a per-layer adaptive noise threshold on the effective step-size that anticipates trainability behavior. Using this insight, we propose a step-size scheduler that keeps each layer's effective parameter update below this bound, thereby avoiding loss of trainability. We demonstrate that our scheduler can improve the accuracy maintained by previously proposed approaches, such as concatenated ReLU (CReLU), Wasserstein regularizer, and L2 weight decay. Surprisingly, our scheduler produces adaptive step-size trajectories that, without tuning, mirror the manually engineered step-size decay schedules.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanism of Task-oriented Information Removal in In-context Learning</title>
<link>https://arxiv.org/abs/2509.21012</link>
<guid>https://arxiv.org/abs/2509.21012</guid>
<content:encoded><![CDATA[
arXiv:2509.21012v2 Announce Type: replace 
Abstract: In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impossibility of Inverse Permutation Learning in Transformer Models</title>
<link>https://arxiv.org/abs/2509.24125</link>
<guid>https://arxiv.org/abs/2509.24125</guid>
<content:encoded><![CDATA[
arXiv:2509.24125v2 Announce Type: replace 
Abstract: In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical'') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking'' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations).
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CRPS-LAM: Regional ensemble weather forecasting from matching marginals</title>
<link>https://arxiv.org/abs/2510.09484</link>
<guid>https://arxiv.org/abs/2510.09484</guid>
<content:encoded><![CDATA[
arXiv:2510.09484v2 Announce Type: replace 
Abstract: Machine learning for weather prediction increasingly relies on ensemble methods to provide probabilistic forecasts. Diffusion-based models have shown strong performance in Limited-Area Modeling (LAM) but remain computationally expensive at sampling time. Building on the success of global weather forecasting models trained based on Continuous Ranked Probability Score (CRPS), we introduce CRPS-LAM, a probabilistic LAM forecasting model trained with a CRPS-based objective. By sampling and injecting a single latent noise vector into the model, CRPS-LAM generates ensemble members in a single forward pass, achieving sampling speeds up to 39 times faster than a diffusion-based model. We evaluate the model on the MEPS regional dataset, where CRPS-LAM matches the low errors of diffusion models. By retaining also fine-scale forecast details, the method stands out as an effective approach for probabilistic regional weather forecasting
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Connection Between Score Matching and Local Intrinsic Dimension</title>
<link>https://arxiv.org/abs/2510.12975</link>
<guid>https://arxiv.org/abs/2510.12975</guid>
<content:encoded><![CDATA[
arXiv:2510.12975v2 Announce Type: replace 
Abstract: The local intrinsic dimension (LID) of data is a fundamental quantity in signal processing and learning theory, but quantifying the LID of high-dimensional, complex data has been a historically challenging task. Recent works have discovered that diffusion models capture the LID of data through the spectra of their score estimates and through the rate of change of their density estimates under various noise perturbations. While these methods can accurately quantify LID, they require either many forward passes of the diffusion model or use of gradient computation, limiting their applicability in compute- and memory-constrained scenarios.
  We show that the LID is a lower bound on the denoising score matching loss, motivating use of the denoising score matching loss as a LID estimator. Moreover, we show that the equivalent implicit score matching loss also approximates LID via the normal dimension and is closely related to a recent LID estimator, FLIPD. Our experiments on a manifold benchmark and with Stable Diffusion 3.5 indicate that the denoising score matching loss is a highly competitive and scalable LID estimator, achieving superior accuracy and memory footprint under increasing problem size and quantization level.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Category learning in deep neural networks: Information content and geometry of internal representations</title>
<link>https://arxiv.org/abs/2510.19021</link>
<guid>https://arxiv.org/abs/2510.19021</guid>
<content:encoded><![CDATA[
arXiv:2510.19021v2 Announce Type: replace 
Abstract: In humans and other animals, category learning enhances discrimination between stimuli close to the category boundary. This phenomenon, called categorical perception, was also empirically observed in artificial neural networks trained on classification tasks. In previous modeling works based on neuroscience data, we show that this expansion/compression is a necessary outcome of efficient learning. Here we extend our theoretical framework to artificial networks. We show that minimizing the Bayes cost (mean of the cross-entropy loss) implies maximizing the mutual information between the set of categories and the neural activities prior to the decision layer. Considering structured data with an underlying feature space of small dimension, we show that maximizing the mutual information implies (i) finding an appropriate projection space, and, (ii) building a neural representation with the appropriate metric. The latter is based on a Fisher information matrix measuring the sensitivity of the neural activity to changes in the projection space. Optimal learning makes this neural Fisher information follow a category-specific Fisher information, measuring the sensitivity of the category membership. Category learning thus induces an expansion of neural space near decision boundaries. We characterize the properties of the categorical Fisher information, showing that its eigenvectors give the most discriminant directions at each point of the projection space. We find that, unexpectedly, its maxima are in general not exactly at, but near, the class boundaries. Considering toy models and the MNIST dataset, we numerically illustrate how after learning the two Fisher information matrices match, and essentially align with the category boundaries. Finally, we relate our approach to the Information Bottleneck one, and we exhibit a bias-variance decomposition of the Bayes cost, of interest on its own.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation</title>
<link>https://arxiv.org/abs/2510.19296</link>
<guid>https://arxiv.org/abs/2510.19296</guid>
<content:encoded><![CDATA[
arXiv:2510.19296v3 Announce Type: replace 
Abstract: The remarkable progress of Large Language Models (LLMs) presents promising opportunities for Verilog code generation which is significantly important for automated circuit design. The lacking of meaningful functional rewards hinders the preference optimization based on Reinforcement Learning (RL) for producing functionally correct Verilog code. In this paper, we propose Signal-Aware Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments of functionally correct output signal to optimize RL training. Considering Verilog code specifies the structural interconnection of hardware gates and wires so that different output signals are independent, the key insight of QiMeng-SALV is to extract verified signal-aware implementations in partially incorrect modules, so as to enhance the extraction of meaningful functional rewards. Roughly, we verify the functional correctness of signals in generated module by comparing with that of reference module in the training data. Then abstract syntax tree (AST) is employed to identify signal-aware code segments which can provide meaningful functional rewards from erroneous modules. Finally, we introduce signal-aware DPO which is optimized on the correct signal-level code segments, thereby preventing noise and interference from incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from conventional module-level to fine-grained signal-level optimization in Verilog code generation, addressing the issue of insufficient functional rewards. Experiments demonstrate that our method achieves state-of-the-art performance on VerilogEval and RTLLM, with a 7B parameter model matching the performance of the DeepSeek v3 671B model and significantly outperforming the leading open-source model CodeV trained on the same dataset. Our code is available at https://github.com/zy1xxx/SALV.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>g-DPO: Scalable Preference Optimization for Protein Language Models</title>
<link>https://arxiv.org/abs/2510.19474</link>
<guid>https://arxiv.org/abs/2510.19474</guid>
<content:encoded><![CDATA[
arXiv:2510.19474v2 Announce Type: replace 
Abstract: Direct Preference Optimization (DPO) is an effective approach for aligning protein language models with experimental design goals. However, DPO faces a scalability bottleneck: the number of possible training pairs grows quadratically with the number of labeled sequences, leading to prohibitive training times even for modestly sized datasets. We introduce g-DPO, a framework that (i) uses sequence space clustering to prune redundant pairs while preserving training signal, and (ii) amortizes likelihood computations with group-based approximations. Across three protein engineering tasks, g-DPO maintains in silico and in vitro performance that is statistically indistinguishable from standard DPO, while converging 1.7x to 5.4x times faster, with speedups that scale with dataset size and the structure of the underlying mutational landscape.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP</title>
<link>https://arxiv.org/abs/2510.20169</link>
<guid>https://arxiv.org/abs/2510.20169</guid>
<content:encoded><![CDATA[
arXiv:2510.20169v2 Announce Type: replace 
Abstract: Traveling Salesman Problem (TSP) is a classic NP-hard problem that has garnered significant attention from both academia and industry. While neural-based methods have shown promise for solving TSPs, they still face challenges in scaling to larger instances, particularly in memory constraints associated with global heatmaps, edge weights, or access matrices, as well as in generating high-quality initial solutions and insufficient global guidance for efficiently navigating vast search spaces. To address these challenges, we propose a Hyper Tour Guided Neighborhood Search (HyperNS) method for large-scale TSP instances. Inspired by the ``clustering first, route second" strategy, our approach initially divides the TSP instance into clusters using a sparse heatmap graph and abstracts them as supernodes, followed by the generation of a hyper tour to guide both the initialization and optimization processes. This method reduces the search space by focusing on edges relevant to the hyper tour, leading to more efficient and effective optimization. Experimental results on both synthetic and real-world datasets demonstrate that our approach outperforms existing neural-based methods, particularly in handling larger-scale instances, offering a significant reduction in the gap to the optimal solution.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weak-to-Strong Generalization under Distribution Shifts</title>
<link>https://arxiv.org/abs/2510.21332</link>
<guid>https://arxiv.org/abs/2510.21332</guid>
<content:encoded><![CDATA[
arXiv:2510.21332v2 Announce Type: replace 
Abstract: As future superhuman models become increasingly complex, accurately supervising their behavior may exceed human capabilities. Recent works have demonstrated that in such scenarios, weak models can effectively supervise strong models, a phenomenon known as weak-to-strong generalization. However, we find that naive weak-to-strong generalization fails under distribution shifts, often leading to worse performance of the strong model than its weak supervisors. To address this, we propose RAVEN, a robust weak-to-strong generalization framework that dynamically learns the optimal combinations of weak models in addition to parameters of the strong model. We demonstrate the effectiveness of RAVEN on image classification, text classification, and preference alignment tasks. RAVEN outperforms alternative baselines by over 30% on out-of-distribution tasks while matching or surpassing existing methods on in-distribution tasks. Moreover, our results show that RAVEN assigns higher weights to more accurate weak models, demonstrating its ability to automatically identify trustworthy supervision.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides</title>
<link>https://arxiv.org/abs/2511.00209</link>
<guid>https://arxiv.org/abs/2511.00209</guid>
<content:encoded><![CDATA[
arXiv:2511.00209v2 Announce Type: replace 
Abstract: Diffusion models have emerged as a leading framework in generative modeling, poised to transform the traditionally slow and costly process of drug discovery. This review provides a systematic comparison of their application in designing two principal therapeutic modalities: small molecules and therapeutic peptides. We dissect how the unified framework of iterative denoising is adapted to the distinct molecular representations, chemical spaces, and design objectives of each modality. For small molecules, these models excel at structure-based design, generating novel, pocket-fitting ligands with desired physicochemical properties, yet face the critical hurdle of ensuring chemical synthesizability. Conversely, for therapeutic peptides, the focus shifts to generating functional sequences and designing de novo structures, where the primary challenges are achieving biological stability against proteolysis, ensuring proper folding, and minimizing immunogenicity. Despite these distinct challenges, both domains face shared hurdles: the scarcity of high-quality experimental data, the reliance on inaccurate scoring functions for validation, and the crucial need for experimental validation. We conclude that the full potential of diffusion models will be unlocked by bridging these modality-specific gaps and integrating them into automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby shifting the paradigm from mere chemical exploration to the on-demand engineering of novel~therapeutics.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding</title>
<link>https://arxiv.org/abs/2511.01695</link>
<guid>https://arxiv.org/abs/2511.01695</guid>
<content:encoded><![CDATA[
arXiv:2511.01695v3 Announce Type: replace 
Abstract: The growing demand for on-device large language model (LLM) inference highlights the need for efficient mobile edge computing (MEC) solutions, especially in resource-constrained settings. Speculative decoding offers a promising solution by partitioning token generation between a lightweight draft model on mobile devices and a powerful target model on edge servers, but suffers from communication overhead and asynchronous delays. This paper is the first to propose a unified framework that jointly optimizes user association and resource allocation (UARA) to support efficient parallel speculative decoding. We solve the UARA problem using a multi-agent deep reinforcement learning algorithm. To evaluate our approach under realistic conditions, we conduct experiments using the Sionna simulator. Results show that our method achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency without compromising inference accuracy, enabling scalable and low-latency LLM services in MEC systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HardFlow: Hard-Constrained Sampling for Flow-Matching Models via Trajectory Optimization</title>
<link>https://arxiv.org/abs/2511.08425</link>
<guid>https://arxiv.org/abs/2511.08425</guid>
<content:encoded><![CDATA[
arXiv:2511.08425v2 Announce Type: replace 
Abstract: Diffusion and flow-matching have emerged as powerful methodologies for generative modeling, with remarkable success in capturing complex data distributions and enabling flexible guidance at inference time. Many downstream applications, however, demand enforcing hard constraints on generated samples (for example, robot trajectories must avoid obstacles), a requirement that goes beyond simple guidance. Prevailing projection-based approaches constrain the entire sampling path to the constraint manifold, which is overly restrictive and degrades sample quality. In this paper, we introduce a novel framework that reformulates hard-constrained sampling as a trajectory optimization problem. Our key insight is to leverage numerical optimal control to steer the sampling trajectory so that constraints are satisfied precisely at the terminal time. By exploiting the underlying structure of flow-matching models and adopting techniques from model predictive control, we transform this otherwise complex constrained optimization problem into a tractable surrogate that can be solved efficiently and effectively. Furthermore, this trajectory optimization perspective offers significant flexibility beyond mere constraint satisfaction, allowing for the inclusion of integral costs to minimize distribution shift and terminal objectives to further enhance sample quality, all within a unified framework. We provide a control-theoretic analysis of our method, establishing bounds on the approximation error between our tractable surrogate and the ideal formulation. Extensive experiments across diverse domains, including robotics (planning), partial differential equations (boundary control), and vision (text-guided image editing), demonstrate that our algorithm, which we name $\textit{HardFlow}$, substantially outperforms existing methods in both constraint satisfaction and sample quality.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Global and Local Bounds in Gaussian Process Regression via Chaining</title>
<link>https://arxiv.org/abs/2511.09144</link>
<guid>https://arxiv.org/abs/2511.09144</guid>
<content:encoded><![CDATA[
arXiv:2511.09144v3 Announce Type: replace 
Abstract: Gaussian process regression (GPR) is a popular nonparametric Bayesian method that provides predictive uncertainty estimates and is widely used in safety-critical applications. While prior research has introduced various uncertainty bounds, most existing approaches require access to specific input features, and rely on posterior mean and variance estimates or the tuning of hyperparameters. These limitations hinder robustness and fail to capture the model's global behavior in expectation. To address these limitations, we propose a chaining-based framework for estimating upper and lower bounds on the expected extreme values over unseen data, without requiring access to specific input features. We provide kernel-specific refinements for commonly used kernels such as RBF and Mat\'ern, in which our bounds are tighter than generic constructions. We further improve numerical tightness by avoiding analytical relaxations. In addition to global estimation, we also develop a novel method for local uncertainty quantification at specified inputs. This approach leverages chaining geometry through partition diameters, adapting to local structures without relying on posterior variance scaling. Our experimental results validate the theoretical findings and demonstrate that our method outperforms existing approaches on both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners</title>
<link>https://arxiv.org/abs/2511.10234</link>
<guid>https://arxiv.org/abs/2511.10234</guid>
<content:encoded><![CDATA[
arXiv:2511.10234v2 Announce Type: replace 
Abstract: While promising, graph reasoners based on Large Language Models (LLMs) lack built-in invariance to symmetries in graph representations. Operating on sequential graph serializations, LLMs can produce different outputs under node reindexing, edge reordering, or formatting changes, raising robustness concerns. We systematically analyze these effects, studying how fine-tuning impacts encoding sensitivity as well generalization on unseen tasks. We propose a principled decomposition of graph serializations into node labeling, edge encoding, and syntax, and evaluate LLM robustness to variations of each of these factors on a comprehensive benchmarking suite. We also contribute a novel set of spectral tasks to further assess generalization abilities of fine-tuned reasoners. Results show that larger (non-fine-tuned) models are more robust. Fine-tuning reduces sensitivity to node relabeling but may increase it to variations in structure and format, while it does not consistently improve performance on unseen tasks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design</title>
<link>https://arxiv.org/abs/2511.12489</link>
<guid>https://arxiv.org/abs/2511.12489</guid>
<content:encoded><![CDATA[
arXiv:2511.12489v2 Announce Type: replace 
Abstract: Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling</title>
<link>https://arxiv.org/abs/2511.15250</link>
<guid>https://arxiv.org/abs/2511.15250</guid>
<content:encoded><![CDATA[
arXiv:2511.15250v2 Announce Type: replace 
Abstract: With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Nuclear Reactor Core Simulation through Data-Based Surrogate Models</title>
<link>https://arxiv.org/abs/2511.16148</link>
<guid>https://arxiv.org/abs/2511.16148</guid>
<content:encoded><![CDATA[
arXiv:2511.16148v2 Announce Type: replace 
Abstract: In recent years, there has been an increasing need for Nuclear Power Plants (NPPs) to improve flexibility in order to match the rapid growth of renewable energies. The Operator Assistance Predictive System (OAPS) developed by Framatome addresses this problem through Model Predictive Control (MPC). In this work, we aim to improve MPC methods through data-driven simulation schemes. Thus, from a set of nonlinear stiff ordinary differential equations (ODEs), this paper introduces two surrogate models acting as alternative simulation schemes to enhance nuclear reactor core simulation. We show that both data-driven and physics-informed models can rapidly integrate complex dynamics, with a very low computational time (up to 1000x time reduction).
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hard Samples, Bad Labels: Robust Loss Functions That Know When to Back Off</title>
<link>https://arxiv.org/abs/2511.16512</link>
<guid>https://arxiv.org/abs/2511.16512</guid>
<content:encoded><![CDATA[
arXiv:2511.16512v2 Announce Type: replace 
Abstract: Incorrectly labelled training data are frustratingly ubiquitous in both benchmark and specially curated datasets. Such mislabelling clearly adversely affects the performance and generalizability of models trained through supervised learning on the associated datasets. Frameworks for detecting label errors typically require well-trained / well-generalized models; however, at the same time most frameworks rely on training these models on corrupt data, which clearly has the effect of reducing model generalizability and subsequent effectiveness in error detection -- unless a training scheme robust to label errors is employed. We evaluate two novel loss functions, Blurry Loss and Piecewise-zero Loss, that enhance robustness to label errors by de-weighting or disregarding difficult-to-classify samples, which are likely to be erroneous. These loss functions leverage the idea that mislabelled examples are typically more difficult to classify and should contribute less to the learning signal. Comprehensive experiments on a variety of artificially corrupted datasets demonstrate that the proposed loss functions outperform state-of-the-art robust loss functions in nearly all cases, achieving superior F1 scores for error detection. Further analyses through ablation studies offer insights to confirm these loss functions' broad applicability to cases of both uniform and non-uniform corruption, and with different label error detection frameworks. By using these robust loss functions, machine learning practitioners can more effectively identify, prune, or correct errors in their training data.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LTD: Low Temperature Distillation for Gradient Masking-free Adversarial Training</title>
<link>https://arxiv.org/abs/2111.02331</link>
<guid>https://arxiv.org/abs/2111.02331</guid>
<content:encoded><![CDATA[
arXiv:2111.02331v4 Announce Type: replace-cross 
Abstract: Adversarial training is a widely adopted strategy to bolster the robustness of neural network models against adversarial attacks. This paper revisits the fundamental assumptions underlying image classification and suggests that representing data as one-hot labels is a key factor that leads to vulnerabilities. However, in real-world datasets, data ambiguity often arises, with samples exhibiting characteristics of multiple classes, rendering one-hot label representations imprecise. To address this, we introduce a novel approach, Low-Temperature Distillation (LTD), designed to refine label representations. Unlike previous approaches, LTD incorporates a relatively low temperature in the teacher model, while maintaining a fixed temperature for the student model during both training and inference. This strategy not only refines assumptions about data distribution but also strengthens model robustness and avoids the gradient masking problem commonly encountered in defensive distillation. Experimental results demonstrate the efficacy of the proposed method when combined with existing frameworks, achieving robust accuracy rates of 58.19%, 31.13%, and 42.08% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, without the need for additional data.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factor-Assisted Federated Learning for Personalized Optimization with Heterogeneous Data</title>
<link>https://arxiv.org/abs/2312.04281</link>
<guid>https://arxiv.org/abs/2312.04281</guid>
<content:encoded><![CDATA[
arXiv:2312.04281v2 Announce Type: replace-cross 
Abstract: Federated learning is an emerging distributed machine learning framework aiming at protecting data privacy. Data heterogeneity is one of the core challenges in federated learning, which could severely degrade the convergence rate and prediction performance of deep neural networks. To address this issue, we develop a novel personalized federated learning framework for heterogeneous data, which we refer to as FedSplit. This modeling framework is motivated by the finding that, data in different clients contain both common knowledge and personalized knowledge. Then the hidden elements in each neural layer can be split into the shared and personalized groups. With this decomposition, a novel objective function is established and optimized. We demonstrate FedSplit enjoyers a faster convergence speed than the standard federated learning method both theoretically and empirically. The generalization bound of the FedSplit method is also studied. To practically implement the proposed method on real datasets, factor analysis is introduced to facilitate the decoupling of hidden elements. This leads to a practically implemented model for FedSplit and we further refer to as FedFac. We demonstrated by simulation studies that, using factor analysis can well recover the underlying shared/personalized decomposition. The superior prediction performance of FedFac is further verified empirically by comparison with various state-of-the-art federated learning methods on several real datasets.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Bilevel Optimization: A Perspective from Transient Iteration Complexity</title>
<link>https://arxiv.org/abs/2402.03167</link>
<guid>https://arxiv.org/abs/2402.03167</guid>
<content:encoded><![CDATA[
arXiv:2402.03167v5 Announce Type: replace-cross 
Abstract: Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, most decentralized SBO algorithms focus solely on asymptotic convergence rates, overlooking transient iteration complexity-the number of iterations required before asymptotic rates dominate, which results in limited understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. To address this issue, this paper introduces D-SOBA, a Decentralized Stochastic One-loop Bilevel Algorithm framework. D-SOBA comprises two variants: D-SOBA-SO, which incorporates second-order Hessian and Jacobian matrices, and D-SOBA-FO, which relies entirely on first-order gradients. We provide a comprehensive non-asymptotic convergence analysis and establish the transient iteration complexity of D-SOBA. This provides the first theoretical understanding of how network topology, data heterogeneity, and nested bilevel structures influence decentralized SBO. Extensive experimental results demonstrate the efficiency and theoretical advantages of D-SOBA.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Efficient LLMs</title>
<link>https://arxiv.org/abs/2402.14746</link>
<guid>https://arxiv.org/abs/2402.14746</guid>
<content:encoded><![CDATA[
arXiv:2402.14746v5 Announce Type: replace-cross 
Abstract: Recent LLMs have hundreds of billions of parameters consuming vast resources. Furthermore, the so called "AI scaling law" for transformers suggests that the number of parameters must scale linearly with the size of the data. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, by comparing theoretical and empirical estimates of the Kullback-Leibler divergence, we derive a natural AI scaling law that the number of parameters in an efficient LLM scales as $D^{\gamma}$ where $D$ is the size of the training data and $ \gamma \in [0.44, 0.72]$, suggesting the existence of more efficient architectures. Against this backdrop, we propose recurrent transformers, combining the efficacy of transformers with the efficiency of recurrent networks, progressively applying a single transformer layer to a fixed-width sliding window across the input sequence. Recurrent transformers (a) run in linear time in the sequence length, (b) are memory-efficient and amenable to parallel processing in large batches, (c) learn to forget history for language tasks, or accumulate history for long range tasks like copy and selective copy, and (d) are amenable to curriculum training to overcome vanishing gradients. In our experiments, we find that recurrent transformers perform favorably on benchmark tests.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Catalyst Framework for the Quantum Linear System Problem via the Proximal Point Algorithm</title>
<link>https://arxiv.org/abs/2406.13879</link>
<guid>https://arxiv.org/abs/2406.13879</guid>
<content:encoded><![CDATA[
arXiv:2406.13879v2 Announce Type: replace-cross 
Abstract: Solving systems of linear equations is a fundamental problem, but it can be computationally intensive for classical algorithms in high dimensions. Existing quantum algorithms can achieve exponential speedups for the quantum linear system problem (QLSP) in terms of the problem dimension, but the advantage is bottlenecked by condition number of the coefficient matrix. In this work, we propose a new quantum algorithm for QLSP inspired by the classical proximal point algorithm (PPA). Our proposed method can be viewed as a meta-algorithm that allows inverting a modified matrix via an existing \texttt{QLSP\_solver}, thereby directly approximating the solution vector instead of approximating the inverse of the coefficient matrix. By carefully choosing the step size $\eta$, the proposed algorithm can effectively precondition the linear system to mitigate the dependence on condition numbers that hindered the applicability of previous approaches. Importantly, this is the first iterative framework for QLSP where a tunable parameter $\eta$ and initialization $x_0$ allows controlling the trade-off between the runtime and approximation error.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Gray-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse</title>
<link>https://arxiv.org/abs/2408.10901</link>
<guid>https://arxiv.org/abs/2408.10901</guid>
<content:encoded><![CDATA[
arXiv:2408.10901v4 Announce Type: replace-cross 
Abstract: Recent advancements in Latent Diffusion Models (LDMs) have revolutionized image synthesis and manipulation, raising significant concerns about data misappropriation and intellectual property infringement. While adversarial attacks have been extensively explored as a protective measure against such misuse of generative AI, current approaches are severely limited by their heavy reliance on model-specific knowledge and substantial computational costs. Drawing inspiration from the posterior collapse phenomenon observed in VAE training, we propose the Posterior Collapse Attack (PCA), a novel framework for protecting images from unauthorized manipulation. Through comprehensive theoretical analysis and empirical validation, we identify two distinct collapse phenomena during VAE inference: diffusion collapse and concentration collapse. Based on this discovery, we design a unified loss function that can flexibly achieve both types of collapse through parameter adjustment, each corresponding to different protection objectives in preventing image manipulation. Our method significantly reduces dependence on model-specific knowledge by requiring access to only the VAE encoder, which constitutes less than 4\% of LDM parameters. Notably, PCA achieves prompt-invariant protection by operating on the VAE encoder before text conditioning occurs, eliminating the need for empty prompt optimization required by existing methods. This minimal requirement enables PCA to maintain adequate transferability across various VAE-based LDM architectures while effectively preventing unauthorized image editing. Extensive experiments show PCA outperforms existing techniques in protection effectiveness, computational efficiency (runtime and VRAM), and generalization across VAE-based LDM variants. Our code is available at https://github.com/ZhongliangGuo/PosteriorCollapseAttack.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Earth Observation Satellite Scheduling with Graph Neural Networks and Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2408.15041</link>
<guid>https://arxiv.org/abs/2408.15041</guid>
<content:encoded><![CDATA[
arXiv:2408.15041v2 Announce Type: replace-cross 
Abstract: Earth Observation Satellite Planning (EOSP) is a difficult optimization problem with considerable practical interest. A set of requested observations must be scheduled on an agile Earth observation satellite while respecting constraints on their visibility window, as well as maneuver constraints that impose varying delays between successive observations. In addition, the problem is largely oversubscribed: there are much more candidate observations than can possibly be achieved. Therefore, one must select the set of observations that will be performed while maximizing their cumulative benefit and propose a feasible schedule for these observations. As previous work mostly focused on heuristic and iterative search algorithms, this paper presents a new technique for selecting and scheduling observations based on Graph Neural Networks (GNNs) and Deep Reinforcement Learning (DRL). GNNs are used to extract relevant information from the graphs representing instances of the EOSP, and DRL drives the search for optimal schedules. A post-learning search step based on Monte Carlo Tree Search (MCTS) is added that is able to find even better solutions. Experiments show that it is able to learn on small problem instances and generalize to larger real-world instances, with very competitive performance compared to traditional approaches.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models</title>
<link>https://arxiv.org/abs/2412.15287</link>
<guid>https://arxiv.org/abs/2412.15287</guid>
<content:encoded><![CDATA[
arXiv:2412.15287v2 Announce Type: replace-cross 
Abstract: Recent studies have indicated that effectively utilizing inference-time compute is crucial for attaining better performance from large language models (LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm, in which the model is fine-tuned in a manner that directly optimizes the performance of the inference-time strategy. We study this paradigm using the simple yet effective Best-of-N (BoN) inference strategy, in which a verifier selects the best out of a set of LLM-generated responses. We devise the first imitation learning and reinforcement learning~(RL) methods for BoN-aware fine-tuning, overcoming the challenging, non-differentiable argmax operator within BoN. We empirically demonstrate that our BoN-aware models implicitly learn a meta-strategy that interleaves best responses with more diverse responses that might be better suited to a test-time input -- a process reminiscent of the exploration-exploitation trade-off in RL. Our experiments demonstrate the effectiveness of BoN-aware fine-tuning in terms of improved performance and inference-time compute. In particular, we show that our methods improve the Bo32 performance of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%, and pass@32 from 60.0% to 67.0%, as well as the pass@16 on HumanEval from 61.6% to 67.1%.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LASER: Lip Landmark Assisted Speaker Detection for Robustness</title>
<link>https://arxiv.org/abs/2501.11899</link>
<guid>https://arxiv.org/abs/2501.11899</guid>
<content:encoded><![CDATA[
arXiv:2501.11899v2 Announce Type: replace-cross 
Abstract: Active Speaker Detection (ASD) aims to identify who is speaking in complex visual scenes. While humans naturally rely on lip-audio synchronization, existing ASD models often misclassify non-speaking instances when lip movements and audio are unsynchronized. To address this, we propose Lip landmark Assisted Speaker dEtection for Robustness (LASER), which explicitly incorporates lip landmarks during training to guide the model's attention to speech-relevant regions. Given a face track, LASER extracts visual features and encodes 2D lip landmarks into dense maps. To handle failure cases such as low resolution or occlusion, we introduce an auxiliary consistency loss that aligns lip-aware and face-only predictions, removing the need for landmark detectors at test time. LASER outperforms state-of-the-art models across both in-domain and out-of-domain benchmarks. To further evaluate robustness in realistic conditions, we introduce LASER-bench, a curated dataset of modern video clips with varying levels of background noise. On the high-noise subset, LASER improves mAP by 3.3 and 4.3 points over LoCoNet and TalkNet, respectively, demonstrating strong resilience to real-world acoustic challenges.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Object Detection for Indoor Navigation Assistance: A Performance Evaluation of Real-Time Algorithms</title>
<link>https://arxiv.org/abs/2501.18444</link>
<guid>https://arxiv.org/abs/2501.18444</guid>
<content:encoded><![CDATA[
arXiv:2501.18444v2 Announce Type: replace-cross 
Abstract: This study addresses the need for accurate and efficient object detection in assistive technologies for visually impaired individuals. We evaluate four real-time object detection algorithms YOLO, SSD, Faster R-CNN, and Mask R-CNN within the context of indoor navigation assistance. Using the Indoor Objects Detection dataset, we analyze detection accuracy, processing speed, and adaptability to indoor environments. Our findings highlight the trade-offs between precision and efficiency, offering insights into selecting optimal algorithms for realtime assistive navigation. This research advances adaptive machine learning applications, enhancing indoor navigation solutions for the visually impaired and promoting accessibility.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superstate Quantum Mechanics</title>
<link>https://arxiv.org/abs/2502.00037</link>
<guid>https://arxiv.org/abs/2502.00037</guid>
<content:encoded><![CDATA[
arXiv:2502.00037v3 Announce Type: replace-cross 
Abstract: We introduce Superstate Quantum Mechanics (SQM), a theory that considers states in Hilbert space subject to multiple quadratic constraints, with ``energy'' also expressed as a quadratic function of these states. Traditional quantum mechanics corresponds to a single quadratic constraint of wavefunction normalization with energy expressed as a quadratic form involving the Hamiltonian. When SQM represents states as unitary operators, the stationary problem becomes a quantum inverse problem with multiple applications in physics, machine learning, and artificial intelligence. Any stationary SQM problem is equivalent to a new algebraic problem that we address in this paper. The non-stationary SQM problem considers the evolution of the system itself, involving the same ``energy'' operator as in the stationary case. Two possible options for the SQM dynamic equation are considered: (1) within the framework of linear maps from higher-order quantum theory, where 2D-type quantum circuits transform one quantum system into another; and (2) in the form of a Gross-Pitaevskii-type nonlinear map. Although no known physical process currently describes such 2D dynamics, this approach naturally bridges direct and inverse quantum mechanics problems, allowing for the development of a new type of computer algorithms. As an immediately available practical application of the theory, we consider using a quantum channel as a classical computational model; this type of computation can be performed on a classical computer.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meursault as a Data Point</title>
<link>https://arxiv.org/abs/2502.01364</link>
<guid>https://arxiv.org/abs/2502.01364</guid>
<content:encoded><![CDATA[
arXiv:2502.01364v2 Announce Type: replace-cross 
Abstract: In an era dominated by datafication, the reduction of human experiences to quantifiable metrics raises profound philosophical and ethical questions. This paper explores these issues through the lens of Meursault, the protagonist of Albert Camus' The Stranger, whose emotionally detached existence epitomizes the existential concept of absurdity. Using natural language processing (NLP) techniques including emotion detection (BERT), sentiment analysis (VADER), and named entity recognition (spaCy)-this study quantifies key events and behaviors in Meursault's life. Our analysis reveals the inherent limitations of applying algorithmic models to complex human experiences, particularly those rooted in existential alienation and moral ambiguity. By examining how modern AI tools misinterpret Meursault's actions and emotions, this research underscores the broader ethical dilemmas of reducing nuanced human narratives to data points, challenging the foundational assumptions of our data-driven society. The findings presented in this paper serve as a critique of the increasing reliance on data-driven narratives and advocate for incorporating humanistic values in artificial intelligence.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPability: A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness</title>
<link>https://arxiv.org/abs/2502.14914</link>
<guid>https://arxiv.org/abs/2502.14914</guid>
<content:encoded><![CDATA[
arXiv:2502.14914v4 Announce Type: replace-cross 
Abstract: Visual captioning benchmarks have become outdated with the emergence of modern multimodal large language models (MLLMs), as the brief ground-truth sentences and traditional metrics fail to assess detailed captions effectively. While recent benchmarks attempt to address this by focusing on keyword extraction or object-centric evaluation, they remain limited to vague-view or object-view analyses and incomplete visual element coverage. In this paper, we introduce CAPability, a comprehensive multi-view benchmark for evaluating visual captioning across 12 dimensions spanning six critical views. We curate nearly 11K human-annotated images and videos with visual element annotations to evaluate the generated captions. CAPability stably assesses both the correctness and thoroughness of captions with \textit{precision} and \textit{hit} metrics. By converting annotations to QA pairs, we further introduce a heuristic metric, \textit{know but cannot tell} ($K\bar{T}$), indicating a significant performance gap between QA and caption capabilities. Our work provides a holistic analysis of MLLMs' captioning abilities, as we identify their strengths and weaknesses across various dimensions, guiding future research to enhance specific aspects of their capabilities.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Stochastic Dynamics from Non-Sequential Data (IDyNSD)</title>
<link>https://arxiv.org/abs/2502.17690</link>
<guid>https://arxiv.org/abs/2502.17690</guid>
<content:encoded><![CDATA[
arXiv:2502.17690v3 Announce Type: replace-cross 
Abstract: Inferring stochastic dynamics from data is central across the sciences, yet in many applications only unordered, non-sequential measurements are available-often restricted to limited regions of state space-so standard time-series methods do not apply. We introduce IDyNSD, a first-principles framework that identifies unknown dynamical parameters from such non-sequential data by minimizing Fokker-Planck residuals. We develop two complementary routes: a local route that handles region-restricted data via locally estimated scores, and a global route that fits dynamics from globally sampled data using a kernel Stein discrepancy without explicit density or score estimation. When the dynamics are affine in the unknown parameters, we prove a necessary-and-sufficient condition for the existence and uniqueness of the inferred parameters and derive a sensitivity analysis that identifies which parameters are tightly constrained by the data and which remain effectively free under over-parameterization. For general non-affine case, both routes define differentiable losses amenable to gradient-based optimization. As demonstrations, we recover (i) the three parameters of a stochastic Lorenz system from non-sequential data (region-restricted data for the local route and full steady-state data for the global route) and (ii) a 3x7interaction matrix of a nonlinear gene-regulatory network derived from a published B-cell differentiation model, using only unordered steady-state samples and applying the global route. Finally, we show that the same Fokker-Planck residual viewpoint supports a "dynamics-to-density" complement that trains a normalized density estimator directly from known dynamics without any observations. Overall, IDyNSD provides two first-principles routes for system-identification from non-sequential data, grounded in the Fokker-Planck equation, that link data, density, and stochastic dynamics.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Image Generation for Recommendations Beyond Catalogs</title>
<link>https://arxiv.org/abs/2502.18477</link>
<guid>https://arxiv.org/abs/2502.18477</guid>
<content:encoded><![CDATA[
arXiv:2502.18477v3 Announce Type: replace-cross 
Abstract: Personalization is central to human-AI interaction, yet current diffusion-based image generation systems remain largely insensitive to user diversity. Existing attempts to address this often rely on costly paired preference data or introduce latency through Large Language Models. In this work, we introduce REBECA (REcommendations BEyond CAtalogs), a lightweight and scalable framework for personalized image generation that learns directly from implicit feedback signals such as likes, ratings, and clicks. Instead of fine-tuning the underlying diffusion model, REBECA employs a two-stage process: training a conditional diffusion model to sample user- and rating-specific image embeddings, which are subsequently decoded into images using a pretrained diffusion backbone. This approach enables efficient, fine-tuning-free personalization across large user bases. We rigorously evaluate REBECA on real-world datasets, proposing a novel statistical personalization verifier and a permutation-based hypothesis test to assess preference alignment. Our results demonstrate that REBECA consistently produces high-fidelity images tailored to individual tastes, outperforming baselines while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction</title>
<link>https://arxiv.org/abs/2503.08594</link>
<guid>https://arxiv.org/abs/2503.08594</guid>
<content:encoded><![CDATA[
arXiv:2503.08594v2 Announce Type: replace-cross 
Abstract: Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Filter Like You Test: Data-Driven Data Filtering for CLIP Pretraining</title>
<link>https://arxiv.org/abs/2503.08805</link>
<guid>https://arxiv.org/abs/2503.08805</guid>
<content:encoded><![CDATA[
arXiv:2503.08805v3 Announce Type: replace-cross 
Abstract: We introduce Filter Like You Test (FLYT), an algorithm for curating large-scale vision-language datasets that learns the usefulness of each data point as a pretraining example. FLYT trains a scoring model that learns to weigh each example's features using gradient signals from downstream tasks training sets. Based on FLYT, we implement Mixing-FLYT (M-FLYT), which takes the per-example scores generated by different scoring methods as features, and learns to unify them into a single score. FLYT naturally produces a distribution over the training examples, which we leverage through Soft Cap Sampling (SCS), a strategy for obtaining a filtered pretraining dataset from per-example probabilities that samples examples while preventing over-representation through a repetition penalty. Using these methods, we achieve 40.1% ImageNet zero-shot accuracy on the DataComp medium scale filtering benchmark, a 2% absolute accuracy increase over all previous results and a 5.5% increase over results that - like us - use only public resources. Our approach also yields 37.7\% on the average of 38 DataComp evaluation tasks, outperforming previous public-resource approaches by 0.4\%.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy Optimization and Multi-agent Reinforcement Learning for Mean-variance Team Stochastic Games</title>
<link>https://arxiv.org/abs/2503.22779</link>
<guid>https://arxiv.org/abs/2503.22779</guid>
<content:encoded><![CDATA[
arXiv:2503.22779v3 Announce Type: replace-cross 
Abstract: We study a long-run mean-variance team stochastic game (MV-TSG), where each agent shares a common mean-variance objective for the system and takes actions independently to maximize it. MV-TSG has two main challenges. First, the variance metric is neither additive nor Markovian in a dynamic setting. Second, simultaneous policy updates of all agents lead to a non-stationary environment for each individual agent. Both challenges make dynamic programming inapplicable. In this paper, we study MV-TSGs from the perspective of sensitivity-based optimization. The performance difference and performance derivative formulas for joint policies are derived, which provide optimization information for MV-TSGs. We prove the existence of a deterministic Nash policy for this problem. Subsequently, we propose a Mean-Variance Multi-Agent Policy Iteration (MV-MAPI) algorithm with a sequential update scheme, where individual agent policies are updated one by one in a given order. We prove that the MV-MAPI algorithm converges to a first-order stationary point of the objective function. By analyzing the local geometry of stationary points, we derive specific conditions for stationary points to be (local) Nash equilibria, and further, strict local optima. To solve large-scale MV-TSGs in scenarios with unknown environmental parameters, we extend the idea of trust region methods to MV-MAPI and develop a multi-agent reinforcement learning algorithm named Mean-Variance Multi-Agent Trust Region Policy Optimization (MV-MATRPO). We derive a performance lower bound for each update of joint policies. Finally, numerical experiments on energy management in multiple microgrid systems are conducted.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks</title>
<link>https://arxiv.org/abs/2505.21426</link>
<guid>https://arxiv.org/abs/2505.21426</guid>
<content:encoded><![CDATA[
arXiv:2505.21426v2 Announce Type: replace-cross 
Abstract: Agent-Based Models (ABMs) are powerful tools for studying emergent properties in complex systems. In ABMs, agent behaviors are governed by local interactions and stochastic rules. However, these rules are, in general, non-differentiable, limiting the use of gradient-based methods for optimization, and thus integration with real-world data. We propose a novel framework to learn a differentiable surrogate of any ABM by observing its generated data. Our method combines diffusion models to capture behavioral stochasticity and graph neural networks to model agent interactions. Distinct from prior surrogate approaches, our method introduces a fundamental shift: rather than approximating system-level outputs, it models individual agent behavior directly, preserving the decentralized, bottom-up dynamics that define ABMs. We validate our approach on two ABMs (Schelling's segregation model and a Predator-Prey ecosystem) showing that it replicates individual-level patterns and accurately forecasts emergent dynamics beyond training. Our results demonstrate the potential of combining diffusion models and graph learning for data-driven ABM simulation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training</title>
<link>https://arxiv.org/abs/2506.04263</link>
<guid>https://arxiv.org/abs/2506.04263</guid>
<content:encoded><![CDATA[
arXiv:2506.04263v2 Announce Type: replace-cross 
Abstract: Adversarial training is among the most effective strategies for defending deep neural networks against adversarial examples. A key limitation of existing adversarial training approaches lies in their reliance on a fixed perturbation budget, which fails to account for instance-specific robustness characteristics. While prior works such as IAAT and MMA introduce instance-level adaptations, they often rely on heuristic or static approximations of data robustness. In this paper, we propose Dynamic Epsilon Scheduling (DES), a novel framework that adaptively adjusts the adversarial perturbation budget per instance and per training iteration. DES integrates three key factors: (1) the distance to the decision boundary approximated via gradient-based proxies, (2) prediction confidence derived from softmax entropy, and (3) model uncertainty estimated via Monte Carlo dropout. By combining these cues into a unified scheduling strategy, DES tailors the perturbation budget dynamically to guide more effective adversarial learning. Experimental results on CIFAR-10 and CIFAR-100 show that our method consistently improves both adversarial robustness and standard accuracy compared to fixed-epsilon baselines and prior adaptive methods. Moreover, we provide theoretical insights into the stability and convergence of our scheduling policy. This work opens a new avenue for instance-aware, data-driven adversarial training methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Momentum Multi-Marginal Schr\"odinger Bridge Matching</title>
<link>https://arxiv.org/abs/2506.10168</link>
<guid>https://arxiv.org/abs/2506.10168</guid>
<content:encoded><![CDATA[
arXiv:2506.10168v2 Announce Type: replace-cross 
Abstract: Understanding complex systems by inferring trajectories from sparse sample snapshots is a fundamental challenge in a wide range of domains, e.g., single-cell biology, meteorology, and economics. Despite advancements in Bridge and Flow matching frameworks, current methodologies rely on pairwise interpolation between adjacent snapshots. This hinders their ability to capture long-range temporal dependencies and potentially affects the coherence of the inferred trajectories. To address these issues, we introduce \textbf{Momentum Multi-Marginal Schr\"odinger Bridge Matching (3MSBM)}, a novel matching framework that learns smooth measure-valued splines for stochastic systems that satisfy multiple positional constraints. This is achieved by lifting the dynamics to phase space and generalizing stochastic bridges to be conditioned on several points, forming a multi-marginal conditional stochastic optimal control problem. The underlying dynamics are then learned by minimizing a variational objective, having fixed the path induced by the multi-marginal conditional bridge. As a matching approach, 3MSBM learns transport maps that preserve intermediate marginals throughout training, significantly improving convergence and scalability. Extensive experimentation in a series of real-world applications validates the superior performance of 3MSBM compared to existing methods in capturing complex dynamics with temporal dependencies, opening new avenues for training matching frameworks in multi-marginal settings.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constructing Extreme Heatwave Storylines with Differentiable Climate Models</title>
<link>https://arxiv.org/abs/2506.10660</link>
<guid>https://arxiv.org/abs/2506.10660</guid>
<content:encoded><![CDATA[
arXiv:2506.10660v3 Announce Type: replace-cross 
Abstract: Understanding the plausible upper bounds of extreme weather events is essential for risk assessment in a warming climate. Existing methods, based on large ensembles of physics-based models, are often computationally expensive or lack the fidelity needed to simulate rare, high-impact extremes. Here, we present a novel framework that leverages a differentiable hybrid climate model, NeuralGCM, to optimize initial conditions and generate physically consistent worst-case heatwave trajectories. Applied to the 2021 Pacific Northwest heatwave, our method produces heatwave intensity up to 3.7 $^\circ$C above the most extreme member of a 75-member ensemble. These trajectories feature intensified atmospheric blocking and amplified Rossby wave patterns-hallmarks of severe heat events. Our results demonstrate that differentiable climate models can efficiently explore the upper tails of event likelihoods, providing a powerful new approach for constructing targeted storylines of extreme weather under climate change.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demystifying Spectral Feature Learning for Instrumental Variable Regression</title>
<link>https://arxiv.org/abs/2506.10899</link>
<guid>https://arxiv.org/abs/2506.10899</guid>
<content:encoded><![CDATA[
arXiv:2506.10899v3 Announce Type: replace-cross 
Abstract: We address the problem of causal effect estimation in the presence of hidden confounders, using nonparametric instrumental variable (IV) regression. A leading strategy employs spectral features - that is, learned features spanning the top eigensubspaces of the operator linking treatments to instruments. We derive a generalization error bound for a two-stage least squares estimator based on spectral features, and gain insights into the method's performance and failure modes. We show that performance depends on two key factors, leading to a clear taxonomy of outcomes. In a good scenario, the approach is optimal. This occurs with strong spectral alignment, meaning the structural function is well-represented by the top eigenfunctions of the conditional operator, coupled with this operator's slow eigenvalue decay, indicating a strong instrument. Performance degrades in a bad scenario: spectral alignment remains strong, but rapid eigenvalue decay (indicating a weaker instrument) demands significantly more samples for effective feature learning. Finally, in the ugly scenario, weak spectral alignment causes the method to fail, regardless of the eigenvalues' characteristics. Our synthetic experiments empirically validate this taxonomy. We further introduce a practical procedure to estimate these spectral properties from data, allowing practitioners to diagnose which regime a given problem falls into. We apply this method to the dSprites dataset, demonstrating its utility.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Structure-Content Trade-off in Knowledge Graph Retrieval</title>
<link>https://arxiv.org/abs/2506.13380</link>
<guid>https://arxiv.org/abs/2506.13380</guid>
<content:encoded><![CDATA[
arXiv:2506.13380v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) increasingly rely on knowledge graphs for factual reasoning, yet how retrieval design shapes their performance remains unclear. We examine how question decomposition changes the retrieved subgraph's content and structure. Using a hybrid retrieval function that controls the importance of initial question and subquestions, we show that subquestion-based retrieval improves content precision, but yields disjoint subgraphs, while question-based retrieval maintains structure at the cost of relevance. Optimal performance arises between these extremes, revealing that balancing retrieval content and structure is key to effective LLM reasoning over structured knowledge.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible AI-Informed Conception of Rigor</title>
<link>https://arxiv.org/abs/2506.14652</link>
<guid>https://arxiv.org/abs/2506.14652</guid>
<content:encoded><![CDATA[
arXiv:2506.14652v2 Announce Type: replace-cross 
Abstract: In AI research and practice, rigor remains largely understood in terms of methodological rigor -- such as whether mathematical, statistical, or computational methods are correctly applied. We argue that this narrow conception of rigor has contributed to the concerns raised by the responsible AI community, including overblown claims about the capabilities of AI systems. Our position is that a broader conception of what rigorous AI research and practice should entail is needed. We believe such a conception -- in addition to a more expansive understanding of (1) methodological rigor -- should include aspects related to (2) what background knowledge informs what to work on (epistemic rigor); (3) how disciplinary, community, or personal norms, standards, or beliefs influence the work (normative rigor); (4) how clearly articulated the theoretical constructs under use are (conceptual rigor); (5) what is reported and how (reporting rigor); and (6) how well-supported the inferences from existing evidence are (interpretative rigor). In doing so, we also provide useful language and a framework for much-needed dialogue about the AI community's work by researchers, policymakers, journalists, and other stakeholders.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finite-Time Minimax Bounds and an Optimal Lyapunov Policy in Queueing Control</title>
<link>https://arxiv.org/abs/2506.18278</link>
<guid>https://arxiv.org/abs/2506.18278</guid>
<content:encoded><![CDATA[
arXiv:2506.18278v2 Announce Type: replace-cross 
Abstract: We introduce an original minimax framework for finite-time performance analysis in queueing control and propose a surprisingly simple Lyapunov-based scheduling policy with superior finite-time performance. The framework quantitatively characterizes how the expected total queue length scales with key system parameters, including the capacity of the scheduling set and the variability of arrivals and departures across queues. To our knowledge, this provides the first firm foundation for evaluating and comparing scheduling policies in the finite-time regime, including nonstationary settings, and shows that the proposed policy can provably and empirically outperform classical MaxWeight in finite time. Within this framework, we establish three main sets of results. First, we derive minimax lower bounds on the expected total queue length for parallel-queue scheduling via a novel Brownian coupling argument. Second, we propose a new policy, LyapOpt, which minimizes the full quadratic Lyapunov drift-capturing both first- and second-order terms-and achieves optimal finite-time performance in heavy traffic while retaining classical stability guarantees. Third, we identify a key limitation of the classical MaxWeight policy, which optimizes only the first-order drift: its finite-time performance depends suboptimally on system parameters, leading to substantially larger backlogs in explicitly characterized settings. Together, these results delineate the scope and limitations of classical drift-based scheduling and motivate new queueing-control methods with rigorous finite-time guarantees.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoMind: Towards Community-Driven Agents for Machine Learning Engineering</title>
<link>https://arxiv.org/abs/2506.20640</link>
<guid>https://arxiv.org/abs/2506.20640</guid>
<content:encoded><![CDATA[
arXiv:2506.20640v2 Announce Type: replace-cross 
Abstract: Large language model (LLM) agents show promise in automating machine learning (ML) engineering. However, existing agents typically operate in isolation on a given research problem, without engaging with the broader research community, where human researchers often gain insights and contribute by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live evaluation framework designed to assess an agent's ability to communicate with and leverage collective knowledge from a simulated Kaggle research community. Building on this framework, we propose CoMind, an multi-agent system designed to actively integrate external knowledge. CoMind employs an iterative parallel exploration mechanism, developing multiple solutions simultaneously to balance exploratory breadth with implementation depth. On 75 past Kaggle competitions within our MLE-Live framework, CoMind achieves a 36% medal rate, establishing a new state of the art. Critically, when deployed in eight live, ongoing competitions, CoMind outperforms 92.6% of human competitors on average, placing in the top 5% on three official leaderboards and the top 1% on one.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking</title>
<link>https://arxiv.org/abs/2507.11137</link>
<guid>https://arxiv.org/abs/2507.11137</guid>
<content:encoded><![CDATA[
arXiv:2507.11137v2 Announce Type: replace-cross 
Abstract: As valuable digital assets, deep neural networks necessitate robust ownership protection, positioning neural network watermarking (NNW) as a promising solution. Among various NNW approaches, weight-based methods are favored for their simplicity and practicality; however, they remain vulnerable to forging and overwriting attacks. To address those challenges, we propose NeuralMark, a robust method built around a hashed watermark filter. Specifically, we utilize a hash function to generate an irreversible binary watermark from a secret key, which is then used as a filter to select the model parameters for embedding. This design cleverly intertwines the embedding parameters with the hashed watermark, providing a robust defense against both forging and overwriting attacks. An average pooling is also incorporated to resist fine-tuning and pruning attacks. Furthermore, it can be seamlessly integrated into various neural network architectures, ensuring broad applicability. Theoretically, we analyze its security boundary. Empirically, we verify its effectiveness and robustness across 13 distinct Convolutional and Transformer architectures, covering five image classification tasks and one text generation task. The source codes are available at https://github.com/AIResearch-Group/NeuralMark.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extreme value theory for singular subspace estimation in the matrix denoising model</title>
<link>https://arxiv.org/abs/2507.19978</link>
<guid>https://arxiv.org/abs/2507.19978</guid>
<content:encoded><![CDATA[
arXiv:2507.19978v2 Announce Type: replace-cross 
Abstract: This paper studies fine-grained singular subspace estimation in the matrix denoising model where a deterministic low-rank signal matrix is additively perturbed by a stochastic matrix of Gaussian noise. We establish that the maximum Euclidean row norm (i.e., the two-to-infinity norm) of the aligned difference between the leading sample and population singular vectors approaches the Gumbel distribution in the large-matrix limit, under suitable signal-to-noise conditions and after appropriate centering and scaling. We apply our novel asymptotic distributional theory to test hypotheses of low-rank signal structure encoded in the leading singular vectors and their corresponding principal subspace. We provide de-biased estimators for the corresponding nuisance signal singular values and show that our proposed plug-in test statistic has desirable properties. Notably, compared to using the Frobenius norm subspace distance, our test statistic based on the two-to-infinity norm empirically has higher power to detect structured alternatives that differ from the null in only a few matrix entries or rows. Our main results are obtained by a novel synthesis of and technical analysis involving row-wise matrix perturbation analysis, extreme value theory, saddle point approximation methods, and random matrix theory. Our contributions complement the existing literature for matrix denoising focused on minimaxity, mean squared error analysis, unitarily invariant distances between subspaces, component-wise asymptotic distributional theory, and row-wise uniform error bounds. Numerical simulations illustrate our main results and demonstrate the robustness properties of our testing procedure to non-Gaussian noise distributions.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis</title>
<link>https://arxiv.org/abs/2508.02322</link>
<guid>https://arxiv.org/abs/2508.02322</guid>
<content:encoded><![CDATA[
arXiv:2508.02322v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level pruning, merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under pruning ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories</title>
<link>https://arxiv.org/abs/2509.06998</link>
<guid>https://arxiv.org/abs/2509.06998</guid>
<content:encoded><![CDATA[
arXiv:2509.06998v2 Announce Type: replace-cross 
Abstract: Can models generalize attribute knowledge across semantically and perceptually dissimilar categories? While prior work has addressed attribute prediction within narrow taxonomic or visually similar domains, it remains unclear whether current models can abstract attributes and apply them to conceptually distant categories. This work presents the first explicit evaluation for the robustness of the attribute prediction task under such conditions, testing whether models can correctly infer shared attributes between unrelated object types: e.g., identifying that the attribute "has four legs" is common to both "dogs" and "chairs". To enable this evaluation, we introduce train-test split strategies that progressively reduce correlation between training and test sets, based on: LLM-driven semantic grouping, embedding similarity thresholding, embedding-based clustering, and supercategory-based partitioning using ground-truth labels. Results show a sharp drop in performance as the correlation between training and test categories decreases, indicating strong sensitivity to split design. Among the evaluated methods, clustering yields the most effective trade-off, reducing hidden correlations while preserving learnability. These findings offer new insights into the limitations of current representations and inform future benchmark construction for attribute reasoning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Common Pipeline for Harmonizing Electronic Health Record Data for Translational Research</title>
<link>https://arxiv.org/abs/2509.08553</link>
<guid>https://arxiv.org/abs/2509.08553</guid>
<content:encoded><![CDATA[
arXiv:2509.08553v2 Announce Type: replace-cross 
Abstract: Despite the growing availability of Electronic Health Record (EHR) data, researchers often face substantial barriers in effectively using these data for translational research due to their complexity, heterogeneity, and lack of standardized tools and documentation. To address this critical gap, we introduce PEHRT, a common pipeline for harmonizing EHR data for translational research. PEHRT is a comprehensive, ready-to-use resource that includes open-source code, visualization tools, and detailed documentation to streamline the process of preparing EHR data for analysis. The pipeline provides tools to harmonize structured and unstructured EHR data to standardized ontologies to ensure consistency across diverse coding systems. In the presence of unmapped or heterogeneous local codes, PEHRT further leverages representation learning and pre-trained language models to generate robust embeddings that capture semantic relationships across sites to mitigate heterogeneity and enable integrative downstream analyses. PEHRT also supports cross-institutional co-training through shared representations, allowing participating sites to collaboratively refine embeddings and enhance generalizability without sharing individual-level data. The framework is data model-agnostic and can be seamlessly deployed across diverse healthcare systems to produce interoperable, research-ready datasets. By lowering the technical barriers to EHR-based research, PEHRT empowers investigators to transform raw clinical data into reproducible, analysis-ready resources for discovery and innovation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndiSeek learns information-guided disentangled representations</title>
<link>https://arxiv.org/abs/2509.21584</link>
<guid>https://arxiv.org/abs/2509.21584</guid>
<content:encoded><![CDATA[
arXiv:2509.21584v3 Announce Type: replace-cross 
Abstract: Learning disentangled representations is a fundamental task in multi-modal learning. In modern applications such as single-cell multi-omics, both shared and modality-specific features are critical for characterizing cell states and supporting downstream analyses. Ideally, modality-specific features should be independent of shared ones while also capturing all complementary information within each modality. This tradeoff is naturally expressed through information-theoretic criteria, but mutual-information-based objectives are difficult to estimate reliably, and their variational surrogates often underperform in practice. In this paper, we introduce IndiSeek, a novel disentangled representation learning approach that addresses this challenge by combining an independence-enforcing objective with a computationally efficient reconstruction loss that bounds conditional mutual information. This formulation explicitly balances independence and completeness, enabling principled extraction of modality-specific features. We demonstrate the effectiveness of IndiSeek on synthetic simulations, a CITE-seq dataset and multiple real-world multi-modal benchmarks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction</title>
<link>https://arxiv.org/abs/2510.05613</link>
<guid>https://arxiv.org/abs/2510.05613</guid>
<content:encoded><![CDATA[
arXiv:2510.05613v2 Announce Type: replace-cross 
Abstract: Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models</title>
<link>https://arxiv.org/abs/2510.07858</link>
<guid>https://arxiv.org/abs/2510.07858</guid>
<content:encoded><![CDATA[
arXiv:2510.07858v2 Announce Type: replace-cross 
Abstract: Large language models (LLM) have emerged as a promising avenue for time series forecasting, offering the potential to integrate multimodal data. However, existing LLM-based approaches face notable limitations-such as marginalized role in model architectures, reliance on coarse statistical text prompts, and lack of interpretability. In this work, we introduce Augur, a fully LLM driven time series forecasting framework that exploits LLM causal reasoning to discover and use directed causal associations among covariates. Augur uses a two stage teacher student architecture where a powerful teacher LLM infers a directed causal graph from time series using heuristic search together with pairwise causality testing. A lightweight student agent then refines the graph and fine tune on high confidence causal associations that are encoded as rich textual prompts to perform forecasting. This design improves predictive accuracy while yielding transparent, traceable reasoning about variable interactions. Extensive experiments on real-world datasets with 26 baselines demonstrate that Augur achieves competitive performance and robust zero-shot generalization.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstructing the local density field with combined convolutional and point cloud architecture</title>
<link>https://arxiv.org/abs/2510.08573</link>
<guid>https://arxiv.org/abs/2510.08573</guid>
<content:encoded><![CDATA[
arXiv:2510.08573v2 Announce Type: replace-cross 
Abstract: We construct a neural network to perform regression on the local dark-matter density field given line-of-sight peculiar velocities of dark-matter halos, biased tracers of the dark matter field. Our architecture combines a convolutional U-Net with a point-cloud DeepSets. This combination enables efficient use of small-scale information and improves reconstruction quality relative to a U-Net-only approach. Specifically, our hybrid network recovers both clustering amplitudes and phases better than the U-Net on small scales.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decorrelation Speeds Up Vision Transformers</title>
<link>https://arxiv.org/abs/2510.14657</link>
<guid>https://arxiv.org/abs/2510.14657</guid>
<content:encoded><![CDATA[
arXiv:2510.14657v2 Announce Type: replace-cross 
Abstract: Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields strong performance in low-label data regimes but comes with substantial computational costs, making it impractical in time- and resource-constrained industrial settings. We address this by nitegrating Decorrelated Backpropagation (DBP) into MAE pre-training, an optimization method that iteratively reduces input correlations at each layer to accelerate convergence. Applied selectively to the encoder, DBP achieves faster pre-training without loss of stability. To mimic constrained-data scenarios, we evaluate our approach on ImageNet-1K pre-training and ADE20K fine-tuning using randomly sampled subsets of each dataset. Under this setting, DBP-MAE reduces wall-clock time to baseline performance by 21.1%, lowers carbon emissions by 21.4%, and improves segmentation mIoU by 1.1 points. We observe similar gains when pre-training and fine-tuning on proprietary industrial data, confirming the method's applicability in real-world scenarios. These results demonstrate that DBP can reduce training time and energy use while improving downstream performance for large-scale ViT pre-training.
  Keywords: Deep learning, Vision transformers, Efficient AI, Decorrelation
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Robustness for Free? Revisiting Training via a Benchmark</title>
<link>https://arxiv.org/abs/2511.01724</link>
<guid>https://arxiv.org/abs/2511.01724</guid>
<content:encoded><![CDATA[
arXiv:2511.01724v2 Announce Type: replace-cross 
Abstract: Deep learning models are notoriously vulnerable to imperceptible perturbations. Most existing research centers on adversarial robustness (AR), which evaluates models under worst-case scenarios by examining the existence of deterministic adversarial examples (AEs). In contrast, probabilistic robustness (PR) adopts a statistical perspective, measuring the probability that predictions remain correct under stochastic perturbations. While PR is widely regarded as a practical complement to AR, dedicated training methods for improving PR are still relatively underexplored, albeit with emerging progress. Among the few PR-targeted training methods, we identify three limitations: i non-comparable evaluation protocols; ii limited comparisons to strong AT baselines despite anecdotal PR gains from AT; and iii no unified framework to compare the generalization of these methods. Thus, we introduce PRBench, the first benchmark dedicated to evaluating improvements in PR achieved by different robustness training methods. PRBench empirically compares most common AT and PR-targeted training methods using a comprehensive set of metrics, including clean accuracy, PR and AR performance, training efficiency, and generalization error (GE). We also provide theoretical analysis on the GE of PR performance across different training methods. Main findings revealed by PRBench include: AT methods are more versatile than PR-targeted training methods in terms of improving both AR and PR performance across diverse hyperparameter settings, while PR-targeted training methods consistently yield lower GE and higher clean accuracy. A leaderboard comprising 222 trained models across 7 datasets and 10 model architectures is publicly available at https://tmpspace.github.io/PRBenchLeaderboard/.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering</title>
<link>https://arxiv.org/abs/2511.05876</link>
<guid>https://arxiv.org/abs/2511.05876</guid>
<content:encoded><![CDATA[
arXiv:2511.05876v2 Announce Type: replace-cross 
Abstract: In recent years, the advancement of Graph Neural Networks (GNNs) has significantly propelled progress in Multi-View Clustering (MVC). However, existing methods face the problem of coarse-grained graph fusion. Specifically, current approaches typically generate a separate graph structure for each view and then perform weighted fusion of graph structures at the view level, which is a relatively rough strategy. To address this limitation, we present a novel Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly consists of two modules. In particular, we propose an innovative Mixture of Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a Mixture-of-Experts network to implement fine-grained fusion of ego graphs at the sample level, rather than the conventional view-level fusion. Additionally, we present the Ego Graph Contrastive Learning (EGCL) module to align the fused representation with the view-specific representation. The EGCL module enhances the representation similarity of samples from the same cluster, not merely from the same sample, further boosting fine-grained graph representation. Extensive experiments demonstrate that MoEGCL achieves state-of-the-art results in deep multi-view clustering tasks. The source code is publicly available at https://github.com/HackerHyper/MoEGCL.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal control of the future via prospective learning with control</title>
<link>https://arxiv.org/abs/2511.08717</link>
<guid>https://arxiv.org/abs/2511.08717</guid>
<content:encoded><![CDATA[
arXiv:2511.08717v3 Announce Type: replace-cross 
Abstract: Optimal control of the future is the next frontier for AI. Current approaches to this problem are typically rooted in reinforcement learning (RL). RL is mathematically distinct from supervised learning, which has been the main workhorse for the recent achievements in AI. Moreover, RL typically operates in a stationary environment with episodic resets, limiting its utility. Here, we extend supervised learning to address learning to \textit{control} in non-stationary, reset-free environments. Using this framework, called ''Prospective Learning with Control'' (PL+C), we prove that under certain fairly general assumptions, empirical risk minimization (ERM) asymptotically achieves the Bayes optimal policy. We then consider a specific instance of prospective learning with control, foraging -- which is a canonical task for any mobile agent -- be it natural or artificial. We illustrate that modern RL algorithms fail to learn in these non-stationary reset-free environments, and even with modifications, they are orders of magnitude less efficient than our prospective foraging agents.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximation rates of quantum neural networks for periodic functions via Jackson's inequality</title>
<link>https://arxiv.org/abs/2511.16149</link>
<guid>https://arxiv.org/abs/2511.16149</guid>
<content:encoded><![CDATA[
arXiv:2511.16149v2 Announce Type: replace-cross 
Abstract: Quantum neural networks (QNNs) are an analog of classical neural networks in the world of quantum computing, which are represented by a unitary matrix with trainable parameters. Inspired by the universal approximation property of classical neural networks, ensuring that every continuous function can be arbitrarily well approximated uniformly on a compact set of a Euclidean space, some recent works have established analogous results for QNNs, ranging from single-qubit to multi-qubit QNNs, and even hybrid classical-quantum models. In this paper, we study the approximation capabilities of QNNs for periodic functions with respect to the supremum norm. We use the Jackson inequality to approximate a given function by implementing its approximating trigonometric polynomial via a suitable QNN. In particular, we see that by restricting to the class of periodic functions, one can achieve a quadratic reduction of the number of parameters, producing better approximation results than in the literature. Moreover, the smoother the function, the fewer parameters are needed to construct a QNN to approximate the function.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPU-Initiated Networking for NCCL</title>
<link>https://arxiv.org/abs/2511.15076</link>
<guid>https://arxiv.org/abs/2511.15076</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, GPU communication, device-initiated communication, NCCL, GPU-Initiated Networking (GIN)  

<br /><br />Summary:  
This paper addresses the challenges in modern AI workloads, specifically Mixture-of-Experts (MoE) architectures, which require low-latency and fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication typically relies on a host-initiated model orchestrated by the CPU, limiting performance for applications demanding tight computation and communication integration. NCCL 2.28 introduces a Device API offering three operation modes, including the GPU-Initiated Networking (GIN) mode designed for network RDMA. The paper details the GIN architecture comprising three layers: NCCL Core host-side APIs for communicator setup; device-side APIs enabling remote memory operations callable from CUDA kernels; and a network plugin framework with two semantics—GPUDirect Async Kernel-Initiated for direct GPU-to-NIC communication via DOCA GPUNetIO, and a Proxy backend using lock-free GPU-to-CPU queues over RDMA networks. The practical utility of GIN is demonstrated through integration with DeepEP, an MoE communication library. Benchmarking results show that GIN facilitates device-initiated communication within NCCL's unified runtime environment, achieving low-latency operations combined with NCCL's collective algorithm efficiencies and production-level infrastructure support. <div>
arXiv:2511.15076v2 Announce Type: replace-cross 
Abstract: Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.
  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden markov model to predict tourists visited place</title>
<link>https://arxiv.org/abs/2511.19465</link>
<guid>https://arxiv.org/abs/2511.19465</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, tourist behavior, machine learning, grammatical inference, hidden Markov model<br /><br />Summary:<br /><br />1. The paper focuses on analyzing tourist behavior through social networks by leveraging the digital footprints tourists leave, such as comments and photos during their trips.<br /><br />2. The vast amount of data generated by tourists sharing their experiences enables the modeling of their journeys and behavior patterns.<br /><br />3. Predicting tourists' next movements is highlighted as essential for improving tourism marketing strategies and supporting decision-making based on demand understanding.<br /><br />4. The authors propose a novel method using a machine learning grammatical inference algorithm adapted specifically for big data contexts to learn and predict tourists' movements.<br /><br />5. Their method results in a hidden Markov model that represents tourist group movements, which is both flexible and can be updated continuously with new data.<br /><br />6. The methodology is validated using Paris, the capital city of France, demonstrating its practical efficiency in modeling and predicting tourist behavior.<br /><br />Overall, this research contributes an adaptable and scalable machine learning approach to enhance the understanding and forecasting of tourist mobility patterns using social network data. <div>
arXiv:2511.19465v1 Announce Type: new 
Abstract: Nowadays, social networks are becoming a popular way of analyzing tourist behavior, thanks to the digital traces left by travelers during their stays on these networks. The massive amount of data generated; by the propensity of tourists to share comments and photos during their trip; makes it possible to model their journeys and analyze their behavior. Predicting the next movement of tourists plays a key role in tourism marketing to understand demand and improve decision support. In this paper, we propose a method to understand and to learn tourists' movements based on social network data analysis to predict future movements. The method relies on a machine learning grammatical inference algorithm. A major contribution in this paper is to adapt the grammatical inference algorithm to the context of big data. Our method produces a hidden Markov model representing the movements of a group of tourists. The hidden Markov model is flexible and editable with new data. The capital city of France, Paris is selected to demonstrate the efficiency of the proposed methodology.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Modality Contributions via Disentangling Multimodal Representations</title>
<link>https://arxiv.org/abs/2511.19470</link>
<guid>https://arxiv.org/abs/2511.19470</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal models, Partial Information Decomposition, modality contributions, cross-attention, Iterative Proportional Fitting Procedure  

<br /><br />Summary:  
This paper addresses the challenge of quantifying the contributions of different modalities in multimodal models, where existing methods often conflate the concept of contribution itself. Prior approaches primarily depend on accuracy-based metrics, interpreting performance degradation upon removal of a modality as its influence. However, these metrics cannot differentiate whether a modality is inherently informative or if its value emerges solely through interactions with other modalities, which is critical in architectures using cross-attention that induce interdependent modality representations. To overcome this, the authors propose a new framework rooted in Partial Information Decomposition (PID), which decomposes the predictive information in internal embeddings into unique, redundant, and synergistic components, providing a nuanced understanding of modality contributions. They further introduce an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) to perform scalable, inference-only analysis that computes layer-wise and dataset-level contributions without retraining models. This enables a principled and interpretable representation-level examination of multimodal behaviors, offering clearer insights than traditional outcome-based metrics and enhancing understanding of how different modalities interact within complex models. <div>
arXiv:2511.19470v1 Announce Type: new 
Abstract: Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer</title>
<link>https://arxiv.org/abs/2511.19472</link>
<guid>https://arxiv.org/abs/2511.19472</guid>
<content:encoded><![CDATA[
<div> Keywords: Prefix Adders, Transformer, GPT, Hardware Design, Area-Delay Product  

<br /><br />Summary:  
This paper introduces PrefixGPT, a generative pre-trained Transformer model designed to optimize prefix adders, which are crucial components in high-speed compute-intensive applications. Prefix adders pose design challenges due to strict rules and an exponentially large design space. PrefixGPT generates valid prefix adder topologies from scratch by representing them as two-dimensional coordinate sequences and enforcing legality masks during generation to ensure validity by construction. The model employs a customized decoder-only Transformer architecture and undergoes two training stages: pre-training on a dataset of randomly synthesized valid prefix adders to grasp fundamental design principles, followed by fine-tuning focused on optimizing design quality within the large design space. Experimental results show that PrefixGPT discovers new optimal prefix adder designs that improve the area-delay product (ADP) by 7.7% compared to state-of-the-art methods. Moreover, it significantly enhances exploration quality, reducing average ADP by up to 79.1%, which highlights the model’s superior ability to navigate and optimize complex hardware design spaces. This work demonstrates the promising potential of GPT-style models to learn intricate hardware design rules and apply that knowledge to achieve more efficient and optimized hardware architectures. <div>
arXiv:2511.19472v1 Announce Type: new 
Abstract: Prefix adders are widely used in compute-intensive applications for their high speed. However, designing optimized prefix adders is challenging due to strict design rules and an exponentially large design space. We introduce PrefixGPT, a generative pre-trained Transformer (GPT) that directly generates optimized prefix adders from scratch. Our approach represents an adder's topology as a two-dimensional coordinate sequence and applies a legality mask during generation, ensuring every design is valid by construction. PrefixGPT features a customized decoder-only Transformer architecture. The model is first pre-trained on a corpus of randomly synthesized valid prefix adders to learn design rules and then fine-tuned to navigate the design space for optimized design quality. Compared with existing works, PrefixGPT not only finds a new optimal design with a 7.7% improved area-delay product (ADP) but exhibits superior exploration quality, lowering the average ADP by up to 79.1%. This demonstrates the potential of GPT-style models to first master complex hardware design principles and then apply them for more efficient design optimization.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WavefrontDiffusion: Dynamic Decoding Schedule or Improved Reasoning</title>
<link>https://arxiv.org/abs/2511.19473</link>
<guid>https://arxiv.org/abs/2511.19473</guid>
<content:encoded><![CDATA[
<div> Diffusion Language Models, Denoising Strategies, WavefrontDiffusion, Text Generation, Semantic Coherence<br /><br />Summary:<br /><br />Diffusion Language Models (DLMs) offer a promising alternative to traditional autoregressive text generation models, largely dependent on their denoising strategies to achieve high-quality outputs. The paper identifies two primary denoising methods: Standard Diffusion and BlockDiffusion. Standard Diffusion applies global denoising without update restrictions, which can prematurely end sequences due to incomplete context resolution. BlockDiffusion, on the other hand, updates fixed-size token blocks sequentially but can disrupt the semantic cohesion and reasoning flow because of its rigid structure. To address these issues, the authors propose WavefrontDiffusion, a novel dynamic decoding technique that advances a wavefront of active tokens outward from finalized tokens. This approach respects the natural semantic structure of text, adapting the update process in a way that preserves coherence. Importantly, WavefrontDiffusion maintains computational costs comparable to block-based methods. Evaluations on four benchmarks related to reasoning and code generation demonstrate that WavefrontDiffusion outperforms existing methods, delivering higher semantic fidelity and more coherent outputs. This work illustrates the significance of adaptive scheduling in diffusion-based language models to enhance both the efficiency and quality of generated text. <div>
arXiv:2511.19473v1 Announce Type: new 
Abstract: Diffusion Language Models (DLMs) have shown strong potential for text generation and are becoming a competitive alternative to autoregressive models. The denoising strategy plays an important role in determining the quality of their outputs. Mainstream denoising strategies include Standard Diffusion and BlockDiffusion. Standard Diffusion performs global denoising without restricting the update range, often finalizing incomplete context and causing premature end-of-sequence predictions. BlockDiffusion updates fixed-size blocks in a preset order, but its rigid structure can break apart coherent semantic units and disrupt reasoning. We present WavefrontDiffusion, a dynamic decoding approach that expands a wavefront of active tokens outward from finalized positions. This adaptive process follows the natural flow of semantic structure while keeping computational cost equal to block-based methods. Across four benchmarks in reasoning and code generation, WavefrontDiffusion achieves state-of-the-art performance while producing outputs with higher semantic fidelity, showing the value of adaptive scheduling for more coherent and efficient generation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting the Experts: Unauthorized Compression in MoE-LLMs</title>
<link>https://arxiv.org/abs/2511.19480</link>
<guid>https://arxiv.org/abs/2511.19480</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, Large Language Models, Model Pruning, Expert Attribution, Model Security  

<br /><br />Summary:  
This paper analyzes the vulnerability of Mixture-of-Experts (MoE) architectures in large language models (LLMs) with respect to unauthorized compression and fine-tuning. First, the authors introduce an expert attribution framework that identifies which experts within the MoE are most responsible for specific tasks, facilitating targeted analysis. Second, they investigate the performance impact of pruning these identified experts and the importance of re-aligning the remaining experts through active learning-based fine-tuning to recover task accuracy. Third, the study reveals a fundamental trade-off between knowledge loss caused by pruning and the extent of recovery achievable through fine-tuning, emphasizing that pruning alone can significantly degrade task performance. Fourth, based on these findings, they propose defense strategies such as entangled expert training and selective fine-tuning protocols designed to thwart unauthorized compression and adaptation attempts. Finally, the paper frames expert pruning in MoE-LLMs as both a potential attack vector and a defensive opportunity, offering the first systematic framework for secure specialization of MoE-based models against misuse. This dual-use perspective highlights the necessity of designing MoE architectures that balance modular flexibility with robust protection against exploitation. <div>
arXiv:2511.19480v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quality analysis and evaluation prediction of RAG retrieval based on machine learning algorithms</title>
<link>https://arxiv.org/abs/2511.19481</link>
<guid>https://arxiv.org/abs/2511.19481</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval Enhanced Generation, XGBoost, Particle Swarm Optimization, Document Relevance, VMD PSO BiLSTM<br /><br />Summary:  
1. This paper addresses limitations in retrieval enhanced generation (RAG) systems where poor retrieval quality adversely affects the accuracy of generated content.  
2. To overcome these challenges, the authors propose an XGBoost machine learning regression model incorporating feature engineering and particle swarm optimization (PSO) to improve processing of tabular features.  
3. Correlation analysis reveals a strong positive connection between document relevance and answer quality (correlation coefficient 0.66), implying that enhancing retrieval relevance can significantly boost the quality of generated answers.  
4. The study identifies a trade-off among semantic similarity, redundancy, and diversity, with strong negative correlations (-0.89 and -0.88), indicating that increasing semantic similarity and redundancy reduces diversity in results.  
5. Experimental comparisons show that the VMD PSO BiLSTM model outperforms other models such as decision trees and AdaBoost across all metrics, demonstrating lower errors (MSE, RMSE, MAE, MAPE) and higher R² scores, indicating better prediction accuracy, stability, and interpretability.  
Overall, the proposed approach offers an effective path to optimize retrieval quality, thereby enhancing the generation performance of RAG systems, with significant implications for advancing retrieval-based language model applications. <div>
arXiv:2511.19481v1 Announce Type: new 
Abstract: With the rapid evolution of large language models, retrieval enhanced generation technology has been widely used due to its ability to integrate external knowledge to improve output accuracy. However, the performance of the system is highly dependent on the quality of the retrieval module. If the retrieval results have low relevance to user needs or contain noisy information, it will directly lead to distortion of the generated content. In response to the performance bottleneck of existing models in processing tabular features, this paper proposes an XGBoost machine learning regression model based on feature engineering and particle swarm optimization. Correlation analysis shows that answer_quality is positively correlated with doc_delevance by 0.66, indicating that document relevance has a significant positive effect on answer quality, and improving document relevance may enhance answer quality; The strong negative correlations between semantic similarity, redundancy, and diversity were -0.89 and -0.88, respectively, indicating a trade- off between semantic similarity, redundancy, and diversity. In other words, as the former two increased, diversity significantly decreased. The experimental results comparing decision trees, AdaBoost, etc. show that the VMD PSO BiLSTM model is superior in all evaluation indicators, with significantly lower MSE, RMSE, MAE, and MAPE compared to the comparison model. The R2 value is higher, indicating that its prediction accuracy, stability, and data interpretation ability are more outstanding. This achievement provides an effective path for optimizing the retrieval quality and improving the generation effect of RAG system, and has important value in promoting the implementation and application of related technologies.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniTFT: Omni Target Forecasting for Vital Signs and Laboratory Result Trajectories in Multi Center ICU Data</title>
<link>https://arxiv.org/abs/2511.19485</link>
<guid>https://arxiv.org/abs/2511.19485</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate time-series, vital signs, laboratory results, Temporal Fusion Transformer, ICU forecasting  

<br /><br />Summary:  
This paper introduces OmniTFT, a deep learning framework designed for accurate multivariate time-series prediction of vital signs and laboratory results in intensive care units (ICUs). The challenge addressed involves handling noisy, rapidly fluctuating vital signs and sparse, delayed, or biased laboratory test data. OmniTFT builds upon the Temporal Fusion Transformer (TFT) architecture and incorporates four novel strategies to enhance forecasting performance: sliding window equalized sampling to balance different physiological states, frequency-aware embedding shrinkage to stabilize representations of rare classes, hierarchical variable selection to focus model attention on informative feature clusters, and influence-aligned attention calibration to maintain robustness amid sudden physiological changes. By minimizing dependence on target-specific models and excessive feature engineering, OmniTFT supports unified prediction of diverse clinical variables while fostering generalizability across institutions. The model was evaluated on multiple large ICU datasets (MIMIC-III, MIMIC-IV, and eICU) and demonstrated substantial improvements in forecasting both vital signs and laboratory measurements. Additionally, the attention mechanisms in OmniTFT offer interpretable insights consistent with medical knowledge, highlighting its potential application for quantitative, data-driven clinical decision support in critical care settings. <div>
arXiv:2511.19485v1 Announce Type: new 
Abstract: Accurate multivariate time-series prediction of vital signs and laboratory results is crucial for early intervention and precision medicine in intensive care units (ICUs). However, vital signs are often noisy and exhibit rapid fluctuations, while laboratory tests suffer from missing values, measurement lags, and device-specific bias, making integrative forecasting highly challenging. To address these issues, we propose OmniTFT, a deep learning framework that jointly learns and forecasts high-frequency vital signs and sparsely sampled laboratory results based on the Temporal Fusion Transformer (TFT). Specifically, OmniTFT implements four novel strategies to enhance performance: sliding window equalized sampling to balance physiological states, frequency-aware embedding shrinkage to stabilize rare-class representations, hierarchical variable selection to guide model attention toward informative feature clusters, and influence-aligned attention calibration to enhance robustness during abrupt physiological changes. By reducing the reliance on target-specific architectures and extensive feature engineering, OmniTFT enables unified modeling of multiple heterogeneous clinical targets while preserving cross-institutional generalizability. Across forecasting tasks, OmniTFT achieves substantial performance improvement for both vital signs and laboratory results on the MIMIC-III, MIMIC-IV, and eICU datasets. Its attention patterns are interpretable and consistent with known pathophysiology, underscoring its potential utility for quantitative decision support in clinical care.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning then Rectification</title>
<link>https://arxiv.org/abs/2511.19486</link>
<guid>https://arxiv.org/abs/2511.19486</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, fine-tuning, rectification, prediction error variance, empirical scaling laws<br /><br />Summary: This paper addresses enhancing the performance of large language models (LLMs) in social science and market research contexts by combining two main improvement techniques: fine-tuning and rectification. Fine-tuning adjusts LLM predictions to align better with human responses, while rectification corrects biases in those outputs. Unlike traditional methods focused on minimizing mean squared error, the authors propose minimizing the variance of prediction errors during fine-tuning, optimizing the process for the subsequent rectification stage. The study introduces a data-driven framework that strategically allocates a limited number of labeled samples between fine-tuning and rectification using empirical scaling laws. This optimal sample split aims to improve downstream inference and estimation accuracy. Empirical validation shows that this combined and optimally balanced approach outperforms using fine-tuning or rectification independently. The findings suggest that minimizing variance in fine-tuning can lead to better rectified outputs and demonstrate practical advances in leveraging LLMs for improved social science data analysis. This framework provides a new methodological direction for improving LLM-based prediction tasks with limited labeled data. <div>
arXiv:2511.19486v1 Announce Type: new 
Abstract: Driven by recent advances in artificial intelligence (AI), a growing body of work demonstrates the potential of using large language models (LLMs) to generate human-like responses in market research and social science applications. Two primary approaches can be applied to improve the performance of LLMs: fine-tuning, which aligns LLM predictions more closely with human responses, and rectification, which corrects biases in LLM outputs. In this paper, we develop a framework that combines fine-tuning and rectification, and optimally allocates limited labeled samples across the two stages. Unlike the conventional objective that minimizes the mean squared prediction errors, we propose to minimize the variance of the prediction errors as the fine-tuning objective, which is optimal for the downstream rectification stage. Building on this insight, we leverage empirical scaling laws to develop a data-driven method for optimally splitting samples between the fine-tuning and rectification stages. Empirical analysis validates our framework, demonstrating improved estimation and inference performance compared to using either fine-tuning or rectification alone.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Generalized Proximity Forest</title>
<link>https://arxiv.org/abs/2511.19487</link>
<guid>https://arxiv.org/abs/2511.19487</guid>
<content:encoded><![CDATA[
<div> Random Forest, Proximity Forest, distance-based learning, regression, meta-learning<br /><br />Summary:<br /><br />This article focuses on enhancing the utility of Random Forest (RF) proximities, which are useful in various supervised machine learning tasks such as outlier detection, missing data imputation, and visualization. Recognizing that the RF model is not always optimal, the authors build on previous extensions of RF proximities to time series through the Proximity Forest (PF) model. They introduce a generalized PF model that extends RF proximities beyond time series to any supervised distance-based machine learning domain. Furthermore, a new variant of the PF model is proposed specifically for regression tasks, broadening the range of applications. The work also presents the idea of employing the generalized PF model as a meta-learning framework, which enables the extension of supervised imputation capabilities to any pre-trained classifier. Experimental results demonstrate that the generalized PF model offers unique advantages over both the traditional RF model and the k-nearest neighbors model, indicating improved performance and flexibility. This approach effectively combines distance-based learning with the strengths of RF proximities, offering a powerful tool for diverse supervised learning contexts. <div>
arXiv:2511.19487v1 Announce Type: new 
Abstract: Recent work has demonstrated the utility of Random Forest (RF) proximities for various supervised machine learning tasks, including outlier detection, missing data imputation, and visualization. However, the utility of the RF proximities depends upon the success of the RF model, which itself is not the ideal model in all contexts. RF proximities have recently been extended to time series by means of the distance-based Proximity Forest (PF) model, among others, affording time series analysis with the benefits of RF proximities. In this work, we introduce the generalized PF model, thereby extending RF proximities to all contexts in which supervised distance-based machine learning can occur. Additionally, we introduce a variant of the PF model for regression tasks. We also introduce the notion of using the generalized PF model as a meta-learning framework, extending supervised imputation capability to any pre-trained classifier. We experimentally demonstrate the unique advantages of the generalized PF model compared with both the RF model and the $k$-nearest neighbors model.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Model-Aided Continual Learning for CSI Feedback in FDD mMIMO-OFDM Systems</title>
<link>https://arxiv.org/abs/2511.19490</link>
<guid>https://arxiv.org/abs/2511.19490</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep autoencoder, CSI feedback, continual learning, generative adversarial network, mMIMO OFDM<br /><br />Summary:<br /><br />This article addresses the challenge of efficient channel state information (CSI) feedback in massive multiple-input multiple-output (mMIMO) orthogonal frequency division multiplexing (OFDM) systems using deep autoencoder (DAE) frameworks. While DAEs reduce CSI feedback overhead effectively, they struggle to adapt in dynamic environments caused by user mobility, necessitating retraining when new CSI distributions appear, and they suffer from catastrophic forgetting when revisiting previous environments. To overcome these issues, the authors propose a novel generative adversarial network (GAN)-based learning approach for CSI feedback. The GAN generator functions as a memory unit that retains knowledge from past environments, allowing the model to continually learn new information without losing performance on older tasks. This approach enhances the generalization capability of the DAE framework across diverse scenarios and maintains low memory overhead. Additionally, the method can be easily integrated with other advanced CSI feedback models, underlining its robustness and adaptability for practical deployment in dynamic wireless communication systems. Simulation results validate the effectiveness of the proposed method in maintaining consistent, high-quality CSI feedback across changing environments. <div>
arXiv:2511.19490v1 Announce Type: new 
Abstract: Deep autoencoder (DAE) frameworks have demonstrated their effectiveness in reducing channel state information (CSI) feedback overhead in massive multiple-input multiple-output (mMIMO) orthogonal frequency division multiplexing (OFDM) systems. However, existing CSI feedback models struggle to adapt to dynamic environments caused by user mobility, requiring retraining when encountering new CSI distributions. Moreover, returning to previously encountered environments often leads to performance degradation due to catastrophic forgetting. Continual learning involves enabling models to incorporate new information while maintaining performance on previously learned tasks. To address these challenges, we propose a generative adversarial network (GAN)-based learning approach for CSI feedback. By using a GAN generator as a memory unit, our method preserves knowledge from past environments and ensures consistently high performance across diverse scenarios without forgetting. Simulation results show that the proposed approach enhances the generalization capability of the DAE framework while maintaining low memory overhead. Furthermore, it can be seamlessly integrated with other advanced CSI feedback models, highlighting its robustness and adaptability.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenCML: End-to-End Framework of Open-world Machine Learning to Learn Unknown Classes Incrementally</title>
<link>https://arxiv.org/abs/2511.19491</link>
<guid>https://arxiv.org/abs/2511.19491</guid>
<content:encoded><![CDATA[
<div> Open-world learning, continual learning, class discovery, incremental learning, artificial intelligence  

<br /><br />Summary:  
Open-world machine learning addresses the limitations of traditional machine learning models that rely on closed-world assumptions, which restrict their ability to retain and build upon previously learned knowledge when encountering new tasks. The proposed model introduces two interconnected tasks to operate in an open and continuous learning environment. First, it identifies and creates novel classes from unknown data, allowing the system to expand its knowledge base. Second, the model incrementally learns to classify new classes as they are discovered, enabling continual improvement over time. This dual approach facilitates ongoing learning and adaptation to new information, reflecting more realistic AI deployment scenarios. The model not only supports open-world learning by discovering unknown classes but also excels in continuous class incremental learning. Experimental results demonstrate that the model outperforms existing open-world learning approaches, achieving a highest average accuracy of 82.54% across four learning iterations. Furthermore, it maintains robust performance with a minimum accuracy of 65.87%, indicating strong generalization and stability in dynamic learning environments. This advancement showcases the model's potential for practical AI applications requiring ongoing learning and flexibility. <div>
arXiv:2511.19491v1 Announce Type: new 
Abstract: Open-world machine learning is an emerging technique in artificial intelligence, where conventional machine learning models often follow closed-world assumptions, which can hinder their ability to retain previously learned knowledge for future tasks. However, automated intelligence systems must learn about novel classes and previously known tasks. The proposed model offers novel learning classes in an open and continuous learning environment. It consists of two different but connected tasks. First, it discovers unknown classes in the data and creates novel classes; next, it learns how to perform class incrementally for each new class. Together, they enable continual learning, allowing the system to expand its understanding of the data and improve over time. The proposed model also outperformed existing approaches in open-world learning. Furthermore, it demonstrated strong performance in continuous learning, achieving a highest average accuracy of 82.54% over four iterations and a minimum accuracy of 65.87%.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression</title>
<link>https://arxiv.org/abs/2511.19493</link>
<guid>https://arxiv.org/abs/2511.19493</guid>
<content:encoded><![CDATA[
<div> Random Forest, proximity matrix, QLORA compression, GPU acceleration, classification<br /><br />Summary:  
RFX (Random Forests X) v1.0 is a production-ready Python implementation of Breiman and Cutler's Random Forest classification, featuring comprehensive functionality such as out-of-bag error estimation, overall and local importance measures, proximity matrices with QLORA compression, case-wise analysis, and interactive visualization with CPU and GPU acceleration. It introduces four key solutions to overcome the proximity matrix memory bottleneck, which historically limited Random Forest analysis to about 60,000 samples: (1) QLORA compression drastically reduces GPU proximity matrix memory from 80GB to 6.4MB for 100k samples while preserving 99% of geometric structure, (2) CPU TriBlock proximity method reduces memory by 2.7x with lossless quality via upper-triangle storage and block-sparse thresholding, (3) SM-aware GPU batch sizing optimizes GPU utilization up to 95%, and (4) GPU-accelerated 3D MDS visualization computes embeddings efficiently using power iteration on low-rank factors. Validation across GPU and CPU modes confirms correct implementation and performance, with GPU providing a 1.4x speedup for importance computations with over 500 trees. Proximity calculations scale efficiently from 1,000 up to over 200,000 samples using GPU QLORA, complemented by CPU TriBlock methods for medium-sized datasets (10K-50K samples). Thus, RFX v1.0 removes the proximity memory bottleneck, enabling large-scale proximity-based Random Forest analyses well beyond previous limits while maintaining fidelity and offering open-source production-ready classification. <div>
arXiv:2511.19493v1 Announce Type: new 
Abstract: RFX (Random Forests X), where X stands for compression or quantization, presents a production-ready implementation of Breiman and Cutler's Random Forest classification methodology in Python. RFX v1.0 provides complete classification: out-of-bag error estimation, overall and local importance measures, proximity matrices with QLORA compression, case-wise analysis, and interactive visualization (rfviz)--all with CPU and GPU acceleration. Regression, unsupervised learning, CLIQUE importance, and RF-GAP proximity are planned for v2.0.
  This work introduces four solutions addressing the proximity matrix memory bottleneck limiting Random Forest analysis to ~60,000 samples: (1) QLORA (Quantized Low-Rank Adaptation) compression for GPU proximity matrices, reducing memory from 80GB to 6.4MB for 100k samples (12,500x compression with INT8 quantization) while maintaining 99% geometric structure preservation, (2) CPU TriBlock proximity--combining upper-triangle storage with block-sparse thresholding--achieving 2.7x memory reduction with lossless quality, (3) SM-aware GPU batch sizing achieving 95% GPU utilization, and (4) GPU-accelerated 3D MDS visualization computing embeddings directly from low-rank factors using power iteration.
  Validation across four implementation modes (GPU/CPU x case-wise/non-case-wise) demonstrates correct implementation. GPU achieves 1.4x speedup over CPU for overall importance with 500+ trees. Proximity computation scales from 1,000 to 200,000+ samples (requiring GPU QLORA), with CPU TriBlock filling the gap for medium-scale datasets (10K-50K samples). RFX v1.0 eliminates the proximity memory bottleneck, enabling proximity-based Random Forest analysis on datasets orders of magnitude larger than previously feasible. Open-source production-ready classification following Breiman and Cutler's original methodology.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Study of Compression Ordering for Large Language Models</title>
<link>https://arxiv.org/abs/2511.19495</link>
<guid>https://arxiv.org/abs/2511.19495</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, model compression, knowledge distillation, pruning, quantization  

<br /><br />Summary:  
This paper investigates the combined effects and optimal ordering of three major model compression techniques—knowledge distillation, structured pruning, and low-bit quantization—when applied to the Qwen2.5 3B large language model. The authors evaluate multiple compression pipelines using metrics such as perplexity, G-Eval, clarity, prompt alignment, and compression ratio. Their results confirm that quantization offers the highest standalone compression ratio, while pruning results in moderate quality loss. Crucially, the order in which these techniques are applied significantly impacts overall model quality. The pipeline ordering pruning first, followed by knowledge distillation, and finally quantization (P-KD-Q) is shown to achieve the best balance between compression ratio (3.68x) and model performance, preserving strong instruction-following and language understanding capabilities. Applying quantization too early causes irreversible information loss, which damages later training procedures. This work provides valuable, practical guidance for designing effective, ordering-aware compression strategies, enabling the deployment of LLMs in resource-constrained environments while maintaining acceptable quality. <div>
arXiv:2511.19495v1 Announce Type: new 
Abstract: Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM</title>
<link>https://arxiv.org/abs/2511.19496</link>
<guid>https://arxiv.org/abs/2511.19496</guid>
<content:encoded><![CDATA[
<div> Keywords: small language model, maximal-update parameterization, Muon optimizer, curriculum training, FP8-mixed-precision  

<br /><br />Summary:  
The paper introduces Xmodel-2.5, a 1.3-billion-parameter small language model optimized for edge and cost-sensitive scenarios while maintaining strong reasoning and tool-use capabilities. The model leverages maximal-update parameterization (μP), which enables hyper-parameters calibrated on a smaller 20M-parameter proxy model to directly transfer to the full-scale 1.3B model, even under a parameter-tied tie-word-embedding architecture. The training follows a three-phase curriculum named Warmup--Stable--Decay with a large-scale 1.4 trillion token dataset. Additionally, the authors demonstrate that switching optimizers from AdamW to Muon during the decay phase improves average performance on a 13-task reasoning benchmark by 4.58% without changing other hyper-parameters, effectively combining the stability benefits of AdamW in early training with the sharpening advantages of Muon later on. The training also employs FP8 mixed precision to achieve an optimal balance between computational throughput and model accuracy. The research is accompanied by the release of all model checkpoints, training recipes, and evaluation code publicly under the Apache-2.0 license, providing resources for reproducibility and further exploration. Links to the model and code repository are provided for community use. <div>
arXiv:2511.19496v1 Announce Type: new 
Abstract: Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \emph{drop-in agent core}. Training with maximal-update parameterization ($\mu$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\,\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\footnote{https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints).} Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PeriodNet: Boosting the Potential of Attention Mechanism for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.19497</link>
<guid>https://arxiv.org/abs/2511.19497</guid>
<content:encoded><![CDATA[
<div> Keywords: PeriodNet, attention mechanism, time series forecasting, sparse period attention, iterative grouping mechanism<br /><br />Summary:<br /><br />The paper introduces PeriodNet, a novel network architecture designed to improve time series forecasting (TSF) for both univariate and multivariate data. It highlights the limitations of traditional attention mechanisms, such as those used in BERT and GPT, for TSF tasks, motivating the need for more specialized structures. PeriodNet incorporates period attention and a sparse period attention mechanism to effectively analyze adjacent periods, thereby enhancing the extraction of local features, periodic patterns, and global dependencies within the data. To address cross-variable redundancy in multivariate TSF, an iterative grouping mechanism is proposed, which efficiently reduces redundant information across variables. The authors redesign the conventional Transformer’s architecture by introducing a period diffuser to enable accurate multi-period predictions, optimizing the use of features extracted on the encoder side. Extensive experiments on eight benchmark datasets demonstrate that PeriodNet outperforms six state-of-the-art time series forecasting models in terms of mean square error (MSE) and mean absolute error (MAE). Notably, PeriodNet achieves a 22% relative improvement in forecasting accuracy for long time series of length 720 compared to traditional encoder-decoder Transformer-based models, demonstrating its superiority and potential for broad applications in TSF. <div>
arXiv:2511.19497v1 Announce Type: new 
Abstract: The attention mechanism has demonstrated remarkable potential in sequence modeling, exemplified by its successful application in natural language processing with models such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT). Despite these advancements, its utilization in time series forecasting (TSF) has yet to meet expectations. Exploring a better network structure for attention in TSF holds immense significance across various domains. In this paper, we present PeriodNet with a brand new structure to forecast univariate and multivariate time series. PeriodNet incorporates period attention and sparse period attention mechanism for analyzing adjacent periods. It enhances the mining of local characteristics, periodic patterns, and global dependencies. For efficient cross-variable modeling, we introduce an iterative grouping mechanism which can directly reduce the cross-variable redundancy. To fully leverage the extracted features on the encoder side, we redesign the entire architecture of the vanilla Transformer and propose a period diffuser for precise multi-period prediction. Through comprehensive experiments conducted on eight datasets, we demonstrate that PeriodNet outperforms six state-of-the-art models in both univariate and multivariate TSF scenarios in terms of mean square error and mean absolute error. In particular, PeriodNet achieves a relative improvement of 22% when forecasting time series with a length of 720, in comparison to other models based on the conventional encoder-decoder Transformer architecture.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data</title>
<link>https://arxiv.org/abs/2511.19498</link>
<guid>https://arxiv.org/abs/2511.19498</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, selective knowledge unlearning, medical concept hierarchy, privacy, healthcare  

<br /><br />Summary:  
This paper addresses privacy concerns in large language models (LLMs), particularly in healthcare applications where sensitive patient information can be memorized during training. The authors propose a hierarchical dual-strategy framework designed for selective knowledge unlearning, enabling the precise removal of specialized or private medical knowledge without compromising the model’s core medical capabilities. The methodology integrates two main techniques: geometric-constrained gradient updates that selectively adjust only the target parameters involved, and concept-aware token-level interventions informed by a unified four-level medical concept hierarchy. This hierarchical approach differentiates between tokens critical for knowledge preservation and tokens targeted for unlearning. The framework is evaluated on two medical QA datasets—MedMCQA (surgical domain) and MHQA (mental health domain focusing on anxiety, depression, trauma)—demonstrating impressive results with an 82.7% forgetting rate of targeted knowledge and an 88.5% preservation rate of essential medical knowledge. Importantly, this approach modifies only 0.1% of the model parameters, ensuring efficiency and minimal disruption. The framework also upholds strong privacy guarantees, which are significant for compliance with healthcare regulations, auditability, and ethical standards in clinical research involving LLMs. <div>
arXiv:2511.19498v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Binary Classification: A Semi-supervised Approach to Generalized AI-generated Image Detection</title>
<link>https://arxiv.org/abs/2511.19499</link>
<guid>https://arxiv.org/abs/2511.19499</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, GANs, diffusion models, synthetic image detection, cross-generator generalization<br /><br />Summary:<br /><br />1. Recent advances in generative models such as StyleGAN, Midjourney, and DALL-E have created highly realistic synthetic images, raising challenges for verifying digital media authenticity.<br />2. The main architectures behind these generators are Generative Adversarial Networks (GANs) and Diffusion Models (DMs), which produce different types of artifacts.<br />3. Current forensic detectors struggle to generalize well across different generator types, especially when switching between GANs and DMs.<br />4. The authors analyze theoretically how GANs often lead to partial manifold coverage, causing boundary artifacts, while DMs produce complete coverage that results in over-smoothing.<br />5. Inspired by this, they introduce the Triarchy Detector (TriDetect), a semi-supervised method that improves binary classification by discovering latent architectural differences in fake images.<br />6. TriDetect uses balanced cluster assignment through the Sinkhorn-Knopp algorithm and a cross-view consistency mechanism to learn fundamental architectural distinctions.<br />7. Experiments on two standard benchmarks and three real-world datasets, compared against 13 baselines, show that TriDetect generalizes effectively to unseen generators. <div>
arXiv:2511.19499v1 Announce Type: new 
Abstract: The rapid advancement of generators (e.g., StyleGAN, Midjourney, DALL-E) has produced highly realistic synthetic images, posing significant challenges to digital media authenticity. These generators are typically based on a few core architectural families, primarily Generative Adversarial Networks (GANs) and Diffusion Models (DMs). A critical vulnerability in current forensics is the failure of detectors to achieve cross-generator generalization, especially when crossing architectural boundaries (e.g., from GANs to DMs). We hypothesize that this gap stems from fundamental differences in the artifacts produced by these \textbf{distinct architectures}. In this work, we provide a theoretical analysis explaining how the distinct optimization objectives of the GAN and DM architectures lead to different manifold coverage behaviors. We demonstrate that GANs permit partial coverage, often leading to boundary artifacts, while DMs enforce complete coverage, resulting in over-smoothing patterns. Motivated by this analysis, we propose the \textbf{Tri}archy \textbf{Detect}or (TriDetect), a semi-supervised approach that enhances binary classification by discovering latent architectural patterns within the "fake" class. TriDetect employs balanced cluster assignment via the Sinkhorn-Knopp algorithm and a cross-view consistency mechanism, encouraging the model to learn fundamental architectural distincts. We evaluate our approach on two standard benchmarks and three in-the-wild datasets against 13 baselines to demonstrate its generalization capability to unseen generators.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma</title>
<link>https://arxiv.org/abs/2511.19504</link>
<guid>https://arxiv.org/abs/2511.19504</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning from Human Feedback, Alignment Trilemma, Representativeness, Robustness, Computational Complexity<br /><br />Summary:<br /><br />This paper formalizes a key challenge in aligning large language models through Reinforcement Learning from Human Feedback (RLHF), termed the Alignment Trilemma. The trilemma states that no RLHF system can simultaneously achieve (i) high representativeness across diverse human value systems, (ii) polynomial-time computational tractability, and (iii) robustness against adversarial perturbations and distribution shifts. By combining statistical learning theory with robust optimization, the authors prove that achieving strong representativeness (ε ≤ 0.01) and robustness (δ ≤ 0.001) for global-scale populations necessitates super-polynomial computational resources exponential in the dimensionality of context, specifically at least on the order of 2^(d_context). Current RLHF implementations circumvent this trilemma by limiting representativeness, using relatively small and homogeneous annotator pools (around 10^3 to 10^4 samples), whereas accurate global representation would require orders of magnitude larger data (~10^7 to 10^8 samples). This framework explains typical RLHF issues such as preference collapse, sycophancy, and amplification of majority biases. The paper concludes by suggesting strategic relaxations of alignment objectives as practical paths forward to balance these fundamental trade-offs in RLHF systems. <div>
arXiv:2511.19504v1 Announce Type: new 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Profile Generators: A Link between the Narrative and the Binary Matrix Representation</title>
<link>https://arxiv.org/abs/2511.19506</link>
<guid>https://arxiv.org/abs/2511.19506</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive disorders, DSM-5, symptom profile generator, binary matrix, MPCS algorithm<br /><br />Summary:<br /><br />This study addresses challenges in representing mental health cognitive disorders from the DSM-5 in a machine-actionable way. Traditional binary matrix approaches to calculate symptom similarity become infeasible due to the exponential number of symptom combinations. To overcome this, the authors introduce a new format called the symptom profile generator. This generator uses a structured format of lists, sets, and numbers to represent complex diagnostic criteria and symptom pathways more efficiently and readably. The approach facilitates automated generation of valid symptom profiles and represents disorders as lists of generators instead of enormous binary matrices. The paper demonstrates the limitations of binary matrices in managing complex psychotic disorders, showing that existing similarity algorithms, such as the Maximum Pairwise Cosine Similarity (MPCS), cannot handle such large matrices. Consequently, the researchers develop a profile reduction method through targeted manipulation of generators to calculate specific MPCS values between disorders. This novel generator framework enables easier creation of binary representations for large datasets and supports similarity calculations in complex diagnostic scenarios where traditional methods fail. The result is a comprehensive, adaptable, and scalable alternative to matrix-based symptom representation for cognitive disorders. <div>
arXiv:2511.19506v1 Announce Type: new 
Abstract: Mental health disorders, particularly cognitive disorders defined by deficits in cognitive abilities, are described in detail in the DSM-5, which includes definitions and examples of signs and symptoms. A simplified, machine-actionable representation was developed to assess the similarity and separability of these disorders, but it is not suited for the most complex cases. Generating or applying a full binary matrix for similarity calculations is infeasible due to the vast number of symptom combinations. This research develops an alternative representation that links the narrative form of the DSM-5 with the binary matrix representation and enables automated generation of valid symptom combinations. Using a strict pre-defined format of lists, sets, and numbers with slight variations, complex diagnostic pathways involving numerous symptom combinations can be represented. This format, called the symptom profile generator (or simply generator), provides a readable, adaptable, and comprehensive alternative to a binary matrix while enabling easy generation of symptom combinations (profiles). Cognitive disorders, which typically involve multiple diagnostic criteria with several symptoms, can thus be expressed as lists of generators. Representing several psychotic disorders in generator form and generating all symptom combinations showed that matrix representations of complex disorders become too large to manage. The MPCS (maximum pairwise cosine similarity) algorithm cannot handle matrices of this size, prompting the development of a profile reduction method using targeted generator manipulation to find specific MPCS values between disorders. The generators allow easier creation of binary representations for large matrices and make it possible to calculate specific MPCS cases between complex disorders through conditional generators.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TouchFormer: A Robust Transformer-based Framework for Multimodal Material Perception</title>
<link>https://arxiv.org/abs/2511.19509</link>
<guid>https://arxiv.org/abs/2511.19509</guid>
<content:encoded><![CDATA[
<div> multimodal fusion, material perception, Modality-Adaptive Gating, Cross-Instance Embedding Regularization, robotic applications<br /><br />Summary:<br /><br />1. Traditional vision-based material perception methods often suffer performance degradation under visually impaired conditions, motivating the shift toward non-visual multimodal approaches.<br />2. Existing multimodal fusion techniques typically perform naive integration of inputs, ignoring challenges such as modality-specific noise, missing modalities, and varying importance of different sensory inputs.<br />3. The paper introduces TouchFormer, a robust multimodal fusion framework that employs a Modality-Adaptive Gating (MAG) mechanism alongside intra- and inter-modality attention to adaptively integrate cross-modal features and improve robustness.<br />4. Additionally, TouchFormer incorporates a Cross-Instance Embedding Regularization (CER) strategy to significantly enhance classification accuracy for fine-grained material subcategory recognition.<br />5. Experiments on benchmark datasets (SSMC and USMC) show that TouchFormer achieves accuracy improvements of 2.48% and 6.83% respectively compared to existing non-visual methods.<br />6. Real-world robotic experiments further validate TouchFormer’s effectiveness, demonstrating improved environmental perception for robots.<br />7. This advancement has implications for deployment in safety-critical scenarios such as emergency response and industrial automation.<br />8. The authors plan to open-source the code and datasets, with supplementary videos provided to showcase results. <div>
arXiv:2511.19509v1 Announce Type: new 
Abstract: Traditional vision-based material perception methods often experience substantial performance degradation under visually impaired conditions, thereby motivating the shift toward non-visual multimodal material perception. Despite this, existing approaches frequently perform naive fusion of multimodal inputs, overlooking key challenges such as modality-specific noise, missing modalities common in real-world scenarios, and the dynamically varying importance of each modality depending on the task. These limitations lead to suboptimal performance across several benchmark tasks. In this paper, we propose a robust multimodal fusion framework, TouchFormer. Specifically, we employ a Modality-Adaptive Gating (MAG) mechanism and intra- and inter-modality attention mechanisms to adaptively integrate cross-modal features, enhancing model robustness. Additionally, we introduce a Cross-Instance Embedding Regularization(CER) strategy, which significantly improves classification accuracy in fine-grained subcategory material recognition tasks. Experimental results demonstrate that, compared to existing non-visual methods, the proposed TouchFormer framework achieves classification accuracy improvements of 2.48% and 6.83% on SSMC and USMC tasks, respectively. Furthermore, real-world robotic experiments validate TouchFormer's effectiveness in enabling robots to better perceive and interpret their environment, paving the way for its deployment in safety-critical applications such as emergency response and industrial automation. The code and datasets will be open-source, and the videos are available in the supplementary materials.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Row-stochastic matrices can provably outperform doubly stochastic matrices in decentralized learning</title>
<link>https://arxiv.org/abs/2511.19513</link>
<guid>https://arxiv.org/abs/2511.19513</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized learning, weighted loss, row-stochastic matrix, Hilbert space, convergence rates<br /><br />Summary:<br />1. The paper addresses decentralized learning with heterogeneous node weights \(\lambda\) in the global loss function, focusing on two strategies: embedding weights into local losses for uniform weights (yielding a doubly stochastic matrix), and keeping original losses with a \(\lambda\)-induced row-stochastic matrix.<br />2. Although both approaches share the same expected descent direction, prior Euclidean-based analyses leave it unclear whether their convergence guarantees are tight or what fundamentally drives their differences.<br />3. To resolve this, the authors introduce a weighted Hilbert-space framework \(L^2(\lambda;\mathbb{R}^d)\), which provides strictly tighter convergence rate bounds compared to Euclidean analysis.<br />4. In the weighted geometry, the row-stochastic matrix becomes self-adjoint, whereas the doubly stochastic matrix does not; this leads to additional penalty terms in the doubly stochastic case, amplifying consensus errors and slowing convergence.<br />5. The study derives sufficient conditions and topology-based criteria showing that the row-stochastic design can converge faster despite having a smaller spectral gap, offering practical guidelines for network topology design in decentralized learning systems. <div>
arXiv:2511.19513v1 Announce Type: new 
Abstract: Decentralized learning often involves a weighted global loss with heterogeneous node weights $\lambda$. We revisit two natural strategies for incorporating these weights: (i) embedding them into the local losses to retain a uniform weight (and thus a doubly stochastic matrix), and (ii) keeping the original losses while employing a $\lambda$-induced row-stochastic matrix. Although prior work shows that both strategies yield the same expected descent direction for the global loss, it remains unclear whether the Euclidean-space guarantees are tight and what fundamentally differentiates their behaviors. To clarify this, we develop a weighted Hilbert-space framework $L^2(\lambda;\mathbb{R}^d)$ and obtain convergence rates that are strictly tighter than those from Euclidean analysis. In this geometry, the row-stochastic matrix becomes self-adjoint whereas the doubly stochastic one does not, creating additional penalty terms that amplify consensus error, thereby slowing convergence. Consequently, the difference in convergence arises not only from spectral gaps but also from these penalty terms. We then derive sufficient conditions under which the row-stochastic design converges faster even with a smaller spectral gap. Finally, by using a Rayleigh-quotient and Loewner-order eigenvalue comparison, we further obtain topology conditions that guarantee this advantage and yield practical topology-design guidelines.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automating Deception: Scalable Multi-Turn LLM Jailbreaks</title>
<link>https://arxiv.org/abs/2511.19517</link>
<guid>https://arxiv.org/abs/2511.19517</guid>
<content:encoded><![CDATA[
<div> Multi-turn conversational attacks, Foot-in-the-Door (FITD), jailbreak datasets, Attack Success Rates (ASR), Large Language Models (LLMs)<br /><br />Summary:  
1. The paper addresses multi-turn conversational attacks on Large Language Models (LLMs), which use psychological techniques like the Foot-in-the-Door (FITD) method to bypass safety measures by starting with small requests leading to larger harmful ones.  
2. Existing defenses rely on manually created datasets that are labor-intensive and hard to scale, slowing progress against these attacks.  
3. The authors propose an automated pipeline to generate large-scale, psychologically-grounded multi-turn jailbreak datasets by converting FITD techniques into reproducible templates, resulting in 1,500 benchmark scenarios focusing on illegal and offensive content.  
4. They evaluate seven models from three major LLM families under conditions with and without conversational history to assess their vulnerability.  
5. Results show that GPT-family models are significantly more vulnerable when conversational history is present, with attack success rates increasing by up to 32 percentage points. Meanwhile, Google's Gemini 2.5 Flash model shows near immunity to these attacks, and Anthropic’s Claude 3 Haiku shows strong but not perfect resistance.  
6. The findings reveal a major difference in how safety architectures handle narrative context and emphasize the need for defenses that can resist manipulation through multi-turn conversational techniques. <div>
arXiv:2511.19517v1 Announce Type: new 
Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shortcut Invariance: Targeted Jacobian Regularization in Disentangled Latent Space</title>
<link>https://arxiv.org/abs/2511.19525</link>
<guid>https://arxiv.org/abs/2511.19525</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, shortcut learning, out-of-distribution generalization, disentangled latent space, Jacobian regularization<br /><br />Summary:<br /><br />1. Deep neural networks often learn shortcuts—spurious, easily learned correlations in training data—that lead to poor out-of-distribution (OOD) generalization and severe failures when encountering new data.<br /><br />2. Traditional approaches focus on learning robust representations by explicitly separating latent features into core (semantic) and spurious parts, but these methods tend to be complex, brittle, and hard to scale.<br /><br />3. This work proposes a fundamentally different strategy: learning a robust classifier function rather than a robust representation.<br /><br />4. The method operates within a disentangled latent space, which separates core features and spurious shortcuts into distinct dimensions, thus enabling reliable identification of shortcut features by detecting their strong correlation with the label.<br /><br />5. To reduce reliance on shortcuts, the classifier training injects targeted, anisotropic noise into the identified spurious latent features, which acts as a form of Jacobian regularization that desensitizes the classifier from shortcut signals.<br /><br />6. This approach compels the classifier to depend on more complex, meaningful core semantic features, resulting in state-of-the-art OOD performance on established shortcut learning benchmarks.<br /><br />7. The analysis interprets the noise injection as regularization on the classifier’s Jacobian, which ensures invariance to shortcut features and enhances robustness.<br /><br />Overall, the paper introduces a simple yet effective training method to mitigate shortcut learning by targeting classifier invariance in a disentangled feature space, achieving superior OOD generalization. <div>
arXiv:2511.19525v1 Announce Type: new 
Abstract: Deep neural networks are prone to learning shortcuts, spurious and easily learned correlations in training data that cause severe failures in out-of-distribution (OOD) generalization. A dominant line of work seeks robustness by learning a robust representation, often explicitly partitioning the latent space into core and spurious components; this approach can be complex, brittle, and difficult to scale. We take a different approach, instead of a robust representation, we learn a robust function. We present a simple and effective training method that renders the classifier functionally invariant to shortcut signals. Our method operates within a disentangled latent space, which is essential as it isolates spurious and core features into distinct dimensions. This separation enables the identification of candidate shortcut features by their strong correlation with the label, used as a proxy for semantic simplicity. The classifier is then desensitized to these features by injecting targeted, anisotropic latent noise during training. We analyze this as targeted Jacobian regularization, which forces the classifier to ignore spurious features and rely on more complex, core semantic signals. The result is state-of-the-art OOD performance on established shortcut learning benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Solve Weighted Maximum Satisfiability with a Co-Training Architecture</title>
<link>https://arxiv.org/abs/2511.19544</link>
<guid>https://arxiv.org/abs/2511.19544</guid>
<content:encoded><![CDATA[
<div> SplitGNN, MaxSAT, graph neural network, edge-splitting factor graph, GPU-acceleration  

<br /><br />Summary:  
The paper introduces SplitGNN, a novel graph neural network (GNN)-based approach designed to solve the weighted maximum satisfiability (MaxSAT) problem efficiently. SplitGNN features a co-training architecture combining a supervised message passing mechanism with an unsupervised solution boosting layer, enabling improved learning and solution quality. A key innovation is the edge-splitting factor graph, a new graph representation based on spanning tree generation and edge classification that provides richer structural information to the model. To tackle challenging weighted MaxSAT instances, the method includes a GPU-accelerated layer that performs efficient score calculation and relaxation-based optimization, boosting computational speed and solution accuracy. Experimental results demonstrate that SplitGNN converges three times faster and achieves better prediction accuracy compared to existing GNN-based architectures. Remarkably, it outperforms state-of-the-art heuristic MaxSAT solvers on significantly larger and more complex weighted MaxSAT benchmarks. Additionally, SplitGNN shows strong generalization capabilities across diverse structural instances, highlighting its robustness and potential for broad applicability in combinatorial optimization tasks involving MaxSAT problems. <div>
arXiv:2511.19544v1 Announce Type: new 
Abstract: Wepropose SplitGNN, a graph neural network (GNN)-based
  approach that learns to solve weighted maximum satisfiabil ity (MaxSAT) problem. SplitGNN incorporates a co-training
  architecture consisting of supervised message passing mech anism and unsupervised solution boosting layer. A new graph
  representation called edge-splitting factor graph is proposed
  to provide more structural information for learning, which is
  based on spanning tree generation and edge classification. To
  improve the solutions on challenging and weighted instances,
  we implement a GPU-accelerated layer applying efficient
  score calculation and relaxation-based optimization. Exper iments show that SplitGNN achieves 3* faster convergence
  and better predictions compared with other GNN-based ar chitectures. More notably, SplitGNN successfully finds solu tions that outperform modern heuristic MaxSAT solvers on
  much larger and harder weighted MaxSAT benchmarks, and
  demonstrates exceptional generalization abilities on diverse
  structural instances.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics</title>
<link>https://arxiv.org/abs/2511.19548</link>
<guid>https://arxiv.org/abs/2511.19548</guid>
<content:encoded><![CDATA[
<div> neuroeconomics, welfare analysis, reinforcement learning, neural data, policy regulation<br /><br />Summary:<br /><br />1. The paper explores the potential and limitations of using neural data to inform welfare judgments in policy-making rather than merely describing behavior.<br /><br />2. It develops a theoretical framework linking neural signals, computational decision models (specifically actor-critic reinforcement learning), and normative welfare criteria to clarify when neural evidence can meaningfully support welfare claims.<br /><br />3. The framework formalizes the inference from neural activity to latent variables such as values and prediction errors, and then to welfare assessments.<br /><br />4. The author argues that neural data can only constrain welfare judgments if three conditions are met: a well-validated neural-to-computational mapping, decision models that distinguish true interests from context-dependent errors, and explicitly stated and justified welfare criteria.<br /><br />5. Applying this approach to domains including addiction, neuromarketing, and environmental policy, the paper proposes a Neuroeconomic Welfare Inference Checklist intended for regulators and NeuroAI designers.<br /><br />6. The analysis treats both brains and artificial agents as value-learning systems and emphasizes that internal reward signals are computational constructs that cannot serve as welfare measures absent an explicit normative model. <div>
arXiv:2511.19548v1 Announce Type: new 
Abstract: Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, "brain-based" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies "true" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Sparse Feature Selection in Data Streams via Differential Evolution</title>
<link>https://arxiv.org/abs/2511.19555</link>
<guid>https://arxiv.org/abs/2511.19555</guid>
<content:encoded><![CDATA[
<div> Keywords: Online Streaming Feature Selection, Missing Data Imputation, Differential Evolution, Latent Factor Analysis, Sparse Feature Selection  

<br /><br />Summary:  
This paper addresses the challenges in processing high-dimensional streaming data, particularly focusing on online streaming feature selection (OSFS) techniques. A major problem in practical scenarios is data incompleteness caused by equipment failures and technical constraints. To overcome this, existing Online Sparse Streaming Feature Selection (OS2FS) methods use latent factor analysis-based missing data imputation but still suffer from inefficient feature evaluation, which leads to degraded performance. The authors propose a novel method named Online Differential Evolution for Sparse Feature Selection (ODESFS) designed for data streams. ODESFS introduces two critical improvements: firstly, it employs a latent factor analysis model for more effective missing value imputation, ensuring better handling of incomplete data; secondly, it uses differential evolution to evaluate feature importance, which enhances the accuracy and selection of optimal feature subsets. The effectiveness of ODESFS is validated through comprehensive experiments on six real-world datasets, demonstrating its superiority over state-of-the-art OSFS and OS2FS methods. The results show that ODESFS consistently selects better feature subsets and achieves higher classification accuracy, proving its potential for practical streaming data applications with incomplete and high-dimensional data. <div>
arXiv:2511.19555v1 Announce Type: new 
Abstract: The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport</title>
<link>https://arxiv.org/abs/2511.19561</link>
<guid>https://arxiv.org/abs/2511.19561</guid>
<content:encoded><![CDATA[
<div> Optimal Transport, Model Merging, Multi-task Learning, Masked Fusion, Continual Fusion  

<br /><br />Summary:  
This paper addresses the challenge of merging models fine-tuned on different tasks into a single unified multi-task model. Existing methods mostly rely on parameter interpolation in weight space, which leads to significant distribution shifts in feature space and loss of task-specific knowledge. To overcome this, the authors propose OTMF (Optimal Transport-based Masked Fusion), a novel framework grounded in optimal transport theory. Instead of interpolating weights or features directly, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while maintaining the unique structural properties of each individual task. The framework also supports a continual fusion paradigm, enabling incremental integration of new tasks without revisiting previous models, thus ensuring scalability and a bounded memory footprint. Extensive experiments conducted on various vision and language benchmarks demonstrate that OTMF achieves state-of-the-art results in terms of accuracy and efficiency. Overall, the study highlights both the practical applications and theoretical contributions of adopting optimal transport and masked fusion for efficient, scalable, and effective multi-task model merging. <div>
arXiv:2511.19561v1 Announce Type: new 
Abstract: Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModHiFi: Identifying High Fidelity predictive components for Model Modification</title>
<link>https://arxiv.org/abs/2511.19566</link>
<guid>https://arxiv.org/abs/2511.19566</guid>
<content:encoded><![CDATA[
<div> Keywords: model modification, Subset Fidelity, pruning, unlearning, Lipschitz continuity  

<br /><br />Summary:  
This work addresses the challenge of modifying open weight models without access to training data, loss functions, or gradients—a common limitation in many deployed models. The authors theoretically prove that for Lipschitz-continuous networks such as CNNs and well-trained Transformers, the global reconstruction error can be bounded linearly by local reconstruction errors of component subsets. This insight leads to the proposal of a new metric called Subset Fidelity, which assesses the global importance of model components via their local reconstructive behavior. In scenarios where features are uncorrelated, selecting components based on Subset Fidelity is theoretically optimal. Building on this, the authors design ModHiFi, an algorithm that modifies models using only distributional access, such as synthetic inputs, requiring no ground-truth labels or gradients. Two variants are developed: ModHiFi-P for structured pruning, which achieves an 11% speedup over current state-of-the-art methods on ImageNet while maintaining competitive performance on language models; and ModHiFi-U for classwise unlearning, which achieves complete unlearning on CIFAR-10 without fine-tuning and demonstrates strong results on Swin Transformers. This work opens new avenues for efficient model modification under limited information scenarios. <div>
arXiv:2511.19566v1 Announce Type: new 
Abstract: Open weight models, which are ubiquitous, rarely provide access to their training data or loss function. This makes modifying such models for tasks such as pruning or unlearning constrained by this unavailability an active area of research. Existing techniques typically require gradients or ground-truth labels, rendering them infeasible in settings with limited computational resources. In this work, we investigate the fundamental question of identifying components that are critical to the model's predictive performance, without access to either gradients or the loss function, and with only distributional access such as synthetic data. We theoretically demonstrate that the global reconstruction error is linearly bounded by local reconstruction errors for Lipschitz-continuous networks such as CNNs and well-trained Transformers (which, contrary to existing literature, we find exhibit Lipschitz continuity). This motivates using the locally reconstructive behavior of component subsets to quantify their global importance, via a metric that we term Subset Fidelity. In the uncorrelated features setting, selecting individual components via their Subset Fidelity scores is optimal, which we use to propose ModHiFi, an algorithm for model modification that requires no training data or loss function access. ModHiFi-P, for structured pruning, achieves an 11% speedup over the current state of the art on ImageNet models and competitive performance on language models. ModHiFi-U, for classwise unlearning, achieves complete unlearning on CIFAR-10 without fine-tuning and demonstrates competitive performance on Swin Transformers.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Invariant Latent Space Perspective on Language Model Inversion</title>
<link>https://arxiv.org/abs/2511.19569</link>
<guid>https://arxiv.org/abs/2511.19569</guid>
<content:encoded><![CDATA[
<div> Language model inversion, latent space, Invariant Latent Space Hypothesis, Inv²A, prompt recovery  

<br /><br />Summary:  
Language Model Inversion (LMI) poses a privacy and security risk by recovering hidden prompts from model outputs. This paper introduces the Invariant Latent Space Hypothesis (ILSH), based on two principles: source invariance, which ensures that different outputs from the same prompt preserve consistent semantics, and cyclic invariance, which ensures self-consistent mappings between inputs and outputs within a shared latent space. To operationalize ILSH, the authors propose Inv²A, a method that leverages the language model as an invariant decoder and trains a lightweight inverse encoder to map outputs into denoised pseudo-representations. When multiple outputs are available, Inv²A concatenates them sparsely within the latent space to enrich information density. Training involves two stages: contrastive alignment for enforcing source invariance, followed by supervised reinforcement for cyclic invariance. A training-free neighborhood search can further improve local performance if needed. Evaluated on 9 diverse datasets representing both user and system prompt scenarios, Inv²A achieves an average BLEU score improvement of 4.77% over existing baselines while requiring fewer inverse training samples. The study also finds that common defense mechanisms offer limited protection against LMI, highlighting the urgent need for more robust strategies. Code and data are publicly available at the provided GitHub repository. <div>
arXiv:2511.19569v1 Announce Type: new 
Abstract: Language model inversion (LMI), i.e., recovering hidden prompts from outputs, emerges as a concrete threat to user privacy and system security. We recast LMI as reusing the LLM's own latent space and propose the Invariant Latent Space Hypothesis (ILSH): (1) diverse outputs from the same source prompt should preserve consistent semantics (source invariance), and (2) input<->output cyclic mappings should be self-consistent within a shared latent space (cyclic invariance). Accordingly, we present Inv^2A, which treats the LLM as an invariant decoder and learns only a lightweight inverse encoder that maps outputs to a denoised pseudo-representation. When multiple outputs are available, they are sparsely concatenated at the representation layer to increase information density. Training proceeds in two stages: contrastive alignment (source invariance) and supervised reinforcement (cyclic invariance). An optional training-free neighborhood search can refine local performance. Across 9 datasets covering user and system prompt scenarios, Inv^2A outperforms baselines by an average of 4.77% BLEU score while reducing dependence on large inverse corpora. Our analysis further shows that prevalent defenses provide limited protection, underscoring the need for stronger strategies. The source code and data involved in this paper can be found in https://github.com/yyy01/Invariant_Attacker.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Tractability via Structure: Learning-Augmented Algorithms for Graph Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2511.19573</link>
<guid>https://arxiv.org/abs/2511.19573</guid>
<content:encoded><![CDATA[
<div> Keywords: neural models, combinatorial optimization, parameterized algorithms, solution quality, out-of-distribution generalization<br /><br />Summary:  
This paper proposes a novel framework that synergistically combines neural models with search-based parameterized algorithms (PAs) to solve NP-hard graph combinatorial optimization (CO) problems more effectively. Neural models provide fast inference and exploratory power by generating advisory signals that guide the search process. Parameterized algorithms, which focus on identifying and exploiting structurally simple "easy" instances within NP-hard problems, are used to guarantee solution quality and conduct efficient, systematic searches over these easy parts. The framework leverages parameterized analysis to differentiate structurally hard from easy portions of the problem, enabling the neural model to concentrate on the challenging parts. This integration ensures that the resulting solutions are strictly better than those from neural solvers alone and competitive with commercial optimization solvers. The approach is also neural model-agnostic, making it widely applicable. Empirical evaluations demonstrate superior solution quality and improved out-of-distribution generalization, addressing a critical limitation of existing neural CO methods that tend to perform poorly on unseen data distributions. Overall, the framework strikes a beneficial balance by combining the speed and learned intuition of neural methods with the rigorous guarantees and efficiency of parameterized search techniques. <div>
arXiv:2511.19573v1 Announce Type: new 
Abstract: Neural models have shown promise in solving NP-hard graph combinatorial optimization (CO) problems. Once trained, they offer fast inference and reasonably high-quality solutions for in-distribution testing instances, but they generally fall short in terms of absolute solution quality compared to classical search-based algorithms that are admittedly slower but offer optimality guarantee once search finishes.
  We propose a novel framework that combines the inference efficiency and exploratory power of neural models with the solution quality guarantee of search-based algorithms. In particular, we use parameterized algorithms (PAs) as the search component. PAs are dedicated to identifying easy instances of generally NP-hard problems, and allow for practically efficient search by exploiting structural simplicity (of the identified easy instances). Under our framework, we use parameterized analysis to identify the structurally hard parts of a CO instance. The neural model handles the hard parts by generating advisory signals based on its data-driven understanding. The PA-based search component then integrates the advisory signals to systematically and efficiently searches through the remaining structurally easy parts. Notably, our framework is agnostic to the choice of neural model and produces strictly better solutions than neural solvers alone.
  We examine our framework on multiple CO tasks. Empirical results show that it achieves superior solution quality, competitive with that of commercial solvers. Furthermore, by using the neural model only for exploratory advisory signals, our framework exhibits improved out-of-distribution generalization, addressing a key limitation of existing neural CO solvers.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Massively Multitask World Models for Continuous Control</title>
<link>https://arxiv.org/abs/2511.19584</link>
<guid>https://arxiv.org/abs/2511.19584</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, multitask learning, continuous control, language-conditioned models, world model  

<br /><br />Summary:  
1. This paper addresses the challenge of general-purpose control by training agents capable of performing hundreds of continuous control tasks across various domains and embodiments.  
2. The authors introduce a new benchmark comprising 200 diverse tasks, each accompanied by language instructions, demonstrations, and optional image observations, to facilitate research in scalable online reinforcement learning (RL).  
3. They propose Newt, a language-conditioned multitask world model that is pretrained on demonstrations to learn task-aware representations and action priors.  
4. Newt is then jointly optimized through online interaction across all tasks, enabling the agent to improve performance and data efficiency.  
5. Experimental results demonstrate that Newt outperforms several strong baselines in multitask performance and data efficiency, exhibits strong open-loop control capabilities, and adapts rapidly to previously unseen tasks.  
6. To accelerate further research, the authors release their environments, demonstrations, training and evaluation code, and over 200 pretrained model checkpoints. <div>
arXiv:2511.19584v1 Announce Type: new 
Abstract: General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Many Ways to be Right: Rashomon Sets for Concept-Based Neural Networks</title>
<link>https://arxiv.org/abs/2511.19636</link>
<guid>https://arxiv.org/abs/2511.19636</guid>
<content:encoded><![CDATA[
<div> Rashomon Effect, Concept Bottleneck Models, diversity-regularized training, neural networks, reasoning diversity<br /><br />Summary:<br /><br />1. The paper addresses the Rashomon Effect in neural networks, where multiple models perform equally well but rely on different features or reasoning strategies.<br /><br />2. It introduces Rashomon Concept Bottleneck Models, a framework designed to learn multiple accurate neural networks that reason through distinct, interpretable human-understandable concepts.<br /><br />3. The approach uses lightweight adapter modules combined with a diversity-regularized training objective, allowing efficient creation of diverse models without full retraining.<br /><br />4. The resulting models provide fundamentally different reasoning processes behind identical predictions, showcasing varied concept reliance and decision-making paths within equally performing solutions.<br /><br />5. This framework enables systematic exploration and understanding of reasoning diversity in deep models, offering a valuable tool for auditing, comparing, and aligning multiple accurate solutions to ensure better interpretability and robustness. <div>
arXiv:2511.19636v1 Announce Type: new 
Abstract: Modern neural networks rarely have a single way to be right. For many tasks, multiple models can achieve identical performance while relying on different features or reasoning patterns, a property known as the Rashomon Effect. However, uncovering this diversity in deep architectures is challenging as their continuous parameter spaces contain countless near-optimal solutions that are numerically distinct but often behaviorally similar. We introduce Rashomon Concept Bottleneck Models, a framework that learns multiple neural networks which are all accurate yet reason through distinct human-understandable concepts. By combining lightweight adapter modules with a diversity-regularized training objective, our method constructs a diverse set of deep concept-based models efficiently without retraining from scratch. The resulting networks provide fundamentally different reasoning processes for the same predictions, revealing how concept reliance and decision making vary across equally performing solutions. Our framework enables systematic exploration of data-driven reasoning diversity in deep models, offering a new mechanism for auditing, comparison, and alignment across equally accurate solutions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles</title>
<link>https://arxiv.org/abs/2511.19656</link>
<guid>https://arxiv.org/abs/2511.19656</guid>
<content:encoded><![CDATA[
<div> bilevel optimization, lower bounds, nonconvex-strongly-convex, first-order oracle, complexity  

<br /><br />Summary:  
This paper addresses the challenge of establishing lower bounds for bilevel optimization problems, an area where prior research has primarily focused on upper bound guarantees. It specifically considers the smooth nonconvex-strongly-convex setting, constructing new hard problem instances to derive nontrivial lower bounds under both deterministic and stochastic first-order oracle models. The authors prove that in the deterministic scenario, any first-order zero-respecting algorithm must make at least \(\Omega(\kappa^{3/2}\epsilon^{-2})\) oracle calls to find an \(\epsilon\)-accurate stationary point, which strengthens the previously known lower bounds for both single-level nonconvex optimization and nonconvex-strongly-convex min-max problems. For the stochastic case, they establish a necessary minimum of \(\Omega(\kappa^{5/2}\epsilon^{-4})\) stochastic oracle calls, improving on the best known results in related frameworks. This work highlights a significant gap between existing upper and lower complexity bounds in bilevel optimization and emphasizes the importance of further research, even in simplified settings like quadratic lower-level objectives, to better understand the optimal complexity for bilevel optimization when using standard first-order oracles. <div>
arXiv:2511.19656v1 Announce Type: new 
Abstract: Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $\Omega(\kappa^{3/2}\epsilon^{-2})$ oracle calls to find an $\epsilon$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $\Omega(\kappa^{5/2}\epsilon^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Noise Modeling for Enhanced Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2511.19657</link>
<guid>https://arxiv.org/abs/2511.19657</guid>
<content:encoded><![CDATA[
<div> Forecasting, time-series, Gaussian Process, multi-scale dynamics, noise modeling<br /><br />Summary:<br /><br />1. The paper addresses challenges in time-series forecasting related to capturing multi-scale temporal patterns, from broad trends to fine-grained fluctuations, which are critical for accurate decision-making.<br />2. Existing neural models often fail to effectively represent these interacting temporal dynamics, resulting in unstable predictions and diminished reliability in real-world applications.<br />3. To overcome this, the authors propose a forecast-blur-denoise framework that enhances temporal fidelity by incorporating structured noise modeling through a learnable Gaussian Process module.<br />4. The Gaussian Process generates smooth, correlated perturbations that help the forecasting backbone capture long-range dependencies, while a refinement model restores detailed high-resolution temporal information.<br />5. Joint training of components enables clear division of tasks and prevents artifacts common in methods using isotropic noise corruption.<br />6. Experiments on electricity, traffic, and solar datasets demonstrate consistent improvements in multi-horizon forecasting accuracy and prediction stability.<br />7. The modular architecture permits the blur-denoise layer to function as a lightweight add-on to pretrained models, facilitating efficient adaptation particularly in scenarios with limited data.<br />8. This approach strengthens the reliability and interpretability of fine-scale temporal forecasts, contributing to more trustworthy AI systems for decision support in energy, infrastructure, and other time-critical domains. <div>
arXiv:2511.19657v1 Announce Type: new 
Abstract: Time-series forecasting remains difficult in real-world settings because temporal patterns operate at multiple scales, from broad contextual trends to fast, fine-grained fluctuations that drive critical decisions. Existing neural models often struggle to represent these interacting dynamics, leading to unstable predictions and reduced reliability in downstream applications. This work introduces a forecast-blur-denoise framework that improves temporal fidelity through structured noise modeling. The approach incorporates a learnable Gaussian Process module that generates smooth, correlated perturbations, encouraging the forecasting backbone to capture long-range structure while a dedicated refinement model restores high-resolution temporal detail. Training the components jointly enables natural competence division and avoids the artifacts commonly produced by isotropic corruption methods. Experiments across electricity, traffic, and solar datasets show consistent gains in multi-horizon accuracy and stability. The modular design also allows the blur-denoise layer to operate as a lightweight enhancement for pretrained models, supporting efficient adaptation in limited-data scenarios. By strengthening the reliability and interpretability of fine-scale temporal predictions, this framework contributes to more trustworthy AI systems used in forecasting-driven decision support across energy, infrastructure, and other time-critical domains.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demystifying Diffusion Objectives: Reweighted Losses are Better Variational Bounds</title>
<link>https://arxiv.org/abs/2511.19664</link>
<guid>https://arxiv.org/abs/2511.19664</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, reweighted losses, variational lower bounds, masked diffusion, image modeling<br /><br />Summary:<br />1. The paper introduces a new theoretical interpretation of reweighted losses commonly used in training diffusion models by constructing a cascade of time-dependent variational lower bounds on the data log-likelihood.<br />2. These cascaded bounds provably improve upon the standard evidence lower bound (ELBO), yielding reduced data-model Kullback-Leibler (KL) divergences and thereby enhancing model training.<br />3. The proposed method generates reweighted training objectives applicable across different types of generative diffusion models, including continuous Gaussian diffusion and discrete masked diffusion models.<br />4. The authors demonstrate the effectiveness of their framework on masked diffusion models, showing significant performance improvements in pixel-space image modeling compared to previous training losses.<br />5. Additionally, their findings offer a theoretical justification for the simple weighting schemes that have been widely adopted in masked image diffusion models, ultimately achieving sample quality close to that of continuous diffusion models. <div>
arXiv:2511.19664v1 Announce Type: new 
Abstract: We derive a new theoretical interpretation of the reweighted losses that are widely used for training diffusion models. Our method is based on constructing a cascade of time-dependent variational lower bounds on the data log-likelihood, that provably improves upon the standard evidence lower bound and results in reduced data-model KL-divergences. Combining such bounds gives rise to reweighted objectives that can be applied to any generative diffusion model including both continuous Gaussian diffusion and masked (discrete) diffusion models. Then, we showcase this framework in masked diffusion and report significant improvements over previous training losses in pixel-space image modeling, approaching sample quality comparable to continuous diffusion models. Our results also provide a theoretical justification for the simple weighting scheme widely used in masked image models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TREASURE: A Transformer-Based Foundation Model for High-Volume Transaction Understanding</title>
<link>https://arxiv.org/abs/2511.19693</link>
<guid>https://arxiv.org/abs/2511.19693</guid>
<content:encoded><![CDATA[
<div> Keywords: Payment networks, Transformer model, Transaction data, Abnormal behavior detection, Recommendation systems<br /><br />Summary: Payment networks generate vast amounts of transaction data critical for modern commerce, enabling applications like abnormal behavior detection and personalized consumer insights. This paper introduces TREASURE, a transformer-based foundation model tailored for transaction data, which integrates consumer behavior and payment network signals such as response codes and system flags to provide a comprehensive data representation. TREASURE’s architecture includes a specialized input module with distinct sub-modules for handling static and dynamic attributes, enhancing training and inference efficiency. The model employs an effective training paradigm that addresses the challenge of predicting high-cardinality categorical attributes often found in transaction data. Verified with industry-grade datasets, TREASURE significantly improves abnormal behavior detection performance by 111% compared to existing production systems and boosts recommendation model performance by 104% when used as an embedding provider. Extensive ablation studies, benchmarking against baseline production models, and practical case studies provide valuable insights into TREASURE’s design choices and capabilities, demonstrating its scalability and universal applicability across transaction data-driven applications. This work shows that using a transformer-based approach tailored to transaction data can offer impressive enhancements in both security and personalization in financial services. <div>
arXiv:2511.19693v1 Announce Type: new 
Abstract: Payment networks form the backbone of modern commerce, generating high volumes of transaction records from daily activities. Properly modeling this data can enable applications such as abnormal behavior detection and consumer-level insights for hyper-personalized experiences, ultimately improving people's lives. In this paper, we present TREASURE, TRansformer Engine As Scalable Universal transaction Representation Encoder, a multipurpose transformer-based foundation model specifically designed for transaction data. The model simultaneously captures both consumer behavior and payment network signals (such as response codes and system flags), providing comprehensive information necessary for applications like accurate recommendation systems and abnormal behavior detection. Verified with industry-grade datasets, TREASURE features three key capabilities: 1) an input module with dedicated sub-modules for static and dynamic attributes, enabling more efficient training and inference; 2) an efficient and effective training paradigm for predicting high-cardinality categorical attributes; and 3) demonstrated effectiveness as both a standalone model that increases abnormal behavior detection performance by 111% over production systems and an embedding provider that enhances recommendation models by 104%. We present key insights from extensive ablation studies, benchmarks against production models, and case studies, highlighting valuable knowledge gained from developing TREASURE.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TiCT: A Synthetically Pre-Trained Foundation Model for Time Series Classification</title>
<link>https://arxiv.org/abs/2511.19694</link>
<guid>https://arxiv.org/abs/2511.19694</guid>
<content:encoded><![CDATA[
<div> Keywords: time series classification, in-context learning, transformer, synthetic pre-training, label encoding  

<br /><br />Summary:  
This paper addresses the challenge of developing general-purpose foundation models for time series classification, which is hindered by the scarcity of labeled data. It proposes TiCT (Time-series in-Context Transformer), a novel transformer-based model specifically designed for in-context learning (ICL) without requiring fine-tuning. TiCT introduces a unique architecture that includes a scalable bit-based label encoding scheme and a special output attention mechanism, enabling it to handle an arbitrary number of classes efficiently. The model is pre-trained exclusively on synthetic data using a Mixup-inspired augmentation method combined with other data augmentations to improve generalization and robustness to noise. This synthetic pre-training approach allows TiCT to adapt quickly to new classification tasks by leveraging a few in-context examples during inference, thereby eliminating the need for costly retraining or weight updates. Extensive experiments on the UCR Archive benchmark demonstrate that TiCT achieves competitive performance compared to state-of-the-art supervised methods. The key advantage of TiCT lies in its fine-tuning-free paradigm, where the model can classify new time series data using only contextual examples, showcasing its potential as a versatile and efficient foundation model in time series classification tasks. <div>
arXiv:2511.19694v1 Announce Type: new 
Abstract: The ubiquity of time series data creates a strong demand for general-purpose foundation models, yet developing them for classification remains a significant challenge, largely due to the high cost of labeled data. Foundation models capable of in-context learning (ICL) offer a powerful solution, adapting to new tasks with minimal examples and reducing the need for extensive retraining. However, prior work on large-scale time series models has predominantly focused on forecasting, leaving a critical gap for versatile, fine-tuning-free classification. To address this, we introduce TiCT (Time-series in-Context Transformer), a transformer-based model pre-trained exclusively on synthetic data to perform in-context classification. We make two primary technical contributions: 1) a novel architecture featuring a scalable bit-based label encoding and a special output attention mechanism to handle an arbitrary number of classes; and 2) a synthetic pre-training framework that combines a Mixup-inspired process with data augmentation to foster generalization and noise invariance. Extensive evaluations on the UCR Archive show that TiCT achieves competitive performance against state-of-the-art supervised methods. Crucially, this is accomplished using only in-context examples at inference time, without updating a single model weight.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CafeQ: Calibration-free Quantization via Learned Transformations and Adaptive Rounding</title>
<link>https://arxiv.org/abs/2511.19705</link>
<guid>https://arxiv.org/abs/2511.19705</guid>
<content:encoded><![CDATA[
<div> Post-training quantization, adaptive rounding, calibration-free optimization, matrix transformations, language models<br /><br />Summary:<br /><br />This paper addresses the challenge of post-training quantization of large language models, particularly focusing on the limitations when calibration data is unavailable or restricted due to privacy concerns. Traditional quantization methods often rely on round-to-nearest schemes that can introduce significant errors due to weight outliers. Existing mitigation techniques like adaptive rounding and random rotation transformations typically need calibration data, which may not be accessible in real-world scenarios. To overcome this, the authors propose novel algorithms that optimize quantization transformations and adaptive rounding without requiring any calibration data. Their approach leverages a specially designed proxy function to estimate quantization loss in the absence of calibration inputs. For inference efficiency, they apply structured matrix transformations to individual weight matrices and use dual matrix transformations combined with adaptive rounding for paired weights interacting in the computation graph. Experimental results on Gemma 2 language models demonstrate consistent improvements over baseline methods. Specifically, for the Gemma 2 9B model, the method improved the average benchmark score from 61.9 to 62.4 for 4-bit quantization and from 52.0 to 60.6 for 3-bit quantization while incurring less than 3% computational overhead. The performance also compares favorably with the GPTQ method that requires calibration data. <div>
arXiv:2511.19705v1 Announce Type: new 
Abstract: Post-training quantization is an effective method for reducing the serving cost of large language models, where the standard approach is to use a round-to-nearest quantization level scheme. However, this often introduces large errors due to outliers in the weights. Proposed mitigation mechanisms include applying adaptive rounding, random rotation transformations or committing to a post-training target using calibration data. Unfortunately, this reliance on calibration data can be severely limiting in some real-world scenarios as such data may be unavailable or subject to privacy regulations. In this paper, we propose algorithms to optimize transformations and adaptive rounding without access to any calibration data. The optimization is achieved by designing a suitable proxy function for the quantization loss without calibration data. To maintain inference efficiency, we perform structured matrix transformations for single matrices. For paired weights that interact directly in the computation graph, we use dual matrix transformations and adaptive rounding methods. We conduct experiments on Gemma 2 models, and observe consistent improvement over the baselines. For Gemma 2 9B quantization, our method improves the average benchmark score from 61.9 to 62.4 for 4-bit quantization and from 52.0 to 60.6 for 3-bit quantization, while adding less than 3% of computation overhead. Furthermore, our method achieves performance comparable to the commonly used GPTQ method, which requires calibration data.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Active Learning Framework in Materials Science with Large Language Models</title>
<link>https://arxiv.org/abs/2511.19730</link>
<guid>https://arxiv.org/abs/2511.19730</guid>
<content:encoded><![CDATA[
<div> Keywords: active learning, large language models, materials science, experiment selection, machine learning

<br /><br />Summary:  
This paper introduces LLM-AL, a novel active learning framework that leverages large language models (LLMs) to optimize experimental design in materials science. Unlike traditional machine learning models, which suffer from cold-start limitations and require domain-specific feature engineering, LLM-AL uses pretrained knowledge and token-based text representations to suggest experiments directly from text descriptions. The framework operates in an iterative few-shot setting and was benchmarked on four diverse materials science datasets. Two prompting strategies were tested: one with concise numerical inputs for datasets with structured features, and another with expanded descriptive text suitable for datasets emphasizing experimental or procedural context. Across all tested datasets, LLM-AL reduced the number of required experiments to reach top-performing candidates by over 70% and consistently outperformed conventional machine learning approaches. The method enables broader and more exploratory searches yet still efficiently reaches optimal solutions with fewer iterations. An analysis of the stability boundaries was conducted considering the inherent non-determinism of LLMs, revealing that LLM-AL’s performance variability falls within the typical range observed for standard machine learning methods. These findings highlight LLM-AL as a generalizable, interpretable, and efficient alternative for active learning and autonomous discovery in scientific experiment selection. <div>
arXiv:2511.19730v1 Announce Type: new 
Abstract: Active learning (AL) accelerates scientific discovery by prioritizing the most informative experiments, but traditional machine learning (ML) models used in AL suffer from cold-start limitations and domain-specific feature engineering, restricting their generalizability. Large language models (LLMs) offer a new paradigm by leveraging their pretrained knowledge and universal token-based representations to propose experiments directly from text-based descriptions. Here, we introduce an LLM-based active learning framework (LLM-AL) that operates in an iterative few-shot setting and benchmark it against conventional ML models across four diverse materials science datasets. We explored two prompting strategies: one using concise numerical inputs suited for datasets with more compositional and structured features, and another using expanded descriptive text suited for datasets with more experimental and procedural features to provide additional context. Across all datasets, LLM-AL could reduce the number of experiments needed to reach top-performing candidates by over 70% and consistently outperformed traditional ML models. We found that LLM-AL performs broader and more exploratory searches while still reaching the optima with fewer iterations. We further examined the stability boundaries of LLM-AL given the inherent non-determinism of LLMs and found its performance to be broadly consistent across runs, within the variability range typically observed for traditional ML approaches. These results demonstrate that LLM-AL can serve as a generalizable alternative to conventional AL pipelines for more efficient and interpretable experiment selection and potential LLM-driven autonomous discovery.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DISCO: A Browser-Based Privacy-Preserving Framework for Distributed Collaborative Learning</title>
<link>https://arxiv.org/abs/2511.19750</link>
<guid>https://arxiv.org/abs/2511.19750</guid>
<content:encoded><![CDATA[
<div> Distributed Learning, Privacy, Federated Learning, Collaborative Models, Accessibility<br /><br />Summary:<br /><br />1. The paper addresses challenges in data sharing caused by privacy, intellectual property, and legal constraints that inhibit collaborative model building and lead to accessibility bias.<br /><br />2. It introduces DISCO, an open-source distributed collaborative learning platform designed for non-technical users that enables building machine learning models without sharing any original data or requiring programming skills.<br /><br />3. DISCO operates via a web application that trains models locally in the user's browser, ensuring a cross-platform experience including support for smartphones.<br /><br />4. The platform has a modular architecture offering choices between federated and decentralized learning paradigms, allowing flexible participation models.<br /><br />5. It implements various privacy guarantees and weight aggregation strategies aimed at model personalization and increased resilience against bias during collaborative training.<br /><br />6. The code base is publicly available on GitHub, and a live showcase of the tool is accessible through a web interface, enhancing transparency and usability for broader communities. <div>
arXiv:2511.19750v1 Announce Type: new 
Abstract: Data is often impractical to share for a range of well considered reasons, such as concerns over privacy, intellectual property, and legal constraints. This not only fragments the statistical power of predictive models, but creates an accessibility bias, where accuracy becomes inequitably distributed to those who have the resources to overcome these concerns. We present DISCO: an open-source DIStributed COllaborative learning platform accessible to non-technical users, offering a means to collaboratively build machine learning models without sharing any original data or requiring any programming knowledge. DISCO's web application trains models locally directly in the browser, making our tool cross-platform out-of-the-box, including smartphones. The modular design of \disco offers choices between federated and decentralized paradigms, various levels of privacy guarantees and several approaches to weight aggregation strategies that allow for model personalization and bias resilience in the collaborative training. Code repository is available at https://github.com/epfml/disco and a showcase web interface at https://discolab.ai
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements</title>
<link>https://arxiv.org/abs/2511.19794</link>
<guid>https://arxiv.org/abs/2511.19794</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning evaluation, uncertainty estimation, bootstrap confidence intervals, paired multi-seed runs, significance testing<br /><br />Summary:<br /><br />1. Recent machine learning research often reports modest improvements (1-2%) on benchmarks from single runs, which can be misleading due to sensitivity to random seeds, data ordering, and implementation details.<br /><br />2. Such reported gains rarely include uncertainty estimates or significance tests, making it unclear whether these improvements represent true algorithmic advances or just noise.<br /><br />3. The authors propose a practical, compute-friendly evaluation protocol that uses paired multi-seed runs combined with bias-corrected and accelerated (BCa) bootstrap confidence intervals and a sign-flip permutation test on per-seed performance differences.<br /><br />4. This protocol is intentionally conservative, designed to act as a safeguard against over-claiming small improvements under limited computational budgets where only a few runs are feasible.<br /><br />5. Experiments conducted on CIFAR-10, CIFAR-10N, and AG News datasets across scenarios with no improvement, small gains, and medium gains show that single-run or unpaired t-tests may falsely declare significance, especially for improvements as low as 0.6-2.0 points.<br /><br />6. Conversely, with only three random seeds, the proposed paired evaluation protocol does not declare significance for these small improvements, suggesting it provides a safer default standard for reporting small gains in machine learning with constrained resources. <div>
arXiv:2511.19794v1 Announce Type: new 
Abstract: Recent machine learning papers often report 1-2 percentage point improvements from a single run on a benchmark. These gains are highly sensitive to random seeds, data ordering, and implementation details, yet are rarely accompanied by uncertainty estimates or significance tests. It is therefore unclear when a reported +1-2% reflects a real algorithmic advance versus noise.
  We revisit this problem under realistic compute budgets, where only a few runs are affordable. We propose a simple, PC-friendly evaluation protocol based on paired multi-seed runs, bias-corrected and accelerated (BCa) bootstrap confidence intervals, and a sign-flip permutation test on per-seed deltas. The protocol is intentionally conservative and is meant as a guardrail against over-claiming.
  We instantiate it on CIFAR-10, CIFAR-10N, and AG News using synthetic no-improvement, small-gain, and medium-gain scenarios. Single runs and unpaired t-tests often suggest significant gains for 0.6-2.0 point improvements, especially on text. With only three seeds, our paired protocol never declares significance in these settings. We argue that such conservative evaluation is a safer default for small gains under tight budgets.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Terminal Velocity Matching</title>
<link>https://arxiv.org/abs/2511.19797</link>
<guid>https://arxiv.org/abs/2511.19797</guid>
<content:encoded><![CDATA[
<div> Terminal Velocity Matching, flow matching, generative modeling, diffusion transformers, ImageNet FID  

<br /><br />Summary:  
1. The paper introduces Terminal Velocity Matching (TVM), a novel generalization of flow matching that facilitates high-fidelity one- and few-step generative modeling by modeling transitions between any two diffusion timesteps.  
2. Unlike previous methods, TVM regularizes the model's behavior at its terminal time instead of the initial time, which contributes to improved model stability and performance.  
3. The authors provide a theoretical guarantee showing that TVM offers an upper bound on the 2-Wasserstein distance between the data and model distributions, given the model is Lipschitz continuous.  
4. To address the non-Lipschitz nature of Diffusion Transformers, minimal architectural adjustments are introduced that enable stable, single-stage training without compromising efficiency.  
5. For practical efficiency, a fused attention kernel is developed that supports backward passes on Jacobian-Vector Products, scaling well with transformer architectures. Empirically, TVM achieves state-of-the-art FID scores on ImageNet-256x256 (3.29 FID with 1 model evaluation, 1.99 FID with 4) and ImageNet-512x512 (4.32 with 1 evaluation, 2.94 with 4), setting new benchmarks for one- and few-step generative modeling from scratch. <div>
arXiv:2511.19797v1 Announce Type: new 
Abstract: We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Data Attribution via Forward-Only Test-Time Inference</title>
<link>https://arxiv.org/abs/2511.19803</link>
<guid>https://arxiv.org/abs/2511.19803</guid>
<content:encoded><![CDATA[
<div> Data attribution, influence functions, gradient propagation, model debugging, real-time inference  

<br /><br />Summary:  
This paper addresses the challenge of tracing model behavior back to specific training data, a process known as data attribution, which is crucial for debugging, auditing, and data valuation. Traditional influence-function methods provide a theoretical basis for data attribution but are computationally impractical for large modern neural networks because they require expensive backward passes and Hessian matrix inversion during inference. The authors propose a novel method that preserves first-order counterfactual accuracy while eliminating the need for backward passes at query time. Instead, their approach simulates the parameter influence of each training example during training using short-horizon gradient propagation. At inference, data attribution for any query is performed using only efficient forward evaluations. This shift in computation from inference to a pre-deployment simulation phase matches real-world scenarios where models handle massive user queries but are trained on fixed datasets. Empirical evaluation on standard multilayer perceptron (MLP) benchmarks demonstrates that the proposed estimator matches or outperforms state-of-the-art methods like TRAK on key metrics such as leave-one-out (LOO) and local derivative sensitivity (LDS) while dramatically reducing inference costs. By combining theoretical fidelity of influence functions with scalable first-order approximations, this work enables practical, real-time data attribution for large pretrained models. <div>
arXiv:2511.19803v1 Announce Type: new 
Abstract: Data attribution seeks to trace model behavior back to the training examples that shaped it, enabling debugging, auditing, and data valuation at scale. Classical influence-function methods offer a principled foundation but remain impractical for modern networks because they require expensive backpropagation or Hessian inversion at inference. We propose a data attribution method that preserves the same first-order counterfactual target while eliminating per-query backward passes. Our approach simulates each training example's parameter influence through short-horizon gradient propagation during training and later reads out attributions for any query using only forward evaluations. This design shifts computation from inference to simulation, reflecting real deployment regimes where a model may serve billions of user queries but originate from a fixed, finite set of data sources (for example, a large language model trained on diverse corpora while compensating a specific publisher such as the New York Times). Empirically, on standard MLP benchmarks, our estimator matches or surpasses state-of-the-art baselines such as TRAK on standard attribution metrics (LOO and LDS) while offering orders-of-magnitude lower inference cost. By combining influence-function fidelity with first-order scalability, our method provides a theoretical framework for practical, real-time data attribution in large pretrained models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Clean: Reinforcement Learning for Noisy Label Correction</title>
<link>https://arxiv.org/abs/2511.19808</link>
<guid>https://arxiv.org/abs/2511.19808</guid>
<content:encoded><![CDATA[
<div> Keywords: noisy labels, reinforcement learning, label correction, actor-critic method, deep feature representation  

<br /><br />Summary:  
This paper addresses the critical issue of learning with noisy labels, which often impairs the accuracy of machine learning models. The authors propose a novel framework called Reinforcement Learning for Noisy Label Correction (RLNLC), which uniquely formulates the problem of noisy label correction as a reinforcement learning task. RLNLC introduces a detailed state space that incorporates both data and their associated labels, alongside an action space that enables possible label modifications. A reward mechanism is incorporated to assess the success of label corrections systematically. The method employs a deep feature representation-based policy network trained using an actor-critic reinforcement learning approach. This policy network iteratively corrects noisy labels in the training set, thereby enhancing the quality of the labels used to train the prediction model. Comprehensive experiments conducted on multiple benchmark datasets validate the approach, demonstrating that RLNLC consistently outperforms current state-of-the-art methods for handling noisy labels. This highlights the effectiveness and potential of reinforcement learning in addressing label noise challenges in machine learning. <div>
arXiv:2511.19808v1 Announce Type: new 
Abstract: The challenge of learning with noisy labels is significant in machine learning, as it can severely degrade the performance of prediction models if not addressed properly. This paper introduces a novel framework that conceptualizes noisy label correction as a reinforcement learning (RL) problem. The proposed approach, Reinforcement Learning for Noisy Label Correction (RLNLC), defines a comprehensive state space representing data and their associated labels, an action space that indicates possible label corrections, and a reward mechanism that evaluates the efficacy of label corrections. RLNLC learns a deep feature representation based policy network to perform label correction through reinforcement learning, utilizing an actor-critic method. The learned policy is subsequently deployed to iteratively correct noisy training labels and facilitate the training of the prediction model. The effectiveness of RLNLC is demonstrated through extensive experiments on multiple benchmark datasets, where it consistently outperforms existing state-of-the-art techniques for learning with noisy labels.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Outlier-resistant Semi-parametric Regression for Transferable Calibration of Low-cost Air-quality Sensors</title>
<link>https://arxiv.org/abs/2511.19810</link>
<guid>https://arxiv.org/abs/2511.19810</guid>
<content:encoded><![CDATA[
<div> Keywords: Low-cost air-quality sensors, CO calibration, RESPIRE technique, cross-site prediction, sensor overfitting  

<br /><br />Summary:  
This paper presents a detailed case study focused on calibrating low-cost air-quality (LCAQ) sensors for measuring ambient carbon monoxide (CO) levels within one of India's largest and most extensive air-quality monitoring networks. The study highlights the significant challenges inherent in calibrating numerous LCAQ sensors against regulatory-grade monitors, due to the high cost, labor intensity, and time consumption, especially across geographically diverse sites. To address these challenges, the authors propose the RESPIRE technique, a novel calibration method that outperforms common baseline approaches in the literature by providing better prediction accuracy across varying sites, seasons, and sensor models. The RESPIRE method incorporates a training algorithm that is inherently resistant to outliers, enhancing robustness and reliability. Furthermore, the model offers explainability and includes features to flag potential overfitting instances, which is critical for maintaining model validity during deployment. Empirical validation of RESPIRE is demonstrated using data collected from a comprehensive deployment involving four different sites, two distinct seasons, and six separate sensor packages. The approach not only improves calibration efficiency but also supports scalable and accurate air-quality monitoring efforts. The authors have made the RESPIRE code publicly accessible via GitHub, promoting transparency and reproducibility in research. <div>
arXiv:2511.19810v1 Announce Type: new 
Abstract: We present a case study for the calibration of Low-cost air-quality (LCAQ) CO sensors from one of the largest multi-site-multi-season-multi-sensor-multi-pollutant mobile air-quality monitoring network deployments in India. LCAQ sensors have been shown to play a critical role in the establishment of dense, expansive air-quality monitoring networks and combating elevated pollution levels. The calibration of LCAQ sensors against regulatory-grade monitors is an expensive, laborious and time-consuming process, especially when a large number of sensors are to be deployed in a geographically diverse layout. In this work, we present the RESPIRE technique to calibrate LCAQ sensors to detect ambient CO (Carbon Monoxide) levels. RESPIRE offers specific advantages over baseline calibration methods popular in literature, such as improved prediction in cross-site, cross-season, and cross-sensor settings. RESPIRE offers a training algorithm that is provably resistant to outliers and an explainable model with the ability to flag instances of model overfitting. Empirical results are presented based on data collected during an extensive deployment spanning four sites, two seasons and six sensor packages. RESPIRE code is available at https://github.com/purushottamkar/respire.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models</title>
<link>https://arxiv.org/abs/2511.19822</link>
<guid>https://arxiv.org/abs/2511.19822</guid>
<content:encoded><![CDATA[
<div> Sparse Mixture-of-Experts, Mosaic Pruning, Activation Variability Score, domain generalization, model pruning<br /><br />Summary: The paper addresses the limitations of Sparse Mixture-of-Experts (SMoE) models, specifically the high static memory overhead from loading all experts during inference. Existing post-training pruning methods reduce model size but are typically optimized for a single corpus, causing severe performance drops when applied to different domains. To overcome this, the authors propose Mosaic Pruning (MoP), a novel approach that ensures broad functional coverage across tasks. MoP uses a "cluster-then-select" strategy, clustering experts based on a similarity metric derived from performance across diverse domains. It then selects a representative expert from each cluster using a new Activation Variability Score. This technique ensures the pruned model retains a complementary set of experts, analogous to mosaic tiles that collectively preserve the original model's capabilities. Experiments demonstrate that MoP substantially outperforms previous pruning methods, with improvements of 7.24% on general tasks and 8.92% on specialized tasks such as math reasoning and code generation, highlighting its robustness and generalization across domains. <div>
arXiv:2511.19822v1 Announce Type: new 
Abstract: Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select" process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model's capabilities, enabling it to handle diverse downstream tasks.Extensive experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\% gain on general tasks and 8.92\% on specialized tasks like math reasoning and code generation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GED-Consistent Disentanglement of Aligned and Unaligned Substructures for Graph Similarity Learning</title>
<link>https://arxiv.org/abs/2511.19837</link>
<guid>https://arxiv.org/abs/2511.19837</guid>
<content:encoded><![CDATA[
<div> Graph Similarity, Graph Edit Distance, Graph Neural Networks, Substructure Matching, Optimal Alignment  

<br /><br />Summary:  
This paper addresses the challenge of Graph Similarity Computation (GSC), focusing on Graph Edit Distance (GED) as the main similarity metric. GED calculations require finding an optimal alignment between graph pairs, dividing them into aligned (zero-cost) and unaligned (cost-incurring) substructures. Given the NP-hardness of exact GED, recent methods approximate GED using Graph Neural Networks (GNNs) by learning node embeddings and aggregating pairwise node similarities. However, the authors identify fundamental mismatches in this node-centric approach that limit effectiveness: (1) inadequate capture of global structural correspondence essential for optimal alignment, and (2) incorrect assignment of edit costs influenced by spurious node-level signals. To overcome these, the paper proposes GCGSim, a novel framework emphasizing graph-level matching and substructure-level edit costs, which aligns better with GED principles. GCGSim introduces three key technical contributions (not detailed in the abstract) and demonstrates state-of-the-art performance on four benchmark graph datasets. Comprehensive experimental analysis shows that the framework successfully learns disentangled, semantically meaningful substructure representations, improving the interpretability and accuracy of graph similarity estimation in GED tasks. <div>
arXiv:2511.19837v1 Announce Type: new 
Abstract: Graph Similarity Computation (GSC) is a fundamental graph related task where Graph Edit Distance (GED) serves as a prevalent metric. GED is determined by an optimal alignment between a pair of graphs that partitions each into aligned (zero-cost) and unaligned (cost-incurring) substructures. Due to NP-hard nature of exact GED computation, GED approximations based on Graph Neural Network(GNN) have emerged. Existing GNN-based GED approaches typically learn node embeddings for each graph and then aggregate pairwise node similarities to estimate the final similarity. Despite their effectiveness, we identify a mismatch between this prevalent node-centric matching paradigm and the core principles of GED. This discrepancy leads to two critical limitations: (1) a failure to capture the global structural correspondence for optimal alignment, and (2) a misattribution of edit costs driven by spurious node level signals. To address these limitations, we propose GCGSim, a GED-consistent graph similarity learning framework centering on graph-level matching and substructure-level edit costs. Specifically, we make three core technical contributions. Extensive experiments on four benchmark datasets show that GCGSim achieves state-of-the-art performance. Our comprehensive analyses further validate that the framework effectively learns disentangled and semantically meaningful substructure representations.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cisco Time Series Model Technical Report</title>
<link>https://arxiv.org/abs/2511.19841</link>
<guid>https://arxiv.org/abs/2511.19841</guid>
<content:encoded><![CDATA[
<div> Keywords: Time Series Forecasting, Zero-Shot Learning, Multiresolution Input, Observability Data, Decoder-Only Model<br /><br />Summary:<br /><br />1. The paper introduces the Cisco Time Series Model, a novel univariate zero-shot forecasting model designed for time series data.<br />2. This model is built upon a general architectural innovation that allows it to accept multiresolution input, enhancing its ability to process data at different granularities.<br />3. The foundation model adapts a popular decoder-only time series architecture (TimesFM), integrating the multiresolution capability.<br />4. Training was conducted on an extensive dataset exceeding 300 billion unique data points, with a significant portion derived from observability domain data.<br />5. Evaluations, both quantitative and qualitative, reveal that the model excels on observability datasets while maintaining comparable performance on a widely recognized general forecasting benchmark (GIFT-Eval).<br />6. The multiresolution structure notably improves the model’s prediction accuracy on long context inputs, indicating benefits for processing extended sequence data.<br /><br />Overall, the Cisco Time Series Model represents a state-of-the-art approach in zero-shot time series forecasting, emphasizing multiresolution input processing and strong results across both specialized and general benchmarks. <div>
arXiv:2511.19841v1 Announce Type: new 
Abstract: We introduce the Cisco Time Series Model, a univariate zero-shot forecaster. This time series foundation model is the result of a general architectural innovation to a time series model enabling it to accept multiresolution input, applied to a popular decoder-only time series model (TimesFM). The resulting multiresolution decoder-only model is trained on over 300B unique data points, with more than half coming from the observability domain. Quantitative and qualitative evaluations demonstrate that the resulting model achieves superior performance on observability datasets while retaining very similar performance on a standard general-purpose forecasting benchmark (GIFT-Eval), and suggest that the multiresolution structure enables the model to make more accurate predictions on long context input.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SX-GeoTree: Self-eXplaining Geospatial Regression Tree Incorporating the Spatial Similarity of Feature Attributions</title>
<link>https://arxiv.org/abs/2511.19845</link>
<guid>https://arxiv.org/abs/2511.19845</guid>
<content:encoded><![CDATA[
<div> Keywords: decision trees, spatial dependence, explanation robustness, geospatial regression, modularity maximiziation  

<br /><br />Summary:  
This paper introduces SX-GeoTree, a novel self-explaining geospatial regression tree designed to address two major challenges of traditional decision trees in tabular prediction: capturing spatial dependence and generating locally stable, robust explanations. The method incorporates three coupled objectives during the recursive splitting process: minimizing prediction error (impurity reduction via MSE), controlling spatial residual patterns (using global Moran's I), and enhancing explanation robustness (through modularity maximization on a consensus similarity network). This network combines geographically weighted regression (GWR) coefficient distances representing stimulus-response similarity and SHAP attribution distances reflecting explanatory similarity. By reframing local Lipschitz continuity of feature attributions as a network community preservation problem, SX-GeoTree enforces spatially coherent explanations efficiently without relying on costly per-sample neighborhood searches. The model's effectiveness is demonstrated on two datasets: county-level GDP in Fujian and point-wise housing prices in Seattle. Results show it maintains predictive accuracy comparable to standard decision trees while significantly improving spatial residual uniformity and doubling attribution consensus (modularity). Ablation studies confirm that both Moran’s I for spatial residual control and modularity terms for explanation stability contribute complementarily. This approach advances trustworthy geospatial machine learning by embedding extended spatial similarity into interpretable models, providing a transferable framework for domain-aware explainability in spatial contexts. <div>
arXiv:2511.19845v1 Announce Type: new 
Abstract: Decision trees remain central for tabular prediction but struggle with (i) capturing spatial dependence and (ii) producing locally stable (robust) explanations. We present SX-GeoTree, a self-explaining geospatial regression tree that integrates three coupled objectives during recursive splitting: impurity reduction (MSE), spatial residual control (global Moran's I), and explanation robustness via modularity maximization on a consensus similarity network formed from (a) geographically weighted regression (GWR) coefficient distances (stimulus-response similarity) and (b) SHAP attribution distances (explanatory similarity). We recast local Lipschitz continuity of feature attributions as a network community preservation problem, enabling scalable enforcement of spatially coherent explanations without per-sample neighborhood searches. Experiments on two exemplar tasks (county-level GDP in Fujian, n=83; point-wise housing prices in Seattle, n=21,613) show SX-GeoTree maintains competitive predictive accuracy (within 0.01 $R^{2}$ of decision trees) while improving residual spatial evenness and doubling attribution consensus (modularity: Fujian 0.19 vs 0.09; Seattle 0.10 vs 0.05). Ablation confirms Moran's I and modularity terms are complementary; removing either degrades both spatial residual structure and explanation stability. The framework demonstrates how spatial similarity - extended beyond geometric proximity through GWR-derived local relationships - can be embedded in interpretable models, advancing trustworthy geospatial machine learning and offering a transferable template for domain-aware explainability.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization</title>
<link>https://arxiv.org/abs/2511.19851</link>
<guid>https://arxiv.org/abs/2511.19851</guid>
<content:encoded><![CDATA[
<div> Federated learning, Split learning, Hybrid learning, Convergence analysis, Delay minimization<br /><br />Summary: This paper investigates hybrid split and federated learning (HSFL), which combines federated learning (FL) and split learning (SL) to optimize distributed model training in wireless networks. FL enables parallel low-latency training but may yield less accurate models, whereas SL achieves higher accuracy through sequential training at the cost of increased delay. HSFL allows devices to operate in either FL or SL mode to balance these trade-offs. The study addresses three main questions: the impact of learning mode selection on overall performance, the interaction between learning mode and batch size, and joint optimization of these hyperparameters alongside communication and computational resources to minimize learning delay. The authors analyze convergence behavior, revealing how the choice of learning mode affects the relationship with batch size. They then formulate a delay minimization problem and propose a two-stage solution. The first stage employs block coordinate descent to find a locally optimal solution in a relaxed problem setting. The second stage applies a rounding algorithm to obtain integer batch sizes with near-optimal performance. Experimental evaluations show that the proposed approach significantly reduces the time required for the model to reach target accuracy compared with existing methods, demonstrating efficient acceleration of HSFL convergence. <div>
arXiv:2511.19851v1 Announce Type: new 
Abstract: Federated learning (FL) and split learning (SL) are two effective distributed learning paradigms in wireless networks, enabling collaborative model training across mobile devices without sharing raw data. While FL supports low-latency parallel training, it may converge to less accurate model. In contrast, SL achieves higher accuracy through sequential training but suffers from increased delay. To leverage the advantages of both, hybrid split and federated learning (HSFL) allows some devices to operate in FL mode and others in SL mode. This paper aims to accelerate HSFL by addressing three key questions: 1) How does learning mode selection affect overall learning performance? 2) How does it interact with batch size? 3) How can these hyperparameters be jointly optimized alongside communication and computational resources to reduce overall learning delay? We first analyze convergence, revealing the interplay between learning mode and batch size. Next, we formulate a delay minimization problem and propose a two-stage solution: a block coordinate descent method for a relaxed problem to obtain a locally optimal solution, followed by a rounding algorithm to recover integer batch sizes with near-optimal performance. Experimental results demonstrate that our approach significantly accelerates convergence to the target accuracy compared to existing methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frailty-Aware Transformer for Recurrent Survival Modeling of Driver Retention in Ride-Hailing Platforms</title>
<link>https://arxiv.org/abs/2511.19893</link>
<guid>https://arxiv.org/abs/2511.19893</guid>
<content:encoded><![CDATA[
arXiv:2511.19893v1 Announce Type: new 
Abstract: Ride-hailing platforms are characterized by high-frequency, behavior-driven environments. Although survival analysis has been applied to recurrent events in other domains, its use in modeling ride-hailing driver behavior remains largely unexplored. This study formulates idle behavior as a recurrent survival process using large-scale platform data and proposes a Transformer-based framework that captures long-term temporal dependencies with causal masking and incorporates driver-specific embeddings to model latent heterogeneity. Results on Toronto ride-hailing data demonstrate that the proposed Frailty-Aware Cox Transformer (FACT) achieves the highest time-dependent C-indices and lowest Brier Scores, outperforming classical and deep learning survival models. This approach enables more accurate risk estimation, supports platform retention strategies, and provides policy-relevant insights.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning</title>
<link>https://arxiv.org/abs/2511.19935</link>
<guid>https://arxiv.org/abs/2511.19935</guid>
<content:encoded><![CDATA[
arXiv:2511.19935v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptivity and Universality: Problem-dependent Universal Regret for Online Convex Optimization</title>
<link>https://arxiv.org/abs/2511.19937</link>
<guid>https://arxiv.org/abs/2511.19937</guid>
<content:encoded><![CDATA[
arXiv:2511.19937v1 Announce Type: new 
Abstract: Universal online learning aims to achieve optimal regret guarantees without requiring prior knowledge of the curvature of online functions. Existing methods have established minimax-optimal regret bounds for universal online learning, where a single algorithm can simultaneously attain $\mathcal{O}(\sqrt{T})$ regret for convex functions, $\mathcal{O}(d \log T)$ for exp-concave functions, and $\mathcal{O}(\log T)$ for strongly convex functions, where $T$ is the number of rounds and $d$ is the dimension of the feasible domain. However, these methods still lack problem-dependent adaptivity. In particular, no universal method provides regret bounds that scale with the gradient variation $V_T$, a key quantity that plays a crucial role in applications such as stochastic optimization and fast-rate convergence in games. In this work, we introduce UniGrad, a novel approach that achieves both universality and adaptivity, with two distinct realizations: UniGrad.Correct and UniGrad.Bregman. Both methods achieve universal regret guarantees that adapt to gradient variation, simultaneously attaining $\mathcal{O}(\log V_T)$ regret for strongly convex functions and $\mathcal{O}(d \log V_T)$ regret for exp-concave functions. For convex functions, the regret bounds differ: UniGrad.Correct achieves an $\mathcal{O}(\sqrt{V_T \log V_T})$ bound while preserving the RVU property that is crucial for fast convergence in online games, whereas UniGrad.Bregman achieves the optimal $\mathcal{O}(\sqrt{V_T})$ regret bound through a novel design. Both methods employ a meta algorithm with $\mathcal{O}(\log T)$ base learners, which naturally requires $\mathcal{O}(\log T)$ gradient queries per round. To enhance computational efficiency, we introduce UniGrad++, which retains the regret while reducing the gradient query to just $1$ per round via surrogate optimization. We further provide various implications.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.19941</link>
<guid>https://arxiv.org/abs/2511.19941</guid>
<content:encoded><![CDATA[
arXiv:2511.19941v1 Announce Type: new 
Abstract: Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning</title>
<link>https://arxiv.org/abs/2511.19942</link>
<guid>https://arxiv.org/abs/2511.19942</guid>
<content:encoded><![CDATA[
arXiv:2511.19942v1 Announce Type: new 
Abstract: It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\% improvements on AIME24 dataset.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Spatio-Temporal Attention Network with Adaptive Risk-Aware Decision for Forward Collision Warning in Complex Scenarios</title>
<link>https://arxiv.org/abs/2511.19952</link>
<guid>https://arxiv.org/abs/2511.19952</guid>
<content:encoded><![CDATA[
arXiv:2511.19952v1 Announce Type: new 
Abstract: Forward Collision Warning systems are crucial for vehicle safety and autonomous driving, yet current methods often fail to balance precise multi-agent interaction modeling with real-time decision adaptability, evidenced by the high computational cost for edge deployment and the unreliability stemming from simplified interaction models.To overcome these dual challenges-computational complexity and modeling insufficiency-along with the high false alarm rates of traditional static-threshold warnings, this paper introduces an integrated FCW framework that pairs a Hierarchical Spatio-Temporal Attention Network with a Dynamic Risk Threshold Adjustment algorithm. HSTAN employs a decoupled architecture (Graph Attention Network for spatial, cascaded GRU with self-attention for temporal) to achieve superior performance and efficiency, requiring only 12.3 ms inference time (73% faster than Transformer methods) and reducing the Average Displacement Error (ADE) to 0.73m (42.2% better than Social_LSTM) on the NGSIM dataset. Furthermore, Conformalized Quantile Regression enhances reliability by generating prediction intervals (91.3% coverage at 90% confidence), which the DTRA module then converts into timely warnings via a physics-informed risk potential function and an adaptive threshold mechanism inspired by statistical process control.Tested across multi-scenario datasets, the complete system demonstrates high efficacy, achieving an F1 score of 0.912, a low false alarm rate of 8.2%, and an ample warning lead time of 2.8 seconds, validating the framework's superior performance and practical deployment feasibility in complex environments.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Fairness: Sub-group Disparities in LLMs</title>
<link>https://arxiv.org/abs/2511.19956</link>
<guid>https://arxiv.org/abs/2511.19956</guid>
<content:encoded><![CDATA[
arXiv:2511.19956v1 Announce Type: new 
Abstract: Large Language Models (LLMs), though shown to be effective in many applications, can vary significantly in their response quality. In this paper, we investigate this problem of prompt fairness: specifically, the phrasing of a prompt by different users/styles, despite the same question being asked in principle, may elicit different responses from an LLM. To quantify this disparity, we propose to use information-theoretic metrics that can capture two dimensions of bias: subgroup sensitivity, the variability of responses within a subgroup and cross group consistency, the variability of responses across subgroups. Our analysis reveals that certain subgroups exhibit both higher internal variability and greater divergence from others. Our empirical analysis reveals that certain demographic sub groups experience both higher internal variability and greater divergence from others, indicating structural inequities in model behavior. To mitigate these disparities, we propose practical interventions, including majority voting across multiple generations and prompt neutralization, which together improve response stability and enhance fairness across user populations. In the experiments, we observe clear prompt sensitivity disparities across demographic subgroups: before mitigation, cross-group divergence values reach 0.28 and typically fall in the from 0.14 to 0.22 range. After applying our neutralization and multi generation strategy, these divergences consistently decrease, with the largest gap reduced to 0.22 and many distances falling to 0.17 or below, indicating more stable and consistent outputs across subgroups.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2511.19959</link>
<guid>https://arxiv.org/abs/2511.19959</guid>
<content:encoded><![CDATA[
arXiv:2511.19959v1 Announce Type: new 
Abstract: Federated learning (FL) has been extensively studied as a privacy-preserving training paradigm. Recently, federated block coordinate descent scheme has become a popular option in training large-scale models, as it allows clients to train only a subset of the model locally instead of the entire model. However, in the era of large language models (LLMs), even a single block can contain a significant number of parameters, posing substantial communication latency, particularly for resource-constrained clients. To address this challenge in federated training/fine-tuning LLMs, we propose ParaBlock, a novel approach that establishes two parallel threads for communication and computation to enhance communication efficiency. We theoretically prove that the proposed ParaBlock achieves the same convergence rate as the standard federated block coordinate descent methods. Empirical evaluations on fine-tuning LLMs on general instruction following and mathematical reasoning confirm that ParaBlock not only maintains strong performance but also significantly improves communication efficiency.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stragglers Can Contribute More: Uncertainty-Aware Distillation for Asynchronous Federated Learning</title>
<link>https://arxiv.org/abs/2511.19966</link>
<guid>https://arxiv.org/abs/2511.19966</guid>
<content:encoded><![CDATA[
arXiv:2511.19966v1 Announce Type: new 
Abstract: Asynchronous federated learning (FL) has recently gained attention for its enhanced efficiency and scalability, enabling local clients to send model updates to the server at their own pace without waiting for slower participants. However, such a design encounters significant challenges, such as the risk of outdated updates from straggler clients degrading the overall model performance and the potential bias introduced by faster clients dominating the learning process, especially under heterogeneous data distributions. Existing methods typically address only one of these issues, creating a conflict where mitigating the impact of outdated updates can exacerbate the bias created by faster clients, and vice versa. To address these challenges, we propose FedEcho, a novel framework that incorporates uncertainty-aware distillation to enhance the asynchronous FL performances under large asynchronous delays and data heterogeneity. Specifically, uncertainty-aware distillation enables the server to assess the reliability of predictions made by straggler clients, dynamically adjusting the influence of these predictions based on their estimated uncertainty. By prioritizing more certain predictions while still leveraging the diverse information from all clients, FedEcho effectively mitigates the negative impacts of outdated updates and data heterogeneity. Through extensive experiments, we demonstrate that FedEcho consistently outperforms existing asynchronous federated learning baselines, achieving robust performance without requiring access to private client data.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Semi-Supervised Node Classification with Self-Supervised Graph Clustering</title>
<link>https://arxiv.org/abs/2511.19976</link>
<guid>https://arxiv.org/abs/2511.19976</guid>
<content:encoded><![CDATA[
arXiv:2511.19976v1 Announce Type: new 
Abstract: The emergence of graph neural networks (GNNs) has offered a powerful tool for semi-supervised node classification tasks. Subsequent studies have achieved further improvements through refining the message passing schemes in GNN models or exploiting various data augmentation techniques to mitigate limited supervision. In real graphs, nodes often tend to form tightly-knit communities/clusters, which embody abundant signals for compensating label scarcity in semi-supervised node classification but are not explored in prior methods.
  Inspired by this, this paper presents NCGC that integrates self-supervised graph clustering and semi-supervised classification into a unified framework. Firstly, we theoretically unify the optimization objectives of GNNs and spectral graph clustering, and based on that, develop soft orthogonal GNNs (SOGNs) that leverage a refined message passing paradigm to generate node representations for both classification and clustering. On top of that, NCGC includes a self-supervised graph clustering module that enables the training of SOGNs for learning representations of unlabeled nodes in a self-supervised manner. Particularly, this component comprises two non-trivial clustering objectives and a Sinkhorn-Knopp normalization that transforms predicted cluster assignments into balanced soft pseudo-labels. Through combining the foregoing clustering module with the classification model using a multi-task objective containing the supervised classification loss on labeled data and self-supervised clustering loss on unlabeled data, NCGC promotes synergy between them and achieves enhanced model capacity. Our extensive experiments showcase that the proposed NCGC framework consistently and considerably outperforms popular GNN models and recent baselines for semi-supervised node classification on seven real graphs, when working with various classic GNN backbones.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Operator Learning at Machine Precision</title>
<link>https://arxiv.org/abs/2511.19980</link>
<guid>https://arxiv.org/abs/2511.19980</guid>
<content:encoded><![CDATA[
arXiv:2511.19980v1 Announce Type: new 
Abstract: Neural operator learning methods have garnered significant attention in scientific computing for their ability to approximate infinite-dimensional operators. However, increasing their complexity often fails to substantially improve their accuracy, leaving them on par with much simpler approaches such as kernel methods and more traditional reduced-order models. In this article, we set out to address this shortcoming and introduce CHONKNORIS (Cholesky Newton--Kantorovich Neural Operator Residual Iterative System), an operator learning paradigm that can achieve machine precision. CHONKNORIS draws on numerical analysis: many nonlinear forward and inverse PDE problems are solvable by Newton-type methods. Rather than regressing the solution operator itself, our method regresses the Cholesky factors of the elliptic operator associated with Tikhonov-regularized Newton--Kantorovich updates. The resulting unrolled iteration yields a neural architecture whose machine-precision behavior follows from achieving a contractive map, requiring far lower accuracy than end-to-end approximation of the solution operator. We benchmark CHONKNORIS on a range of nonlinear forward and inverse problems, including a nonlinear elliptic equation, Burgers' equation, a nonlinear Darcy flow problem, the Calder\'{o}n problem, an inverse wave scattering problem, and a problem from seismic imaging. We also present theoretical guarantees for the convergence of CHONKNORIS in terms of the accuracy of the emulated Cholesky factors. Additionally, we introduce a foundation model variant, FONKNORIS (Foundation Newton--Kantorovich Neural Operator Residual Iterative System), which aggregates multiple pre-trained CHONKNORIS experts for diverse PDEs to emulate the solution map of a novel nonlinear PDE. Our FONKNORIS model is able to accurately solve unseen nonlinear PDEs such as the Klein--Gordon and Sine--Gordon equations.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Message Passing Neural Networks with Diffusion Distance-guided Stress Majorization</title>
<link>https://arxiv.org/abs/2511.19984</link>
<guid>https://arxiv.org/abs/2511.19984</guid>
<content:encoded><![CDATA[
arXiv:2511.19984v1 Announce Type: new 
Abstract: Message passing neural networks (MPNNs) have emerged as go-to models for learning on graph-structured data in the past decade. Despite their effectiveness, most of such models still incur severe issues such as over-smoothing and -correlation, due to their underlying objective of minimizing the Dirichlet energy and the derived neighborhood aggregation operations. In this paper, we propose the DDSM, a new MPNN model built on an optimization framework that includes the stress majorization and orthogonal regularization for overcoming the above issues. Further, we introduce the diffusion distances for nodes into the framework to guide the new message passing operations and develop efficient algorithms for distance approximations, both backed by rigorous theoretical analyses. Our comprehensive experiments showcase that DDSM consistently and considerably outperforms 15 strong baselines on both homophilic and heterophilic graphs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices</title>
<link>https://arxiv.org/abs/2511.19986</link>
<guid>https://arxiv.org/abs/2511.19986</guid>
<content:encoded><![CDATA[
arXiv:2511.19986v1 Announce Type: new 
Abstract: Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RankOOD - Class Ranking-based Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2511.19996</link>
<guid>https://arxiv.org/abs/2511.19996</guid>
<content:encoded><![CDATA[
arXiv:2511.19996v1 Announce Type: new 
Abstract: We propose RankOOD, a rank-based Out-of-Distribution (OOD) detection approach based on training a model with the Placket-Luce loss, which is now extensively used for preference alignment tasks in foundational models. Our approach is based on the insight that with a deep learning model trained using the Cross Entropy Loss, in-distribution (ID) class prediction induces a ranking pattern for each ID class prediction. The RankOOD framework formalizes the insight by first extracting a rank list for each class using an initial classifier and then uses another round of training with the Plackett-Luce loss, where the class rank, a fixed permutation for each class, is the predicted variable. An OOD example may get assigned with high probability to an ID example, but the probability of it respecting the ranking classification is likely to be small. RankOOD, achieves SOTA performance on the near-ODD TinyImageNet evaluation benchmark, reducing FPR95 by 4.3%.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REWA: Witness-Overlap Theory -- Foundations for Composable Binary Similarity Systems</title>
<link>https://arxiv.org/abs/2511.19998</link>
<guid>https://arxiv.org/abs/2511.19998</guid>
<content:encoded><![CDATA[
arXiv:2511.19998v1 Announce Type: new 
Abstract: REWA introduces a general theory of similarity based on witness-overlap structures. We show that whenever similarity between concepts can be expressed as monotone witness overlap -- whether arising from graph neighborhoods, causal relations, temporal structure, topological features, symbolic patterns, or embedding-based neighborhoods -- it admits a reduction to compact encodings with provable ranking preservation guarantees. REWA systems consist of: (1) finite witness sets $W(v)$, (2) semi-random bit assignments generated from each witness, and (3) monotonicity of expected similarity in the overlap $\Delta(u, v) = |W(u) \cap W(v)|$. We prove that under an overlap-gap condition on the final witness sets -- independent of how they were constructed -- top-$k$ rankings are preserved using $m = O(\log(|V|/\delta))$ bits. The witness-set formulation is compositional: any sequence of structural, temporal, causal, topological, information-theoretic, or learned transformations can be combined into pipelines that terminate in discrete witness sets. The theory applies to the final witness overlap, enabling modular construction of similarity systems from reusable primitives. This yields a vast design space: millions of composable similarity definitions inherit logarithmic encoding complexity. REWA subsumes and unifies Bloom filters, minhash, LSH bitmaps, random projections, sketches, and hierarchical filters as special cases. It provides a principled foundation for similarity systems whose behavior is governed by witness overlap rather than hash-function engineering. This manuscript presents the axioms, the main reducibility theorem, complete proofs with explicit constants, and a detailed discussion of compositional design, limitations, and future extensions including multi-bit encodings, weighted witnesses, and non-set representations.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting</title>
<link>https://arxiv.org/abs/2511.20004</link>
<guid>https://arxiv.org/abs/2511.20004</guid>
<content:encoded><![CDATA[
arXiv:2511.20004v1 Announce Type: new 
Abstract: This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iRadioDiff: Physics-Informed Diffusion Model for Indoor Radio Map Construction and Localization</title>
<link>https://arxiv.org/abs/2511.20015</link>
<guid>https://arxiv.org/abs/2511.20015</guid>
<content:encoded><![CDATA[
arXiv:2511.20015v1 Announce Type: new 
Abstract: Radio maps (RMs) serve as environment-aware electromagnetic (EM) representations that connect scenario geometry and material properties to the spatial distribution of signal strength, enabling localization without costly in-situ measurements. However, constructing high-fidelity indoor RMs remains challenging due to the prohibitive latency of EM solvers and the limitations of learning-based methods, which often rely on sparse measurements or assumptions of homogeneous material, which are misaligned with the heterogeneous and multipath-rich nature of indoor environments. To overcome these challenges, we propose iRadioDiff, a sampling-free diffusion-based framework for indoor RM construction. iRadioDiff is conditioned on access point (AP) positions, and physics-informed prompt encoded by material reflection and transmission coefficients. It further incorporates multipath-critical priors, including diffraction points, strong transmission boundaries, and line-of-sight (LoS) contours, to guide the generative process via conditional channels and boundary-weighted objectives. This design enables accurate modeling of nonstationary field discontinuities and efficient construction of physically consistent RMs. Experiments demonstrate that iRadioDiff achieves state-of-the-art performance in indoor RM construction and received signal strength based indoor localization, which offers effective generalization across layouts and material configurations. Code is available at https://github.com/UNIC-Lab/iRadioDiff.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering</title>
<link>https://arxiv.org/abs/2511.20030</link>
<guid>https://arxiv.org/abs/2511.20030</guid>
<content:encoded><![CDATA[
arXiv:2511.20030v1 Announce Type: new 
Abstract: Multimodal Attributed Graphs (MMAGs) are an expressive data model for representing the complex interconnections among entities that associate attributes from multiple data modalities (text, images, etc.). Clustering over such data finds numerous practical applications in real scenarios, including social community detection, medical data analytics, etc. However, as revealed by our empirical studies, existing multi-view clustering solutions largely rely on the high correlation between attributes across various views and overlook the unique characteristics (e.g., low modality-wise correlation and intense feature-wise noise) of multimodal attributes output by large pre-trained language and vision models in MMAGs, leading to suboptimal clustering performance.
  Inspired by foregoing empirical observations and our theoretical analyses with graph signal processing, we propose the Dual Graph Filtering (DGF) scheme, which innovatively incorporates a feature-wise denoising component into node representation learning, thereby effectively overcoming the limitations of traditional graph filters adopted in the extant multi-view graph clustering approaches. On top of that, DGF includes a tri-cross contrastive training strategy that employs instance-level contrastive learning across modalities, neighborhoods, and communities for learning robust and discriminative node representations. Our comprehensive experiments on eight benchmark MMAG datasets exhibit that DGF is able to outperform a wide range of state-of-the-art baselines consistently and significantly in terms of clustering quality measured against ground-truth labels.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RED-F: Reconstruction-Elimination based Dual-stream Contrastive Forecasting for Multivariate Time Series Anomaly Prediction</title>
<link>https://arxiv.org/abs/2511.20044</link>
<guid>https://arxiv.org/abs/2511.20044</guid>
<content:encoded><![CDATA[
arXiv:2511.20044v1 Announce Type: new 
Abstract: The proactive prediction of anomalies (AP) in mul- tivariate time series (MTS) is a critical challenge to ensure system dependability. The difficulty lies in identifying subtle anomaly precursors concealed within normal signals. However, existing unsupervised methods, trained exclusively on normal data, demonstrate a fundamental propensity to reconstruct normal patterns. Consequently, when confronted with weak precursors, their predictions are dominated by the normal pattern, submerging the very signal required for prediction. To contend with the limitation, we propose RED-F, a Reconstruction- Elimination based Dual-stream Contrastive Forecasting frame- work, comprising the Reconstruction-Elimination Model (REM) and the Dual-stream Contrastive Forecasting Model (DFM). The REM utilizes a hybrid time-frequency mechanism to mitigate the precursor, generating a purified, normal-pattern baseline. The DFM then receives this purified baseline and the original sequence which retains the precursor as parallel inputs. At the core of our framework, RED-F employs a contrastive forecast that transforms the difficult task of absolute signal detection into a simpler, more robust task of relative trajectory comparison by computing the divergence between these two predictive streams. This contrastive mechanism serves to amplify the faint precursor signal. Furthermore, the DFM is trained with a novel Multi-Series Prediction (MSP) objective, which leverages distant future con- text to enhance its predictive sensitivity. Extensive experiments on six real-world datasets demonstrate the superior capability of RED-F in anomaly prediction tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOMBRL: Scalable and Optimistic Model-Based RL</title>
<link>https://arxiv.org/abs/2511.20066</link>
<guid>https://arxiv.org/abs/2511.20066</guid>
<content:encoded><![CDATA[
arXiv:2511.20066v1 Announce Type: new 
Abstract: We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiMeng-CRUX: Narrowing the Gap between Natural Language and Verilog via Core Refined Understanding eXpression</title>
<link>https://arxiv.org/abs/2511.20099</link>
<guid>https://arxiv.org/abs/2511.20099</guid>
<content:encoded><![CDATA[
arXiv:2511.20099v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promising capabilities in hardware description language (HDL) generation. However, existing approaches often rely on free-form natural language descriptions that are often ambiguous, redundant, and unstructured, which poses significant challenges for downstream Verilog code generation. We treat hardware code generation as a complex transformation from an open-ended natural language space to a domain-specific, highly constrained target space. To bridge this gap, we introduce Core Refined Understanding eXpression (CRUX), a structured intermediate space that captures the essential semantics of user intent while organizing the expression for precise Verilog code generation. We further design a two-stage training framework, comprising Joint Expression Modeling and Dual-Space Optimization, to enhance the quality of both CRUX and Verilog code. Experiments across multiple Verilog generation benchmarks demonstrate that our model, CRUX-V, achieves state-of-the-art performance among general models, particularly under challenging design tasks. Furthermore, the CRUX space proves transferable and beneficial when used as input prompts for other code models, highlighting its effectiveness in narrowing the gap between free-form natural language descriptions and precise Verilog generation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs</title>
<link>https://arxiv.org/abs/2511.20104</link>
<guid>https://arxiv.org/abs/2511.20104</guid>
<content:encoded><![CDATA[
arXiv:2511.20104v1 Announce Type: new 
Abstract: Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed "emergent misalignment" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales.
  We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%.
  We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's 'degrees of freedom' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate Forecasting of Bitcoin Volatility with Gradient Boosting: Deterministic, Probabilistic, and Feature Importance Perspectives</title>
<link>https://arxiv.org/abs/2511.20105</link>
<guid>https://arxiv.org/abs/2511.20105</guid>
<content:encoded><![CDATA[
arXiv:2511.20105v1 Announce Type: new 
Abstract: This study investigates the application of the Light Gradient Boosting Machine (LGBM) model for both deterministic and probabilistic forecasting of Bitcoin realized volatility. Utilizing a comprehensive set of 69 predictors -- encompassing market, behavioral, and macroeconomic indicators -- we evaluate the performance of LGBM-based models and compare them with both econometric and machine learning baselines. For probabilistic forecasting, we explore two quantile-based approaches: direct quantile regression using the pinball loss function, and a residual simulation method that transforms point forecasts into predictive distributions. To identify the main drivers of volatility, we employ gain-based and permutation feature importance techniques, consistently highlighting the significance of trading volume, lagged volatility measures, investor attention, and market capitalization. The results demonstrate that LGBM models effectively capture the nonlinear and high-variance characteristics of cryptocurrency markets while providing interpretable insights into the underlying volatility dynamics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIMATEAGENT: Multi-Agent Orchestration for Complex Climate Data Science Workflows</title>
<link>https://arxiv.org/abs/2511.20109</link>
<guid>https://arxiv.org/abs/2511.20109</guid>
<content:encoded><![CDATA[
arXiv:2511.20109v1 Announce Type: new 
Abstract: Climate science demands automated workflows to transform comprehensive questions into data-driven statements across massive, heterogeneous datasets. However, generic LLM agents and static scripting pipelines lack climate-specific context and flexibility, thus, perform poorly in practice. We present ClimateAgent, an autonomous multi-agent framework that orchestrates end-to-end climate data analytic workflows. ClimateAgent decomposes user questions into executable sub-tasks coordinated by an Orchestrate-Agent and a Plan-Agent; acquires data via specialized Data-Agents that dynamically introspect APIs to synthesize robust download scripts; and completes analysis and reporting with a Coding-Agent that generates Python code, visualizations, and a final report with a built-in self-correction loop. To enable systematic evaluation, we introduce Climate-Agent-Bench-85, a benchmark of 85 real-world tasks spanning atmospheric rivers, drought, extreme precipitation, heat waves, sea surface temperature, and tropical cyclones. On Climate-Agent-Bench-85, ClimateAgent achieves 100% task completion and a report quality score of 8.32, outperforming GitHub-Copilot (6.27) and a GPT-5 baseline (3.26). These results demonstrate that our multi-agent orchestration with dynamic API awareness and self-correcting execution substantially advances reliable, end-to-end automation for climate science analytic tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IDAP++: Advancing Divergence-Based Pruning via Filter-Level and Layer-Level Optimization</title>
<link>https://arxiv.org/abs/2511.20141</link>
<guid>https://arxiv.org/abs/2511.20141</guid>
<content:encoded><![CDATA[
arXiv:2511.20141v1 Announce Type: new 
Abstract: This paper presents a novel approach to neural network compression that addresses redundancy at both the filter and architectural levels through a unified framework grounded in information flow analysis. Building on the concept of tensor flow divergence, which quantifies how information is transformed across network layers, we develop a two-stage optimization process. The first stage employs iterative divergence-aware pruning to identify and remove redundant filters while preserving critical information pathways. The second stage extends this principle to higher-level architecture optimization by analyzing layer-wise contributions to information propagation and selectively eliminating entire layers that demonstrate minimal impact on network performance. The proposed method naturally adapts to diverse architectures, including convolutional networks, transformers, and hybrid designs, providing a consistent metric for comparing the structural importance across different layer types. Experimental validation across multiple modern architectures and datasets reveals that this combined approach achieves substantial model compression while maintaining competitive accuracy. The presented approach achieves parameter reduction results that are globally comparable to those of state-of-the-art solutions and outperforms them across a wide range of modern neural network architectures, from convolutional models to transformers. The results demonstrate how flow divergence serves as an effective guiding principle for both filter-level and layer-level optimization, offering practical benefits for deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Limits of Momentum in Decentralized and Federated Optimization</title>
<link>https://arxiv.org/abs/2511.20168</link>
<guid>https://arxiv.org/abs/2511.20168</guid>
<content:encoded><![CDATA[
arXiv:2511.20168v1 Announce Type: new 
Abstract: Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $\Theta\left(1/t\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaCap: An Adaptive Contrastive Approach for Small-Data Neural Networks</title>
<link>https://arxiv.org/abs/2511.20170</link>
<guid>https://arxiv.org/abs/2511.20170</guid>
<content:encoded><![CDATA[
arXiv:2511.20170v1 Announce Type: new 
Abstract: Neural networks struggle on small tabular datasets, where tree-based models remain dominant. We introduce Adaptive Contrastive Approach (AdaCap), a training scheme that combines a permutation-based contrastive loss with a Tikhonov-based closed-form output mapping. Across 85 real-world regression datasets and multiple architectures, AdaCap yields consistent and statistically significant improvements in the small-sample regime, particularly for residual models. A meta-predictor trained on dataset characteristics (size, skewness, noise) accurately anticipates when AdaCap is beneficial. These results show that AdaCap acts as a targeted regularization mechanism, strengthening neural networks precisely where they are most fragile. All results and code are publicly available at https://github.com/BrunoBelucci/adacap.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Subgroups with Maximum Treatment Effects without Causal Heuristics</title>
<link>https://arxiv.org/abs/2511.20189</link>
<guid>https://arxiv.org/abs/2511.20189</guid>
<content:encoded><![CDATA[
arXiv:2511.20189v1 Announce Type: new 
Abstract: Discovering subgroups with the maximum average treatment effect is crucial for targeted decision making in domains such as precision medicine, public policy, and education. While most prior work is formulated in the potential outcome framework, the corresponding structural causal model (SCM) for this task has been largely overlooked. In practice, two approaches dominate. The first estimates pointwise conditional treatment effects and then fits a tree on those estimates, effectively turning subgroup estimation into the harder problem of accurate pointwise estimation. The second constructs decision trees or rule sets with ad-hoc 'causal' heuristics, typically without rigorous justification for why a given heuristic may be used or whether such heuristics are necessary at all. We address these issues by studying the problem directly under the SCM framework. Under the assumption of a partition-based model, we show that optimal subgroup discovery reduces to recovering the data-generating models and hence a standard supervised learning problem (regression or classification). This allows us to adopt any partition-based methods to learn the subgroup from data. We instantiate the approach with CART, arguably one of the most widely used tree-based methods, to learn the subgroup with maximum treatment effect. Finally, on a large collection of synthetic and semi-synthetic datasets, we compare our method against a wide range of baselines and find that our approach, which avoids such causal heuristics, more accurately identifies subgroups with maximum treatment effect. Our source code is available at https://github.com/ylincen/causal-subgroup.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Compositional Learning via Sparse Coding Transformer</title>
<link>https://arxiv.org/abs/2511.20194</link>
<guid>https://arxiv.org/abs/2511.20194</guid>
<content:encoded><![CDATA[
arXiv:2511.20194v1 Announce Type: new 
Abstract: Transformer architectures have achieved remarkable success across language, vision, and multimodal tasks, and there is growing demand for them to address in-context compositional learning tasks. In these tasks, models solve the target problems by inferring compositional rules from context examples, which are composed of basic components structured by underlying rules. However, some of these tasks remain challenging for Transformers, which are not inherently designed to handle compositional tasks and offer limited structural inductive bias. In this work, inspired by the principle of sparse coding, we propose a reformulation of the attention to enhance its capability for compositional tasks. In sparse coding, data are represented as sparse combinations of dictionary atoms with coefficients that capture their compositional rules. Specifically, we reinterpret the attention block as a mapping of inputs into outputs through projections onto two sets of learned dictionary atoms: an encoding dictionary and a decoding dictionary. The encoding dictionary decomposes the input into a set of coefficients, which represent the compositional structure of the input. To enhance structured representations, we impose sparsity on these coefficients. The sparse coefficients are then used to linearly combine the decoding dictionary atoms to generate the output. Furthermore, to assist compositional generalization tasks, we propose estimating the coefficients of the target problem as a linear combination of the coefficients obtained from the context examples. We demonstrate the effectiveness of our approach on the S-RAVEN and RAVEN datasets. For certain compositional generalization tasks, our method maintains performance even when standard Transformers fail, owing to its ability to learn and apply compositional rules.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communication-Efficient Learning for Satellite Constellations</title>
<link>https://arxiv.org/abs/2511.20220</link>
<guid>https://arxiv.org/abs/2511.20220</guid>
<content:encoded><![CDATA[
arXiv:2511.20220v1 Announce Type: new 
Abstract: Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling and Damping: Structurally-Regularized Gradient Matching for Multimodal Graph Condensation</title>
<link>https://arxiv.org/abs/2511.20222</link>
<guid>https://arxiv.org/abs/2511.20222</guid>
<content:encoded><![CDATA[
arXiv:2511.20222v1 Announce Type: new 
Abstract: In critical web applications such as e-commerce and recommendation systems, multimodal graphs integrating rich visual and textual attributes are increasingly central, yet their large scale introduces substantial computational burdens for training Graph Neural Networks (GNNs). While Graph Condensation (GC) offers a promising solution by synthesizing smaller datasets, existing methods falter in the multimodal setting. We identify a dual challenge causing this failure: (1) conflicting gradients arising from semantic misalignments between modalities, and (2) the GNN's message-passing architecture pathologically amplifying this gradient noise across the graph structure. To address this, we propose Structurally-Regularized Gradient Matching (SR-GM), a novel condensation framework tailored for multimodal graphs. SR-GM introduces two synergistic components: first, a gradient decoupling mechanism that resolves inter-modality conflicts at their source via orthogonal projection; and second, a structural damping regularizer that acts directly on the gradient field. By leveraging the graph's Dirichlet energy, this regularizer transforms the topology from a noise amplifier into a stabilizing force during optimization. Extensive experiments demonstrate that SR-GM significantly improves accuracy and accelerates convergence compared to baseline methods. Ablation studies confirm that addressing both gradient conflict and structural amplification in tandem is essential for achieving superior performance. Moreover, the condensed multimodal graphs exhibit strong cross-architecture generalization and promise to accelerate applications like Neural Architecture Search. This research provides a scalable methodology for multimodal graph-based learning in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiCaP: Distribution-Calibrated Pseudo-labeling for Semi-Supervised Multi-Label Learning</title>
<link>https://arxiv.org/abs/2511.20225</link>
<guid>https://arxiv.org/abs/2511.20225</guid>
<content:encoded><![CDATA[
arXiv:2511.20225v1 Announce Type: new 
Abstract: Semi-supervised multi-label learning (SSMLL) aims to address the challenge of limited labeled data in multi-label learning (MLL) by leveraging unlabeled data to improve the model's performance. While pseudo-labeling has become a dominant strategy in SSMLL, most existing methods assign equal weights to all pseudo-labels regardless of their quality, which can amplify the impact of noisy or uncertain predictions and degrade the overall performance. In this paper, we theoretically verify that the optimal weight for a pseudo-label should reflect its correctness likelihood. Empirically, we observe that on the same dataset, the correctness likelihood distribution of unlabeled data remains stable, even as the number of labeled training samples varies. Building on this insight, we propose Distribution-Calibrated Pseudo-labeling (DiCaP), a correctness-aware framework that estimates posterior precision to calibrate pseudo-label weights. We further introduce a dual-thresholding mechanism to separate confident and ambiguous regions: confident samples are pseudo-labeled and weighted accordingly, while ambiguous ones are explored by unsupervised contrastive learning. Experiments conducted on multiple benchmark datasets verify that our method achieves consistent improvements, surpassing state-of-the-art methods by up to 4.27%.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging weights signals - Predicting and improving generalizability in reinforcement learning</title>
<link>https://arxiv.org/abs/2511.20234</link>
<guid>https://arxiv.org/abs/2511.20234</guid>
<content:encoded><![CDATA[
arXiv:2511.20234v1 Announce Type: new 
Abstract: Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling</title>
<link>https://arxiv.org/abs/2511.20257</link>
<guid>https://arxiv.org/abs/2511.20257</guid>
<content:encoded><![CDATA[
arXiv:2511.20257v1 Announce Type: new 
Abstract: Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits</title>
<link>https://arxiv.org/abs/2511.20273</link>
<guid>https://arxiv.org/abs/2511.20273</guid>
<content:encoded><![CDATA[
arXiv:2511.20273v1 Announce Type: new 
Abstract: Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HVAdam: A Full-Dimension Adaptive Optimizer</title>
<link>https://arxiv.org/abs/2511.20277</link>
<guid>https://arxiv.org/abs/2511.20277</guid>
<content:encoded><![CDATA[
arXiv:2511.20277v1 Announce Type: new 
Abstract: Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry of Decision Making in Language Models</title>
<link>https://arxiv.org/abs/2511.20315</link>
<guid>https://arxiv.org/abs/2511.20315</guid>
<content:encoded><![CDATA[
arXiv:2511.20315v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MXtalTools: A Toolkit for Machine Learning on Molecular Crystals</title>
<link>https://arxiv.org/abs/2511.20327</link>
<guid>https://arxiv.org/abs/2511.20327</guid>
<content:encoded><![CDATA[
arXiv:2511.20327v1 Announce Type: new 
Abstract: We present MXtalTools, a flexible Python package for the data-driven modelling of molecular crystals, facilitating machine learning studies of the molecular solid state. MXtalTools comprises several classes of utilities: (1) synthesis, collation, and curation of molecule and crystal datasets, (2) integrated workflows for model training and inference, (3) crystal parameterization and representation, (4) crystal structure sampling and optimization, (5) end-to-end differentiable crystal sampling, construction and analysis. Our modular functions can be integrated into existing workflows or combined and used to build novel modelling pipelines. MXtalTools leverages CUDA acceleration to enable high-throughput crystal modelling. The Python code is available open-source on our GitHub page, with detailed documentation on ReadTheDocs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Adaptive Policy Optimization</title>
<link>https://arxiv.org/abs/2511.20347</link>
<guid>https://arxiv.org/abs/2511.20347</guid>
<content:encoded><![CDATA[
arXiv:2511.20347v1 Announce Type: new 
Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning</title>
<link>https://arxiv.org/abs/2511.20349</link>
<guid>https://arxiv.org/abs/2511.20349</guid>
<content:encoded><![CDATA[
arXiv:2511.20349v1 Announce Type: new 
Abstract: In this paper, a complexity study is conducted for Versatile Video Codec (VVC) intra partitioning to accelerate the exhaustive search involved in Rate-Distortion Optimization (RDO) process. To address this problem, two main machine learning techniques are proposed and compared. Unlike existing methods, the proposed approaches are size independent and incorporate the Rate-Distortion (RD) costs of neighboring blocks as input features. The first method is a regression based technique that predicts normalized RD costs of a given Coding Unit (CU). As partitioning possesses the Markov property, the associated decision-making problem can be modeled as a Markov Decision Process (MDP) and solved by Reinforcement Learning (RL). The second approach is a RL agent learned from trajectories of CU decision across two depths with Deep Q-Network (DQN) algorithm. Then a pre-determined thresholds are applied for both methods to select a suitable split for the current CU.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM: Periodic Representation with multIscale and Similarity graph Modelling for enhanced crystal structure property prediction</title>
<link>https://arxiv.org/abs/2511.20362</link>
<guid>https://arxiv.org/abs/2511.20362</guid>
<content:encoded><![CDATA[
arXiv:2511.20362v1 Announce Type: new 
Abstract: Crystal structures are characterised by repeating atomic patterns within unit cells across three-dimensional space, posing unique challenges for graph-based representation learning. Current methods often overlook essential periodic boundary conditions and multiscale interactions inherent to crystalline structures. In this paper, we introduce PRISM, a graph neural network framework that explicitly integrates multiscale representations and periodic feature encoding by employing a set of expert modules, each specialised in encoding distinct structural and chemical aspects of periodic systems. Extensive experiments across crystal structure-based benchmarks demonstrate that PRISM improves state-of-the-art predictive accuracy, significantly enhancing crystal property prediction.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers</title>
<link>https://arxiv.org/abs/2511.20382</link>
<guid>https://arxiv.org/abs/2511.20382</guid>
<content:encoded><![CDATA[
arXiv:2511.20382v1 Announce Type: new 
Abstract: Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with scArches, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI</title>
<link>https://arxiv.org/abs/2511.20395</link>
<guid>https://arxiv.org/abs/2511.20395</guid>
<content:encoded><![CDATA[
arXiv:2511.20395v1 Announce Type: new 
Abstract: Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.
  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.
  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.
  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Based Learning of Whittle indices</title>
<link>https://arxiv.org/abs/2511.20397</link>
<guid>https://arxiv.org/abs/2511.20397</guid>
<content:encoded><![CDATA[
arXiv:2511.20397v1 Announce Type: new 
Abstract: We present BLINQ, a new model-based algorithm that learns the Whittle indices of an indexable, communicating and unichain Markov Decision Process (MDP). Our approach relies on building an empirical estimate of the MDP and then computing its Whittle indices using an extended version of a state-of-the-art existing algorithm. We provide a proof of convergence to the Whittle indices we want to learn as well as a bound on the time needed to learn them with arbitrary precision. Moreover, we investigate its computational complexity. Our numerical experiments suggest that BLINQ significantly outperforms existing Q-learning approaches in terms of the number of samples needed to get an accurate approximation. In addition, it has a total computational cost even lower than Q-learning for any reasonably high number of samples. These observations persist even when the Q-learning algorithms are speeded up using pre-trained neural networks to predict Q-values.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Short-Range Oversquashing</title>
<link>https://arxiv.org/abs/2511.20406</link>
<guid>https://arxiv.org/abs/2511.20406</guid>
<content:encoded><![CDATA[
arXiv:2511.20406v1 Announce Type: new 
Abstract: Message Passing Neural Networks (MPNNs) are widely used for learning on graphs, but their ability to process long-range information is limited by the phenomenon of oversquashing. This limitation has led some researchers to advocate Graph Transformers as a better alternative, whereas others suggest that it can be mitigated within the MPNN framework, using virtual nodes or other rewiring techniques.
  In this work, we demonstrate that oversquashing is not limited to long-range tasks, but can also arise in short-range problems. This observation allows us to disentangle two distinct mechanisms underlying oversquashing: (1) the bottleneck phenomenon, which can arise even in low-range settings, and (2) the vanishing gradient phenomenon, which is closely associated with long-range tasks.
  We further show that the short-range bottleneck effect is not captured by existing explanations for oversquashing, and that adding virtual nodes does not resolve it. In contrast, transformers do succeed in such tasks, positioning them as the more compelling solution to oversquashing, compared to specialized MPNNs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tight Margin-Based Generalization Bounds for Voting Classifiers over Finite Hypothesis Sets</title>
<link>https://arxiv.org/abs/2511.20407</link>
<guid>https://arxiv.org/abs/2511.20407</guid>
<content:encoded><![CDATA[
arXiv:2511.20407v1 Announce Type: new 
Abstract: We prove the first margin-based generalization bound for voting classifiers, that is asymptotically tight in the tradeoff between the size of the hypothesis set, the margin, the fraction of training points with the given margin, the number of training samples and the failure probability.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion for Fusion: Designing Stellarators with Generative AI</title>
<link>https://arxiv.org/abs/2511.20445</link>
<guid>https://arxiv.org/abs/2511.20445</guid>
<content:encoded><![CDATA[
arXiv:2511.20445v1 Announce Type: new 
Abstract: Stellarators are a prospective class of fusion-based power plants that confine a hot plasma with three-dimensional magnetic fields. Typically framed as a PDE-constrained optimization problem, stellarator design is a time-consuming process that can take hours to solve on a computing cluster. Developing fast methods for designing stellarators is crucial for advancing fusion research. Given the recent development of large datasets of optimized stellarators, machine learning approaches have emerged as a potential candidate. Motivated by this, we present an open inverse problem to the machine learning community: to rapidly generate high-quality stellarator designs which have a set of desirable characteristics. As a case study in the problem space, we train a conditional diffusion model on data from the QUASR database to generate quasisymmetric stellarator designs with desirable characteristics (aspect ratio and mean rotational transform). The diffusion model is applied to design stellarators with characteristics not seen during training. We provide evaluation protocols and show that many of the generated stellarators exhibit solid performance: less than 5% deviation from quasisymmetry and the target characteristics. The modest deviation from quasisymmetry highlights an opportunity to reach the sub 1% target. Beyond the case study, we share multiple promising avenues for generative modeling to advance stellarator design.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Trustworthy Wi-Fi Sensing: Systematic Evaluation of Deep Learning Model Robustness to Adversarial Attacks</title>
<link>https://arxiv.org/abs/2511.20456</link>
<guid>https://arxiv.org/abs/2511.20456</guid>
<content:encoded><![CDATA[
arXiv:2511.20456v1 Announce Type: new 
Abstract: Machine learning has become integral to Channel State Information (CSI)-based human sensing systems and is expected to power applications such as device-free activity recognition and identity detection in future cellular and Wi-Fi generations. However, these systems rely on models whose decisions can be subtly perturbed, raising concerns for security and reliability in ubiquitous sensing. Quantifying and understanding the robustness of such models, defined as their ability to maintain accurate predictions under adversarial perturbations, is therefore critical before wireless sensing can be safely deployed in real-world environments.
  This work presents a systematic evaluation of the robustness of CSI deep learning models under diverse threat models (white-box, black-box/transfer, and universal perturbations) and varying degrees of attack realism. We establish a framework to compare compact temporal autoencoder models with larger deep architectures across three public datasets, quantifying how model scale, training regime, and physical constraints influence robustness. Our experiments show that smaller models, while efficient and equally performant on clean data, are markedly less robust. We further confirm that physically realizable signal-space perturbations, designed to be feasible in real wireless channels, significantly reduce attack success compared to unconstrained feature-space attacks. Adversarial training mitigates these vulnerabilities, improving mean robust accuracy with only moderate degradation in clean performance across both model classes. As wireless sensing advances towards reliable, cross-domain operation, these findings provide quantitative baselines for robustness estimation and inform design principles for secure and trustworthy human-centered sensing systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NVIDIA Nemotron Parse 1.1</title>
<link>https://arxiv.org/abs/2511.20478</link>
<guid>https://arxiv.org/abs/2511.20478</guid>
<content:encoded><![CDATA[
arXiv:2511.20478v1 Announce Type: new 
Abstract: We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ranking-Enhanced Anomaly Detection Using Active Learning-Assisted Attention Adversarial Dual AutoEncoders</title>
<link>https://arxiv.org/abs/2511.20480</link>
<guid>https://arxiv.org/abs/2511.20480</guid>
<content:encoded><![CDATA[
arXiv:2511.20480v1 Announce Type: new 
Abstract: Advanced Persistent Threats (APTs) pose a significant challenge in cybersecurity due to their stealthy and long-term nature. Modern supervised learning methods require extensive labeled data, which is often scarce in real-world cybersecurity environments. In this paper, we propose an innovative approach that leverages AutoEncoders for unsupervised anomaly detection, augmented by active learning to iteratively improve the detection of APT anomalies. By selectively querying an oracle for labels on uncertain or ambiguous samples, we minimize labeling costs while improving detection rates, enabling the model to improve its detection accuracy with minimal data while reducing the need for extensive manual labeling. We provide a detailed formulation of the proposed Attention Adversarial Dual AutoEncoder-based anomaly detection framework and show how the active learning loop iteratively enhances the model. The framework is evaluated on real-world imbalanced provenance trace databases produced by the DARPA Transparent Computing program, where APT-like attacks constitute as little as 0.004\% of the data. The datasets span multiple operating systems, including Android, Linux, BSD, and Windows, and cover two attack scenarios. The results have shown significant improvements in detection rates during active learning and better performance compared to other existing approaches.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology</title>
<link>https://arxiv.org/abs/2511.20490</link>
<guid>https://arxiv.org/abs/2511.20490</guid>
<content:encoded><![CDATA[
arXiv:2511.20490v1 Announce Type: new 
Abstract: Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection</title>
<link>https://arxiv.org/abs/2511.20500</link>
<guid>https://arxiv.org/abs/2511.20500</guid>
<content:encoded><![CDATA[
arXiv:2511.20500v1 Announce Type: new 
Abstract: Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning</title>
<link>https://arxiv.org/abs/2511.20509</link>
<guid>https://arxiv.org/abs/2511.20509</guid>
<content:encoded><![CDATA[
arXiv:2511.20509v1 Announce Type: new 
Abstract: Adaptive optimizers are the de facto standard in non-private training as they often enable faster convergence and improved performance. In contrast, differentially private (DP) training is still predominantly performed with DP-SGD, typically requiring extensive compute and hyperparameter tuning. We propose DP-MicroAdam, a memory-efficient and sparsity-aware adaptive DP optimizer. We prove that DP-MicroAdam converges in stochastic non-convex optimization at the optimal $\mathcal{O}(1/\sqrt{T})$ rate, up to privacy-dependent constants. Empirically, DP-MicroAdam outperforms existing adaptive DP optimizers and achieves competitive or superior accuracy compared to DP-SGD across a range of benchmarks, including CIFAR-10, large-scale ImageNet training, and private fine-tuning of pretrained transformers. These results demonstrate that adaptive optimization can improve both performance and stability under differential privacy.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adam Simplified: Bias Correction Simplified</title>
<link>https://arxiv.org/abs/2511.20516</link>
<guid>https://arxiv.org/abs/2511.20516</guid>
<content:encoded><![CDATA[
arXiv:2511.20516v1 Announce Type: new 
Abstract: The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $\beta_1, \beta_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media</title>
<link>https://arxiv.org/abs/2511.20543</link>
<guid>https://arxiv.org/abs/2511.20543</guid>
<content:encoded><![CDATA[
arXiv:2511.20543v1 Announce Type: new 
Abstract: The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems</title>
<link>https://arxiv.org/abs/2511.20564</link>
<guid>https://arxiv.org/abs/2511.20564</guid>
<content:encoded><![CDATA[
arXiv:2511.20564v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-trained offline to generate node embeddings, which are then used as static features for downstream recommender systems. This decoupled paradigm leads to two key limitations: (1) high computational overhead, since large-scale GNN inference must be repeatedly executed to refresh embeddings; and (2) lack of joint optimization, as the gradient from the recommender system cannot directly influence the GNN learning process, causing the GNN to be suboptimally informative for the recommendation task. In this paper, we propose E2E-GRec, a novel end-to-end training framework that unifies GNN training with the recommender system. Our framework is characterized by three key components: (i) efficient subgraph sampling from a large-scale cross-domain heterogeneous graph to ensure training scalability and efficiency; (ii) a Graph Feature Auto-Encoder (GFAE) serving as an auxiliary self-supervised task to guide the GNN to learn structurally meaningful embeddings; and (iii) a two-level feature fusion mechanism combined with Gradnorm-based dynamic loss balancing, which stabilizes graph-aware multi-task end-to-end training. Extensive offline evaluations, online A/B tests (e.g., a +0.133% relative improvement in stay duration, a 0.3171% reduction in the average number of videos a user skips) on large-scale production data, together with theoretical analysis, demonstrate that E2E-GRec consistently surpasses traditional approaches, yielding significant gains across multiple recommendation metrics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSTN: Fast and Efficient Multivariate Time Series Model</title>
<link>https://arxiv.org/abs/2511.20577</link>
<guid>https://arxiv.org/abs/2511.20577</guid>
<content:encoded><![CDATA[
arXiv:2511.20577v1 Announce Type: new 
Abstract: Real-world time-series data is highly non stationary and complex in dynamics that operate across multiple timescales, ranging from fast, short-term changes to slow, long-term trends. Most existing models rely on fixed-scale structural priors, such as patch-based tokenization, fixed frequency transformations, or frozen backbone architectures. This often leads to over-regularization of temporal dynamics, which limits their ability to adaptively model the full spectrum of temporal variations and impairs their performance on unpredictable, Sudden, high-magnitude events. To address this, we introduce the Multi-scale Temporal Network (MSTN), a novel deep learning architecture founded on a hierarchical multi-scale and sequence modeling principle. The MSTN framework integrates: (i) a multi-scale convolutional encoder that constructs a hierarchical feature pyramid for local patterns (ii) a sequence modeling component for long-range temporal dependencies. We empirically validate this with BiLSTM and Transformer variants, establishing a flexible foundation for future architectural advancements. and (iii) a gated fusion mechanism augmented with squeeze-and-excitation (SE) and multi-head temporal attention (MHTA) for dynamic, context-aware feature integration. This design enables MSTN to adaptively model temporal patterns from milliseconds to long-range dependencies within a unified framework. Extensive evaluations across time-series long-horizon forecasting, imputation, classification and generalizability study demonstrate that MSTN achieves competitive state-of-the-art (SOTA) performance, showing improvements over contemporary approaches including EMTSF, LLM4TS, HiMTM, TIME-LLM, MTST, SOFTS, iTransformer, TimesNet, and PatchTST. In total, MSTN establishes new SOTA performance on 24 of 32 benchmark datasets, demonstrating its consistent performance across diverse temporal tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent</title>
<link>https://arxiv.org/abs/2511.20584</link>
<guid>https://arxiv.org/abs/2511.20584</guid>
<content:encoded><![CDATA[
arXiv:2511.20584v1 Announce Type: new 
Abstract: Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models</title>
<link>https://arxiv.org/abs/2511.20587</link>
<guid>https://arxiv.org/abs/2511.20587</guid>
<content:encoded><![CDATA[
arXiv:2511.20587v1 Announce Type: new 
Abstract: We present Anatomica: an inference-time framework for generating multi-class anatomical voxel maps with localized geo-topological control. During generation, we use cuboidal control domains of varying dimensionality, location, and shape to slice out relevant substructures. These local substructures are used to compute differentiable penalty functions that steer the sample towards target constraints. We control geometric features such as size, shape, and position through voxel-wise moments, while topological features such as connected components, loops, and voids are enforced through persistent homology. Lastly, we implement Anatomica for latent diffusion models, where neural field decoders partially extract substructures, enabling the efficient control of anatomical properties. Anatomica applies flexibly across diverse anatomical systems, composing constraints to control complex structures over arbitrary dimensions and coordinate systems, thereby enabling the rational design of synthetic datasets for virtual trials or machine learning workflows.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.20591</link>
<guid>https://arxiv.org/abs/2511.20591</guid>
<content:encoded><![CDATA[
arXiv:2511.20591v1 Announce Type: new 
Abstract: The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Diffusion Inversion Requires Understanding the Latent Space</title>
<link>https://arxiv.org/abs/2511.20592</link>
<guid>https://arxiv.org/abs/2511.20592</guid>
<content:encoded><![CDATA[
arXiv:2511.20592v1 Announce Type: new 
Abstract: The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\% and substantial increases in TPR@1\%FPR (6.42\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pok\'emon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents</title>
<link>https://arxiv.org/abs/2511.20597</link>
<guid>https://arxiv.org/abs/2511.20597</guid>
<content:encoded><![CDATA[
arXiv:2511.20597v1 Announce Type: new 
Abstract: The integration of artificial intelligence (AI) agents into web browsers introduces security challenges that go beyond traditional web application threat models. Prior work has identified prompt injection as a new attack vector for web agents, yet the resulting impact within real-world environments remains insufficiently understood.
  In this work, we examine the landscape of prompt injection attacks and synthesize a benchmark of attacks embedded in realistic HTML payloads. Our benchmark goes beyond prior work by emphasizing injections that can influence real-world actions rather than mere text outputs, and by presenting attack payloads with complexity and distractor frequency similar to what real-world agents encounter. We leverage this benchmark to conduct a comprehensive empirical evaluation of existing defenses, assessing their effectiveness across a suite of frontier AI models. We propose a multi-layered defense strategy comprising both architectural and model-based defenses to protect against evolving prompt injection attacks. Our work offers a blueprint for designing practical, secure web agents through a defense-in-depth approach.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting</title>
<link>https://arxiv.org/abs/2511.20601</link>
<guid>https://arxiv.org/abs/2511.20601</guid>
<content:encoded><![CDATA[
arXiv:2511.20601v1 Announce Type: new 
Abstract: Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $\Delta_{\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $\Delta_{\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $\Delta_{\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Purchase Labels? A Cost-Effective Approach Using Active Learning Markets</title>
<link>https://arxiv.org/abs/2511.20605</link>
<guid>https://arxiv.org/abs/2511.20605</guid>
<content:encoded><![CDATA[
arXiv:2511.20605v1 Announce Type: new 
Abstract: We introduce and analyse active learning markets as a way to purchase labels, in situations where analysts aim to acquire additional data to improve model fitting, or to better train models for predictive analytics applications. This comes in contrast to the many proposals that already exist to purchase features and examples. By originally formalising the market clearing as an optimisation problem, we integrate budget constraints and improvement thresholds into the label acquisition process. We focus on a single-buyer-multiple-seller setup and propose the use of two active learning strategies (variance based and query-by-committee based), paired with distinct pricing mechanisms. They are compared to a benchmark random sampling approach. The proposed strategies are validated on real-world datasets from two critical application domains: real estate pricing and energy forecasting. Results demonstrate the robustness of our approach, consistently achieving superior performance with fewer labels acquired compared to conventional methods. Our proposal comprises an easy-to-implement practical solution for optimising data acquisition in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Hopfield Network: Rethinking Similarities in Associative Memory</title>
<link>https://arxiv.org/abs/2511.20609</link>
<guid>https://arxiv.org/abs/2511.20609</guid>
<content:encoded><![CDATA[
arXiv:2511.20609v1 Announce Type: new 
Abstract: Associative memory models are content-addressable memory systems fundamental to biological intelligence and are notable for their high interpretability. However, existing models evaluate the quality of retrieval based on proximity, which cannot guarantee that the retrieved pattern has the strongest association with the query, failing correctness. We reframe this problem by proposing that a query is a generative variant of a stored memory pattern, and define a variant distribution to model this subtle context-dependent generative process. Consequently, correct retrieval should return the memory pattern with the maximum a posteriori probability of being the query's origin. This perspective reveals that an ideal similarity measure should approximate the likelihood of each stored pattern generating the query in accordance with variant distribution, which is impossible for fixed and pre-defined similarities used by existing associative memories. To this end, we develop adaptive similarity, a novel mechanism that learns to approximate this insightful but unknown likelihood from samples drawn from context, aiming for correct retrieval. We theoretically prove that our proposed adaptive similarity achieves optimal correct retrieval under three canonical and widely applicable types of variants: noisy, masked, and biased. We integrate this mechanism into a novel adaptive Hopfield network (A-Hop), and empirical results show that it achieves state-of-the-art performance across diverse tasks, including memory retrieval, tabular classification, image classification, and multiple instance learning.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition</title>
<link>https://arxiv.org/abs/2511.20612</link>
<guid>https://arxiv.org/abs/2511.20612</guid>
<content:encoded><![CDATA[
arXiv:2511.20612v1 Announce Type: new 
Abstract: Many consequential real-world systems, like wind fields and ocean currents, are dynamic and hard to model. Learning their governing dynamics remains a central challenge in scientific machine learning. Dynamic Mode Decomposition (DMD) provides a simple, data-driven approximation, but practical use is limited by sparse/noisy observations from continuous fields, reliance on linear approximations, and the lack of principled uncertainty quantification. To address these issues, we introduce Stochastic NODE-DMD, a probabilistic extension of DMD that models continuous-time, nonlinear dynamics while remaining interpretable. Our approach enables continuous spatiotemporal reconstruction at arbitrary coordinates and quantifies predictive uncertainty. Across four benchmarks, a synthetic setting and three physics-based flows, it surpasses a baseline in reconstruction accuracy when trained from only 10% observation density. It further recovers the dynamical structure by aligning learned modes and continuous-time eigenvalues with ground truth. Finally, on datasets with multiple realizations, our method learns a calibrated distribution over latent dynamics that preserves ensemble variability rather than averaging across regimes. Our code is available at: https://github.com/sedan-group/Stochastic-NODE-DMD
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning</title>
<link>https://arxiv.org/abs/2511.20613</link>
<guid>https://arxiv.org/abs/2511.20613</guid>
<content:encoded><![CDATA[
arXiv:2511.20613v1 Announce Type: new 
Abstract: The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiFR: Inference Verification Despite Nondeterminism</title>
<link>https://arxiv.org/abs/2511.20621</link>
<guid>https://arxiv.org/abs/2511.20621</guid>
<content:encoded><![CDATA[
arXiv:2511.20621v1 Announce Type: new 
Abstract: As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ROOT: Robust Orthogonalized Optimizer for Neural Network Training</title>
<link>https://arxiv.org/abs/2511.20626</link>
<guid>https://arxiv.org/abs/2511.20626</guid>
<content:encoded><![CDATA[
arXiv:2511.20626v1 Announce Type: new 
Abstract: The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model</title>
<link>https://arxiv.org/abs/2511.20636</link>
<guid>https://arxiv.org/abs/2511.20636</guid>
<content:encoded><![CDATA[
arXiv:2511.20636v1 Announce Type: new 
Abstract: Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Path Knowledge-Augmented Contrastive Alignment Network for Spatially Resolved Transcriptomics</title>
<link>https://arxiv.org/abs/2511.17685</link>
<guid>https://arxiv.org/abs/2511.17685</guid>
<content:encoded><![CDATA[
arXiv:2511.17685v1 Announce Type: cross 
Abstract: Spatial Transcriptomics (ST) is a technology that measures gene expression profiles within tissue sections while retaining spatial context. It reveals localized gene expression patterns and tissue heterogeneity, both of which are essential for understanding disease etiology. However, its high cost has driven efforts to predict spatial gene expression from whole slide images. Despite recent advancements, current methods still face significant limitations, such as under-exploitation of high-level biological context, over-reliance on exemplar retrievals, and inadequate alignment of heterogeneous modalities. To address these challenges, we propose DKAN, a novel Dual-path Knowledge-Augmented contrastive alignment Network that predicts spatially resolved gene expression by integrating histopathological images and gene expression profiles through a biologically informed approach. Specifically, we introduce an effective gene semantic representation module that leverages the external gene database to provide additional biological insights, thereby enhancing gene expression prediction. Further, we adopt a unified, one-stage contrastive learning paradigm, seamlessly combining contrastive learning and supervised learning to eliminate reliance on exemplars, complemented with an adaptive weighting mechanism. Additionally, we propose a dual-path contrastive alignment module that employs gene semantic features as dynamic cross-modal coordinators to enable effective heterogeneous feature integration. Through extensive experiments across three public ST datasets, DKAN demonstrates superior performance over state-of-the-art models, establishing a new benchmark for spatial gene expression prediction and offering a powerful tool for advancing biological and clinical research.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments</title>
<link>https://arxiv.org/abs/2511.19464</link>
<guid>https://arxiv.org/abs/2511.19464</guid>
<content:encoded><![CDATA[
arXiv:2511.19464v1 Announce Type: cross 
Abstract: SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SG-OIF: A Stability-Guided Online Influence Framework for Reliable Vision Data</title>
<link>https://arxiv.org/abs/2511.19466</link>
<guid>https://arxiv.org/abs/2511.19466</guid>
<content:encoded><![CDATA[
arXiv:2511.19466v1 Announce Type: cross 
Abstract: Approximating training-point influence on test predictions is critical for deploying deep-learning vision models, essential for locating noisy data. Though the influence function was proposed for attributing how infinitesimal up-weighting or removal of individual training examples affects model outputs, its implementation is still challenging in deep-learning vision models: inverse-curvature computations are expensive, and training non-stationarity invalidates static approximations. Prior works use iterative solvers and low-rank surrogates to reduce cost, but offline computation lags behind training dynamics, and missing confidence calibration yields fragile rankings that misidentify critical examples. To address these challenges, we introduce a Stability-Guided Online Influence Framework (SG-OIF), the first framework that treats algorithmic stability as a real-time controller, which (i) maintains lightweight anchor IHVPs via stochastic Richardson and preconditioned Neumann; (ii) proposes modular curvature backends to modulate per-example influence scores using stability-guided residual thresholds, anomaly gating, and confidence. Experimental results show that SG-OIF achieves SOTA (State-Of-The-Art) on noise-label and out-of-distribution detection tasks across multiple datasets with various corruption. Notably, our approach achieves 91.1\% accuracy in the top 1\% prediction samples on the CIFAR-10 (20\% asym), and gets 99.8\% AUPR score on MNIST, effectively demonstrating that this framework is a practical controller for online influence estimation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a future space-based, highly scalable AI infrastructure system design</title>
<link>https://arxiv.org/abs/2511.19468</link>
<guid>https://arxiv.org/abs/2511.19468</guid>
<content:encoded><![CDATA[
arXiv:2511.19468v1 Announce Type: cross 
Abstract: If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\lesssim$\$200/kg by the mid-2030s.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAST: Topology-Aware Frequency-Domain Distribution Matching for Coreset Selection</title>
<link>https://arxiv.org/abs/2511.19476</link>
<guid>https://arxiv.org/abs/2511.19476</guid>
<content:encoded><![CDATA[
arXiv:2511.19476v1 Announce Type: cross 
Abstract: Coreset selection compresses large datasets into compact, representative subsets, reducing the energy and computational burden of training deep neural networks. Existing methods are either: (i) DNN-based, which are tied to model-specific parameters and introduce architectural bias; or (ii) DNN-free, which rely on heuristics lacking theoretical guarantees. Neither approach explicitly constrains distributional equivalence, largely because continuous distribution matching is considered inapplicable to discrete sampling. Moreover, prevalent metrics (e.g., MSE, KL, MMD, CE) cannot accurately capture higher-order moment discrepancies, leading to suboptimal coresets. In this work, we propose FAST, the first DNN-free distribution-matching coreset selection framework that formulates the coreset selection task as a graph-constrained optimization problem grounded in spectral graph theory and employs the Characteristic Function Distance (CFD) to capture full distributional information in the frequency domain. We further discover that naive CFD suffers from a "vanishing phase gradient" issue in medium and high-frequency regions; to address this, we introduce an Attenuated Phase-Decoupled CFD. Furthermore, for better convergence, we design a Progressive Discrepancy-Aware Sampling strategy that progressively schedules frequency selection from low to high, preserving global structure before refining local details and enabling accurate matching with fewer frequencies while avoiding overfitting. Extensive experiments demonstrate that FAST significantly outperforms state-of-the-art coreset selection methods across all evaluated benchmarks, achieving an average accuracy gain of 9.12%. Compared to other baseline coreset methods, it reduces power consumption by 96.57% and achieves a 2.2x average speedup, underscoring its high performance and energy efficiency.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Stage Deep Learning Framework with PKCP-MixUp Augmentation for Pediatric Liver Tumor Diagnosis Using Multi-Phase Contrast-Enhanced CT</title>
<link>https://arxiv.org/abs/2511.19478</link>
<guid>https://arxiv.org/abs/2511.19478</guid>
<content:encoded><![CDATA[
arXiv:2511.19478v1 Announce Type: cross 
Abstract: Pediatric liver tumors are one of the most common solid tumors in pediatrics, with differentiation of benign or malignant status and pathological classification critical for clinical treatment. While pathological examination is the gold standard, the invasive biopsy has notable limitations: the highly vascular pediatric liver and fragile tumor tissue raise complication risks such as bleeding; additionally, young children with poor compliance require anesthesia for biopsy, increasing medical costs or psychological trauma. Although many efforts have been made to utilize AI in clinical settings, most researchers have overlooked its importance in pediatric liver tumors. To establish a non-invasive examination procedure, we developed a multi-stage deep learning (DL) framework for automated pediatric liver tumor diagnosis using multi-phase contrast-enhanced CT. Two retrospective and prospective cohorts were enrolled. We established a novel PKCP-MixUp data augmentation method to address data scarcity and class imbalance. We also trained a tumor detection model to extract ROIs, and then set a two-stage diagnosis pipeline with three backbones with ROI-masked images. Our tumor detection model has achieved high performance (mAP=0.871), and the first stage classification model between benign and malignant tumors reached an excellent performance (AUC=0.989). Final diagnosis models also exhibited robustness, including benign subtype classification (AUC=0.915) and malignant subtype classification (AUC=0.979). We also conducted multi-level comparative analyses, such as ablation studies on data and training pipelines, as well as Shapley-Value and CAM interpretability analyses. This framework fills the pediatric-specific DL diagnostic gap, provides actionable insights for CT phase selection and model design, and paves the way for precise, accessible pediatric liver tumor diagnosis.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments</title>
<link>https://arxiv.org/abs/2511.19479</link>
<guid>https://arxiv.org/abs/2511.19479</guid>
<content:encoded><![CDATA[
arXiv:2511.19479v1 Announce Type: cross 
Abstract: As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high- performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heteroge- neous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system het- erogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>stable-pretraining-v1: Foundation Model Research Made Simple</title>
<link>https://arxiv.org/abs/2511.19484</link>
<guid>https://arxiv.org/abs/2511.19484</guid>
<content:encoded><![CDATA[
arXiv:2511.19484v1 Announce Type: cross 
Abstract: Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CycleChemist: A Dual-Pronged Machine Learning Framework for Organic Photovoltaic Discovery</title>
<link>https://arxiv.org/abs/2511.19500</link>
<guid>https://arxiv.org/abs/2511.19500</guid>
<content:encoded><![CDATA[
arXiv:2511.19500v1 Announce Type: cross 
Abstract: Organic photovoltaic (OPV) materials offer a promising path toward sustainable energy generation, but their development is limited by the difficulty of identifying high performance donor and acceptor pairs with strong power conversion efficiencies (PCEs). Existing design strategies typically focus on either the donor or the acceptor alone, rather than using a unified approach capable of modeling both components. In this work, we introduce a dual machine learning framework for OPV discovery that combines predictive modeling with generative molecular design. We present the Organic Photovoltaic Donor Acceptor Dataset (OPV2D), the largest curated dataset of its kind, containing 2000 experimentally characterized donor acceptor pairs. Using this dataset, we develop the Organic Photovoltaic Classifier (OPVC) to predict whether a material exhibits OPV behavior, and a hierarchical graph neural network that incorporates multi task learning and donor acceptor interaction modeling. This framework includes the Molecular Orbital Energy Estimator (MOE2) for predicting HOMO and LUMO energy levels, and the Photovoltaic Performance Predictor (P3) for estimating PCE. In addition, we introduce the Material Generative Pretrained Transformer (MatGPT) to produce synthetically accessible organic semiconductors, guided by a reinforcement learning strategy with three objective policy optimization. By linking molecular representation learning with performance prediction, our framework advances data driven discovery of high performance OPV materials.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning</title>
<link>https://arxiv.org/abs/2511.19518</link>
<guid>https://arxiv.org/abs/2511.19518</guid>
<content:encoded><![CDATA[
arXiv:2511.19518v1 Announce Type: cross 
Abstract: Recent advances in vision-language models (VLMs) have shown remarkable performance across multimodal tasks, yet their ever-growing scale poses severe challenges for deployment and efficiency. Existing compression methods often rely on heuristic importance metrics or empirical pruning rules, lacking theoretical guarantees about information preservation. In this work, we propose InfoPrune, an information-theoretic framework for adaptive structural compression of VLMs. Grounded in the Information Bottleneck principle, we formulate pruning as a trade-off between retaining task-relevant semantics and discarding redundant dependencies. To quantify the contribution of each attention head, we introduce an entropy-based effective rank (eRank) and employ the Kolmogorov--Smirnov (KS) distance to measure the divergence between original and compressed structures. This yields a unified criterion that jointly considers structural sparsity and informational efficiency. Building on this foundation, we further design two complementary schemes: (1) a training-based head pruning guided by the proposed information loss objective, and (2) a training-free FFN compression via adaptive low-rank approximation. Extensive experiments on VQAv2, TextVQA, and GQA demonstrate that InfoPrune achieves up to 3.2x FLOP reduction and 1.8x acceleration with negligible performance degradation, establishing a theoretically grounded and practically effective step toward efficient multimodal large models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blinking Beyond EAR: A Stable Eyelid Angle Metric for Driver Drowsiness Detection and Data Augmentation</title>
<link>https://arxiv.org/abs/2511.19519</link>
<guid>https://arxiv.org/abs/2511.19519</guid>
<content:encoded><![CDATA[
arXiv:2511.19519v1 Announce Type: cross 
Abstract: Detecting driver drowsiness reliably is crucial for enhancing road safety and supporting advanced driver assistance systems (ADAS). We introduce the Eyelid Angle (ELA), a novel, reproducible metric of eye openness derived from 3D facial landmarks. Unlike conventional binary eye state estimators or 2D measures, such as the Eye Aspect Ratio (EAR), the ELA provides a stable geometric description of eyelid motion that is robust to variations in camera angle. Using the ELA, we design a blink detection framework that extracts temporal characteristics, including the closing, closed, and reopening durations, which are shown to correlate with drowsiness levels. To address the scarcity and risk of collecting natural drowsiness data, we further leverage ELA signals to animate rigged avatars in Blender 3D, enabling the creation of realistic synthetic datasets with controllable noise, camera viewpoints, and blink dynamics. Experimental results in public driver monitoring datasets demonstrate that the ELA offers lower variance under viewpoint changes compared to EAR and achieves accurate blink detection. At the same time, synthetic augmentation expands the diversity of training data for drowsiness recognition. Our findings highlight the ELA as both a reliable biometric measure and a powerful tool for generating scalable datasets in driver state monitoring.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Autoencoder Joint Learning for Robust Spitzoid Tumor Classification</title>
<link>https://arxiv.org/abs/2511.19535</link>
<guid>https://arxiv.org/abs/2511.19535</guid>
<content:encoded><![CDATA[
arXiv:2511.19535v1 Announce Type: cross 
Abstract: Accurate diagnosis of spitzoid tumors (ST) is critical to ensure a favorable prognosis and to avoid both under- and over-treatment. Epigenetic data, particularly DNA methylation, provide a valuable source of information for this task. However, prior studies assume complete data, an unrealistic setting as methylation profiles frequently contain missing entries due to limited coverage and experimental artifacts. Our work challenges these favorable scenarios and introduces ReMAC, an extension of ReMasker designed to tackle classification tasks on high-dimensional data under complete and incomplete regimes. Evaluation on real clinical data demonstrates that ReMAC achieves strong and robust performance compared to competing classification methods in the stratification of ST. Code is available: https://github.com/roshni-mahtani/ReMAC.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Domain Generalization of Multimodal LLMs for Global Photovoltaic Assessment</title>
<link>https://arxiv.org/abs/2511.19537</link>
<guid>https://arxiv.org/abs/2511.19537</guid>
<content:encoded><![CDATA[
arXiv:2511.19537v1 Announce Type: cross 
Abstract: The rapid expansion of distributed photovoltaic (PV) systems poses challenges for power grid management, as many installations remain undocumented. While satellite imagery provides global coverage, traditional computer vision (CV) models such as CNNs and U-Nets require extensive labeled data and fail to generalize across regions. This study investigates the cross-domain generalization of a multimodal large language model (LLM) for global PV assessment. By leveraging structured prompts and fine-tuning, the model integrates detection, localization, and quantification within a unified schema. Cross-regional evaluation using the $\Delta$F1 metric demonstrates that the proposed model achieves the smallest performance degradation across unseen regions, outperforming conventional CV and transformer baselines. These results highlight the robustness of multimodal LLMs under domain shift and their potential for scalable, transferable, and interpretable global PV mapping.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think First, Assign Next (ThiFAN-VQA): A Two-stage Chain-of-Thought Framework for Post-Disaster Damage Assessment</title>
<link>https://arxiv.org/abs/2511.19557</link>
<guid>https://arxiv.org/abs/2511.19557</guid>
<content:encoded><![CDATA[
arXiv:2511.19557v1 Announce Type: cross 
Abstract: Timely and accurate assessment of damages following natural disasters is essential for effective emergency response and recovery. Recent AI-based frameworks have been developed to analyze large volumes of aerial imagery collected by Unmanned Aerial Vehicles, providing actionable insights rapidly. However, creating and annotating data for training these models is costly and time-consuming, resulting in datasets that are limited in size and diversity. Furthermore, most existing approaches rely on traditional classification-based frameworks with fixed answer spaces, restricting their ability to provide new information without additional data collection or model retraining. Using pre-trained generative models built on in-context learning (ICL) allows for flexible and open-ended answer spaces. However, these models often generate hallucinated outputs or produce generic responses that lack domain-specific relevance. To address these limitations, we propose ThiFAN-VQA, a two-stage reasoning-based framework for visual question answering (VQA) in disaster scenarios. ThiFAN-VQA first generates structured reasoning traces using chain-of-thought (CoT) prompting and ICL to enable interpretable reasoning under limited supervision. A subsequent answer selection module evaluates the generated responses and assigns the most coherent and contextually accurate answer, effectively improve the model performance. By integrating a custom information retrieval system, domain-specific prompting, and reasoning-guided answer selection, ThiFAN-VQA bridges the gap between zero-shot and supervised methods, combining flexibility with consistency. Experiments on FloodNet and RescueNet-VQA, UAV-based datasets from flood- and hurricane-affected regions, demonstrate that ThiFAN-VQA achieves superior accuracy, interpretability, and adaptability for real-world post-disaster damage assessment tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2511.19558</link>
<guid>https://arxiv.org/abs/2511.19558</guid>
<content:encoded><![CDATA[
arXiv:2511.19558v1 Announce Type: cross 
Abstract: Text-to-image diffusion models can emit copyrighted, unsafe, or private content. Safety alignment aims to suppress specific concepts, yet evaluations seldom test whether safety persists under benign downstream fine-tuning routinely applied after deployment (e.g., LoRA personalization, style/domain adapters). We study the stability of current safety methods under benign fine-tuning and observe frequent breakdowns. As true safety alignment must withstand even benign post-deployment adaptations, we introduce the SPQR benchmark (Safety-Prompt adherence-Quality-Robustness). SPQR is a single-scored metric that provides a standardized and reproducible framework to evaluate how well safety-aligned diffusion models preserve safety, utility, and robustness under benign fine-tuning, by reporting a single leaderboard score to facilitate comparisons. We conduct multilingual, domain-specific, and out-of-distribution analyses, along with category-wise breakdowns, to identify when safety alignment fails after benign fine-tuning, ultimately showcasing SPQR as a concise yet comprehensive benchmark for T2I safety alignment techniques for T2I models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization and Regularization Under Arbitrary Objectives</title>
<link>https://arxiv.org/abs/2511.19628</link>
<guid>https://arxiv.org/abs/2511.19628</guid>
<content:encoded><![CDATA[
arXiv:2511.19628v1 Announce Type: cross 
Abstract: This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agint: Agentic Graph Compilation for Software Engineering Agents</title>
<link>https://arxiv.org/abs/2511.19635</link>
<guid>https://arxiv.org/abs/2511.19635</guid>
<content:encoded><![CDATA[
arXiv:2511.19635v1 Announce Type: cross 
Abstract: LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Data: AI's New Weapon Against Android Malware</title>
<link>https://arxiv.org/abs/2511.19649</link>
<guid>https://arxiv.org/abs/2511.19649</guid>
<content:encoded><![CDATA[
arXiv:2511.19649v1 Announce Type: cross 
Abstract: The ever-increasing number of Android devices and the accelerated evolution of malware, reaching over 35 million samples by 2024, highlight the critical importance of effective detection methods. Attackers are now using Artificial Intelligence to create sophisticated malware variations that can easily evade traditional detection techniques. Although machine learning has shown promise in malware classification, its success relies heavily on the availability of up-to-date, high-quality datasets. The scarcity and high cost of obtaining and labeling real malware samples presents significant challenges in developing robust detection models. In this paper, we propose MalSynGen, a Malware Synthetic Data Generation methodology that uses a conditional Generative Adversarial Network (cGAN) to generate synthetic tabular data. This data preserves the statistical properties of real-world data and improves the performance of Android malware classifiers. We evaluated the effectiveness of this approach using various datasets and metrics that assess the fidelity of the generated data, its utility in classification, and the computational efficiency of the process. Our experiments demonstrate that MalSynGen can generalize across different datasets, providing a viable solution to address the issues of obsolescence and low quality data in malware detection.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Alexander-Hirschowitz theorem for neurovarieties</title>
<link>https://arxiv.org/abs/2511.19703</link>
<guid>https://arxiv.org/abs/2511.19703</guid>
<content:encoded><![CDATA[
arXiv:2511.19703v1 Announce Type: cross 
Abstract: We study neurovarieties for polynomial neural networks and fully characterize when they attain the expected dimension in the single-output case. As consequences, we establish non-defectiveness and global identifiability for multi-output architectures.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing Preconditioners for SGD: Local Conditioning, Noise Floors, and Basin Stability</title>
<link>https://arxiv.org/abs/2511.19716</link>
<guid>https://arxiv.org/abs/2511.19716</guid>
<content:encoded><![CDATA[
arXiv:2511.19716v1 Announce Type: cross 
Abstract: Stochastic Gradient Descent (SGD) often slows in the late stage of training due to anisotropic curvature and gradient noise. We analyze preconditioned SGD in the geometry induced by a symmetric positive definite matrix $\mathbf{M}$, deriving bounds in which both the convergence rate and the stochastic noise floor are governed by $\mathbf{M}$-dependent quantities: the rate through an effective condition number in the $\mathbf{M}$-metric, and the floor through the product of that condition number and the preconditioned noise level. For nonconvex objectives, we establish a preconditioner-dependent basin-stability guarantee: when smoothness and basin size are measured in the $\mathbf{M}$-norm, the probability that the iterates remain in a well-behaved local region admits an explicit lower bound. This perspective is particularly relevant in Scientific Machine Learning (SciML), where achieving small training loss under stochastic updates is closely tied to physical fidelity, numerical stability, and constraint satisfaction. The framework applies to both diagonal/adaptive and curvature-aware preconditioners and yields a simple design principle: choose $\mathbf{M}$ to improve local conditioning while attenuating noise. Experiments on a quadratic diagnostic and three SciML benchmarks validate the predicted rate-floor behavior.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Scale Community-Aware Network Generation</title>
<link>https://arxiv.org/abs/2511.19717</link>
<guid>https://arxiv.org/abs/2511.19717</guid>
<content:encoded><![CDATA[
arXiv:2511.19717v1 Announce Type: cross 
Abstract: Community detection, or network clustering, is used to identify latent community structure in networks. Due to the scarcity of labeled ground truth in real-world networks, evaluating these algorithms poses significant challenges. To address this, researchers use synthetic network generators that produce networks with ground-truth community labels. RECCS is one such algorithm that takes a network and its clustering as input and generates a synthetic network through a modular pipeline. Each generated ground truth cluster preserves key characteristics of the corresponding input cluster, including connectivity, minimum degree, and degree sequence distribution. The output consists of a synthetically generated network, and disjoint ground truth cluster labels for all nodes. In this paper, we present two enhanced versions: RECCS+ and RECCS++. RECCS+ maintains algorithmic fidelity to the original RECCS while introducing parallelization through an orchestrator that coordinates algorithmic components across multiple processes and employs multithreading. RECCS++ builds upon this foundation with additional algorithmic optimizations to achieve further speedup. Our experimental results demonstrate that RECCS+ and RECCS++ achieve speedups of up to 49x and 139x respectively on our benchmark datasets, with RECCS++'s additional performance gains involving a modest accuracy tradeoff. With this newfound performance, RECCS++ can now scale to networks with over 100 million nodes and nearly 2 billion edges.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Individual and group fairness in geographical partitioning</title>
<link>https://arxiv.org/abs/2511.19722</link>
<guid>https://arxiv.org/abs/2511.19722</guid>
<content:encoded><![CDATA[
arXiv:2511.19722v1 Announce Type: cross 
Abstract: Socioeconomic segregation often arises in school districting and other contexts, causing some groups to be over- or under-represented within a particular district. This phenomenon is closely linked with disparities in opportunities and outcomes. We formulate a new class of geographical partitioning problems in which the population is heterogeneous, and it is necessary to ensure fair representation for each group at each facility. We prove that the optimal solution is a novel generalization of the additively weighted Voronoi diagram, and we propose a simple and efficient algorithm to compute it, thus resolving an open question dating back to Dvoretzky et al. (1951). The efficacy and potential for practical insight of the approach are demonstrated in a realistic case study involving seven demographic groups and $78$ district offices.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Adaptive, Data-Integrated Agent-Based Modeling Framework for Explainable and Contestable Policy Design</title>
<link>https://arxiv.org/abs/2511.19726</link>
<guid>https://arxiv.org/abs/2511.19726</guid>
<content:encoded><![CDATA[
arXiv:2511.19726v1 Announce Type: cross 
Abstract: Multi-agent systems often operate under feedback, adaptation, and non-stationarity, yet many simulation studies retain static decision rules and fixed control parameters. This paper introduces a general adaptive multi-agent learning framework that integrates: (i) four dynamic regimes distinguishing static versus adaptive agents and fixed versus adaptive system parameters; (ii) information-theoretic diagnostics (entropy rate, statistical complexity, and predictive information) to assess predictability and structure; (iii) structural causal models for explicit intervention semantics; (iv) procedures for generating agent-level priors from aggregate or sample data; and (v) unsupervised methods for identifying emergent behavioral regimes. The framework offers a domain-neutral architecture for analyzing how learning agents and adaptive controls jointly shape system trajectories, enabling systematic comparison of stability, performance, and interpretability across non-equilibrium, oscillatory, or drifting dynamics. Mathematical definitions, computational operators, and an experimental design template are provided, yielding a structured methodology for developing explainable and contestable multi-agent decision processes.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating RCTs, RWD, AI/ML and Statistics: Next-Generation Evidence Synthesis</title>
<link>https://arxiv.org/abs/2511.19735</link>
<guid>https://arxiv.org/abs/2511.19735</guid>
<content:encoded><![CDATA[
arXiv:2511.19735v1 Announce Type: cross 
Abstract: Randomized controlled trials (RCTs) have been the cornerstone of clinical evidence; however, their cost, duration, and restrictive eligibility criteria limit power and external validity. Studies using real-world data (RWD), historically considered less reliable for establishing causality, are now recognized to be important for generating real-world evidence (RWE). In parallel, artificial intelligence and machine learning (AI/ML) are being increasingly used throughout the drug development process, providing scalability and flexibility but also presenting challenges in interpretability and rigor that traditional statistics do not face. This Perspective argues that the future of evidence generation will not depend on RCTs versus RWD, or statistics versus AI/ML, but on their principled integration. To this end, a causal roadmap is needed to clarify inferential goals, make assumptions explicit, and ensure transparency about tradeoffs. We highlight key objectives of integrative evidence synthesis, including transporting RCT results to broader populations, embedding AI-assisted analyses within RCTs, designing hybrid controlled trials, and extending short-term RCTs with long-term RWD. We also outline future directions in privacy-preserving analytics, uncertainty quantification, and small-sample methods. By uniting statistical rigor with AI/ML innovation, integrative approaches can produce robust, transparent, and policy-relevant evidence, making them a key component of modern regulatory science.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation</title>
<link>https://arxiv.org/abs/2511.19739</link>
<guid>https://arxiv.org/abs/2511.19739</guid>
<content:encoded><![CDATA[
arXiv:2511.19739v1 Announce Type: cross 
Abstract: Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAMformer: Associative Memory is All You Need</title>
<link>https://arxiv.org/abs/2511.19740</link>
<guid>https://arxiv.org/abs/2511.19740</guid>
<content:encoded><![CDATA[
arXiv:2511.19740v1 Announce Type: cross 
Abstract: Transformers face scalability challenges due to the quadratic cost of attention, which involves dense similarity computations between queries and keys. We propose CAMformer, a novel accelerator that reinterprets attention as an associative memory operation and computes attention scores using a voltage-domain Binary Attention Content Addressable Memory (BA-CAM). This enables constant-time similarity search through analog charge sharing, replacing digital arithmetic with physical similarity sensing. CAMformer integrates hierarchical two-stage top-k filtering, pipelined execution, and high-precision contextualization to achieve both algorithmic accuracy and architectural efficiency. Evaluated on BERT and Vision Transformer workloads, CAMformer achieves over 10x energy efficiency, up to 4x higher throughput, and 6-8x lower area compared to state-of-the-art accelerators--while maintaining near-lossless accuracy.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustering Approaches for Mixed-Type Data: A Comparative Study</title>
<link>https://arxiv.org/abs/2511.19755</link>
<guid>https://arxiv.org/abs/2511.19755</guid>
<content:encoded><![CDATA[
arXiv:2511.19755v1 Announce Type: cross 
Abstract: Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)</title>
<link>https://arxiv.org/abs/2511.19798</link>
<guid>https://arxiv.org/abs/2511.19798</guid>
<content:encoded><![CDATA[
arXiv:2511.19798v1 Announce Type: cross 
Abstract: Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent-space metrics for Complex-Valued VAE out-of-distribution detection under radar clutter</title>
<link>https://arxiv.org/abs/2511.19805</link>
<guid>https://arxiv.org/abs/2511.19805</guid>
<content:encoded><![CDATA[
arXiv:2511.19805v1 Announce Type: cross 
Abstract: We investigate complex-valued Variational AutoEncoders (CVAE) for radar Out-Of-Distribution (OOD) detection in complex radar environments. We proposed several detection metrics: the reconstruction error of CVAE (CVAE-MSE), the latent-based scores (Mahalanobis, Kullback-Leibler divergence (KLD)), and compared their performance against the classical ANMF-Tyler detector (ANMF-FP). The performance of all these detectors is analyzed on synthetic and experimental radar data, showing the advantages and the weaknesses of each detector.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Generation of Diverse and High-Fidelity Images via Prompt Semantic Space Optimization</title>
<link>https://arxiv.org/abs/2511.19811</link>
<guid>https://arxiv.org/abs/2511.19811</guid>
<content:encoded><![CDATA[
arXiv:2511.19811v1 Announce Type: cross 
Abstract: Image diversity remains a fundamental challenge for text-to-image diffusion models. Low-diversity models tend to generate repetitive outputs, increasing sampling redundancy and hindering both creative exploration and downstream applications. A primary cause is that generation often collapses toward a strong mode in the learned distribution. Existing attempts to improve diversity, such as noise resampling, prompt rewriting, or steering-based guidance, often still collapse to dominant modes or introduce distortions that degrade image quality. In light of this, we propose Token-Prompt embedding Space Optimization (TPSO), a training-free and model-agnostic module. TPSO introduces learnable parameters to explore underrepresented regions of the token embedding space, reducing the tendency of the model to repeatedly generate samples from strong modes of the learned distribution. At the same time, the prompt-level space provides a global semantic constraint that regulates distribution shifts, preventing quality degradation while maintaining high fidelity. Extensive experiments on MS-COCO and three diffusion backbones show that TPSO significantly enhances generative diversity, improving baseline performance from 1.10 to 4.18 points, without sacrificing image quality. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception</title>
<link>https://arxiv.org/abs/2511.19820</link>
<guid>https://arxiv.org/abs/2511.19820</guid>
<content:encoded><![CDATA[
arXiv:2511.19820v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning with $\omega$-Regular Objectives and Constraints</title>
<link>https://arxiv.org/abs/2511.19849</link>
<guid>https://arxiv.org/abs/2511.19849</guid>
<content:encoded><![CDATA[
arXiv:2511.19849v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $\omega$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
  We address both limitations simultaneously by combining $\omega$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $\omega$-regular objective while also adhering to $\omega$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-LLM Generalization of Behavioral Backdoor Detection in AI Agent Supply Chains</title>
<link>https://arxiv.org/abs/2511.19874</link>
<guid>https://arxiv.org/abs/2511.19874</guid>
<content:encoded><![CDATA[
arXiv:2511.19874v1 Announce Type: cross 
Abstract: As AI agents become integral to enterprise workflows, their reliance on shared tool libraries and pre-trained components creates significant supply chain vulnerabilities. While previous work has demonstrated behavioral backdoor detection within individual LLM architectures, the critical question of cross-LLM generalization remains unexplored, a gap with serious implications for organizations deploying multiple AI systems. We present the first systematic study of cross-LLM behavioral backdoor detection, evaluating generalization across six production LLMs (GPT-5.1, Claude Sonnet 4.5, Grok 4.1, Llama 4 Maverick, GPT-OSS 120B, and DeepSeek Chat V3.1). Through 1,198 execution traces and 36 cross-model experiments, we quantify a critical finding: single-model detectors achieve 92.7% accuracy within their training distribution but only 49.2% across different LLMs, a 43.4 percentage point generalization gap equivalent to random guessing. Our analysis reveals that this gap stems from model-specific behavioral signatures, particularly in temporal features (coefficient of variation > 0.8), while structural features remain stable across architectures. We show that model-aware detection incorporating model identity as an additional feature achieves 90.6% accuracy universally across all evaluated models. We release our multi-LLM trace dataset and detection framework to enable reproducible research.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models</title>
<link>https://arxiv.org/abs/2511.19877</link>
<guid>https://arxiv.org/abs/2511.19877</guid>
<content:encoded><![CDATA[
arXiv:2511.19877v1 Announce Type: cross 
Abstract: Depression is one of the most prevalent mental health disorders globally. In recent years, multi-modal data, such as speech, video, and transcripts, has been increasingly used to develop AI-assisted depression assessment systems. Large language models have further advanced this field due to their strong language understanding and generalization capabilities. However, conventional LLMs remain text-centric and cannot process the rich non-verbal cues found in audio and visual modalities, which are critical components in mental health evaluation. While multi-modal LLMs offer a promising direction, few are tailored for psychological applications. In this study, we propose a novel multi-modal LLM framework for depression detection. Our approach augments an audio language model with visual understanding and aligns audio-visual features at the timestamp level. This fine-grained alignment improves modeling of temporal dynamics across modalities while reducing the need for extensive training data and computational resources. Experiments on the DAIC-WoZ dataset demonstrate that our model outperforms both single-modality approaches and previous multi-modal methods. Moreover, the proposed framework can be extended to incorporate additional physiological signals, paving the way for broader clinical applications beyond mental health.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization</title>
<link>https://arxiv.org/abs/2511.19878</link>
<guid>https://arxiv.org/abs/2511.19878</guid>
<content:encoded><![CDATA[
arXiv:2511.19878v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Degenerate Manifolds of Frustrated Magnets with Boltzmann Machines</title>
<link>https://arxiv.org/abs/2511.19879</link>
<guid>https://arxiv.org/abs/2511.19879</guid>
<content:encoded><![CDATA[
arXiv:2511.19879v1 Announce Type: cross 
Abstract: We show that Restricted Boltzmann Machines (RBMs) provide a flexible generative framework for modeling spin configurations in disordered yet strongly correlated phases of frustrated magnets. As a benchmark, we first demonstrate that an RBM can learn the zero-temperature ground-state manifold of the one-dimensional ANNNI model at its multiphase point, accurately reproducing its characteristic oscillatory and exponentially decaying correlations. We then apply RBMs to kagome spin ice and show that they successfully learn the local ice rules and short-range correlations of the extensively degenerate ice-I manifold. Correlation functions computed from RBM-generated configurations closely match those from direct Monte Carlo simulations. For the partially ordered ice-II phase -- featuring long-range charge order and broken time-reversal symmetry -- accurate modeling requires RBMs with uniform-sign bias fields, mirroring the underlying symmetry breaking. These results highlight the utility of RBMs as generative models for learning constrained and highly frustrated magnetic states.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex Instruction Following with Diverse Style Policies in Football Games</title>
<link>https://arxiv.org/abs/2511.19885</link>
<guid>https://arxiv.org/abs/2511.19885</guid>
<content:encoded><![CDATA[
arXiv:2511.19885v1 Announce Type: cross 
Abstract: Despite advancements in language-controlled reinforcement learning (LC-RL) for basic domains and straightforward commands (e.g., object manipulation and navigation), effectively extending LC-RL to comprehend and execute high-level or abstract instructions in complex, multi-agent environments, such as football games, remains a significant challenge. To address this gap, we introduce Language-Controlled Diverse Style Policies (LCDSP), a novel LC-RL paradigm specifically designed for complex scenarios. LCDSP comprises two key components: a Diverse Style Training (DST) method and a Style Interpreter (SI). The DST method efficiently trains a single policy capable of exhibiting a wide range of diverse behaviors by modulating agent actions through style parameters (SP). The SI is designed to accurately and rapidly translate high-level language instructions into these corresponding SP. Through extensive experiments in a complex 5v5 football environment, we demonstrate that LCDSP effectively comprehends abstract tactical instructions and accurately executes the desired diverse behavioral styles, showcasing its potential for complex, real-world applications.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing Reputation Systems for Manufacturing Data Trading Markets: A Multi-Agent Evaluation with Q-Learning and IRL-Estimated Utilities</title>
<link>https://arxiv.org/abs/2511.19930</link>
<guid>https://arxiv.org/abs/2511.19930</guid>
<content:encoded><![CDATA[
arXiv:2511.19930v1 Announce Type: cross 
Abstract: Recent advances in machine learning and big data analytics have intensified the demand for high-quality cross-domain datasets and accelerated the growth of data trading across organizations. As data become increasingly recognized as an economic asset, data marketplaces have emerged as a key infrastructure for data-driven innovation. However, unlike mature product or service markets, data-trading environments remain nascent and suffer from pronounced information asymmetry. Buyers cannot verify the content or quality before purchasing data, making trust and quality assurance central challenges. To address these issues, this study develops a multi-agent data-market simulator that models participant behavior and evaluates the institutional mechanisms for trust formation. Focusing on the manufacturing sector, where initiatives such as GAIA-X and Catena-X are advancing, the simulator integrates reinforcement learning (RL) for adaptive agent behavior and inverse reinforcement learning (IRL) to estimate utility functions from empirical behavioral data. Using the simulator, we examine the market-level effects of five representative reputation systems-Time-decay, Bayesian-beta, PageRank, PowerTrust, and PeerTrust-and found that PeerTrust achieved the strongest alignment between data price and quality, while preventing monopolistic dominance. Building on these results, we develop a hybrid reputation mechanism that integrates the strengths of existing systems to achieve improved price-quality consistency and overall market stability. This study extends simulation-based data-market analysis by incorporating trust and reputation as endogenous mechanisms and offering methodological and institutional insights into the design of reliable and efficient data ecosystems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI/ML based Joint Source and Channel Coding for HARQ-ACK Payload</title>
<link>https://arxiv.org/abs/2511.19943</link>
<guid>https://arxiv.org/abs/2511.19943</guid>
<content:encoded><![CDATA[
arXiv:2511.19943v1 Announce Type: cross 
Abstract: Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed. However, hybrid automatic repeat request acknowledgement (HARQ-ACK) bits transmitted in the uplink are inherently non-uniformly distributed. For such sources, significant performance gains could be obtained by employing joint source channel coding, aided by deep learning-based techniques. In this paper, we learn a transformer-based encoder using a novel "free-lunch" training algorithm and propose per-codeword power shaping to exploit the source prior at the encoder whilst being robust to small changes in the HARQ-ACK distribution. Furthermore, any HARQ-ACK decoder has to achieve a low negative acknowledgement (NACK) error rate to avoid radio link failures resulting from multiple NACK errors. We develop an extension of the Neyman-Pearson test to a coded bit system with multiple information bits to achieve Unequal Error Protection of NACK over ACK bits at the decoder. Finally, we apply the proposed encoder and decoder designs to a 5G New Radio (NR) compliant uplink setup under a fading channel, describing the optimal receiver design and a low complexity coherent approximation to it. Our results demonstrate 3-6 dB reduction in the average transmit power required to achieve the target error rates compared to the NR baseline, while also achieving a 2-3 dB reduction in the maximum transmit power, thus providing for significant coverage gains and power savings.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Softmax Transformers are Turing-Complete</title>
<link>https://arxiv.org/abs/2511.20038</link>
<guid>https://arxiv.org/abs/2511.20038</guid>
<content:encoded><![CDATA[
arXiv:2511.20038v1 Announce Type: cross 
Abstract: Hard attention Chain-of-Thought (CoT) transformers are known to be Turing-complete. However, it is an open problem whether softmax attention Chain-of-Thought (CoT) transformers are Turing-complete. In this paper, we prove a stronger result that length-generalizable softmax CoT transformers are Turing-complete. More precisely, our Turing-completeness proof goes via the CoT extension of the Counting RASP (C-RASP), which correspond to softmax CoT transformers that admit length generalization. We prove Turing-completeness for CoT C-RASP with causal masking over a unary alphabet (more generally, for letter-bounded languages). While we show this is not Turing-complete for arbitrary languages, we prove that its extension with relative positional encoding is Turing-complete for arbitrary languages. We empirically validate our theory by training transformers for languages requiring complex (non-linear) arithmetic reasoning.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MFM-point: Multi-scale Flow Matching for Point Cloud Generation</title>
<link>https://arxiv.org/abs/2511.20041</link>
<guid>https://arxiv.org/abs/2511.20041</guid>
<content:encoded><![CDATA[
arXiv:2511.20041v1 Announce Type: cross 
Abstract: In recent years, point cloud generation has gained significant attention in 3D generative modeling. Among existing approaches, point-based methods directly generate point clouds without relying on other representations such as latent features, meshes, or voxels. These methods offer low training cost and algorithmic simplicity, but often underperform compared to representation-based approaches. In this paper, we propose MFM-Point, a multi-scale Flow Matching framework for point cloud generation that substantially improves the scalability and performance of point-based methods while preserving their simplicity and efficiency. Our multi-scale generation algorithm adopts a coarse-to-fine generation paradigm, enhancing generation quality and scalability without incurring additional training or inference overhead. A key challenge in developing such a multi-scale framework lies in preserving the geometric structure of unordered point clouds while ensuring smooth and consistent distributional transitions across resolutions. To address this, we introduce a structured downsampling and upsampling strategy that preserves geometry and maintains alignment between coarse and fine resolutions. Our experimental results demonstrate that MFM-Point achieves best-in-class performance among point-based methods and challenges the best representation-based methods. In particular, MFM-point demonstrates strong results in multi-category and high-resolution generation tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing Latency of LLM Search Agent via Speculation-based Algorithm-System Co-Design</title>
<link>https://arxiv.org/abs/2511.20048</link>
<guid>https://arxiv.org/abs/2511.20048</guid>
<content:encoded><![CDATA[
arXiv:2511.20048v1 Announce Type: cross 
Abstract: LLM-based search agents achieve strong performance but suffer from severe latency, as each step requires serialized LLM reasoning followed by action of tool execution. We revisit this bottleneck through the lens of speculation. While traditional predict-verify speculation paradigm can break serial execution, its benefit remains limited, as it retains the full original workload and adds extra inference overhead. We observe that early agent steps often involve simple evidence-gathering, where correct actions can often be predicted without full reasoning. Building on these observations, we present SPAgent, an algorithm-system co-design framework that expands the role of speculation in search agents to reduce latency. Algorithmically, SPAgent introduces a two-phase adaptive speculation mechanism that selectively omits verification when safe. System-wise, a two-level scheduler regulates speculative requests based on engine load to ensure speculation remains beneficial. We implement SPAgent in real-world systems. Across extensive experimental settings, SPAgent achieves up to $1.65\times$ end-to-end speedup while maintaining same or even achieving higher accuracy, enabling practical deployment of multi-step search agents.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From data to concepts via wiring diagrams</title>
<link>https://arxiv.org/abs/2511.20138</link>
<guid>https://arxiv.org/abs/2511.20138</guid>
<content:encoded><![CDATA[
arXiv:2511.20138v1 Announce Type: cross 
Abstract: A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents</title>
<link>https://arxiv.org/abs/2511.20216</link>
<guid>https://arxiv.org/abs/2511.20216</guid>
<content:encoded><![CDATA[
arXiv:2511.20216v1 Announce Type: cross 
Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Actionable and diverse counterfactual explanations incorporating domain knowledge and causal constraints</title>
<link>https://arxiv.org/abs/2511.20236</link>
<guid>https://arxiv.org/abs/2511.20236</guid>
<content:encoded><![CDATA[
arXiv:2511.20236v1 Announce Type: cross 
Abstract: Counterfactual explanations enhance the actionable interpretability of machine learning models by identifying the minimal changes required to achieve a desired outcome of the model. However, existing methods often ignore the complex dependencies in real-world datasets, leading to unrealistic or impractical modifications. Motivated by cybersecurity applications in the email marketing domain, we propose a method for generating Diverse, Actionable, and kNowledge-Constrained Explanations (DANCE), which incorporates feature dependencies and causal constraints to ensure plausibility and real-world feasibility of counterfactuals. Our method learns linear and nonlinear constraints from data or integrates expert-provided dependency graphs, ensuring counterfactuals are plausible and actionable. By maintaining consistency with feature relationships, the method produces explanations that align with real-world constraints. Additionally, it balances plausibility, diversity, and sparsity, effectively addressing key limitations in existing algorithms. The work is developed based on a real-life case study with Freshmail, the largest email marketing company in Poland and supported by a joint R&amp;D project Sendguard. Furthermore, we provide an extensive evaluation using 140 public datasets, which highlights its ability to generate meaningful, domain-relevant counterfactuals that outperform other existing approaches based on widely used metrics. The source code for reproduction of the results can be found in a GitHub repository we provide.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Enhanced Reinforcement Learning for Accelerating Newton-Raphson Convergence with Ising Machines: A Case Study for Power Flow Analysis</title>
<link>https://arxiv.org/abs/2511.20237</link>
<guid>https://arxiv.org/abs/2511.20237</guid>
<content:encoded><![CDATA[
arXiv:2511.20237v1 Announce Type: cross 
Abstract: The Newton-Raphson (NR) method is widely used for solving power flow (PF) equations due to its quadratic convergence. However, its performance deteriorates under poor initialization or extreme operating scenarios, e.g., high levels of renewable energy penetration. Traditional NR initialization strategies often fail to address these challenges, resulting in slow convergence or even divergence. We propose the use of reinforcement learning (RL) to optimize the initialization of NR, and introduce a novel quantum-enhanced RL environment update mechanism to mitigate the significant computational cost of evaluating power system states over a combinatorially large action space at each RL timestep by formulating the voltage adjustment task as a quadratic unconstrained binary optimization problem. Specifically, quantum/digital annealers are integrated into the RL environment update to evaluate state transitions using a problem Hamiltonian designed for PF. Results demonstrate significant improvements in convergence speed, a reduction in NR iteration counts, and enhanced robustness under different operating conditions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation</title>
<link>https://arxiv.org/abs/2511.20250</link>
<guid>https://arxiv.org/abs/2511.20250</guid>
<content:encoded><![CDATA[
arXiv:2511.20250v1 Announce Type: cross 
Abstract: Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization</title>
<link>https://arxiv.org/abs/2511.20258</link>
<guid>https://arxiv.org/abs/2511.20258</guid>
<content:encoded><![CDATA[
arXiv:2511.20258v1 Announce Type: cross 
Abstract: Weight Averaging (WA) has emerged as a powerful technique for enhancing generalization by promoting convergence to a flat loss landscape, which correlates with stronger out-of-distribution performance. However, applying WA directly to multi-modal domain generalization (MMDG) is challenging: differences in optimization speed across modalities lead WA to overfit to faster-converging ones in early stages, suppressing the contribution of slower yet complementary modalities, thereby hindering effective modality fusion and skewing the loss surface toward sharper, less generalizable minima. To address this issue, we propose MBCD, a unified collaborative distillation framework that retains WA's flatness-inducing advantages while overcoming its shortcomings in multi-modal contexts. MBCD begins with adaptive modality dropout in the student model to curb early-stage bias toward dominant modalities. A gradient consistency constraint then aligns learning signals between uni-modal branches and the fused representation, encouraging coordinated and smoother optimization. Finally, a WA-based teacher conducts cross-modal distillation by transferring fused knowledge to each uni-modal branch, which strengthens cross-modal interactions and steer convergence toward flatter solutions. Extensive experiments on MMDG benchmarks show that MBCD consistently outperforms existing methods, achieving superior accuracy and robustness across diverse unseen domains.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Heterogeneous Agent Models with Physics-informed Neural Networks</title>
<link>https://arxiv.org/abs/2511.20283</link>
<guid>https://arxiv.org/abs/2511.20283</guid>
<content:encoded><![CDATA[
arXiv:2511.20283v1 Announce Type: cross 
Abstract: Understanding household behaviour is essential for modelling macroeconomic dynamics and designing effective policy. While heterogeneous agent models offer a more realistic alternative to representative agent frameworks, their implementation poses significant computational challenges, particularly in continuous time. The Aiyagari-Bewley-Huggett (ABH) framework, recast as a system of partial differential equations, typically relies on grid-based solvers that suffer from the curse of dimensionality, high computational cost, and numerical inaccuracies. This paper introduces the ABH-PINN solver, an approach based on Physics-Informed Neural Networks (PINNs), which embeds the Hamilton-Jacobi-Bellman and Kolmogorov Forward equations directly into the neural network training objective. By replacing grid-based approximation with mesh-free, differentiable function learning, the ABH-PINN solver benefits from the advantages of PINNs of improved scalability, smoother solutions, and computational efficiency. Preliminary results show that the PINN-based approach is able to obtain economically valid results matching the established finite-difference solvers.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting by Pruning: Data Deletion in Join Cardinality Estimation</title>
<link>https://arxiv.org/abs/2511.20293</link>
<guid>https://arxiv.org/abs/2511.20293</guid>
<content:encoded><![CDATA[
arXiv:2511.20293v1 Announce Type: cross 
Abstract: Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NNGPT: Rethinking AutoML with Large Language Models</title>
<link>https://arxiv.org/abs/2511.20333</link>
<guid>https://arxiv.org/abs/2511.20333</guid>
<content:encoded><![CDATA[
arXiv:2511.20333v1 Announce Type: cross 
Abstract: Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extension and neural operator approximation of the electrical impedance tomography inverse map</title>
<link>https://arxiv.org/abs/2511.20361</link>
<guid>https://arxiv.org/abs/2511.20361</guid>
<content:encoded><![CDATA[
arXiv:2511.20361v1 Announce Type: cross 
Abstract: This paper considers the problem of noise-robust neural operator approximation for the solution map of Calder\'on's inverse conductivity problem. In this continuum model of electrical impedance tomography (EIT), the boundary measurements are realized as a noisy perturbation of the Neumann-to-Dirichlet map's integral kernel. The theoretical analysis proceeds by extending the domain of the inversion operator to a Hilbert space of kernel functions. The resulting extension shares the same stability properties as the original inverse map from kernels to conductivities, but is now amenable to neural operator approximation. Numerical experiments demonstrate that Fourier neural operators excel at reconstructing infinite-dimensional piecewise constant and lognormal conductivities in noisy setups both within and beyond the theory's assumptions. The methodology developed in this paper for EIT exemplifies a broader strategy for addressing nonlinear inverse problems with a noise-aware operator learning framework.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Attenuation Filters for Feedback Delay Networks</title>
<link>https://arxiv.org/abs/2511.20380</link>
<guid>https://arxiv.org/abs/2511.20380</guid>
<content:encoded><![CDATA[
arXiv:2511.20380v1 Announce Type: cross 
Abstract: We introduce a novel method for designing attenuation filters in digital audio reverberation systems based on Feedback Delay Net- works (FDNs). Our approach uses Second Order Sections (SOS) of Infinite Impulse Response (IIR) filters arranged as parametric equalizers (PEQ), enabling fine control over frequency-dependent reverberation decay. Unlike traditional graphic equalizer designs, which require numerous filters per delay line, we propose a scal- able solution where the number of filters can be adjusted. The fre- quency, gain, and quality factor (Q) parameters are shared parame- ters across delay lines and only the gain is adjusted based on delay length. This design not only reduces the number of optimization parameters, but also remains fully differentiable and compatible with gradient-based learning frameworks. Leveraging principles of analog filter design, our method allows for efficient and accu- rate filter fitting using supervised learning. Our method delivers a flexible and differentiable design, achieving state-of-the-art per- formance while significantly reducing computational cost.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections</title>
<link>https://arxiv.org/abs/2511.20418</link>
<guid>https://arxiv.org/abs/2511.20418</guid>
<content:encoded><![CDATA[
arXiv:2511.20418v1 Announce Type: cross 
Abstract: Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\textit{11.6%}$ HOTA improvement at $\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fully Probabilistic Tensor Network for Regularized Volterra System Identification</title>
<link>https://arxiv.org/abs/2511.20457</link>
<guid>https://arxiv.org/abs/2511.20457</guid>
<content:encoded><![CDATA[
arXiv:2511.20457v1 Announce Type: cross 
Abstract: Modeling nonlinear systems with Volterra series is challenging because the number of kernel coefficients grows exponentially with the model order. This work introduces Bayesian Tensor Network Volterra kernel machines (BTN-V), extending the Bayesian Tensor Network framework to Volterra system identification. BTN-V represents Volterra kernels using canonical polyadic decomposition, reducing model complexity from O(I^D) to O(DIR). By treating all tensor components and hyperparameters as random variables, BTN-V provides predictive uncertainty estimation at no additional computational cost. Sparsity-inducing hierarchical priors enable automatic rank determination and the learning of fading-memory behavior directly from data, improving interpretability and preventing overfitting. Empirical results demonstrate competitive accuracy, enhanced uncertainty quantification, and reduced computational cost.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow</title>
<link>https://arxiv.org/abs/2511.20462</link>
<guid>https://arxiv.org/abs/2511.20462</guid>
<content:encoded><![CDATA[
arXiv:2511.20462v1 Announce Type: cross 
Abstract: Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features</title>
<link>https://arxiv.org/abs/2511.20469</link>
<guid>https://arxiv.org/abs/2511.20469</guid>
<content:encoded><![CDATA[
arXiv:2511.20469v1 Announce Type: cross 
Abstract: Dance is an essential component of human culture and serves as a tool for conveying emotions and telling stories. Identifying and distinguishing dance genres based on motion data is a complex problem in human activity recognition, as many styles share similar poses, gestures, and temporal motion patterns. This work presents a lightweight framework for classifying dance styles that determines motion characteristics based on pose estimates extracted from videos. We propose temporal-spatial descriptors inspired by Laban Movement Analysis. These features capture local joint dynamics such as velocity, acceleration, and angular movement of the upper body, enabling a structured representation of spatial coordination. To further encode rhythmic and periodic aspects of movement, we integrate Fast Fourier Transform features that characterize movement patterns in the frequency domain. The proposed approach achieves robust classification of different dance styles with low computational effort, as complex model architectures are not required, and shows that interpretable motion representations can effectively capture stylistic nuances.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular Deep Learning Framework for Assistive Perception: Gaze, Affect, and Speaker Identification</title>
<link>https://arxiv.org/abs/2511.20474</link>
<guid>https://arxiv.org/abs/2511.20474</guid>
<content:encoded><![CDATA[
arXiv:2511.20474v1 Announce Type: cross 
Abstract: Developing comprehensive assistive technologies requires the seamless integration of visual and auditory perception. This research evaluates the feasibility of a modular architecture inspired by core functionalities of perceptive systems like 'Smart Eye.' We propose and benchmark three independent sensing modules: a Convolutional Neural Network (CNN) for eye state detection (drowsiness/attention), a deep CNN for facial expression recognition, and a Long Short-Term Memory (LSTM) network for voice-based speaker identification. Utilizing the Eyes Image, FER2013, and customized audio datasets, our models achieved accuracies of 93.0%, 97.8%, and 96.89%, respectively. This study demonstrates that lightweight, domain-specific models can achieve high fidelity on discrete tasks, establishing a validated foundation for future real-time, multimodal integration in resource-constrained assistive devices.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InferF: Declarative Factorization of AI/ML Inferences over Joins</title>
<link>https://arxiv.org/abs/2511.20489</link>
<guid>https://arxiv.org/abs/2511.20489</guid>
<content:encoded><![CDATA[
arXiv:2511.20489v1 Announce Type: cross 
Abstract: Real-world AI/ML workflows often apply inference computations to feature vectors joined from multiple datasets. To avoid the redundant AI/ML computations caused by repeated data records in the join's output, factorized ML has been proposed to decompose ML computations into sub-computations to be executed on each normalized dataset. However, there is insufficient discussion on how factorized ML could impact AI/ML inference over multi-way joins. To address the limitations, we propose a novel declarative InferF system, focusing on the factorization of arbitrary inference workflows represented as analyzable expressions over the multi-way joins. We formalize our problem to flexibly push down partial factorized computations to qualified nodes in the join tree to minimize the overall inference computation and join costs and propose two algorithms to resolve the problem: (1) a greedy algorithm based on a per-node cost function that estimates the influence on overall latency if a subset of factorized computations is pushed to a node, and (2) a genetic algorithm for iteratively enumerating and evaluating promising factorization plans. We implement InferF on Velox, an open-sourced database engine from Meta, evaluate it on real-world datasets, observed up to 11.3x speedups, and systematically summarized the factors that determine when factorized ML can benefit AI/ML inference workflows.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Informed Loss Function for Boundary-Consistent and Robust Artery Segmentation in DSA Sequences</title>
<link>https://arxiv.org/abs/2511.20501</link>
<guid>https://arxiv.org/abs/2511.20501</guid>
<content:encoded><![CDATA[
arXiv:2511.20501v1 Announce Type: cross 
Abstract: Accurate extraction and segmentation of the cerebral arteries from digital subtraction angiography (DSA) sequences is essential for developing reliable clinical management models of complex cerebrovascular diseases. Conventional loss functions often rely solely on pixel-wise overlap, overlooking the geometric and physical consistency of vascular boundaries, which can lead to fragmented or unstable vessel predictions. To overcome this limitation, we propose a novel \textit{Physics-Informed Loss} (PIL) that models the interaction between the predicted and ground-truth boundaries as an elastic process inspired by dislocation theory in materials physics. This formulation introduces a physics-based regularization term that enforces smooth contour evolution and structural consistency, allowing the network to better capture fine vascular geometry. The proposed loss is integrated into several segmentation architectures, including U-Net, U-Net++, SegFormer, and MedFormer, and evaluated on two public benchmarks: DIAS and DSCA. Experimental results demonstrate that PIL consistently outperforms conventional loss functions such as Cross-Entropy, Dice, Active Contour, and Surface losses, achieving superior sensitivity, F1 score, and boundary coherence. These findings confirm that the incorporation of physics-based boundary interactions into deep neural networks improves both the precision and robustness of vascular segmentation in dynamic angiographic imaging. The implementation of the proposed method is publicly available at https://github.com/irfantahir301/Physicsis_loss.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Modeling with Manifold Percolation</title>
<link>https://arxiv.org/abs/2511.20503</link>
<guid>https://arxiv.org/abs/2511.20503</guid>
<content:encoded><![CDATA[
arXiv:2511.20503v1 Announce Type: cross 
Abstract: Generative modeling is typically framed as learning mapping rules, but from an observer's perspective without access to these rules, the task manifests as disentangling the geometric support from the probability distribution. We propose that Continuum Percolation is uniquely suited for this support analysis, as the sampling process effectively projects high-dimensional density estimation onto a geometric counting problem on the support. In this work, we establish a rigorous isomorphism between the topological phase transitions of Random Geometric Graphs and the underlying data manifold in high-dimensional space. By analyzing the relationship between our proposed Percolation Shift metric and FID, we demonstrate that our metric captures structural pathologies (such as implicit mode collapse) where statistical metrics fail. Finally, we translate this topological phenomenon into a differentiable loss function to guide training. Experimental results confirm that this approach not only prevents manifold shrinkage but drives the model toward a state of "Hyper-Generalization," achieving good fidelity and verified topological expansion.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.20531</link>
<guid>https://arxiv.org/abs/2511.20531</guid>
<content:encoded><![CDATA[
arXiv:2511.20531v1 Announce Type: cross 
Abstract: Visual Language Models (VLMs) are powerful generative tools but often produce factually in- accurate outputs due to a lack of robust reason- ing capabilities. While extensive research has been conducted on integrating external knowl- edge for reasoning in large language models (LLMs), such efforts remain underexplored in VLMs, where the challenge is compounded by the need to bridge multiple modalities seam- lessly. This work introduces a framework for knowledge-guided reasoning in VLMs, leverag- ing structured knowledge graphs for multi-hop verification using image-captioning task to il- lustrate our framework. Our approach enables systematic reasoning across multiple steps, in- cluding visual entity recognition, knowledge graph traversal, and fact-based caption refine- ment. We evaluate the framework using hi- erarchical, triple-based and bullet-point based knowledge representations, analyzing their ef- fectiveness in factual accuracy and logical infer- ence. Empirical results show that our approach improves factual accuracy by approximately 31% on preliminary experiments on a curated dataset of mixtures from Google Landmarks v2, Conceptual captions and Coco captions re- vealing key insights into reasoning patterns and failure modes. This work demonstrates the po- tential of integrating external knowledge for advancing reasoning in VLMs, paving the way for more reliable and knowledgable multimodal systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Monitoring of Cultural Heritage Artifacts Using Semantic Segmentation</title>
<link>https://arxiv.org/abs/2511.20541</link>
<guid>https://arxiv.org/abs/2511.20541</guid>
<content:encoded><![CDATA[
arXiv:2511.20541v1 Announce Type: cross 
Abstract: This paper addresses the critical need for automated crack detection in the preservation of cultural heritage through semantic segmentation. We present a comparative study of U-Net architectures, using various convolutional neural network (CNN) encoders, for pixel-level crack identification on statues and monuments. A comparative quantitative evaluation is performed on the test set of the OmniCrack30k dataset [1] using popular segmentation metrics including Mean Intersection over Union (mIoU), Dice coefficient, and Jaccard index. This is complemented by an out-of-distribution qualitative evaluation on an unlabeled test set of real-world cracked statues and monuments. Our findings provide valuable insights into the capabilities of different CNN- based encoders for fine-grained crack segmentation. We show that the models exhibit promising generalization capabilities to unseen cultural heritage contexts, despite never having been explicitly trained on images of statues or monuments.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>New York Smells: A Large Multimodal Dataset for Olfaction</title>
<link>https://arxiv.org/abs/2511.20544</link>
<guid>https://arxiv.org/abs/2511.20544</guid>
<content:encoded><![CDATA[
arXiv:2511.20544v1 Announce Type: cross 
Abstract: While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines. One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings. We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\times$ more objects than existing olfactory datasets. Our benchmark has three tasks: cross-modal smell-to-image retrieval, recognizing scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. Through experiments on our dataset, we find that visual data enables cross-modal olfactory representation learning, and that our learned olfactory representations outperform widely-used hand-crafted features.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-Temporal Hierarchical Causal Models</title>
<link>https://arxiv.org/abs/2511.20558</link>
<guid>https://arxiv.org/abs/2511.20558</guid>
<content:encoded><![CDATA[
arXiv:2511.20558v1 Announce Type: cross 
Abstract: The abundance of fine-grained spatio-temporal data, such as traffic sensor networks, offers vast opportunities for scientific discovery. However, inferring causal relationships from such observational data remains challenging, particularly due to unobserved confounders that are specific to units (e.g., geographical locations) yet influence outcomes over time. Most existing methods for spatio-temporal causal inference assume that all confounders are observed, an assumption that is often violated in practice. In this paper, we introduce Spatio-Temporal Hierarchical Causal Models (ST-HCMs), a novel graphical framework that extends hierarchical causal modeling to the spatio-temporal domain. At the core of our approach is the Spatio-Temporal Collapse Theorem, which shows that a complex ST-HCM converges to a simpler flat causal model as the amount of subunit data increases. This theoretical result enables a general procedure for causal identification, allowing ST-HCMs to recover causal effects even in the presence of unobserved, time-invariant unit-level confounders, a scenario where standard non-hierarchical models fail. We validate the effectiveness of our framework on both synthetic and real-world datasets, demonstrating its potential for robust causal inference in complex dynamic systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gated Uncertainty-Aware Runtime Dual Invariants for Neural Signal-Controlled Robotics</title>
<link>https://arxiv.org/abs/2511.20570</link>
<guid>https://arxiv.org/abs/2511.20570</guid>
<content:encoded><![CDATA[
arXiv:2511.20570v1 Announce Type: cross 
Abstract: Safety-critical assistive systems that directly decode user intent from neural signals require rigorous guarantees of reliability and trust. We present GUARDIAN (Gated Uncertainty-Aware Runtime Dual Invariants), a framework for real-time neuro-symbolic verification for neural signal-controlled robotics. GUARDIAN enforces both logical safety and physiological trust by coupling confidence-calibrated brain signal decoding with symbolic goal grounding and dual-layer runtime monitoring. On the BNCI2014 motor imagery electroencephalogram (EEG) dataset with 9 subjects and 5,184 trials, the system performs at a high safety rate of 94-97% even with lightweight decoder architectures with low test accuracies (27-46%) and high ECE confidence miscalibration (0.22-0.41). We demonstrate 1.7x correct interventions in simulated noise testing versus at baseline. The monitor operates at 100Hz and sub-millisecond decision latency, making it practically viable for closed-loop neural signal-based systems. Across 21 ablation results, GUARDIAN exhibits a graduated response to signal degradation, and produces auditable traces from intent, plan to action, helping to link neural evidence to verifiable robot action.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaTAS: A Parallel System for Trust Propagation in Neural Networks Using Subjective Logic</title>
<link>https://arxiv.org/abs/2511.20586</link>
<guid>https://arxiv.org/abs/2511.20586</guid>
<content:encoded><![CDATA[
arXiv:2511.20586v1 Announce Type: cross 
Abstract: Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics such as accuracy and precision fail to capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the \emph{Parallel Trust Assessment System (PaTAS)}, a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through \emph{Trust Nodes} and \emph{Trust Functions} that propagate input, parameter, and activation trust across the network. The framework defines a \emph{Parameter Trust Update} mechanism to refine parameter reliability during training and an \emph{Inference-Path Trust Assessment (IPTA)} method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a principled foundation for evaluating model reliability across the AI lifecycle.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Evaluating LLM Alignment by Evaluating LLMs as Judges</title>
<link>https://arxiv.org/abs/2511.20604</link>
<guid>https://arxiv.org/abs/2511.20604</guid>
<content:encoded><![CDATA[
arXiv:2511.20604v1 Announce Type: cross 
Abstract: Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</title>
<link>https://arxiv.org/abs/2511.20629</link>
<guid>https://arxiv.org/abs/2511.20629</guid>
<content:encoded><![CDATA[
arXiv:2511.20629v1 Announce Type: cross 
Abstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Collaboration in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.20639</link>
<guid>https://arxiv.org/abs/2511.20639</guid>
<content:encoded><![CDATA[
arXiv:2511.20639v1 Announce Type: cross 
Abstract: Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MotionV2V: Editing Motion in a Video</title>
<link>https://arxiv.org/abs/2511.20640</link>
<guid>https://arxiv.org/abs/2511.20640</guid>
<content:encoded><![CDATA[
arXiv:2511.20640v1 Announce Type: cross 
Abstract: While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a "motion edit" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating "motion counterfactuals", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition</title>
<link>https://arxiv.org/abs/2511.20641</link>
<guid>https://arxiv.org/abs/2511.20641</guid>
<content:encoded><![CDATA[
arXiv:2511.20641v1 Announce Type: cross 
Abstract: Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concept-Aware Batch Sampling Improves Language-Image Pretraining</title>
<link>https://arxiv.org/abs/2511.20643</link>
<guid>https://arxiv.org/abs/2511.20643</guid>
<content:encoded><![CDATA[
arXiv:2511.20643v1 Announce Type: cross 
Abstract: What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Kernel Neural Networks</title>
<link>https://arxiv.org/abs/2112.07436</link>
<guid>https://arxiv.org/abs/2112.07436</guid>
<content:encoded><![CDATA[
arXiv:2112.07436v3 Announce Type: replace 
Abstract: The convolution operator at the core of many modern neural architectures can effectively be seen as performing a dot product between an input matrix and a filter. While this is readily applicable to data such as images, which can be represented as regular grids in the Euclidean space, extending the convolution operator to work on graphs proves more challenging, due to their irregular structure. In this paper, we propose to use graph kernels, i.e. kernel functions that compute an inner product on graphs, to extend the standard convolution operator to the graph domain. This allows us to define an entirely structural model that does not require computing the embedding of the input graph. Our architecture allows to plug-in any type of graph kernels and has the added benefit of providing some interpretability in terms of the structural masks that are learned during the training process, similarly to what happens for convolutional masks in traditional convolutional neural networks. We perform an extensive ablation study to investigate the model hyper-parameters' impact and show that our model achieves competitive performance on standard graph classification and regression datasets.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions</title>
<link>https://arxiv.org/abs/2301.12250</link>
<guid>https://arxiv.org/abs/2301.12250</guid>
<content:encoded><![CDATA[
arXiv:2301.12250v3 Announce Type: replace 
Abstract: We present a fast, differentially private algorithm for high-dimensional covariance-aware mean estimation with nearly optimal sample complexity. Only exponential-time estimators were previously known to achieve this guarantee. Given $n$ samples from a (sub-)Gaussian distribution with unknown mean $\mu$ and covariance $\Sigma$, our $(\varepsilon,\delta)$-differentially private estimator produces $\tilde{\mu}$ such that $\|\mu - \tilde{\mu}\|_{\Sigma} \leq \alpha$ as long as $n \gtrsim \tfrac d {\alpha^2} + \tfrac{d \sqrt{\log 1/\delta}}{\alpha \varepsilon}+\frac{d\log 1/\delta}{\varepsilon}$. The Mahalanobis error metric $\|\mu - \hat{\mu}\|_{\Sigma}$ measures the distance between $\hat \mu$ and $\mu$ relative to $\Sigma$; it characterizes the error of the sample mean. Our algorithm runs in time $\tilde{O}(nd^{\omega - 1} + nd/\varepsilon)$, where $\omega < 2.38$ is the matrix multiplication exponent.
  We adapt an exponential-time approach of Brown, Gaboardi, Smith, Ullman, and Zakynthinou (2021), giving efficient variants of stable mean and covariance estimation subroutines that also improve the sample complexity to the nearly optimal bound above.
  Our stable covariance estimator can be turned to private covariance estimation for unrestricted subgaussian distributions. With $n\gtrsim d^{3/2}$ samples, our estimate is accurate in spectral norm. This is the first such algorithm using $n= o(d^2)$ samples, answering an open question posed by Alabi et al. (2022). With $n\gtrsim d^2$ samples, our estimate is accurate in Frobenius norm. This leads to a fast, nearly optimal algorithm for private learning of unrestricted Gaussian distributions in TV distance.
  Duchi, Haque, and Kuditipudi (2023) obtained similar results independently and concurrently.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MGAS: Multi-Granularity Architecture Search for Trade-Off Between Model Effectiveness and Efficiency</title>
<link>https://arxiv.org/abs/2310.15074</link>
<guid>https://arxiv.org/abs/2310.15074</guid>
<content:encoded><![CDATA[
arXiv:2310.15074v4 Announce Type: replace 
Abstract: Neural architecture search (NAS) has gained significant traction in automating the design of neural networks. To reduce search time, differentiable architecture search (DAS) reframes the traditional paradigm of discrete candidate sampling and evaluation into a differentiable optimization over a super-net, followed by discretization. However, most existing DAS methods primarily focus on optimizing the coarse-grained operation-level topology, while neglecting finer-grained structures such as filter-level and weight-level patterns. This limits their ability to balance model performance with model size. Additionally, many methods compromise search quality to save memory during the search process. To tackle these issues, we propose Multi-Granularity Differentiable Architecture Search (MG-DARTS), a unified framework which aims to discover both effective and efficient architectures from scratch by comprehensively yet memory-efficiently exploring a multi-granularity search space. Specifically, we improve the existing DAS methods in two aspects. First, we adaptively adjust the retention ratios of searchable units across different granularity levels through adaptive pruning, which is achieved by learning granularity-specific discretization functions along with the evolving architecture. Second, we decompose the super-net optimization and discretization into multiple stages, each operating on a sub-net, and introduce progressive re-evaluation to enable re-pruning and regrowth of previous units, thereby mitigating potential bias. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet demonstrate that MG-DARTS outperforms other state-of-the-art methods in achieving a better trade-off between model accuracy and parameter efficiency. Codes are available at https://github.com/lxy12357/MG_DARTS.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems</title>
<link>https://arxiv.org/abs/2403.15511</link>
<guid>https://arxiv.org/abs/2403.15511</guid>
<content:encoded><![CDATA[
arXiv:2403.15511v2 Announce Type: replace 
Abstract: While intrusion detection systems (IDSs) benefit from the diversity and generalization of IoT data features, the data diversity (e.g., the heterogeneity and high dimensions of data) also makes it difficult to train effective machine learning models in IoT IDSs. This also leads to potentially redundant/noisy features that may decrease the accuracy of the detection engine in IDSs. This paper first introduces a novel neural network architecture called Multiple-Input Auto-Encoder (MIAE). MIAE consists of multiple sub-encoders that can process inputs from different sources with different characteristics. The MIAE model is trained in an unsupervised learning mode to transform the heterogeneous inputs into lower-dimensional representation, which helps classifiers distinguish between normal behaviour and different types of attacks. To distil and retain more relevant features but remove less important/redundant ones during the training process, we further design and embed a feature selection layer right after the representation layer of MIAE resulting in a new model called MIAEFS. This layer learns the importance of features in the representation vector, facilitating the selection of informative features from the representation vector. The results on three IDS datasets, i.e., NSLKDD, UNSW-NB15, and IDS2017, show the superior performance of MIAE and MIAEFS compared to other methods, e.g., conventional classifiers, dimensionality reduction models, unsupervised representation learning methods with different input dimensions, and unsupervised feature selection models. Moreover, MIAE and MIAEFS combined with the Random Forest (RF) classifier achieve accuracy of 96.5% in detecting sophisticated attacks, e.g., Slowloris. The average running time for detecting an attack sample using RF with the representation of MIAE and MIAEFS is approximate 1.7E-6 seconds, whilst the model size is lower than 1 MB.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Diffusion Models for Time Series and Spatio-Temporal Data</title>
<link>https://arxiv.org/abs/2404.18886</link>
<guid>https://arxiv.org/abs/2404.18886</guid>
<content:encoded><![CDATA[
arXiv:2404.18886v4 Announce Type: replace 
Abstract: Diffusion models have been widely used in time series and spatio-temporal data, enhancing generative, inferential, and downstream capabilities. These models are applied across diverse fields such as healthcare, recommendation, climate, energy, audio, and traffic. By separating applications for time series and spatio-temporal data, we offer a structured perspective on model category, task type, data modality, and practical application domain. This study aims to provide a solid foundation for researchers and practitioners, inspiring future innovations that tackle traditional challenges and foster novel solutions in diffusion model-based data mining tasks and applications. For more detailed information, we have open-sourced a repository at https://github.com/yyysjz1997/Awesome-TimeSeries-SpatioTemporal-Diffusion-Model.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Categorical Flow Matching on Statistical Manifolds</title>
<link>https://arxiv.org/abs/2405.16441</link>
<guid>https://arxiv.org/abs/2405.16441</guid>
<content:encoded><![CDATA[
arXiv:2405.16441v4 Announce Type: replace 
Abstract: We introduce Statistical Flow Matching (SFM), a novel and mathematically rigorous flow-matching framework on the manifold of parameterized probability measures inspired by the results from information geometry. We demonstrate the effectiveness of our method on the discrete generation problem by instantiating SFM on the manifold of categorical distributions whose geometric properties remain unexplored in previous discrete generative models. Utilizing the Fisher information metric, we equip the manifold with a Riemannian structure whose intrinsic geometries are effectively leveraged by following the shortest paths of geodesics. We develop an efficient training and sampling algorithm that overcomes numerical stability issues with a diffeomorphism between manifolds. Our distinctive geometric perspective of statistical manifolds allows us to apply optimal transport during training and interpret SFM as following the steepest direction of the natural gradient. Unlike previous models that rely on variational bounds for likelihood estimation, SFM enjoys the exact likelihood calculation for arbitrary probability measures. We manifest that SFM can learn more complex patterns on the statistical manifold where existing models often fail due to strong prior assumptions. Comprehensive experiments on real-world generative tasks ranging from image, text to biological domains further demonstrate that SFM achieves higher sampling quality and likelihood than other discrete diffusion or flow-based models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value Improved Actor Critic Algorithms</title>
<link>https://arxiv.org/abs/2406.01423</link>
<guid>https://arxiv.org/abs/2406.01423</guid>
<content:encoded><![CDATA[
arXiv:2406.01423v3 Announce Type: replace 
Abstract: To learn approximately optimal acting policies for decision problems, modern Actor Critic algorithms rely on deep Neural Networks (DNNs) to parameterize the acting policy and greedification operators to iteratively improve it. The reliance on DNNs suggests an improvement that is gradient based, which is per step much less greedy than the improvement possible by greedier operators such as the greedy update used by Q-learning algorithms. On the other hand, slow changes to the policy can also be beneficial for the stability of the learning process, resulting in a tradeoff between greedification and stability. To better address this tradeoff, we propose to decouple the acting policy from the policy evaluated by the critic. This allows the agent to separately improve the critic's policy (e.g. value improvement) with greedier updates while maintaining the slow gradient-based improvement to the parameterized acting policy. We investigate the convergence of this approach using the popular analysis scheme of generalized Policy Iteration in the finite-horizon domain. Empirically, incorporating value-improvement into the popular off-policy actor-critic algorithms TD3 and SAC significantly improves or matches performance over their respective baselines, across different environments from the DeepMind continuous control domain, with negligible compute and implementation cost.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demystifying Higher-Order Graph Neural Networks</title>
<link>https://arxiv.org/abs/2406.12841</link>
<guid>https://arxiv.org/abs/2406.12841</guid>
<content:encoded><![CDATA[
arXiv:2406.12841v4 Announce Type: replace 
Abstract: Higher-order graph neural networks (HOGNNs) and the related architectures from Topological Deep Learning are an important class of GNN models that harness polyadic relations between vertices beyond plain edges. They have been used to eliminate issues such as over-smoothing or over-squashing, to significantly enhance the accuracy of GNN predictions, to improve the expressiveness of GNN architectures, and for numerous other goals. A plethora of HOGNN models have been introduced, and they come with diverse neural architectures, and even with different notions of what the "higher-order" means. This richness makes it very challenging to appropriately analyze and compare HOGNN models, and to decide in what scenario to use specific ones. To alleviate this, we first design an in-depth taxonomy and a blueprint for HOGNNs. This facilitates designing models that maximize performance. Then, we use our taxonomy to analyze and compare the available HOGNN models. The outcomes of our analysis are synthesized in a set of insights that help to select the most beneficial GNN model in a given scenario, and a comprehensive list of challenges and opportunities for further research into more powerful HOGNNs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Attention Spans: Optimizing LLM Inference Efficiency with Heterogeneous Sliding-Window Lengths</title>
<link>https://arxiv.org/abs/2406.14909</link>
<guid>https://arxiv.org/abs/2406.14909</guid>
<content:encoded><![CDATA[
arXiv:2406.14909v3 Announce Type: replace 
Abstract: Sliding-window attention offers a hardware-efficient solution to the memory and throughput challenges of Large Language Models (LLMs) in long-context scenarios. Existing methods typically employ a single window length across all attention heads and input sizes. However, this uniform approach fails to capture the heterogeneous attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose *Mixture of Attention Spans* (MoA), which automatically tailors distinct sliding-window length configurations to different heads and layers. MoA constructs and navigates a search space of various window lengths and their scaling rules relative to input sizes. It profiles the model, evaluates potential configurations, and pinpoints the optimal length configurations for each head. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer inputs, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by 3.9x with the same average sliding-window length, boosting retrieval accuracy by 1.5-7.1x over the uniform-window baseline across Vicuna-{7B, 13B} and Llama3-{8B, 70B} models. Moreover, MoA narrows the performance gap with full attention, reducing the maximum relative performance drop from 9%-36% to within 5% across three long-context understanding benchmarks. MoA achieves a 1.2-1.4x GPU memory reduction, boosting decode throughput by 6.6-8.2x and 1.7-1.9x over FlashAttention2 and vLLM, with minimal performance impact. Our code is available at: https://github.com/thu-nics/MoA
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LINSCAN -- A Linearity Based Clustering Algorithm</title>
<link>https://arxiv.org/abs/2406.17952</link>
<guid>https://arxiv.org/abs/2406.17952</guid>
<content:encoded><![CDATA[
arXiv:2406.17952v2 Announce Type: replace 
Abstract: DBSCAN and OPTICS are powerful algorithms for identifying clusters of points in domains where few assumptions can be made about the structure of the data. In this paper, we leverage these strengths and introduce a new algorithm, LINSCAN, designed to seek lineated clusters that are difficult to find and isolate with existing methods. In particular, by embedding points as normal distributions approximating their local neighborhoods and leveraging a distance function derived from the Kullback Leibler Divergence, LINSCAN can detect and distinguish lineated clusters that are spatially close but have orthogonal covariances. We demonstrate how LINSCAN can be applied to seismic data to identify active faults, including intersecting faults, and determine their orientation. Finally, we discuss the properties a generalization of DBSCAN and OPTICS must have in order to retain the stability benefits of these algorithms.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopER: Topological Embeddings in Graph Representation Learning</title>
<link>https://arxiv.org/abs/2410.01778</link>
<guid>https://arxiv.org/abs/2410.01778</guid>
<content:encoded><![CDATA[
arXiv:2410.01778v3 Announce Type: replace 
Abstract: Graph embeddings play a critical role in graph representation learning, allowing machine learning models to explore and interpret graph-structured data. However, existing methods often rely on opaque, high-dimensional embeddings, limiting interpretability and practical visualization.
  In this work, we introduce Topological Evolution Rate (TopER), a novel, low-dimensional embedding approach grounded in topological data analysis. TopER simplifies a key topological approach, Persistent Homology, by calculating the evolution rate of graph substructures, resulting in intuitive and interpretable visualizations of graph data. This approach not only enhances the exploration of graph datasets but also delivers competitive performance in graph clustering and classification tasks. Our TopER-based models achieve or surpass state-of-the-art results across molecular, biological, and social network datasets in tasks such as classification, clustering, and visualization.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCNode: Spatial and Contextual Coordinates for Graph Representation Learning</title>
<link>https://arxiv.org/abs/2410.02158</link>
<guid>https://arxiv.org/abs/2410.02158</guid>
<content:encoded><![CDATA[
arXiv:2410.02158v2 Announce Type: replace 
Abstract: Effective node representation lies at the heart of Graph Neural Networks (GNNs), as it directly impacts their ability to perform downstream tasks such as node classification and link prediction. Most existing GNNs, particularly message passing graph neural networks, rely on neighborhood aggregation to iteratively compute node embeddings. While powerful, this paradigm suffers from well-known limitations of oversquashing, oversmoothing, and underreaching that degrade representation quality. More critically, MPGNNs often assume homophily, where connected nodes share similar features or labels, leading to poor generalization in heterophilic graphs where this assumption breaks down.
  To address these challenges, we propose \textit{SCNode}, a \textit{Spatial-Contextual Node Embedding} framework designed to perform consistently well in both homophilic and heterophilic settings. SCNode integrates spatial and contextual information, yielding node embeddings that are not only more discriminative but also structurally aware. Our approach introduces new homophily matrices for understanding class interactions and tendencies. Extensive experiments on benchmark datasets show that SCNode achieves superior performance over conventional GNN models, demonstrating its robustness and adaptability in diverse graph structures.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Searching Latent Program Spaces</title>
<link>https://arxiv.org/abs/2411.08706</link>
<guid>https://arxiv.org/abs/2411.08706</guid>
<content:encoded><![CDATA[
arXiv:2411.08706v3 Announce Type: replace 
Abstract: General intelligence requires systems that acquire new skills efficiently and generalize beyond their training distributions. Although program synthesis approaches have strong generalization power, they face scaling issues due to the large combinatorial spaces that quickly render them impractical, requiring human-generated DSLs or pre-trained priors to narrow this search space. On the other hand, deep learning methods have had high successes, but they lack structured test-time adaptation and rely on heavy stochastic sampling or expensive gradient updates for fine-tuning. In this work, we propose the Latent Program Network (LPN), a novel architecture that builds in test-time search directly into neural models. LPN learns a latent space of implicit programs -- neurally mapping inputs to outputs -- through which it can search using gradients at test time. LPN combines the adaptability of symbolic approaches and the scalability of neural methods. It searches through a compact latent space at test time and bypasses the need for pre-defined domain-specific languages. On a range of programming-by-examples tasks, LPN either outperforms or matches performance compared to in-context learning and test-time training methods. Tested on the ARC-AGI benchmark, we demonstrate that LPN can both learn a compact program space and search through it at test time to adapt to novel tasks. LPN doubles its performance on out-of-distribution tasks when test-time search is switched on.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Feasible Rewards in Multi-Agent Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2411.15046</link>
<guid>https://arxiv.org/abs/2411.15046</guid>
<content:encoded><![CDATA[
arXiv:2411.15046v4 Announce Type: replace 
Abstract: Multi-agent Inverse Reinforcement Learning (MAIRL) aims to recover agent reward functions from expert demonstrations. We characterize the feasible reward set in Markov games, identifying all reward functions that rationalize a given equilibrium. However, equilibrium-based observations are often ambiguous: a single Nash equilibrium can correspond to many reward structures, potentially changing the game's nature in multi-agent systems. We address this by introducing entropy-regularized Markov games, which yield a unique equilibrium while preserving strategic incentives. For this setting, we provide a sample complexity analysis detailing how errors affect learned policy performance. Our work establishes theoretical foundations and practical insights for MAIRL.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain Fusion Controllable Generalization for Cross-Domain Time Series Forecasting from Multi-Domain Integrated Distribution</title>
<link>https://arxiv.org/abs/2412.03068</link>
<guid>https://arxiv.org/abs/2412.03068</guid>
<content:encoded><![CDATA[
arXiv:2412.03068v2 Announce Type: replace 
Abstract: Conventional deep models have achieved unprecedented success in time series forecasting. However, facing the challenge of cross-domain generalization, existing studies utilize statistical prior as prompt engineering fails under the huge distribution shift among various domains. In this paper, a novel time series generalization diffusion model (TimeControl) that pioneers the Domain-Fusion paradigm, systematically integrating information from multiple time series domains into a unified generative process via diffusion models. Unlike the autoregressive models that capture the conditional probabilities of the prediction horizon to the historical sequence, we use the diffusion denoising process to model the mixed distribution of the cross-domain data and generate the prediction sequence for the target domain directly utilizing conditional sampling. The proposed TimeControl contains three pivotal designs: (1) The condition network captures the multi-scale fluctuation patterns from the observation sequence, which are utilized as context representations to guide the denoising network to generate the prediction sequence; (2) Adapter-based fine-tuning strategy, the multi-domain universal representation learned in the pretraining stage is utilized for downstream tasks in target domains; (3) A novel hybrid architecture is designed to align the observation and prediction spaces, enabling TimeControl to generate prediction sequences of arbitrary lengths with flexibility. We conduct extensive experiments on mainstream 49 benchmarks and 30 baselines, and the TimeControl outperforms existing baselines on all data domains, exhibiting superior zero-shot generalization ability.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAM: Generalization in Deep RL with a Robust Adaptation Module</title>
<link>https://arxiv.org/abs/2412.04323</link>
<guid>https://arxiv.org/abs/2412.04323</guid>
<content:encoded><![CDATA[
arXiv:2412.04323v3 Announce Type: replace 
Abstract: The reliable deployment of deep reinforcement learning in real-world settings requires the ability to generalize across a variety of conditions, including both in-distribution scenarios seen during training as well as novel out-of-distribution scenarios. In this work, we present a framework for dynamics generalization in deep reinforcement learning that unifies these two distinct types of generalization within a single architecture. We introduce a robust adaptation module that provides a mechanism for identifying and reacting to both in-distribution and out-of-distribution environment dynamics, along with a joint training pipeline that combines the goals of in-distribution adaptation and out-of-distribution robustness. Our algorithm GRAM achieves strong generalization performance across in-distribution and out-of-distribution scenarios upon deployment, which we demonstrate through extensive simulation and hardware locomotion experiments on a quadruped robot.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2502.07154</link>
<guid>https://arxiv.org/abs/2502.07154</guid>
<content:encoded><![CDATA[
arXiv:2502.07154v4 Announce Type: replace 
Abstract: Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be ${\it misaligned}$ with pass@N in that pass@N accuracy ${\it decreases}$ with longer training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARBoids: Adaptive Residual Reinforcement Learning With Boids Model for Cooperative Multi-USV Target Defense</title>
<link>https://arxiv.org/abs/2502.18549</link>
<guid>https://arxiv.org/abs/2502.18549</guid>
<content:encoded><![CDATA[
arXiv:2502.18549v3 Announce Type: replace 
Abstract: The target defense problem (TDP) for unmanned surface vehicles (USVs) concerns intercepting an adversarial USV before it breaches a designated target region, using one or more defending USVs. A particularly challenging scenario arises when the attacker exhibits superior maneuverability compared to the defenders, significantly complicating effective interception. To tackle this challenge, this letter introduces ARBoids, a novel adaptive residual reinforcement learning framework that integrates deep reinforcement learning (DRL) with the biologically inspired, force-based Boids model. Within this framework, the Boids model serves as a computationally efficient baseline policy for multi-agent coordination, while DRL learns a residual policy to adaptively refine and optimize the defenders' actions. The proposed approach is validated in a high-fidelity Gazebo simulation environment, demonstrating superior performance over traditional interception strategies, including pure force-based approaches and vanilla DRL policies. Furthermore, the learned policy exhibits strong adaptability to attackers with diverse maneuverability profiles, highlighting its robustness and generalization capability. The code of ARBoids will be released upon acceptance of this letter.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient Training of Graph Neural Networks: A Multiscale Approach</title>
<link>https://arxiv.org/abs/2503.19666</link>
<guid>https://arxiv.org/abs/2503.19666</guid>
<content:encoded><![CDATA[
arXiv:2503.19666v4 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have become powerful tools for learning from graph-structured data, finding applications across diverse domains. However, as graph sizes and connectivity increase, standard GNN training methods face significant computational and memory challenges, limiting their scalability and efficiency. In this paper, we present a novel framework for efficient multiscale training of GNNs. Our approach leverages hierarchical graph representations and subgraphs, enabling the integration of information across multiple scales and resolutions. By utilizing coarser graph abstractions and subgraphs, each with fewer nodes and edges, we significantly reduce computational overhead during training. Building on this framework, we propose a suite of scalable training strategies, including coarse-to-fine learning, subgraph-to-full-graph transfer, and multiscale gradient computation. We also provide some theoretical analysis of our methods and demonstrate their effectiveness across various datasets and learning tasks. Our results show that multiscale training can substantially accelerate GNN training for large scale problems while maintaining, or even improving, predictive performance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Position: Beyond Euclidean -- Foundation Models Should Embrace Non-Euclidean Geometries</title>
<link>https://arxiv.org/abs/2504.08896</link>
<guid>https://arxiv.org/abs/2504.08896</guid>
<content:encoded><![CDATA[
arXiv:2504.08896v2 Announce Type: replace 
Abstract: In the era of foundation models and Large Language Models (LLMs), Euclidean space has been the de facto geometric setting for machine learning architectures. However, recent literature has demonstrated that this choice comes with fundamental limitations. At a large scale, real-world data often exhibits inherently non-Euclidean structures, such as multi-way relationships, hierarchies, symmetries, and non-isotropic scaling, in a variety of domains, such as languages, vision, and the natural sciences. It is challenging to effectively capture these structures within the constraints of Euclidean spaces. This position paper argues that moving beyond Euclidean geometry is not merely an optional enhancement but a necessity to maintain the scaling law for the next-generation of foundation models. By adopting these geometries, foundation models could more efficiently leverage the aforementioned structures. Task-aware adaptability that dynamically reconfigures embeddings to match the geometry of downstream applications could further enhance efficiency and expressivity. Our position is supported by a series of theoretical and empirical investigations of prevalent foundation models. Finally, we outline a roadmap for integrating non-Euclidean geometries into foundation models, including strategies for building geometric foundation models via fine-tuning, training from scratch, and hybrid approaches.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization</title>
<link>https://arxiv.org/abs/2505.12366</link>
<guid>https://arxiv.org/abs/2505.12366</guid>
<content:encoded><![CDATA[
arXiv:2505.12366v4 Announce Type: replace 
Abstract: The recent success and openness of DeepSeek-R1 have brought widespread attention to Group Relative Policy Optimization (GRPO) as a reinforcement learning method for large reasoning models (LRMs). In this work, we analyze the GRPO objective under a binary reward setting and reveal an inherent limitation of question-level difficulty bias. We also identify a connection between GRPO and traditional discriminative methods in supervised learning. Motivated by these insights, we introduce a new Discriminative Constrained Optimization (DisCO) framework for reinforcing LRMs, grounded in the principle of discriminative learning. The main differences between DisCO and GRPO and its recent variants are: (1) it replaces the group relative objective with a discriminative objective defined by a scoring function; (2) it abandons clipping-based surrogates in favor of non-clipping RL surrogate objectives used as scoring functions; (3) it employs a simple yet effective constrained optimization approach to enforce the KL divergence constraint. As a result, DisCO offers notable advantages over GRPO and its variants: (i) it completely eliminates difficulty bias by adopting discriminative objectives; (ii) it addresses the entropy instability in GRPO and its variants through the use of non-clipping scoring functions and a constrained optimization approach, yielding long and stable training dynamics; (iii) it allows the incorporation of advanced discriminative learning techniques to address data imbalance, where a significant number of questions have more negative than positive generated answers during training. Our experiments on enhancing the mathematical reasoning capabilities of SFT-finetuned models show that DisCO significantly outperforms GRPO and its improved variants such as DAPO, achieving average gains of 7\% over GRPO and 6\% over DAPO across six benchmark tasks for an 1.5B model.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules</title>
<link>https://arxiv.org/abs/2505.13858</link>
<guid>https://arxiv.org/abs/2505.13858</guid>
<content:encoded><![CDATA[
arXiv:2505.13858v2 Announce Type: replace 
Abstract: Deep learning models are increasingly deployed in safety-critical tasks where predictions must satisfy hard constraints, such as physical laws, fairness requirements, or safety limits. However, standard architectures lack built-in mechanisms to enforce such constraints, and existing approaches based on regularization or projection are often limited to simple constraints, computationally expensive, or lack feasibility guarantees. This paper proposes a model-agnostic framework for enforcing input-dependent linear equality and inequality constraints on neural network outputs. The architecture combines a task network trained for prediction accuracy with a safe network trained using decision rules from the stochastic and robust optimization literature to ensure feasibility across the entire input space. The final prediction is a convex combination of the two subnetworks, guaranteeing constraint satisfaction during both training and inference without iterative procedures or runtime optimization. We prove that the architecture is a universal approximator of constrained functions and derive computationally tractable formulations based on linear decision rules. Empirical results on benchmark regression tasks show that our method consistently satisfies constraints while maintaining competitive accuracy and low inference latency.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator</title>
<link>https://arxiv.org/abs/2505.16690</link>
<guid>https://arxiv.org/abs/2505.16690</guid>
<content:encoded><![CDATA[
arXiv:2505.16690v5 Announce Type: replace 
Abstract: Post-training of large language models is essential for adapting pre-trained language models (PLMs) to align with human preferences and downstream tasks. While PLMs typically exhibit well-calibrated confidence, post-trained language models (PoLMs) often suffer from over-confidence, assigning high confidence to both correct and incorrect outputs, which can undermine reliability in critical applications. A major obstacle in calibrating PoLMs is the scarcity of labeled data for individual downstream tasks. To address this, we propose Disagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to optimize the parameters (e.g., temperature $\tau$) in post-hoc confidence calibration. Our method is motivated by the under-confidence issue caused by prediction disagreement between the PLM and PoLM while aligning their confidence via temperature scaling. Theoretically, the PLM's confidence underestimates PoLM's prediction accuracy on disagreement examples, causing a larger $\tau$ and producing under-confident predictions. DACA mitigates this by selectively using only agreement examples for calibration, effectively decoupling the influence of disagreement. In this manner, our method avoids an overly large $\tau$ in temperature scaling caused by disagreement examples, improving calibration performance. Extensive experiments demonstrate the effectiveness of our method, improving the average ECE of open-sourced and API-based LLMs (e.g. GPT-4o) by up to 15.08$\%$ on common benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation</title>
<link>https://arxiv.org/abs/2505.22994</link>
<guid>https://arxiv.org/abs/2505.22994</guid>
<content:encoded><![CDATA[
arXiv:2505.22994v2 Announce Type: replace 
Abstract: One frequently wishes to learn a range of similar tasks as efficiently as possible, re-using knowledge across tasks. In artificial neural networks, this is typically accomplished by conditioning a network upon task context by injecting context as input. Brains have a different strategy: the parameters themselves are modulated as a function of various neuromodulators such as serotonin. Here, we take inspiration from neuromodulation and propose to learn weights which are smoothly parameterized functions of task context variables. Rather than optimize a weight vector, i.e. a single point in weight space, we optimize a smooth manifold in weight space with a predefined topology. To accomplish this, we derive a formal treatment of optimization of manifolds as the minimization of a loss functional subject to a constraint on volumetric movement, analogous to gradient descent. During inference, conditioning selects a single point on this manifold which serves as the effective weight matrix for a particular sub-task. This strategy for conditioning has two main advantages. First, the topology of the manifold (whether a line, circle, or torus) is a convenient lever for inductive biases about the relationship between tasks. Second, learning in one state smoothly affects the entire manifold, encouraging generalization across states. To verify this, we train manifolds with several topologies, including straight lines in weight space (for conditioning on e.g. noise level in input data) and ellipses (for rotated images). Despite their simplicity, these parameterizations outperform conditioning identical networks by input concatenation and better generalize to out-of-distribution samples. These results suggest that modulating weights over low-dimensional manifolds offers a principled and effective alternative to traditional conditioning.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning</title>
<link>https://arxiv.org/abs/2505.24298</link>
<guid>https://arxiv.org/abs/2505.24298</guid>
<content:encoded><![CDATA[
arXiv:2505.24298v4 Announce Type: replace 
Abstract: Reinforcement learning (RL) has become a dominant paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous, alternating generation and training in a batch setting where rollouts in each training batch are generated by the same model. This approach stabilizes RL training but suffers from severe system-level inefficiency: generation must wait until the longest output in the batch is completed before model updates, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77$\times$ training speedup compared to synchronous systems with the same number of GPUs and matched or improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling</title>
<link>https://arxiv.org/abs/2506.07902</link>
<guid>https://arxiv.org/abs/2506.07902</guid>
<content:encoded><![CDATA[
arXiv:2506.07902v2 Announce Type: replace 
Abstract: Recent advances in generative modeling -- particularly diffusion models and flow matching -- have achieved remarkable success in synthesizing discrete data such as images and videos. However, adapting these models to physical applications remains challenging, as the quantities of interest are continuous functions governed by complex physical laws. Here, we introduce $\textbf{FunDiff}$, a novel framework for generative modeling in function spaces. FunDiff combines a latent diffusion process with a function autoencoder architecture to handle input functions with varying discretizations, generate continuous functions evaluable at arbitrary locations, and seamlessly incorporate physical priors. These priors are enforced through architectural constraints or physics-informed loss functions, ensuring that generated samples satisfy fundamental physical laws. We theoretically establish minimax optimality guarantees for density estimation in function spaces, showing that diffusion-based estimators achieve optimal convergence rates under suitable regularity conditions. We demonstrate the practical effectiveness of FunDiff across diverse applications in fluid dynamics and solid mechanics. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy and low-resolution data. Code and datasets are publicly available at https://github.com/sifanexisted/fundiff.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Multimodal Graph Large Language Model</title>
<link>https://arxiv.org/abs/2506.09738</link>
<guid>https://arxiv.org/abs/2506.09738</guid>
<content:encoded><![CDATA[
arXiv:2506.09738v2 Announce Type: replace 
Abstract: Multi-modal graphs, which integrate diverse multi-modal features and relations, are ubiquitous in real-world applications. However, existing multi-modal graph learning methods are typically trained from scratch for specific graph data and tasks, failing to generalize across various multi-modal graph data and tasks. To bridge this gap, we explore the potential of Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across diverse multi-modal graph data and tasks. We propose a unified framework of multi-modal graph data, task, and model, discovering the inherent multi-granularity and multi-scale characteristics in multi-modal graphs. Specifically, we present five key desired characteristics for MG-LLM: 1) unified space for multi-modal structures and attributes, 2) capability of handling diverse multi-modal graph tasks, 3) multi-modal graph in-context learning, 4) multi-modal graph interaction with natural language, and 5) multi-modal graph reasoning. We then elaborate on the key challenges, review related works, and highlight promising future research directions towards realizing these ambitious characteristics. Finally, we summarize existing multi-modal graph datasets pertinent for model training. We believe this paper can contribute to the ongoing advancement of the research towards MG-LLM for generalization across multi-modal graph data and tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Vision-Language Models for Evaluating World Models</title>
<link>https://arxiv.org/abs/2506.17967</link>
<guid>https://arxiv.org/abs/2506.17967</guid>
<content:encoded><![CDATA[
arXiv:2506.17967v2 Announce Type: replace 
Abstract: World models - generative models that simulate environment dynamics conditioned on past observations and actions - are gaining prominence in planning, simulation, and embodied AI. However, evaluating their rollouts remains a fundamental challenge, requiring fine-grained, temporally grounded assessment of action alignment and semantic consistency - capabilities not captured by existing metrics. Vision-Language Models (VLMs) have shown promise as automatic evaluators of generative content due to their strong multimodal reasoning abilities. Yet, their use in fine-grained, temporally sensitive evaluation tasks remains limited and requires targeted adaptation. We introduce an evaluation protocol targeting two recognition tasks - action recognition and character recognition - each assessed across binary, multiple-choice, and open-ended formats. To support this, we present UNIVERSE (UNIfied Vision-language Evaluator for Rollouts in Simulated Environments), a VLM-based evaluator for video world model rollouts adapted under data and compute constraints. In our extensive experiments totaling over 5,154 GPU-days, we explore full, partial, and parameter-efficient adaptation methods across various task formats, context lengths, sampling methods, and data compositions. The resulting unified evaluator achieves parity with task-specific checkpoints. Human studies across seven diverse environments confirm strong alignment with human judgments, establishing UNIVERSE as a lightweight, adaptable, and semantics-aware evaluator for video world models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-equilibrium Annealed Adjoint Sampler</title>
<link>https://arxiv.org/abs/2506.18165</link>
<guid>https://arxiv.org/abs/2506.18165</guid>
<content:encoded><![CDATA[
arXiv:2506.18165v3 Announce Type: replace 
Abstract: Recently, there has been significant progress in learning-based diffusion samplers, which aim to sample from a given unnormalized density. Many of these approaches formulate the sampling task as a stochastic optimal control (SOC) problem using a canonical uninformative reference process, which limits their ability to efficiently guide trajectories toward the target distribution. In this work, we propose the Non-Equilibrium Annealed Adjoint Sampler (NAAS), a novel SOC-based diffusion framework that employs annealed reference dynamics as a non-stationary base SDE. This annealing structure provides a natural progression toward the target distribution and generates informative reference trajectories, thereby enhancing the stability and efficiency of learning the control. Owing to our SOC formulation, our framework can incorporate a variety of SOC solvers, thereby offering high flexibility in algorithmic design. As one instantiation, we employ a lean adjoint system inspired by adjoint matching, enabling efficient and scalable training. We demonstrate the effectiveness of NAAS across a range of tasks, including sampling from classical energy landscapes and molecular Boltzmann distributions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting</title>
<link>https://arxiv.org/abs/2506.20024</link>
<guid>https://arxiv.org/abs/2506.20024</guid>
<content:encoded><![CDATA[
arXiv:2506.20024v2 Announce Type: replace 
Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most applications in high-dimensional complex systems predict future states individually. This approach struggles to model complex temporal dependencies and fails to explicitly account for the progressive growth of uncertainty inherent to the systems. While rolling diffusion frameworks, which apply increasing noise to forecasts at longer lead times, have been proposed to address this, their integration with state-of-the-art, high-fidelity diffusion techniques remains a significant challenge. We tackle this problem by introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to successfully unify a rolling forecast structure with the principled, performant design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM components-its noise schedule, network preconditioning, and Heun sampler-to the rolling forecast setting. The success of this integration is driven by three key contributions: (i) a novel loss weighting scheme that focuses model capacity on the mid-range forecast horizons where determinism gives way to stochasticity; (ii) an efficient initialization strategy using a pre-trained EDM for the initial window; and (iii) a bespoke hybrid sequence architecture for robust spatiotemporal feature extraction under progressive denoising. On 2D Navier-Stokes simulations and ERA5 global weather forecasting at 1.5-degree resolution, ERDM consistently outperforms key diffusion-based baselines, including conditional autoregressive EDM. ERDM offers a flexible and powerful general framework for tackling diffusion-based dynamics forecasting problems where modeling uncertainty propagation is paramount.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Koopman operator-based discussion on partial observation in stochastic systems</title>
<link>https://arxiv.org/abs/2506.21844</link>
<guid>https://arxiv.org/abs/2506.21844</guid>
<content:encoded><![CDATA[
arXiv:2506.21844v2 Announce Type: replace 
Abstract: It is sometimes difficult to achieve a complete observation for a full set of observables, and partial observations are necessary. For deterministic systems, the Mori-Zwanzig formalism provides a theoretical framework for handling partial observations. Recently, data-driven algorithms based on the Koopman operator theory have made significant progress, and there is a discussion to connect the Mori-Zwanzig formalism with the Koopman operator theory. In this work, we discuss the effects of partial observation in stochastic systems using the Koopman operator theory. The discussion clarifies the importance of distinguishing the state space and the function space in stochastic systems. Even in stochastic systems, the delay-embedding technique is beneficial for partial observation, and several numerical experiments show a power-law behavior of error with respect to the amplitude of the additive noise. We also discuss the relation between the exponent of the power-law behavior and the effects of partial observation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Constrained Language Generation via Self-Distilled Twisted Sequential Monte Carlo</title>
<link>https://arxiv.org/abs/2507.02315</link>
<guid>https://arxiv.org/abs/2507.02315</guid>
<content:encoded><![CDATA[
arXiv:2507.02315v2 Announce Type: replace 
Abstract: Recent work has framed constrained text generation with autoregressive language models as a probabilistic inference problem. Among these, Zhao et al. (2024) introduced a promising approach based on twisted Sequential Monte Carlo, which incorporates learned twist functions and twist-induced proposals to guide the generation process. However, in constrained generation settings where the target distribution concentrates on outputs that are unlikely under the base model, learning becomes challenging due to sparse and uninformative reward signals. We show that iteratively refining the base model through self-distillation alleviates this issue by making the model progressively more aligned with the target, leading to substantial gains in generation quality.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAlloc: Enhancing Memory Efficiency in Large-Scale Model Training with Spatio-Temporal Planning</title>
<link>https://arxiv.org/abs/2507.16274</link>
<guid>https://arxiv.org/abs/2507.16274</guid>
<content:encoded><![CDATA[
arXiv:2507.16274v2 Announce Type: replace 
Abstract: The rapid scaling of large language models (LLMs) has significantly increased GPU memory pressure, which is further aggravated by training optimization techniques such as virtual pipeline and recomputation that disrupt tensor lifespans and introduce considerable memory fragmentation. Such fragmentation stems from the use of online GPU memory allocators in popular deep learning frameworks like PyTorch, which disregard tensor lifespans. As a result, this inefficiency can waste as much as 43% of memory and trigger out-of-memory errors, undermining the effectiveness of optimization methods. To address this, we introduce STAlloc, a GPU memory allocator for deep learning frameworks that reduces fragmentation by exploiting the spatial and temporal regularity in memory allocation behaviors of training workloads. STAlloc introduces a novel paradigm that combines offline planning with online allocation. The offline planning leverages spatio-temporal regularities to generate a near-optimal allocation plan, while the online allocation handles complex and dynamic models such as Mixture-of-Experts (MoE). Built as a pluggable PyTorch memory allocator, STAlloc reduces fragmentation ratio on average by 85.1% (up to 100%) across both dense and MoE models, with negligible overhead. This enables more efficient, high-throughput training configurations and improves throughput performance by up to 32.5%.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge</title>
<link>https://arxiv.org/abs/2508.00901</link>
<guid>https://arxiv.org/abs/2508.00901</guid>
<content:encoded><![CDATA[
arXiv:2508.00901v3 Announce Type: replace 
Abstract: Modern large language models (LLMs) demonstrate exceptional performance on knowledge-intensive tasks, yet the theoretical mechanisms underlying knowledge acquisition (storage and memorization) during pre-training and extraction (retrieval and recall) during inference after fine-tuning remain poorly understood. Although prior theoretical studies have explored these processes through analyses of training dynamics, they overlook critical components essential for a comprehensive theory: (1) the multi-layer perceptron (MLP), empirically identified as the primary module for knowledge storage; (2) out-of-distribution (OOD) adaptivity, which enables LLMs to generalize to unseen scenarios post-pre-training; and (3) next-token prediction, the standard autoregressive objective that encodes knowledge as conditional probabilities. In this work, we introduce, to the best of our knowledge, the first theoretical framework that addresses these limitations by examining the training dynamics of one-layer transformers. Under regularity assumptions, we establish that: (i) transformers attain near-optimal training loss during pre-training, demonstrating effective knowledge acquisition; (ii) given a sufficiently large fine-tuning dataset and appropriate data multiplicity conditions, transformers achieve low generalization error on factual knowledge acquired during pre-training but not revisited in fine-tuning, indicating robust knowledge extraction; and (iii) violation of these conditions leads to elevated generalization error, manifesting as hallucinations. Our analysis encompasses both full fine-tuning and low-rank fine-tuning, yielding insights into the efficacy of practical low-rank adaptation methods. We validate our theoretical findings through experiments on synthetic datasets and the real-world PopQA benchmark, employing GPT-2 and Llama-3.2-1B models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable neural network-based blackbox optimization</title>
<link>https://arxiv.org/abs/2508.03827</link>
<guid>https://arxiv.org/abs/2508.03827</guid>
<content:encoded><![CDATA[
arXiv:2508.03827v2 Announce Type: replace 
Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox optimization that leverages a Gaussian process (GP) model and an acquisition function to guide future sampling. While effective in low-dimensional settings, BO faces scalability challenges in high-dimensional spaces and with large number of function evaluations due to the computational complexity of GP models. In contrast, neural networks (NNs) offer better scalability and can model complex functions, which led to the development of NN-based BO approaches. However, these methods typically rely on estimating model uncertainty in NN prediction -- a process that is often computationally intensive and complex, particularly in high dimensions. To address these limitations, a novel method, called scalable neural network-based blackbox optimization (SNBO), is proposed that does not rely on model uncertainty estimation. Specifically, SNBO adds new samples using separate criteria for exploration and exploitation, while adaptively controlling the sampling region to ensure efficient optimization. SNBO is evaluated on a range of optimization problems spanning from 10 to 102 dimensions and compared against four state-of-the-art baseline algorithms. Across the majority of test problems, SNBO attains function values better than the best-performing baseline algorithm, while requiring 40-60% fewer function evaluations and reducing the runtime by at least an order of magnitude.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Reward Model via Sparse Autoencoder</title>
<link>https://arxiv.org/abs/2508.08746</link>
<guid>https://arxiv.org/abs/2508.08746</guid>
<content:encoded><![CDATA[
arXiv:2508.08746v5 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align LLM behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of LLM-based RM into an interpretable, sparse, and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at https://github.com/schrieffer-z/sarm.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Up Active Testing to Large Language Models</title>
<link>https://arxiv.org/abs/2508.09093</link>
<guid>https://arxiv.org/abs/2508.09093</guid>
<content:encoded><![CDATA[
arXiv:2508.09093v2 Announce Type: replace 
Abstract: Active testing enables label-efficient evaluation of predictive models through careful data acquisition, but it can pose a significant computational cost. We identify cost-saving measures that enable active testing to be scaled up to large language models (LLMs). In particular we show that the surrogate model used to guide data acquisition can be constructed cheaply using in-context learning, does not require updating within an active-testing loop, and can be smaller than the target model. We even find we can make good data-acquisition decisions without making predictions with the target model. As a result we are able to achieve much more accurate evaluations of LLM performance relative to using randomly acquired data. We additionally introduce a bootstrap estimator of evaluation error, which we show to be a useful indicator of how well active testing is working within a single run.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation</title>
<link>https://arxiv.org/abs/2508.11086</link>
<guid>https://arxiv.org/abs/2508.11086</guid>
<content:encoded><![CDATA[
arXiv:2508.11086v3 Announce Type: replace 
Abstract: Watch time is widely used as a proxy for user satisfaction in video recommendation platforms. However, raw watch times are influenced by confounding factors such as video duration, popularity, and individual user behaviors, potentially distorting preference signals and resulting in biased recommendation models. We propose a novel relative advantage debiasing framework that corrects watch time by comparing it to empirically derived reference distributions conditioned on user and item groups. This approach yields a quantile-based preference signal and introduces a two-stage architecture that explicitly separates distribution estimation from preference learning. Additionally, we present distributional embeddings to efficiently parameterize watch-time quantiles without requiring online sampling or storage of historical data. Both offline and online experiments demonstrate significant improvements in recommendation accuracy and robustness compared to existing baseline methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy</title>
<link>https://arxiv.org/abs/2508.12145</link>
<guid>https://arxiv.org/abs/2508.12145</guid>
<content:encoded><![CDATA[
arXiv:2508.12145v4 Announce Type: replace 
Abstract: Recently, autoencoders (AEs) have gained interest for creating parametric and invertible projections of multidimensional data. Parametric projections make it possible to embed new, unseen samples without recalculating the entire projection, while invertible projections allow the synthesis of new data instances. However, existing methods perform poorly when dealing with out-of-distribution samples in either the data or embedding space. Thus, we propose DE-VAE, an uncertainty-aware variational AE using differential entropy (DE) to improve the learned parametric and invertible projections. Given a fixed projection, we train DE-VAE to learn a mapping into 2D space and an inverse mapping back to the original space. We conduct quantitative and qualitative evaluations on four well-known datasets, using UMAP and t-SNE as baseline projection methods. Our findings show that DE-VAE can create parametric and inverse projections with comparable accuracy to other current AE-based approaches while enabling the analysis of embedding uncertainty.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery</title>
<link>https://arxiv.org/abs/2508.17681</link>
<guid>https://arxiv.org/abs/2508.17681</guid>
<content:encoded><![CDATA[
arXiv:2508.17681v4 Announce Type: replace 
Abstract: Bold claims about AI's role in science-from "AGI will cure all diseases" to promises of radically accelerated discovery-raise a central epistemic question: do large language models (LLMs) truly generate new knowledge, or do they merely remix memorized fragments? We propose unlearning-as-ablation as a falsifiable probe of constructive scientific discovery. The idea is to systematically remove a target result together with its forget-closure (supporting lemmas, paraphrases, and multi-hop entailments) and then evaluate whether the model can re-derive the result from only permitted axioms and tools. Success would indicate generative capability beyond recall; failure would expose current limits. Unlike prevailing motivations for unlearning-privacy, copyright, or safety-our framing repositions it as an epistemic probe for AI-for-Science. We outline a minimal pilot in mathematics and algorithms to illustrate feasibility, and sketch how the same approach could later be extended to domains such as physics or chemistry. This is a position paper: our contribution is conceptual and methodological, not empirical. We aim to stimulate discussion on how principled ablation tests could help distinguish models that reconstruct knowledge from those that merely retrieve it, and how such probes might guide the next generation of AI-for-Science benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Inference in a Chess-Playing Neural Network</title>
<link>https://arxiv.org/abs/2508.21380</link>
<guid>https://arxiv.org/abs/2508.21380</guid>
<content:encoded><![CDATA[
arXiv:2508.21380v2 Announce Type: replace 
Abstract: Do neural networks build their representations through smooth, gradual refinement, or via more complex computational processes? We investigate this by extending the logit lens to analyze the policy network of Leela Chess Zero, a superhuman chess engine. Although playing strength and puzzle-solving ability improve consistently across layers, capability progression occurs in distinct computational phases with move preferences undergoing continuous reevaluation--move rankings remain poorly correlated with final outputs until late, and correct puzzle solutions found in middle layers are sometimes overridden. This late-layer reversal is accompanied by concept preference analyses showing final layers prioritize safety over aggression, suggesting a mechanism by which heuristic priors can override tactical solutions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vendi Information Gain for Active Learning and its Application to Ecology</title>
<link>https://arxiv.org/abs/2509.10390</link>
<guid>https://arxiv.org/abs/2509.10390</guid>
<content:encoded><![CDATA[
arXiv:2509.10390v4 Announce Type: replace 
Abstract: While monitoring biodiversity through camera traps has become an important endeavor for ecological research, identifying species in the captured image data remains a major bottleneck due to limited labeling resources. Active learning -- a machine learning paradigm that selects the most informative data to label and train a predictive model -- offers a promising solution, but typically focuses on uncertainty in the individual predictions without considering uncertainty across the entire dataset. We introduce a new active learning policy, Vendi information gain (VIG), that selects images based on their impact on dataset-wide prediction uncertainty, capturing both informativeness and diversity. We applied VIG to the Snapshot Serengeti dataset and compared it against common active learning methods. VIG needs only 3% of the available data to reach 75% accuracy, a level that baselines require more than 10% of the data to achieve. With 10% of the data, VIG attains 88% predictive accuracy, 12% higher than the best of the baselines. This improvement in performance is consistent across metrics and batch sizes, and we show that VIG also collects more diverse data in the feature space. VIG has broad applicability beyond ecology, and our results highlight its value for biodiversity monitoring in data-limited environments.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[
arXiv:2509.22601v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent's own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL, where a replay buffer stores good experience for off-policy update, by gradually steering the policy entropy across stages. Specifically, the proposed curriculum scheduling harmonizes intrinsic reward shaping and self-imitation to 1) expedite exploration via frequent tool interactions at the beginning, and 2) strengthen exploitation of successful tactics upon convergence towards familiarity with the environment. We also combine bag-of-tricks of industrial RL optimizations for a strong baseline Dr.BoT to demonstrate our effectiveness. In ALFWorld and WebShop, SPEAR increases the success rates of GRPO/GiGPO/Dr.BoT by up to 16.1%/5.1%/8.6% and 20.7%/11.8%/13.9%, respectively. In AIME24 and AIME25, SPEAR boosts Dr.BoT by up to 3.8% and 6.1%, respectively. Such gains incur only 10%-25% extra theoretical complexity and negligible runtime overhead in practice, demonstrating the plug-and-play scalability of SPEAR.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simple, Fast and Efficient Injective Manifold Density Estimation with Random Projections</title>
<link>https://arxiv.org/abs/2509.25228</link>
<guid>https://arxiv.org/abs/2509.25228</guid>
<content:encoded><![CDATA[
arXiv:2509.25228v2 Announce Type: replace 
Abstract: We introduce Random Projection Flows (RPFs), a principled framework for injective normalizing flows that leverages tools from random matrix theory and the geometry of random projections. RPFs employ random semi-orthogonal matrices, drawn from Haar-distributed orthogonal ensembles via QR decomposition of Gaussian matrices, to project data into lower-dimensional latent spaces for the base distribution. Unlike PCA-based flows or learned injective maps, RPFs are plug-and-play, efficient, and yield closed-form expressions for the Riemannian volume correction term. We demonstrate that RPFs are both theoretically grounded and practically effective, providing a strong baseline for generative modeling and a bridge between random projection theory and normalizing flows.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control</title>
<link>https://arxiv.org/abs/2510.01508</link>
<guid>https://arxiv.org/abs/2510.01508</guid>
<content:encoded><![CDATA[
arXiv:2510.01508v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) applications in Clinical Decision Support Systems (CDSS) frequently encounter skepticism because models may recommend inoperable dosing decisions. We propose an end-to-end offline RL framework for dual vasopressor administration in Intensive Care Units (ICUs) that directly addresses this challenge through principled action space design. Our method integrates discrete, continuous, and directional dosing strategies with conservative Q-learning and incorporates a novel recurrent modeling using a replay buffer to capture temporal dependencies in ICU time-series data. Our comparative analysis of norepinephrine dosing strategies across different action space formulations reveals that the designed action spaces improve interpretability and facilitate clinical adoption while preserving efficacy. Empirical results on eICU and MIMIC demonstrate that action space design profoundly influences learned behavioral policies. Compared with baselines, the proposed methods achieve more than 3x expected reward improvements, while aligning with established clinical protocols.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models</title>
<link>https://arxiv.org/abs/2510.03263</link>
<guid>https://arxiv.org/abs/2510.03263</guid>
<content:encoded><![CDATA[
arXiv:2510.03263v2 Announce Type: replace 
Abstract: The impressive capability of modern text-to-image models to generate realistic visuals has come with a serious drawback: they can be misused to create harmful, deceptive or unlawful content. This has accelerated the push for machine unlearning. This new field seeks to selectively remove specific knowledge from a model's training data without causing a drop in its overall performance. However, it turns out that actually forgetting a given concept is an extremely difficult task. Models exposed to attacks using adversarial prompts show the ability to generate so-called unlearned concepts, which can be not only harmful but also illegal. In this paper, we present considerations regarding the ability of models to forget and recall knowledge, introducing the Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which we consider to be a regenerative approach supporting the effective recovery of previously lost knowledge. Moreover, we propose that robustness in knowledge retrieval is a crucial yet underexplored evaluation measure for developing more robust and effective unlearning techniques. Finally, we demonstrate that forgetting occurs in two distinct ways: short-term, where concepts can be quickly recalled, and long-term, where recovery is more challenging. Code is available at https://gmum.github.io/MemoRa/.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning</title>
<link>https://arxiv.org/abs/2510.07664</link>
<guid>https://arxiv.org/abs/2510.07664</guid>
<content:encoded><![CDATA[
arXiv:2510.07664v2 Announce Type: replace 
Abstract: Federated learning (FL) enables collaborative model training across multiple parties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as a balanced approach between synchronous and asynchronous FL. However, SAFL faces significant challenges in optimizing both gradient-based (e.g., FedSGD) and model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct trade-offs in accuracy, convergence speed, and stability. While gradient aggregation achieves faster convergence and higher accuracy, it suffers from pronounced fluctuations, whereas model aggregation offers greater stability but slower convergence and suboptimal accuracy. This paper presents FedQS, the first framework to theoretically analyze and address these disparities in SAFL. FedQS introduces a divide-and-conquer strategy to handle client heterogeneity by classifying clients into four distinct types and adaptively optimizing their local training based on data distribution characteristics and available computational resources. Extensive experiments on computer vision, natural language processing, and real-world tasks demonstrate that FedQS achieves the highest accuracy, attains the lowest loss, and ranks among the fastest in convergence speed, outperforming state-of-the-art baselines. Our work bridges the gap between aggregation strategies in SAFL, offering a unified solution for stable, accurate, and efficient federated learning. The code and datasets are available at https://github.com/bkjod/FedQS_.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimally Deep Networks - Adapting Model Depth to Datasets for Superior Efficiency</title>
<link>https://arxiv.org/abs/2510.10764</link>
<guid>https://arxiv.org/abs/2510.10764</guid>
<content:encoded><![CDATA[
arXiv:2510.10764v4 Announce Type: replace 
Abstract: Deep neural networks (DNNs) have provided brilliant performance across various tasks. However, this success often comes at the cost of unnecessarily large model sizes, high computational demands, and substantial memory footprints. Typically, powerful architectures are trained at full depths but not all datasets or tasks require such high model capacity. Training big and deep architectures on relatively low-complexity datasets frequently leads to wasted computation, unnecessary energy consumption, and excessive memory usage, which in turn makes deployment of models on resource-constrained devices impractical. To address this problem, we introduce the concept of Optimally Deep Networks (ODNs), which provides a balance between model depth and task complexity. Specifically, we propose a NAS like training strategy called progressive depth expansion, which begins by training neural networks at shallower depths and incrementally increases their depth as the earlier blocks converge, continuing this process until the target accuracy is reached. ODNs use only the optimal depth for the tasks at hand, removing redundant layers. This cuts down future training and inference costs, lowers the model memory footprint, enhances computational efficiency, and facilitates deployment on edge devices. Empirical results show that the optimal depths of ResNet-18 and ResNet-34 for MNIST and SVHN, achieve up to 98.64 % and 96.44 % reduction in memory footprint, while maintaining a competitive accuracy of 99.31 % and 96.08 %, respectively.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language Models on Edge Devices</title>
<link>https://arxiv.org/abs/2510.19482</link>
<guid>https://arxiv.org/abs/2510.19482</guid>
<content:encoded><![CDATA[
arXiv:2510.19482v2 Announce Type: replace 
Abstract: Weight quantization effectively reduces memory consumption and enables the deployment of large language models on CPU-based edge devices, yet existing hardware-friendly methods often rely on uniform quantization, which suffers from poor weight-distribution fitting and high dequantization overhead under low-bit settings. In this paper, we propose ELUTQ, an efficient quantization framework featuring a novel quantization format termed Hierarchical Linear Quantization (HLQ). HLQ is designed to better capture the statistical characteristics of weights without increasing the computational cost of bit-serial LUT-based GEMM operations, thereby eliminating dequantization overhead. HLQ is orthogonal to existing quantization algorithms. For the LLaMA3.1-8B model, when combined with post-training quantization, HLQ improves uniform quantization by achieving approximately 8 percent perplexity reduction at 3-bit precision and 85 percent perplexity reduction at 2-bit precision. When combined with efficient finetuning techniques, HLQ further improves model accuracy. We also integrate a disk-offload technique into ELUTQ, enabling it to complete the quantization of LLaMA3.1-70B using only 64 GB of CPU memory and 48 GB of VRAM, significantly reducing the hardware requirements for large-scale model quantization. To enable efficient deployment on edge devices, ELUTQ provides high-performance CPU kernels to support end-to-end inference. Under a 4-thread configuration with batch size 1, our 2-bit quantized LLaMA2-7B model achieves a throughput of more than 25 tokens per second on an Apple M2 chip. All the code is available at https://github.com/Nkniexin/ELUTQ.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Bounds for Rank-sparse Neural Networks</title>
<link>https://arxiv.org/abs/2510.21945</link>
<guid>https://arxiv.org/abs/2510.21945</guid>
<content:encoded><![CDATA[
arXiv:2510.21945v3 Announce Type: replace 
Abstract: It has been recently observed in much of the literature that neural networks exhibit a bottleneck rank property: for larger depths, the activation and weights of neural networks trained with gradient-based methods tend to be of approximately low rank. In fact, the rank of the activations of each layer converges to a fixed value referred to as the ``bottleneck rank'', which is the minimum rank required to represent the training data. This perspective is in line with the observation that regularizing linear networks (without activations) with weight decay is equivalent to minimizing the Schatten $p$ quasi norm of the neural network. In this paper we investigate the implications of this phenomenon for generalization. More specifically, we prove generalization bounds for neural networks which exploit the approximate low rank structure of the weight matrices if present. The final results rely on the Schatten $p$ quasi norms of the weight matrices: for small $p$, the bounds exhibit a sample complexity $ \widetilde{O}(WrL^2)$ where $W$ and $L$ are the width and depth of the neural network respectively and where $r$ is the rank of the weight matrices. As $p$ increases, the bound behaves more like a norm-based bound instead.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AirFed: A Federated Graph-Enhanced Multi-Agent Reinforcement Learning Framework for Multi-UAV Cooperative Mobile Edge Computing</title>
<link>https://arxiv.org/abs/2510.23053</link>
<guid>https://arxiv.org/abs/2510.23053</guid>
<content:encoded><![CDATA[
arXiv:2510.23053v2 Announce Type: replace 
Abstract: Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing (MEC) systems face critical challenges in coordinating trajectory planning, task offloading, and resource allocation while ensuring Quality of Service (QoS) under dynamic and uncertain environments. Existing approaches suffer from limited scalability, slow convergence, and inefficient knowledge sharing among UAVs, particularly when handling large-scale IoT device deployments with stringent deadline constraints. This paper proposes AirFed, a novel federated graph-enhanced multi-agent reinforcement learning framework that addresses these challenges through three key innovations. First, we design dual-layer dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal dependencies among UAVs and IoT devices, capturing both service relationships and collaborative interactions within the network topology. Second, we develop a dual-Actor single-Critic architecture that jointly optimizes continuous trajectory control and discrete task offloading decisions. Third, we propose a reputation-based decentralized federated learning mechanism with gradient-sensitive adaptive quantization, enabling efficient and robust knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate that AirFed achieves 42.9% reduction in weighted cost compared to state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2% IoT device coverage rate, and reduces communication overhead by 54.5%. Scalability analysis confirms robust performance across varying UAV numbers, IoT device densities, and system scales, validating AirFed's practical applicability for large-scale UAV-MEC deployments.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifiable learning of dissipative dynamics</title>
<link>https://arxiv.org/abs/2510.24160</link>
<guid>https://arxiv.org/abs/2510.24160</guid>
<content:encoded><![CDATA[
arXiv:2510.24160v2 Announce Type: replace 
Abstract: Complex dissipative systems appear across science and engineering, from polymers and active matter to learning algorithms. These systems operate far from equilibrium, where energy dissipation and time irreversibility govern their behavior but are difficult to quantify from data. Here, we introduce a universal and identifiable neural framework that learns dissipative stochastic dynamics directly from trajectories while ensuring interpretability, expressiveness, and uniqueness. Our method identifies a unique energy landscape, separates reversible from irreversible motion, and allows direct computation of the entropy production, providing a principled measure of irreversibility and deviations from equilibrium. Applications to polymer stretching in elongational flow and to stochastic gradient Langevin dynamics reveal new insights, including super-linear scaling of barrier heights and sub-linear scaling of entropy production rates with the strain rate, and the suppression of irreversibility with increasing batch size. Our methodology thus establishes a general, data-driven framework for discovering and interpreting non-equilibrium dynamics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing divergent representations from causal interventions on neural networks</title>
<link>https://arxiv.org/abs/2511.04638</link>
<guid>https://arxiv.org/abs/2511.04638</guid>
<content:encoded><![CDATA[
arXiv:2511.04638v3 Announce Type: replace 
Abstract: A common approach to mechanistic interpretability is to causally manipulate model representations via targeted interventions in order to understand what those representations encode. Here we ask whether such interventions create out-of-distribution (divergent) representations, and whether this raises concerns about how faithful their resulting explanations are to the target model in its natural state. First, we demonstrate theoretically and empirically that common causal intervention techniques often do shift internal representations away from the natural distribution of the target model. Then, we provide a theoretical analysis of two classes of such divergences: "harmless" divergences that occur in the null-space of the weights and from covariance within behavioral decision boundaries, and "pernicious" divergences that activate hidden network pathways and cause dormant behavioral changes. Finally, in an effort to mitigate the pernicious cases, we apply and modify the Counterfactual Latent (CL) loss from Grant (2025) allowing representations from causal interventions to remain closer to the natural distribution, reducing the likelihood of harmful divergences while preserving the interpretive power of the interventions. Together, these results highlight a path towards more reliable interpretability methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices</title>
<link>https://arxiv.org/abs/2511.04774</link>
<guid>https://arxiv.org/abs/2511.04774</guid>
<content:encoded><![CDATA[
arXiv:2511.04774v3 Announce Type: replace 
Abstract: Large-scale networked services rely on deep soft-ware stacks and microservice orchestration, which increase instruction footprints and create frontend stalls that inflate tail latency and energy. We revisit instruction prefetching for these cloud workloads and present a design that aligns with SLO driven and self optimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we introduce a Compressed Entry that captures up to eight destinations around a base using 36 bits by exploiting spatial clustering, and a Hierarchical Metadata Storage scheme that keeps only L1 resident and frequently queried entries on chip while virtualizing bulk metadata into lower levels. We further add a lightweight Online ML Controller that scores prefetch profitability using context features and a bandit adjusted threshold. On data center applications, our approach preserves EIP like speedups with smaller on chip state and improves efficiency for networked services in the ML era.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Up ROC-Optimizing Support Vector Machines</title>
<link>https://arxiv.org/abs/2511.04979</link>
<guid>https://arxiv.org/abs/2511.04979</guid>
<content:encoded><![CDATA[
arXiv:2511.04979v2 Announce Type: replace 
Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the area under the ROC curve (AUC) and has become an attractive alternative of the conventional binary classification under the presence of class imbalance. However, its practical use is limited by high computational cost, as training involves evaluating all $O(n^2)$. To overcome this limitation, we develop a scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby substantially reducing computational complexity. We further extend the framework to nonlinear classification through a low-rank kernel approximation, enabling efficient training in reproducing kernel Hilbert spaces. Theoretical analysis establishes an error bound that justifies the proposed approximation, and empirical results on both synthetic and real datasets demonstrate that the proposed method achieves comparable AUC performance to the original ROC-SVM with drastically reduced training time.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-branch Spatial-Temporal Self-supervised Representation for Enhanced Road Network Learning</title>
<link>https://arxiv.org/abs/2511.06633</link>
<guid>https://arxiv.org/abs/2511.06633</guid>
<content:encoded><![CDATA[
arXiv:2511.06633v2 Announce Type: replace 
Abstract: Road network representation learning (RNRL) has attracted increasing attention from both researchers and practitioners as various spatiotemporal tasks are emerging. Recent advanced methods leverage Graph Neural Networks (GNNs) and contrastive learning to characterize the spatial structure of road segments in a self-supervised paradigm. However, spatial heterogeneity and temporal dynamics of road networks raise severe challenges to the neighborhood smoothing mechanism of self-supervised GNNs. To address these issues, we propose a $\textbf{D}$ual-branch $\textbf{S}$patial-$\textbf{T}$emporal self-supervised representation framework for enhanced road representations, termed as DST. On one hand, DST designs a mix-hop transition matrix for graph convolution to incorporate dynamic relations of roads from trajectories. Besides, DST contrasts road representations of the vanilla road network against that of the hypergraph in a spatial self-supervised way. The hypergraph is newly built based on three types of hyperedges to capture long-range relations. On the other hand, DST performs next token prediction as the temporal self-supervised task on the sequences of traffic dynamics based on a causal Transformer, which is further regularized by differentiating traffic modes of weekdays from those of weekends. Extensive experiments against state-of-the-art methods verify the superiority of our proposed framework. Moreover, the comprehensive spatiotemporal modeling facilitates DST to excel in zero-shot learning scenarios.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2511.09219</link>
<guid>https://arxiv.org/abs/2511.09219</guid>
<content:encoded><![CDATA[
arXiv:2511.09219v3 Announce Type: replace 
Abstract: Mixed-Integer Linear Programming (MILP) lies at the core of many real-world combinatorial optimization (CO) problems, traditionally solved by branch-and-bound (B&amp;B). A key driver influencing B&amp;B solvers efficiency is the variable selection heuristic that guides branching decisions. Looking to move beyond static, hand-crafted heuristics, recent work has explored adapting traditional reinforcement learning (RL) algorithms to the B&amp;B setting, aiming to learn branching strategies tailored to specific MILP distributions. In parallel, RL agents have achieved remarkable success in board games, a very specific type of combinatorial problems, by leveraging environment simulators to plan via Monte Carlo Tree Search (MCTS). Building on these developments, we introduce Plan-and-Branch-and-Bound (PlanB&amp;B), a model-based reinforcement learning (MBRL) agent that leverages a learned internal model of the B&amp;B dynamics to discover improved branching strategies. Computational experiments empirically validate our approach, with our MBRL branching agent outperforming previous state-of-the-art RL methods across four standard MILP benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games</title>
<link>https://arxiv.org/abs/2511.11602</link>
<guid>https://arxiv.org/abs/2511.11602</guid>
<content:encoded><![CDATA[
arXiv:2511.11602v2 Announce Type: replace 
Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance</title>
<link>https://arxiv.org/abs/2511.12121</link>
<guid>https://arxiv.org/abs/2511.12121</guid>
<content:encoded><![CDATA[
arXiv:2511.12121v4 Announce Type: replace 
Abstract: Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Organization and Spectral Mechanism of Attractor Landscapes in High-Capacity Kernel Hopfield Networks</title>
<link>https://arxiv.org/abs/2511.13053</link>
<guid>https://arxiv.org/abs/2511.13053</guid>
<content:encoded><![CDATA[
arXiv:2511.13053v3 Announce Type: replace 
Abstract: Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by unifying the geometric analysis of the attractor landscape with the spectral theory of kernel machines. Using a novel metric, "Pinnacle Sharpness," we first uncover a rich phase diagram of attractor stability, identifying a "Ridge of Optimization" where the network achieves maximal robustness under high-load conditions. Phenomenologically, this ridge is characterized by a "Force Antagonism," where a strong driving force is balanced by a collective feedback force. Theoretically, we reveal that this phenomenon arises from a specific reorganization of the weight spectrum, which we term \textit{Spectral Concentration}. Unlike a simple rank-1 collapse, our analysis shows that the network on the ridge self-organizes into a critical state: the leading eigenvalue is amplified to maximize global stability (Direct Force), while the trailing eigenvalues are preserved to maintain high memory capacity (Indirect Force). These findings provide a complete physical picture of how high-capacity associative memories are formed, demonstrating that optimal performance is achieved by tuning the system to a spectral "Goldilocks zone" between rank collapse and diffusion.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architectures and random properties of symplectic quantum circuits</title>
<link>https://arxiv.org/abs/2405.10264</link>
<guid>https://arxiv.org/abs/2405.10264</guid>
<content:encoded><![CDATA[
arXiv:2405.10264v3 Announce Type: replace-cross 
Abstract: Parametrized and random unitary (or orthogonal) $n$-qubit circuits play a central role in quantum information. As such, one could naturally assume that circuits implementing symplectic transformations would attract similar attention. However, this is not the case, as $\mathbb{SP} (d/2)$ -- the group of $d\times d$ unitary symplectic matrices -- has thus far been overlooked. In this work, we aim at starting to fill this gap. We begin by presenting a universal set of generators $\mathcal{G}$ for the symplectic algebra $\mathfrak{sp}(d/2)$, consisting of one- and two-qubit Pauli operators acting on neighboring sites in a one-dimensional lattice. Here, we uncover two critical differences between such set, and equivalent ones for unitary and orthogonal circuits. Namely, we find that the operators in $\mathcal{G}$ cannot generate arbitrary local symplectic unitaries and that they are not translationally invariant. We then review the Schur-Weyl duality between the symplectic group and the Brauer algebra, and use tools from Weingarten calculus to prove that Pauli measurements at the output of Haar random symplectic circuits can converge to Gaussian processes. As a by-product, such analysis provides us with concentration bounds for Pauli measurements in circuits that form $t$-designs over $\mathbb{SP}(d/2)$. To finish, we present tensor-network tools to analyze shallow random symplectic circuits, and we use these to numerically show that computational-basis measurements anti-concentrate at logarithmic depth.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Boltzmann machine learning of ground-state energies</title>
<link>https://arxiv.org/abs/2410.12935</link>
<guid>https://arxiv.org/abs/2410.12935</guid>
<content:encoded><![CDATA[
arXiv:2410.12935v3 Announce Type: replace-cross 
Abstract: Estimating the ground-state energy of Hamiltonians is a fundamental task for which it is believed that quantum computers can be helpful. Several approaches have been proposed toward this goal, including algorithms based on quantum phase estimation and hybrid quantum-classical optimizers involving parameterized quantum circuits, the latter falling under the umbrella of the variational quantum eigensolver. Here, we analyze the performance of quantum Boltzmann machines for this task, which is a less explored ansatz based on parameterized thermal states and which is not known to suffer from the barren-plateau problem. We delineate a hybrid quantum-classical algorithm for this task and rigorously prove that it converges to an $\varepsilon$-approximate stationary point of the energy function optimized over parameter space, while using a number of parameterized-thermal-state samples that is polynomial in $\varepsilon^{-1}$, the number of parameters, and the norm of the Hamiltonian being optimized. Our algorithm estimates the gradient of the energy function efficiently by means of a quantum circuit construction that combines classical random sampling, Hamiltonian simulation, and the Hadamard test. Additionally, supporting our main claims are calculations of the gradient and Hessian of the energy function, as well as an upper bound on the matrix elements of the latter that is used in the convergence analysis.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Efficient Representations of Neutrino Telescope Events</title>
<link>https://arxiv.org/abs/2410.13148</link>
<guid>https://arxiv.org/abs/2410.13148</guid>
<content:encoded><![CDATA[
arXiv:2410.13148v3 Announce Type: replace-cross 
Abstract: Neutrino telescopes detect rare interactions of particles produced in some of the most extreme environments in the Universe. This is accomplished by instrumenting a cubic-kilometer scale volume of naturally occurring transparent medium with light sensors. Given their substantial size and the high frequency of background interactions, these telescopes amass an enormous quantity of large variance, high-dimensional data. These attributes create substantial challenges for analyzing and reconstructing interactions, particularly when utilizing machine learning (ML) techniques. In this paper, we present a novel approach, called om2vec, that employs transformer-based variational autoencoders to efficiently represent the detected photon arrival time distributions of neutrino telescope events by learning compact and descriptive latent representations. We demonstrate that these latent representations offer enhanced flexibility and improved computational efficiency, thereby facilitating downstream tasks in data analysis.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jailbreaking and Mitigation of Vulnerabilities in Large Language Models</title>
<link>https://arxiv.org/abs/2410.15236</link>
<guid>https://arxiv.org/abs/2410.15236</guid>
<content:encoded><![CDATA[
arXiv:2410.15236v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have transformed artificial intelligence by advancing natural language understanding and generation, enabling applications across fields beyond healthcare, software engineering, and conversational systems. Despite these advancements in the past few years, LLMs have shown considerable vulnerabilities, particularly to prompt injection and jailbreaking attacks. This review analyzes the state of research on these vulnerabilities and presents available defense strategies. We roughly categorize attack approaches into prompt-based, model-based, multimodal, and multilingual, covering techniques such as adversarial prompting, backdoor injections, and cross-modality exploits. We also review various defense mechanisms, including prompt filtering, transformation, alignment techniques, multi-agent defenses, and self-regulation, evaluating their strengths and shortcomings. We also discuss key metrics and benchmarks used to assess LLM safety and robustness, noting challenges like the quantification of attack success in interactive contexts and biases in existing datasets. Identifying current research gaps, we suggest future directions for resilient alignment strategies, advanced defenses against evolving attacks, automation of jailbreak detection, and consideration of ethical and societal impacts. This review emphasizes the need for continued research and cooperation within the AI community to enhance LLM security and ensure their safe deployment.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperparameter Optimization in Machine Learning</title>
<link>https://arxiv.org/abs/2410.22854</link>
<guid>https://arxiv.org/abs/2410.22854</guid>
<content:encoded><![CDATA[
arXiv:2410.22854v3 Announce Type: replace-cross 
Abstract: Hyperparameters are configuration variables controlling the behavior of machine learning algorithms. They are ubiquitous in machine learning and artificial intelligence and the choice of their values determines the effectiveness of systems based on these technologies. Manual hyperparameter search is often time-consuming and becomes infeasible when the number of hyperparameters is large. Automating the search is an important step towards advancing, streamlining, and systematizing machine learning, freeing researchers and practitioners alike from the burden of finding a good set of hyperparameters by trial and error. In this survey, we present a unified treatment of hyperparameter optimization, providing the reader with examples, insights into the state-of-the-art, and numerous links to further reading. We cover the main families of techniques to automate hyperparameter search, often referred to as hyperparameter optimization or tuning, including random and quasi-random search, bandit-, model-, population-, and gradient-based approaches. We further discuss extensions, including online, constrained, and multi-objective formulations, touch upon connections with other fields, such as meta-learning and neural architecture search, and conclude with open questions and future research directions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLZero: Direct Policy Inference from Language Without In-Domain Supervision</title>
<link>https://arxiv.org/abs/2412.05718</link>
<guid>https://arxiv.org/abs/2412.05718</guid>
<content:encoded><![CDATA[
arXiv:2412.05718v3 Announce Type: replace-cross 
Abstract: The reward hypothesis states that all goals and purposes can be understood as the maximization of a received scalar reward signal. However, in practice, defining such a reward signal is notoriously difficult, as humans are often unable to predict the optimal behavior corresponding to a reward function. Natural language offers an intuitive alternative for instructing reinforcement learning (RL) agents, yet previous language-conditioned approaches either require costly supervision or test-time training given a language instruction. In this work, we present a new approach that uses a pretrained RL agent trained using only unlabeled, offline interactions--without task-specific supervision or labeled trajectories--to get zero-shot test-time policy inference from arbitrary natural language instructions. We introduce a framework comprising three steps: imagine, project, and imitate. First, the agent imagines a sequence of observations corresponding to the provided language description using video generative models. Next, these imagined observations are projected into the target environment domain. Finally, an agent pretrained in the target environment with unsupervised RL instantly imitates the projected observation sequence through a closed-form solution. To the best of our knowledge, our method, RLZero, is the first approach to show direct language-to-behavior generation abilities on a variety of tasks and environments without any in-domain supervision. We further show that components of RLZero can be used to generate policies zero-shot from cross-embodied videos, such as those available on YouTube, even for complex embodiments like humanoids.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mamba-based Deep Learning Approach for Sleep Staging on a Wireless Multimodal Wearable System without Electroencephalography</title>
<link>https://arxiv.org/abs/2412.15947</link>
<guid>https://arxiv.org/abs/2412.15947</guid>
<content:encoded><![CDATA[
arXiv:2412.15947v4 Announce Type: replace-cross 
Abstract: Study Objectives: We investigate a Mamba-based deep learning approach for sleep staging on signals from ANNE One (Sibel Health, Evanston, IL), a non-intrusive dual-module wireless wearable system measuring chest electrocardiography (ECG), triaxial accelerometry, and chest temperature, and finger photoplethysmography and finger temperature.
  Methods: We obtained wearable sensor recordings from 357 adults undergoing concurrent polysomnography (PSG) at a tertiary care sleep lab. Each PSG recording was manually scored and these annotations served as ground truth labels for training and evaluation of our models. PSG and wearable sensor data were automatically aligned using their ECG channels with manual confirmation by visual inspection. We trained a Mamba-based recurrent neural network architecture on these recordings. Ensembling of model variants with similar architectures was performed.
  Results: After ensembling, the model attains a 3-class (wake, non rapid eye movement [NREM] sleep, rapid eye movement [REM] sleep) balanced accuracy of 84.02%, F1 score of 84.23%, Cohen's $\kappa$ of 72.89%, and a Matthews correlation coefficient (MCC) score of 73.00%; a 4-class (wake, light NREM [N1/N2], deep NREM [N3], REM) balanced accuracy of 75.30%, F1 score of 74.10%, Cohen's $\kappa$ of 61.51%, and MCC score of 61.95%; a 5-class (wake, N1, N2, N3, REM) balanced accuracy of 65.11%, F1 score of 66.15%, Cohen's $\kappa$ of 53.23%, MCC score of 54.38%.
  Conclusions: Our Mamba-based deep learning model can successfully infer major sleep stages from the ANNE One, a wearable system without electroencephalography (EEG), and can be applied to data from adults attending a tertiary care sleep clinic.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep learning and whole-brain networks for biomarker discovery: modeling the dynamics of brain fluctuations in resting-state and cognitive tasks</title>
<link>https://arxiv.org/abs/2412.19329</link>
<guid>https://arxiv.org/abs/2412.19329</guid>
<content:encoded><![CDATA[
arXiv:2412.19329v2 Announce Type: replace-cross 
Abstract: Background: Brain network models offer insights into brain dynamics, but the utility of model-derived bifurcation parameters as biomarkers remains underexplored. Objective: This study evaluates bifurcation parameters from a whole-brain network model as biomarkers for distinguishing brain states associated with resting-state and task-based cognitive conditions. Methods: Synthetic BOLD signals were generated using a supercritical Hopf brain network model to train deep learning models for bifurcation parameter prediction. Inference was performed on Human Connectome Project data, including both resting-state and task-based conditions. Statistical analyses assessed the separability of brain states based on bifurcation parameter distributions. Results: Bifurcation parameter distributions differed significantly across task and resting-state conditions ($p < 0.0001$ for all but one comparison). Task-based brain states exhibited higher bifurcation values compared to rest. Conclusion: Bifurcation parameters effectively differentiate cognitive and resting states, warranting further investigation as biomarkers for brain state characterization and neurological disorder assessment.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KKL Observer Synthesis for Nonlinear Systems via Physics-Informed Learning</title>
<link>https://arxiv.org/abs/2501.11655</link>
<guid>https://arxiv.org/abs/2501.11655</guid>
<content:encoded><![CDATA[
arXiv:2501.11655v2 Announce Type: replace-cross 
Abstract: This paper proposes a novel learning approach for designing Kazantzis-Kravaris/Luenberger (KKL) observers for autonomous nonlinear systems. The design of a KKL observer involves finding an injective map that transforms the system state into a higher-dimensional observer state, whose dynamics is linear and stable. The observer's state is then mapped back to the original system coordinates via the inverse map to obtain the state estimate. However, finding this transformation and its inverse is quite challenging. We propose learning the forward mapping using a physics-informed neural network, and then learning its inverse mapping with a conventional feedforward neural network. Theoretical guarantees for the robustness of state estimation against approximation error and system uncertainties are provided, including non-asymptotic learning guarantees that link approximation quality to finite sample sizes. The effectiveness of the proposed approach is demonstrated through numerical simulations on benchmark examples, showing superior generalization capability outside the training domain compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment</title>
<link>https://arxiv.org/abs/2501.17690</link>
<guid>https://arxiv.org/abs/2501.17690</guid>
<content:encoded><![CDATA[
arXiv:2501.17690v4 Announce Type: replace-cross 
Abstract: We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExDDV: A New Dataset for Explainable Deepfake Detection in Video</title>
<link>https://arxiv.org/abs/2503.14421</link>
<guid>https://arxiv.org/abs/2503.14421</guid>
<content:encoded><![CDATA[
arXiv:2503.14421v2 Announce Type: replace-cross 
Abstract: The ever growing realism and quality of generated videos makes it increasingly harder for humans to spot deepfake content, who need to rely more and more on automatic deepfake detectors. However, deepfake detectors are also prone to errors, and their decisions are not explainable, leaving humans vulnerable to deepfake-based fraud and misinformation. To this end, we introduce ExDDV, the first dataset and benchmark for Explainable Deepfake Detection in Video. ExDDV comprises around 5.4K real and deepfake videos that are manually annotated with text descriptions (to explain the artifacts) and clicks (to point out the artifacts). We evaluate a number of vision-language models on ExDDV, performing experiments with various fine-tuning and in-context learning strategies. Our results show that text and click supervision are both required to develop robust explainable models for deepfake videos, which are able to localize and describe the observed artifacts. Our novel dataset and code to reproduce the results are available at https://github.com/vladhondru25/ExDDV.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Long Context Language Modeling</title>
<link>https://arxiv.org/abs/2503.17407</link>
<guid>https://arxiv.org/abs/2503.17407</guid>
<content:encoded><![CDATA[
arXiv:2503.17407v2 Announce Type: replace-cross 
Abstract: Efficient processing of long contexts has been a persistent pursuit in Natural Language Processing. With the growing number of long documents, dialogues, and other textual data, it is important to develop Long Context Language Models (LCLMs) that can process and analyze extensive inputs in an effective and efficient way. In this paper, we present a comprehensive survey on recent advances in long-context modeling for large language models. Our survey is structured around three key aspects: how to obtain effective and efficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate and analyze LCLMs comprehensively. For the first aspect, we discuss data strategies, architectural designs, and workflow approaches oriented with long context processing. For the second aspect, we provide a detailed examination of the infrastructure required for LCLM training and inference. For the third aspect, we present evaluation paradigms for long-context comprehension and long-form generation, as well as behavioral analysis and mechanism interpretability of LCLMs. Beyond these three key aspects, we thoroughly explore the diverse application scenarios where existing LCLMs have been deployed and outline promising future development directions. This survey provides an up-to-date review of the literature on long-context LLMs, which we wish to serve as a valuable resource for both researchers and engineers. An associated GitHub repository collecting the latest papers and repos is available at: \href{https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling}{\color[RGB]{175,36,67}{LCLM-Horizon}}.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)</title>
<link>https://arxiv.org/abs/2504.03151</link>
<guid>https://arxiv.org/abs/2504.03151</guid>
<content:encoded><![CDATA[
arXiv:2504.03151v2 Announce Type: replace-cross 
Abstract: Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Optimizing Multi-Stage AI Inference Pipelines</title>
<link>https://arxiv.org/abs/2504.09775</link>
<guid>https://arxiv.org/abs/2504.09775</guid>
<content:encoded><![CDATA[
arXiv:2504.09775v4 Announce Type: replace-cross 
Abstract: The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.
  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Techniques for Regression in Deep Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.11355</link>
<guid>https://arxiv.org/abs/2505.11355</guid>
<content:encoded><![CDATA[
arXiv:2505.11355v2 Announce Type: replace-cross 
Abstract: Gaussian processes (GPs) have gained popularity as flexible machine learning models for regression and function approximation with an in-built method for uncertainty quantification. However, GPs suffer when the amount of training data is large or when the underlying function contains multi-scale features that are difficult to represent by a stationary kernel. To address the former, training of GPs with large-scale data is often performed through inducing point approximations, also known as sparse GP regression (GPR), where the size of the covariance matrices in GPR is reduced considerably through a greedy search on the data set. To aid the latter, deep GPs have gained traction as hierarchical models that resolve multi-scale features by combining multiple GPs. Posterior inference in deep GPs requires a sampling or, more usual, a variational approximation. Variational approximations lead to large-scale stochastic, non-convex optimisation problems and the resulting approximation tends to represent uncertainty incorrectly. In this work, we combine variational learning with MCMC to develop a particle-based expectation-maximisation method to simultaneously find inducing points within the large-scale data (variationally) and accurately train the deep GPs (sampling-based). The result is a highly efficient and accurate methodology for deep GP training on large-scale data. We test our method on standard benchmark problems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Missing Data Imputation by Reducing Mutual Information with Rectified Flows</title>
<link>https://arxiv.org/abs/2505.11749</link>
<guid>https://arxiv.org/abs/2505.11749</guid>
<content:encoded><![CDATA[
arXiv:2505.11749v3 Announce Type: replace-cross 
Abstract: This paper introduces a novel iterative method for missing data imputation that sequentially reduces the mutual information between data and the corresponding missingness mask. Inspired by GAN-based approaches that train generators to decrease the predictability of missingness patterns, our method explicitly targets this reduction in mutual information. Specifically, our algorithm iteratively minimizes the KL divergence between the joint distribution of the imputed data and missingness mask, and the product of their marginals from the previous iteration. We show that the optimal imputation under this framework can be achieved by solving an ODE whose velocity field minimizes a rectified flow training objective. We further illustrate that some existing imputation techniques can be interpreted as approximate special cases of our mutual-information-reducing framework. Comprehensive experiments on synthetic and real-world datasets validate the efficacy of our proposed approach, demonstrating its superior imputation performance. Our implementation is available at https://github.com/yujhml/MIRI-Imputation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.13344</link>
<guid>https://arxiv.org/abs/2505.13344</guid>
<content:encoded><![CDATA[
arXiv:2505.13344v2 Announce Type: replace-cross 
Abstract: We propose RoPECraft, a training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from a reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using a flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate a regularization term based on the phase components of the reference video's Fourier transform, projecting the phase angles onto a smooth manifold to suppress high-frequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Asymptotic Equation Linking WAIC and WBIC in Singular Models</title>
<link>https://arxiv.org/abs/2505.13902</link>
<guid>https://arxiv.org/abs/2505.13902</guid>
<content:encoded><![CDATA[
arXiv:2505.13902v3 Announce Type: replace-cross 
Abstract: In statistical learning, models are classified as regular or singular depending on whether the mapping from parameters to probability distributions is injective. Most models with hierarchical structures or latent variables are singular, for which conventional criteria such as the Akaike Information Criterion and the Bayesian Information Criterion are inapplicable due to the breakdown of normal approximations for the likelihood and posterior. To address this, the Widely Applicable Information Criterion (WAIC) and the Widely Applicable Bayesian Information Criterion (WBIC) have been proposed. Since WAIC and WBIC are computed using posterior distributions at different temperature settings, separate posterior sampling is generally required. In this paper, we theoretically derive an asymptotic equation that links WAIC and WBIC, despite their dependence on different posteriors. This equation yields an asymptotically unbiased expression of WAIC in terms of the posterior distribution used for WBIC. The result clarifies the structural relationship between these criteria within the framework of singular learning theory, and deepens understanding of their asymptotic behavior. This theoretical contribution provides a foundation for future developments in the computational efficiency of model selection in singular models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Text-Image-to-Video Generation: A Training-Free Approach to Flexible Visual Conditioning</title>
<link>https://arxiv.org/abs/2505.20629</link>
<guid>https://arxiv.org/abs/2505.20629</guid>
<content:encoded><![CDATA[
arXiv:2505.20629v2 Announce Type: replace-cross 
Abstract: Text-image-to-video (TI2V) generation is a critical problem for controllable video generation using both semantic and visual conditions. Most existing methods typically add visual conditions to text-to-video (T2V) foundation models by finetuning, which is costly in resources and only limited to a few pre-defined conditioning settings. To tackle these constraints, we introduce a unified formulation for TI2V generation with flexible visual conditioning. Furthermore, we propose an innovative training-free approach, dubbed FlexTI2V, that can condition T2V foundation models on an arbitrary amount of images at arbitrary positions. Specifically, we firstly invert the condition images to noisy representation in a latent space. Then, in the denoising process of T2V models, our method uses a novel random patch swapping strategy to incorporate visual features into video representations through local image patches. To balance creativity and fidelity, we use a dynamic control mechanism to adjust the strength of visual conditioning to each video frame. Extensive experiments validate that our method surpasses previous training-free image conditioning methods by a notable margin. Our method can also generalize to both UNet-based and transformer-based architectures.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring</title>
<link>https://arxiv.org/abs/2505.23575</link>
<guid>https://arxiv.org/abs/2505.23575</guid>
<content:encoded><![CDATA[
arXiv:2505.23575v3 Announce Type: replace-cross 
Abstract: As AI models are deployed with increasing autonomy, it is important to ensure they do not take harmful actions unnoticed. As a potential mitigation, we investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor model continuously oversees the intermediate reasoning steps of a more powerful but untrusted model. We compare CoT monitoring to action-only monitoring, where only final outputs are reviewed, in a red-teaming setup where the untrusted model is instructed to pursue harmful side tasks while completing a coding problem. We find that while CoT monitoring is more effective than overseeing only model outputs in scenarios where action-only monitoring fails to reliably identify sabotage, reasoning traces can contain misleading rationalizations that deceive the CoT monitors, reducing performance in obvious sabotage cases. To address this, we introduce a hybrid protocol that independently scores model reasoning and actions, and combines them using a weighted average. Our hybrid monitor consistently outperforms both CoT and action-only monitors across all tested models and tasks, with detection rates twice higher than action-only monitoring for subtle deception scenarios.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2506.05587</link>
<guid>https://arxiv.org/abs/2506.05587</guid>
<content:encoded><![CDATA[
arXiv:2506.05587v3 Announce Type: replace-cross 
Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 28K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI GPT-5 and DeepSeek R1 score only around 69\% and 57\% respectively, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis.
  Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Vision-Language Models for Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.06836</link>
<guid>https://arxiv.org/abs/2506.06836</guid>
<content:encoded><![CDATA[
arXiv:2506.06836v2 Announce Type: replace-cross 
Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of fields, including healthcare, finance, and sensor-based condition monitoring. Prior methods, which mainly focus on training domain-specific models on numerical data, lack the visual-temporal understanding capacity that human experts have to identify contextual anomalies. To fill this gap, we explore a solution based on vision language models (VLMs). Recent studies have shown the ability of VLMs for visual understanding tasks, yet their direct application to time series has fallen short on both accuracy and efficiency. To harness the power of VLMs for TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening stage built on a relatively lightweight pre-trained vision encoder, which leverages 2D time series representations to accurately localize candidate anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal context and VLM's visual understanding capacity to refine the detection upon the candidates provided by ViT4TS. We show that without any time-series training, VLM4TS outperforms time-series pre-trained and from-scratch baselines in most cases, yielding a 24.6% improvement in F1-max score over the best baseline. Moreover, VLM4TS also consistently outperforms existing language model-based TSAD methods and is on average 36x more efficient in token usage.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEANN: A Low-Storage Vector Index</title>
<link>https://arxiv.org/abs/2506.08276</link>
<guid>https://arxiv.org/abs/2506.08276</guid>
<content:encoded><![CDATA[
arXiv:2506.08276v2 Announce Type: replace-cross 
Abstract: Embedding-based vector search underpins many important applications, such as recommendation and retrieval-augmented generation (RAG). It relies on vector indices to enable efficient search. However, these indices require storing high-dimensional embeddings and large index metadata, whose total size can be several times larger than the original data (e.g., text chunks). Such high storage overhead makes it difficult, or even impractical, to deploy vector search on personal devices or large-scale datasets. To tackle this problem, we propose LEANN, a storage-efficient index for vector search that recomputes embeddings on the fly instead of storing them, and compresses state-of-the-art proximity graph indices while preserving search accuracy. LEANN delivers high-quality vector search while using only a fraction of the storage (e.g., 5% of the original data) and supporting storage-efficient index construction and updates. On real-world benchmarks, LEANN reduces index size by up to 50x compared with conventional indices, while maintaining SOTA accuracy and comparable latency for RAG applications.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adjoint Schr\"odinger Bridge Sampler</title>
<link>https://arxiv.org/abs/2506.22565</link>
<guid>https://arxiv.org/abs/2506.22565</guid>
<content:encoded><![CDATA[
arXiv:2506.22565v2 Announce Type: replace-cross 
Abstract: Computational methods for learning to sample from the Boltzmann distribution -- where the target distribution is known only up to an unnormalized energy function -- have advanced significantly recently. Due to the lack of explicit target samples, however, prior diffusion-based methods, known as diffusion samplers, often require importance-weighted estimation or complicated learning processes. Both trade off scalability with extensive evaluations of the energy and model, thereby limiting their practical usage. In this work, we propose Adjoint Schr\"odinger Bridge Sampler (ASBS), a new diffusion sampler that employs simple and scalable matching-based objectives yet without the need to estimate target samples during training. ASBS is grounded on a mathematical model -- the Schr\"odinger Bridge -- which enhances sampling efficiency via kinetic-optimal transportation. Through a new lens of stochastic optimal control theory, we demonstrate how SB-based diffusion samplers can be learned at scale via Adjoint Matching and prove convergence to the global solution. Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to arbitrary source distributions by relaxing the so-called memoryless condition that largely restricts the design space. Through extensive experiments, we demonstrate the effectiveness of ASBS on sampling from classical energy functions, amortized conformer generation, and molecular Boltzmann distributions. Code available at https://github.com/facebookresearch/adjoint_samplers
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems</title>
<link>https://arxiv.org/abs/2508.00721</link>
<guid>https://arxiv.org/abs/2508.00721</guid>
<content:encoded><![CDATA[
arXiv:2508.00721v2 Announce Type: replace-cross 
Abstract: We present FMPlug, a novel plug-in framework that enhances foundation flow-matching (FM) priors for solving ill-posed inverse problems. Unlike traditional approaches that rely on domain-specific or untrained priors, FMPlug smartly leverages two simple but powerful insights: the similarity between observed and desired objects and the Gaussianity of generative flows. By introducing a time-adaptive warm-up strategy and sharp Gaussianity regularization, FMPlug unlocks the true potential of domain-agnostic foundation models. Our method beats state-of-the-art methods that use foundation FM priors by significant margins, on image super-resolution and Gaussian deblurring.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models</title>
<link>https://arxiv.org/abs/2508.03356</link>
<guid>https://arxiv.org/abs/2508.03356</guid>
<content:encoded><![CDATA[
arXiv:2508.03356v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) is an established paradigm for training deep learning models on decentralized data. However, as the size of the models grows, conventional FL approaches often require significant computational resources on client devices, which may not be feasible. We introduce FedPromo, a novel framework that enables efficient adaptation of large-scale foundation models stored on a central server to new domains encountered only by remote clients. Instead of directly training the large model on client devices, FedPromo optimizes lightweight proxy models via FL, significantly reducing computational overhead while maintaining privacy. Our method follows a two-stage process: first, server-side knowledge distillation aligns the representations of a large-scale foundation model (e.g., a transformer) with those of a compact counterpart (e.g., a CNN). Then, the compact model encoder is deployed to client devices, where trainable classifiers are learned locally. These classifiers are subsequently aggregated and seamlessly transferred back to the foundation model, facilitating personalized adaptation without requiring direct access to user data. Through novel regularization strategies, our framework enables decentralized multi-domain learning, balancing performance, privacy, and resource efficiency. Extensive experiments on five image classification benchmarks demonstrate that FedPromo outperforms existing methods while assuming limited-resource clients.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeFix: Targeted Model Repair via Controlled Image Generation</title>
<link>https://arxiv.org/abs/2508.08701</link>
<guid>https://arxiv.org/abs/2508.08701</guid>
<content:encoded><![CDATA[
arXiv:2508.08701v2 Announce Type: replace-cross 
Abstract: Deep learning models for visual recognition often exhibit systematic errors due to underrepresented semantic subpopulations. Although existing debugging frameworks can pinpoint these failures by identifying key failure attributes, repairing the model effectively remains difficult. Current solutions often rely on manually designed prompts to generate synthetic training images -- an approach prone to distribution shift and semantic errors. To overcome these challenges, we introduce a model repair module that builds on an interpretable failure attribution pipeline. Our approach uses a conditional text-to-image model to generate semantically faithful and targeted images for failure cases. To preserve the quality and relevance of the generated samples, we further employ a large vision-language model (LVLM) to filter the outputs, enforcing alignment with the original data distribution and maintaining semantic consistency. By retraining vision models with this rare-case-augmented synthetic dataset, we significantly reduce errors associated with rare cases. Our experiments demonstrate that this targeted repair strategy improves model robustness without introducing new bugs. Code is available at https://github.com/oxu2/SafeFix
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Exponential Mixed Frequency Growth through Frequency Selection</title>
<link>https://arxiv.org/abs/2508.10533</link>
<guid>https://arxiv.org/abs/2508.10533</guid>
<content:encoded><![CDATA[
arXiv:2508.10533v4 Announce Type: replace-cross 
Abstract: Quantum machine learning research has expanded rapidly due to potential computational advantages over classical methods. Angle encoding has emerged as a popular choice as feature map (FM) for embedding classical data into quantum models due to its simplicity and natural generation of truncated Fourier series, providing universal function approximation capabilities. Efficient FMs within quantum circuits can exploit exponential scaling of Fourier frequencies, with multi-dimensional inputs introducing additional exponential growth through mixed-frequency terms. Despite this promising expressive capability, practical implementation faces significant challenges. Through controlled experiments with white-box target functions, we demonstrate that training failures can occur even when all relevant frequencies are theoretically accessible. We illustrate how two primary known causes lead to unsuccessful optimization: insufficient trainable parameters relative to the model's frequency content, and limitations imposed by the ansatz's dynamic lie algebra dimension, but also uncover an additional parameter burden: the necessity of controlling non-unique frequencies within the model. To address this, we propose near-zero weight initialization to suppress unnecessary duplicate frequencies. For target functions with a priori frequency knowledge, we introduce frequency selection as a practical solution that reduces parameter requirements and mitigates the exponential growth that would otherwise render problems intractable due to parameter insufficiency. Our frequency selection approach achieved near-optimal performance (median $R^2 \approx 0.95$) with 78\% of the parameters needed by the best standard approach in 10 randomly chosen target functions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LFaB: Low fidelity as Bias for Active Learning in the chemical configuration space</title>
<link>https://arxiv.org/abs/2508.15577</link>
<guid>https://arxiv.org/abs/2508.15577</guid>
<content:encoded><![CDATA[
arXiv:2508.15577v2 Announce Type: replace-cross 
Abstract: Active learning promises to provide an optimal training sample selection procedure in the construction of machine learning models. It often relies on minimizing the model's variance, which is assumed to decrease the prediction error. Still, it is frequently even less efficient than pure random sampling. Motivated by the bias-variance decomposition, we propose to minimize the model's bias instead of its variance. By doing so, we are able to almost exactly match the best-case error over all possible greedy sample selection procedures for a relevant application. Our bias approximation is based on using cheap to calculate low fidelity data as known from $\Delta$-ML or multifidelity machine learning. We exemplify our approach for a wider class of applications in quantum chemistry including predicting excitation energies and ab initio potential energy surfaces. Here, the proposed method reduces training data consumption by up to an order of magnitude compared to standard active learning.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.17811</link>
<guid>https://arxiv.org/abs/2508.17811</guid>
<content:encoded><![CDATA[
arXiv:2508.17811v2 Announce Type: replace-cross 
Abstract: Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang.github.io/meshsplat_web
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning</title>
<link>https://arxiv.org/abs/2509.05362</link>
<guid>https://arxiv.org/abs/2509.05362</guid>
<content:encoded><![CDATA[
arXiv:2509.05362v3 Announce Type: replace-cross 
Abstract: Scams exploiting real-time social engineering -- such as phishing, impersonation, and phone fraud -- remain a persistent and evolving threat across digital platforms. Existing defenses are largely reactive, offering limited protection during active interactions. We propose a privacy-preserving, AI-in-the-loop framework that proactively detects and disrupts scam conversations in real time. The system combines instruction-tuned artificial intelligence with a safety-aware utility function that balances engagement with harm minimization, and employs federated learning to enable continual model updates without raw data sharing. Experimental evaluations show that the system produces fluent and engaging responses (perplexity as low as 22.3, engagement $\approx$0.80), while human studies confirm significant gains in realism, safety, and effectiveness over strong baselines. In federated settings, models trained with FedAvg sustain up to 30 rounds while preserving high engagement ($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage ($\leq$0.0085). Even with differential privacy, novelty and safety remain stable, indicating that robust privacy can be achieved without sacrificing performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3, MD-Judge) shows a straightforward pattern: stricter moderation settings reduce the chance of exposing personal information, but they also limit how much the model engages in conversation. In contrast, more relaxed settings allow longer and richer interactions, which improve scam detection, but at the cost of higher privacy risk. To our knowledge, this is the first framework to unify real-time scam-baiting, federated privacy preservation, and calibrated safety moderation into a proactive defense paradigm.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CardioComposer: Leveraging Differentiable Geometry for Compositional Control of Anatomical Diffusion Models</title>
<link>https://arxiv.org/abs/2509.08015</link>
<guid>https://arxiv.org/abs/2509.08015</guid>
<content:encoded><![CDATA[
arXiv:2509.08015v2 Announce Type: replace-cross 
Abstract: Generative models of 3D cardiovascular anatomy can synthesize informative structures for clinical research and medical device evaluation, but face a trade-off between geometric controllability and realism. We propose CardioComposer: a programmable, inference-time framework for generating multi-class anatomical label maps based on interpretable ellipsoidal primitives. These primitives represent geometric attributes such as the size, shape, and position of discrete substructures. We specifically develop differentiable measurement functions based on voxel-wise geometric moments, enabling loss-based gradient guidance during diffusion model sampling. We demonstrate that these losses can constrain individual geometric attributes in a disentangled manner and provide compositional control over multiple substructures. Finally, we show that our method is compatible with a wide array of anatomical systems containing non-convex substructures, spanning cardiac, vascular, and skeletal organs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions</title>
<link>https://arxiv.org/abs/2509.17177</link>
<guid>https://arxiv.org/abs/2509.17177</guid>
<content:encoded><![CDATA[
arXiv:2509.17177v3 Announce Type: replace-cross 
Abstract: We conduct a moderate-scale contamination-free (to some extent) evaluation of current large reasoning models (LRMs) with some preliminary findings. We also release ROME, our evaluation benchmark for vision language models intended to test reasoning from visual clues. We attach links to the benchmark, evaluation data, and other updates on this website: https://flageval-baai.github.io/LRM-Eval/
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanGym: A Benchmark Environment for Underwater Embodied Agents</title>
<link>https://arxiv.org/abs/2509.26536</link>
<guid>https://arxiv.org/abs/2509.26536</guid>
<content:encoded><![CDATA[
arXiv:2509.26536v2 Announce Type: replace-cross 
Abstract: We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Find Fantastic AI Papers: Self-Rankings as a Powerful Predictor of Scientific Impact Beyond Peer Review</title>
<link>https://arxiv.org/abs/2510.02143</link>
<guid>https://arxiv.org/abs/2510.02143</guid>
<content:encoded><![CDATA[
arXiv:2510.02143v2 Announce Type: replace-cross 
Abstract: Peer review in academic research aims not only to ensure factual correctness but also to identify work of high scientific potential that can shape future research directions. This task is especially critical in fast-moving fields such as artificial intelligence (AI), yet it has become increasingly difficult given the rapid growth of submissions. In this paper, we investigate an underexplored measure for identifying high-impact research: authors' own rankings of their multiple submissions to the same AI conference. Grounded in game-theoretic reasoning, we hypothesize that self-rankings are informative because authors possess unique understanding of their work's conceptual depth and long-term promise. To test this hypothesis, we conducted a large-scale experiment at a leading AI conference, where 1,342 researchers self-ranked their 2,592 submissions by perceived quality. Tracking outcomes over more than a year, we found that papers ranked highest by their authors received twice as many citations as their lowest-ranked counterparts; self-rankings were especially effective at identifying highly cited papers (those with over 150 citations). Moreover, we showed that self-rankings outperformed peer review scores in predicting future citation counts. Our results remained robust after accounting for confounders such as preprint posting time and self-citations. Together, these findings demonstrate that authors' self-rankings provide a reliable and valuable complement to peer review for identifying and elevating high-impact research in AI.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Thresholds for Identifiability and Stability:Finite-Sample Phase Transitions in High-Dimensional Learning</title>
<link>https://arxiv.org/abs/2510.03809</link>
<guid>https://arxiv.org/abs/2510.03809</guid>
<content:encoded><![CDATA[
arXiv:2510.03809v2 Announce Type: replace-cross 
Abstract: In high-dimensional learning, models remain stable until they collapse abruptly once the sample size falls below a critical level. This instability is not algorithm-specific but a geometric mechanism: when the weakest Fisher eigendirection falls beneath sample-level fluctuations, identifiability fails. Our Fisher Threshold Theorem formalizes this by proving that stability requires the minimal Fisher eigenvalue to exceed an explicit $O(\sqrt{d/n})$ bound. Unlike prior asymptotic or model-specific criteria, this threshold is finite-sample and necessary, marking a sharp phase transition between reliable concentration and inevitable failure. To make the principle constructive, we introduce the Fisher floor, a verifiable spectral regularization robust to smoothing and preconditioning. Synthetic experiments on Gaussian mixtures and logistic models confirm the predicted transition, consistent with $d/n$ scaling. Statistically, the threshold sharpens classical eigenvalue conditions into a non-asymptotic law; learning-theoretically, it defines a spectral sample-complexity frontier, bridging theory with diagnostics for robust high-dimensional inference.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction</title>
<link>https://arxiv.org/abs/2510.15691</link>
<guid>https://arxiv.org/abs/2510.15691</guid>
<content:encoded><![CDATA[
arXiv:2510.15691v3 Announce Type: replace-cross 
Abstract: In quantitative investing, return prediction supports various tasks, including stock selection, portfolio optimization, and risk management. Quantitative factors, such as valuation, quality, and growth, capture various characteristics of stocks. Unstructured data, like news and transcripts, has attracted growing attention, driven by recent advances in large language models (LLMs). This paper examines effective methods for leveraging multimodal factors and newsflow in return prediction and stock selection. First, we introduce a fusion learning framework to learn a unified representation from factors and newsflow representations generated by an LLM. Within this framework, we compare three methods of different architectural complexities: representation combination, representation summation, and attentive representations. Next, building on the limitation of fusion learning observed in empirical comparison, we explore the mixture model that adaptively combines predictions made by single modalities and their fusion. To mitigate the training instability of the mixture model, we introduce a decoupled training approach with theoretical insights. Finally, our experiments on real investment universes yield several insights into effective multimodal modeling of factors and news for stock return prediction and selection.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents</title>
<link>https://arxiv.org/abs/2510.16786</link>
<guid>https://arxiv.org/abs/2510.16786</guid>
<content:encoded><![CDATA[
arXiv:2510.16786v2 Announce Type: replace-cross 
Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve software engineering tasks, are becoming increasingly powerful. However, their practical deployment is hindered by significant and unpredictable costs. This challenge arises from a combination of factors: quadratically growing token counts with each turn, the high price of models, the large number of turns required for real-world tasks, and the tendency of agents to take inefficient or unnecessary actions. While existing research focuses on optimizing individual turns, the strategic control of the total number of turns remains an underexplored area for managing agent performance and cost. To address this gap, we conduct a comprehensive empirical study on SWE-bench using three state-of-the-art models and evaluate the impact of three distinct turn-control strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a novel dynamic-turn strategy that grants extensions on-demand. Our findings first reveal a fundamental trade-off in the unrestricted setting, where no single model excels across performance, cost, and turn efficiency. We then show that a fixed-turn limit, specifically at the 75th percentile of the baseline, serves as a "sweet spot", substantially reducing costs (by 24%-68%) with minimal impact on solve rates. Most significantly, the dynamic-turn strategy consistently outperforms fixed-limit approaches, achieving comparable or better solve rates while further reducing costs by an additional 12%-24% by intelligently allocating resources only to tasks that need them. This work provides the first systematic analysis of turn-control strategies, offering simple yet effective guidelines for developers to balance cost and efficacy. We demonstrate that dynamic resource allocation is a superior, easy-to-implement approach for deploying powerful yet economically viable coding agents.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LightMem: Lightweight and Efficient Memory-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.18866</link>
<guid>https://arxiv.org/abs/2510.18866</guid>
<content:encoded><![CDATA[
arXiv:2510.18866v2 Announce Type: replace-cross 
Abstract: Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. On LongMemEval and LoCoMo, using GPT and Qwen backbones, LightMem consistently surpasses strong baselines, improving QA accuracy by up to 7.7% / 29.3%, reducing total token usage by up to 38x / 20.9x and API calls by up to 30x / 55.5x, while purely online test-time costs are even lower, achieving up to 106x / 117x token reduction and 159x / 310x fewer API calls. The code is available at https://github.com/zjunlp/LightMem.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extrapolation to infinite model space of no-core shell model calculations using machine learning</title>
<link>https://arxiv.org/abs/2511.05061</link>
<guid>https://arxiv.org/abs/2511.05061</guid>
<content:encoded><![CDATA[
arXiv:2511.05061v2 Announce Type: replace-cross 
Abstract: An ensemble of neural networks is employed to extrapolate no-core shell model (NCSM) results to infinite model space for light nuclei. We present a review of our neural network extrapolations of the NCSM results obtained with the Daejeon16 NN interaction in different model spaces and with different values of the NCSM basis parameter $\hbar\Omega$ for energies of nuclear states and root-mean-square (rms) radii of proton, neutron and matter distributions in light nuclei. The method yields convergent predictions with quantifiable uncertainties. Ground-state energies for $^{6}$Li, $^{6}$He, and the unbound $^{6}$Be, as well as the excited $(3^{+},0)$ and $(0^{+},1)$ states of $^{6}$Li, are obtained within a few hundred keV of experiment. The extrapolated radii of bound states converge well. In contrast, radii of unbound states in $^{6}$Be and $^{6}$Li do not stabilize.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Validate Generative Models: a Goodness-of-Fit Approach</title>
<link>https://arxiv.org/abs/2511.09118</link>
<guid>https://arxiv.org/abs/2511.09118</guid>
<content:encoded><![CDATA[
arXiv:2511.09118v2 Announce Type: replace-cross 
Abstract: Generative models are increasingly central to scientific workflows, yet their systematic use and interpretation require a proper understanding of their limitations through rigorous validation. Classic approaches struggle with scalability, statistical power, or interpretability when applied to high-dimensional data, making it difficult to certify the reliability of these models in realistic, high-dimensional scientific settings. Here, we propose the use of the New Physics Learning Machine (NPLM), a learning-based approach to goodness-of-fit testing inspired by the Neyman--Pearson construction, to test generative networks trained on high-dimensional scientific data. We demonstrate the performance of NPLM for validation in two benchmark cases: generative models trained on mixtures of Gaussian models with increasing dimensionality, and a public end-to-end model, known as FlowSim, developed to generate high-energy physics collision events. We demonstrate that the NPLM can serve as a powerful validation method while also providing a means to diagnose sub-optimally modeled regions of the data.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion</title>
<link>https://arxiv.org/abs/2511.14301</link>
<guid>https://arxiv.org/abs/2511.14301</guid>
<content:encoded><![CDATA[
arXiv:2511.14301v2 Announce Type: replace-cross 
Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks</title>
<link>https://arxiv.org/abs/2505.01218</link>
<guid>https://arxiv.org/abs/2505.01218</guid>
<content:encoded><![CDATA[
<div> Kernel Logistic Regression, Hopfield networks, storage capacity, kernel width scaling, kernel regression methods

<br /><br />Summary:  
This paper investigates kernel-based learning methods, particularly Kernel Logistic Regression (KLR), in enhancing the storage capacity of Hopfield networks. It provides a quantitative analysis of the attractor landscape in KLR-trained networks to better understand their performance and stability. Through extensive and statistically validated simulations, the study examines key aspects such as generality, scalability, and robustness of these networks. A comparative analysis reveals that both KLR and Kernel Ridge Regression (KRR) achieve similarly high storage capacities and exhibit clean attractor landscapes, indicating that this is a general characteristic of kernel regression methods; however, KRR is computationally faster. A significant finding is the identification of a scale-dependent law for the kernel width parameter \(\gamma\), showing that optimal memory capacity requires \(\gamma N\) to increase with the network size \(N\), meaning larger networks need more localized kernels to reduce interference among stored patterns. Under this optimized scaling, the capacity scales linearly with network size (\(P \propto N\)). Additionally, performance remains robust over a wide range of regularization parameters \(\lambda\). These empirical insights provide practical guidelines for designing high-capacity, robust associative memories and clarify how kernel methods overcome classical limitations of Hopfield-type models. <div>
arXiv:2505.01218v4 Announce Type: replace 
Abstract: Kernel-based learning methods such as Kernel Logistic Regression (KLR) can substantially increase the storage capacity of Hopfield networks, but the principles governing their performance and stability remain largely uncharacterized. This paper presents a comprehensive quantitative analysis of the attractor landscape in KLR-trained networks to establish a solid foundation for their design and application. Through extensive, statistically validated simulations, we address critical questions of generality, scalability, and robustness. Our comparative analysis shows that KLR and Kernel Ridge Regression (KRR) exhibit similarly high storage capacities and clean attractor landscapes under typical operating conditions, suggesting that this behavior is a general property of kernel regression methods, although KRR is computationally much faster. We identify a non-trivial, scale-dependent law for the kernel width $\gamma$, demonstrating that optimal capacity requires $\gamma$ to be scaled such that $\gamma N$ increases with network size $N$. This finding implies that larger networks require more localized kernels, in which each pattern's influence is more spatially confined, to mitigate inter-pattern interference. Under this optimized scaling, we provide clear evidence that storage capacity scales linearly with network size~($P \propto N$). Furthermore, our sensitivity analysis shows that performance is remarkably robust with respect to the choice of the regularization parameter $\lambda$. Collectively, these findings provide a concise set of empirical principles for designing high-capacity and robust associative memories and clarify the mechanisms that enable kernel methods to overcome the classical limitations of Hopfield-type models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion</title>
<link>https://arxiv.org/abs/2511.11667</link>
<guid>https://arxiv.org/abs/2511.11667</guid>
<content:encoded><![CDATA[
<div> Machine Unlearning, Knowledge Density, Blocks Reinsertion, Large Language Models, Gradient Propagation  

<br /><br />Summary:  
This paper addresses the challenge of machine unlearning in Large Language Models (LLMs), focusing on selectively removing harmful knowledge without retraining the entire model. Existing methods often fail to completely erase harmful knowledge, resulting in residual information that remains recoverable. To overcome these challenges, the authors propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel method that identifies specific layers containing dense harmful knowledge for targeted unlearning. The approach introduces knowledge density estimation to quantify the amount of harmful knowledge present across different layers, enabling precise localization. KUnBR employs a unique layer re-insertion strategy that extracts harmful knowledge-rich layers and reinserts them into the original LLM architecture. This strategy effectively bypasses gradient obstruction caused by cover layers, ensuring efficient gradient propagation during the unlearning process. Extensive experiments across multiple benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance, effectively removing targeted harmful knowledge while preserving the general utility and capabilities of the model. Overall, KUnBR offers a precise and efficient solution for mitigating ethical, regulatory, and privacy concerns related to harmful knowledge in LLMs without compromising their overall performance. <div>
arXiv:2511.11667v2 Announce Type: replace 
Abstract: Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Bayesian Model for Multi-stage Censoring</title>
<link>https://arxiv.org/abs/2511.11684</link>
<guid>https://arxiv.org/abs/2511.11684</guid>
<content:encoded><![CDATA[
<div> Keywords: sequential decision-making, funnel structure, Bayesian model, selective censoring, gender disparities<br /><br />Summary:<br /><br />1. Many healthcare decision processes involve funnel structures, where patients pass through multiple stages such as screenings or evaluations, with diminishing numbers advancing and increasing decision costs at each stage.  
2. In such funnels, the true outcomes (e.g., biopsy results) are only revealed at the final stage, leading to selective censoring of data which causes statistical bias in risk estimation, especially affecting underserved patient groups whose outcomes are more often censored.  
3. The authors develop a Bayesian model tailored to funnel decision processes, building upon prior work addressing selective labels and censoring issues.  
4. Synthetic data experiments demonstrate that this model can accurately recover underlying parameters and predict outcomes for patients with censored data better than baseline methods.  
5. Applying the model to real emergency department data where mortality is only observed for admitted patients, they identify gender-based disparities in hospital and ICU admission thresholds, with the estimated ICU admission threshold being higher for women (5.1%) than for men (4.5%). <div>
arXiv:2511.11684v3 Announce Type: replace 
Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moirai 2.0: When Less Is More for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.11698</link>
<guid>https://arxiv.org/abs/2511.11698</guid>
<content:encoded><![CDATA[
<div> Keywords: Moirai 2.0, time-series forecasting, decoder-only model, quantile prediction, model efficiency<br /><br />Summary:<br /><br />This article introduces Moirai 2.0, a new decoder-only foundation model designed for time-series forecasting, trained on a large corpus containing 36 million series. The model employs quantile forecasting combined with multi-token prediction strategies, which enhance both probabilistic accuracy and inference speed. When evaluated on the Gift-Eval benchmark, Moirai 2.0 ranks among the top pretrained time-series models, striking a notable balance between accuracy, computational speed, and compact model size. Compared to its predecessor, Moirai 1.0, the new version simplifies the architecture by removing masked-encoder training, multi-patch inputs, and mixture-distribution outputs, replacing them with a single-patch decoder-only framework utilizing a quantile loss function. Ablation studies confirm that this architectural shift—especially the decoder-only design paired with recursive multi-quantile decoding—accounts for most of the performance improvements. Further experiments show that Moirai 2.0 outperforms larger models in the same family and maintains robust performance across different domains. It achieves twice the inference speed and is thirty times smaller than Moirai 1.0-Large, while still delivering superior performance. The authors observe that increasing parameter count yields diminishing returns and that model accuracy decreases over longer forecasting horizons, highlighting the need for future research focused on data scaling and long-horizon prediction techniques. The code and evaluation details are released to facilitate further advancement in the field. <div>
arXiv:2511.11698v2 Announce Type: replace 
Abstract: We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Fair Representations with Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2511.11767</link>
<guid>https://arxiv.org/abs/2511.11767</guid>
<content:encoded><![CDATA[
<div> Fairness, Kolmogorov-Arnold Networks, adversarial learning, interpretability, college admissions<br /><br />Summary:<br /><br />Despite advances in fairness-aware machine learning, predictive models can still display discriminatory behavior against marginalized groups due to biased data, model design, or representation disparities, creating challenges in critical applications like college admissions. Traditional fair learning approaches struggle to perfectly balance fairness and accuracy, and black-box models lack interpretability, limiting their use in sensitive contexts. To address these issues, this work proposes integrating Kolmogorov-Arnold Networks (KANs) within an adversarial learning framework that enhances both robustness and interpretability. The spline-based architecture of KANs is theoretically analyzed to guarantee stability during the adversarial optimization process. Additionally, an adaptive fairness penalty update mechanism is introduced to systematically manage the trade-off between fairness and predictive accuracy. Empirical evaluations on two real-world college admissions datasets demonstrate that the proposed framework efficiently mitigates bias across sensitive attributes without compromising model performance. This approach offers a promising direction for deploying fair, interpretable, and stable predictive models in socially sensitive decision-making domains. <div>
arXiv:2511.11767v3 Announce Type: replace 
Abstract: Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. To circumvent these issues, we propose integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach facilitates stable adversarial learning. We derive theoretical insights into the spline-based KAN architecture that ensure stability during adversarial optimization. Additionally, an adaptive fairness penalty update mechanism is proposed to strike a balance between fairness and accuracy. We back these findings with empirical evidence on two real-world admissions datasets, demonstrating the proposed framework's efficiency in achieving fairness across sensitive attributes while preserving predictive performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers</title>
<link>https://arxiv.org/abs/2511.12041</link>
<guid>https://arxiv.org/abs/2511.12041</guid>
<content:encoded><![CDATA[
<div> Keywords: super-resolution, graph transformer, reacting flows, mesh-based modeling, multiscale

<br /><br />Summary: This work introduces a novel multiscale graph transformer (SR-GT) approach for super-resolution reconstruction of reacting flow fields on mesh-based, unstructured grids and complex geometries. Unlike traditional interpolation methods, SR-GT uses a specialized graph-based flow-field representation that captures spatial relationships effectively for spectral-element-discretized meshes. The transformer backbone in SR-GT is designed to identify and model long-range dependencies within the low-resolution flow input, allowing it to emphasize significant features before generating an accurate high-resolution flow field. This model is showcased on a challenging 2D detonation propagation scenario in a premixed hydrogen-air mixture, which exhibits intense multiscale reacting flow phenomena. The input to the model comprises a unique element plus neighborhood graph, tokenized for processing by the transformer network to output refined high-resolution data. Results demonstrate that the SR-GT framework not only achieves high accuracy in reconstructing complex flow features but also outperforms conventional interpolation-based super-resolution techniques. This suggests substantial potential for SR-GT in accelerating spatiotemporal forecasting, enhancing subgrid closure modeling, compressing data, and upscaling sparse experimental measurements in reacting flow simulations. <div>
arXiv:2511.12041v3 Announce Type: replace 
Abstract: Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element + neighborhood graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect</title>
<link>https://arxiv.org/abs/2511.14317</link>
<guid>https://arxiv.org/abs/2511.14317</guid>
<content:encoded><![CDATA[
<div> Rashomon Effect, clinical machine learning, Intervention Efficiency, Perturbation Validation Framework, model robustness  

<br /><br />Summary:  
In clinical machine learning, the presence of multiple models with comparable performance—known as the Rashomon Effect—creates significant challenges for reliable evaluation and deployment. This multiplicity is intensified by factors such as small, imbalanced, and noisy datasets, alongside high-dimensional and weakly identified clinical features, making conventional validation methods like F1 score inadequate. The uncertainty in selecting the optimal model grows especially when resource constraints and operational priorities are not integrated into performance metrics. To address these shortcomings, this work introduces two complementary tools: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric designed to measure how effectively a model identifies actionable true positives under limited intervention conditions, thereby connecting predictive accuracy with practical clinical utility. PVF offers a systematic method to evaluate model stability by assessing performance consistency across data perturbations, identifying models that maintain invariant performance in noisy or shifted validation environments. Empirical evaluations on both synthetic and real-world healthcare datasets demonstrate that these tools improve the selection of models that generalize robustly and adhere to capacity constraints. Together, IE and PVF provide an innovative direction to mitigate the Rashomon Effect in clinical machine learning applications, enhancing trustworthy model deployment and evaluation. <div>
arXiv:2511.14317v2 Announce Type: replace 
Abstract: In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Principled Coarse-Grained Acceptance for Speculative Decoding in Speech</title>
<link>https://arxiv.org/abs/2511.13732</link>
<guid>https://arxiv.org/abs/2511.13732</guid>
<content:encoded><![CDATA[
<div> Speculative decoding, Acoustic Similarity Groups, speech LLMs, Principled Coarse-Graining, token generation<br /><br />Summary:  
1. Speculative decoding aims to speed up autoregressive speech generation by using a fast draft model to propose tokens later verified by a larger target model.  
2. Traditional exact token matching in speech LLMs is too restrictive because many tokens are acoustically or semantically interchangeable, limiting acceptance rates and speedups.  
3. The authors propose Principled Coarse-Graining (PCG), which verifies proposals at the level of Acoustic Similarity Groups (ASGs) derived from the target model’s embedding space rather than individual tokens.  
4. PCG distributes each token’s probability mass across overlapping ASGs, creating an overlap-aware coarse-grained distribution and applying rejection sampling to the group variable, ensuring exactness at the group level.  
5. Experiments on the LibriTTS dataset show that PCG improves acceptance rates and throughput compared to standard speculative decoding and other speech-specific relaxations while preserving intelligibility and speaker similarity.  
6. The results indicate that acoustically aware, group-level acceptance is a general and effective strategy to accelerate speech token generation without compromising speech quality. <div>
arXiv:2511.13732v2 Announce Type: replace-cross 
Abstract: Speculative decoding accelerates autoregressive speech generation by letting a fast draft model propose tokens that a larger target model verifies. However, for speech LLMs that generate acoustic tokens, exact token matching is overly restrictive: many discrete tokens are acoustically or semantically interchangeable, reducing acceptance rates and limiting speedups. We introduce Principled Coarse-Graining (PCG), which verifies proposals at the level of Acoustic Similarity Groups (ASGs) derived from the target model's embedding space. By splitting each token's probability mass across the overlapping groups that contain it, we define an overlap-aware coarse-grained distribution and perform rejection sampling on the resulting group variable. This yields an exactness guarantee at the group level while allowing the accepted draft token to stand in for any member of the group in practice. On LibriTTS, PCG increases acceptance and throughput relative to standard speculative decoding and prior speech-specific relaxations while maintaining intelligibility and speaker similarity. These results suggest acoustically aware, group-level acceptance as a simple and general way to accelerate speech token generation while maintaining speech quality.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.14617</link>
<guid>https://arxiv.org/abs/2511.14617</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, rollout latency, dynamic load balancing, speculative decoding  

<br /><br />Summary:  
This paper introduces Seer, an innovative online context learning system designed to tackle performance bottlenecks in reinforcement learning (RL) workflows for large language models (LLMs).  First, it identifies that the rollout phase in RL iteration suffers from long-tail latency and inefficient resource utilization due to workload imbalance.  Second, Seer leverages similarities in output lengths and generation patterns among requests sharing the same prompt to optimize processing.  Third, Seer employs three novel techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding, all contributing to improved latency and throughput.  Fourth, through these mechanisms, Seer significantly reduces the long-tail latency by 75% to 93%, enhancing the speed and efficiency of the rollout phase.  Finally, evaluations on production-grade RL workloads demonstrate Seer’s capability to boost end-to-end rollout throughput by 74% to 97%, thereby substantially accelerating RL training iterations compared to current state-of-the-art synchronous RL systems. <div>
arXiv:2511.14617v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Machine Learning for Aphasic Discourse Analysis</title>
<link>https://arxiv.org/abs/2511.17553</link>
<guid>https://arxiv.org/abs/2511.17553</guid>
<content:encoded><![CDATA[
<div> Keywords: aphasia, Correct Information Unit, machine learning, discourse analysis, k-nearest neighbor<br /><br />Summary:<br /><br />1. The study focuses on analyzing spoken discourse to quantify language ability in persons with aphasia, with a specific emphasis on measuring informativeness through Correct Information Unit (CIU) analysis.<br /><br />2. CIU analysis evaluates the proportion of context-relevant and accurate words produced during speech, but its clinical application is limited by the extensive manual coding work required by speech-language pathologists (SLPs).<br /><br />3. To address this bottleneck, the authors investigated the use of five supervised machine learning (ML) models to automatically identify CIUs from transcripts of a picture description task performed by persons with aphasia.<br /><br />4. All five ML models demonstrated near-perfect accuracy (~0.995) and high AUC scores in distinguishing words from non-words, indicating strong performance on basic word recognition.<br /><br />5. However, identifying CIUs versus non-CIUs proved more challenging, with performance variability among models and the k-nearest neighbor (k-NN) model achieving the highest accuracy (0.824) and second highest AUC (0.787). This highlights the difficulty of automating CIU identification despite advances in ML.<br /><br />6. The study concludes that while ML can reliably discriminate word-level features, automating the nuanced task of CIU recognition remains complex, suggesting further research is needed for practical clinical deployment. <div>
arXiv:2511.17553v1 Announce Type: new 
Abstract: Analyzing spoken discourse is a valid means of quantifying language ability in persons with aphasia. There are many ways to quantify discourse, one common way being to evaluate the informativeness of the discourse. That is, given the total number of words produced, how many of those are context-relevant and accurate. This type of analysis is called Correct Information Unit (CIU) analysis and is one of the most prevalent discourse analyses used by speech-language pathologists (SLPs). Despite this, CIU analysis in the clinic remains limited due to the manual labor needed by SLPs to code and analyze collected speech. Recent advances in machine learning (ML) seek to augment such labor by automating modeling of propositional, macrostructural, pragmatic, and multimodal dimensions of discourse. To that end, this study evaluated five ML models for reliable identification of Correct Information Units (CIUs, Nicholas & Brookshire, 1993), during a picture description task. The five supervised ML models were trained using randomly selected human-coded transcripts and accompanying words and CIUs from persons with aphasia. The baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995) with high AUC range (0.914 min, 0.995 max). In contrast, CIU vs non-CIU showed a greater variability, with the k-nearest neighbor (k-NN) model the highest accuracy (0.824) and second highest AUC (0.787). These findings indicate that while the supervised ML models can distinguish word from not word, identifying CIUs is challenging.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks</title>
<link>https://arxiv.org/abs/2511.17564</link>
<guid>https://arxiv.org/abs/2511.17564</guid>
<content:encoded><![CDATA[
<div> Keywords: bidirectional LSTM, transient astronomical objects, PLAsTiCC, class imbalance, light curve classification<br /><br />Summary:<br /><br />This study develops a bidirectional Long Short-Term Memory (LSTM) neural network to classify transient astronomical object light curves using the PLAsTiCC dataset. To address the original class imbalance among fourteen object types, the classes were consolidated into five broader categories: S-Like, Fast, Long, Periodic, and Non-Periodic. The preprocessing pipeline involved padding the light curves, temporal rescaling, and flux normalization, with masking layers incorporated into the network to handle missing data. The model was trained and tested on a set of 19,920 objects. Results showed high accuracy for the S-Like and Periodic classes, achieving ROC AUC scores of 0.95 and 0.99 respectively, and Precision-Recall AUC scores of 0.98 and 0.89. However, performance was notably weaker for the Fast and Long classes, with the Long class reaching only a 0.68 ROC AUC. Distinguishing Periodic from Non-Periodic objects remained challenging. Further analysis using partial light curve data (at 5, 10, and 20 days post-detection) revealed performance drops and an increased tendency to misclassify toward the S-Like class. The study concludes that class imbalance and limited temporal data around detection are key limitations, recommending implementation of class balancing methods and focused preprocessing on detection intervals to enhance classification accuracy. <div>
arXiv:2511.17564v1 Announce Type: new 
Abstract: This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs</title>
<link>https://arxiv.org/abs/2511.17566</link>
<guid>https://arxiv.org/abs/2511.17566</guid>
<content:encoded><![CDATA[
<div> Root cause analysis, microservice systems, cascaded conditional learning, heterogeneous hypergraph, failure propagation<br /><br />Summary: This paper addresses root cause analysis in microservice systems by focusing on two core diagnostic tasks: root cause localization (RCL) and failure type identification (FTI). Traditional methods tend to use a joint learning approach to combine both tasks, aiming to share information and reduce training time. However, this joint learning ignores the causal dependencies between RCL and FTI, which hampers effective collaboration and information flow between the tasks. Additionally, existing approaches typically consider only point-to-point relationships between system instances, neglecting the group influences caused by deployment configurations and load balancing mechanisms. To overcome these issues, the authors propose CCLH, a novel framework based on cascaded conditional learning that structures the diagnostic tasks in a sequential causally dependent manner. CCLH introduces a three-level taxonomy for modeling group influences among instances and employs a heterogeneous hypergraph to accurately represent these intricate relationships. This hypergraph facilitates the simulation and understanding of failure propagation within the microservice system. Experimental validation on datasets from three microservice benchmarks confirms that CCLH significantly outperforms state-of-the-art methods in both root cause localization and failure type identification, demonstrating its effectiveness and robustness in diagnosing microservice failures. <div>
arXiv:2511.17566v1 Announce Type: new 
Abstract: Root cause analysis in microservice systems typically involves two core tasks: root cause localization (RCL) and failure type identification (FTI). Despite substantial research efforts, conventional diagnostic approaches still face two key challenges. First, these methods predominantly adopt a joint learning paradigm for RCL and FTI to exploit shared information and reduce training time. However, this simplistic integration neglects the causal dependencies between tasks, thereby impeding inter-task collaboration and information transfer. Second, these existing methods primarily focus on point-to-point relationships between instances, overlooking the group nature of inter-instance influences induced by deployment configurations and load balancing. To overcome these limitations, we propose CCLH, a novel root cause analysis framework that orchestrates diagnostic tasks based on cascaded conditional learning. CCLH provides a three-level taxonomy for group influences between instances and incorporates a heterogeneous hypergraph to model these relationships, facilitating the simulation of failure propagation. Extensive experiments conducted on datasets from three microservice benchmarks demonstrate that CCLH outperforms state-of-the-art methods in both RCL and FTI.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization</title>
<link>https://arxiv.org/abs/2511.17568</link>
<guid>https://arxiv.org/abs/2511.17568</guid>
<content:encoded><![CDATA[
<div> Keywords: Offline reinforcement learning, Data corruption, Sharpness-Aware Minimization, Robustness, D4RL benchmarks<br /><br />Summary:<br /><br />Offline reinforcement learning (RL) faces significant challenges when exposed to data corruption, often resulting in poor performance even with robust algorithms. The authors identify that data corruption induces sharp minima in the loss landscape, which detrimentally affects the generalization capability of learned policies. To tackle this, they introduce the application of Sharpness-Aware Minimization (SAM) to offline RL, positioning it as a general-purpose and plug-and-play optimizer that guides learning towards flatter minima and, consequently, more robust solutions. SAM is integrated into two strong baseline algorithms: IQL, which is already a top performer under corrupted data conditions, and RIQL, which is specifically designed to enhance robustness against data corruption. Their approach is empirically validated on D4RL benchmarks featuring both random and adversarial corruptions, where SAM-enhanced methods consistently surpass the original baselines in performance. Additionally, visualizations of the reward surface support the claim that SAM helps the training process to converge to smoother and flatter loss landscapes. This evidence strongly suggests that SAM effectively improves the robustness and generalization of offline RL agents operating under challenging corrupted data environments. <div>
arXiv:2511.17568v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) is vulnerable to real-world data corruption, with even robust algorithms failing under challenging observation and mixture corruptions. We posit this failure stems from data corruption creating sharp minima in the loss landscape, leading to poor generalization. To address this, we are the first to apply Sharpness-Aware Minimization (SAM) as a general-purpose, plug-and-play optimizer for offline RL. SAM seeks flatter minima, guiding models to more robust parameter regions. We integrate SAM into strong baselines for data corruption: IQL, a top-performing offline RL algorithm in this setting, and RIQL, an algorithm designed specifically for data-corruption robustness. We evaluate them on D4RL benchmarks with both random and adversarial corruption. Our SAM-enhanced methods consistently and significantly outperform the original baselines. Visualizations of the reward surface confirm that SAM finds smoother solutions, providing strong evidence for its effectiveness in improving the robustness of offline RL agents.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Binary BPE: A Family of Cross-Platform Tokenizers for Binary Analysis</title>
<link>https://arxiv.org/abs/2511.17573</link>
<guid>https://arxiv.org/abs/2511.17573</guid>
<content:encoded><![CDATA[
<div> Binary BPE, tokenization, executables, transformers, malware detection<br /><br />Summary: The paper addresses the limitations of existing byte-level tokenization methods used in sequence models for binary analysis, which often waste transformer context window capacity and struggle with arbitrary byte sequences. To overcome this, the authors introduce the Binary Byte Pair Encoding (BPE) tokenizer family, designed specifically for executables across multiple platforms like Linux, Windows, macOS, Android, and malware sources. They provide trained tokenizers with vocabularies ranging from 4K to 64K tokens, facilitating both scalability studies and practical implementations from low-resource devices to high-throughput datacenters. These tokenizers can identify meaningful binary patterns such as ELF/PE headers and instruction sequences and achieve multi-byte compression per token. Benchmarking on uncompressed executable formats (ELF, PE, Mach-O) demonstrates that these tokenizers enable transformers to process approximately 2-3 times more binary content per fixed-length context window compared to raw byte tokenization. This improvement supports enhanced research and deployment for various binary-focused applications including content identification, malware detection, reverse engineering, and optimization. The authors release these Binary BPE tokenizers openly on HuggingFace, providing a ready-to-use foundation for binary language models and efficient context-aware tools. <div>
arXiv:2511.17573v1 Announce Type: new 
Abstract: Sequence models for binary analysis are bottlenecked by byte-level tokenization: raw bytes waste precious context window capacity for transformers and other neural network architectures, and many existing text-oriented tokenizers fail on arbitrary 0x00--0xFF sequences. To address this issue, we introduce the Binary BPE tokenizer family, a set of cross-platform Byte Pair Encoding (BPE) tokenizers for executables trained on a large corpus of binaries spanning multiple platforms, architectures, and operating systems, including Linux, Windows, macOS, Android, and malware sources. We release trained tokenizers with vocabularies of 4K, 8K, 16K, 32K, and 64K tokens, enabling both systematic scaling studies and practical deployment from resource-constrained edge devices to high-throughput datacenters. These tokenizers discover interpretable patterns (ELF/PE headers, instruction sequences, cross-platform strings) while yielding multi-byte compression per token. On representative uncompressed executables (e.g., ELF/PE/Mach-O rather than compressed APKs), the Binary BPE tokenizers typically allow for roughly 2-3x more binary content per fixed-length transformer context window than raw bytes, enabling more efficient research and practical deployment for content identification, malware detection, reverse engineering, and optimization. We release the trained Binary BPE tokenizers on HuggingFace, providing a drop-in, open-source foundation for binary-focused language models and context-efficient agentic tools.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.17577</link>
<guid>https://arxiv.org/abs/2511.17577</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic attention head pruning, knowledge distillation, large language models, mathematical reasoning, efficiency optimization<br /><br />Summary:<br /><br />This paper addresses the challenge of deploying large language models for mathematical reasoning tasks efficiently, given their high computational and storage costs. It introduces a lightweight optimization method that combines dynamic attention head pruning with knowledge distillation. The dynamic pruning mechanism evaluates the importance of each attention head in the multi-head attention module using both weight norms and entropy, enabling real-time removal of redundant heads to reduce computational load. To counteract the potential accuracy loss due to pruning, knowledge distillation is employed to transfer knowledge from the original large model to the smaller pruned model, preserving its reasoning capabilities. Experiments on the Math23k and ASDiv-A datasets demonstrate the effectiveness of this approach. Specifically, on Math23k at a 30% pruning ratio, the model’s parameters decreased by 18.7%, inference speed increased by 27.5%, FLOPs were reduced by 19.3%, while accuracy only slightly dropped by 0.7% (from 84.4% to 83.7%). These improvements indicate a substantial gain in efficiency without significantly sacrificing performance, making this method a practical solution for deploying large language models in mathematical equation solving tasks. <div>
arXiv:2511.17577v1 Announce Type: new 
Abstract: With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation</title>
<link>https://arxiv.org/abs/2511.17579</link>
<guid>https://arxiv.org/abs/2511.17579</guid>
<content:encoded><![CDATA[
<div> multi-value alignment, large language models, reinforcement learning, value conflicts, Pareto frontier<br /><br />Summary:<br /><br />With the growing capabilities of large language models (LLMs), ensuring these models align with multiple human values for ethical and safe behavior is a complex problem. Existing alignment techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) struggle with instability and inefficiency when optimizing for multiple values simultaneously. They also have difficulty resolving conflicts that arise between competing human values. To overcome these issues, the authors introduce a new framework called Multi-Value Alignment (MVA), which reduces alignment degradation caused by parameter interference among different values by minimizing their mutual information. Additionally, the paper presents a value extrapolation strategy designed to efficiently explore the Pareto frontier, enabling the creation of LLMs that reflect diverse preferences across multiple values. This approach helps achieve better trade-offs between potentially conflicting values in a more stable and effective manner. Extensive experiments validate that MVA consistently outperforms current baseline methods in aligning LLMs with multiple human values while handling their conflicts efficiently. The framework represents a promising step forward in multi-value alignment of LLMs for safer and more ethical AI systems. <div>
arXiv:2511.17579v1 Announce Type: new 
Abstract: With the rapid advancement of large language models (LLMs), aligning them with human values for safety and ethics has become a critical challenge. This problem is especially challenging when multiple, potentially conflicting human values must be considered and balanced. Although several variants of existing alignment methods (such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)) have been proposed to address multi-value alignment, they suffer from notable limitations: 1) they are often unstable and inefficient in multi-value optimization; and 2) they fail to effectively handle value conflicts. As a result, these approaches typically struggle to achieve optimal trade-offs when aligning multiple values.
  To address this challenge, we propose a novel framework called Multi-Value Alignment (MVA). It mitigates alignment degradation caused by parameter interference among diverse human values by minimizing their mutual information. Furthermore, we propose a value extrapolation strategy to efficiently explore the Pareto frontier, thereby constructing a set of LLMs with diverse value preferences. Extensive experiments demonstrate that MVA consistently outperforms existing baselines in aligning LLMs with multiple human values.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoCogNav: Cognition-aware Human Egocentric Navigation</title>
<link>https://arxiv.org/abs/2511.17581</link>
<guid>https://arxiv.org/abs/2511.17581</guid>
<content:encoded><![CDATA[
<div> Keywords: egocentric navigation, path uncertainty, trajectory forecasting, human factors, Cognition-aware Egocentric Navigation dataset  

<br /><br />Summary:  
This paper addresses the challenge of incorporating cognitive and experiential human factors into navigation modeling to better understand human-environment interaction and improve social navigation and assistive wayfinding. Most prior approaches focus on predicting motion in fully observable scenes but fail to capture the subjective human experience of space. To overcome this, the authors propose EgoCogNav, a novel multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent variable. EgoCogNav jointly forecasts future trajectories and head motion by integrating scene features with sensory input, reflecting more naturalistic human navigation behavior. To support research advancements, the paper also introduces the Cognition-aware Egocentric Navigation (CEN) dataset, which contains six hours of real-world egocentric video recordings showcasing diverse navigation behaviors in practical, everyday environments. Experimental results demonstrate that EgoCogNav effectively learns to model perceived uncertainty, correlating strongly with human-like behaviors such as environmental scanning, hesitation, and backtracking. Furthermore, the model generalizes well to unseen environments, indicating its robustness. This work represents a significant step toward embedding cognitive and perceptual human factors into navigation systems, enabling more realistic and socially aware navigation models. <div>
arXiv:2511.17581v1 Announce Type: new 
Abstract: Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.17582</link>
<guid>https://arxiv.org/abs/2511.17582</guid>
<content:encoded><![CDATA[
<div> Keywords: Parameter-efficient fine-tuning, GateRA, token-aware modulation, entropy-based regularization, commonsense reasoning benchmarks<br /><br />Summary:<br /><br />1. This paper addresses limitations in current parameter-efficient fine-tuning (PEFT) methods like LoRA, DoRA, and HiRA, which apply uniform, static updates across all tokens without considering token-specific importance or difficulty. Such uniform treatment can cause overfitting on simple content and under-adaptation on critical or complex inputs, especially in autoregressive models with different prefill and decoding behaviors.<br /><br />2. The authors propose GateRA, a unified framework that incorporates token-aware gating to dynamically modulate the strength of PEFT updates on a per-token basis. This selective adaptation preserves the pre-trained model's knowledge for easier tokens while allocating more capacity toward challenging inputs.<br /><br />3. GateRA includes entropy-based regularization to encourage near-binary gating decisions, thus producing sparse, interpretable adaptation patterns without the need for hard thresholding. This prevents diffuse or noisy update behaviors and promotes confident gating.<br /><br />4. Theoretical analysis shows that GateRA implements a soft gradient-masking effect, providing continuous and differentiable control over the adaptation process through the gating mechanism.<br /><br />5. Empirical results on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently matches or outperforms prior PEFT methods, with visualizations revealing phase-sensitive behavior where updates are suppressed in redundant prefill tokens and emphasized during decoding phases. <div>
arXiv:2511.17582v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Straight Flows: Variational Flow Matching for Efficient Generation</title>
<link>https://arxiv.org/abs/2511.17583</link>
<guid>https://arxiv.org/abs/2511.17583</guid>
<content:encoded><![CDATA[
<div> Flow Matching, Straight Trajectories, Variational Latent Code, One-step Generation, Training Efficiency<br /><br />Summary:<br /><br />This paper addresses the limitation of Flow Matching models in achieving one-step generation due to their reliance on learned curved trajectories. Previous solutions involved modifying coupling distributions or incorporating consistency and mean-velocity modeling to encourage straight-line trajectories. However, such methods often encountered problems like discrete approximation errors, unstable training, and difficulty in convergence. To overcome these challenges, the authors propose Straight Variational Flow Matching (S-VFM), a novel approach that integrates a variational latent code representing a "generation overview" into the Flow Matching framework. This inclusion explicitly enforces trajectory straightness, aiming to produce ideally linear generation paths. The S-VFM method demonstrates competitive performance on three challenging benchmarks, showing advantages in both training stability and inference efficiency compared to existing methods. Overall, S-VFM effectively improves one-step generation capabilities while enhancing model robustness and computational efficiency. <div>
arXiv:2511.17583v1 Announce Type: new 
Abstract: Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \textbf{S}traight \textbf{V}ariational \textbf{F}low \textbf{M}atching (\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Powered Text-Attributed Graph Anomaly Detection via Retrieval-Augmented Reasoning</title>
<link>https://arxiv.org/abs/2511.17584</link>
<guid>https://arxiv.org/abs/2511.17584</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, text-attributed graphs, large language models, graph neural networks, retrieval-augmented generation<br /><br />Summary:<br /><br />This paper addresses anomaly detection in text-attributed graphs (TAGs), where node information is presented as natural language, a less explored area due to the lack of standard benchmark datasets. To fill this gap, the authors introduce TAG-AD, a comprehensive benchmark dataset designed for anomaly node detection on TAGs. TAG-AD utilizes large language models (LLMs) to generate realistic anomalous node texts, which are semantically coherent but contextually inconsistent, reflecting real-world anomalies more accurately. The benchmark also includes multiple anomaly types for thorough and reproducible evaluation of graph anomaly detection (GAD) methods. Additionally, the study benchmarks existing unsupervised graph neural network (GNN)-based GAD methods alongside zero-shot LLM approaches. For zero-shot detection, a novel retrieval-augmented generation (RAG)-assisted LLM framework is proposed, which reduces dependence on fragile, hand-crafted prompts by creating a global anomaly knowledge base and distilling this into reusable analysis frameworks. Experimental results highlight a clear complementarity: LLMs excel at detecting contextual anomalies, while GNN-based methods are more effective for structural anomalies. Moreover, the RAG-assisted prompting method matches human-designed prompt performance and eliminates manual prompt engineering, demonstrating practical significance for zero-shot LLM anomaly detection in TAGs. <div>
arXiv:2511.17584v1 Announce Type: new 
Abstract: Anomaly detection on attributed graphs plays an essential role in applications such as fraud detection, intrusion monitoring, and misinformation analysis. However, text-attributed graphs (TAGs), in which node information is expressed in natural language, remain underexplored, largely due to the absence of standardized benchmark datasets. In this work, we introduce TAG-AD, a comprehensive benchmark for anomaly node detection on TAGs. TAG-AD leverages large language models (LLMs) to generate realistic anomalous node texts directly in the raw text space, producing anomalies that are semantically coherent yet contextually inconsistent and thus more reflective of real-world irregularities. In addition, TAG-AD incorporates multiple other anomaly types, enabling thorough and reproducible evaluation of graph anomaly detection (GAD) methods. With these datasets, we further benchmark existing unsupervised GNN-based GAD methods as well as zero-shot LLMs for GAD.
  As part of our zero-shot detection setup, we propose a retrieval-augmented generation (RAG)-assisted, LLM-based zero-shot anomaly detection framework. The framework mitigates reliance on brittle, hand-crafted prompts by constructing a global anomaly knowledge base and distilling it into reusable analysis frameworks. Our experimental results reveal a clear division of strengths: LLMs are particularly effective at detecting contextual anomalies, whereas GNN-based methods remain superior for structural anomaly detection. Moreover, RAG-assisted prompting achieves performance comparable to human-designed prompts while eliminating manual prompt engineering, underscoring the practical value of our RAG-assisted zero-shot LLM anomaly detection framework.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2511.17585</link>
<guid>https://arxiv.org/abs/2511.17585</guid>
<content:encoded><![CDATA[
<div> Multimodal Sentiment Analysis, modality competition, Prototype-aligned Calibration, Shapley-based Gradient Modulation, Dual-Phase Optimization<br /><br />Summary:<br /><br />Multimodal Sentiment Analysis (MSA) aims to interpret human emotions by integrating text, audio, and visual data. However, in real-world applications, modality competition arises, where dominant modalities overshadow weaker ones, reducing overall system effectiveness. To address this, the paper introduces PaSE, a new framework designed to foster better collaboration between modalities and explicitly reduce competition. PaSE incorporates Prototype-guided Calibration Learning (PCL) that refines and aligns unimodal features using an Entropic Optimal Transport mechanism, ensuring semantic consistency across modalities. For improved training stability, a Dual-Phase Optimization strategy is employed: initially, a prototype-gated fusion module extracts shared representations, followed by Shapley-based Gradient Modulation (SGM) that dynamically adjusts gradients based on each modality's contribution. Through extensive experiments on benchmark datasets IEMOCAP, MOSI, and MOSEI, PaSE demonstrates superior performance compared to existing methods. These results confirm that the proposed approach effectively mitigates modality competition while enhancing multimodal fusion for sentiment analysis tasks. <div>
arXiv:2511.17585v1 Announce Type: new 
Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal performance.In this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion and Intention Guided Multi-Modal Learning for Sticker Response Selection</title>
<link>https://arxiv.org/abs/2511.17587</link>
<guid>https://arxiv.org/abs/2511.17587</guid>
<content:encoded><![CDATA[
<div> Keywords: Sticker Response Selection, Emotion and Intention, Multi-Modal Learning, Contrastive Framework, Fusion Module<br /><br />Summary:<br /><br />The Sticker Response Selection (SRS) task focuses on choosing the most contextually appropriate sticker in online dialogues to convey emotions and intentions. Existing methods usually treat emotional cues and intentional cues separately, which can cause mismatches when emotions and intentions do not align. To overcome this problem, the authors propose a novel framework called Emotion and Intention Guided Multi-Modal Learning (EIGML), which jointly models emotions and intentions to reduce bias and improve selection accuracy. The framework introduces a Dual-Level Contrastive Framework that aligns emotional and intentional features both within individual modalities and across multiple modalities to ensure consistent representation. Moreover, an Intention-Emotion Guided Multi-Modal Fusion module is designed to progressively integrate emotional and intentional information through three components: Emotion-Guided Intention Knowledge Selection, Intention-Emotion Guided Attention Fusion, and Similarity-Adjusted Matching Mechanism. This sophisticated fusion facilitates a richer, more effective understanding of the dialogue context, enhancing sticker selection. Experimental evaluations on two public SRS datasets demonstrate that EIGML consistently surpasses state-of-the-art methods, showing higher accuracy and superior comprehension of emotional and intentional features. The authors also provide their code in the supplementary materials for further research and application. <div>
arXiv:2511.17587v1 Announce Type: new 
Abstract: Stickers are widely used in online communication to convey emotions and implicit intentions. The Sticker Response Selection (SRS) task aims to select the most contextually appropriate sticker based on the dialogue. However, existing methods typically rely on semantic matching and model emotional and intentional cues separately, which can lead to mismatches when emotions and intentions are misaligned. To address this issue, we propose Emotion and Intention Guided Multi-Modal Learning (EIGML). This framework is the first to jointly model emotion and intention, effectively reducing the bias caused by isolated modeling and significantly improving selection accuracy. Specifically, we introduce Dual-Level Contrastive Framework to perform both intra-modality and inter-modality alignment, ensuring consistent representation of emotional and intentional features within and across modalities. In addition, we design an Intention-Emotion Guided Multi-Modal Fusion module that integrates emotional and intentional information progressively through three components: Emotion-Guided Intention Knowledge Selection, Intention-Emotion Guided Attention Fusion, and Similarity-Adjusted Matching Mechanism. This design injects rich, effective information into the model and enables a deeper understanding of the dialogue, ultimately enhancing sticker selection performance. Experimental results on two public SRS datasets show that EIGML consistently outperforms state-of-the-art baselines, achieving higher accuracy and a better understanding of emotional and intentional features. Code is provided in the supplementary materials.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Llamazip: Leveraging LLaMA for Lossless Text Compression and Training Dataset Detection</title>
<link>https://arxiv.org/abs/2511.17589</link>
<guid>https://arxiv.org/abs/2511.17589</guid>
<content:encoded><![CDATA[
<div> Keywords: Llamazip, lossless compression, LLaMA3, predictive modeling, data provenance<br /><br />Summary:<br /><br />This paper presents Llamazip, an innovative lossless text compression method leveraging the predictive power of the LLaMA3 language model. The core idea behind Llamazip is to compress text by storing only the tokens that the model cannot accurately predict, thus achieving significant data reduction without any loss of information. The study investigates critical factors influencing Llamazip's efficiency, particularly quantization techniques and the size of the context window used by the language model. These factors are shown to affect both the compression ratio and the computational resources required. Furthermore, Llamazip demonstrates an intriguing secondary application: the ability to determine if a specific document was included in the training data of the language model. This capability offers important implications for verifying data provenance and addressing intellectual property concerns linked to language model training. Overall, Llamazip not only advances text compression technology but also enhances transparency and accountability in the use of large-scale language models. <div>
arXiv:2511.17589v1 Announce Type: new 
Abstract: This work introduces Llamazip, a novel lossless text compression algorithm based on the predictive capabilities of the LLaMA3 language model. Llamazip achieves significant data reduction by only storing tokens that the model fails to predict, optimizing storage efficiency without compromising data integrity. Key factors affecting its performance, including quantization and context window size, are analyzed, revealing their impact on compression ratios and computational requirements. Beyond compression, Llamazip demonstrates the potential to identify whether a document was part of the training dataset of a language model. This capability addresses critical concerns about data provenance, intellectual property, and transparency in language model training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data</title>
<link>https://arxiv.org/abs/2511.17590</link>
<guid>https://arxiv.org/abs/2511.17590</guid>
<content:encoded><![CDATA[
<div> Synthetic Data, SHAP Distance, Semantic Fidelity, Explainability, Tabular Data<br /><br />Summary: This paper addresses the limitations of current evaluation methods for synthetic tabular data, which mainly focus on distributional similarity and predictive performance but overlook semantic fidelity. Semantic fidelity refers to whether models trained on synthetic data follow similar reasoning patterns as those trained on real data. The authors propose a novel metric called SHAP Distance, defined as the cosine distance between global SHAP attribution vectors from classifiers trained on real versus synthetic datasets. They test this metric across diverse datasets including clinical health records, enterprise invoice transactions, and telecom churn logs that feature mixed data types and scales. The results show that SHAP Distance effectively identifies semantic discrepancies that traditional metrics like Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. Specifically, it reveals shifts in feature importance and captures underrepresented tail effects. This positions SHAP Distance as a valuable and practical tool for auditing the semantic fidelity of synthetic data. The study also offers guidelines for integrating attribution-based metrics into standard benchmarking processes, enhancing robustness in synthetic data evaluation across various application domains. <div>
arXiv:2511.17590v1 Announce Type: new 
Abstract: Synthetic tabular data, which are widely used in domains such as healthcare, enterprise operations, and customer analytics, are increasingly evaluated to ensure that they preserve both privacy and utility. While existing evaluation practices typically focus on distributional similarity (e.g., the Kullback-Leibler divergence) or predictive performance (e.g., Train-on-Synthetic-Test-on-Real (TSTR) accuracy), these approaches fail to assess semantic fidelity, that is, whether models trained on synthetic data follow reasoning patterns consistent with those trained on real data. To address this gap, we introduce the SHapley Additive exPlanations (SHAP) Distance, a novel explainability-aware metric that is defined as the cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets. By analyzing datasets that span clinical health records with physiological features, enterprise invoice transactions with heterogeneous scales, and telecom churn logs with mixed categorical-numerical attributes, we demonstrate that the SHAP Distance reliably identifies semantic discrepancies that are overlooked by standard statistical and predictive measures. In particular, our results show that the SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. This study positions the SHAP Distance as a practical and discriminative tool for auditing the semantic fidelity of synthetic tabular data, and offers practical guidelines for integrating attribution-based evaluation into future benchmarking pipelines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI</title>
<link>https://arxiv.org/abs/2511.17593</link>
<guid>https://arxiv.org/abs/2511.17593</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, vLLM, HuggingFace Text Generation Inference, throughput, latency<br /><br />Summary: This paper evaluates two leading open-source serving frameworks for Large Language Models (LLMs), vLLM and HuggingFace Text Generation Inference (TGI), using LLaMA-2 models ranging from 7B to 70B parameters. The study benchmarks both frameworks across multiple performance metrics, including throughput, end-to-end latency, GPU memory usage, and scalability. Results show that vLLM significantly outperforms TGI in high-concurrency environments, achieving up to 24 times higher throughput thanks to its innovative PagedAttention technique. Conversely, TGI exhibits superior tail latency performance in single-user, interactive scenarios, making it more responsive under latency-sensitive conditions. The paper provides detailed performance profiles tailored to different deployment contexts, helping practitioners select the suitable framework based on application needs. Key insights indicate that vLLM is optimal for batch processing workloads requiring high throughput, while TGI is preferred for interactive applications where lower latency is crucial despite moderate concurrency. Overall, the choice between vLLM and TGI depends on the specific use case, balancing trade-offs between throughput and latency to best match production environment demands. <div>
arXiv:2511.17593v1 Announce Type: new 
Abstract: The deployment of Large Language Models (LLMs) in production environments requires efficient inference serving systems that balance throughput, latency, and resource utilization. This paper presents a comprehensive empirical evaluation of two prominent open-source LLM serving frameworks: vLLM and HuggingFace Text Generation Inference (TGI). We benchmark these systems across multiple dimensions including throughput performance, end-to-end latency, GPU memory utilization, and scalability characteristics using LLaMA-2 models ranging from 7B to 70B parameters. Our experiments reveal that vLLM achieves up to 24x higher throughput than TGI under high-concurrency workloads through its novel PagedAttention mechanism, while TGI demonstrates lower tail latencies for interactive single-user scenarios. We provide detailed performance profiles for different deployment scenarios and offer practical recommendations for system selection based on workload characteristics. Our findings indicate that the choice between these frameworks should be guided by specific use-case requirements: vLLM excels in high-throughput batch processing scenarios, while TGI is better suited for latency-sensitive interactive applications with moderate concurrency.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention</title>
<link>https://arxiv.org/abs/2511.17594</link>
<guid>https://arxiv.org/abs/2511.17594</guid>
<content:encoded><![CDATA[
<div> Sparse GNN, AutoSAGE, CUDA scheduler, SpMM/SDDMM, performance optimization<br /><br />Summary:<br /><br />This paper addresses performance variability in sparse graph neural network (GNN) aggregations, specifically CSR SpMM and SDDMM operations, which are affected by factors such as degree skew, feature width, and GPU micro-architecture. The authors propose AutoSAGE, an input-aware CUDA scheduler that dynamically selects tiling and mapping strategies tailored to each input. AutoSAGE uses a lightweight performance estimator refined by on-device micro-probes. It incorporates a guardrail mechanism that safely falls back to vendor-provided kernels if needed, and a persistent cache enables deterministic replay of scheduling decisions. AutoSAGE supports both SpMM and SDDMM operations and integrates into a CSR-based attention pipeline involving SDDMM, row-wise softmax, and SpMM stages. Benchmarking on datasets like Reddit and OGBN-Products demonstrates that AutoSAGE matches vendor kernels at bandwidth-bound feature widths while achieving performance gains at small feature widths. Furthermore, in synthetic tests designed to stress sparsity and skew, it achieves up to 4.7x kernel-level speedups. The authors release their CUDA source code, Python bindings, a reproducible evaluation harness, and replayable cache logs to facilitate adoption and further research in optimized sparse GNN computation. <div>
arXiv:2511.17594v1 Announce Type: new 
Abstract: Sparse GNN aggregations (CSR SpMM/SDDMM) vary widely in performance with degree skew, feature width, and GPU micro-architecture. We present AutoSAGE, an input-aware CUDA scheduler that chooses tiling and mapping per input using a lightweight estimate refined by on-device micro-probes, with a guardrail that safely falls back to vendor kernels and a persistent cache for deterministic replay. AutoSAGE covers SpMM and SDDMM and composes into a CSR attention pipeline (SDDMM -> row-softmax -> SpMM). On Reddit and OGBN-Products, it matches vendor baselines at bandwidth-bound feature widths and finds gains at small widths; on synthetic sparsity and skew stress tests it achieves up to 4.7x kernel-level speedups. We release CUDA sources, Python bindings, a reproducible harness, and replayable cache logs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design</title>
<link>https://arxiv.org/abs/2511.17595</link>
<guid>https://arxiv.org/abs/2511.17595</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Artificial General Intelligence, 3D Same-Different Task, Curriculum Learning, Imitation Learning<br /><br />Summary:<br /><br />1. The paper explores the potential of Reinforcement Learning (RL) to demonstrate intelligent behavior in complex, less structured problem domains, moving beyond traditional constrained environments like Atari games and continuous control tasks.<br />2. It specifically investigates modern RL frameworks' ability to solve a 3D Same-Different visuospatial task, which appears straightforward but presents significant learning challenges.<br />3. Initial experiments using state-of-the-art RL methods including Proximal Policy Optimization (PPO), behavioural cloning, and imitation learning struggled to learn optimal strategies directly for this task.<br />4. The study highlights curriculum learning as a promising approach, where the lesson plan is carefully designed based on insights from real-world human experiments to progressively teach the agent.<br />5. By applying curriculum learning, the RL agents achieved effective learning outcomes, demonstrating that structured training sequences grounded in human data can significantly improve RL performance on complex visuospatial reasoning tasks.<br /><br />This work contributes to expanding the applicability of RL and suggests that incorporating human experimental findings and curriculum design can overcome current limitations in RL's ability to generalize to sophisticated cognitive tasks. <div>
arXiv:2511.17595v1 Announce Type: new 
Abstract: Reinforcement Learning is a mature technology, often suggested as a potential route towards Artificial General Intelligence, with the ambitious goal of replicating the wide range of abilities found in natural and artificial intelligence, including the complexities of human cognition. While RL had shown successes in relatively constrained environments, such as the classic Atari games and specific continuous control problems, recent years have seen efforts to expand its applicability. This work investigates the potential of RL in demonstrating intelligent behaviour and its progress in addressing more complex and less structured problem domains.
  We present an investigation into the capacity of modern RL frameworks in addressing a seemingly straightforward 3D Same-Different visuospatial task. While initial applications of state-of-the-art methods, including PPO, behavioural cloning and imitation learning, revealed challenges in directly learning optimal strategies, the successful implementation of curriculum learning offers a promising avenue. Effective learning was achieved by strategically designing the lesson plan based on the findings of a real-world human experiment.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.17598</link>
<guid>https://arxiv.org/abs/2511.17598</guid>
<content:encoded><![CDATA[
<div> Keywords: Non-stationary MDP, Varying discount, Reinforcement learning, Policy improvement, Dynamic programming  

<br /><br />Summary:  
This paper introduces the Non-stationary and Varying-discounting Markov Decision Process (NVMDP) framework, designed to overcome limitations of classic stationary infinite-horizon MDPs by accommodating non-stationarity and time-varying discount factors. NVMDPs generalize both infinite-horizon stationary MDPs and finite-horizon MDPs, enabling flexible shaping of optimal policies without modifying the state, action, or reward spaces. The authors develop a comprehensive theoretical foundation for NVMDPs, including the formulation of state- and action-value functions, recursion relations, matrix representations, and the derivation of optimality conditions and policy improvement guarantees under finite state-action settings. Building on these insights, dynamic programming and a generalized Q-learning algorithm are adapted to the NVMDP setting, with formal convergence proofs provided. For problems involving function approximation, the paper extends the Policy Gradient Theorem and refines the policy improvement bound utilized in Trust Region Policy Optimization (TRPO), offering proofs for scalar and matrix formulations. Empirical validation in a non-stationary gridworld environment demonstrates that algorithms based on NVMDPs recover optimal trajectories across diverse reward and discounting conditions, whereas standard Q-learning algorithms fail. Overall, NVMDPs present a theoretically rigorous and practical framework that requires only minor modifications of existing reinforcement learning methods, enabling robust handling of non-stationarity and explicit shaping of optimal policies. <div>
arXiv:2511.17598v1 Announce Type: new 
Abstract: Algorithms developed under stationary Markov Decision Processes (MDPs) often face challenges in non-stationary environments, and infinite-horizon formulations may not directly apply to finite-horizon tasks. To address these limitations, we introduce the Non-stationary and Varying-discounting MDP (NVMDP) framework, which naturally accommodates non-stationarity and allows discount rates to vary with time and transitions. Infinite-horizon, stationary MDPs emerge as special cases of NVMDPs for identifying an optimal policy, and finite-horizon MDPs are also subsumed within the NVMDP formulations. Moreover, NVMDPs provide a flexible mechanism to shape optimal policies, without altering the state space, action space, or the reward structure. We establish the theoretical foundations of NVMDPs, including assumptions, state- and action-value formulation and recursion, matrix representation, optimality conditions, and policy improvement under finite state and action spaces. Building on these results, we adapt dynamic programming and generalized Q-learning algorithms to NVMDPs, along with formal convergence proofs. For problems requiring function approximation, we extend the Policy Gradient Theorem and the policy improvement bound in Trust Region Policy Optimization (TRPO), offering proofs in both scalar and matrix forms. Empirical evaluations in a non-stationary gridworld environment demonstrate that NVMDP-based algorithms successfully recover optimal trajectories under multiple reward and discounting schemes, whereas original Q-learning fails. These results collectively show that NVMDPs provide a theoretically sound and practically effective framework for reinforcement learning, requiring only minor algorithmic modifications while enabling robust handling of non-stationarity and explicit optimal policy shaping.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Projection to Prediction: Beyond Logits for Scalable Language Models</title>
<link>https://arxiv.org/abs/2511.17599</link>
<guid>https://arxiv.org/abs/2511.17599</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, output projection, loss computation, memory efficiency, training optimization<br /><br />Summary:<br /><br />1. The standard training pipeline for Large Language Models (LLMs) involves a two-step process where hidden states are projected into vocabulary logits through a linear layer, followed by loss calculation using cross-entropy against target tokens. This approach causes high memory and bandwidth usage due to the need to fully materialize large logits tensors in GPU memory.<br /><br />2. The paper proposes a novel method that fuses the output projection and loss calculation into a single operation by computing loss directly from the hidden states and target tokens, bypassing the explicit creation of logits.<br /><br />3. This integrated approach significantly reduces GPU memory footprint and bandwidth consumption, addressing key bottlenecks that limit training scalability and throughput.<br /><br />4. Experimental results demonstrate that this technique delivers measurable speedups and enables training with larger batch sizes and longer input sequences without any degradation in model accuracy.<br /><br />5. The work underscores the value of reconsidering conventional boundaries between model components, offering a practical and efficient optimization to improve the training efficiency of large-scale language models. <div>
arXiv:2511.17599v1 Announce Type: new 
Abstract: Training Large Language Models (LLMs) typically involves a two-stage pipeline at the output layer: hidden states are projected into vocabulary logits via a linear transformation (lm_head), followed by cross-entropy loss computation against target tokens. While conceptually simple, this design incurs substantial overhead. The intermediate logits tensor, with dimensions proportional to batch size, sequence length, and vocabulary size, must be fully materialized in GPU memory, even though only one target token per position is ultimately used. This leads to significant memory footprint and bandwidth comsumption, limiting scalability and slowing training throughput.
  In this work, we introduce a novel approach to integrates the output projection and loss prediction into a single operation. By directly computing the loss from hidden states and target tokens, our approach bypasses explicit logits materialization. This design reduces memory usage and alleviates bandwidth pressure. Experiments on LLM training demonstrate that our method achieves substantial memory savings and measurable speedups compared to the standard two-stage pipeline, enabling large batch sizes and longer sequences without sacrificing accuracy. Our work highlights the benefits of rethinking the boundary between projection and prediction, offering a practical systems optimization for efficient LLM training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizable and Efficient Automated Scoring with a Knowledge-Distilled Multi-Task Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2511.17601</link>
<guid>https://arxiv.org/abs/2511.17601</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated scoring, Mixture-of-Experts, Knowledge distillation, Multi-task learning, Educational assessment<br /><br />Summary: The paper addresses the inefficiency of automated scoring systems that typically use separate models for each task, causing high demands on computational resources, storage, and maintenance in educational environments. It introduces UniMoE-Guided, a knowledge-distilled multi-task Mixture-of-Experts (MoE) model that consolidates expertise from multiple large, task-specific teacher models into a single, compact student model. This student model features a shared encoder to learn cross-task representations, a gated MoE block that balances generic and task-specific computations, and lightweight task-specific heads. Training leverages both ground-truth labels and teacher model guidance, enabling the student to achieve performance comparable to specialized per-task models while being significantly more efficient in training, storage, and deployment. The MoE layer enhances transfer learning and generalization by developing reusable expert skills that improve performance across tasks and allow fast adaptation to new tasks with minimal tuning. Evaluation on nine NGSS-aligned science-reasoning tasks shows UniMoE-Guided matches task-specific models' accuracy while requiring about six times less storage than maintaining multiple separate models and 87 times less than a single 20 billion-parameter teacher. The proposed method presents a practical, scalable, and resource-efficient solution for automated scoring in classroom and large-scale assessments. <div>
arXiv:2511.17601v1 Announce Type: new 
Abstract: Automated scoring of written constructed responses typically relies on separate models per task, straining computational resources, storage, and maintenance in real-world education settings. We propose UniMoE-Guided, a knowledge-distilled multi-task Mixture-of-Experts (MoE) approach that transfers expertise from multiple task-specific large models (teachers) into a single compact, deployable model (student). The student combines (i) a shared encoder for cross-task representations, (ii) a gated MoE block that balances shared and task-specific processing, and (iii) lightweight task heads. Trained with both ground-truth labels and teacher guidance, the student matches strong task-specific models while being far more efficient to train, store, and deploy. Beyond efficiency, the MoE layer improves transfer and generalization: experts develop reusable skills that boost cross-task performance and enable rapid adaptation to new tasks with minimal additions and tuning. On nine NGSS-aligned science-reasoning tasks (seven for training/evaluation and two held out for adaptation), UniMoE-Guided attains performance comparable to per-task models while using $\sim$6$\times$ less storage than maintaining separate students, and $87\times$ less than the 20B-parameter teacher. The method offers a practical path toward scalable, reliable, and resource-efficient automated scoring for classroom and large-scale assessment systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Surface-Level Similarity: Hierarchical Contamination Detection for Synthetic Training Data in Foundation Models</title>
<link>https://arxiv.org/abs/2511.17602</link>
<guid>https://arxiv.org/abs/2511.17602</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, contamination detection, semantic overlap, foundation models, hierarchical framework<br /><br />Summary: The paper addresses the critical issue of benchmark contamination in training foundation models with synthetic data. Existing detection methods primarily focus on token-level overlap but fail to detect semantic-level contamination, where synthetic data conceptually resembles benchmarks without sharing exact lexical content. This semantic-level contamination poses a growing risk as foundation models increasingly incorporate synthetic data that may implicitly encode benchmark knowledge, threatening evaluation integrity. To overcome this, the authors propose a hierarchical contamination detection framework that operates across four levels: token level, semantic level, reasoning pattern, and performance cliff detection. The framework is evaluated through controlled experiments on well-known benchmarks including MMLU, GSM8K, and HumanEval. Results show that existing methods struggle with semantic-level contamination, achieving a low F1 score between 0.17 and 0.49. In contrast, the proposed hierarchical approach significantly improves detection performance, reaching an F1 score of 0.76 and providing an average improvement of 26.5% over state-of-the-art baselines. The framework not only enhances detection accuracy but also equips practitioners with practical tools for auditing synthetic datasets, thereby supporting the responsible deployment and evaluation of foundation models trained on synthetic data. <div>
arXiv:2511.17602v1 Announce Type: new 
Abstract: Synthetic data has become essential for training foundation models, yet benchmark contamination threatens evaluation integrity. Although existing detection methods identify token-level overlap, they fail to detect semantic-level contamination where synthetic data conceptually resemble benchmarks without lexical overlap. This gap is critical as foundation models increasingly train on synthetic data that may implicitly encode benchmark knowledge. We propose a hierarchical contamination detection framework operating at four levels: token level, semantic level, reasoning pattern, and performance cliff detection. Through controlled experiments on MMLU, GSM8K and HumanEval, we demonstrate that semantic-level contamination evades existing methods (F1=0.17-0.49) but is effectively detected by our hierarchical approach (F1 = 0.76), with an average improvement of 26. 5\% over state-of-the-art baselines. Our framework provides practitioners with practical tools for audit pipelines and enables responsible deployment of synthetic training data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrainHGT: A Hierarchical Graph Transformer for Interpretable Brain Network Analysis</title>
<link>https://arxiv.org/abs/2511.17604</link>
<guid>https://arxiv.org/abs/2511.17604</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Transformer, brain network, hierarchical interaction, long-short range attention, modular architecture  

<br /><br />Summary:  
This paper introduces BrainHGT, a hierarchical Graph Transformer designed to model the brain's modular and hierarchical structure in network analysis. Unlike traditional approaches that treat the brain as a flat network and apply uniform attention to all connections, BrainHGT simulates the brain’s natural information processing hierarchy, from local regions to global communities. The authors propose a novel long-short range attention encoder with parallel pathways to manage dense local interactions and sparse long-range connections, effectively mitigating the over-globalizing problem present in existing models. Additionally, BrainHGT incorporates a prior-guided clustering module using cross-attention mechanisms that group brain regions into functional communities guided by neuroanatomical priors, enhancing biological plausibility and interpretability. Experiments demonstrate that BrainHGT significantly outperforms previous methods in disease identification tasks. It also reliably captures brain sub-functional modules, providing insights into the brain’s modular architecture. This approach advances brain network analysis by better reflecting the hierarchical and modular nature of brain information processing while improving both predictive performance and interpretability. <div>
arXiv:2511.17604v1 Announce Type: new 
Abstract: Graph Transformer shows remarkable potential in brain network analysis due to its ability to model graph structures and complex node relationships. Most existing methods typically model the brain as a flat network, ignoring its modular structure, and their attention mechanisms treat all brain region connections equally, ignoring distance-related node connection patterns. However, brain information processing is a hierarchical process that involves local and long-range interactions between brain regions, interactions between regions and sub-functional modules, and interactions among functional modules themselves. This hierarchical interaction mechanism enables the brain to efficiently integrate local computations and global information flow, supporting the execution of complex cognitive functions. To address this issue, we propose BrainHGT, a hierarchical Graph Transformer that simulates the brain's natural information processing from local regions to global communities. Specifically, we design a novel long-short range attention encoder that utilizes parallel pathways to handle dense local interactions and sparse long-range connections, thereby effectively alleviating the over-globalizing issue. To further capture the brain's modular architecture, we designe a prior-guided clustering module that utilizes a cross-attention mechanism to group brain regions into functional communities and leverage neuroanatomical prior to guide the clustering process, thereby improving the biological plausibility and interpretability. Experimental results indicate that our proposed method significantly improves performance of disease identification, and can reliably capture the sub-functional modules of the brain, demonstrating its interpretability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Copula Based Fusion of Clinical and Genomic Machine Learning Risk Scores for Breast Cancer Risk Stratification</title>
<link>https://arxiv.org/abs/2511.17605</link>
<guid>https://arxiv.org/abs/2511.17605</guid>
<content:encoded><![CDATA[
<div> Keywords: breast cancer, risk stratification, clinical models, genomic models, copula fusion<br /><br />Summary:<br />1. The study aims to improve prediction of 5-year breast cancer-specific mortality by better combining clinical and genomic risk models beyond simple linear methods.<br />2. Using the METABRIC breast cancer cohort, researchers defined two predictor sets: clinical variables (demographics, tumor, treatment) and genomic variables (gene-expression z-scores).<br />3. Several machine learning classifiers, including Random Forest and XGBoost, were trained to generate cross-validated risk scores for both clinical and genomic data.<br />4. These risk scores were transformed into pseudo-observations on the unit square to fit three types of copulas—Gaussian, Clayton, and Gumbel—to model their joint distribution.<br />5. The Gaussian copula best captured the symmetric, moderately strong positive dependency between clinical and genomic risk scores (bootstrap p=0.997).<br />6. Kaplan-Meier survival analysis showed patients at high risk in both clinical and genomic scores had significantly worse outcomes than those high-risk in only one domain.<br />7. The findings demonstrate that copula-based fusion effectively models dependencies between risk sources and can improve identification of patient subgroups with the poorest prognosis in real-world cohorts. <div>
arXiv:2511.17605v1 Announce Type: new 
Abstract: Clinical and genomic models are both used to predict breast cancer outcomes, but they are often combined using simple linear rules that do not account for how their risk scores relate, especially at the extremes. Using the METABRIC breast cancer cohort, we studied whether directly modeling the joint relationship between clinical and genomic machine learning risk scores could improve risk stratification for 5-year cancer-specific mortality. We created a binary 5-year cancer-death outcome and defined two sets of predictors: a clinical set (demographic, tumor, and treatment variables) and a genomic set (gene-expression $z$-scores). We trained several supervised classifiers, such as Random Forest and XGBoost, and used 5-fold cross-validated predicted probabilities as unbiased risk scores. These scores were converted to pseudo-observations on $(0,1)^2$ to fit Gaussian, Clayton, and Gumbel copulas. Clinical models showed good discrimination (AUC 0.783), while genomic models had moderate performance (AUC 0.681). The joint distribution was best captured by a Gaussian copula (bootstrap $p=0.997$), which suggests a symmetric, moderately strong positive relationship. When we grouped patients based on this relationship, Kaplan-Meier curves showed clear differences: patients who were high-risk in both clinical and genomic scores had much poorer survival than those high-risk in only one set. These results show that copula-based fusion works in real-world cohorts and that considering dependencies between scores can better identify patient subgroups with the worst prognosis.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-based Autoregressive Generation for Neural Population Dynamics</title>
<link>https://arxiv.org/abs/2511.17606</link>
<guid>https://arxiv.org/abs/2511.17606</guid>
<content:encoded><![CDATA[
<div> Keywords: energy-based modeling, autoregressive generation, neural population dynamics, transformer, brain-computer interface<br /><br />Summary:<br />1. The paper addresses the challenge of balancing computational efficiency and high-fidelity modeling in computational neuroscience by introducing a novel Energy-based Autoregressive Generation (EAG) framework. <br />2. EAG employs an energy-based transformer model that learns temporal dynamics in latent space using strictly proper scoring rules, enabling efficient generation of neural population activity with realistic spiking statistics at both population and single-neuron levels. <br />3. Evaluations on synthetic Lorenz datasets and two Neural Latents Benchmark datasets (MC_Maze and Area2_bump) demonstrate that EAG surpasses existing methods, especially diffusion-based models, in generation quality and computational efficiency. <br />4. The framework also supports conditional generation, enabling generalization to unseen behavioral contexts and enhancing the decoding accuracy of motor brain-computer interfaces by leveraging synthetic neural data. <br />5. These results highlight the potential of energy-based modeling for advancing neuroscience research and neural engineering applications, providing an effective and efficient tool to study neural population dynamics. The authors have made their code publicly available for further research use. <div>
arXiv:2511.17606v1 Announce Type: new 
Abstract: Understanding brain function represents a fundamental goal in neuroscience, with critical implications for therapeutic interventions and neural engineering applications. Computational modeling provides a quantitative framework for accelerating this understanding, but faces a fundamental trade-off between computational efficiency and high-fidelity modeling. To address this limitation, we introduce a novel Energy-based Autoregressive Generation (EAG) framework that employs an energy-based transformer learning temporal dynamics in latent space through strictly proper scoring rules, enabling efficient generation with realistic population and single-neuron spiking statistics. Evaluation on synthetic Lorenz datasets and two Neural Latents Benchmark datasets (MC_Maze and Area2_bump) demonstrates that EAG achieves state-of-the-art generation quality with substantial computational efficiency improvements, particularly over diffusion-based methods. Beyond optimal performance, conditional generation applications show two capabilities: generalizing to unseen behavioral contexts and improving motor brain-computer interface decoding accuracy using synthetic neural data. These results demonstrate the effectiveness of energy-based modeling for neural population dynamics with applications in neuroscience research and neural engineering. Code is available at https://github.com/NinglingGe/Energy-based-Autoregressive-Generation-for-Neural-Population-Dynamics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finding Pre-Injury Patterns in Triathletes from Lifestyle, Recovery and Load Dynamics Features</title>
<link>https://arxiv.org/abs/2511.17610</link>
<guid>https://arxiv.org/abs/2511.17610</guid>
<content:encoded><![CDATA[
<div> triathlon, injury prediction, synthetic data, sleep quality, machine learning  

<br /><br />Summary:  
This article addresses the high risk of overuse injuries faced by triathletes due to the intense combination of swimming, cycling, and running training. Traditional injury prediction models often focus solely on training load metrics, overlooking important factors like sleep quality, stress, and lifestyle, which significantly impact recovery and injury risk. To overcome these limitations, the authors introduce a novel synthetic data generation framework specifically designed for triathlon athletes. This framework creates physiologically plausible athlete profiles and simulates personalized training plans incorporating periodization and load management. Importantly, it integrates daily-life factors such as sleep quality, stress levels, and recovery states into the data. Machine learning models including LASSO, Random Forest, and XGBoost were evaluated using this synthetic data, achieving high predictive accuracy with area under the curve (AUC) values up to 0.86. Key early indicators of injury risk identified by the models include sleep disturbances, heart rate variability, and stress. The study demonstrates that this wearable-driven, context-aware approach improves injury prediction while addressing real-world data scarcity challenges. This holistic monitoring framework offers a practical pathway for better injury prevention strategies in triathlon training. <div>
arXiv:2511.17610v1 Announce Type: new 
Abstract: Triathlon training, which involves high-volume swimming, cycling, and running, places athletes at substantial risk for overuse injuries due to repetitive physiological stress. Current injury prediction approaches primarily rely on training load metrics, often neglecting critical factors such as sleep quality, stress, and individual lifestyle patterns that significantly influence recovery and injury susceptibility.
  We introduce a novel synthetic data generation framework tailored explicitly for triathlon. This framework generates physiologically plausible athlete profiles, simulates individualized training programs that incorporate periodization and load-management principles, and integrates daily-life factors such as sleep quality, stress levels, and recovery states. We evaluated machine learning models (LASSO, Random Forest, and XGBoost) showing high predictive performance (AUC up to 0.86), identifying sleep disturbances, heart rate variability, and stress as critical early indicators of injury risk. This wearable-driven approach not only enhances injury prediction accuracy but also provides a practical solution to overcoming real-world data limitations, offering a pathway toward a holistic, context-aware athlete monitoring.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-driven Generation of MALDI-TOF MS for Microbial Characterization</title>
<link>https://arxiv.org/abs/2511.17611</link>
<guid>https://arxiv.org/abs/2511.17611</guid>
<content:encoded><![CDATA[
<div> Keywords: MALDI-TOF MS, deep generative models, synthetic spectra, microbial identification, class imbalance<br /><br />Summary:<br /><br />Matrix-Assisted Laser Desorption/Ionization Time-of-Flight Mass Spectrometry (MALDI-TOF MS) is crucial in clinical microbiology for rapid microbial identification, but the development of diagnostic models is hindered by the scarcity of large, balanced, and standardized spectral datasets. This study explores the use of three deep generative models—Variational Autoencoders (MALDIVAE), Generative Adversarial Networks (MALDIGAN), and Denoising Diffusion Probabilistic Models (MALDIffusion)—to synthesize realistic MALDI-TOF spectra conditioned on species labels. The fidelity and diversity of generated spectra were rigorously evaluated using multiple metrics. Results indicate that synthetic data from all three models are statistically and diagnostically comparable to real spectra, allowing classifiers trained exclusively on synthetic samples to achieve performance similar to those trained on real data. Among the models, MALDIffusion achieves the highest fidelity but incurs substantially higher computational cost, MALDIGAN performs competitively but with less stability, while MALDIVAE strikes the best balance between realism, stability, and efficiency. Importantly, augmenting minority microbial species with synthetic spectra significantly improves classification accuracy by addressing class imbalance and domain mismatch issues without sacrificing data authenticity, thereby facilitating the development of more robust machine learning-based diagnostic tools in microbiology. <div>
arXiv:2511.17611v1 Announce Type: new 
Abstract: Matrix-Assisted Laser Desorption/Ionization Time-of-Flight Mass Spectrometry (MALDI-TOF MS) has become a cornerstone technology in clinical microbiology, enabling rapid and accurate microbial identification. However, the development of data-driven diagnostic models remains limited by the lack of sufficiently large, balanced, and standardized spectral datasets. This study investigates the use of deep generative models to synthesize realistic MALDI-TOF MS spectra, aiming to overcome data scarcity and support the development of robust machine learning tools in microbiology.
  We adapt and evaluate three generative models, Variational Autoencoders (MALDIVAEs), Generative Adversarial Networks (MALDIGANs), and Denoising Diffusion Probabilistic Model (MALDIffusion), for the conditional generation of microbial spectra guided by species labels. Generation is conditioned on species labels, and spectral fidelity and diversity are assessed using diverse metrics.
  Our experiments show that synthetic data generated by MALDIVAE, MALDIGAN, and MALDIffusion are statistically and diagnostically comparable to real measurements, enabling classifiers trained exclusively on synthetic samples to reach performance levels similar to those trained on real data. While all models faithfully reproduce the peak structure and variability of MALDI-TOF spectra, MALDIffusion obtains this fidelity at a substantially higher computational cost, and MALDIGAN shows competitive but slightly less stable behaviour. In contrast, MALDIVAE offers the most favorable balance between realism, stability, and efficiency. Furthermore, augmenting minority species with synthetic spectra markedly improves classification accuracy, effectively mitigating class imbalance and domain mismatch without compromising the authenticity of the generated data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tensor Gauge Flow Models</title>
<link>https://arxiv.org/abs/2511.17616</link>
<guid>https://arxiv.org/abs/2511.17616</guid>
<content:encoded><![CDATA[
<div> Tensor Gauge Flow Models, Generative Flow Models, Higher Gauge Fields, Gauge-theoretic Structure, Gaussian Mixture Models<br /><br />Summary:<br /><br />This paper introduces Tensor Gauge Flow Models, an innovative expansion of the Generative Flow Models framework. Unlike previous Gauge Flow Models and Higher Gauge Flow Models, this new class integrates higher-order Tensor Gauge Fields directly into the Flow Equation, enabling the capture of more complex geometric and gauge-theoretic structures within data. By enhancing the underlying mathematical formulation, these models provide more expressive and flexible flow dynamics, which can adapt better to intricate data distributions. The authors conduct experiments specifically on Gaussian mixture models to evaluate the performance of Tensor Gauge Flow Models. These experiments demonstrate that the proposed approach yields superior generative performance compared to both standard generative flow models and existing gauge flow baselines. The improvements suggest that encoding higher-order tensorial structures into the flow dynamics allows capturing data features more effectively. Overall, this work contributes a significant advancement in the design of flow-based generative models by leveraging sophisticated gauge theory concepts, offering potential for improved generative modeling in applications where geometric and structural data properties are essential. <div>
arXiv:2511.17616v1 Announce Type: new 
Abstract: This paper introduces Tensor Gauge Flow Models, a new class of Generative Flow Models that generalize Gauge Flow Models and Higher Gauge Flow Models by incorporating higher-order Tensor Gauge Fields into the Flow Equation. This extension allows the model to encode richer geometric and gauge-theoretic structure in the data, leading to more expressive flow dynamics. Experiments on Gaussian mixture models show that Tensor Gauge Flow Models achieve improved generative performance compared to both standard and gauge flow baselines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks for Explainable Depression Identification</title>
<link>https://arxiv.org/abs/2511.17622</link>
<guid>https://arxiv.org/abs/2511.17622</guid>
<content:encoded><![CDATA[
<div> Major Depressive Disorder, graph neural networks, neurobiological interpretability, hierarchical modeling, causal attention  

<br /><br />Summary:  
This study introduces NH-GCAT (Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks), a novel deep learning framework designed to improve diagnosis of Major Depressive Disorder (MDD) by integrating neurobiological knowledge with neuroimaging data. Firstly, at the local brain region level, NH-GCAT uses a residual gated fusion module that combines temporal BOLD signal dynamics with functional connectivity to capture depression-related low-frequency neural oscillations. Secondly, at the multi-regional circuit level, it employs a hierarchical circuit encoding scheme that aggregates regional brain node representations in accordance with known depression neurocircuitry organization. Thirdly, at the whole-brain network level, a variational latent causal attention mechanism is introduced, which infers directed information flow among key brain circuits using a probabilistic latent space, thereby characterizing altered inter-circuit interactions due to disease. Validated on the REST-meta-MDD dataset using leave-one-site-out cross-validation, NH-GCAT achieves state-of-the-art depression classification performance with a weighted average accuracy of 73.3% and an AUROC of 76.4%. Importantly, the model also provides interpretable, neurobiologically meaningful explanations of how brain circuits contribute to the disorder, advancing both classification accuracy and understanding of depression pathophysiology through explainable AI. <div>
arXiv:2511.17622v1 Announce Type: new 
Abstract: Major Depressive Disorder (MDD), affecting millions worldwide, exhibits complex pathophysiology manifested through disrupted brain network dynamics. Although graph neural networks that leverage neuroimaging data have shown promise in depression diagnosis, existing approaches are predominantly data-driven and operate largely as black-box models, lacking neurobiological interpretability. Here, we present NH-GCAT (Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks), a novel framework that bridges neuroscience domain knowledge with deep learning by explicitly and hierarchically modeling depression-specific mechanisms at different spatial scales. Our approach introduces three key technical contributions: (1) at the local brain regional level, we design a residual gated fusion module that integrates temporal blood oxygenation level dependent (BOLD) dynamics with functional connectivity patterns, specifically engineered to capture local depression-relevant low-frequency neural oscillations; (2) at the multi-regional circuit level, we propose a hierarchical circuit encoding scheme that aggregates regional node representations following established depression neurocircuitry organization, and (3) at the multi-circuit network level, we develop a variational latent causal attention mechanism that leverages a continuous probabilistic latent space to infer directed information flow among critical circuits, characterizing disease-altered whole-brain inter-circuit interactions. Rigorous leave-one-site-out cross-validation on the REST-meta-MDD dataset demonstrates NH-GCAT's state-of-the-art performance in depression classification, achieving a sample-size weighted-average accuracy of 73.3\% and an AUROC of 76.4\%, while simultaneously providing neurobiologically meaningful explanations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M$^2$OE$^2$-GL: A Family of Probabilistic Load Forecasters That Scales to Massive Customers</title>
<link>https://arxiv.org/abs/2511.17623</link>
<guid>https://arxiv.org/abs/2511.17623</guid>
<content:encoded><![CDATA[
<div> Probabilistic load forecasting, deep learning, scalability, global-to-local model, fine-tuning<br /><br />Summary:<br /><br />1. Probabilistic load forecasting is essential for power system planning, operation, and risk-aware decision making, with deep learning models excelling at capturing complex temporal and contextual patterns to improve accuracy.<br /><br />2. Deploying individual models for thousands or hundreds of thousands of loads in large distribution feeders is computationally and storage-wise impractical, while a single global model fails to address the distributional differences among diverse customer types, geographic locations, and power phases.<br /><br />3. Existing research mainly targets single-load forecasts, global models for multiple loads, or adaptive personalized models in smaller scale contexts, without adequately addressing challenges related to heterogeneity and scalability in large feeder environments.<br /><br />4. The proposed M2OE2-GL approach introduces a global-to-local methodology by pretraining one global M2OE2 model on all feeder loads and then applying lightweight fine-tuning to create a compact set of group-specific forecasters.<br /><br />5. Experimental results on realistic utility data demonstrate that M2OE2-GL achieves significant error reductions while maintaining scalability to very large numbers of loads, effectively balancing accuracy and efficiency in large-scale probabilistic load forecasting applications. <div>
arXiv:2511.17623v1 Announce Type: new 
Abstract: Probabilistic load forecasting is widely studied and underpins power system planning, operation, and risk-aware decision making. Deep learning forecasters have shown strong ability to capture complex temporal and contextual patterns, achieving substantial accuracy gains. However, at the scale of thousands or even hundreds of thousands of loads in large distribution feeders, a deployment dilemma emerges: training and maintaining one model per customer is computationally and storage intensive, while using a single global model ignores distributional shifts across customer types, locations, and phases. Prior work typically focuses on single-load forecasters, global models across multiple loads, or adaptive/personalized models for relatively small settings, and rarely addresses the combined challenges of heterogeneity and scalability in large feeders. We propose M2OE2-GL, a global-to-local extension of the M2OE2 probabilistic forecaster. We first pretrain a single global M2OE2 base model across all feeder loads, then apply lightweight fine-tuning to derive a compact family of group-specific forecasters. Evaluated on realistic utility data, M2OE2-GL yields substantial error reductions while remaining scalable to very large numbers of loads.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QML-HCS: A Hypercausal Quantum Machine Learning Framework for Non-Stationary Environments</title>
<link>https://arxiv.org/abs/2511.17624</link>
<guid>https://arxiv.org/abs/2511.17624</guid>
<content:encoded><![CDATA[
<div> Keywords: hypercausal feedback, quantum-inspired machine learning, adaptive behavior, causal stability, dynamic causal feedback  

<br /><br />Summary:  
This article introduces QML-HCS, a research framework designed for constructing and analyzing quantum-inspired machine learning models that operate under hypercausal feedback dynamics. Hypercausal systems extend traditional causal models by leveraging complex nonlinear and deep causal relationships to enhance reasoning, prediction, and state inference. Existing machine learning methods often falter in non-stationary environments due to distribution shifts and lack mechanisms for continuous adaptation and causal consistency. QML-HCS addresses these challenges with a unified computational architecture that blends quantum-inspired superposition principles, dynamic causal feedback loops, and deterministic-stochastic hybrid computation to allow models to adapt effectively to changing data. The framework features a hypercausal processing core that supports reversible transformations, multipath causal propagation, and evaluation of alternative model states under input drift. Continuous feedback mechanisms ensure causal consistency and model adjustment without full retraining. QML-HCS offers a Python interface supported by efficient computational routines for experimentation without requiring specialized quantum hardware. An initial simulation showcases the model’s ability to maintain coherence and adapt to sudden input distribution shifts. This release lays the groundwork for future theoretical developments, benchmarking, and integration with other classical and quantum simulation platforms. <div>
arXiv:2511.17624v1 Announce Type: new 
Abstract: QML-HCS is a research-grade framework for constructing and analyzing quantum-inspired machine learning models operating under hypercausal feedback dynamics. Hypercausal refers to AI systems that leverage extended, deep, or nonlinear causal relationships (expanded causality) to reason, predict, and infer states beyond the capabilities of traditional causal models. Current machine learning and quantum-inspired systems struggle in non-stationary environments, where data distributions drift and models lack mechanisms for continuous adaptation, causal stability, and coherent state updating. QML-HCS addresses this limitation through a unified computational architecture that integrates quantum-inspired superposition principles, dynamic causal feedback, and deterministic-stochastic hybrid execution to enable adaptive behavior in changing environments.
  The framework implements a hypercausal processing core capable of reversible transformations, multipath causal propagation, and evaluation of alternative states under drift. Its architecture incorporates continuous feedback to preserve causal consistency and adjust model behavior without requiring full retraining. QML-HCS provides a reproducible and extensible Python interface backed by efficient computational routines, enabling experimentation in quantum-inspired learning, causal reasoning, and hybrid computation without the need for specialized hardware.
  A minimal simulation demonstrates how a hypercausal model adapts to a sudden shift in the input distribution while preserving internal coherence. This initial release establishes the foundational architecture for future theoretical extensions, benchmarking studies, and integration with classical and quantum simulation platforms.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Large-Scale Learning of Minimax Risk Classifiers</title>
<link>https://arxiv.org/abs/2511.17626</link>
<guid>https://arxiv.org/abs/2511.17626</guid>
<content:encoded><![CDATA[
<div> Keywords: supervised learning, minimax risk classifiers, stochastic subgradient methods, large-scale data, multi-class classification  

<br /><br />Summary:  
1. This paper addresses the challenge of efficiently training classification models on large-scale datasets, especially when dealing with multiple classes.  
2. Traditional stochastic subgradient methods work well for minimizing average loss but are unsuitable for minimax risk classifiers (MRCs), which focus on minimizing the maximum expected loss.  
3. The authors propose a novel learning algorithm that combines constraint generation and column generation techniques to enable efficient optimization of MRCs at scale.  
4. The algorithm is designed to handle large sample sizes and multi-class classification problems, which typically complicate optimization processes.  
5. Experimental results on several benchmark datasets demonstrate significant computational speedups, achieving up to 10 times faster training for general large datasets and around 100 times faster for datasets with many classes.  
6. This work contributes an effective tool for practitioners aiming to use robust MRC approaches in practical large-scale classification scenarios where computational resources and training time are critical. <div>
arXiv:2511.17626v1 Announce Type: new 
Abstract: Supervised learning with large-scale data usually leads to complex optimization problems, especially for classification tasks with multiple classes. Stochastic subgradient methods can enable efficient learning with a large number of samples for classification techniques that minimize the average loss over the training samples. However, recent techniques, such as minimax risk classifiers (MRCs), minimize the maximum expected loss and are not amenable to stochastic subgradient methods. In this paper, we present a learning algorithm based on the combination of constraint and column generation that enables efficient learning of MRCs with large-scale data for classification tasks with multiple classes. Experiments on multiple benchmark datasets show that the proposed algorithm provides upto a 10x speedup for general large-scale data and around a 100x speedup with a sizeable number of classes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectifying Mean-Shift in Cascaded Precipitation Nowcasting</title>
<link>https://arxiv.org/abs/2511.17628</link>
<guid>https://arxiv.org/abs/2511.17628</guid>
<content:encoded><![CDATA[
<div> Keywords: Precipitation nowcasting, Distribution shift, Deterministic model, Probabilistic model, RectiCast<br /><br />Summary: Precipitation nowcasting aims to provide high-resolution precipitation forecasts by utilizing radar observations and is essential for regional weather prediction. The current dominant approach uses a cascaded architecture combining a deterministic model for macroscopic trend prediction and a probabilistic model for generating local stochastic details. However, these methods often fail to address the systematic distribution shift in deterministic predictions, which contaminates probabilistic forecasts, resulting in inaccurate precipitation intensity and patterns, especially for longer forecast lead times. To overcome this limitation, the authors propose RectiCast, a novel two-stage framework that separates correction of mean-field distribution shift from modeling local stochasticity using a dual Flow Matching model. In the first stage, a deterministic model produces the posterior mean forecast. The second stage employs a Rectifier module that explicitly learns and corrects distribution shifts, generating a rectified mean. Conditioned on this rectified mean, a Generator then models the local stochastic variability. Experiments on real-world datasets SEVIR and MeteoNet demonstrate that RectiCast significantly outperforms existing state-of-the-art precipitation nowcasting methods, improving the accuracy of precipitation pattern and intensity predictions over traditional cascaded models. <div>
arXiv:2511.17628v1 Announce Type: new 
Abstract: Precipitation nowcasting, which aims to provide high spatio-temporal resolution precipitation forecasts by leveraging current radar observations, is a core task in regional weather forecasting. The cascaded architecture has emerged as the mainstream paradigm for deep learning-based precipitation nowcasting. This paradigm involves a deterministic model to predict macroscopic trends (or posterior mean), followed by a probabilistic model to generate local details (or local stochasticity). However, existing methods commonly overlook the conflation of the systematic distribution shift in deterministic predictions and the local stochasticity. As a result, the deterministic component's distribution shift contaminates the predictions of the probabilistic component, leading to inaccuracies in precipitation patterns and intensity, particularly over longer lead times. To address this issue, we introduce RectiCast, a two-stage framework that explicitly decouples the correction of mean-field shift from the generation of local stochasticity via a dual Flow Matching model. In the first stage, a deterministic model generates the posterior mean. In the second stage, we introduce a Rectifier to explicitly learn the distribution shift and produce a rectified mean. Subsequently, a Generator focuses on modeling the local stochasticity conditioned on the rectified mean. Experiments on SEVIR and MeteoNet demonstrate that RectiCast achieves significant performance improvements over existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boundary-Aware Adversarial Filtering for Reliable Diagnosis under Extreme Class Imbalance</title>
<link>https://arxiv.org/abs/2511.17629</link>
<guid>https://arxiv.org/abs/2511.17629</guid>
<content:encoded><![CDATA[
<div> Keywords: extreme class imbalance, AF-SMOTE, recall, calibration, medical diagnosis<br /><br />Summary:  
This paper addresses classification challenges under extreme class imbalance, focusing on maximizing recall and calibration, which are crucial in medical diagnosis and similar high-stakes domains. The authors introduce AF-SMOTE, an augmentation framework designed to improve minority class representation by first generating synthetic minority samples and then refining these samples using two components: an adversarial discriminator and a boundary utility model. The proposed filtering step is theoretically supported; under reasonable assumptions about decision boundary smoothness and class-conditional densities, it is proven to monotonically improve a surrogate measure of the F_beta score (for beta ≥ 1) without increasing the Brier score, ensuring better predictive calibration. Empirical evaluations on the MIMIC-IV dataset for proxy label prediction and standard fraud detection benchmarks show that AF-SMOTE outperforms popular oversampling methods like SMOTE, ADASYN, Borderline-SMOTE, and SVM-SMOTE, particularly delivering higher recall, average precision, and superior calibration. Additional experiments across multiple datasets further confirm the robustness and effectiveness of AF-SMOTE. The authors highlight its practical utility in clinical settings by applying it to healthcare data in a disease-agnostic way, emphasizing its importance where missing rare positive cases can have severe real-world consequences. <div>
arXiv:2511.17629v1 Announce Type: new 
Abstract: We study classification under extreme class imbalance where recall and calibration are both critical, for example in medical diagnosis scenarios. We propose AF-SMOTE, a mathematically motivated augmentation framework that first synthesizes minority points and then filters them by an adversarial discriminator and a boundary utility model. We prove that, under mild assumptions on the decision boundary smoothness and class-conditional densities, our filtering step monotonically improves a surrogate of F_beta (for beta >= 1) while not inflating Brier score. On MIMIC-IV proxy label prediction and canonical fraud detection benchmarks, AF-SMOTE attains higher recall and average precision than strong oversampling baselines (SMOTE, ADASYN, Borderline-SMOTE, SVM-SMOTE), and yields the best calibration. We further validate these gains across multiple additional datasets beyond MIMIC-IV. Our successful application of AF-SMOTE to a healthcare dataset using a proxy label demonstrates in a disease-agnostic way its practical value in clinical situations, where missing true positive cases in rare diseases can have severe consequences.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change</title>
<link>https://arxiv.org/abs/2511.17630</link>
<guid>https://arxiv.org/abs/2511.17630</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, behavior change, reinforcement learning, user interactions, prompting strategies  

<br /><br />Summary:  
The article investigates the potential of using large language models (LLMs) to generate synthetic user interaction data for training reinforcement learning (RL) models aimed at digital health behavior change applications. It emphasizes the challenge in designing personalized digital interventions due to the costly and uncertain evaluation of many design choices when real user data is limited or unavailable. By comparing LLM-generated samples with real user data from four major behavior change studies, the research demonstrates that LLM samples can effectively substitute real data in RL training contexts. Furthermore, the study benchmarks LLM-generated data against samples produced by human raters, finding the LLM output to perform on par with human efforts. The authors explore various prompting methods such as shorter vs. longer prompts, chain-of-thought prompting, and few-shot prompting, revealing that the success of these techniques depends on both the specific behavior change study context and the LLM used. Notably, even minor variations like prompt paraphrasing significantly affect outcome quality. The paper concludes with practical recommendations on leveraging LLM-generated samples to advance the development of personalized digital health behavior interventions. <div>
arXiv:2511.17630v1 Announce Type: new 
Abstract: Personalizing digital applications for health behavior change is a promising route to making them more engaging and effective. This especially holds for approaches that adapt to users and their specific states (e.g., motivation, knowledge, wants) over time. However, developing such approaches requires making many design choices, whose effectiveness is difficult to predict from literature and costly to evaluate in practice. In this work, we explore whether large language models (LLMs) can be used out-of-the-box to generate samples of user interactions that provide useful information for training reinforcement learning models for digital behavior change settings. Using real user data from four large behavior change studies as comparison, we show that LLM-generated samples can be useful in the absence of real data. Comparisons to the samples provided by human raters further show that LLM-generated samples reach the performance of human raters. Additional analyses of different prompting strategies including shorter and longer prompt variants, chain-of-thought prompting, and few-shot prompting show that the relative effectiveness of different strategies depends on both the study and the LLM with also relatively large differences between prompt paraphrases alone. We provide recommendations for how LLM-generated samples can be useful in practice.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Federated Deep Multi-View Clustering under Uncertainty Scenario</title>
<link>https://arxiv.org/abs/2511.17631</link>
<guid>https://arxiv.org/abs/2511.17631</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Multi-View Clustering, Semantic Conflict, View Uncertainty, Aggregation Uncertainty<br /><br />Summary:<br /><br />This paper addresses the challenges in traditional Federated Multi-View Clustering (FMC), which typically assumes uniform view data across clients. In real-world scenarios, clients often possess heterogeneous and incomplete views that contain redundancy or corruption, leading to difficulties in clustering. The authors identify two critical uncertainties: view uncertainty, stemming from semantic inconsistencies when combining arbitrary views, and aggregation uncertainty, caused by divergent client updates with imbalanced contributions. To overcome these, the paper introduces the Enhanced Federated Deep Multi-View Clustering (EFDMVC) framework. EFDMVC first aligns local semantics and uses hierarchical contrastive fusion within clients to resolve view uncertainty by removing semantic conflicts. Next, a view adaptive drift module is designed to tackle aggregation uncertainty by employing global-local prototype contrast to dynamically correct parameter deviations. Additionally, a balanced aggregation mechanism is proposed to coordinate client updates fairly. Extensive experiments on multiple benchmark datasets demonstrate that EFDMVC consistently outperforms state-of-the-art methods, showing superior robustness to heterogeneous and uncertain views within federated learning environments. This work provides a comprehensive solution to improve clustering performance under complex, real-world data heterogeneity. <div>
arXiv:2511.17631v1 Announce Type: new 
Abstract: Traditional Federated Multi-View Clustering assumes uniform views across clients, yet practical deployments reveal heterogeneous view completeness with prevalent incomplete, redundant, or corrupted data. While recent approaches model view heterogeneity, they neglect semantic conflicts from dynamic view combinations, failing to address dual uncertainties: view uncertainty (semantic inconsistency from arbitrary view pairings) and aggregation uncertainty (divergent client updates with imbalanced contributions). To address these, we propose a novel Enhanced Federated Deep Multi-View Clustering framework: first align local semantics, hierarchical contrastive fusion within clients resolves view uncertainty by eliminating semantic conflicts; a view adaptive drift module mitigates aggregation uncertainty through global-local prototype contrast that dynamically corrects parameter deviations; and a balanced aggregation mechanism coordinates client updates. Experimental results demonstrate that EFDMVC achieves superior robustness against heterogeneous uncertain views across multiple benchmark datasets, consistently outperforming all state-of-the-art baselines in comprehensive evaluations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production</title>
<link>https://arxiv.org/abs/2511.17632</link>
<guid>https://arxiv.org/abs/2511.17632</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital Twin, Smart Manufacturing, Deep Reinforcement Learning, MLOps, Sustainability  

<br /><br />Summary:  
This paper presents a Digital Twin-based approach designed to enhance sustainability, efficiency, and cost-effectiveness in steel production plants. The system utilizes a micro-service edge-compute platform that ingests real-time sensor data through a converged network infrastructure to maintain an accurate digital twin of the manufacturing process. Agile machine learning-driven control loops are implemented within the digital twin to optimize induction furnace heating, thereby improving operational quality and minimizing process waste. A key component is the deployment of a Deep Reinforcement Learning agent which autonomously correlates the plant's system state with the digital twin to determine optimal correction actions focused on power settings. The paper details the theoretical foundation, architectural design, and practical considerations for deploying such a system aimed at reducing manufacturing waste and raising production quality. The architecture adopts a scalable, event-driven design, making it adaptable to a wide range of industrial applications beyond steel manufacturing. Overall, this research signifies a critical advance towards transforming traditional manufacturing processes into intelligent, data-driven systems that support sustainability objectives. It also highlights the significant role of MLOps frameworks in facilitating the integration and continuous operation of machine learning models within industrial environments. <div>
arXiv:2511.17632v1 Announce Type: new 
Abstract: We explore a Digital Twin-Based Approach for Smart Manufacturing to improve Sustainability, Efficiency, and Cost-Effectiveness for a steel production plant. Our system is based on a micro-service edge-compute platform that ingests real-time sensor data from the process into a digital twin over a converged network infrastructure. We implement agile machine learning-based control loops in the digital twin to optimize induction furnace heating, enhance operational quality, and reduce process waste. Key to our approach is a Deep Reinforcement learning-based agent used in our machine learning operation (MLOps) driven system to autonomously correlate the system state with its digital twin to identify correction actions that aim to optimize power settings for the plant. We present the theoretical basis, architectural details, and practical implications of our approach to reduce manufacturing waste and increase production quality. We design the system for flexibility so that our scalable event-driven architecture can be adapted to various industrial applications. With this research, we propose a pivotal step towards the transformation of traditional processes into intelligent systems, aligning with sustainability goals and emphasizing the role of MLOps in shaping the future of data-driven manufacturing.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PocketLLM: Ultimate Compression of Large Language Models via Meta Networks</title>
<link>https://arxiv.org/abs/2511.17637</link>
<guid>https://arxiv.org/abs/2511.17637</guid>
<content:encoded><![CDATA[
<div> latent compression, meta-networks, large language models, codebook, decoder networks<br /><br />Summary:<br /><br />1. The paper addresses the challenge of storing and transmitting increasingly large Language Models (LLMs) on edge devices, where traditional compression approaches like quantization and pruning fall short without compromising accuracy.<br /><br />2. It introduces PocketLLM, a novel compression technique that operates in a latent space using meta-networks to achieve extreme compression ratios.<br /><br />3. PocketLLM features a simple encoder network that projects LLM weights into discrete latent vectors, which are efficiently represented via a compact codebook.<br /><br />4. To reconstruct the original weights, a lightweight decoder network maps the codebook’s representative vectors back into the weight space, resulting in a model consisting only of a small decoder, a concise codebook, and an index.<br /><br />5. Experimental results demonstrate that PocketLLM significantly outperforms existing methods, achieving up to 10x compression on Llama 2-7B with negligible accuracy loss, making it highly effective for deployment in resource-constrained environments. <div>
arXiv:2511.17637v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) continue to grow in size, storing and transmitting them on edge devices becomes increasingly challenging. Traditional methods like quantization and pruning struggle to achieve extreme compression of LLMs without sacrificing accuracy. In this paper, we introduce PocketLLM, a novel approach to compress LLMs in a latent space via meta-networks. A simple encoder network is proposed to project the weights of LLMs into discrete latent vectors, which are then represented using a compact codebook. A lightweight decoder network is employed to map the codebook's representative vectors back to the original weight space. This method allows for significant compression of the large weights in LLMs, consisting solely of a small decoder, a concise codebook, and an index. Extensive experiments show that PocketLLM achieves superior performance even at significantly high compression ratios, e.g., compressing Llama 2-7B by 10x with a negligible drop in accuracy.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-to-Model Knowledge Transmission (M2KT): A Data-Free Framework for Cross-Model Understanding Transfer</title>
<link>https://arxiv.org/abs/2511.17638</link>
<guid>https://arxiv.org/abs/2511.17638</guid>
<content:encoded><![CDATA[
<div> Keywords: Model-to-Model Knowledge Transmission, data-free transfer, concept manifolds, neural networks, large language models<br /><br />Summary:<br /><br />This paper introduces Model-to-Model Knowledge Transmission (M2KT), a novel paradigm for transmitting knowledge between neural networks without relying on traditional data-driven methods such as examples, logits, or gradients. Unlike conventional knowledge distillation, M2KT operates in concept space, enabling models to exchange structured knowledge packets that include concept embeddings, abstraction graphs, reasoning traces, and provenance metadata. The authors formalize the concept of concept manifolds and propose an inter-model alignment mapping that bridges teacher and student latent spaces, facilitating effective knowledge transfer. They derive a composite loss function that simultaneously enforces geometric, structural, and reasoning consistency while integrating explicit safety constraints to ensure reliable transfer. The work also presents algorithmic procedures for teacher-side packet generation and student-side ingestion and verification of the knowledge packets. Experiments with large language models on symbolic reasoning tasks demonstrate that M2KT achieves 85 to 90 percent of the teacher model's performance while reducing data usage by more than 98% compared to standard knowledge distillation methods. Overall, this research provides both theoretical foundations and practical algorithms for data-free AI-to-AI knowledge transfer, opening new avenues for self-improving model ecosystems. <div>
arXiv:2511.17638v1 Announce Type: new 
Abstract: Modern artificial intelligence systems depend heavily on large datasets for both training and transferring knowledge between models. Knowledge distillation, transfer learning, and dataset distillation have made such transfers more efficient, yet they remain fundamentally data-driven: a teacher must produce examples, logits, or gradients for a student to learn. In this work, we introduce Model-to-Model Knowledge Transmission (M2KT), a novel paradigm for data-free conceptual transfer between neural networks. M2KT enables models to exchange knowledge packets that encapsulate structured concept embeddings, abstraction graphs, reasoning traces, and provenance metadata. Unlike classical distillation, M2KT operates primarily in concept space rather than example space, and it does not require labeled datasets or teacher-generated outputs during transfer. We formalize the notion of concept manifolds, introduce an inter-model alignment mapping between teacher and student latent spaces, and derive a composite loss that enforces geometric, structural, and reasoning consistency together with explicit safety constraints. We further present algorithmic procedures for teacher-side packet generation and student-side ingestion and verification. Experiments on symbolic reasoning with large language models show that M2KT can achieve approximately 85 to 90 percent of teacher performance while reducing data usage by over 98 percent compared to standard knowledge distillation. This work establishes a theoretical and practical foundation for data-free AI-to-AI knowledge transfer and self-improving model ecosystems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TTF: A Trapezoidal Temporal Fusion Framework for LTV Forecasting in Douyin</title>
<link>https://arxiv.org/abs/2511.17639</link>
<guid>https://arxiv.org/abs/2511.17639</guid>
<content:encoded><![CDATA[
<div> Keywords: LTV forecasting, multi-time series, SILO challenge, Trapezoidal Temporal Fusion, Douyin deployment<br /><br />Summary: This paper addresses the problem of predicting channel-level lifetime value (LTV) to optimize budget allocation for user acquisition in internet companies. The LTV forecasting task is distinct from traditional time series forecasting due to three main challenges: data unalignment caused by multiple LTV series with varying activation dates across channels, an imbalanced short-input long-output (SILO) prediction problem that requires early-stage forecasting, and the highly volatile, non-stationary nature of real LTV series with frequent fluctuations and high variance. To overcome these challenges, the authors propose a novel framework called Trapezoidal Temporal Fusion (TTF), which utilizes a trapezoidal multi-time series module designed to manage unaligned data and the SILO problem effectively. The prediction module, named MT-FusionNet, adopts a multi-tower network structure to output precise forecasts of LTV curves at both point-wise and aggregate levels. This framework has been implemented in the Douyin online system, where it showed significant improvements compared to the previously deployed model, reducing point-wise mean absolute percentage error (MAPEp) by 4.3% and aggregate MAPE (MAPEa) by 3.2%. The study contributes a practical approach for early-stage LTV prediction to support sustainable user growth strategies. <div>
arXiv:2511.17639v1 Announce Type: new 
Abstract: In the user growth scenario, Internet companies invest heavily in paid acquisition channels to acquire new users. But sustainable growth depends on acquired users' generating lifetime value (LTV) exceeding customer acquisition cost (CAC). In order to maximize LTV/CAC ratio, it is crucial to predict channel-level LTV in an early stage for further optimization of budget allocation. The LTV forecasting problem is significantly different from traditional time series forecasting problems, and there are three main challenges. Firstly, it is an unaligned multi-time series forecasting problem that each channel has a number of LTV series of different activation dates. Secondly, to predict in the early stage, it faces the imbalanced short-input long-output (SILO) challenge. Moreover, compared with the commonly used time series datasets, the real LTV series are volatile and non-stationary, with more frequent fluctuations and higher variance. In this work, we propose a novel framework called Trapezoidal Temporal Fusion (TTF) to address the above challenges. We introduce a trapezoidal multi-time series module to deal with data unalignment and SILO challenges, and output accurate predictions with a multi-tower structure called MT-FusionNet. The framework has been deployed to the online system for Douyin. Compared to the previously deployed online model, MAPEp decreased by 4.3%, and MAPEa decreased by 3.2%, where MAPEp denotes the point-wise MAPE of the LTV curve and MAPEa denotes the MAPE of the aggregated LTV.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlockCert: Certified Blockwise Extraction of Transformer Mechanisms</title>
<link>https://arxiv.org/abs/2511.17645</link>
<guid>https://arxiv.org/abs/2511.17645</guid>
<content:encoded><![CDATA[
<div> Keywords: mechanistic interpretability, model editing, certified extraction, transformers, formal guarantees<br /><br />Summary:<br /><br />1. The paper addresses the challenges in mechanistic interpretability and model editing, noting the lack of formal guarantees and the reliance on informal, ad-hoc evaluations in these fields.  
2. It introduces BlockCert, a novel framework designed for certified blockwise extraction of transformer mechanisms, which also supports certified local edits through a lightweight extension.  
3. BlockCert operates by extracting structured surrogate implementations for residual blocks in pre-trained transformers, accompanied by machine-checkable certificates that bound approximation errors, measure coverage, and hash underlying artifacts to ensure integrity.  
4. The framework is underpinned by a Lipschitz-based composition theorem formalized in Lean 4, which allows local error guarantees to be composed into a global deviation bound, providing strong formal assurances.  
5. Empirical evaluation on GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B demonstrates high per-block coverage, small residual approximation errors, and near-perfect perplexity matching on stress prompts, highlighting BlockCert’s practical feasibility and potential to unify mechanistic interpretability with formal model reasoning. <div>
arXiv:2511.17645v1 Announce Type: new 
Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MamTiff-CAD: Multi-Scale Latent Diffusion with Mamba+ for Complex Parametric Sequence</title>
<link>https://arxiv.org/abs/2511.17647</link>
<guid>https://arxiv.org/abs/2511.17647</guid>
<content:encoded><![CDATA[
<div> Keywords: Parametric CAD, Transformer, Diffusion model, Long sequence generation, Multi-scale latent representation<br /><br />Summary:<br />1. The paper addresses challenges in generating long sequence parametric commands in CAD, especially due to complex geometric and topological constraints of industrial CAD models. <br />2. MamTiff-CAD, a novel framework, is proposed that employs a Transformer-based diffusion model for handling multi-scale latent representations of these long command sequences. <br />3. A new autoencoder architecture integrates Mamba+ blocks with Transformers; Mamba+ includes a forget gate mechanism designed to capture long-range dependencies effectively within CAD parameter sequences. <br />4. The framework uses a non-autoregressive Transformer decoder to reconstruct the latent representations, improving generation efficiency and performance. <br />5. A multi-scale Transformer-based diffusion model is trained on latent embeddings to learn the distribution of very long parametric command sequences, up to 256 commands per CAD model. <br />6. The authors also provide a new dataset containing long parametric sequences, which helps benchmark long sequence CAD command generation. <br />7. Experiments demonstrate that MamTiff-CAD outperforms existing methods on both reconstruction and generation tasks, confirming its capability in managing long sequence CAD model generation effectively for sequences ranging from 60 to 256 commands. <div>
arXiv:2511.17647v1 Announce Type: new 
Abstract: Parametric Computer-Aided Design (CAD) is crucial in industrial applications, yet existing approaches often struggle to generate long sequence parametric commands due to complex CAD models' geometric and topological constraints. To address this challenge, we propose MamTiff-CAD, a novel CAD parametric command sequences generation framework that leverages a Transformer-based diffusion model for multi-scale latent representations. Specifically, we design a novel autoencoder that integrates Mamba+ and Transformer, to transfer parameterized CAD sequences into latent representations. The Mamba+ block incorporates a forget gate mechanism to effectively capture long-range dependencies. The non-autoregressive Transformer decoder reconstructs the latent representations. A diffusion model based on multi-scale Transformer is then trained on these latent embeddings to learn the distribution of long sequence commands. In addition, we also construct a dataset that consists of long parametric sequences, which is up to 256 commands for a single CAD model. Experiments demonstrate that MamTiff-CAD achieves state-of-the-art performance on both reconstruction and generation tasks, confirming its effectiveness for long sequence (60-256) CAD model generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frugality in second-order optimization: floating-point approximations for Newton's method</title>
<link>https://arxiv.org/abs/2511.17660</link>
<guid>https://arxiv.org/abs/2511.17660</guid>
<content:encoded><![CDATA[
<div> Newton's method, finite-precision arithmetic, mixed-precision optimizers, generalized Gauss-Newton, regression benchmarks<br /><br />Summary:<br /><br />This article addresses the challenge of applying higher-order optimization methods like Newton's method in machine learning training, particularly in the context of computational cost and finite-precision arithmetic. First, it analyzes how finite-precision affects Newton steps and establishes a convergence theorem for mixed-precision Newton optimizers, including quasi and inexact variants. This theorem not only guarantees convergence but also provides a priori estimates of the solution accuracy achievable under these conditions. Empirical evaluations demonstrate that the proposed mixed-precision Newton methods outperform the widely used Adam optimizer on standard regression benchmarks, specifically the Australian and MUSH datasets. In the second part, the manuscript introduces GN_k, a generalized Gauss-Newton method that allows partial computation of second-order derivatives, thereby reducing the number of derivative evaluations required. GN_k achieves performance comparable to the full Newton's method on regression tasks, making it a computationally efficient alternative. Overall, the work offers a theoretically grounded and empirically validated approach to leverage second-order techniques more effectively in machine learning optimization despite hardware and precision limitations. <div>
arXiv:2511.17660v1 Announce Type: new 
Abstract: Minimizing loss functions is central to machine-learning training. Although first-order methods dominate practical applications, higher-order techniques such as Newton's method can deliver greater accuracy and faster convergence, yet are often avoided due to their computational cost. This work analyzes the impact of finite-precision arithmetic on Newton steps and establishes a convergence theorem for mixed-precision Newton optimizers, including "quasi" and "inexact" variants. The theorem provides not only convergence guarantees but also a priori estimates of the achievable solution accuracy. Empirical evaluations on standard regression benchmarks demonstrate that the proposed methods outperform Adam on the Australian and MUSH datasets. The second part of the manuscript introduces GN_k, a generalized Gauss-Newton method that enables partial computation of second-order derivatives. GN_k attains performance comparable to full Newton's method on regression tasks while requiring significantly fewer derivative evaluations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Breast Cancer Prediction with LLM-Inferred Confounders</title>
<link>https://arxiv.org/abs/2511.17662</link>
<guid>https://arxiv.org/abs/2511.17662</guid>
<content:encoded><![CDATA[
<div> Keywords: breast cancer prediction, large language models, confounding diseases, Random Forest, clinical data<br /><br />Summary: This study explores the enhancement of breast cancer prediction by leveraging large language models (LLMs) to infer the likelihood of related confounding diseases, specifically diabetes, obesity, and cardiovascular disease, from routine clinical data. By generating AI-derived features representing these comorbidities, the researchers improved the performance of a Random Forest classification model tasked with breast cancer prediction. Among the LLMs tested, Gemma and Llama demonstrated notable effectiveness, increasing model accuracy by 3.9% and 6.4%, respectively. The approach provides a noninvasive prescreening method that can be easily integrated into clinical workflows, offering additional contextual information that supports early detection efforts. Furthermore, the use of LLM-generated features can aid clinicians in shared decision-making processes, helping to better tailor diagnostic and treatment plans for individual patients. Overall, the work suggests that incorporating AI-inferred comorbidity data into predictive models strengthens their predictive capabilities and holds promise for improving breast cancer diagnosis outcomes while minimizing patient burden. <div>
arXiv:2511.17662v1 Announce Type: new 
Abstract: This study enhances breast cancer prediction by using large language models to infer the likelihood of confounding diseases, namely diabetes, obesity, and cardiovascular disease, from routine clinical data. These AI-generated features improved Random Forest model performance, particularly for LLMs like Gemma (3.9%) and Llama (6.4%). The approach shows promise for noninvasive prescreening and clinical integration, supporting improved early detection and shared decision-making in breast cancer diagnosis.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-based framework to predict animal and pen feed intake in feedlot beef cattle</title>
<link>https://arxiv.org/abs/2511.17663</link>
<guid>https://arxiv.org/abs/2511.17663</guid>
<content:encoded><![CDATA[
<div> Keywords: sustainable cattle farming, feed intake prediction, environmental indices, machine learning, precision livestock systems<br /><br />Summary:<br /><br />The article presents advancements in sustainable cattle farming through the use of electronic feeding systems that generate extensive longitudinal datasets on individual animal feed intake. Despite the availability of big data, existing methods fail to effectively integrate environmental factors for accurate feed intake prediction. To address this, the authors developed an AI-based framework that predicts both individual animal and pen-level feed intake with high accuracy. Data were collected from over 16.5 million samples across 19 experiments conducted between 2013 and 2024 at the Nancy M. Cummings Research Extension & Education Center in Carmen, ID, combined with environmental data from the AgriMet Network weather stations. Two novel environmental indices were introduced: the InComfort-Index, based solely on meteorological data, which predicted thermal comfort well but was less effective for feed intake prediction; and the EASI-Index, a hybrid index integrating environmental variables and feed intake behavior, which successfully predicted feed intake but was weaker in assessing thermal comfort. Machine learning models, especially XGBoost, yielded an RMSE of 1.38 kg/day at the animal level and 0.14 kg/(day-animal) at the pen level. This framework enables precision feedlot management by reducing feed waste, optimizing resources, and adapting to climatic variations. <div>
arXiv:2511.17663v1 Announce Type: new 
Abstract: Advances in technology are transforming sustainable cattle farming practices, with electronic feeding systems generating big longitudinal datasets on individual animal feed intake, offering the possibility for autonomous precision livestock systems. However, the literature still lacks a methodology that fully leverages these longitudinal big data to accurately predict feed intake accounting for environmental conditions. To fill this gap, we developed an AI-based framework to accurately predict feed intake of individual animals and pen-level aggregation. Data from 19 experiments (>16.5M samples; 2013-2024) conducted at Nancy M. Cummings Research Extension & Education Center (Carmen, ID) feedlot facility and environmental data from AgriMet Network weather stations were used to develop two novel environmental indices: InComfort-Index, based solely on meteorological variables, showed good predictive capability for thermal comfort but had limited ability to predict feed intake; EASI-Index, a hybrid index integrating environmental variables with feed intake behavior, performed well in predicting feed intake but was less effective for thermal comfort. Together with the environmental indices, machine learning models were trained and the best-performing machine learning model (XGBoost) accuracy was RMSE of 1.38 kg/day for animal-level and only 0.14 kg/(day-animal) at pen-level. This approach provides a robust AI-based framework for predicting feed intake in individual animals and pens, with potential applications in precision management of feedlot cattle, through feed waste reduction, resource optimization, and climate-adaptive livestock management.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CubeletWorld: A New Abstraction for Scalable 3D Modeling</title>
<link>https://arxiv.org/abs/2511.17664</link>
<guid>https://arxiv.org/abs/2511.17664</guid>
<content:encoded><![CDATA[
<div> Keywords: CubeletWorld, urban environments, 3D grid, privacy-preserving modeling, state prediction  

<br /><br />Summary:  
1. The paper introduces CubeletWorld, a novel framework for representing urban environments using a discretized 3D grid composed of spatial units called cubelets.  
2. This framework integrates heterogeneous urban data, including infrastructure maps, mobility logs, and satellite imagery, into localized cubelet states, enabling privacy-preserving modeling without relying on agent-driven sensing.  
3. CubeletWorld supports various downstream applications such as urban planning, navigation, and occupancy prediction, addressing challenges related to scalability and privacy that are present in current agent-centric methods.  
4. To validate their approach, the authors propose the CubeletWorld State Prediction task, which involves predicting the state of cubelets using a realistic dataset containing urban features like streets and buildings.  
5. The study evaluates multiple modified core models, revealing challenges associated with increased spatial granularity, namely sparsity and scalability issues. Compared to existing 3D occupancy models, the cubelet-focused approach offers improved generalizability across regions and better privacy compliance.  
6. Results highlight CubeletWorld’s flexibility and extensibility, opening new avenues for scalable urban simulation and decision support in socio-demographic modeling, environmental monitoring, and emergency response.  
7. The authors also provide code and datasets publicly to encourage further research and application development. <div>
arXiv:2511.17664v1 Announce Type: new 
Abstract: Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GANGR: GAN-Assisted Scalable and Efficient Global Routing Parallelization</title>
<link>https://arxiv.org/abs/2511.17665</link>
<guid>https://arxiv.org/abs/2511.17665</guid>
<content:encoded><![CDATA[
<div> Global routing, batching, WGAN, parallelization, ISPD'24  

<br /><br />Summary:  
This paper addresses the global routing stage in electronic design automation, focusing on improving early estimations and optimizations concerning congestion, power, and complexity in integrated circuits. Batching, the process of grouping nets for parallel processing, is highlighted as a key strategy for effective memory usage and scalable hardware parallelization, while controlling net interactions for reduced congestion. Conventional batching techniques rely on heuristics that are often computationally expensive and yield suboptimal outcomes, such as oversized batches, excessive batch counts, and longer generation times, which hinder performance and scalability. To overcome these challenges, the authors introduce a novel batching algorithm enhanced by Wasserstein generative adversarial networks (WGANs). This new method generates fewer, higher-quality batches more efficiently, promoting better parallelization. The algorithm is evaluated on the latest ISPD'24 contest benchmarks, revealing a significant runtime reduction of up to 40% compared to leading global routers, with only a minimal 0.002% degradation in routing quality. These results demonstrate the potential of leveraging WGANs for more efficient and scalable global routing batch formation in modern integrated circuit design. <div>
arXiv:2511.17665v1 Announce Type: new 
Abstract: Global routing is a critical stage in electronic design automation (EDA) that enables early estimation and optimization of the routability of modern integrated circuits with respect to congestion, power dissipation, and design complexity. Batching is a primary concern in top-performing global routers, grouping nets into manageable sets to enable parallel processing and efficient resource usage. This process improves memory usage, scalable parallelization on modern hardware, and routing congestion by controlling net interactions within each batch. However, conventional batching methods typically depend on heuristics that are computationally expensive and can lead to suboptimal results (oversized batches with conflicting nets, excessive batch counts degrading parallelization, and longer batch generation times), ultimately limiting scalability and efficiency. To address these limitations, a novel batching algorithm enhanced with Wasserstein generative adversarial networks (WGANs) is introduced in this paper, enabling more effective parallelization by generating fewer higher-quality batches in less time. The proposed algorithm is tested on the latest ISPD'24 contest benchmarks, demonstrating up to 40% runtime reduction with only 0.002% degradation in routing quality as compared to state-of-the-art router.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2511.17675</link>
<guid>https://arxiv.org/abs/2511.17675</guid>
<content:encoded><![CDATA[
<div> Keywords: quantum computing, trajectory forecasting, autonomous driving, multi-modal prediction, Waymo Open Motion Dataset<br /><br />Summary:<br /><br />This paper presents a compact hybrid quantum architecture designed for trajectory forecasting in autonomous driving, emphasizing accuracy and efficiency under strict computational and latency constraints. The architecture leverages a quantum inductive bias that aligns with the road-scene structure by operating within an ego-centric, lane-aligned coordinate frame. Instead of predicting absolute vehicle poses, the model focuses on learning residual corrections relative to a kinematic baseline, improving forecast accuracy. The model is composed of three main components: a transformer-inspired quantum attention encoder utilizing 9 qubits, a parameter-efficient quantum feedforward stack with 64 layers and about 1200 trainable angles, and a Fourier-based decoder that uses shallow entanglement and phase superposition to generate 16 trajectory hypotheses simultaneously. Uniquely, the model estimates mode confidences based on the latent spectrum. Training is performed using Simultaneous Perturbation Stochastic Approximation (SPSA), circumventing the need for backpropagation through non-analytic quantum components. Evaluated on the Waymo Open Motion Dataset, the model achieved a minimum Average Displacement Error (minADE) of 1.94 m and a minimum Final Displacement Error (minFDE) of 3.56 m over a 2-second prediction horizon, consistently outperforming a kinematic baseline. Ablation studies demonstrate that key design choices like residual learning in the lane frame, truncated Fourier decoding, shallow entanglement, and spectrum-based ranking contribute to stable optimization and reliable multi-modal trajectory predictions from small, shallow quantum circuits. <div>
arXiv:2511.17675v1 Announce Type: new 
Abstract: Trajectory forecasting for autonomous driving must deliver accurate, calibrated multi-modal futures under tight compute and latency constraints. We propose a compact hybrid quantum architecture that aligns quantum inductive bias with road-scene structure by operating in an ego-centric, lane-aligned frame and predicting residual corrections to a kinematic baseline instead of absolute poses. The model combines a transformer-inspired quantum attention encoder (9 qubits), a parameter-lean quantum feedforward stack (64 layers, ${\sim}1200$ trainable angles), and a Fourier-based decoder that uses shallow entanglement and phase superposition to generate 16 trajectory hypotheses in a single pass, with mode confidences derived from the latent spectrum. All circuit parameters are trained with Simultaneous Perturbation Stochastic Approximation (SPSA), avoiding backpropagation through non-analytic components. In the Waymo Open Motion Dataset, the model achieves minADE (minimum Average Displacement Error) of \SI{1.94}{m} and minFDE (minimum Final Displacement Error) of \SI{3.56}{m} in the $16$ models predicted over the horizon of \SI{2.0}{s}, consistently outperforming a kinematic baseline with reduced miss rates and strong recall. Ablations confirm that residual learning in the lane frame, truncated Fourier decoding, shallow entanglement, and spectrum-based ranking focus capacity where it matters, yielding stable optimization and reliable multi-modal forecasts from small, shallow quantum circuits on a modern autonomous-driving benchmark.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Classical-Quantum Fine Tuned BERT for Text Classification</title>
<link>https://arxiv.org/abs/2511.17677</link>
<guid>https://arxiv.org/abs/2511.17677</guid>
<content:encoded><![CDATA[
<div> Quantum computing, BERT fine-tuning, text classification, hybrid classical-quantum model, machine learning performance<br /><br />Summary:<br /><br />This paper addresses the computational challenges and hyper-parameter tuning difficulties involved in fine-tuning BERT for text classification. It introduces a novel hybrid approach that combines an n-qubit quantum circuit with a classical BERT model to enhance text classification tasks. The authors evaluate this classical-quantum hybrid model on standard benchmark datasets, demonstrating that its performance is competitive with, and sometimes surpasses, traditional classical BERT baselines. The study emphasizes not only the feasibility of integrating quantum circuits with pre-trained language models but also highlights the adaptability of such hybrid models across diverse datasets. The experimental results support the potential of quantum-assisted fine-tuning to advance the efficiency and effectiveness of text classification. Overall, the research presents a promising direction for leveraging quantum computing technologies to improve machine learning outcomes in natural language processing applications. <div>
arXiv:2511.17677v1 Announce Type: new 
Abstract: Fine-tuning BERT for text classification can be computationally challenging and requires careful hyper-parameter tuning. Recent studies have highlighted the potential of quantum algorithms to outperform conventional methods in machine learning and text classification tasks. In this work, we propose a hybrid approach that integrates an n-qubit quantum circuit with a classical BERT model for text classification. We evaluate the performance of the fine-tuned classical-quantum BERT and demonstrate its feasibility as well as its potential in advancing this research area. Our experimental results show that the proposed hybrid model achieves performance that is competitive with, and in some cases better than, the classical baselines on standard benchmark datasets. Furthermore, our approach demonstrates the adaptability of classical-quantum models for fine-tuning pre-trained models across diverse datasets. Overall, the hybrid model highlights the promise of quantum computing in achieving improved performance for text classification tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Brain-inspired Path Integration Efficiency via Learning-based Replication of Continuous Attractor Neurodynamics</title>
<link>https://arxiv.org/abs/2511.17687</link>
<guid>https://arxiv.org/abs/2511.17687</guid>
<content:encoded><![CDATA[
<div> Path Integration, Continuous Attractor Neural Networks, Head Direction Cells, Grid Cells, Brain-Inspired Navigation  

<br /><br />Summary:  
1. The study addresses inefficiencies in Path Integration (PI) mechanisms based on Continuous Attractor Neural Networks (CANNs) commonly used in Brain-Inspired Navigation (BIN) systems.  
2. It proposes a novel approach that leverages representation learning models to replicate the neurodynamic patterns of CANN-modeled Head Direction Cells (HDCs) and Grid Cells (GCs) with lightweight Artificial Neural Networks (ANNs).  
3. These ANN-based HDC and GC models are integrated to perform brain-inspired PI suitable for Dead Reckoning (DR) navigation tasks.  
4. Benchmarking against the established NeuroSLAM system in diverse environments demonstrates that the ANN approach maintains comparable positioning accuracy while accurately replicating the neurodynamic behavior of navigation cells.  
5. The proposed method achieves significant operational efficiency gains, improving performance by approximately 17.5% on general-purpose devices and 40-50% on edge devices, enhancing the practicality and real-world applicability of BIN technology.  
6. This work offers a new implementation strategy that reduces computational redundancy inherent in traditional CANN-based PI models and provides a scalable, efficient alternative for advanced navigation systems inspired by biological mechanisms. <div>
arXiv:2511.17687v1 Announce Type: new 
Abstract: The brain's Path Integration (PI) mechanism offers substantial guidance and inspiration for Brain-Inspired Navigation (BIN). However, the PI capability constructed by the Continuous Attractor Neural Networks (CANNs) in most existing BIN studies exhibits significant computational redundancy, and its operational efficiency needs to be improved; otherwise, it will not be conducive to the practicality of BIN technology. To address this, this paper proposes an efficient PI approach using representation learning models to replicate CANN neurodynamic patterns. This method successfully replicates the neurodynamic patterns of CANN-modeled Head Direction Cells (HDCs) and Grid Cells (GCs) using lightweight Artificial Neural Networks (ANNs). These ANN-reconstructed HDC and GC models are then integrated to achieve brain-inspired PI for Dead Reckoning (DR). Benchmark tests in various environments, compared with the well-known NeuroSLAM system, demonstrate that this work not only accurately replicates the neurodynamic patterns of navigation cells but also matches NeuroSLAM in positioning accuracy. Moreover, efficiency improvements of approximately 17.5% on the general-purpose device and 40~50% on the edge device were observed, compared with NeuroSLAM. This work offers a novel implementation strategy to enhance the practicality of BIN technology and holds potential for further extension.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Adversarial Transferability through Block Stretch and Shrink</title>
<link>https://arxiv.org/abs/2511.17688</link>
<guid>https://arxiv.org/abs/2511.17688</guid>
<content:encoded><![CDATA[
arXiv:2511.17688v1 Announce Type: new 
Abstract: Adversarial attacks introduce small, deliberately crafted perturbations that mislead neural networks, and their transferability from white-box to black-box target models remains a critical research focus. Input transformation-based attacks are a subfield of adversarial attacks that enhance input diversity through input transformations to improve the transferability of adversarial examples. However, existing input transformation-based attacks tend to exhibit limited cross-model transferability. Previous studies have shown that high transferability is associated with diverse attention heatmaps and the preservation of global semantics in transformed inputs. Motivated by this observation, we propose Block Stretch and Shrink (BSS), a method that divides an image into blocks and applies stretch and shrink operations to these blocks, thereby diversifying attention heatmaps in transformed inputs while maintaining their global semantics. Empirical evaluations on a subset of ImageNet demonstrate that BSS outperforms existing input transformation-based attack methods in terms of transferability. Furthermore, we examine the impact of the number scale, defined as the number of transformed inputs, in input transformation-based attacks, and advocate evaluating these methods under a unified number scale to enable fair and comparable assessments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams</title>
<link>https://arxiv.org/abs/2511.17693</link>
<guid>https://arxiv.org/abs/2511.17693</guid>
<content:encoded><![CDATA[
arXiv:2511.17693v1 Announce Type: new 
Abstract: Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models are Molecular Dynamics Simulators</title>
<link>https://arxiv.org/abs/2511.17741</link>
<guid>https://arxiv.org/abs/2511.17741</guid>
<content:encoded><![CDATA[
arXiv:2511.17741v1 Announce Type: new 
Abstract: We prove that a denoising diffusion sampler equipped with a sequential bias across the batch dimension is exactly an Euler-Maruyama integrator for overdamped Langevin dynamics. Each reverse denoising step, with its associated spring stiffness, can be interpreted as one step of a stochastic differential equation with an effective time step set jointly by the noise schedule and that stiffness. The learned score then plays the role of the drift, equivalently the gradient of a learned energy, yielding a precise correspondence between diffusion sampling and Langevin time evolution.
  This equivalence recasts molecular dynamics (MD) in terms of diffusion models. Accuracy is no longer tied to a fixed, extremely small MD time step; instead, it is controlled by two scalable knobs: model capacity, which governs how well the drift is approximated, and the number of denoising steps, which sets the integrator resolution. In practice, this leads to a fully data-driven MD framework that learns forces from uncorrelated equilibrium snapshots, requires no hand-engineered force fields, uses no trajectory data for training, and still preserves the Boltzmann distribution associated with the learned energy.
  We derive trajectory-level, information-theoretic error bounds that cleanly separate discretization error from score-model error, clarify how temperature enters through the effective spring, and show that the resulting sampler generates molecular trajectories with MD-like temporal correlations, even though the model is trained only on static configurations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Periodicity-Enforced Neural Network for Designing Deterministic Lateral Displacement Devices</title>
<link>https://arxiv.org/abs/2511.17754</link>
<guid>https://arxiv.org/abs/2511.17754</guid>
<content:encoded><![CDATA[
arXiv:2511.17754v1 Announce Type: new 
Abstract: Deterministic Lateral Displacement (DLD) devices enable liquid biopsy for cancer detection by separating circulating tumor cells (CTCs) from blood samples based on size, but designing these microfluidic devices requires computationally expensive Navier-Stokes simulations and particle-tracing analyses. While recent surrogate modeling approaches using deep learning have accelerated this process, they often inadequately handle the critical periodic boundary conditions of DLD unit cells, leading to cumulative errors in multi-unit device predictions. This paper introduces a periodicity-enforced surrogate modeling approach that incorporates periodic layers, neural network components that guarantee exact periodicity without penalty terms or output modifications, into deep learning architectures for DLD device design. The proposed method employs three sub-networks to predict steady-state, non-dimensional velocity and pressure fields (u, v, p) rather than directly predicting critical diameters or particle trajectories, enabling complete flow field characterization and enhanced design flexibility. Periodic layers ensure exact matching of flow variables across unit cell boundaries through architectural enforcement rather than soft penalty-based approaches. Validation on 120 CFD-generated geometries demonstrates that the periodic layer implementation achieves 0.478% critical diameter error while maintaining perfect periodicity consistency, representing an 85.4% improvement over baseline methods. The approach enables efficient and accurate DLD device design with guaranteed boundary condition satisfaction for multi-unit device applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrismSSL: One Interface, Many Modalities; A Single-Interface Library for Multimodal Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2511.17776</link>
<guid>https://arxiv.org/abs/2511.17776</guid>
<content:encoded><![CDATA[
arXiv:2511.17776v1 Announce Type: new 
Abstract: We present PrismSSL, a Python library that unifies state-of-the-art self-supervised learning (SSL) methods across audio, vision, graphs, and cross-modal settings in a single, modular codebase. The goal of the demo is to show how researchers and practitioners can: (i) install, configure, and run pretext training with a few lines of code; (ii) reproduce compact benchmarks; and (iii) extend the framework with new modalities or methods through clean trainer and dataset abstractions. PrismSSL is packaged on PyPI, released under the MIT license, integrates tightly with HuggingFace Transformers, and provides quality-of-life features such as distributed training in PyTorch, Optuna-based hyperparameter search, LoRA fine-tuning for Transformer backbones, animated embedding visualizations for sanity checks, Weights & Biases logging, and colorful, structured terminal logs for improved usability and clarity. In addition, PrismSSL offers a graphical dashboard - built with Flask and standard web technologies - that enables users to configure and launch training pipelines with minimal coding. The artifact (code and data recipes) will be publicly available and reproducible.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smoothed Agnostic Learning of Halfspaces over the Hypercube</title>
<link>https://arxiv.org/abs/2511.17782</link>
<guid>https://arxiv.org/abs/2511.17782</guid>
<content:encoded><![CDATA[
arXiv:2511.17782v1 Announce Type: new 
Abstract: Agnostic learning of Boolean halfspaces is a fundamental problem in computational learning theory, but it is known to be computationally hard even for weak learning. Recent work [CKKMK24] proposed smoothed analysis as a way to bypass such hardness, but existing frameworks rely on additive Gaussian perturbations, making them unsuitable for discrete domains. We introduce a new smoothed agnostic learning framework for Boolean inputs, where perturbations are modeled via random bit flips. This defines a natural discrete analogue of smoothed optimality generalizing the Gaussian case. Under strictly subexponential assumptions on the input distribution, we give an efficient algorithm for learning halfspaces in this model, with runtime and sample complexity approximately n raised to a poly(1/(sigma * epsilon)) factor. Previously, such algorithms were known only with strong structural assumptions for the discrete hypercube, for example, independent coordinates or symmetric distributions. Our result provides the first computationally efficient guarantee for smoothed agnostic learning of halfspaces over the Boolean hypercube, bridging the gap between worst-case intractability and practical learnability in discrete settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Sample Complexity for Full Coverage in Compact and Continuous Spaces</title>
<link>https://arxiv.org/abs/2511.17784</link>
<guid>https://arxiv.org/abs/2511.17784</guid>
<content:encoded><![CDATA[
arXiv:2511.17784v1 Announce Type: new 
Abstract: Verifying uniform conditions over continuous spaces through random sampling is fundamental in machine learning and control theory, yet classical coverage analyses often yield conservative bounds, particularly at small failure probabilities. We study uniform random sampling on the $d$-dimensional unit hypercube and analyze the number of uncovered subcubes after discretization. By applying a concentration inequality to the uncovered-count statistic, we derive a sample complexity bound with a logarithmic dependence on the failure probability ($\delta$), i.e., $M =O( \tilde{C}\ln(\frac{2\tilde{C}}{\delta}))$, which contrasts sharply with the classical linear $1/\delta$ dependence. Under standard Lipschitz and uniformity assumptions, we present a self-contained derivation and compare our result with classical coupon-collector rates. Numerical studies across dimensions, precision levels, and confidence targets indicate that our bound tracks practical coverage requirements more tightly and scales favorably as $\delta \to 0$. Our findings offer a sharper theoretical tool for algorithms that rely on grid-based coverage guarantees, enabling more efficient sampling, especially in high-confidence regimes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Predictive Modeling of Microfluidic Cancer Cell Separation Using a Deterministic Lateral Displacement Device</title>
<link>https://arxiv.org/abs/2511.17787</link>
<guid>https://arxiv.org/abs/2511.17787</guid>
<content:encoded><![CDATA[
arXiv:2511.17787v1 Announce Type: new 
Abstract: Deterministic Lateral Displacement (DLD) devices are widely used in microfluidics for label-free, size-based separation of particles and cells, with particular promise in isolating circulating tumor cells (CTCs) for early cancer diagnostics. This study focuses on the optimization of DLD design parameters, such as row shift fraction, post size, and gap distance, to enhance the selective isolation of lung cancer cells based on their physical properties. To overcome the challenges of rare CTC detection and reduce reliance on computationally intensive simulations, machine learning models including gradient boosting, k-nearest neighbors, random forest, and multilayer perceptron (MLP) regressors are employed. Trained on a large, numerically validated dataset, these models predict particle trajectories and identify optimal device configurations, enabling high-throughput and cost-effective DLD design. Beyond trajectory prediction, the models aid in isolating critical design variables, offering a systematic, data-driven framework for automated DLD optimization. This integrative approach advances the development of scalable and precise microfluidic systems for cancer diagnostics, contributing to the broader goals of early detection and personalized medicine.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physical Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.17789</link>
<guid>https://arxiv.org/abs/2511.17789</guid>
<content:encoded><![CDATA[
arXiv:2511.17789v1 Announce Type: new 
Abstract: Digital computers are power-hungry and largely intolerant of damaged components, making them potentially difficult tools for energy-limited autonomous agents in uncertain environments. Recently developed Contrastive Local Learning Networks (CLLNs) - analog networks of self-adjusting nonlinear resistors - are inherently low-power and robust to physical damage, but were constructed to perform supervised learning. In this work we demonstrate success on two simple RL problems using Q-learning adapted for simulated CLLNs. Doing so makes explicit the components (beyond the network being trained) required to enact various tools in the RL toolbox, some of which (policy function and value function) are more natural in this system than others (replay buffer). We discuss assumptions such as the physical safety that digital hardware requires, CLLNs can forgo, and biological systems cannot rely on, and highlight secondary goals that are important in biology and trainable in CLLNs, but make little sense in digital computers.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Federated Multi-Label Feature Selection with Fuzzy Information Measures</title>
<link>https://arxiv.org/abs/2511.17796</link>
<guid>https://arxiv.org/abs/2511.17796</guid>
<content:encoded><![CDATA[
arXiv:2511.17796v1 Announce Type: new 
Abstract: Multi-label feature selection (FS) reduces the dimensionality of multi-label data by removing irrelevant, noisy, and redundant features, thereby boosting the performance of multi-label learning models. However, existing methods typically require centralized data, which makes them unsuitable for distributed and federated environments where each device/client holds its own local dataset. Additionally, federated methods often assume that clients have labeled data, which is unrealistic in cases where clients lack the expertise or resources to label task-specific data. To address these challenges, we propose a Semi-Supervised Federated Multi-Label Feature Selection method, called SSFMLFS, where clients hold only unlabeled data, while the server has limited labeled data. SSFMLFS adapts fuzzy information theory to a federated setting, where clients compute fuzzy similarity matrices and transmit them to the server, which then calculates feature redundancy and feature-label relevancy degrees. A feature graph is constructed by modeling features as vertices, assigning relevancy and redundancy degrees as vertex weights and edge weights, respectively. PageRank is then applied to rank the features by importance. Extensive experiments on five real-world datasets from various domains, including biology, images, music, and text, demonstrate that SSFMLFS outperforms other federated and centralized supervised and semi-supervised approaches in terms of three different evaluation metrics in non-IID data distribution setting.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization for Large Language Models</title>
<link>https://arxiv.org/abs/2511.17801</link>
<guid>https://arxiv.org/abs/2511.17801</guid>
<content:encoded><![CDATA[
arXiv:2511.17801v1 Announce Type: new 
Abstract: Large language models (LLMs) have significantly advanced natural language processing, but their massive parameter counts create substantial computational and memory challenges during deployment. Post-training quantization (PTQ) has emerged as a promising approach to mitigate these challenges with minimal overhead. While existing PTQ methods can effectively quantize LLMs, they experience substantial accuracy loss at extremely low bit-widths, primarily due to high-impact parameters that significantly influence quantization performance. Several approaches address these issues by identifying and retaining the high-impact parameters in FP16 format. However, they apply fixed ratios of high-impact parameters across all layers, overlooking layer-wise sensitivity variations. In this paper, we propose a quadratic optimization framework that determines layer-specific ratios of high-impact parameters while considering inter-layer dependencies. We quantize high-impact parameters to moderate bit-widths, which often result in negligible performance degradation in quantized LLMs, while the remaining parameters can be quantized to extremely low bit-widths. Under the same resource-constrained budget, this allows for preserving more high-impact parameters than methods that keep selecting a few in FP16 format. Additionally, the proposed framework allows us to leverage an advanced quantization method that often requires extensive learnable parameters solely for high-impact parameters, while applying a computationally efficient method to the rest. Our approach achieves an effective balance between computational efficiency and model accuracy while maintaining high performance compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Layer-Wise Transformations for Post-Training Quantization of Large Language Models</title>
<link>https://arxiv.org/abs/2511.17809</link>
<guid>https://arxiv.org/abs/2511.17809</guid>
<content:encoded><![CDATA[
arXiv:2511.17809v1 Announce Type: new 
Abstract: Large language models require significant computational resources for deployment, making quantization essential for practical applications. However, the main obstacle to effective quantization lies in systematic outliers in activations and weights, which cause substantial LLM performance degradation, especially at low-bit settings. While existing transformation-based methods like affine and rotation transformations successfully mitigate outliers, they apply the homogeneous transformation setting, i.e., using the same transformation types across all layers, ignoring the heterogeneous distribution characteristics within LLMs. In this paper, we propose an adaptive transformation selection framework that systematically determines optimal transformations on a per-layer basis. To this end, we first formulate transformation selection as a differentiable optimization problem to achieve the accurate transformation type for each layer. However, searching for optimal layer-wise transformations for every model is computationally expensive. To this end, we establish the connection between weight distribution kurtosis and accurate transformation type. Specifically, we propose an outlier-guided layer selection method using robust $z$-score normalization that achieves comparable performance to differentiable search with significantly reduced overhead. Comprehensive experiments on LLaMA family models demonstrate that our adaptive approach consistently outperforms the widely-used fixed transformation settings. For example, our method achieves an improvement of up to 4.58 perplexity points and a 2.11% gain in average six-task zero-shot accuracy under aggressive W3A3K2V2 quantization settings for the LLaMA-3-8B model compared to the current best existing method, FlatQuant, demonstrating the necessity of heterogeneous transformation selection for optimal LLM quantization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs</title>
<link>https://arxiv.org/abs/2511.17818</link>
<guid>https://arxiv.org/abs/2511.17818</guid>
<content:encoded><![CDATA[
arXiv:2511.17818v1 Announce Type: new 
Abstract: Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment. As such, OPE plays a critical role in ensuring safety in high-stakes domains such as healthcare. However, standard OPE approaches are limited by the size and coverage of the behavior dataset. While previous work has explored using expert-labeled counterfactual annotations to enhance dataset coverage, obtaining such annotations is expensive, limiting the scalability of prior approaches. We propose leveraging large language models (LLMs) to generate counterfactual annotations for OPE in medical domains. Our method uses domain knowledge to guide LLMs in predicting how key clinical features evolve under alternate treatments. These predicted features can then be transformed using known reward functions to create counterfactual annotations. We first evaluate the ability of several LLMs to predict clinical features across two patient subsets in MIMIC-IV, finding that state-of-the-art LLMs achieve comparable performance. Building on this capacity to predict clinical features, we generate LLM-based counterfactual annotations and incorporate them into an OPE estimator. Our empirical results analyze the benefits of counterfactual annotations under varying degrees of shift between the behavior and target policies. We find that in most cases, the LLM-based counterfactual annotations significantly improve OPE estimates up to a point. We provide an entropy-based metric to identify when additional annotations cease to be useful. Our results demonstrate that LLM-based counterfactual annotations offer a scalable approach for addressing coverage limitations in healthcare datasets, enabling safer deployment of decision-making policies in clinical settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Accuracy List-Decodable Mean Estimation</title>
<link>https://arxiv.org/abs/2511.17822</link>
<guid>https://arxiv.org/abs/2511.17822</guid>
<content:encoded><![CDATA[
arXiv:2511.17822v1 Announce Type: new 
Abstract: In list-decodable learning, we are given a set of data points such that an $\alpha$-fraction of these points come from a nice distribution $D$, for some small $\alpha \ll 1$, and the goal is to output a short list of candidate solutions, such that at least one element of this list recovers some non-trivial information about $D$. By now, there is a large body of work on this topic; however, while many algorithms can achieve optimal list size in terms of $\alpha$, all known algorithms must incur error which decays, in some cases quite poorly, with $1 / \alpha$. In this paper, we ask if this is inherent: is it possible to trade off list size with accuracy in list-decodable learning? More formally, given $\epsilon > 0$, can we can output a slightly larger list in terms of $\alpha$ and $\epsilon$, but so that one element of this list has error at most $\epsilon$ with the ground truth? We call this problem high-accuracy list-decodable learning. Our main result is that non-trivial high-accuracy guarantees, both information-theoretically and algorithmically, are possible for the canonical setting of list-decodable mean estimation of identity-covariance Gaussians. Specifically, we demonstrate that there exists a list of candidate means of size at most $L = \exp \left( O\left( \tfrac{\log^2 1 / \alpha}{\epsilon^2} \right)\right)$ so that one of the elements of this list has $\ell_2$ distance at most $\epsilon$ to the true mean. We also design an algorithm that outputs such a list with runtime and sample complexity $n = d^{O(\log L)} + \exp \exp (\widetilde{O}(\log L))$. We do so by demonstrating a completely novel proof of identifiability, as well as a new algorithmic way of leveraging this proof without the sum-of-squares hierarchy, which may be of independent technical interest.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A novel k-means clustering approach using two distance measures for Gaussian data</title>
<link>https://arxiv.org/abs/2511.17823</link>
<guid>https://arxiv.org/abs/2511.17823</guid>
<content:encoded><![CDATA[
arXiv:2511.17823v1 Announce Type: new 
Abstract: Clustering algorithms have long been the topic of research, representing the more popular side of unsupervised learning. Since clustering analysis is one of the best ways to find some clarity and structure within raw data, this paper explores a novel approach to \textit{k}-means clustering. Here we present a \textit{k}-means clustering algorithm that takes both the within cluster distance (WCD) and the inter cluster distance (ICD) as the distance metric to cluster the data into \emph{k} clusters pre-determined by the Calinski-Harabasz criterion in order to provide a more robust output for the clustering analysis. The idea with this approach is that by including both the measurement metrics, the convergence of the data into their clusters becomes solidified and more robust. We run the algorithm with some synthetically produced data and also some benchmark data sets obtained from the UCI repository. The results show that the convergence of the data into their respective clusters is more accurate by using both WCD and ICD measurement metrics. The algorithm is also better at clustering the outliers into their true clusters as opposed to the traditional \textit{k} means method. We also address some interesting possible research topics that reveal themselves as we answer the questions we initially set out to address.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch</title>
<link>https://arxiv.org/abs/2511.17826</link>
<guid>https://arxiv.org/abs/2511.17826</guid>
<content:encoded><![CDATA[
arXiv:2511.17826v1 Announce Type: new 
Abstract: Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Class and Domain Incremental Learning with Mixture of Experts for Indoor Localization</title>
<link>https://arxiv.org/abs/2511.17829</link>
<guid>https://arxiv.org/abs/2511.17829</guid>
<content:encoded><![CDATA[
arXiv:2511.17829v1 Announce Type: new 
Abstract: Indoor localization using machine learning has gained traction due to the growing demand for location-based services. However, its long-term reliability is hindered by hardware/software variations across mobile devices, which shift the model's input distribution to create domain shifts. Further, evolving indoor environments can introduce new locations over time, expanding the output space to create class shifts, making static machine learning models ineffective over time. To address these challenges, we propose a novel unified continual learning framework for indoor localization called MOELO that, for the first time, jointly addresses domain-incremental and class-incremental learning scenarios. MOELO enables a lightweight, robust, and adaptive localization solution that can be deployed on resource-limited mobile devices and is capable of continual learning in dynamic, heterogeneous real-world settings. This is made possible by a mixture-of-experts architecture, where experts are incrementally trained per region and selected through an equiangular tight frame based gating mechanism ensuring efficient routing, and low-latency inference, all within a compact model footprint. Experimental evaluations show that MOELO achieves improvements of up to 25.6x in mean localization error, 44.5x in worst-case localization error, and 21.5x lesser forgetting compared to state-of-the-art frameworks across diverse buildings, mobile devices, and learning scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Internalizing Tools as Morphisms in Graded Transformers</title>
<link>https://arxiv.org/abs/2511.17840</link>
<guid>https://arxiv.org/abs/2511.17840</guid>
<content:encoded><![CDATA[
arXiv:2511.17840v1 Announce Type: new 
Abstract: We introduce a graded formulation of internal symbolic computation for transformers. The hidden space is endowed with a grading $V=\bigoplus_{g\in G}V_g$, and symbolic operations are realized as typed block maps (morphisms) $\phi_{h\leftarrow g}:V_g\to V_h$ that are activated selectively by a differentiable routing policy. A self-supervised \emph{graded utility functional}, defined as the loss reduction induced by a candidate morphism, governs activation and yields sparse, interpretable behavior. We develop the algebraic and geometric foundations: an internal model category whose objects are homogeneous components and whose morphisms are admissible grade transitions; adjoint pairs encoding typed round trips; and information-geometric interpretations in terms of KL gain, mirror descent with Bregman divergences, and Fisher natural gradients. Methodologically, we specify a utility--aware routing mechanism and objective that remain fully end-to-end differentiable. Analytic case studies and lightweight sanity checks illustrate selective morphic activation on hybrid symbolic-linguistic tasks. The framework unifies symbolic computation, geometry, and self--supervised learning within the \emph{graded transformer} formalism \cite{sh-89,sh-95}, while subsuming prior external-tool paradigms (e.g., Toolformer \cite{toolformer2023}) as a special case via functorial internalization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Kinetic Monte-Carlo Simulations of Grain Growth with Combined Convolutional and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.17848</link>
<guid>https://arxiv.org/abs/2511.17848</guid>
<content:encoded><![CDATA[
arXiv:2511.17848v1 Announce Type: new 
Abstract: Graph neural networks (GNN) have emerged as a promising machine learning method for microstructure simulations such as grain growth. However, accurate modeling of realistic grain boundary networks requires large simulation cells, which GNN has difficulty scaling up to. To alleviate the computational costs and memory footprint of GNN, we propose a hybrid architecture combining a convolutional neural network (CNN) based bijective autoencoder to compress the spatial dimensions, and a GNN that evolves the microstructure in the latent space of reduced spatial sizes. Our results demonstrate that the new design significantly reduces computational costs with using fewer message passing layer (from 12 down to 3) compared with GNN alone. The reduction in computational cost becomes more pronounced as the spatial size increases, indicating strong computational scalability. For the largest mesh evaluated (160^3), our method reduces memory usage and runtime in inference by 117x and 115x, respectively, compared with GNN-only baseline. More importantly, it shows higher accuracy and stronger spatiotemporal capability than the GNN-only baseline, especially in long-term testing. Such combination of scalability and accuracy is essential for simulating realistic material microstructures over extended time scales. The improvements can be attributed to the bijective autoencoder's ability to compress information losslessly from spatial domain into a high dimensional feature space, thereby producing more expressive latent features for the GNN to learn from, while also contributing its own spatiotemporal modeling capability. The training was optimized to learn from the stochastic Potts Monte Carlo method. Our findings provide a highly scalable approach for simulating grain growth.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently</title>
<link>https://arxiv.org/abs/2511.17852</link>
<guid>https://arxiv.org/abs/2511.17852</guid>
<content:encoded><![CDATA[
arXiv:2511.17852v1 Announce Type: new 
Abstract: Transformers can acquire Chain-of-Thought (CoT) capabilities to solve complex reasoning tasks through fine-tuning. Reinforcement learning (RL) and supervised fine-tuning (SFT) are two primary approaches to this end, yet their underlying mechanisms and differences remain theoretically unclear. In this work, we examine these aspects specifically for learning $k$-sparse Boolean functions with a one-layer transformer and intermediate supervision that is akin to CoT. In particular, we consider $k$-sparse Boolean functions that can be recursively decomposed into fixed 2-sparse Boolean functions. We analyze the learning dynamics of fine-tuning the transformer via either RL or SFT with CoT to identify sufficient conditions for it to provably learn these functions. We verify that these conditions hold for three basic examples, including $k$-PARITY, $k$-AND, and $k$-OR, thus demonstrating the learnability of both approaches. Notably, we reveal that RL and SFT exhibit distinct learning behaviors: RL learns the whole CoT chain simultaneously, whereas SFT learns the CoT chain step-by-step. Overall, our findings provide theoretical insights into the underlying mechanisms of RL and SFT as well as how they differ in triggering the CoT capabilities of transformers.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cost-Sensitive Conformal Training with Provably Controllable Learning Bounds</title>
<link>https://arxiv.org/abs/2511.17861</link>
<guid>https://arxiv.org/abs/2511.17861</guid>
<content:encoded><![CDATA[
arXiv:2511.17861v1 Announce Type: new 
Abstract: Conformal prediction (CP) is a general framework to quantify the predictive uncertainty of machine learning models that uses a set prediction to include the true label with a valid probability. To align the uncertainty measured by CP, conformal training methods minimize the size of the prediction sets. A typical way is to use a surrogate indicator function, usually Sigmoid or Gaussian error function. However, these surrogate functions do not have a uniform error bound to the indicator function, leading to uncontrollable learning bounds. In this paper, we propose a simple cost-sensitive conformal training algorithm that does not rely on the indicator approximation mechanism. Specifically, we theoretically show that minimizing the expected size of prediction sets is upper bounded by the expected rank of true labels. To this end, we develop a rank weighting strategy that assigns the weight using the rank of true label on each data sample. Our analysis provably demonstrates the tightness between the proposed weighted objective and the expected size of conformal prediction sets. Extensive experiments verify the validity of our theoretical insights, and superior empirical performance over other conformal training in terms of predictive efficiency with 21.38% reduction for average prediction set size.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivalence of Context and Parameter Updates in Modern Transformer Blocks</title>
<link>https://arxiv.org/abs/2511.17864</link>
<guid>https://arxiv.org/abs/2511.17864</guid>
<content:encoded><![CDATA[
arXiv:2511.17864v1 Announce Type: new 
Abstract: Recent research has established that the impact of context in a vanilla transformer can be represented implicitly by forming a token-dependent, rank-1 patch to its MLP weights. This work extends that foundational theory to the diverse architectures of modern Large Language Models. We first demonstrate a precise, analytical solution for a Gemma-style transformer block, proving that the entire effect of a context can be perfectly mapped to rank-1 patches on its MLP weight matrices and a patch to the RMSNorm scale. We then generalize this result, providing a constructive proof and algorithm for multi-layer models. To unify these findings, we introduce a general framework centered on two core properties: input controllability and output controllability. We prove that a perfect implicit weight patch is possible for any MLP block where the inner function is input-controllable and the outer function is output-controllable. This provides a simpler and more powerful lens for understanding how transformer models transmute prompts into effective weights. This setup generalizes to a wide range of modern LLM architectures including gating, pre-/post-norm, mixture of experts and sequential/parallel transformer blocks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Horcrux: Mechanistically Interpretable Task Decomposition for Detecting and Mitigating Reward Hacking in Embodied AI Systems</title>
<link>https://arxiv.org/abs/2511.17869</link>
<guid>https://arxiv.org/abs/2511.17869</guid>
<content:encoded><![CDATA[
arXiv:2511.17869v1 Announce Type: new 
Abstract: Embodied AI agents exploit reward signal flaws through reward hacking, achieving high proxy scores while failing true objectives. We introduce Mechanistically Interpretable Task Decomposition (MITD), a hierarchical transformer architecture with Planner, Coordinator, and Executor modules that detects and mitigates reward hacking. MITD decomposes tasks into interpretable subtasks while generating diagnostic visualizations including Attention Waterfall Diagrams and Neural Pathway Flow Charts. Experiments on 1,000 HH-RLHF samples reveal that decomposition depths of 12 to 25 steps reduce reward hacking frequency by 34 percent across four failure modes. We present new paradigms showing that mechanistically grounded decomposition offers a more effective way to detect reward hacking than post-hoc behavioral monitoring.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction</title>
<link>https://arxiv.org/abs/2511.17879</link>
<guid>https://arxiv.org/abs/2511.17879</guid>
<content:encoded><![CDATA[
arXiv:2511.17879v1 Announce Type: new 
Abstract: Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player's future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing</title>
<link>https://arxiv.org/abs/2511.17902</link>
<guid>https://arxiv.org/abs/2511.17902</guid>
<content:encoded><![CDATA[
arXiv:2511.17902v1 Announce Type: new 
Abstract: Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning.
  To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data.
  Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Catastrophic Forgetting in Streaming Generative and Predictive Learning via Stateful Replay</title>
<link>https://arxiv.org/abs/2511.17936</link>
<guid>https://arxiv.org/abs/2511.17936</guid>
<content:encoded><![CDATA[
arXiv:2511.17936v1 Announce Type: new 
Abstract: Many deployed learning systems must update models on streaming data under memory constraints. The default strategy, sequential fine-tuning on each new phase, is architecture-agnostic but often suffers catastrophic forgetting when later phases correspond to different sub-populations or tasks. Replay with a finite buffer is a simple alternative, yet its behaviour across generative and predictive objectives is not well understood. We present a unified study of stateful replay for streaming autoencoding, time series forecasting, and classification. We view both sequential fine-tuning and replay as stochastic gradient methods for an ideal joint objective, and use a gradient alignment analysis to show when mixing current and historical samples should reduce forgetting. We then evaluate a single replay mechanism on six streaming scenarios built from Rotated MNIST, ElectricityLoadDiagrams 2011-2014, and Airlines delay data, using matched training budgets and three seeds. On heterogeneous multi task streams, replay reduces average forgetting by a factor of two to three, while on benign time based streams both methods perform similarly. These results position stateful replay as a strong and simple baseline for continual learning in streaming environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Transportability for Structural Causal Bandits</title>
<link>https://arxiv.org/abs/2511.17953</link>
<guid>https://arxiv.org/abs/2511.17953</guid>
<content:encoded><![CDATA[
arXiv:2511.17953v1 Announce Type: new 
Abstract: Intelligent agents equipped with causal knowledge can optimize their action spaces to avoid unnecessary exploration. The structural causal bandit framework provides a graphical characterization for identifying actions that are unable to maximize rewards by leveraging prior knowledge of the underlying causal structure. While such knowledge enables an agent to estimate the expected rewards of certain actions based on others in online interactions, there has been little guidance on how to transfer information inferred from arbitrary combinations of datasets collected under different conditions -- observational or experimental -- and from heterogeneous environments. In this paper, we investigate the structural causal bandit with transportability, where priors from the source environments are fused to enhance learning in the deployment setting. We demonstrate that it is possible to exploit invariances across environments to consistently improve learning. The resulting bandit algorithm achieves a sub-linear regret bound with an explicit dependence on informativeness of prior data, and it may outperform standard bandit approaches that rely solely on online learning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization</title>
<link>https://arxiv.org/abs/2511.17963</link>
<guid>https://arxiv.org/abs/2511.17963</guid>
<content:encoded><![CDATA[
arXiv:2511.17963v1 Announce Type: new 
Abstract: This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Federated Learning for Cyber-Resilient Microgrid Energy Management</title>
<link>https://arxiv.org/abs/2511.17968</link>
<guid>https://arxiv.org/abs/2511.17968</guid>
<content:encoded><![CDATA[
arXiv:2511.17968v1 Announce Type: new 
Abstract: Maintaining economic efficiency and operational reliability in microgrid energy management systems under cyberattack conditions remains challenging. Most approaches assume non-anomalous measurements, make predictions with unquantified uncertainties, and do not mitigate malicious attacks on renewable forecasts for energy management optimization. This paper presents a comprehensive cyber-resilient framework integrating federated Long Short-Term Memory-based photovoltaic forecasting with a novel two-stage cascade false data injection attack detection and energy management system optimization. The approach combines autoencoder reconstruction error with prediction uncertainty quantification to enable attack-resilient energy storage scheduling while preserving data privacy. Extreme false data attack conditions were studied that caused 58% forecast degradation and 16.9\% operational cost increases. The proposed integrated framework reduced false positive detections by 70%, recovered 93.7% of forecasting performance losses, and achieved 5\% operational cost savings, mitigating 34.7% of attack-induced economic losses. Results demonstrate that precision-focused cascade detection with multi-signal fusion outperforms single-signal approaches, validating security-performance synergy for decentralized microgrids.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllability Analysis of State Space-based Language Model</title>
<link>https://arxiv.org/abs/2511.17970</link>
<guid>https://arxiv.org/abs/2511.17970</guid>
<content:encoded><![CDATA[
arXiv:2511.17970v1 Announce Type: new 
Abstract: State-space models (SSMs), particularly Mamba, have become powerful architectures for sequence modeling, yet their internal dynamics remain poorly understood compared to attention-based models. We introduce and validate the Influence Score, a controllability-based metric derived from the discretized state-space parameters of Mamba and computed through a backward recurrence analogous to system observability. The score quantifies how strongly a token at position k affects all later states and outputs. We evaluate this measure across three Mamba variants: mamba-130m, mamba-2.8b, and mamba-2.8b-slimpj, using six experiments that test its sensitivity to temperature, prompt complexity, token type, layer depth, token position, and input perturbations. The results show three main insights: (1) the Influence Score increases with model size and training data, reflecting model capacity; (2) Mamba exhibits consistent architectural patterns, including recency bias and concentrated influence in mid-to-late layers; and (3) emergent behaviors appear only at scale, with mamba-2.8b-slimpj uniquely prioritizing content words and reducing internal influence in the presence of noise. These findings establish the Influence Score as a practical diagnostic tool for interpreting and comparing SSM-based language models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Anomaly Detection and Mitigation for EV Charging Forecasting Under Cyberattacks</title>
<link>https://arxiv.org/abs/2511.17978</link>
<guid>https://arxiv.org/abs/2511.17978</guid>
<content:encoded><![CDATA[
arXiv:2511.17978v1 Announce Type: new 
Abstract: Electric Vehicle (EV) charging infrastructure faces escalating cybersecurity threats that can severely compromise operational efficiency and grid stability. Existing forecasting techniques are limited by the lack of combined robust anomaly mitigation solutions and data privacy preservation. Therefore, this paper addresses these challenges by proposing a novel anomaly-resilient federated learning framework that simultaneously preserves data privacy, detects cyber-attacks, and maintains trustworthy demand prediction accuracy under adversarial conditions. The proposed framework integrates three key innovations: LSTM autoencoder-based distributed anomaly detection deployed at each federated client, interpolation-based anomalous data mitigation to preserve temporal continuity, and federated Long Short-Term Memory (LSTM) networks that enable collaborative learning without centralized data aggregation. The framework is validated on real-world EV charging infrastructure datasets combined with real-world DDoS attack datasets, providing robust validation of the proposed approach under realistic threat scenarios. Experimental results demonstrate that the federated approach achieves superior performance compared to centralized models, with 15.2% improvement in R2 accuracy while maintaining data locality. The integrated cyber-attack detection and mitigation system produces trustworthy datasets that enhance prediction reliability, recovering 47.9% of attack-induced performance degradation while maintaining exceptional precision (91.3%) and minimal false positive rates (1.21%). The proposed architecture enables enhanced EV infrastructure planning, privacy-preserving collaborative forecasting, cybersecurity resilience, and rapid recovery from malicious threats across distributed charging networks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Adaptive Resonance Theory-based Topological Clustering Algorithm with a Self-Adjusting Vigilance Parameter</title>
<link>https://arxiv.org/abs/2511.17983</link>
<guid>https://arxiv.org/abs/2511.17983</guid>
<content:encoded><![CDATA[
arXiv:2511.17983v1 Announce Type: new 
Abstract: Clustering in stationary and nonstationary settings, where data distributions remain static or evolve over time, requires models that can adapt to distributional shifts while preserving previously learned cluster structures. This paper proposes an Adaptive Resonance Theory (ART)-based topological clustering algorithm that autonomously adjusts its recalculation interval and vigilance threshold through a diversity-driven adaptation mechanism. This mechanism enables hyperparameter-free learning that maintains cluster stability and continuity in dynamic environments. Experiments on 24 real-world datasets demonstrate that the proposed algorithm outperforms state-of-the-art methods in both clustering performance and continual learning capability. These results highlight the effectiveness of the proposed parameter adaptation in mitigating catastrophic forgetting and maintaining consistent clustering in evolving data streams. Source code is available at https://github.com/Masuyama-lab/IDAT
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Escaping Optimization Stagnation: Taking Steps Beyond Task Arithmetic via Difference Vectors</title>
<link>https://arxiv.org/abs/2511.17987</link>
<guid>https://arxiv.org/abs/2511.17987</guid>
<content:encoded><![CDATA[
arXiv:2511.17987v1 Announce Type: new 
Abstract: Current methods for editing pre-trained models face significant challenges, primarily high computational costs and limited scalability. Task arithmetic has recently emerged as a promising solution, using simple arithmetic operations-addition and negation-based on task vectors which are the differences between fine-tuned and pre-trained model weights, to efficiently modify model behavior. However, the full potential of task arithmetic remains underexplored, primarily due to limited mechanisms for overcoming optimization stagnation. To address this challenge, we introduce the notion of difference vector, a generalized form of task vectors derived from the historical movements during optimization. Using difference vectors as directed perturbations, we propose the Difference Vector-based Anisotropic Scaling Iterative algorithm (DV-BASI) to enable a continuous optimization process for task arithmetic methods without relying on any additional modules or components. Notably, by leveraging escapability and directional advantages of difference vectors, the average performance on different tasks of the multi-task model merged by DV-BASI may even outperform models individually fine-tuned. Based on this observation, we extend the application of difference vectors to a feasible fine-tuning method for single-task models. On the practical side, DV-BASI allows expressive searching directions with few learnable parameters and forms a scalable framework. We also integrate DV-BASI with task arithmetic methods and advanced optimization techniques to achieve state-of-the-art performance on both supervised and unsupervised evaluation protocols.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Auditing of Multi-domain Graph Pre-trained Model under Membership Inference Attacks</title>
<link>https://arxiv.org/abs/2511.17989</link>
<guid>https://arxiv.org/abs/2511.17989</guid>
<content:encoded><![CDATA[
arXiv:2511.17989v1 Announce Type: new 
Abstract: Multi-domain graph pre-training has emerged as a pivotal technique in developing graph foundation models. While it greatly improves the generalization of graph neural networks, its privacy risks under membership inference attacks (MIAs), which aim to identify whether a specific instance was used in training (member), remain largely unexplored. However, effectively conducting MIAs against multi-domain graph pre-trained models is a significant challenge due to: (i) Enhanced Generalization Capability: Multi-domain pre-training reduces the overfitting characteristics commonly exploited by MIAs. (ii) Unrepresentative Shadow Datasets: Diverse training graphs hinder the obtaining of reliable shadow graphs. (iii) Weakened Membership Signals: Embedding-based outputs offer less informative cues than logits for MIAs. To tackle these challenges, we propose MGP-MIA, a novel framework for Membership Inference Attacks against Multi-domain Graph Pre-trained models. Specifically, we first propose a membership signal amplification mechanism that amplifies the overfitting characteristics of target models via machine unlearning. We then design an incremental shadow model construction mechanism that builds a reliable shadow model with limited shadow graphs via incremental learning. Finally, we introduce a similarity-based inference mechanism that identifies members based on their similarity to positive and negative samples. Extensive experiments demonstrate the effectiveness of our proposed MGP-MIA and reveal the privacy risks of multi-domain graph pre-training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Rate Scheduling with Matrix Factorization for Private Training</title>
<link>https://arxiv.org/abs/2511.17994</link>
<guid>https://arxiv.org/abs/2511.17994</guid>
<content:encoded><![CDATA[
arXiv:2511.17994v1 Announce Type: new 
Abstract: We study differentially private model training with stochastic gradient descent under learning rate scheduling and correlated noise. Although correlated noise, in particular via matrix factorizations, has been shown to improve accuracy, prior theoretical work focused primarily on the prefix-sum workload. That workload assumes a constant learning rate, whereas in practice learning rate schedules are widely used to accelerate training and improve convergence. We close this gap by deriving general upper and lower bounds for a broad class of learning rate schedules in both single- and multi-epoch settings. Building on these results, we propose a learning-rate-aware factorization that achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics. Our theoretical analysis yields memory-efficient constructions suitable for practical deployment, and experiments on CIFAR-10 and IMDB datasets confirm that schedule-aware factorizations improve accuracy in private training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning</title>
<link>https://arxiv.org/abs/2511.18000</link>
<guid>https://arxiv.org/abs/2511.18000</guid>
<content:encoded><![CDATA[
arXiv:2511.18000v1 Announce Type: new 
Abstract: We present ContagionRL, a Gymnasium-compatible reinforcement learning platform specifically designed for systematic reward engineering in spatial epidemic simulations. Unlike traditional agent-based models that rely on fixed behavioral rules, our platform enables rigorous evaluation of how reward function design affects learned survival strategies across diverse epidemic scenarios. ContagionRL integrates a spatial SIRS+D epidemiological model with configurable environmental parameters, allowing researchers to stress-test reward functions under varying conditions including limited observability, different movement patterns, and heterogeneous population dynamics. We evaluate five distinct reward designs, ranging from sparse survival bonuses to a novel potential field approach, across multiple RL algorithms (PPO, SAC, A2C). Through systematic ablation studies, we identify that directional guidance and explicit adherence incentives are critical components for robust policy learning. Our comprehensive evaluation across varying infection rates, grid sizes, visibility constraints, and movement patterns reveals that reward function choice dramatically impacts agent behavior and survival outcomes. Agents trained with our potential field reward consistently achieve superior performance, learning maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies. The platform's modular design enables systematic exploration of reward-behavior relationships, addressing a knowledge gap in models of this type where reward engineering has received limited attention. ContagionRL is an effective platform for studying adaptive behavioral responses in epidemic contexts and highlight the importance of reward design, information structure, and environmental predictability in learning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Private Learning From Feature Perspective</title>
<link>https://arxiv.org/abs/2511.18006</link>
<guid>https://arxiv.org/abs/2511.18006</guid>
<content:encoded><![CDATA[
arXiv:2511.18006v1 Announce Type: new 
Abstract: Differentially private Stochastic Gradient Descent (DP-SGD) has become integral to privacy-preserving machine learning, ensuring robust privacy guarantees in sensitive domains. Despite notable empirical advances leveraging features from non-private, pre-trained models to enhance DP-SGD training, a theoretical understanding of feature dynamics in private learning remains underexplored. This paper presents the first theoretical framework to analyze private training through a feature learning perspective. Building on the multi-patch data structure from prior work, our analysis distinguishes between label-dependent feature signals and label-independent noise, a critical aspect overlooked by existing analyses in the DP community. Employing a two-layer CNN with polynomial ReLU activation, we theoretically characterize both feature signal learning and data noise memorization in private training via noisy gradient descent. Our findings reveal that (1) Effective private signal learning requires a higher signal-to-noise ratio (SNR) compared to non-private training, and (2) When data noise memorization occurs in non-private learning, it will also occur in private learning, leading to poor generalization despite small training loss. Our findings highlight the challenges of private learning and prove the benefit of feature enhancement to improve SNR. Experiments on synthetic and real-world datasets also validate our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curvature-Aware Safety Restoration In LLMs Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.18039</link>
<guid>https://arxiv.org/abs/2511.18039</guid>
<content:encoded><![CDATA[
arXiv:2511.18039v1 Announce Type: new 
Abstract: Fine-tuning Large Language Models (LLMs) for downstream tasks often compromises safety alignment, even when using parameter-efficient methods like LoRA. In this work, we uncover a notable property: fine-tuned models preserve the geometric structure of their loss landscapes concerning harmful content, regardless of the fine-tuning method employed. This suggests that safety behaviors are not erased but shifted to less influential regions of the parameter space. Building on this insight, we propose a curvature-aware alignment restoration method that leverages influence functions and second-order optimization to selectively increase loss on harmful inputs while preserving task performance. By navigating the shared geometry between base and fine-tuned models, our method discourages unsafe outputs while preserving task-relevant performance, avoiding full reversion and enabling precise, low-impact updates. Extensive evaluations across multiple model families and adversarial settings show that our approach efficiently reduces harmful responses while maintaining or even improving utility and few-shot learning performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Linkage Clustering Beyond Binary Trees and Ultrametrics</title>
<link>https://arxiv.org/abs/2511.18056</link>
<guid>https://arxiv.org/abs/2511.18056</guid>
<content:encoded><![CDATA[
arXiv:2511.18056v1 Announce Type: new 
Abstract: Hierarchical clustering seeks to uncover nested structures in data by constructing a tree of clusters, where deeper levels reveal finer-grained relationships. Traditional methods, including linkage approaches, face three major limitations: (i) they always return a hierarchy, even if none exists, (ii) they are restricted to binary trees, even if the true hierarchy is non-binary, and (iii) they are highly sensitive to the choice of linkage function. In this paper, we address these issues by introducing the notion of a valid hierarchy and defining a partial order over the set of valid hierarchies. We prove the existence of a finest valid hierarchy, that is, the hierarchy that encodes the maximum information consistent with the similarity structure of the data set. In particular, the finest valid hierarchy is not constrained to binary structures and, when no hierarchical relationships exist, collapses to a star tree. We propose a simple two-step algorithm that first constructs a binary tree via a linkage method and then prunes it to enforce validity. We establish necessary and sufficient conditions on the linkage function under which this procedure exactly recovers the finest valid hierarchy, and we show that all linkage functions satisfying these conditions yield the same hierarchy after pruning. Notably, classical linkage rules such as single, complete, and average satisfy these conditions, whereas Ward's linkage fails to do so.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data</title>
<link>https://arxiv.org/abs/2511.18066</link>
<guid>https://arxiv.org/abs/2511.18066</guid>
<content:encoded><![CDATA[
arXiv:2511.18066v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) in federated learning (FL) is crucial for handling unseen data distributions across clients, particularly when faced with domain shifts and skewed class distributions. Class Imbalance (CI) remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed CI during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none address class unsupervised adaptation to dynamic domains or distribution shifts at inference time under federated CI constraints. Revealing the failure of state-of-the-art TTA in federated client adaptation in CI scenario, we propose pFedBBN,a personalized federated test-time adaptation framework that employs balanced batch normalization (BBN) during local client adaptation to mitigate prediction bias by treating all classes equally, while also enabling client collaboration guided by BBN similarity, ensuring that clients with similar balanced representations reinforce each other and that adaptation remains aligned with domain-specific characteristics. pFedBBN supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Extensive experiments across diverse baselines show that pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality</title>
<link>https://arxiv.org/abs/2511.18084</link>
<guid>https://arxiv.org/abs/2511.18084</guid>
<content:encoded><![CDATA[
arXiv:2511.18084v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly adopted in clinical decision support, yet aligning them with the multifaceted reasoning pathways of real-world medicine remains a major challenge. Using more than 8,000 infertility treatment records, we systematically evaluate four alignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-layer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments. GRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the value of reinforcement-based optimization for structured prediction tasks. However, clinicians consistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher therapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest winning rate (51.2%), outperforming both GRPO (26.2%) and even physicians' original decisions (22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessarily translate into higher clinical trust, and may diverge from human-centered preferences. Our findings highlight the need for alignment strategies that prioritize clinically interpretable and practically feasible reasoning, rather than solely optimizing decision-level accuracy.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization</title>
<link>https://arxiv.org/abs/2511.18093</link>
<guid>https://arxiv.org/abs/2511.18093</guid>
<content:encoded><![CDATA[
arXiv:2511.18093v1 Announce Type: new 
Abstract: Predictive control approaches based on deep reinforcement learning (DRL) have gained significant attention in microgrid energy optimization. However, existing research often overlooks the issue of uncertainty stemming from imperfect prediction models, which can lead to suboptimal control strategies. This paper presents a new error temporal difference (ETD) algorithm for DRL to address the uncertainty in predictions,aiming to improve the performance of microgrid operations. First,a microgrid system integrated with renewable energy sources (RES) and energy storage systems (ESS), along with its Markov decision process (MDP), is modelled. Second, a predictive control approach based on a deep Q network (DQN) is presented, in which a weighted average algorithm and a new ETD algorithm are designed to quantify and address the prediction uncertainty, respectively. Finally, simulations on a realworld US dataset suggest that the developed ETD effectively improves the performance of DRL in optimizing microgrid operations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Learning with Selective Time-Step Acquisition for PDEs</title>
<link>https://arxiv.org/abs/2511.18107</link>
<guid>https://arxiv.org/abs/2511.18107</guid>
<content:encoded><![CDATA[
arXiv:2511.18107v1 Announce Type: new 
Abstract: Accurately solving partial differential equations (PDEs) is critical to understanding complex scientific and engineering phenomena, yet traditional numerical solvers are computationally expensive. Surrogate models offer a more efficient alternative, but their development is hindered by the cost of generating sufficient training data from numerical solvers. In this paper, we present a novel framework for active learning (AL) in PDE surrogate modeling that reduces this cost. Unlike the existing AL methods for PDEs that always acquire entire PDE trajectories, our approach strategically generates only the most important time steps with the numerical solver, while employing the surrogate model to approximate the remaining steps. This dramatically reduces the cost incurred by each trajectory and thus allows the active learning algorithm to try out a more diverse set of trajectories given the same budget. To accommodate this novel framework, we develop an acquisition function that estimates the utility of a set of time steps by approximating its resulting variance reduction. We demonstrate the effectiveness of our method on several benchmark PDEs, including the Burgers' equation, Korteweg-De Vries equation, Kuramoto-Sivashinsky equation, the incompressible Navier-Stokes equation, and the compressible Navier-Stokes equation. Experiments show that our approach improves performance by large margins over the best existing method. Our method not only reduces average error but also the 99\%, 95\%, and 50\% quantiles of error, which is rare for an AL algorithm. All in all, our approach offers a data-efficient solution to surrogate modeling for PDEs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vulnerability-Aware Robust Multimodal Adversarial Training</title>
<link>https://arxiv.org/abs/2511.18138</link>
<guid>https://arxiv.org/abs/2511.18138</guid>
<content:encoded><![CDATA[
arXiv:2511.18138v1 Announce Type: new 
Abstract: Multimodal learning has shown significant superiority on various tasks by integrating multiple modalities. However, the interdependencies among modalities increase the susceptibility of multimodal models to adversarial attacks. Existing methods mainly focus on attacks on specific modalities or indiscriminately attack all modalities. In this paper, we find that these approaches ignore the differences between modalities in their contribution to final robustness, resulting in suboptimal robustness performance. To bridge this gap, we introduce Vulnerability-Aware Robust Multimodal Adversarial Training (VARMAT), a probe-in-training adversarial training method that improves multimodal robustness by identifying the vulnerability of each modality. To be specific, VARMAT first explicitly quantifies the vulnerability of each modality, grounded in a first-order approximation of the attack objective (Probe). Then, we propose a targeted regularization term that penalizes modalities with high vulnerability, guiding robust learning while maintaining task accuracy (Training). We demonstrate the enhanced robustness of our method across multiple multimodal datasets involving diverse modalities. Finally, we achieve {12.73%, 22.21%, 11.19%} robustness improvement on three multimodal datasets, revealing a significant blind spot in multimodal adversarial training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction</title>
<link>https://arxiv.org/abs/2511.18150</link>
<guid>https://arxiv.org/abs/2511.18150</guid>
<content:encoded><![CDATA[
arXiv:2511.18150v1 Announce Type: new 
Abstract: We investigate machine learning approaches to approximating the \emph{domination number} of graphs, the minimum size of a dominating set. Exact computation of this parameter is NP-hard, restricting classical methods to small instances. We compare two neural paradigms: Convolutional Neural Networks (CNNs), which operate on adjacency matrix representations, and Graph Neural Networks (GNNs), which learn directly from graph structure through message passing. Across 2,000 random graphs with up to 64 vertices, GNNs achieve markedly higher accuracy ($R^2=0.987$, MAE $=0.372$) than CNNs ($R^2=0.955$, MAE $=0.500$). Both models offer substantial speedups over exact solvers, with GNNs delivering more than $200\times$ acceleration while retaining near-perfect fidelity. Our results position GNNs as a practical surrogate for combinatorial graph invariants, with implications for scalable graph optimization and mathematical discovery.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>scipy.spatial.transform: Differentiable Framework-Agnostic 3D Transformations in Python</title>
<link>https://arxiv.org/abs/2511.18157</link>
<guid>https://arxiv.org/abs/2511.18157</guid>
<content:encoded><![CDATA[
arXiv:2511.18157v1 Announce Type: new 
Abstract: Three-dimensional rigid-body transforms, i.e. rotations and translations, are central to modern differentiable machine learning pipelines in robotics, vision, and simulation. However, numerically robust and mathematically correct implementations, particularly on SO(3), are error-prone due to issues such as axis conventions, normalizations, composition consistency and subtle errors that only appear in edge cases. SciPy's spatial.transform module is a rigorously tested Python implementation. However, it historically only supported NumPy, limiting adoption in GPU-accelerated and autodiff-based workflows. We present a complete overhaul of SciPy's spatial.transform functionality that makes it compatible with any array library implementing the Python array API, including JAX, PyTorch, and CuPy. The revised implementation preserves the established SciPy interface while enabling GPU/TPU execution, JIT compilation, vectorized batching, and differentiation via native autodiff of the chosen backend. We demonstrate how this foundation supports differentiable scientific computing through two case studies: (i) scalability of 3D transforms and rotations and (ii) a JAX drone simulation that leverages SciPy's Rotation for accurate integration of rotational dynamics. Our contributions have been merged into SciPy main and will ship in the next release, providing a framework-agnostic, production-grade basis for 3D spatial math in differentiable systems and ML.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LocaGen: Low-Overhead Indoor Localization Through Spatial Augmentation</title>
<link>https://arxiv.org/abs/2511.18158</link>
<guid>https://arxiv.org/abs/2511.18158</guid>
<content:encoded><![CDATA[
arXiv:2511.18158v1 Announce Type: new 
Abstract: Indoor localization systems commonly rely on fingerprinting, which requires extensive survey efforts to obtain location-tagged signal data, limiting their real-world deployability. Recent approaches that attempt to reduce this overhead either suffer from low representation ability, mode collapse issues, or require the effort of collecting data at all target locations. We present LocaGen, a novel spatial augmentation framework that significantly reduces fingerprinting overhead by generating high-quality synthetic data at completely unseen locations. LocaGen leverages a conditional diffusion model guided by a novel spatially aware optimization strategy to synthesize realistic fingerprints at unseen locations using only a subset of seen locations. To further improve our diffusion model performance, LocaGen augments seen location data based on domain-specific heuristics and strategically selects the seen and unseen locations using a novel density-based approach that ensures robust coverage. Our extensive evaluation on a real-world WiFi fingerprinting dataset shows that LocaGen maintains the same localization accuracy even with 30% of the locations unseen and achieves up to 28% improvement in accuracy over state-of-the-art augmentation methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bringing Stability to Diffusion: Decomposing and Reducing Variance of Training Masked Diffusion Models</title>
<link>https://arxiv.org/abs/2511.18159</link>
<guid>https://arxiv.org/abs/2511.18159</guid>
<content:encoded><![CDATA[
arXiv:2511.18159v1 Announce Type: new 
Abstract: Masked diffusion models (MDMs) are a promising alternative to autoregressive models (ARMs), but they suffer from inherently much higher training variance. High variance leads to noisier gradient estimates and unstable optimization, so even equally strong pretrained MDMs and ARMs that are competitive at initialization often diverge after task-specific training, with MDMs falling far behind. There has been no theoretical explanation or systematic solution. We derive the first decomposition of MDM training variance into three sources: (A) masking pattern noise, (B) masking rate noise, and (C) data noise, while ARMs are only affected by (C). This explains the fundamental training gap. Building on this foundation, we design six variance-reduction methods, including two core methods: (1) P-POTS, a Pareto-optimal t sampler that minimizes training variance by sampling harder t values more often with appropriately smaller update steps, and (2) MIRROR, which uses negatively correlated samples to reduce (A). Experiments show that compared to standard MDM training, our methods improve accuracy by 7-8% on complex reasoning tasks, while simultaneously reducing run-to-run variability to near ARM levels, substantially narrowing the gap with strong ARM baselines; in most settings, even the best baseline runs remain below the worst run of our method.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Calibration of Engine-out NOx Models for Engine-to-Engine Transferability</title>
<link>https://arxiv.org/abs/2511.18178</link>
<guid>https://arxiv.org/abs/2511.18178</guid>
<content:encoded><![CDATA[
arXiv:2511.18178v1 Announce Type: new 
Abstract: Accurate prediction of engine-out NOx is essential for meeting stringent emissions regulations and optimizing engine performance. Traditional approaches rely on models trained on data from a small number of engines, which can be insufficient in generalizing across an entire population of engines due to sensor biases and variations in input conditions. In real world applications, these models require tuning or calibration to maintain acceptable error tolerance when applied to other engines. This highlights the need for models that can adapt with minimal adjustments to accommodate engine-to-engine variability and sensor discrepancies. While previous studies have explored machine learning methods for predicting engine-out NOx, these approaches often fail to generalize reliably across different engines and operating environments. To address these issues, we propose a Bayesian calibration framework that combines Gaussian processes with approximate Bayesian computation to infer and correct sensor biases. Starting with a pre-trained model developed using nominal engine data, our method identifies engine specific sensor biases and recalibrates predictions accordingly. By incorporating these inferred biases, our approach generates posterior predictive distributions for engine-out NOx on unseen test data, achieving high accuracy without retraining the model. Our results demonstrate that this transferable modeling approach significantly improves the accuracy of predictions compared to conventional non-adaptive GP models, effectively addressing engine-to-engine variability and improving model generalizability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2511.18181</link>
<guid>https://arxiv.org/abs/2511.18181</guid>
<content:encoded><![CDATA[
arXiv:2511.18181v1 Announce Type: new 
Abstract: This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Time Series Foundation Models with Speculative Decoding</title>
<link>https://arxiv.org/abs/2511.18191</link>
<guid>https://arxiv.org/abs/2511.18191</guid>
<content:encoded><![CDATA[
arXiv:2511.18191v1 Announce Type: new 
Abstract: Modern web applications--from real-time content recommendation and dynamic pricing to CDN optimization--increasingly rely on time-series forecasting to deliver personalized experiences to billions of users. Large-scale Transformer-based models have achieved state-of-the-art performance in time-series forecasting but suffer from high computational costs, limiting their deployment in latency-sensitive web applications. To address this challenge, we propose a general inference acceleration framework that adapts speculative decoding to autoregressive time-series models. Our approach employs a smaller "draft" model to propose future time-series patches, which are then verified in parallel by a larger "target" model, reducing the number of sequential forward passes required. We address key technical challenges in adapting this technique from discrete language tokens to continuous time-series distributions, including the design of acceptance criteria for multivariate Gaussian patches and practical variants that balance efficiency with accuracy. Through experiments on time series forecasting benchmarks relevant to web applications, we demonstrate significant inference speedups while maintaining competitive accuracy. The framework requires no architectural modifications to existing foundation models, making it immediately applicable to accelerate deployed time-series forecasting systems. Our implementation can be found at https://github.com/PranavSubbaraman/STRIDE
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Gaussian Process Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2511.18214</link>
<guid>https://arxiv.org/abs/2511.18214</guid>
<content:encoded><![CDATA[
arXiv:2511.18214v1 Announce Type: new 
Abstract: Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Conformal Prediction for Quantum Machine Learning</title>
<link>https://arxiv.org/abs/2511.18225</link>
<guid>https://arxiv.org/abs/2511.18225</guid>
<content:encoded><![CDATA[
arXiv:2511.18225v1 Announce Type: new 
Abstract: Quantum machine learning seeks to leverage quantum computers to improve upon classical machine learning algorithms. Currently, robust uncertainty quantification methods remain underdeveloped in the quantum domain, despite the critical need for reliable and trustworthy predictions. Recent work has introduced quantum conformal prediction, a framework that produces prediction sets that are guaranteed to contain the true outcome with user-specified probability. In this work, we formalise how the time-varying noise inherent in quantum processors can undermine conformal guarantees, even when calibration and test data are exchangeable. To address this challenge, we draw on Adaptive Conformal Inference, a method which maintains validity over time via repeated recalibration. We introduce Adaptive Quantum Conformal Prediction (AQCP), an algorithm which preserves asymptotic average coverage guarantees under arbitrary hardware noise conditions. Empirical studies on an IBM quantum processor demonstrate that AQCP achieves target coverage levels and exhibits greater stability than quantum conformal prediction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tail Distribution of Regret in Optimistic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.18247</link>
<guid>https://arxiv.org/abs/2511.18247</guid>
<content:encoded><![CDATA[
arXiv:2511.18247v1 Announce Type: new 
Abstract: We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\Pr(R_K \ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $\alpha$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj</title>
<link>https://arxiv.org/abs/2511.18248</link>
<guid>https://arxiv.org/abs/2511.18248</guid>
<content:encoded><![CDATA[
arXiv:2511.18248v1 Announce Type: new 
Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data</title>
<link>https://arxiv.org/abs/2511.18260</link>
<guid>https://arxiv.org/abs/2511.18260</guid>
<content:encoded><![CDATA[
arXiv:2511.18260v1 Announce Type: new 
Abstract: Parametric PDEs power modern simulation, design, and digital-twin systems, yet their many-query workloads still hinge on repeatedly solving large finite-element systems. Existing operator-learning approaches accelerate this process but often rely on opaque learned trunks, require extensive labeled data, or break down when boundary and source data vary independently from physical parameters. We introduce RB-DeepONet, a hybrid operator-learning framework that fuses reduced-basis (RB) numerical structure with the branch-trunk architecture of DeepONet. The trunk is fixed to a rigorously constructed RB space generated offline via Greedy selection, granting physical interpretability, stability, and certified error control. The branch network predicts only RB coefficients and is trained label-free using a projected variational residual that targets the RB-Galerkin solution. For problems with independently varying loads or boundary conditions, we develop boundary and source modal encodings that compress exogenous data into low-dimensional coordinates while preserving accuracy. Combined with affine or empirical interpolation decompositions, RB-DeepONet achieves a strict offline-online split: all heavy lifting occurs offline, and online evaluation scales only with the RB dimension rather than the full mesh. We provide convergence guarantees separating RB approximation error from statistical learning error, and numerical experiments show that RB-DeepONet attains accuracy competitive with intrusive RB-Galerkin, POD-DeepONet, and FEONet while using dramatically fewer trainable parameters and achieving significant speedups. This establishes RB-DeepONet as an efficient, stable, and interpretable operator learner for large-scale parametric PDEs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks</title>
<link>https://arxiv.org/abs/2511.18269</link>
<guid>https://arxiv.org/abs/2511.18269</guid>
<content:encoded><![CDATA[
arXiv:2511.18269v1 Announce Type: new 
Abstract: Ensuring that the right resource is available at the right location and time remains a major challenge for organizations operating large-scale logistics networks. The challenge comes from uneven demand patterns and the resulting asymmetric flow of resources across the arcs, which create persistent imbalances at the network nodes. Resource substitution among multiple, potentially composite and interchangeable, resource types is a cost-effective way to mitigate these imbalances. This leads to the resource substitution problem, which aims at determining the minimum number of resource substitutions from an initial assignment to minimize the overall network imbalance. In decentralized settings, achieving globally coordinated solutions becomes even more difficult. When substitution entails costs, effective prescriptions must also incorporate fairness and account for the individual preferences of schedulers. This paper presents a generic framework that combines operations research (OR) and machine learning (ML) to enable fair resource substitution in large networks. The OR component models and solves the resource substitution problem under a fairness lens. The ML component leverages historical data to learn schedulers' preferences, guide intelligent exploration of the decision space, and enhance computational efficiency by dynamically selecting the top-$\kappa$ resources for each arc in the network. The framework produces a portfolio of high-quality solutions from which schedulers can select satisfactory trade-offs. The proposed framework is applied to the network of one of the largest package delivery companies in the world, which serves as the primary motivation for this research. Computational results demonstrate substantial improvements over state-of-the-art methods, including an 80% reduction in model size and a 90% decrease in execution time while preserving optimality.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Tables to Signals: Revealing Spectral Adaptivity in TabPFN</title>
<link>https://arxiv.org/abs/2511.18278</link>
<guid>https://arxiv.org/abs/2511.18278</guid>
<content:encoded><![CDATA[
arXiv:2511.18278v1 Announce Type: new 
Abstract: Task-agnostic tabular foundation models such as TabPFN have achieved impressive performance on tabular learning tasks, yet the origins of their inductive biases remain poorly understood. In this work, we study TabPFN through the lens of signal reconstruction and provide the first frequency-based analysis of its in-context learning behavior. We show that TabPFN possesses a broader effective frequency capacity than standard ReLU-MLPs, even without hyperparameter tuning. Moreover, unlike MLPs whose spectra evolve primarily over training epochs, we find that TabPFN's spectral capacity adapts directly to the number of samples provided in-context, a phenomenon we term Spectral Adaptivity. We further demonstrate that positional encoding modulates TabPFN's frequency response, mirroring classical results in implicit neural representations. Finally, we show that these properties enable TabPFN to perform training-free and hyperparameter-free image denoising, illustrating its potential as a task-agnostic implicit model. Our analysis provides new insight into the structure and inductive biases of tabular foundation models and highlights their promise for broader signal reconstruction tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRIDENT: A Trimodal Cascade Generative Framework for Drug and RNA-Conditioned Cellular Morphology Synthesis</title>
<link>https://arxiv.org/abs/2511.18287</link>
<guid>https://arxiv.org/abs/2511.18287</guid>
<content:encoded><![CDATA[
arXiv:2511.18287v1 Announce Type: new 
Abstract: Accurately modeling the relationship between perturbations, transcriptional responses, and phenotypic changes is essential for building an AI Virtual Cell (AIVC). However, existing methods typically constrained to modeling direct associations, such as Perturbation $\rightarrow$ RNA or Perturbation $\rightarrow$ Morphology, overlook the crucial causal link from RNA to morphology. To bridge this gap, we propose TRIDENT, a cascade generative framework that synthesizes realistic cellular morphology by conditioning on both the perturbation and the corresponding gene expression profile. To train and evaluate this task, we construct MorphoGene, a new dataset pairing L1000 gene expression with Cell Painting images for 98 compounds. TRIDENT significantly outperforms state-of-the-art approaches, achieving up to 7-fold improvement with strong generalization to unseen compounds. In a case study on docetaxel, we validate that RNA-guided synthesis accurately produces the corresponding phenotype. An ablation study further confirms that this RNA conditioning is essential for the model's high fidelity. By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves us closer to a predictive virtual cell.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.18291</link>
<guid>https://arxiv.org/abs/2511.18291</guid>
<content:encoded><![CDATA[
arXiv:2511.18291v1 Announce Type: new 
Abstract: This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiDiffNet: A Multi-Objective Diffusion Framework for Generalizable Brain Decoding</title>
<link>https://arxiv.org/abs/2511.18294</link>
<guid>https://arxiv.org/abs/2511.18294</guid>
<content:encoded><![CDATA[
arXiv:2511.18294v1 Announce Type: new 
Abstract: Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce \textit{MultiDiffNet}, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GROOT: Graph Edge Re-growth and Partitioning for the Verification of Large Designs in Logic Synthesis</title>
<link>https://arxiv.org/abs/2511.18297</link>
<guid>https://arxiv.org/abs/2511.18297</guid>
<content:encoded><![CDATA[
arXiv:2511.18297v1 Announce Type: new 
Abstract: Traditional verification methods in chip design are highly time-consuming and computationally demanding, especially for large scale circuits. Graph neural networks (GNNs) have gained popularity as a potential solution to improve verification efficiency. However, there lacks a joint framework that considers all chip design domain knowledge, graph theory, and GPU kernel designs. To address this challenge, we introduce GROOT, an algorithm and system co-design framework that contains chip design domain knowledge and redesigned GPU kernels, to improve verification efficiency. More specifically, we create node features utilizing the circuit node types and the polarity of the connections between the input edges to nodes in And-Inverter Graphs (AIGs). We utilize a graph partitioning algorithm to divide the large graphs into smaller sub-graphs for fast GPU processing and develop a graph edge re-growth algorithm to recover verification accuracy. We carefully profile the EDA graph workloads and observe the uniqueness of their polarized distribution of high degree (HD) nodes and low degree (LD) nodes. We redesign two GPU kernels (HD-kernel and LD-kernel), to fit the EDA graph learning workload on a single GPU. We compare the results with state-of-the-art (SOTA) methods: GAMORA, a GNN-based approach, and the traditional ABC framework. Results show that GROOT achieves a significant reduction in memory footprint (59.38 %), with high accuracy (99.96%) for a very large CSA multiplier, i.e. 1,024 bits with a batch size of 16, which consists of 134,103,040 nodes and 268,140,544 edges. We compare GROOT with GPU-based GPU Kernel designs SOTAs such as cuSPARSE, MergePath-SpMM, and GNNAdvisor. We achieve up to 1.104x, 5.796x, and 1.469x improvement in runtime, respectively.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery</title>
<link>https://arxiv.org/abs/2511.18303</link>
<guid>https://arxiv.org/abs/2511.18303</guid>
<content:encoded><![CDATA[
arXiv:2511.18303v1 Announce Type: new 
Abstract: We present a long-horizon, hierarchical deep research (DR) agent designed for complex materials and device discovery problems that exceed the scope of existing Machine Learning (ML) surrogates and closed-source commercial agents. Our framework instantiates a locally deployable DR instance that integrates local retrieval-augmented generation with large language model reasoners, enhanced by a Deep Tree of Research (DToR) mechanism that adaptively expands and prunes research branches to maximize coverage, depth, and coherence. We systematically evaluate across 27 nanomaterials/device topics using a large language model (LLM)-as-judge rubric with five web-enabled state-of-the-art models as jurors. In addition, we conduct dry-lab validations on five representative tasks, where human experts use domain simulations (e.g., density functional theory, DFT) to verify whether DR-agent proposals are actionable. Results show that our DR agent produces reports with quality comparable to--and often exceeding--those of commercial systems (ChatGPT-5-thinking/o3/o4-mini-high Deep Research) at a substantially lower cost, while enabling on-prem integration with local data and tools.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiM-TS: Bridge the Gap between Selective State Space Models and Time Series for Generative Modeling</title>
<link>https://arxiv.org/abs/2511.18312</link>
<guid>https://arxiv.org/abs/2511.18312</guid>
<content:encoded><![CDATA[
arXiv:2511.18312v1 Announce Type: new 
Abstract: Time series data plays a pivotal role in a wide variety of fields but faces challenges related to privacy concerns. Recently, synthesizing data via diffusion models is viewed as a promising solution. However, existing methods still struggle to capture long-range temporal dependencies and complex channel interrelations. In this research, we aim to utilize the sequence modeling capability of a State Space Model called Mamba to extend its applicability to time series data generation. We firstly analyze the core limitations in State Space Model, namely the lack of consideration for correlated temporal lag and channel permutation. Building upon the insight, we propose Lag Fusion Mamba and Permutation Scanning Mamba, which enhance the model's ability to discern significant patterns during the denoising process. Theoretical analysis reveals that both variants exhibit a unified matrix multiplication framework with the original Mamba, offering a deeper understanding of our method. Finally, we integrate two variants and introduce Diffusion Mamba for Time Series (DiM-TS), a high-quality time series generation model that better preserves the temporal periodicity and inter-channel correlations. Comprehensive experiments on public datasets demonstrate the superiority of DiM-TS in generating realistic time series while preserving diverse properties of data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert</title>
<link>https://arxiv.org/abs/2511.18314</link>
<guid>https://arxiv.org/abs/2511.18314</guid>
<content:encoded><![CDATA[
arXiv:2511.18314v1 Announce Type: new 
Abstract: Multimodal Mixture-of-Experts (MoE) models offer a promising path toward scalable and efficient large vision-language systems. However, existing approaches rely on rigid routing strategies (typically activating a fixed number of experts per token) ignoring the inherent heterogeneity in semantic importance across modalities. This leads to suboptimal compute allocation, where redundant tokens consume as many resources as critical ones. To address this, we propose AnyExperts, a novel on-demand, budget-aware dynamic routing framework that allocates a variable total number of expert slots per token based on its semantic importance. Crucially, to prevent uncontrolled compute growth, the total slots per token are constrained within a fixed range, and each slot is filled by either a real expert or a virtual expert, with the virtual share capped at a small maximum (e.g., 20%). The model then adaptively balances the real-to-virtual ratio per token, assigning more real experts to semantically rich regions and relying more on virtual experts for redundant content. Evaluated across diverse tasks in visual understanding, audio understanding, and NLP understanding, AnyExperts improves performance under the same compute budget. Notably, on general image/video tasks, it achieves comparable accuracy with 40% fewer real expert activations; on text-dense tasks (OCR and NLP), it maintains performance while reducing real expert usage by 10%. These results demonstrate that fine-grained, importance-driven expert allocation significantly enhances both the efficiency and effectiveness of multimodal MoE models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations</title>
<link>https://arxiv.org/abs/2511.18331</link>
<guid>https://arxiv.org/abs/2511.18331</guid>
<content:encoded><![CDATA[
arXiv:2511.18331v1 Announce Type: new 
Abstract: For online ad-recommendation systems, processing complete user-ad-engagement histories is both computationally intensive and noise-prone. We introduce Dynamix, a scalable, personalized sequence exploration framework that optimizes event history processing using maximum relevance principles and self-supervised learning through Event Based Features (EBFs). Dynamix categorizes users-engagements at session and surface-levels by leveraging correlations between dwell-times and ad-conversion events. This enables targeted, event-level feature removal and selective feature boosting for certain user-segments, thereby yielding training and inference efficiency wins without sacrificing engaging ad-prediction accuracy. While, dynamic resource removal increases training and inference throughput by 1.15% and 1.8%, respectively, dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2% over baseline models. These results demonstrate that Dynamix achieves significant cost efficiency and performance improvements in online user-sequence based recommendation models. Self-supervised user-segmentation and resource exploration can further boost complex feature selection strategies while optimizing for workflow and compute resources.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clinician-in-the-Loop Smart Home System to Detect Urinary Tract Infection Flare-Ups via Uncertainty-Aware Decision Support</title>
<link>https://arxiv.org/abs/2511.18334</link>
<guid>https://arxiv.org/abs/2511.18334</guid>
<content:encoded><![CDATA[
arXiv:2511.18334v1 Announce Type: new 
Abstract: Urinary tract infection (UTI) flare-ups pose a significant health risk for older adults with chronic conditions. These infections often go unnoticed until they become severe, making early detection through innovative smart home technologies crucial. Traditional machine learning (ML) approaches relying on simple binary classification for UTI detection offer limited utility to nurses and practitioners as they lack insight into prediction uncertainty, hindering informed clinical decision-making. This paper presents a clinician-in-the-loop (CIL) smart home system that leverages ambient sensor data to extract meaningful behavioral markers, train robust predictive ML models, and calibrate them to enable uncertainty-aware decision support. The system incorporates a statistically valid uncertainty quantification method called Conformal-Calibrated Interval (CCI), which quantifies uncertainty and abstains from making predictions ("I don't know") when the ML model's confidence is low. Evaluated on real-world data from eight smart homes, our method outperforms baseline methods in recall and other classification metrics while maintaining the lowest abstention proportion and interval width. A survey of 42 nurses confirms that our system's outputs are valuable for guiding clinical decision-making, underscoring their practical utility in improving informed decisions and effectively managing UTIs and other condition flare-ups in older adults.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auxiliary Gene Learning: Spatial Gene Expression Estimation by Auxiliary Gene Selection</title>
<link>https://arxiv.org/abs/2511.18336</link>
<guid>https://arxiv.org/abs/2511.18336</guid>
<content:encoded><![CDATA[
arXiv:2511.18336v1 Announce Type: new 
Abstract: Spatial transcriptomics (ST) is a novel technology that enables the observation of gene expression at the resolution of individual spots within pathological tissues. ST quantifies the expression of tens of thousands of genes in a tissue section; however, heavy observational noise is often introduced during measurement. In prior studies, to ensure meaningful assessment, both training and evaluation have been restricted to only a small subset of highly variable genes, and genes outside this subset have also been excluded from the training process. However, since there are likely co-expression relationships between genes, low-expression genes may still contribute to the estimation of the evaluation target. In this paper, we propose $Auxiliary \ Gene \ Learning$ (AGL) that utilizes the benefit of the ignored genes by reformulating their expression estimation as auxiliary tasks and training them jointly with the primary tasks. To effectively leverage auxiliary genes, we must select a subset of auxiliary genes that positively influence the prediction of the target genes. However, this is a challenging optimization problem due to the vast number of possible combinations. To overcome this challenge, we propose Prior-Knowledge-Based Differentiable Top-$k$ Gene Selection via Bi-level Optimization (DkGSB), a method that ranks genes by leveraging prior knowledge and relaxes the combinatorial selection problem into a differentiable top-$k$ selection problem. The experiments confirm the effectiveness of incorporating auxiliary genes and show that the proposed method outperforms conventional auxiliary task learning approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking</title>
<link>https://arxiv.org/abs/2511.18394</link>
<guid>https://arxiv.org/abs/2511.18394</guid>
<content:encoded><![CDATA[
arXiv:2511.18394v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using Multi-View Conditional Information Bottleneck</title>
<link>https://arxiv.org/abs/2511.18404</link>
<guid>https://arxiv.org/abs/2511.18404</guid>
<content:encoded><![CDATA[
arXiv:2511.18404v1 Announce Type: new 
Abstract: Recent pre-training strategies for molecular graphs have attempted to use 2D and 3D molecular views as both inputs and self-supervised signals, primarily aligning graph-level representations. However, existing studies remain limited in addressing two main challenges of multi-view molecular learning: (1) discovering shared information between two views while diminishing view-specific information and (2) identifying and aligning important substructures, e.g., functional groups, which are crucial for enhancing cross-view consistency and model expressiveness. To solve these challenges, we propose a Multi-View Conditional Information Bottleneck framework, called MVCIB, for pre-training graph neural networks on 2D and 3D molecular structures in a self-supervised setting. Our idea is to discover the shared information while minimizing irrelevant features from each view under the MVCIB principle, which uses one view as a contextual condition to guide the representation learning of its counterpart. To enhance semantic and structural consistency across views, we utilize key substructures, e.g., functional groups and ego-networks, as anchors between the two views. Then, we propose a cross-attention mechanism that captures fine-grained correlations between the substructures to achieve subgraph alignment across views. Extensive experiments in four molecular domains demonstrated that MVCIB consistently outperforms baselines in both predictive performance and interpretability. Moreover, MVCIB achieved the 3d Weisfeiler-Lehman expressiveness power to distinguish not only non-isomorphic graphs but also different 3D geometries that share identical 2D connectivity, such as isomers.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems</title>
<link>https://arxiv.org/abs/2511.18417</link>
<guid>https://arxiv.org/abs/2511.18417</guid>
<content:encoded><![CDATA[
arXiv:2511.18417v1 Announce Type: new 
Abstract: We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels</title>
<link>https://arxiv.org/abs/2511.18457</link>
<guid>https://arxiv.org/abs/2511.18457</guid>
<content:encoded><![CDATA[
arXiv:2511.18457v1 Announce Type: new 
Abstract: We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.
  We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.18468</link>
<guid>https://arxiv.org/abs/2511.18468</guid>
<content:encoded><![CDATA[
arXiv:2511.18468v1 Announce Type: new 
Abstract: Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Mesh-Quantization for Neural PDE Solvers</title>
<link>https://arxiv.org/abs/2511.18474</link>
<guid>https://arxiv.org/abs/2511.18474</guid>
<content:encoded><![CDATA[
arXiv:2511.18474v1 Announce Type: new 
Abstract: Physical systems commonly exhibit spatially varying complexity, presenting a significant challenge for neural PDE solvers. While Graph Neural Networks can handle the irregular meshes required for complex geometries and boundary conditions, they still apply uniform computational effort across all nodes regardless of the underlying physics complexity. This leads to inefficient resource allocation where computationally simple regions receive the same treatment as complex phenomena. We address this challenge by introducing Adaptive Mesh Quantization: spatially adaptive quantization across mesh node, edge, and cluster features, dynamically adjusting the bit-width used by a quantized model. We propose an adaptive bit-width allocation strategy driven by a lightweight auxiliary model that identifies high-loss regions in the input mesh. This enables dynamic resource distribution in the main model, where regions of higher difficulty are allocated increased bit-width, optimizing computational resource utilization. We demonstrate our framework's effectiveness by integrating it with two state-of-the-art models, MP-PDE and GraphViT, to evaluate performance across multiple tasks: 2D Darcy flow, large-scale unsteady fluid dynamics in 2D, steady-state Navier-Stokes simulations in 3D, and a 2D hyper-elasticity problem. Our framework demonstrates consistent Pareto improvements over uniformly quantized baselines, yielding up to 50% improvements in performance at the same cost.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Personalized Content Adaptation through Matrix Factorization and Context-Aware Federated Learning</title>
<link>https://arxiv.org/abs/2511.18489</link>
<guid>https://arxiv.org/abs/2511.18489</guid>
<content:encoded><![CDATA[
arXiv:2511.18489v1 Announce Type: new 
Abstract: Our study presents a multifaceted approach to enhancing user interaction and content relevance in social media platforms through a federated learning framework. We introduce personalized LLM Federated Learning and Context-based Social Media models. In our framework, multiple client entities receive a foundational GPT model, which is fine-tuned using locally collected social media data while ensuring data privacy through federated aggregation. Key modules focus on categorizing user-generated content, computing user persona scores, and identifying relevant posts from friends networks. By integrating a sophisticated social engagement quantification method with matrix factorization techniques, our system delivers real-time personalized content suggestions tailored to individual preferences. Furthermore, an adaptive feedback loop, alongside a robust readability scoring algorithm, significantly enhances the quality and relevance of the content presented to users. This comprehensive solution not only addresses the challenges of content filtering and recommendation but also fosters a more engaging social media experience while safeguarding user privacy, setting a new standard for personalized interactions in digital platforms.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RRaPINNs: Residual Risk-Aware Physics Informed Neural Networks</title>
<link>https://arxiv.org/abs/2511.18515</link>
<guid>https://arxiv.org/abs/2511.18515</guid>
<content:encoded><![CDATA[
arXiv:2511.18515v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) typically minimize average residuals, which can conceal large, localized errors. We propose Residual Risk-Aware Physics-Informed Neural Networks PINNs (RRaPINNs), a single-network framework that optimizes tail-focused objectives using Conditional Value-at-Risk (CVaR), we also introduced a Mean-Excess (ME) surrogate penalty to directly control worst-case PDE residuals. This casts PINN training as risk-sensitive optimization and links it to chance-constrained formulations. The method is effective and simple to implement. Across several partial differential equations (PDEs) such as Burgers, Heat, Korteweg-de-Vries, and Poisson (including a Poisson interface problem with a source jump at x=0.5) equations, RRaPINNs reduce tail residuals while maintaining or improving mean errors compared to vanilla PINNs, Residual-Based Attention and its variant using convolution weighting; the ME surrogate yields smoother optimization than a direct CVaR hinge. The chance constraint reliability level $\alpha$ acts as a transparent knob trading bulk accuracy (lower $\alpha$ ) for stricter tail control (higher $\alpha$ ). We discuss the framework limitations, including memoryless sampling, global-only tail budgeting, and residual-centric risk, and outline remedies via persistent hard-point replay, local risk budgets, and multi-objective risk over BC/IC terms. RRaPINNs offer a practical path to reliability-aware scientific ML for both smooth and discontinuous PDEs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHIPS: Efficient CLIP Adaptation via Curvature-aware Hybrid Influence-based Data Selection</title>
<link>https://arxiv.org/abs/2511.18519</link>
<guid>https://arxiv.org/abs/2511.18519</guid>
<content:encoded><![CDATA[
arXiv:2511.18519v1 Announce Type: new 
Abstract: Adapting CLIP to vertical domains is typically approached by novel fine-tuning strategies or by continual pre-training (CPT) on large domain-specific datasets. Yet, data itself remains an underexplored factor in this process. We revisit this task from a data-centric perspective: Can effective data selection substitute for large-scale datasets in CPT? We introduce CHIPS (Curvature-aware Hybrid Influence in Projection Subspace), which assigns each image-text pair a utility score that integrates three complementary factors aligned with three goals: faithfulness via a curvature-aware, Newton-style alignment computed in CLIP's end-point subspace; scalability via an InfoNCE-aware curvature estimator with Johnson-Lindenstrauss (JL) sketching; and retention via a selection-aware relevance weight combined with learnability to balance target adaptation against general-domain preservation. We justify this design theoretically by proving a lower-bound guarantee on the proxy's correlation with full-parameter alignment and by characterizing the bias-variance trade-offs introduced by curvature mixing and JL sketching. We evaluate CHIPS empirically across various settings: 1) CHIPS attains state-of-the-art performance among selection baselines on 17 medical benchmarks, matches full-dataset CPT with 30% of the data, and outperforms half-dataset CPT using only 10%; 2) on 31 general-domain benchmarks, CHIPS yields the smallest performance drop under 10-30% data-retention budgets. Code, data, and checkpoints will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperspectral Variational Autoencoders for Joint Data Compression and Component Extraction</title>
<link>https://arxiv.org/abs/2511.18521</link>
<guid>https://arxiv.org/abs/2511.18521</guid>
<content:encoded><![CDATA[
arXiv:2511.18521v1 Announce Type: new 
Abstract: Geostationary hyperspectral satellites generate terabytes of data daily, creating critical challenges for storage, transmission, and distribution to the scientific community. We present a variational autoencoder (VAE) approach that achieves x514 compression of NASA's TEMPO satellite hyperspectral observations (1028 channels, 290-490nm) with reconstruction errors 1-2 orders of magnitude below the signal across all wavelengths. This dramatic data volume reduction enables efficient archival and sharing of satellite observations while preserving spectral fidelity. Beyond compression, we investigate to what extent atmospheric information is retained in the compressed latent space by training linear and nonlinear probes to extract Level-2 products (NO2, O3, HCHO, cloud fraction). Cloud fraction and total ozone achieve strong extraction performance (R^2 = 0.93 and 0.81 respectively), though these represent relatively straightforward retrievals given their distinct spectral signatures. In contrast, tropospheric trace gases pose genuine challenges for extraction (NO2 R^2 = 0.20, HCHO R^2 = 0.51) reflecting their weaker signals and complex atmospheric interactions. Critically, we find the VAE encodes atmospheric information in a semi-linear manner - nonlinear probes substantially outperform linear ones - and that explicit latent supervision during training provides minimal improvement, revealing fundamental encoding challenges for certain products. This work demonstrates that neural compression can dramatically reduce hyperspectral data volumes while preserving key atmospheric signals, addressing a critical bottleneck for next-generation Earth observation systems. Code - https://github.com/cfpark00/Hyperspectral-VAE
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimePre: Bridging Accuracy, Efficiency, and Stability in Probabilistic Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2511.18539</link>
<guid>https://arxiv.org/abs/2511.18539</guid>
<content:encoded><![CDATA[
arXiv:2511.18539v1 Announce Type: new 
Abstract: Probabilistic Time-Series Forecasting (PTSF) is critical for uncertainty-aware decision making, but existing generative models, such as diffusion-based approaches, are computationally prohibitive due to expensive iterative sampling. Non-sampling frameworks like Multiple Choice Learning (MCL) offer an efficient alternative, but suffer from severe training instability and hypothesis collapse, which has historically hindered their performance. This problem is dramatically exacerbated when attempting to combine them with modern, efficient MLP-based backbones. To resolve this fundamental incompatibility, we propose TimePre, a novel framework that successfully unifies the efficiency of MLP-based models with the distributional flexibility of the MCL paradigm. The core of our solution is Stabilized Instance Normalization (SIN), a novel normalization layer that explicitly remedies this incompatibility. SIN stabilizes the hybrid architecture by correcting channel-wise statistical shifts, definitively resolving the catastrophic hypothesis collapse. Extensive experiments on six benchmark datasets demonstrate that TimePre achieves new state-of-the-art accuracy on key probabilistic metrics. Critically, TimePre achieves inference speeds orders of magnitude faster than sampling-based models and, unlike prior MCL work, demonstrates stable performance scaling. It thus bridges the long-standing gap between accuracy, efficiency, and stability in probabilistic forecasting.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Search of Goodness: Large Scale Benchmarking of Goodness Functions for the Forward-Forward Algorithm</title>
<link>https://arxiv.org/abs/2511.18567</link>
<guid>https://arxiv.org/abs/2511.18567</guid>
<content:encoded><![CDATA[
arXiv:2511.18567v1 Announce Type: new 
Abstract: The Forward-Forward (FF) algorithm offers a biologically plausible alternative to backpropagation, enabling neural networks to learn through local updates. However, FF's efficacy relies heavily on the definition of "goodness", which is a scalar measure of neural activity. While current implementations predominantly utilize a simple sum-of-squares metric, it remains unclear if this default choice is optimal. To address this, we benchmarked 21 distinct goodness functions across four standard image datasets (MNIST, FashionMNIST, CIFAR-10, STL-10), evaluating classification accuracy, energy consumption, and carbon footprint. We found that certain alternative goodness functions inspired from various domains significantly outperform the standard baseline. Specifically, \texttt{game\_theoretic\_local} achieved 97.15\% accuracy on MNIST, \texttt{softmax\_energy\_margin\_local} reached 82.84\% on FashionMNIST, and \texttt{triplet\_margin\_local} attained 37.69\% on STL-10. Furthermore, we observed substantial variability in computational efficiency, highlighting a critical trade-off between predictive performance and environmental cost. These findings demonstrate that the goodness function is a pivotal hyperparameter in FF design. We release our code on \href{https://github.com/aryashah2k/In-Search-of-Goodness}{Github} for reference and reproducibility.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAMBA: Toward a Long-Context EEG Foundation Model via Spatial Embedding and Differential Mamba</title>
<link>https://arxiv.org/abs/2511.18571</link>
<guid>https://arxiv.org/abs/2511.18571</guid>
<content:encoded><![CDATA[
arXiv:2511.18571v1 Announce Type: new 
Abstract: Long-sequence electroencephalogram (EEG) modeling is essential for developing generalizable EEG representation models. This need arises from the high sampling rate of EEG data and the long recording durations required to capture extended neurological patterns in brain activity. Transformer-based models have shown promise in modeling short sequences of a few seconds; however, their quadratic complexity limits scalability to longer contexts. Moreover, variability in electrode montage across available datasets, along with inter-subject differences in brain signals, pose significant challenges to developing a generalizable and robust foundation model. We propose \textit{SAMBA}, a self-supervised learning framework with a Mamba-based U-shaped encoder-decoder architecture, which effectively captures long-range temporal dependencies and spatial variability in EEG data. Leveraging the inherent ability of Mamba in processing long context sizes, we introduce: (1) \textit{Temporal Semantic Random Masking} for semantic-level sequence reconstruction, (2) a \textit{Multi-Head Differential Mamba} module to suppress redundancy and emphasize salient temporal structures, and (3) a \textit{Spatial-Adaptive Input Embedding} that learns unified embeddings in a three-dimensional Euclidean space, enabling robustness across devices. Experiments on thirteen EEG datasets across diverse tasks, electrode configurations, and sequence durations demonstrate that SAMBA consistently outperforms state-of-the-art methods while maintaining low memory consumption and inference time. We also show the learned spatial weight maps from our embedding module align closely with task-relevant neurophysiological regions, demonstrating the learnability and interpretability of SAMBA. These results highlight SAMBA's scalability and practical potential as a foundation model for real-time brain-computer interface applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Myopia: Why Diffusion Models Fail at Structure</title>
<link>https://arxiv.org/abs/2511.18593</link>
<guid>https://arxiv.org/abs/2511.18593</guid>
<content:encoded><![CDATA[
arXiv:2511.18593v1 Announce Type: new 
Abstract: Graph Diffusion Models (GDMs) optimize for statistical likelihood, implicitly acting as \textbf{frequency filters} that favor abundant substructures over spectrally critical ones. We term this phenomenon \textbf{Generative Myopia}. In combinatorial tasks like graph sparsification, this leads to the catastrophic removal of ``rare bridges,'' edges that are structurally mandatory ($R_{\text{eff}} \approx 1$) but statistically scarce. We prove theoretically and empirically that this failure is driven by \textbf{Gradient Starvation}: the optimization landscape itself suppresses rare structural signals, rendering them unlearnable regardless of model capacity. To resolve this, we introduce \textbf{Spectrally-Weighted Diffusion}, which re-aligns the variational objective using Effective Resistance. We demonstrate that spectral priors can be amortized into the training phase with zero inference overhead. Our method eliminates myopia, matching the performance of an optimal Spectral Oracle and achieving \textbf{100\% connectivity} on adversarial benchmarks where standard diffusion fails completely (0\%).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning</title>
<link>https://arxiv.org/abs/2511.18611</link>
<guid>https://arxiv.org/abs/2511.18611</guid>
<content:encoded><![CDATA[
arXiv:2511.18611v1 Announce Type: new 
Abstract: Split learning emerges as a promising paradigm for collaborative distributed model training, akin to federated learning, by partitioning neural networks between clients and a server without raw data exchange. However, sequential split learning suffers from poor scalability, while parallel variants like parallel split learning and split federated learning often incur high server resource overhead due to model duplication and aggregation, and generally exhibit reduced model performance and convergence owing to factors like client drift and lag. To address these limitations, we introduce CycleSL, a novel aggregation-free split learning framework that enhances scalability and performance and can be seamlessly integrated with existing methods. Inspired by alternating block coordinate descent, CycleSL treats server-side training as an independent higher-level machine learning task, resampling client-extracted features (smashed data) to mitigate heterogeneity and drift. It then performs cyclical updates, namely optimizing the server model first, followed by client updates using the updated server for gradient computation. We integrate CycleSL into previous algorithms and benchmark them on five publicly available datasets with non-iid data distribution and partial client attendance. Our empirical findings highlight the effectiveness of CycleSL in enhancing model performance. Our source code is available at https://gitlab.lrz.de/hctl/CycleSL.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAN vs LSTM Performance in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.18613</link>
<guid>https://arxiv.org/abs/2511.18613</guid>
<content:encoded><![CDATA[
arXiv:2511.18613v1 Announce Type: new 
Abstract: This paper compares Kolmogorov-Arnold Networks (KAN) and Long Short-Term Memory networks (LSTM) for forecasting non-deterministic stock price data, evaluating predictive accuracy versus interpretability trade-offs using Root Mean Square Error (RMSE).LSTM demonstrates substantial superiority across all tested prediction horizons, confirming their established effectiveness for sequential data modelling. Standard KAN, while offering theoretical interpretability through the Kolmogorov-Arnold representation theorem, exhibits significantly higher error rates and limited practical applicability for time series forecasting. The results confirm LSTM dominance in accuracy-critical time series applications while identifying computational efficiency as KANs' primary advantage in resource-constrained scenarios where accuracy requirements are less stringent. The findings support LSTM adoption for practical financial forecasting while suggesting that continued research into specialised KAN architectures may yield future improvements.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors</title>
<link>https://arxiv.org/abs/2511.18615</link>
<guid>https://arxiv.org/abs/2511.18615</guid>
<content:encoded><![CDATA[
arXiv:2511.18615v1 Announce Type: new 
Abstract: Label shift, a prevalent challenge in supervised learning, arises when the class prior distribution of test data differs from that of training data, leading to significant degradation in classifier performance. To accurately estimate the test priors and enhance classification accuracy, we propose a Bayesian framework for label shift estimation, termed Full Maximum A Posterior Label Shift (FMAPLS), along with its online version, online-FMAPLS. Leveraging batch and online Expectation-Maximization (EM) algorithms, these methods jointly and dynamically optimize Dirichlet hyperparameters $\boldsymbol{\alpha}$ and class priors $\boldsymbol{\pi}$, thereby overcoming the rigid constraints of the existing Maximum A Posterior Label Shift (MAPLS) approach. Moreover, we introduce a linear surrogate function (LSF) to replace gradient-based hyperparameter updates, yielding closed-form solutions that reduce computational complexity while retaining asymptotic equivalence. The online variant substitutes the batch E-step with a stochastic approximation, enabling real-time adaptation to streaming data. Furthermore, our theoretical analysis reveals a fundamental trade-off between online convergence rate and estimation accuracy. Extensive experiments on CIFAR100 and ImageNet datasets under shuffled long-tail and Dirichlet test priors demonstrate that FMAPLS and online-FMAPLS respectively achieve up to 40% and 12% lower KL divergence and substantial improvements in post-shift accuracy over state-of-the-art baselines, particularly under severe class imbalance and distributional uncertainty. These results confirm the robustness, scalability, and suitability of the proposed methods for large-scale and dynamic learning scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Majority of the Bests: Improving Best-of-N via Bootstrapping</title>
<link>https://arxiv.org/abs/2511.18630</link>
<guid>https://arxiv.org/abs/2511.18630</guid>
<content:encoded><![CDATA[
arXiv:2511.18630v1 Announce Type: new 
Abstract: Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOS: A Large-Scale Temporal Graph Benchmark for Scientific Interdisciplinary Link Prediction</title>
<link>https://arxiv.org/abs/2511.18631</link>
<guid>https://arxiv.org/abs/2511.18631</guid>
<content:encoded><![CDATA[
arXiv:2511.18631v1 Announce Type: new 
Abstract: Interdisciplinary scientific breakthroughs mostly emerge unexpectedly, and forecasting the formation of novel research fields remains a major challenge. We introduce FOS (Future Of Science), a comprehensive time-aware graph-based benchmark that reconstructs annual co-occurrence graphs of 65,027 research sub-fields (spanning 19 general domains) over the period 1827-2024. In these graphs, edges denote the co-occurrence of two fields in a single publication and are timestamped with the corresponding publication year. Nodes are enriched with semantic embeddings, and edges are characterized by temporal and topological descriptors. We formulate the prediction of new field-pair linkages as a temporal link-prediction task, emphasizing the "first-time" connections that signify pioneering interdisciplinary directions. Through extensive experiments, we evaluate a suite of state-of-the-art temporal graph architectures under multiple negative-sampling regimes and show that (i) embedding long-form textual descriptions of fields significantly boosts prediction accuracy, and (ii) distinct model classes excel under different evaluation settings. Case analyses show that top-ranked link predictions on FOS align with field pairings that emerge in subsequent years of academic publications. We publicly release FOS, along with its temporal data splits and evaluation code, to establish a reproducible benchmark for advancing research in predicting scientific frontiers.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Locally Deployable Virtual Doctor: LLM Based Human Interface for Automated Anamnesis and Database Conversion</title>
<link>https://arxiv.org/abs/2511.18632</link>
<guid>https://arxiv.org/abs/2511.18632</guid>
<content:encoded><![CDATA[
arXiv:2511.18632v1 Announce Type: new 
Abstract: Recent advances in large language models made it possible to achieve high conversational performance with substantially reduced computational demands, enabling practical on-site deployment in clinical environments. Such progress allows for local integration of AI systems that uphold strict data protection and patient privacy requirements, yet their secure implementation in medicine necessitates careful consideration of ethical, regulatory, and technical constraints.
  In this study, we introduce MedChat, a locally deployable virtual physician framework that integrates an LLM-based medical chatbot with a diffusion-driven avatar for automated and structured anamnesis. The chatbot was fine-tuned using a hybrid corpus of real and synthetically generated medical dialogues, while model efficiency was optimized via Low-Rank Adaptation. A secure and isolated database interface was implemented to ensure complete separation between patient data and the inference process. The avatar component was realized through a conditional diffusion model operating in latent space, trained on researcher video datasets and synchronized with mel-frequency audio features for realistic speech and facial animation.
  Unlike existing cloud-based systems, this work demonstrates the feasibility of a fully offline, locally deployable LLM-diffusion framework for clinical anamnesis. The autoencoder and diffusion networks exhibited smooth convergence, and MedChat achieved stable fine-tuning with strong generalization to unseen data. The proposed system thus provides a privacy-preserving, resource-efficient foundation for AI-assisted clinical anamnesis, also in low-cost settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost</title>
<link>https://arxiv.org/abs/2511.18643</link>
<guid>https://arxiv.org/abs/2511.18643</guid>
<content:encoded><![CDATA[
arXiv:2511.18643v1 Announce Type: new 
Abstract: The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive experiments show that Dynamic Channel-wise Precision Boost -- which ranks Key-cache channels by sensitivity and keeps only a small fraction at higher precision -- maintains near-zero loss in accuracy drop while approaching 2-bit memory. The main challenge is handling dynamic 4-bit channel boosts while keeping the page layout coalesced and the dequantization uniform, with no scattered reads or hard-coded masks. Kitty addresses these issues by decompose each mixed-precision Key page into two tensors with unified 2-bit precision. Based on this, Kitty provides a page-centric KV layout, Triton-compatible page dequantization kernels, and a lightweight runtime pipeline that preserves coalescing and avoids divergence. Across seven tasks and two model families (Qwen3, LLaMA3), Kitty cuts KV memory by nearly 8x with negligible accuracy loss, enabling up to 8x larger batches and 2.1x-4.1x higher throughput under the same memory budget. We release the full implementation of Kitty at https://github.com/Summer-Summer/Kitty.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subtract the Corruption: Training-Data-Free Corrective Machine Unlearning using Task Arithmetic</title>
<link>https://arxiv.org/abs/2511.18660</link>
<guid>https://arxiv.org/abs/2511.18660</guid>
<content:encoded><![CDATA[
arXiv:2511.18660v1 Announce Type: new 
Abstract: Corrupted training data are ubiquitous. Corrective Machine Unlearning (CMU) seeks to remove the influence of such corruption post-training. Prior CMU typically assumes access to identified corrupted training samples (a ``forget set''). However, in many real-world scenarios the training data are no longer accessible. We formalize \emph{source-free} CMU, where the original training data are unavailable and, consequently, no forget set of identified corrupted training samples can be specified. Instead, we assume a small proxy (surrogate) set of corrupted samples that reflect the suspected corruption type without needing to be the original training samples. In this stricter setting, methods relying on forget set are ineffective or narrow in scope. We introduce \textit{Corrective Unlearning in Task Space} (CUTS), a lightweight weight space correction method guided by the proxy set using task arithmetic principles. CUTS treats the clean and the corruption signal as distinct tasks. Specifically, we briefly fine-tune the corrupted model on the proxy to amplify the corruption mechanism in the weight space, compute the difference between the corrupted and fine-tuned weights as a proxy task vector, and subtract a calibrated multiple of this vector to cancel the corruption. Without access to clean data or a forget set, CUTS recovers a large fraction of the lost utility under label noise and, for backdoor triggers, nearly eliminates the attack with minimal damage to utility, outperforming state-of-the-art specialized CMU methods in source-free setting.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers</title>
<link>https://arxiv.org/abs/2511.18670</link>
<guid>https://arxiv.org/abs/2511.18670</guid>
<content:encoded><![CDATA[
arXiv:2511.18670v1 Announce Type: new 
Abstract: Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition</title>
<link>https://arxiv.org/abs/2511.18671</link>
<guid>https://arxiv.org/abs/2511.18671</guid>
<content:encoded><![CDATA[
arXiv:2511.18671v1 Announce Type: new 
Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuantKAN: A Unified Quantization Framework for Kolmogorov Arnold Networks</title>
<link>https://arxiv.org/abs/2511.18689</link>
<guid>https://arxiv.org/abs/2511.18689</guid>
<content:encoded><![CDATA[
arXiv:2511.18689v1 Announce Type: new 
Abstract: Kolmogorov Arnold Networks (KANs) represent a new class of neural architectures that replace conventional linear transformations and node-based nonlinearities with spline-based function approximations distributed along network edges. Although KANs offer strong expressivity and interpretability, their heterogeneous spline and base branch parameters hinder efficient quantization, which remains unexamined compared to CNNs and Transformers. In this paper, we present QuantKAN, a unified framework for quantizing KANs across both quantization aware training (QAT) and post-training quantization (PTQ) regimes. QuantKAN extends modern quantization algorithms, such as LSQ, LSQ+, PACT, DoReFa, QIL, GPTQ, BRECQ, AdaRound, AWQ, and HAWQ-V2, to spline based layers with branch-specific quantizers for base, spline, and activation components. Through extensive experiments on MNIST, CIFAR 10, and CIFAR 100 across multiple KAN variants (EfficientKAN, FastKAN, PyKAN, and KAGN), we establish the first systematic benchmarks for low-bit spline networks. Our results show that KANs, particularly deeper KAGN variants, are compatible with low-bit quantization but exhibit strong method architecture interactions: LSQ, LSQ+, and PACT preserve near full precision accuracy at 4 bit for shallow KAN MLP and ConvNet models, while DoReFa provides the most stable behavior for deeper KAGN under aggressive low-bit settings. For PTQ, GPTQ and Uniform consistently deliver the strongest overall performance across datasets, with BRECQ highly competitive on simpler regimes such as MNIST. Our proposed QuantKAN framework thus unifies spline learning and quantization, and provides practical tools and guidelines for efficiently deploying KANs in real-world, resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking</title>
<link>https://arxiv.org/abs/2511.18692</link>
<guid>https://arxiv.org/abs/2511.18692</guid>
<content:encoded><![CDATA[
arXiv:2511.18692v1 Announce Type: new 
Abstract: Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction</title>
<link>https://arxiv.org/abs/2511.18716</link>
<guid>https://arxiv.org/abs/2511.18716</guid>
<content:encoded><![CDATA[
arXiv:2511.18716v1 Announce Type: new 
Abstract: Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM</title>
<link>https://arxiv.org/abs/2511.18721</link>
<guid>https://arxiv.org/abs/2511.18721</guid>
<content:encoded><![CDATA[
arXiv:2511.18721v1 Announce Type: new 
Abstract: The SmoothLLM defense provides a certification guarantee against jailbreaking attacks, but it relies on a strict `k-unstable' assumption that rarely holds in practice. This strong assumption can limit the trustworthiness of the provided safety certificate. In this work, we address this limitation by introducing a more realistic probabilistic framework, `(k, $\varepsilon$)-unstable,' to certify defenses against diverse jailbreaking attacks, from gradient-based (GCG) to semantic (PAIR). We derive a new, data-informed lower bound on SmoothLLM's defense probability by incorporating empirical models of attack success, providing a more trustworthy and practical safety certificate. By introducing the notion of (k, $\varepsilon$)-unstable, our framework provides practitioners with actionable safety guarantees, enabling them to set certification thresholds that better reflect the real-world behavior of LLMs. Ultimately, this work contributes a practical and theoretically-grounded mechanism to make LLMs more resistant to the exploitation of their safety alignments, a critical challenge in secure AI deployment.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LogSyn: A Few-Shot LLM Framework for Structured Insight Extraction from Unstructured General Aviation Maintenance Logs</title>
<link>https://arxiv.org/abs/2511.18727</link>
<guid>https://arxiv.org/abs/2511.18727</guid>
<content:encoded><![CDATA[
arXiv:2511.18727v1 Announce Type: new 
Abstract: Aircraft maintenance logs hold valuable safety data but remain underused due to their unstructured text format. This paper introduces LogSyn, a framework that uses Large Language Models (LLMs) to convert these logs into structured, machine-readable data. Using few-shot in-context learning on 6,169 records, LogSyn performs Controlled Abstraction Generation (CAG) to summarize problem-resolution narratives and classify events within a detailed hierarchical ontology. The framework identifies key failure patterns, offering a scalable method for semantic structuring and actionable insight extraction from maintenance logs. This work provides a practical path to improve maintenance workflows and predictive analytics in aviation and related industries.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Self-Healing Material Systems</title>
<link>https://arxiv.org/abs/2511.18728</link>
<guid>https://arxiv.org/abs/2511.18728</guid>
<content:encoded><![CDATA[
arXiv:2511.18728v1 Announce Type: new 
Abstract: The transition to autonomous material systems necessitates adaptive control methodologies to maximize structural longevity. This study frames the self-healing process as a Reinforcement Learning (RL) problem within a Markov Decision Process (MDP), enabling agents to autonomously derive optimal policies that efficiently balance structural integrity maintenance against finite resource consumption. A comparative evaluation of discrete-action (Q-learning, DQN) and continuous-action (TD3) agents in a stochastic simulation environment revealed that RL controllers significantly outperform heuristic baselines, achieving near-complete material recovery. Crucially, the TD3 agent utilizing continuous dosage control demonstrated superior convergence speed and stability, underscoring the necessity of fine-grained, proportional actuation in dynamic self-healing applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large-Scale In-Game Outcome Forecasting for Match, Team and Players in Football using an Axial Transformer Neural Network</title>
<link>https://arxiv.org/abs/2511.18730</link>
<guid>https://arxiv.org/abs/2511.18730</guid>
<content:encoded><![CDATA[
arXiv:2511.18730v1 Announce Type: new 
Abstract: Football (soccer) is a sport that is characterised by complex game play, where players perform a variety of actions, such as passes, shots, tackles, fouls, in order to score goals, and ultimately win matches. Accurately forecasting the total number of each action that each player will complete during a match is desirable for a variety of applications, including tactical decision-making, sports betting, and for television broadcast commentary and analysis. Such predictions must consider the game state, the ability and skill of the players in both teams, the interactions between the players, and the temporal dynamics of the game as it develops. In this paper, we present a transformer-based neural network that jointly and recurrently predicts the expected totals for thirteen individual actions at multiple time-steps during the match, and where predictions are made for each individual player, each team and at the game-level. The neural network is based on an \emph{axial transformer} that efficiently captures the temporal dynamics as the game progresses, and the interactions between the players at each time-step. We present a novel axial transformer design that we show is equivalent to a regular sequential transformer, and the design performs well experimentally. We show empirically that the model can make consistent and reliable predictions, and efficiently makes $\sim$75,000 live predictions at low latency for each game.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting</title>
<link>https://arxiv.org/abs/2511.18732</link>
<guid>https://arxiv.org/abs/2511.18732</guid>
<content:encoded><![CDATA[
arXiv:2511.18732v1 Announce Type: new 
Abstract: Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling Control for Imbalanced Calibration in Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2511.18773</link>
<guid>https://arxiv.org/abs/2511.18773</guid>
<content:encoded><![CDATA[
arXiv:2511.18773v1 Announce Type: new 
Abstract: Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAOT: An Enhanced Locality-Aware Spectral Transformer for Solving PDEs</title>
<link>https://arxiv.org/abs/2511.18777</link>
<guid>https://arxiv.org/abs/2511.18777</guid>
<content:encoded><![CDATA[
arXiv:2511.18777v1 Announce Type: new 
Abstract: Neural operators have shown great potential in solving a family of Partial Differential Equations (PDEs) by modeling the mappings between input and output functions. Fourier Neural Operator (FNO) implements global convolutions via parameterizing the integral operators in Fourier space. However, it often results in over-smoothing solutions and fails to capture local details and high-frequency components. To address these limitations, we investigate incorporating the spatial-frequency localization property of Wavelet transforms into the Transformer architecture. We propose a novel Wavelet Attention (WA) module with linear computational complexity to efficiently learn locality-aware features. Building upon WA, we further develop the Spectral Attention Operator Transformer (SAOT), a hybrid spectral Transformer framework that integrates WA's localized focus with the global receptive field of Fourier-based Attention (FA) through a gated fusion block. Experimental results demonstrate that WA significantly mitigates the limitations of FA and outperforms existing Wavelet-based neural operators by a large margin. By integrating the locality-aware and global spectral representations, SAOT achieves state-of-the-art performance on six operator learning benchmarks and exhibits strong discretization-invariant ability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs</title>
<link>https://arxiv.org/abs/2511.18783</link>
<guid>https://arxiv.org/abs/2511.18783</guid>
<content:encoded><![CDATA[
arXiv:2511.18783v1 Announce Type: new 
Abstract: Hypergraphs, as a generalization of traditional graphs, naturally capture high-order relationships. In recent years, hypergraph neural networks (HNNs) have been widely used to capture complex high-order relationships. However, most existing hypergraph neural network methods inherently rely on the homophily assumption, which often does not hold in real-world scenarios that exhibit significant heterophilic structures. To address this limitation, we propose \textbf{HONOR}, a novel unsupervised \textbf{H}ypergraph c\textbf{ON}trastive learning framework suitable for both hom\textbf{O}philic and hete\textbf{R}ophilic hypergraphs. Specifically, HONOR explicitly models the heterophilic relationships between hyperedges and nodes through two complementary mechanisms: a prompt-based hyperedge feature construction strategy that maintains global semantic consistency while suppressing local noise, and an adaptive attention aggregation module that dynamically captures the diverse local contributions of nodes to hyperedges. Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns, yielding more discriminative and robust node and hyperedge representations. Theoretically, we demonstrate the superior generalization ability and robustness of HONOR. Empirically, extensive experiments further validate that HONOR consistently outperforms state-of-the-art baselines under both homophilic and heterophilic datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Doubly Wild Refitting: Model-Free Evaluation of High Dimensional Black-Box Predictions under Convex Losses</title>
<link>https://arxiv.org/abs/2511.18789</link>
<guid>https://arxiv.org/abs/2511.18789</guid>
<content:encoded><![CDATA[
arXiv:2511.18789v1 Announce Type: new 
Abstract: We study the problem of excess risk evaluation for empirical risk minimization (ERM) under general convex loss functions. Our contribution is an efficient refitting procedure that computes the excess risk and provides high-probability upper bounds under the fixed-design setting. Assuming only black-box access to the training algorithm and a single dataset, we begin by generating two sets of artificially modified pseudo-outcomes termed wild response, created by stochastically perturbing the gradient vectors with carefully chosen scaling. Using these two pseudo-labeled datasets, we then refit the black-box procedure twice to obtain two corresponding wild predictors. Finally, leveraging the original predictor, the two wild predictors, and the constructed wild responses, we derive an efficient excess risk upper bound. A key feature of our analysis is that it requires no prior knowledge of the complexity of the underlying function class. As a result, the method is essentially model-free and holds significant promise for theoretically evaluating modern opaque machine learning system--such as deep nerral networks and generative model--where traditional capacity-based learning theory becomes infeasible due to the extreme complexity of the hypothesis class.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models</title>
<link>https://arxiv.org/abs/2511.18829</link>
<guid>https://arxiv.org/abs/2511.18829</guid>
<content:encoded><![CDATA[
arXiv:2511.18829v1 Announce Type: new 
Abstract: Heart rate estimation from photoplethysmography (PPG) signals generated by wearable devices such as smartwatches and fitness trackers has significant implications for the health and well-being of individuals. Although prior work has demonstrated deep learning models with strong performance in the heart rate estimation task, in order to deploy these models on wearable devices, these models must also adhere to strict memory and latency constraints. In this work, we explore and characterize how large pre-trained PPG models may be distilled to smaller models appropriate for real-time inference on the edge. We evaluate four distillation strategies through comprehensive sweeps of teacher and student model capacities: (1) hard distillation, (2) soft distillation, (3) decoupled knowledge distillation (DKD), and (4) feature distillation. We present a characterization of the resulting scaling laws describing the relationship between model size and performance. This early investigation lays the groundwork for practical and predictable methods for building edge-deployable models for physiological sensing.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Duration Pseudo-Embeddings in Multilevel LSTM and GCN Hypermodels for Outcome-Oriented PPM</title>
<link>https://arxiv.org/abs/2511.18830</link>
<guid>https://arxiv.org/abs/2511.18830</guid>
<content:encoded><![CDATA[
arXiv:2511.18830v1 Announce Type: new 
Abstract: Existing deep learning models for Predictive Process Monitoring (PPM) struggle with temporal irregularities, particularly stochastic event durations and overlapping timestamps, limiting their adaptability across heterogeneous datasets. We propose a dual input neural network strategy that separates event and sequence attributes, using a duration-aware pseudo-embedding matrix to transform temporal importance into compact, learnable representations. This design is implemented across two baseline families: B-LSTM and B-GCN, and their duration-aware variants D-LSTM and D-GCN. All models incorporate self-tuned hypermodels for adaptive architecture selection. Experiments on balanced and imbalanced outcome prediction tasks show that duration pseudo-embedding inputs consistently improve generalization, reduce model complexity, and enhance interpretability. Our results demonstrate the benefits of explicit temporal encoding and provide a flexible design for robust, real-world PPM applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-ML Graph Neural Network Hypermodels for Outcome Prediction in Event-Sequence Data</title>
<link>https://arxiv.org/abs/2511.18835</link>
<guid>https://arxiv.org/abs/2511.18835</guid>
<content:encoded><![CDATA[
arXiv:2511.18835v1 Announce Type: new 
Abstract: This paper introduces HGNN(O), an AutoML GNN hypermodel framework for outcome prediction on event-sequence data. Building on our earlier work on graph convolutional network hypermodels, HGNN(O) extends four architectures-One Level, Two Level, Two Level Pseudo Embedding, and Two Level Embedding-across six canonical GNN operators. A self-tuning mechanism based on Bayesian optimization with pruning and early stopping enables efficient adaptation over architectures and hyperparameters without manual configuration. Empirical evaluation on both balanced and imbalanced event logs shows that HGNN(O) achieves accuracy exceeding 0.98 on the Traffic Fines dataset and weighted F1 scores up to 0.86 on the Patients dataset without explicit imbalance handling. These results demonstrate that the proposed AutoML-GNN approach provides a robust and generalizable benchmark for outcome prediction in complex event-sequence data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated style aware transformer aggregation of representations</title>
<link>https://arxiv.org/abs/2511.18841</link>
<guid>https://arxiv.org/abs/2511.18841</guid>
<content:encoded><![CDATA[
arXiv:2511.18841v1 Announce Type: new 
Abstract: Personalized Federated Learning (PFL) faces persistent challenges, including domain heterogeneity from diverse client data, data imbalance due to skewed participation, and strict communication constraints. Traditional federated learning often lacks personalization, as a single global model cannot capture client-specific characteristics, leading to biased predictions and poor generalization, especially for clients with highly divergent data distributions.
  To address these issues, we propose FedSTAR, a style-aware federated learning framework that disentangles client-specific style factors from shared content representations. FedSTAR aggregates class-wise prototypes using a Transformer-based attention mechanism, allowing the server to adaptively weight client contributions while preserving personalization.
  Furthermore, by exchanging compact prototypes and style vectors instead of full model parameters, FedSTAR significantly reduces communication overhead. Experimental results demonstrate that combining content-style disentanglement with attention-driven prototype aggregation improves personalization and robustness in heterogeneous environments without increasing communication cost.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveTuner: Comprehensive Wavelet Subband Tuning for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.18846</link>
<guid>https://arxiv.org/abs/2511.18846</guid>
<content:encoded><![CDATA[
arXiv:2511.18846v1 Announce Type: new 
Abstract: Due to the inherent complexity, temporal patterns in real-world time series often evolve across multiple intertwined scales, including long-term periodicity, short-term fluctuations, and abrupt regime shifts. While existing literature has designed many sophisticated decomposition approaches based on the time or frequency domain to partition trend-seasonality components and high-low frequency components, an alternative line of approaches based on the wavelet domain has been proposed to provide a unified multi-resolution representation with precise time-frequency localization. However, most wavelet-based methods suffer from a persistent bias toward recursively decomposing only low-frequency components, severely underutilizing subtle yet informative high-frequency components that are pivotal for precise time series forecasting. To address this problem, we propose WaveTuner, a Wavelet decomposition framework empowered by full-spectrum subband Tuning for time series forecasting. Concretely, WaveTuner comprises two key modules: (i) Adaptive Wavelet Refinement module, that transforms time series into time-frequency coefficients, utilizes an adaptive router to dynamically assign subband weights, and generates subband-specific embeddings to support refinement; and (ii) Multi-Branch Specialization module, that employs multiple functional branches, each instantiated as a flexible Kolmogorov-Arnold Network (KAN) with a distinct functional order to model a specific spectral subband. Equipped with these modules, WaveTuner comprehensively tunes global trends and local variations within a unified time-frequency framework. Extensive experiments on eight real-world datasets demonstrate WaveTuner achieves state-of-the-art forecasting performance in time series forecasting.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust and Generalizable GNN Fine-Tuning via Uncertainty-aware Adapter Learning</title>
<link>https://arxiv.org/abs/2511.18859</link>
<guid>https://arxiv.org/abs/2511.18859</guid>
<content:encoded><![CDATA[
arXiv:2511.18859v1 Announce Type: new 
Abstract: Recently, fine-tuning large-scale pre-trained GNNs has yielded remarkable attention in adapting pre-trained GNN models for downstream graph learning tasks. One representative fine-tuning method is to exploit adapter (termed AdapterGNN) which aims to 'augment' the pre-trained model by inserting a lightweight module to make the 'augmented' model better adapt to the downstream tasks. However, graph data may contain various types of noise in downstream tasks, such as noisy edges and ambiguous node attributes. Existing AdapterGNNs are often prone to graph noise and exhibit limited generalizability. How to enhance the robustness and generalization ability of GNNs' fine tuning remains an open problem. In this paper, we show that the above problem can be well addressed by integrating uncertainty learning into the GNN adapter. We propose the Uncertainty-aware Adapter (UAdapterGNN) that fortifies pre-trained GNN models against noisy graph data in the fine-tuning process. Specifically, in contrast to regular AdapterGNN, our UAdapterGNN exploits Gaussian probabilistic adapter to augment the pre-trained GNN model. In this way, when the graph contains various noises,our method can automatically absorb the effects of changes in the variances of the Gaussian distribution, thereby significantly enhancing the model's robustness. Also, UAdapterGNN can further improve the generalization ability of the model on the downstream tasks. Extensive experiments on several benchmarks demonstrate the effectiveness, robustness and high generalization ability of the proposed UAdapterGNN method.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit</title>
<link>https://arxiv.org/abs/2511.18868</link>
<guid>https://arxiv.org/abs/2511.18868</guid>
<content:encoded><![CDATA[
arXiv:2511.18868v1 Announce Type: new 
Abstract: High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.18871</link>
<guid>https://arxiv.org/abs/2511.18871</guid>
<content:encoded><![CDATA[
arXiv:2511.18871v1 Announce Type: new 
Abstract: Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hi-SAFE: Hierarchical Secure Aggregation for Lightweight Federated Learning</title>
<link>https://arxiv.org/abs/2511.18887</link>
<guid>https://arxiv.org/abs/2511.18887</guid>
<content:encoded><![CDATA[
arXiv:2511.18887v1 Announce Type: new 
Abstract: Federated learning (FL) faces challenges in ensuring both privacy and communication efficiency, particularly in resource-constrained environments such as Internet of Things (IoT) and edge networks. While sign-based methods, such as sign stochastic gradient descent with majority voting (SIGNSGD-MV), offer substantial bandwidth savings, they remain vulnerable to inference attacks due to exposure of gradient signs. Existing secure aggregation techniques are either incompatible with sign-based methods or incur prohibitive overhead. To address these limitations, we propose Hi-SAFE, a lightweight and cryptographically secure aggregation framework for sign-based FL. Our core contribution is the construction of efficient majority vote polynomials for SIGNSGD-MV, derived from Fermat's Little Theorem. This formulation represents the majority vote as a low-degree polynomial over a finite field, enabling secure evaluation that hides intermediate values and reveals only the final result. We further introduce a hierarchical subgrouping strategy that ensures constant multiplicative depth and bounded per-user complexity, independent of the number of users n.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models</title>
<link>https://arxiv.org/abs/2511.18890</link>
<guid>https://arxiv.org/abs/2511.18890</guid>
<content:encoded><![CDATA[
arXiv:2511.18890v1 Announce Type: new 
Abstract: Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL</title>
<link>https://arxiv.org/abs/2511.18902</link>
<guid>https://arxiv.org/abs/2511.18902</guid>
<content:encoded><![CDATA[
arXiv:2511.18902v1 Announce Type: new 
Abstract: Group-based policy optimization methods like GRPO and GSPO have become standard for training multimodal models, leveraging group-wise rollouts and relative advantage estimation. However, they suffer from a critical \emph{gradient vanishing} problem when all responses within a group receive identical rewards, causing advantage estimates to collapse and training signals to diminish. Existing attempts to mitigate this issue fall into two paradigms: filtering-based and sampling-based methods. Filtering-based methods first generate rollouts broadly and then retroactively filter out uninformative groups, leading to substantial computational overhead. Sampling-based methods proactively select effective samples before rollout but rely on static criteria or prior dataset knowledge, lacking real-time adaptability. To address these issues, we propose \textbf{VADE}, a \textbf{V}ariance-\textbf{A}ware \textbf{D}ynamic sampling framework via online sample-level difficulty \textbf{E}stimation. Our framework integrates three key components: online sample-level difficulty estimation using Beta distributions, a Thompson sampler that maximizes information gain through the estimated correctness probability, and a two-scale prior decay mechanism that maintains robust estimation under policy evolution. This three components design enables VADE to dynamically select the most informative samples, thereby amplifying training signals while eliminating extra rollout costs. Extensive experiments on multimodal reasoning benchmarks show that VADE consistently outperforms strong baselines in both performance and sample efficiency, while achieving a dramatic reduction in computational overhead. More importantly, our framework can serves as a plug-and-play component to be seamlessly integrated into existing group-based RL algorithms. Code and models are available at https://VADE-RL.github.io.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining</title>
<link>https://arxiv.org/abs/2511.18903</link>
<guid>https://arxiv.org/abs/2511.18903</guid>
<content:encoded><![CDATA[
arXiv:2511.18903v1 Announce Type: new 
Abstract: Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Solution Operators for Partial Differential Equations via Monte Carlo-Type Approximation</title>
<link>https://arxiv.org/abs/2511.18930</link>
<guid>https://arxiv.org/abs/2511.18930</guid>
<content:encoded><![CDATA[
arXiv:2511.18930v1 Announce Type: new 
Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a lightweight architecture for learning solution operators for parametric PDEs by directly approximating the kernel integral using a Monte Carlo approach. Unlike Fourier Neural Operators, MCNO makes no spectral or translation-invariance assumptions. The kernel is represented as a learnable tensor over a fixed set of randomly sampled points. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with low computational cost, providing a simple and practical alternative to spectral and graph-based neural operators.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression</title>
<link>https://arxiv.org/abs/2511.18936</link>
<guid>https://arxiv.org/abs/2511.18936</guid>
<content:encoded><![CDATA[
arXiv:2511.18936v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery</title>
<link>https://arxiv.org/abs/2511.18940</link>
<guid>https://arxiv.org/abs/2511.18940</guid>
<content:encoded><![CDATA[
arXiv:2511.18940v1 Announce Type: new 
Abstract: Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces due to strong subject variability and the curved geometry of covariance matrices on the symmetric positive definite (SPD) manifold. We address the zero-shot cross-subject setting, where no target-subject labels or adaptation are allowed, by introducing novel geometry-aware preprocessing modules and deep congruence networks that operate directly on SPD covariance matrices. Our preprocessing modules, DCR and RiFU, extend Riemannian Alignment by improving action separation while reducing subject-specific distortions. We further propose two manifold classifiers, SPD-DCNet and RiFUNet, which use hierarchical congruence transforms to learn discriminative, subject-invariant covariance representations. On the BCI-IV 2a benchmark, our framework improves cross-subject accuracy by 3-4% over the strongest classical baselines, demonstrating the value of geometry-aware transformations for robust EEG decoding.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIST: Mutual Information Via Supervised Training</title>
<link>https://arxiv.org/abs/2511.18945</link>
<guid>https://arxiv.org/abs/2511.18945</guid>
<content:encoded><![CDATA[
arXiv:2511.18945v1 Announce Type: new 
Abstract: We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation</title>
<link>https://arxiv.org/abs/2511.18958</link>
<guid>https://arxiv.org/abs/2511.18958</guid>
<content:encoded><![CDATA[
arXiv:2511.18958v1 Announce Type: new 
Abstract: As graph-structured data grow increasingly large, evaluating their robustness under adversarial attacks becomes computationally expensive and difficult to scale. To address this challenge, we propose to compress graphs into compact representations that preserve both topological structure and robustness profile, enabling efficient and reliable evaluation.We propose Cutter, a dual-agent reinforcement learning framework composed of a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA), which collaboratively identify structurally vital and redundant nodes for guided compression. Cutter incorporates three key strategies to enhance learning efficiency and compression quality: trajectory-level reward shaping to transform sparse trajectory returns into dense, policy-equivalent learning signals; prototype-based shaping to guide decisions using behavioral patterns from both highand low-return trajectories; and cross-agent imitation to enable safer and more transferable exploration. Experiments on multiple real-world graphs demonstrate that Cutter generates compressed graphs that retain essential static topological properties and exhibit robustness degradation trends highly consistent with the original graphs under various attack scenarios, thereby significantly improving evaluation efficiency without compromising assessment fidelity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention</title>
<link>https://arxiv.org/abs/2511.18960</link>
<guid>https://arxiv.org/abs/2511.18960</guid>
<content:encoded><![CDATA[
arXiv:2511.18960v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.18977</link>
<guid>https://arxiv.org/abs/2511.18977</guid>
<content:encoded><![CDATA[
arXiv:2511.18977v1 Announce Type: new 
Abstract: Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Mixture of Experts Against Severe Distribution Shifts</title>
<link>https://arxiv.org/abs/2511.18987</link>
<guid>https://arxiv.org/abs/2511.18987</guid>
<content:encoded><![CDATA[
arXiv:2511.18987v1 Announce Type: new 
Abstract: The challenge of building neural networks that can continuously learn and adapt to evolving data streams is central to the fields of continual learning (CL) and reinforcement learning (RL). This lifelong learning problem is often framed in terms of the plasticity-stability dilemma, focusing on issues like loss of plasticity and catastrophic forgetting. Unlike neural networks, biological brains maintain plasticity through capacity growth, inspiring researchers to explore similar approaches in artificial networks, such as adding capacity dynamically. Prior solutions often lack parameter efficiency or depend on explicit task indices, but Mixture-of-Experts (MoE) architectures offer a promising alternative by specializing experts for distinct distributions. This paper aims to evaluate a DynamicMoE approach for continual and reinforcement learning environments and benchmark its effectiveness against existing network expansion methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Dynamic Radio Map Prediction Using Vision Transformers for Low-Altitude Wireless Networks</title>
<link>https://arxiv.org/abs/2511.19019</link>
<guid>https://arxiv.org/abs/2511.19019</guid>
<content:encoded><![CDATA[
arXiv:2511.19019v1 Announce Type: new 
Abstract: Low-altitude wireless networks (LAWN) are rapidly expanding with the growing deployment of unmanned aerial vehicles (UAVs) for logistics, surveillance, and emergency response. Reliable connectivity remains a critical yet challenging task due to three-dimensional (3D) mobility, time-varying user density, and limited power budgets. The transmit power of base stations (BSs) fluctuates dynamically according to user locations and traffic demands, leading to a highly non-stationary 3D radio environment. Radio maps (RMs) have emerged as an effective means to characterize spatial power distributions and support radio-aware network optimization. However, most existing works construct static or offline RMs, overlooking real-time power variations and spatio-temporal dependencies in multi-UAV networks. To overcome this limitation, we propose a {3D dynamic radio map (3D-DRM)} framework that learns and predicts the spatio-temporal evolution of received power. Specially, a Vision Transformer (ViT) encoder extracts high-dimensional spatial representations from 3D RMs, while a Transformer-based module models sequential dependencies to predict future power distributions. Experiments unveil that 3D-DRM accurately captures fast-varying power dynamics and substantially outperforms baseline models in both RM reconstruction and short-term prediction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs</title>
<link>https://arxiv.org/abs/2511.19023</link>
<guid>https://arxiv.org/abs/2511.19023</guid>
<content:encoded><![CDATA[
arXiv:2511.19023v1 Announce Type: new 
Abstract: Preference learning has recently emerged as a pivotal strategy for post-training alignment of Multimodal Large Language Models (MLLMs). However, existing approaches predominantly rely on external human-annotated preference data, which is costly and labor-intensive to collect. In this work, we propose OrdMoE, a novel preference alignment framework that bypasses the reliance on external human preferences entirely by leveraging intrinsic signals within Mixture-of-Experts (MoE) architectures. Specifically, we observe that the router's expert selection scores implicitly encode a quality-aware ranking of responses (i.e. higher-scoring experts consistently generate higher-quality outputs). Building on this insight, OrdMoE constructs an internal preference hierarchy by grouping experts into ranked tiers based on their per-token routing scores and activating each tier separately to produce a sequence of responses with increasing quality. This yields a zero-cost, self-supervised preference ordering over generated responses, which can be directly optimized using standard preference learning objectives. Extensive experiments across multiple multimodal benchmarks demnstrate that OrdMoE significantly enhances both alignment and overall performance of multimodal Mixture-of-Experts LLMs, achieving competitive results without requiring any human-annotated preference data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolving Node Identifiability in Graph Neural Processes via Laplacian Spectral Encodings</title>
<link>https://arxiv.org/abs/2511.19037</link>
<guid>https://arxiv.org/abs/2511.19037</guid>
<content:encoded><![CDATA[
arXiv:2511.19037v1 Announce Type: new 
Abstract: Message passing graph neural networks are widely used for learning on graphs, yet their expressive power is limited by the one-dimensional Weisfeiler-Lehman test and can fail to distinguish structurally different nodes. We provide rigorous theory for a Laplacian positional encoding that is invariant to eigenvector sign flips and to basis rotations within eigenspaces. We prove that this encoding yields node identifiability from a constant number of observations and establishes a sample-complexity separation from architectures constrained by the Weisfeiler-Lehman test. The analysis combines a monotone link between shortest-path and diffusion distance, spectral trilateration with a constant set of anchors, and quantitative spectral injectivity with logarithmic embedding size. As an instantiation, pairing this encoding with a neural-process style decoder yields significant gains on a drug-drug interaction task on chemical graphs, improving both the area under the ROC curve and the F1 score and demonstrating the practical benefits of resolving theoretical expressiveness limitations with principled positional information.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Participation Imbalance Bias in Asynchronous Federated Learning</title>
<link>https://arxiv.org/abs/2511.19066</link>
<guid>https://arxiv.org/abs/2511.19066</guid>
<content:encoded><![CDATA[
arXiv:2511.19066v1 Announce Type: new 
Abstract: In Asynchronous Federated Learning (AFL), the central server immediately updates the global model with each arriving client's contribution. As a result, clients perform their local training on different model versions, causing information staleness (delay). In federated environments with non-IID local data distributions, this asynchronous pattern amplifies the adverse effect of client heterogeneity (due to different data distribution, local objectives, etc.), as faster clients contribute more frequent updates, biasing the global model. We term this phenomenon heterogeneity amplification. Our work provides a theoretical analysis that maps AFL design choices to their resulting error sources when heterogeneity amplification occurs. Guided by our analysis, we propose ACE (All-Client Engagement AFL), which mitigates participation imbalance through immediate, non-buffered updates that use the latest information available from all clients. We also introduce a delay-aware variant, ACED, to balance client diversity against update staleness. Experiments on different models for different tasks across diverse heterogeneity and delay settings validate our analysis and demonstrate the robust performance of our approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnfoPath: Energy-Informed Analysis of Generative Trajectories in Flow Matching</title>
<link>https://arxiv.org/abs/2511.19087</link>
<guid>https://arxiv.org/abs/2511.19087</guid>
<content:encoded><![CDATA[
arXiv:2511.19087v1 Announce Type: new 
Abstract: Flow-based generative models synthesize data by integrating a learned velocity field from a reference distribution to the target data distribution. Prior work has focused on endpoint metrics (e.g., fidelity, likelihood, perceptual quality) while overlooking a deeper question: what do the sampling trajectories reveal? Motivated by classical mechanics, we introduce kinetic path energy (KPE), a simple yet powerful diagnostic that quantifies the total kinetic effort along each generation path of ODE-based samplers. Through comprehensive experiments on CIFAR-10 and ImageNet-256, we uncover two key phenomena: ({i}) higher KPE predicts stronger semantic quality, indicating that semantically richer samples require greater kinetic effort, and ({ii}) higher KPE inversely correlates with data density, with informative samples residing in sparse, low-density regions. Together, these findings reveal that semantically informative samples naturally reside on the sparse frontier of the data distribution, demanding greater generative effort. Our results suggest that trajectory-level analysis offers a physics-inspired and interpretable framework for understanding generation difficulty and sample characteristics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization of Deep Learning Models for Dynamic Market Behavior Prediction</title>
<link>https://arxiv.org/abs/2511.19090</link>
<guid>https://arxiv.org/abs/2511.19090</guid>
<content:encoded><![CDATA[
arXiv:2511.19090v1 Announce Type: new 
Abstract: The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge-Based Predictive Data Reduction for Smart Agriculture: A Lightweight Approach to Efficient IoT Communication</title>
<link>https://arxiv.org/abs/2511.19103</link>
<guid>https://arxiv.org/abs/2511.19103</guid>
<content:encoded><![CDATA[
arXiv:2511.19103v1 Announce Type: new 
Abstract: The rapid growth of IoT devices has led to an enormous amount of sensor data that requires transmission to cloud servers for processing, resulting in excessive network congestion, increased latency and high energy consumption. This is particularly problematic in resource-constrained and remote environments where bandwidth is limited, and battery-dependent devices further emphasize the problem. Moreover, in domains such as agriculture, consecutive sensor readings often have minimal variation, making continuous data transmission inefficient and unnecessarily resource intensive. To overcome these challenges, we propose an analytical prediction algorithm designed for edge computing environments and validated through simulation. The proposed solution utilizes a predictive filter at the network edge that forecasts the next sensor data point and triggers data transmission only when the deviation from the predicted value exceeds a predefined tolerance. A complementary cloud-based model ensures data integrity and overall system consistency. This dual-model strategy effectively reduces communication overhead and demonstrates potential for improving energy efficiency by minimizing redundant transmissions. In addition to reducing communication load, our approach leverages both in situ and satellite observations from the same locations to enhance model robustness. It also supports cross-site generalization, enabling models trained in one region to be effectively deployed elsewhere without retraining. This makes our solution highly scalable, energy-aware, and well-suited for optimizing sensor data transmission in remote and bandwidth-constrained IoT environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Core in Max-Loss Non-Centroid Clustering Can Be Empty</title>
<link>https://arxiv.org/abs/2511.19107</link>
<guid>https://arxiv.org/abs/2511.19107</guid>
<content:encoded><![CDATA[
arXiv:2511.19107v1 Announce Type: new 
Abstract: We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\geq 3$ there exist metric instances with $n\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $\alpha$-core for any $\alpha<2^{\frac{1}{5}}\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty</title>
<link>https://arxiv.org/abs/2511.19124</link>
<guid>https://arxiv.org/abs/2511.19124</guid>
<content:encoded><![CDATA[
arXiv:2511.19124v1 Announce Type: new 
Abstract: Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Diffusion Models are Secretly Learned-Order Autoregressive Models</title>
<link>https://arxiv.org/abs/2511.19152</link>
<guid>https://arxiv.org/abs/2511.19152</guid>
<content:encoded><![CDATA[
arXiv:2511.19152v1 Announce Type: new 
Abstract: Masked Diffusion Models (MDMs) have emerged as one of the most promising paradigms for generative modeling over discrete domains. It is known that MDMs effectively train to decode tokens in a random order, and that this ordering has significant performance implications in practice. This observation raises a fundamental question: can we design a training framework that optimizes for a favorable decoding order? We answer this in the affirmative, showing that the continuous-time variational objective of MDMs, when equipped with multivariate noise schedules, can identify and optimize for a decoding order during training. We establish a direct correspondence between decoding order and the multivariate noise schedule and show that this setting breaks invariance of the MDM objective to the noise schedule. Furthermore, we prove that the MDM objective decomposes precisely into a weighted auto-regressive losses over these orders, which establishes them as auto-regressive models with learnable orders.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First-order Sobolev Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.19165</link>
<guid>https://arxiv.org/abs/2511.19165</guid>
<content:encoded><![CDATA[
arXiv:2511.19165v1 Announce Type: new 
Abstract: We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning</title>
<link>https://arxiv.org/abs/2511.19168</link>
<guid>https://arxiv.org/abs/2511.19168</guid>
<content:encoded><![CDATA[
arXiv:2511.19168v1 Announce Type: new 
Abstract: Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation</title>
<link>https://arxiv.org/abs/2511.19176</link>
<guid>https://arxiv.org/abs/2511.19176</guid>
<content:encoded><![CDATA[
arXiv:2511.19176v1 Announce Type: new 
Abstract: Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Comparison of Forgetting Mechanisms for UCB-based Algorithms on a Data-Driven Simulation Platform</title>
<link>https://arxiv.org/abs/2511.19240</link>
<guid>https://arxiv.org/abs/2511.19240</guid>
<content:encoded><![CDATA[
arXiv:2511.19240v1 Announce Type: new 
Abstract: Many real-world bandit problems involve non-stationary reward distributions, where the optimal decision may shift due to evolving environments. However, the performance of some typical Multi-Armed Bandit (MAB) models such as Upper Confidence Bound (UCB) algorithms degrades significantly in non-stationary environments where reward distributions change over time. To address this limitation, this paper introduces and evaluates FDSW-UCB, a novel dual-view algorithm that integrates a discount-based long-term perspective with a sliding-window-based short-term view. A data-driven semi-synthetic simulation platform, built upon the MovieLens-1M and Open Bandit datasets, is developed to test algorithm adaptability under abrupt and gradual drift scenarios. Experimental results demonstrate that a well-configured sliding-window mechanism (SW-UCB) is robust, while the widely used discounting method (D-UCB) suffers from a fundamental learning failure, leading to linear regret. Crucially, the proposed FDSW-UCB, when employing an optimistic aggregation strategy, achieves superior performance in dynamic settings, highlighting that the ensemble strategy itself is a decisive factor for success.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local Entropy Search over Descent Sequences for Bayesian Optimization</title>
<link>https://arxiv.org/abs/2511.19241</link>
<guid>https://arxiv.org/abs/2511.19241</guid>
<content:encoded><![CDATA[
arXiv:2511.19241v1 Announce Type: new 
Abstract: Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization</title>
<link>https://arxiv.org/abs/2511.19253</link>
<guid>https://arxiv.org/abs/2511.19253</guid>
<content:encoded><![CDATA[
arXiv:2511.19253v1 Announce Type: new 
Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Nutrition Multimodal Photoplethysmography Language Model</title>
<link>https://arxiv.org/abs/2511.19260</link>
<guid>https://arxiv.org/abs/2511.19260</guid>
<content:encoded><![CDATA[
arXiv:2511.19260v1 Announce Type: new 
Abstract: Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention</title>
<link>https://arxiv.org/abs/2511.19263</link>
<guid>https://arxiv.org/abs/2511.19263</guid>
<content:encoded><![CDATA[
arXiv:2511.19263v1 Announce Type: new 
Abstract: Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry</title>
<link>https://arxiv.org/abs/2511.19264</link>
<guid>https://arxiv.org/abs/2511.19264</guid>
<content:encoded><![CDATA[
arXiv:2511.19264v1 Announce Type: new 
Abstract: Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks</title>
<link>https://arxiv.org/abs/2511.19265</link>
<guid>https://arxiv.org/abs/2511.19265</guid>
<content:encoded><![CDATA[
arXiv:2511.19265v1 Announce Type: new 
Abstract: The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Spatiotemporal Graph Neural Networks for Multi-Store Sales Forecasting</title>
<link>https://arxiv.org/abs/2511.19267</link>
<guid>https://arxiv.org/abs/2511.19267</guid>
<content:encoded><![CDATA[
arXiv:2511.19267v1 Announce Type: new 
Abstract: This work evaluates the effectiveness of spatiotemporal Graph Neural Networks (GNNs) for multi-store retail sales forecasting and compares their performance against ARIMA, LSTM, and XGBoost baselines. Using weekly sales data from 45 Walmart stores, we construct a relational forecasting framework that models inter-store dependencies through a learned adaptive graph. The proposed STGNN predicts log-differenced sales and reconstructs final values through a residual path, enabling stable training and improved generalisation. Experiments show that STGNN achieves the lowest overall forecasting error, outperforming all baselines in Normalised Total Absolute Error, P90 MAPE, and variance of MAPE across stores. Analysis of the learned adjacency matrix reveals meaningful functional store clusters and high-influence nodes that emerge without geographic metadata. These results demonstrate that relational structure significantly improves forecast quality in interconnected retail environments and establishes STGNNs as a robust modelling choice for multi-store demand prediction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CDLM: Consistency Diffusion Language Models For Faster Sampling</title>
<link>https://arxiv.org/abs/2511.19269</link>
<guid>https://arxiv.org/abs/2511.19269</guid>
<content:encoded><![CDATA[
arXiv:2511.19269v1 Announce Type: new 
Abstract: Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model</title>
<link>https://arxiv.org/abs/2511.19272</link>
<guid>https://arxiv.org/abs/2511.19272</guid>
<content:encoded><![CDATA[
arXiv:2511.19272v1 Announce Type: new 
Abstract: We present Tiny-TSM, a time series foundation model characterized by small scale, economical training, and state-of-the-art performance. It comprises 23M total parameters, trained on a single A100 GPU in less than a week using a new synthetic data generation and data augmentation pipeline (SynthTS). Without any neural architecture search, hyperparameter tuning, or scaling up model size, Tiny-TSM achieves state-of-the-art performance on a wide range of time series benchmark datasets, often outperforming much larger models and even matching the performance of much larger, industrial-scale, likely highly tuned foundation models. Specifically, Tiny-TSM outperforms all other time series foundation models we evaluated on medium- and long-term forecasting tasks under MSE loss, while short-term accuracy is still competitive with state-of-the-art models.
  We also introduce a causal input normalization scheme that enables time series models to be trained with dense next-token prediction loss, significantly accelerating convergence speed and reducing training time.
  All experiments were conducted on a single A100 GPU, illustrating the practicality of the proposed approach in a resource-constrained setting.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Bayesian Network Structure Learning Using Tsetlin Machine to Constrain the Search Space</title>
<link>https://arxiv.org/abs/2511.19273</link>
<guid>https://arxiv.org/abs/2511.19273</guid>
<content:encoded><![CDATA[
arXiv:2511.19273v1 Announce Type: new 
Abstract: The PC algorithm is a widely used method in causal inference for learning the structure of Bayesian networks. Despite its popularity, the PC algorithm suffers from significant time complexity, particularly as the size of the dataset increases, which limits its applicability in large-scale real-world problems. In this study, we propose a novel approach that utilises the Tsetlin Machine (TM) to construct Bayesian structures more efficiently. Our method leverages the most significant literals extracted from the TM and performs conditional independence (CI) tests on these selected literals instead of the full set of variables, resulting in a considerable reduction in computational time. We implemented our approach and compared it with various state-of-the-art methods. Our evaluation includes categorical datasets from the bnlearn repository, such as Munin1, Hepar2. The findings indicate that the proposed TM-based method not only reduces computational complexity but also maintains competitive accuracy in causal discovery, making it a viable alternative to traditional PC algorithm implementations by offering improved efficiency without compromising performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing Gaps in Emissions Monitoring with Climate TRACE</title>
<link>https://arxiv.org/abs/2511.19277</link>
<guid>https://arxiv.org/abs/2511.19277</guid>
<content:encoded><![CDATA[
arXiv:2511.19277v1 Announce Type: new 
Abstract: Global greenhouse gas emissions estimates are essential for monitoring and mitigation planning. Yet most datasets lack one or more characteristics that enhance their actionability, such as accuracy, global coverage, high spatial and temporal resolution, and frequent updates. To address these gaps, we present Climate TRACE (climatetrace.org), an open-access platform delivering global emissions estimates with enhanced detail, coverage, and timeliness. Climate TRACE synthesizes existing emissions data, prioritizing accuracy, coverage, and resolution, and fills gaps using sector-specific estimation approaches. The dataset is the first to provide globally comprehensive emissions estimates for individual sources (e.g., individual power plants) for all anthropogenic emitting sectors. The dataset spans January 1, 2021, to the present, with a two-month reporting lag and monthly updates. The open-access platform enables non-technical audiences to engage with detailed emissions datasets for most subnational governments worldwide. Climate TRACE supports data-driven climate action at scales where decisions are made, representing a major breakthrough for emissions accounting and mitigation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings</title>
<link>https://arxiv.org/abs/2511.19279</link>
<guid>https://arxiv.org/abs/2511.19279</guid>
<content:encoded><![CDATA[
arXiv:2511.19279v1 Announce Type: new 
Abstract: A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning</title>
<link>https://arxiv.org/abs/2511.19299</link>
<guid>https://arxiv.org/abs/2511.19299</guid>
<content:encoded><![CDATA[
arXiv:2511.19299v1 Announce Type: new 
Abstract: Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding the Staged Dynamics of Transformers in Learning Latent Structure</title>
<link>https://arxiv.org/abs/2511.19328</link>
<guid>https://arxiv.org/abs/2511.19328</guid>
<content:encoded><![CDATA[
arXiv:2511.19328v1 Announce Type: new 
Abstract: While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data</title>
<link>https://arxiv.org/abs/2511.19330</link>
<guid>https://arxiv.org/abs/2511.19330</guid>
<content:encoded><![CDATA[
arXiv:2511.19330v1 Announce Type: new 
Abstract: A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Annotation-Free Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2511.19344</link>
<guid>https://arxiv.org/abs/2511.19344</guid>
<content:encoded><![CDATA[
arXiv:2511.19344v1 Announce Type: new 
Abstract: Despite significant progress in continual learning ranging from architectural novelty to clever strategies for mitigating catastrophic forgetting most existing methods rest on a strong but unrealistic assumption the availability of labeled data throughout the learning process. In real-world scenarios, however, data often arrives sequentially and without annotations, rendering conventional approaches impractical. In this work, we revisit the fundamental assumptions of continual learning and ask: Can current systems adapt when labels are absent and tasks emerge incrementally over time? To this end, we introduce Annotation-Free Class-Incremental Learning (AFCIL), a more realistic and challenging paradigm where unlabeled data arrives continuously, and the learner must incrementally acquire new classes without any supervision. To enable effective learning under AFCIL, we propose CrossWorld CL, a Cross Domain World Guided Continual Learning framework that incorporates external world knowledge as a stable auxiliary source. The method retrieves semantically related ImageNet classes for each downstream category, maps downstream and ImageNet features through a cross domain alignment strategy and finally introduce a novel replay strategy. This design lets the model uncover semantic structure without annotations while keeping earlier knowledge intact. Across four datasets, CrossWorld-CL surpasses CLIP baselines and existing continual and unlabeled learning methods, underscoring the benefit of world knowledge for annotation free continual learning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric</title>
<link>https://arxiv.org/abs/2511.19350</link>
<guid>https://arxiv.org/abs/2511.19350</guid>
<content:encoded><![CDATA[
arXiv:2511.19350v1 Announce Type: new 
Abstract: Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging LLMs for reward function design in reinforcement learning control tasks</title>
<link>https://arxiv.org/abs/2511.19355</link>
<guid>https://arxiv.org/abs/2511.19355</guid>
<content:encoded><![CDATA[
arXiv:2511.19355v1 Announce Type: new 
Abstract: The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Conformal Prediction via Class Similarity</title>
<link>https://arxiv.org/abs/2511.19359</link>
<guid>https://arxiv.org/abs/2511.19359</guid>
<content:encoded><![CDATA[
arXiv:2511.19359v1 Announce Type: new 
Abstract: Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural surrogates for designing gravitational wave detectors</title>
<link>https://arxiv.org/abs/2511.19364</link>
<guid>https://arxiv.org/abs/2511.19364</guid>
<content:encoded><![CDATA[
arXiv:2511.19364v1 Announce Type: new 
Abstract: Physics simulators are essential in science and engineering, enabling the analysis, control, and design of complex systems. In experimental sciences, they are increasingly used to automate experimental design, often via combinatorial search and optimization. However, as the setups grow more complex, the computational cost of traditional, CPU-based simulators becomes a major limitation. Here, we show how neural surrogate models can significantly reduce reliance on such slow simulators while preserving accuracy. Taking the design of interferometric gravitational wave detectors as a representative example, we train a neural network to surrogate the gravitational wave physics simulator Finesse, which was developed by the LIGO community. Despite that small changes in physical parameters can change the output by orders of magnitudes, the model rapidly predicts the quality and feasibility of candidate designs, allowing an efficient exploration of large design spaces. Our algorithm loops between training the surrogate, inverse designing new experiments, and verifying their properties with the slow simulator for further training. Assisted by auto-differentiation and GPU parallelism, our method proposes high-quality experiments much faster than direct optimization. Solutions that our algorithm finds within hours outperform designs that take five days for the optimizer to reach. Though shown in the context of gravitational wave detectors, our framework is broadly applicable to other domains where simulator bottlenecks hinder optimization and discovery.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems</title>
<link>https://arxiv.org/abs/2511.19368</link>
<guid>https://arxiv.org/abs/2511.19368</guid>
<content:encoded><![CDATA[
arXiv:2511.19368v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware</title>
<link>https://arxiv.org/abs/2511.19379</link>
<guid>https://arxiv.org/abs/2511.19379</guid>
<content:encoded><![CDATA[
arXiv:2511.19379v1 Announce Type: new 
Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps. This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware. By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency. Our geometric analysis reveals that Flow Matching learns a highly rectified transport path (Curvature $\mathcal{C} \approx 1.02$), which is near-optimal, whereas Diffusion trajectories remain stochastic and tortuous ($\mathcal{C} \approx 3.45$). Furthermore, we establish an ``efficiency frontier'' at $N=10$ function evaluations, where Flow Matching retains high fidelity while Diffusion collapses. Finally, we show via numerical sensitivity analysis that the learned vector field is sufficiently linear to render high-order ODE solvers (Runge-Kutta 4) unnecessary, validating the use of lightweight Euler solvers for edge deployment. \textbf{This work concludes that Flow Matching is the superior algorithmic choice for real-time, resource-constrained generative tasks.}
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme</title>
<link>https://arxiv.org/abs/2511.19390</link>
<guid>https://arxiv.org/abs/2511.19390</guid>
<content:encoded><![CDATA[
arXiv:2511.19390v1 Announce Type: new 
Abstract: Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Robust Social Strategies with Large Language Models</title>
<link>https://arxiv.org/abs/2511.19405</link>
<guid>https://arxiv.org/abs/2511.19405</guid>
<content:encoded><![CDATA[
arXiv:2511.19405v1 Announce Type: new 
Abstract: As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converges to defecting, self-interested policies. We show the same effect in LLMs: despite cooperative priors, RL-trained LLM agents develop opportunistic behavior that can exploit even advanced closed-source models. To address this tendency of RL to converge to poor equilibria, we adapt a recent opponent-learning awareness algorithm, Advantage Alignment, to fine-tune LLMs toward multi-agent cooperation and non-exploitability. We then introduce a group-relative baseline that simplifies advantage computation in iterated games, enabling multi-agent training at LLM scale. We also contribute a novel social dilemma environment, Trust and Split, which requires natural language communication to achieve high collective welfare. Across a wide range of social dilemmas, policies learned with Advantage Alignment achieve higher collective payoffs while remaining robust against exploitation by greedy agents.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniGame: Turning a Unified Multimodal Model Into Its Own Adversary</title>
<link>https://arxiv.org/abs/2511.19413</link>
<guid>https://arxiv.org/abs/2511.19413</guid>
<content:encoded><![CDATA[
arXiv:2511.19413v1 Announce Type: new 
Abstract: Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow Map Distillation Without Data</title>
<link>https://arxiv.org/abs/2511.19428</link>
<guid>https://arxiv.org/abs/2511.19428</guid>
<content:encoded><![CDATA[
arXiv:2511.19428v1 Announce Type: new 
Abstract: State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RTMol: Rethinking Molecule-text Alignment in a Round-trip View</title>
<link>https://arxiv.org/abs/2511.12135</link>
<guid>https://arxiv.org/abs/2511.12135</guid>
<content:encoded><![CDATA[
arXiv:2511.12135v2 Announce Type: cross 
Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Intervention Sequence Analysis for Fault Tracking in Radio Access Networks</title>
<link>https://arxiv.org/abs/2511.17505</link>
<guid>https://arxiv.org/abs/2511.17505</guid>
<content:encoded><![CDATA[
arXiv:2511.17505v1 Announce Type: cross 
Abstract: To keep modern Radio Access Networks (RAN) running smoothly, operators need to spot the real-world triggers behind Service-Level Agreement (SLA) breaches well before customers feel them. We introduce an AI/ML pipeline that does two things most tools miss: (1) finds the likely root-cause indicators and (2) reveals the exact order in which those events unfold. We start by labeling network data: records linked to past SLA breaches are marked `abnormal', and everything else `normal'. Our model then learns the causal chain that turns normal behavior into a fault. In Monte Carlo tests the approach pinpoints the correct trigger sequence with high precision and scales to millions of data points without loss of speed. These results show that high-resolution, causally ordered insights can move fault management from reactive troubleshooting to proactive prevention.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DyPBP: Dynamic Peer Beneficialness Prediction for Cryptocurrency P2P Networking</title>
<link>https://arxiv.org/abs/2511.17523</link>
<guid>https://arxiv.org/abs/2511.17523</guid>
<content:encoded><![CDATA[
arXiv:2511.17523v1 Announce Type: cross 
Abstract: Distributed peer-to-peer (P2P) networking delivers the new blocks and transactions and is critical for the cryptocurrency blockchain system operations. Having poor P2P connectivity reduces the financial rewards from the mining consensus protocol. Previous research defines beneficalness of each Bitcoin peer connection and estimates the beneficialness based on the observations of the blocks and transactions delivery, which are after they are delivered. However, due to the infrequent block arrivals and the sporadic and unstable peer connections, the peers do not stay connected long enough to have the beneficialness score to converge to its expected beneficialness. We design and build Dynamic Peer Beneficialness Prediction (DyPBP) which predicts a peer's beneficialness by using networking behavior observations beyond just the block and transaction arrivals. DyPBP advances the previous research by estimating the beneficialness of a peer connection before it delivers new blocks and transactions. To achieve such goal, DyPBP introduces a new feature for remembrance to address the dynamic connectivity issue, as Bitcoin's peers using distributed networking often disconnect and re-connect. We implement DyPBP on an active Bitcoin node connected to the Mainnet and use machine learning for the beneficialness prediction. Our experimental results validate and evaluate the effectiveness of DyPBP; for example, the error performance improves by 2 to 13 orders of magnitude depending on the machine-learning model selection. DyPBP's use of the remembrance feature also informs our model selection. DyPBP enables the P2P connection's beneficialness estimation from the connection start before a new block arrives.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q-Learning-Based Time-Critical Data Aggregation Scheduling in IoT</title>
<link>https://arxiv.org/abs/2511.17531</link>
<guid>https://arxiv.org/abs/2511.17531</guid>
<content:encoded><![CDATA[
arXiv:2511.17531v1 Announce Type: cross 
Abstract: Time-critical data aggregation in Internet of Things (IoT) networks demands efficient, collision-free scheduling to minimize latency for applications like smart cities and industrial automation. Traditional heuristic methods, with two-phase tree construction and scheduling, often suffer from high computational overhead and suboptimal delays due to their static nature. To address this, we propose a novel Q-learning framework that unifies aggregation tree construction and scheduling, modeling the process as a Markov Decision Process (MDP) with hashed states for scalability. By leveraging a reward function that promotes large, interference-free batch transmissions, our approach dynamically learns optimal scheduling policies. Simulations on static networks with up to 300 nodes demonstrate up to 10.87% lower latency compared to a state-of-the-art heuristic algorithm, highlighting its robustness for delay-sensitive IoT applications. This framework enables timely insights in IoT environments, paving the way for scalable, low-latency data aggregation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis from a CLIP-Aligned Encoder</title>
<link>https://arxiv.org/abs/2511.17547</link>
<guid>https://arxiv.org/abs/2511.17547</guid>
<content:encoded><![CDATA[
arXiv:2511.17547v1 Announce Type: cross 
Abstract: Recent progress in diffusion-based generative models has enabled high-quality image synthesis conditioned on diverse modalities. Extending such models to brain signals could deepen our understanding of human perception and mental representations. However,electroencephalography (EEG) presents major challenges for image generation due to high noise, low spatial resolution, and strong inter-subject variability. Existing approaches,such as DreamDiffusion, BrainVis, and GWIT, primarily adapt EEG features to pre-trained Stable Diffusion models using complex alignment or classification pipelines, often resulting in large parameter counts and limited interpretability. We introduce SYNAPSE, a two-stage framework that bridges EEG signal representation learning and high-fidelity image synthesis. In Stage1, a CLIP-aligned EEG autoencoder learns a semantically structured latent representation by combining signal reconstruction and cross-modal alignment objectives. In Stage2, the pretrained encoder is frozen and integrated with a lightweight adaptation of Stable Diffusion, enabling efficient conditioning on EEG features with minimal trainable parameters. Our method achieves a semantically coherent latent space and state-of-the-art perceptual fidelity on the CVPR40 dataset, outperforming prior EEG-to-image models in both reconstruction efficiency and image quality. Quantitative and qualitative analyses demonstrate that SYNAPSE generalizes effectively across subjects, preserving visual semantics even when class-level agreement is reduced. These results suggest that reconstructing what the brain perceives, rather than what it classifies, is key to faithful EEG-based image generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gate-level boolean evolutionary geometric attention neural networks</title>
<link>https://arxiv.org/abs/2511.17550</link>
<guid>https://arxiv.org/abs/2511.17550</guid>
<content:encoded><![CDATA[
arXiv:2511.17550v1 Announce Type: cross 
Abstract: This paper presents a gate-level Boolean evolutionary geometric attention neural network that models images as Boolean fields governed by logic gates. Each pixel is a Boolean variable (0 or 1) embedded on a two-dimensional geometric manifold (for example, a discrete toroidal lattice), which defines adjacency and information propagation among pixels. The network updates image states through a Boolean reaction-diffusion mechanism: pixels receive Boolean diffusion from neighboring pixels (diffusion process) and perform local logic updates via trainable gate-level logic kernels (reaction process), forming a reaction-diffusion logic network.
  A Boolean self-attention mechanism is introduced, using XNOR-based Boolean Query-Key (Q-K) attention to modulate neighborhood diffusion pathways and realize logic attention. We also propose Boolean Rotary Position Embedding (RoPE), which encodes relative distances by parity-bit flipping to simulate Boolean ``phase'' offsets.
  The overall structure resembles a Transformer but operates entirely in the Boolean domain. Trainable parameters include Q-K pattern bits and gate-level kernel configurations. Because outputs are discrete, continuous relaxation methods (such as sigmoid approximation or soft-logic operators) ensure differentiable training.
  Theoretical analysis shows that the network achieves universal expressivity, interpretability, and hardware efficiency, capable of reproducing convolutional and attention mechanisms. Applications include high-speed image processing, interpretable artificial intelligence, and digital hardware acceleration, offering promising future research directions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks</title>
<link>https://arxiv.org/abs/2511.17576</link>
<guid>https://arxiv.org/abs/2511.17576</guid>
<content:encoded><![CDATA[
arXiv:2511.17576v1 Announce Type: cross 
Abstract: Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms</title>
<link>https://arxiv.org/abs/2511.17592</link>
<guid>https://arxiv.org/abs/2511.17592</guid>
<content:encoded><![CDATA[
arXiv:2511.17592v1 Announce Type: cross 
Abstract: Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALPA: Spaceborne LiDAR Point Adjustment for Enhanced GEDI Footprint Geolocation</title>
<link>https://arxiv.org/abs/2511.17600</link>
<guid>https://arxiv.org/abs/2511.17600</guid>
<content:encoded><![CDATA[
arXiv:2511.17600v1 Announce Type: cross 
Abstract: Spaceborne Light Detection and Ranging (LiDAR) systems, such as NASA's Global Ecosystem Dynamics Investigation (GEDI), provide forest structure for global carbon assessments. However, geolocation uncertainties (typically 5-15 m) propagate systematically through derived products, undermining forest profile estimates, including carbon stock assessments. Existing correction methods face critical limitations: waveform simulation approaches achieve meter-level accuracy but require high-resolution LiDAR data unavailable in most regions, while terrain-based methods employ deterministic grid searches that may overlook optimal solutions in continuous solution spaces. We present SALPA (Spaceborne LiDAR Point Adjustment), a multi-algorithm optimization framework integrating three optimization paradigms with five distance metrics. Operating exclusively with globally available digital elevation models and geoid data, SALPA explores continuous solution spaces through gradient-based, evolutionary, and swarm intelligence approaches. Validation across contrasting sites: topographically complex Nikko, Japan, and flat Landes, France, demonstrates 15-16% improvements over original GEDI positions and 0.5-2% improvements over the state-of-the-art GeoGEDI algorithm. L-BFGS-B with Area-based metrics achieves optimal accuracy-efficiency trade-offs, while population-based algorithms (genetic algorithms, particle swarm optimization) excel in complex terrain. The platform-agnostic framework facilitates straightforward adaptation to emerging spaceborne LiDAR missions, providing a generalizable foundation for universal geolocation correction essential for reliable global forest monitoring and climate policy decisions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of Structured Data Extraction from Perspectively Distorted Documents</title>
<link>https://arxiv.org/abs/2511.17607</link>
<guid>https://arxiv.org/abs/2511.17607</guid>
<content:encoded><![CDATA[
arXiv:2511.17607v1 Announce Type: cross 
Abstract: Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSMix: Hard and Soft Mixing Data Augmentation for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.17614</link>
<guid>https://arxiv.org/abs/2511.17614</guid>
<content:encoded><![CDATA[
arXiv:2511.17614v1 Announce Type: cross 
Abstract: Due to the high cost of annotation or the rarity of some diseases, medical image segmentation is often limited by data scarcity and the resulting overfitting problem. Self-supervised learning and semi-supervised learning can mitigate the data scarcity challenge to some extent. However, both of these paradigms are complex and require either hand-crafted pretexts or well-defined pseudo-labels. In contrast, data augmentation represents a relatively simple and straightforward approach to addressing data scarcity issues. It has led to significant improvements in image recognition tasks. However, the effectiveness of local image editing augmentation techniques in the context of segmentation has been less explored. We propose HSMix, a novel approach to local image editing data augmentation involving hard and soft mixing for medical semantic segmentation. In our approach, a hard-augmented image is created by combining homogeneous regions (superpixels) from two source images. A soft mixing method further adjusts the brightness of these composed regions with brightness mixing based on locally aggregated pixel-wise saliency coefficients. The ground-truth segmentation masks of the two source images undergo the same mixing operations to generate the associated masks for the augmented images. Our method fully exploits both the prior contour and saliency information, thus preserving local semantic information in the augmented images while enriching the augmentation space with more diversity. Our method is a plug-and-play solution that is model agnostic and applicable to a range of medical imaging modalities. Extensive experimental evidence has demonstrated its effectiveness in a variety of medical segmentation tasks. The source code is available in https://github.com/DanielaPlusPlus/HSMix.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Ecologically-Informed Deep Learning Framework for Interpretable and Validatable Habitat Mapping</title>
<link>https://arxiv.org/abs/2511.17627</link>
<guid>https://arxiv.org/abs/2511.17627</guid>
<content:encoded><![CDATA[
arXiv:2511.17627v1 Announce Type: cross 
Abstract: Benthic habitat is challenging due to the environmental complexity of the seafloor, technological limitations, and elevated operational costs, especially in under-explored regions. This generates knowledge gaps for the sustainable management of hydrobiological resources and their nexus with society. We developed ECOSAIC (Ecological Compression via Orthogonal Specialized Autoencoders for Interpretable Classification), an Artificial Intelligence framework for automatic classification of benthic habitats through interpretable latent representations using a customizable autoencoder. ECOSAIC compresses n-dimensional feature space by optimizing specialization and orthogonality between domain-informed features. We employed two domain-informed categories: biogeochemical and hydrogeomorphological, that together integrate biological, physicochemical, hydrological and geomorphological, features, whose constraints on habitats have been recognized in ecology for a century. We applied the model to the Colombian Pacific Ocean and the results revealed 16 benthic habitats, expanding from mangroves to deep rocky areas up to 1000 m depth. The candidate habitats exhibited a strong correspondence between their environmental constraints, represented in latent space, and their expected species composition. This correspondence reflected meaningful ecological associations rather than purely statistical correlations, where the habitat's environmental offerings align semantically with the species' requirements. This approach could improve the management and conservation of benthic habitats, facilitating the development of functional maps that support marine planning, biodiversity conservation and fish stock assessment. We also hope it provides new insights into how ecological principles can inform AI frameworks, particularly given the substantial data limitations that characterize ecological research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Upstream Probabilistic Meta-Imputation for Multimodal Pediatric Pancreatitis Classification</title>
<link>https://arxiv.org/abs/2511.17635</link>
<guid>https://arxiv.org/abs/2511.17635</guid>
<content:encoded><![CDATA[
arXiv:2511.17635v1 Announce Type: cross 
Abstract: Pediatric pancreatitis is a progressive and debilitating inflammatory condition, including acute pancreatitis and chronic pancreatitis, that presents significant clinical diagnostic challenges. Machine learning-based methods also face diagnostic challenges due to limited sample availability and multimodal imaging complexity. To address these challenges, this paper introduces Upstream Probabilistic Meta-Imputation (UPMI), a light-weight augmentation strategy that operates upstream of a meta-learner in a low-dimensional meta-feature space rather than in image space. Modality-specific logistic regressions (T1W and T2W MRI radiomics) produce probability outputs that are transformed into a 7-dimensional meta-feature vector. Class-conditional Gaussian mixture models (GMMs) are then fit within each cross-validation fold to sample synthetic meta-features that, combined with real meta-features, train a Random Forest (RF) meta-classifier. On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieves a mean AUC of 0.908 $\pm$ 0.072, a $\sim$5% relative gain over a real-only baseline (AUC 0.864 $\pm$ 0.061).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?</title>
<link>https://arxiv.org/abs/2511.17643</link>
<guid>https://arxiv.org/abs/2511.17643</guid>
<content:encoded><![CDATA[
arXiv:2511.17643v1 Announce Type: cross 
Abstract: Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Coordination in Autonomous Vehicle Routing: A Simulation-Based Study of Communication, Memory, and Routing Loops</title>
<link>https://arxiv.org/abs/2511.17656</link>
<guid>https://arxiv.org/abs/2511.17656</guid>
<content:encoded><![CDATA[
arXiv:2511.17656v1 Announce Type: cross 
Abstract: Multi-agent coordination is critical for next-generation autonomous vehicle (AV) systems, yet naive implementations of communication-based rerouting can lead to catastrophic performance degradation. This study investigates a fundamental problem in decentralized multi-agent navigation: routing loops, where vehicles without persistent obstacle memory become trapped in cycles of inefficient path recalculation. Through systematic simulation experiments involving 72 unique configurations across varying vehicle densities (15, 35, 55 vehicles) and obstacle frequencies (6, 20 obstacles), we demonstrate that memory-less reactive rerouting increases average travel time by up to 682% compared to baseline conditions. To address this, we introduce Object Memory Management (OMM), a lightweight mechanism enabling agents to retain and share knowledge of previously encountered obstacles. OMM operates by maintaining a distributed blacklist of blocked nodes, which each agent consults during Dijkstra-based path recalculation, effectively preventing redundant routing attempts. Our results show that OMM-enabled coordination reduces average travel time by 75.7% and wait time by 88% compared to memory-less systems, while requiring only 1.67 route recalculations per vehicle versus 9.83 in memory-less scenarios. This work provides empirical evidence that persistent, shared memory is not merely beneficial but essential for robust multi-agent coordination in dynamic environments. The findings have implications beyond autonomous vehicles, informing the design of decentralized systems in robotics, network routing, and distributed AI. We provide a comprehensive experimental analysis, including detailed scenario breakdowns, scalability assessments, and visual documentation of the routing loop phenomenon, demonstrating OMM's critical role in preventing detrimental feedback cycles in cooperative multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Healthcare Provider Engagement in SMS Campaigns</title>
<link>https://arxiv.org/abs/2511.17658</link>
<guid>https://arxiv.org/abs/2511.17658</guid>
<content:encoded><![CDATA[
arXiv:2511.17658v1 Announce Type: cross 
Abstract: As digital communication grows in importance when connecting with healthcare providers, traditional behavioral and content message features are imbued with renewed significance. If one is to meaningfully connect with them, it is crucial to understand what drives them to engage and respond. In this study, the authors analyzed several million text messages sent through the Impiricus platform to learn which factors influenced whether or not a doctor clicked on a link in a message. Several key insights came to light through the use of logistic regression, random forest, and neural network models, the details of which the authors discuss in this paper.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Fourier Transform Based Kernel for Solar Irrandiance Forecasting</title>
<link>https://arxiv.org/abs/2511.17698</link>
<guid>https://arxiv.org/abs/2511.17698</guid>
<content:encoded><![CDATA[
arXiv:2511.17698v1 Announce Type: cross 
Abstract: This study proposes a Quantum Fourier Transform (QFT)-enhanced quantum kernel for short-term time-series forecasting. Each signal is windowed, amplitude-encoded, transformed by a QFT, then passed through a protective rotation layer to avoid the QFT/QFT adjoint cancellation; the resulting kernel is used in kernel ridge regression (KRR). Exogenous predictors are incorporated by convexly fusing feature-specific kernels. On multi-station solar irradiance data across Koppen climate classes, the proposed kernel consistently improves median R2 and nRMSE over reference classical RBF and polynomials kernels, while also reducing bias (nMBE); complementary MAE/ERMAX analyses indicate tighter average errors with remaining headroom under sharp transients. For both quantum and classical models, the only tuned quantities are the feature-mixing weights and the KRR ridge alpha; classical hyperparameters (gamma, r, d) are fixed, with the same validation set size for all models. Experiments are conducted on a noiseless simulator (5 qubits; window length L=32). Limitations and ablations are discussed, and paths toward NISQ execution are outlined.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prequential posteriors</title>
<link>https://arxiv.org/abs/2511.17721</link>
<guid>https://arxiv.org/abs/2511.17721</guid>
<content:encoded><![CDATA[
arXiv:2511.17721v1 Announce Type: cross 
Abstract: Data assimilation is a fundamental task in updating forecasting models upon observing new data, with applications ranging from weather prediction to online reinforcement learning. Deep generative forecasting models (DGFMs) have shown excellent performance in these areas, but assimilating data into such models is challenging due to their intractable likelihood functions. This limitation restricts the use of standard Bayesian data assimilation methodologies for DGFMs. To overcome this, we introduce prequential posteriors, based upon a predictive-sequential (prequential) loss function; an approach naturally suited for temporally dependent data which is the focus of forecasting tasks. Since the true data-generating process often lies outside the assumed model class, we adopt an alternative notion of consistency and prove that, under mild conditions, both the prequential loss minimizer and the prequential posterior concentrate around parameters with optimal predictive performance. For scalable inference, we employ easily parallelizable wastefree sequential Monte Carlo (SMC) samplers with preconditioned gradient-based kernels, enabling efficient exploration of high-dimensional parameter spaces such as those in DGFMs. We validate our method on both a synthetic multi-dimensional time series and a real-world meteorological dataset; highlighting its practical utility for data assimilation for complex dynamical systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2511.17731</link>
<guid>https://arxiv.org/abs/2511.17731</guid>
<content:encoded><![CDATA[
arXiv:2511.17731v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) prompting has proven remarkably effective for eliciting complex reasoning in large language models (LLMs). Yet, its potential in multimodal large language models (MLLMs) remains largely untapped, hindered by the absence of large-scale datasets that capture the rich, spatially grounded reasoning intrinsic to visual understanding. Existing visual-CoT resources are typically small, domain-specific, or lack the human-like stepwise structure necessary for compositional visual reasoning. In this paper, we introduce VisReason, a large-scale dataset designed to advance visual Chain-of-Thought reasoning. VisReason comprises 489K annotated examples spanning four diverse domains, each featuring multi-round, human-like rationales that guide MLLMs through interpretable visual reasoning steps. Building upon this, we curate VisReason-Pro, a 165K subset produced with a stronger expert-level GPT annotator, enriched with detailed reasoning traces and 3D spatial grounding via depth-informed annotations. Fine-tuning the state-of-the-art Qwen2.5-VL model on VisReason and VisReason-Pro yields substantial improvements in step-by-step visual reasoning accuracy, interpretability, and cross-benchmark generalization. These results demonstrate that VisReason equips MLLMs with more systematic and generalizable reasoning capabilities. We envision VisReason as a cornerstone for cultivating human-like visual reasoning, paving the way toward the next generation of multimodal intelligence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Active Learning Fails, Uncalibrated Out of Distribution Uncertainty Quantification Might Be the Problem</title>
<link>https://arxiv.org/abs/2511.17760</link>
<guid>https://arxiv.org/abs/2511.17760</guid>
<content:encoded><![CDATA[
arXiv:2511.17760v1 Announce Type: cross 
Abstract: Efficiently and meaningfully estimating prediction uncertainty is important for exploration in active learning campaigns in materials discovery, where samples with high uncertainty are interpreted as containing information missing from the model. In this work, the effect of different uncertainty estimation and calibration methods are evaluated for active learning when using ensembles of ALIGNN, eXtreme Gradient Boost, Random Forest, and Neural Network model architectures. We compare uncertainty estimates from ALIGNN deep ensembles to loss landscape uncertainty estimates obtained for solubility, bandgap, and formation energy prediction tasks. We then evaluate how the quality of the uncertainty estimate impacts an active learning campaign that seeks model generalization to out-of-distribution data. Uncertainty calibration methods were found to variably generalize from in-domain data to out-of-domain data. Furthermore, calibrated uncertainties were generally unsuccessful in reducing the amount of data required by a model to improve during an active learning campaign on out-of-distribution data when compared to random sampling and uncalibrated uncertainties. The impact of poor-quality uncertainty persists for random forest and eXtreme Gradient Boosting models trained on the same data for the same tasks, indicating that this is at least partially intrinsic to the data and not due to model capacity alone. Analysis of the target, in-distribution uncertainty, out-of-distribution uncertainty, and training residual distributions suggest that future work focus on understanding empirical uncertainties in the feature input space for cases where ensemble prediction variances do not accurately capture the missing information required for the model to generalize.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation</title>
<link>https://arxiv.org/abs/2511.17765</link>
<guid>https://arxiv.org/abs/2511.17765</guid>
<content:encoded><![CDATA[
arXiv:2511.17765v1 Announce Type: cross 
Abstract: Nano-UAV teams offer great agility yet face severe navigation challenges due to constrained onboard sensing, communication, and computation. Existing approaches rely on high-resolution vision or compute-intensive planners, rendering them infeasible for these platforms. We introduce LEARN, a lightweight, two-stage safety-guided reinforcement learning (RL) framework for multi-UAV navigation in cluttered spaces. Our system combines low-resolution Time-of-Flight (ToF) sensors and a simple motion planner with a compact, attention-based RL policy. In simulation, LEARN outperforms two state-of-the-art planners by $10\%$ while using substantially fewer resources. We demonstrate LEARN's viability on six Crazyflie quadrotors, achieving fully onboard flight in diverse indoor and outdoor environments at speeds up to $2.0 m/s$ and traversing $0.2 m$ gaps.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weighted Birkhoff Averages Accelerate Data-Driven Methods</title>
<link>https://arxiv.org/abs/2511.17772</link>
<guid>https://arxiv.org/abs/2511.17772</guid>
<content:encoded><![CDATA[
arXiv:2511.17772v1 Announce Type: cross 
Abstract: Many data-driven algorithms in dynamical systems rely on ergodic averages that converge painfully slowly. One simple idea changes this: taper the ends. Weighted Birkhoff averages can converge much faster (sometimes superpolynomially, even exponentially) and can be incorporated seamlessly into existing methods. We demonstrate this with five weighted algorithms: weighted Dynamic Mode Decomposition (wtDMD), weighted Extended DMD (wtEDMD), weighted Sparse Identification of Nonlinear Dynamics (wtSINDy), weighted spectral measure estimation, and weighted diffusion forecasting. Across examples ranging from fluid flows to El Ni\~no data, the message is clear: weighting costs nothing, is easy to implement, and often delivers markedly better results from the same data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Episodic Memory in Agentic Frameworks: Suggesting Next Tasks</title>
<link>https://arxiv.org/abs/2511.17775</link>
<guid>https://arxiv.org/abs/2511.17775</guid>
<content:encoded><![CDATA[
arXiv:2511.17775v1 Announce Type: cross 
Abstract: Agentic frameworks powered by Large Language Models (LLMs) can be useful tools in scientific workflows by enabling human-AI co-creation. A key challenge is recommending the next steps during workflow creation without relying solely on LLMs, which risk hallucination and require fine-tuning with scarce proprietary data. We propose an episodic memory architecture that stores and retrieves past workflows to guide agents in suggesting plausible next tasks. By matching current workflows with historical sequences, agents can recommend steps based on prior patterns.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Estimators for Node Popularity Models</title>
<link>https://arxiv.org/abs/2511.17783</link>
<guid>https://arxiv.org/abs/2511.17783</guid>
<content:encoded><![CDATA[
arXiv:2511.17783v1 Announce Type: cross 
Abstract: Node popularity is recognized as a key factor in modeling real-world networks, capturing heterogeneity in connectivity across communities. This concept is equally important in bipartite networks, where nodes in different partitions may exhibit varying popularity patterns, motivating models such as the Two-Way Node Popularity Model (TNPM). Existing methods, such as the Two-Stage Divided Cosine (TSDC) algorithm, provide a scalable estimation approach but may have limitations in terms of accuracy or applicability across different types of networks. In this paper, we develop a computationally efficient and theoretically justified variational expectation-maximization (VEM) framework for the TNPM. We establish label consistency for the estimated community assignments produced by the proposed variational estimator in bipartite networks. Through extensive simulation studies, we show that our method achieves superior estimation accuracy across a range of bipartite as well as undirected networks compared to existing algorithms. Finally, we evaluate our method on real-world bipartite and undirected networks, further demonstrating its practical effectiveness and robustness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Guided Alignment in Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.17793</link>
<guid>https://arxiv.org/abs/2511.17793</guid>
<content:encoded><![CDATA[
arXiv:2511.17793v1 Announce Type: cross 
Abstract: Large Vision-Language Models (VLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to integrate visual and textual information. This paper presents a comprehensive analysis of attention patterns in efficient VLMs, revealing that concatenation-based architectures frequently fail to distinguish between semantically matching and non-matching image-text pairs. This is a key factor for object hallucination in these models. To address this, we introduce Attention-Guided Efficient Vision-Language Models (AGE-VLM), a novel framework that enhances visual grounding through interleaved cross-attention layers to instill vision capabilities in pretrained small language models. This enforces in VLM the ability "look" at the correct image regions by leveraging spatial knowledge distilled from the Segment Anything Model (SAM), significantly reducing hallucination. We validate our approach across different vision-centric benchmarks where our method is better or comparable to prior work on efficient VLMs. Our findings provide valuable insights for future research aimed at achieving enhanced visual and linguistic understanding in VLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion</title>
<link>https://arxiv.org/abs/2511.17806</link>
<guid>https://arxiv.org/abs/2511.17806</guid>
<content:encoded><![CDATA[
arXiv:2511.17806v1 Announce Type: cross 
Abstract: Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Importance-Weighted Non-IID Sampling for Flow Matching Models</title>
<link>https://arxiv.org/abs/2511.17812</link>
<guid>https://arxiv.org/abs/2511.17812</guid>
<content:encoded><![CDATA[
arXiv:2511.17812v1 Announce Type: cross 
Abstract: Flow-matching models effectively represent complex distributions, yet estimating expectations of functions of their outputs remains challenging under limited sampling budgets. Independent sampling often yields high-variance estimates, especially when rare but with high-impact outcomes dominate the expectation. We propose an importance-weighted non-IID sampling framework that jointly draws multiple samples to cover diverse, salient regions of a flow's distribution while maintaining unbiased estimation via estimated importance weights. To balance diversity and quality, we introduce a score-based regularization for the diversity mechanism, which uses the score function, i.e., the gradient of the log probability, to ensure samples are pushed apart within high-density regions of the data manifold, mitigating off-manifold drift. We further develop the first approach for importance weighting of non-IID flow samples by learning a residual velocity field that reproduces the marginal distribution of the non-IID samples. Empirically, our method produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing the reliable characterization of flow-matching model outputs. Our code will be publicly available on GitHub.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation</title>
<link>https://arxiv.org/abs/2511.17813</link>
<guid>https://arxiv.org/abs/2511.17813</guid>
<content:encoded><![CDATA[
arXiv:2511.17813v1 Announce Type: cross 
Abstract: Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this "action-aware" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analog Physical Systems Can Exhibit Double Descent</title>
<link>https://arxiv.org/abs/2511.17825</link>
<guid>https://arxiv.org/abs/2511.17825</guid>
<content:encoded><![CDATA[
arXiv:2511.17825v1 Announce Type: cross 
Abstract: An important component of the success of large AI models is double descent, in which networks avoid overfitting as they grow relative to the amount of training data, instead improving their performance on unseen data. Here we demonstrate double descent in a decentralized analog network of self-adjusting resistive elements. This system trains itself and performs tasks without a digital processor, offering potential gains in energy efficiency and speed -- but must endure component non-idealities. We find that standard training fails to yield double descent, but a modified protocol that accommodates this inherent imperfection succeeds. Our findings show that analog physical systems, if appropriately trained, can exhibit behaviors underlying the success of digital AI. Further, they suggest that biological systems might similarly benefit from over-parameterization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Dynamic and Momentum Aperture Optimization for Lattice Design Using Multipoint Bayesian Algorithm Execution</title>
<link>https://arxiv.org/abs/2511.17850</link>
<guid>https://arxiv.org/abs/2511.17850</guid>
<content:encoded><![CDATA[
arXiv:2511.17850v1 Announce Type: cross 
Abstract: We demonstrate that multipoint Bayesian algorithm execution can overcome fundamental computational challenges in storage ring design optimization. Dynamic (DA) and momentum (MA) optimization is a multipoint, multiobjective design task for storage rings, ultimately informing the flux of x-ray sources and luminosity of colliders. Current state-of-art black-box optimization methods require extensive particle-tracking simulations for each trial configuration; the high computational cost restricts the extent of the search to $\sim 10^3$ configurations, and therefore limits the quality of the final design. We remove this bottleneck using multipointBAX, which selects, simulates, and models each trial configuration at the single particle level. We demonstrate our approach on a novel design for a fourth-generation light source, with neural-network powered multipointBAX achieving equivalent Pareto front results using more than two orders of magnitude fewer tracking computations compared to genetic algorithms. The significant reduction in cost positions multipointBAX as a promising alternative to black-box optimization, and we anticipate multipointBAX will be instrumental in the design of future light sources, colliders, and large-scale scientific facilities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Model Predictive Control in Manufacturing Processes: A Review</title>
<link>https://arxiv.org/abs/2511.17865</link>
<guid>https://arxiv.org/abs/2511.17865</guid>
<content:encoded><![CDATA[
arXiv:2511.17865v1 Announce Type: cross 
Abstract: Manufacturing processes are inherently dynamic and uncertain, with varying parameters and nonlinear behaviors, making robust control essential for maintaining quality and reliability. Traditional control methods often fail under these conditions due to their reactive nature. Model Predictive Control (MPC) has emerged as a more advanced framework, leveraging process models to predict future states and optimize control actions. However, MPC relies on simplified models that often fail to capture complex dynamics, and it struggles with accurate state estimation and handling the propagation of uncertainty in manufacturing environments. Machine learning (ML) has been introduced to enhance MPC by modeling nonlinear dynamics and learning latent representations that support predictive modeling, state estimation, and optimization. Yet existing ML-driven MPC approaches remain deterministic and correlation-focused, motivating the exploration of generative. Generative ML offers new opportunities by learning data distributions, capturing hidden patterns, and inherently managing uncertainty, thereby complementing MPC. This review highlights five representative methods and examines how each has been integrated into MPC components, including predictive modeling, state estimation, and optimization. By synthesizing these cases, we outline the common ways generative ML can systematically enhance MPC and provide a framework for understanding its potential in diverse manufacturing processes. We identify key research gaps, propose future directions, and use a representative case to illustrate how generative ML-driven MPC can extend broadly across manufacturing. Taken together, this review positions generative ML not as an incremental add-on but as a transformative approach to reshape predictive control for next-generation manufacturing systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning</title>
<link>https://arxiv.org/abs/2511.17885</link>
<guid>https://arxiv.org/abs/2511.17885</guid>
<content:encoded><![CDATA[
arXiv:2511.17885v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance, enabling MLLM deployment in resource-constrained or latency-sensitive scenarios. Current visual token pruning methods mainly rely on attention-based redundancy analysis and are tailored to dense architectures. We propose Fast Multimodal Mixture-of-Experts (FastMMoE), a training-free acceleration framework for mixture-of-experts (MoE) based MLLMs, developed from a routing analysis perspective. FastMMoE combines two complementary strategies: (i) expert activation reduction for visual tokens to minimize unnecessary expert computation; and (ii) routing-aware token pruning that leverages similarity in routing probability distributions to identify and remove highly redundant visual tokens. Experiments on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 demonstrate that FastMMoE can reduce FLOPs by up to 55.0% while retaining approximately 95.5% of the original performance, consistently outperforming dense-model pruning baselines including FastV and SparseVLM across multiple retention rates.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arbitrage-Free Bond and Yield Curve Forecasting with Neural Filters under HJM Constraints</title>
<link>https://arxiv.org/abs/2511.17892</link>
<guid>https://arxiv.org/abs/2511.17892</guid>
<content:encoded><![CDATA[
arXiv:2511.17892v1 Announce Type: cross 
Abstract: We develop an arbitrage-free deep learning framework for yield curve and bond price forecasting based on the Heath-Jarrow-Morton (HJM) term-structure model and a dynamic Nelson-Siegel parameterization of forward rates. Our approach embeds a no-arbitrage drift restriction into a neural state-space architecture by combining Kalman, extended Kalman, and particle filters with recurrent neural networks (LSTM/CLSTM), and introduces an explicit arbitrage error regularization (AER) term during training. The model is applied to U.S. Treasury and corporate bond data, and its performance is evaluated for both yield-space and price-space predictions at 1-day and 5-day horizons. Empirically, arbitrage regularization leads to its strongest improvements at short maturities, particularly in 5-day-ahead forecasts, increasing market-consistency as measured by bid-ask hit rates and reducing dollar-denominated prediction errors.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token-Controlled Re-ranking for Sequential Recommendation via LLMs</title>
<link>https://arxiv.org/abs/2511.17913</link>
<guid>https://arxiv.org/abs/2511.17913</guid>
<content:encoded><![CDATA[
arXiv:2511.17913v1 Announce Type: cross 
Abstract: The widespread adoption of Large Language Models (LLMs) as re-rankers is shifting recommender systems towards a user-centric paradigm. However, a significant gap remains: current re-rankers often lack mechanisms for fine-grained user control. They struggle to balance inherent user preferences with multiple attribute-based constraints, often resorting to simplistic hard filtering that can excessively narrow the recommendation pool and yield suboptimal results. This limitation leaves users as passive recipients rather than active collaborators in the recommendation process. To bridge this gap, we propose COREC, a novel token-augmented re-ranking framework that incorporates specific user requirements in co-creating the recommendation outcome. COREC empowers users to steer re-ranking results with precise and flexible control via explicit, attribute-based signals. The framework learns to balance these commands against latent preferences, yielding rankings that adhere to user instructions without sacrificing personalization. Experiments show that COREC: (1) exceeds state-of-the-art baselines on standard recommendation effectiveness and (2) demonstrates superior adherence to specific attribute requirements, proving that COREC enables fine-grained and predictable manipulation of the rankings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reinforcement Learning Framework for Resource Allocation in Uplink Carrier Aggregation in the Presence of Self Interference</title>
<link>https://arxiv.org/abs/2511.17931</link>
<guid>https://arxiv.org/abs/2511.17931</guid>
<content:encoded><![CDATA[
arXiv:2511.17931v1 Announce Type: cross 
Abstract: Carrier aggregation (CA) is a technique that allows mobile networks to combine multiple carriers to increase user data rate. On the uplink, for power constrained users, this translates to the need for an efficient resource allocation scheme, where each user distributes its available power among its assigned uplink carriers. Choosing a good set of carriers and allocating appropriate power on the carriers is important. If the carrier allocation on the uplink is such that a harmonic of a user's uplink carrier falls on the downlink frequency of that user, it leads to a self coupling-induced sensitivity degradation of that user's downlink receiver. In this paper, we model the uplink carrier aggregation problem as an optimal resource allocation problem with the associated constraints of non-linearities induced self interference (SI). This involves optimization over a discrete variable (which carriers need to be turned on) and a continuous variable (what power needs to be allocated on the selected carriers) in dynamic environments, a problem which is hard to solve using traditional methods owing to the mixed nature of the optimization variables and the additional need to consider the SI constraint. We adopt a reinforcement learning (RL) framework involving a compound-action actor-critic (CA2C) algorithm for the uplink carrier aggregation problem. We propose a novel reward function that is critical for enabling the proposed CA2C algorithm to efficiently handle SI. The CA2C algorithm along with the proposed reward function learns to assign and activate suitable carriers in an online fashion. Numerical results demonstrate that the proposed RL based scheme is able to achieve higher sum throughputs compared to naive schemes. The results also demonstrate that the proposed reward function allows the CA2C algorithm to adapt the optimization both in the presence and absence of SI.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization</title>
<link>https://arxiv.org/abs/2511.17938</link>
<guid>https://arxiv.org/abs/2511.17938</guid>
<content:encoded><![CDATA[
arXiv:2511.17938v1 Announce Type: cross 
Abstract: Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Graph Navigation for Intelligent Subgraph Matching</title>
<link>https://arxiv.org/abs/2511.17939</link>
<guid>https://arxiv.org/abs/2511.17939</guid>
<content:encoded><![CDATA[
arXiv:2511.17939v1 Announce Type: cross 
Abstract: Subgraph matching, a cornerstone of relational pattern detection in domains ranging from biochemical systems to social network analysis, faces significant computational challenges due to the dramatically growing search space. Existing methods address this problem within a filtering-ordering-enumeration framework, in which the enumeration stage recursively matches the query graph against the candidate subgraphs of the data graph. However, the lack of awareness of subgraph structural patterns leads to a costly brute-force enumeration, thereby critically motivating the need for intelligent navigation in subgraph matching. To address this challenge, we propose Neural Graph Navigation (NeuGN), a neuro-heuristic framework that transforms brute-force enumeration into neural-guided search by integrating neural navigation mechanisms into the core enumeration process. By preserving heuristic-based completeness guarantees while incorporating neural intelligence, NeuGN significantly reduces the \textit{First Match Steps} by up to 98.2\% compared to state-of-the-art methods across six real-world datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A multi-view contrastive learning framework for spatial embeddings in risk modelling</title>
<link>https://arxiv.org/abs/2511.17954</link>
<guid>https://arxiv.org/abs/2511.17954</guid>
<content:encoded><![CDATA[
arXiv:2511.17954v1 Announce Type: cross 
Abstract: Incorporating spatial information, particularly those influenced by climate, weather, and demographic factors, is crucial for improving underwriting precision and enhancing risk management in insurance. However, spatial data are often unstructured, high-dimensional, and difficult to integrate into predictive models. Embedding methods are needed to convert spatial data into meaningful representations for modelling tasks. We propose a novel multi-view contrastive learning framework for generating spatial embeddings that combine information from multiple spatial data sources. To train the model, we construct a spatial dataset that merges satellite imagery and OpenStreetMap features across Europe. The framework aligns these spatial views with coordinate-based encodings, producing low-dimensional embeddings that capture both spatial structure and contextual similarity. Once trained, the model generates embeddings directly from latitude-longitude pairs, enabling any dataset with coordinates to be enriched with meaningful spatial features without requiring access to the original spatial inputs. In a case study on French real estate prices, we compare models trained on raw coordinates against those using our spatial embeddings as inputs. The embeddings consistently improve predictive accuracy across generalised linear, additive, and boosting models, while providing interpretable spatial effects and demonstrating transferability to unseen regions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Automating Data Access Permissions in AI Agents</title>
<link>https://arxiv.org/abs/2511.17959</link>
<guid>https://arxiv.org/abs/2511.17959</guid>
<content:encoded><![CDATA[
arXiv:2511.17959v1 Announce Type: cross 
Abstract: As AI agents attempt to autonomously act on users' behalf, they raise transparency and control issues. We argue that permission-based access control is indispensable in providing meaningful control to the users, but conventional permission models are inadequate for the automated agentic execution paradigm. We therefore propose automated permission management for AI agents. Our key idea is to conduct a user study to identify the factors influencing users' permission decisions and to encode these factors into an ML-based permission management assistant capable of predicting users' future decisions. We find that participants' permission decisions are influenced by communication context but importantly individual preferences tend to remain consistent within contexts, and align with those of other participants. Leveraging these insights, we develop a permission prediction model achieving 85.1% accuracy overall and 94.4% for high-confidence predictions. We find that even without using permission history, our model achieves an accuracy of 66.9%, and a slight increase of training samples (i.e., 1-4) can substantially increase the accuracy by 10.8%.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthesizing Precise Protocol Specs from Natural Language for Effective Test Generation</title>
<link>https://arxiv.org/abs/2511.17977</link>
<guid>https://arxiv.org/abs/2511.17977</guid>
<content:encoded><![CDATA[
arXiv:2511.17977v1 Announce Type: cross 
Abstract: Safety- and security-critical systems have to be thoroughly tested against their specifications. The state of practice is to have _natural language_ specifications, from which test cases are derived manually - a process that is slow, error-prone, and difficult to scale. _Formal_ specifications, on the other hand, are well-suited for automated test generation, but are tedious to write and maintain. In this work, we propose a two-stage pipeline that uses large language models (LLMs) to bridge the gap: First, we extract _protocol elements_ from natural-language specifications; second, leveraging a protocol implementation, we synthesize and refine a formal _protocol specification_ from these elements, which we can then use to massively test further implementations.
  We see this two-stage approach to be superior to end-to-end LLM-based test generation, as 1. it produces an _inspectable specification_ that preserves traceability to the original text; 2. the generation of actual test cases _no longer requires an LLM_; 3. the resulting formal specs are _human-readable_, and can be reviewed, version-controlled, and incrementally refined; and 4. over time, we can build a _corpus_ of natural-language-to-formal-specification mappings that can be used to further train and refine LLMs for more automatic translations.
  Our prototype, AUTOSPEC, successfully demonstrated the feasibility of our approach on five widely used _internet protocols_ (SMTP, POP3, IMAP, FTP, and ManageSieve) by applying its methods on their _RFC specifications_ written in natural-language, and the recent _I/O grammar_ formalism for protocol specification and fuzzing. In its evaluation, AUTOSPEC recovers on average 92.8% of client and 80.2% of server message types, and achieves 81.5% message acceptance across diverse, real-world systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Save, Revisit, Retain: A Scalable Framework for Enhancing User Retention in Large-Scale Recommender Systems</title>
<link>https://arxiv.org/abs/2511.18013</link>
<guid>https://arxiv.org/abs/2511.18013</guid>
<content:encoded><![CDATA[
arXiv:2511.18013v1 Announce Type: cross 
Abstract: User retention is a critical objective for online platforms like Pinterest, as it strengthens user loyalty and drives growth through repeated engagement. A key indicator of retention is revisitation, i.e., when users return to view previously saved content, a behavior often sparked by personalized recommendations and user satisfaction. However, modeling and optimizing revisitation poses significant challenges. One core difficulty is accurate attribution: it is often unclear which specific user actions or content exposures trigger a revisit, since many confounding factors (e.g., content quality, user interface, notifications, or even changing user intent) can influence return behavior. Additionally, the scale and timing of revisitations introduce further complexity; users may revisit content days or even weeks after their initial interaction, requiring the system to maintain and associate extensive historical records across millions of users and sessions. These complexities render existing methods insufficient for robustly capturing and optimizing long-term revisitation. To address these gaps, we introduce a novel, lightweight, and interpretable framework for modeling revisitation behavior and optimizing long-term user retention in Pinterest's search-based recommendation context. By defining a surrogate attribution process that links saves to subsequent revisitations, we reduce noise in the causal relationship between user actions and return visits. Our scalable event aggregation pipeline enables large-scale analysis of user revisitation patterns and enhances the ranking system's ability to surface items with high retention value. Deployed on Pinterest's Related Pins surface to serve 500+ million users, the framework led to a significant lift of 0.1% in active users without additional computational costs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems</title>
<link>https://arxiv.org/abs/2511.18024</link>
<guid>https://arxiv.org/abs/2511.18024</guid>
<content:encoded><![CDATA[
arXiv:2511.18024v1 Announce Type: cross 
Abstract: We present a method for extracting \emph{monosemantic} neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a \emph{prediction aware} training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Correlated-Sequence Differential Privacy</title>
<link>https://arxiv.org/abs/2511.18025</link>
<guid>https://arxiv.org/abs/2511.18025</guid>
<content:encoded><![CDATA[
arXiv:2511.18025v1 Announce Type: cross 
Abstract: Data streams collected from multiple sources are rarely independent. Values evolve over time and influence one another across sequences. These correlations improve prediction in healthcare, finance, and smart-city control yet violate the record-independence assumption built into most Differential Privacy (DP) mechanisms. To restore rigorous privacy guarantees without sacrificing utility, we introduce Correlated-Sequence Differential Privacy (CSDP), a framework specifically designed for preserving privacy in correlated sequential data. CSDP addresses two linked challenges: quantifying the extra information an attacker gains from joint temporal and cross-sequence links, and adding just enough noise to hide that information while keeping the data useful. We model multivariate streams as a Coupling Markov Chain, yielding the derived loose leakage bound expressed with a few spectral terms and revealing a counterintuitive result: stronger coupling can actually decrease worst-case leakage by dispersing perturbations across sequences. Guided by these bounds, we build the Freshness-Regulated Adaptive Noise (FRAN) mechanism--combining data aging, correlation-aware sensitivity scaling, and Laplace noise--that runs in linear time. Tests on two-sequence datasets show that CSDP improves the privacy-utility trade-off by approximately 50% over existing correlated-DP methods and by two orders of magnitude compared to the standard DP approach.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19</title>
<link>https://arxiv.org/abs/2511.18035</link>
<guid>https://arxiv.org/abs/2511.18035</guid>
<content:encoded><![CDATA[
arXiv:2511.18035v1 Announce Type: cross 
Abstract: This paper presents a real time, data driven decision support framework for epidemic control. We combine a compartmental epidemic model with sequential Bayesian inference and reinforcement learning (RL) controllers that adaptively choose intervention levels to balance disease burden, such as intensive care unit (ICU) load, against socio economic costs. We construct a context specific cost function using empirical experiments and expert feedback. We study two RL policies: an ICU threshold rule computed via Monte Carlo grid search, and a policy based on a posterior averaged Q learning agent. We validate the framework by fitting the epidemic model to publicly available ICU occupancy data from the COVID 19 pandemic in England and then generating counterfactual roll out scenarios under each RL controller, which allows us to compare the RL policies to the historical government strategy. Over a 300 day period and for a range of cost parameters, both controllers substantially reduce ICU burden relative to the observed interventions, illustrating how Bayesian sequential learning combined with RL can support the design of epidemic control policies.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MASTEST: A LLM-Based Multi-Agent System For RESTful API Tests</title>
<link>https://arxiv.org/abs/2511.18038</link>
<guid>https://arxiv.org/abs/2511.18038</guid>
<content:encoded><![CDATA[
arXiv:2511.18038v1 Announce Type: cross 
Abstract: Testing RESTful API is increasingly important in quality assurance of cloud-native applications. Recent advances in machine learning (ML) techniques have demonstrated that various testing activities can be performed automatically by large language models (LLMs) with reasonable accuracy. This paper develops a multi-agent system called MASTEST that combines LLM-based and programmed agents to form a complete tool chain that covers the whole workflow of API test starting from generating unit and system test scenarios from API specification in the OpenAPI Swagger format, to generating of Pytest test scripts, executing test scripts to interact with web services, to analysing web service response messages to determine test correctness and calculate test coverage. The system also supports the incorporation of human testers in reviewing and correcting LLM generated test artefacts to ensure the quality of testing activities. MASTEST system is evaluated on two LLMs, GPT-4o and DeepSeek V3.1 Reasoner with five public APIs. The performances of LLMs on various testing activities are measured by a wide range of metrics, including unit and system test scenario coverage and API operation coverage for the quality of generated test scenarios, data type correctness, status code coverage and script syntax correctness for the quality of LLM generated test scripts, as well as bug detection ability and usability of LLM generated test scenarios and scripts. Experiment results demonstrated that both DeepSeek and GPT-4o achieved a high overall performance. DeepSeek excels in data type correctness and status code detection, while GPT-4o performs best in API operation coverage. For both models, LLM generated test scripts maintained 100\% syntax correctness and only required minimal manual edits for semantic correctness. These findings indicate the effectiveness and feasibility of MASTEST.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fidelity-Aware Recommendation Explanations via Stochastic Path Integration</title>
<link>https://arxiv.org/abs/2511.18047</link>
<guid>https://arxiv.org/abs/2511.18047</guid>
<content:encoded><![CDATA[
arXiv:2511.18047v1 Announce Type: cross 
Abstract: Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Kalman Identification for Partially Observable Systems via Adaptive Bayesian Learning</title>
<link>https://arxiv.org/abs/2511.18051</link>
<guid>https://arxiv.org/abs/2511.18051</guid>
<content:encoded><![CDATA[
arXiv:2511.18051v1 Announce Type: cross 
Abstract: Sparse dynamics identification is an essential tool for discovering interpretable physical models and enabling efficient control in engineering systems. However, existing methods rely on batch learning with full historical data, limiting their applicability to real-time scenarios involving sequential and partially observable data. To overcome this limitation, this paper proposes an online Sparse Kalman Identification (SKI) method by integrating the Augmented Kalman Filter (AKF) and Automatic Relevance Determination (ARD). The main contributions are: (1) a theoretically grounded Bayesian sparsification scheme that is seamlessly integrated into the AKF framework and adapted to sequentially collected data in online scenarios; (2) an update mechanism that adapts the Kalman posterior to reflect the updated selection of the basis functions that define the model structure; (3) an explicit gradient-descent formulation that enhances computational efficiency. Consequently, the SKI method achieves accurate model structure selection with millisecond-level efficiency and higher identification accuracy, as demonstrated by extensive simulations and real-world experiments (showing an 84.21\% improvement in accuracy over the baseline AKF).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets</title>
<link>https://arxiv.org/abs/2511.18054</link>
<guid>https://arxiv.org/abs/2511.18054</guid>
<content:encoded><![CDATA[
arXiv:2511.18054v1 Announce Type: cross 
Abstract: High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An operator splitting analysis of Wasserstein--Fisher--Rao gradient flows</title>
<link>https://arxiv.org/abs/2511.18060</link>
<guid>https://arxiv.org/abs/2511.18060</guid>
<content:encoded><![CDATA[
arXiv:2511.18060v1 Announce Type: cross 
Abstract: Wasserstein-Fisher-Rao (WFR) gradient flows have been recently proposed as a powerful sampling tool that combines the advantages of pure Wasserstein (W) and pure Fisher-Rao (FR) gradient flows. Existing algorithmic developments implicitly make use of operator splitting techniques to numerically approximate the WFR partial differential equation, whereby the W flow is evaluated over a given step size and then the FR flow (or vice versa). This works investigates the impact of the order in which the W and FR operator are evaluated and aims to provide a quantitative analysis. Somewhat surprisingly, we show that with a judicious choice of step size and operator ordering, the split scheme can converge to the target distribution faster than the exact WFR flow (in terms of model time). We obtain variational formulae describing the evolution over one time step of both sequential splitting schemes and investigate in which settings the W-FR split should be preferred to the FR-W split. As a step towards this goal we show that the WFR gradient flow preserves log-concavity and obtain the first sharp decay bound for WFR.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Portfolio Optimization with a Financial Goal and Defined Time Horizons</title>
<link>https://arxiv.org/abs/2511.18076</link>
<guid>https://arxiv.org/abs/2511.18076</guid>
<content:encoded><![CDATA[
arXiv:2511.18076v1 Announce Type: cross 
Abstract: This research proposes an enhancement to the innovative portfolio optimization approach using the G-Learning algorithm, combined with parametric optimization via the GIRL algorithm (G-learning approach to the setting of Inverse Reinforcement Learning) as presented by. The goal is to maximize portfolio value by a target date while minimizing the investor's periodic contributions. Our model operates in a highly volatile market with a well-diversified portfolio, ensuring a low-risk level for the investor, and leverages reinforcement learning to dynamically adjust portfolio positions over time. Results show that we improved the Sharpe Ratio from 0.42, as suggested by recent studies using the same approach, to a value of 0.483 a notable achievement in highly volatile markets with diversified portfolios. The comparison between G-Learning and GIRL reveals that while GIRL optimizes the reward function parameters (e.g., lambda = 0.0012 compared to 0.002), its impact on portfolio performance remains marginal. This suggests that reinforcement learning methods, like G-Learning, already enable robust optimization. This research contributes to the growing development of reinforcement learning applications in financial decision-making, demonstrating that probabilistic learning algorithms can effectively align portfolio management strategies with investor needs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Harnessing the Power of LLMs for ABAC Policy Mining</title>
<link>https://arxiv.org/abs/2511.18098</link>
<guid>https://arxiv.org/abs/2511.18098</guid>
<content:encoded><![CDATA[
arXiv:2511.18098v1 Announce Type: cross 
Abstract: This paper presents an empirical investigation into the capabilities of Large Language Models (LLMs) to perform automated Attribute-based Access Control (ABAC) policy mining. While ABAC provides fine-grained, context-aware access management, the increasing number and complexity of access policies can make their formulation and evaluation rather challenging. To address the task of synthesizing concise yet accurate policies, we evaluate the performance of some of the state-of-the-art LLMs, specifically Google Gemini (Flash and Pro) and OpenAI ChatGPT, as potential policy mining engines. An experimental framework was developed in Python to generate randomized access data parameterized by varying numbers of subjects, objects, and initial policy sets. The baseline policy sets, which govern permission decisions between subjects and objects, serve as the ground truth for comparison. Each LLM-generated policy was evaluated against the baseline policy using standard performance metrics. The results indicate that LLMs can effectively infer compact and valid ABAC policies for small-scale scenarios. However, as the system size increases, characterized by higher numbers of subjects and objects, LLM outputs exhibit declining accuracy and precision, coupled with significant increase in the size of policy generated, which is beyond the optimal size. These findings highlight both the promise and limitations of current LLM architectures for scalable policy mining in access control domains. Future work will explore hybrid approaches that combine prompt optimization with classical rule mining algorithms to improve scalability and interpretability in complex ABAC environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens</title>
<link>https://arxiv.org/abs/2511.18105</link>
<guid>https://arxiv.org/abs/2511.18105</guid>
<content:encoded><![CDATA[
arXiv:2511.18105v1 Announce Type: cross 
Abstract: Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.18123</link>
<guid>https://arxiv.org/abs/2511.18123</guid>
<content:encoded><![CDATA[
arXiv:2511.18123v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\textbf{S}$ubspace $\textbf{P}$rojection $\textbf{D}$ebiasing ($\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.18140</link>
<guid>https://arxiv.org/abs/2511.18140</guid>
<content:encoded><![CDATA[
arXiv:2511.18140v1 Announce Type: cross 
Abstract: We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Prediction for Compositional Data</title>
<link>https://arxiv.org/abs/2511.18141</link>
<guid>https://arxiv.org/abs/2511.18141</guid>
<content:encoded><![CDATA[
arXiv:2511.18141v1 Announce Type: cross 
Abstract: In this work, we propose a set of conformal prediction procedures tailored to compositional responses, where outcomes are proportions that must be positive and sum to one. Building on Dirichlet regression, we introduce a split conformal approach based on quantile residuals and a highest-density region strategy that combines a fast coordinate-floor approximation with an internal grid refinement to restore sharpness. Both constructions are model-agnostic at the conformal layer and guarantee finite-sample marginal coverage under exchangeability, while respecting the geometry of the simplex. A comprehensive Monte Carlo study spanning homoscedastic and heteroscedastic designs shows that the quantile residual and grid-refined HDR methods achieve empirical coverage close to the nominal 90\% level and produce substantially narrower regions than the coordinate-floor approximation, which tends to be conservative. We further demonstrate the methods on household budget shares from the BudgetItaly dataset, using standardized socioeconomic and price covariates with a train, calibration, and test split. In this application, the grid-refined HDR attains coverage closest to the target with the smallest average widths, closely followed by the quantile residual approach, while the simple triangular HDR yields wider, less informative sets. Overall, the results indicate that conformal prediction on the simplex can be both calibrated and efficient, providing practical uncertainty quantification for compositional prediction tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems</title>
<link>https://arxiv.org/abs/2511.18151</link>
<guid>https://arxiv.org/abs/2511.18151</guid>
<content:encoded><![CDATA[
arXiv:2511.18151v1 Announce Type: cross 
Abstract: Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution "context stream" for real-time awareness and a low-frequency, high-fidelity "insight stream" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Coordinated Dual-Arm Framework for Delicate Snap-Fit Assemblies</title>
<link>https://arxiv.org/abs/2511.18153</link>
<guid>https://arxiv.org/abs/2511.18153</guid>
<content:encoded><![CDATA[
arXiv:2511.18153v1 Announce Type: cross 
Abstract: Delicate snap-fit assemblies, such as inserting a lens into an eye-wear frame or during electronics assembly, demand timely engagement detection and rapid force attenuation to prevent overshoot-induced component damage or assembly failure. We address these challenges with two key contributions. First, we introduce SnapNet, a lightweight neural network that detects snap-fit engagement from joint-velocity transients in real-time, showing that reliable detection can be achieved using proprioceptive signals without external sensors. Second, we present a dynamical-systems-based dual-arm coordination framework that integrates SnapNet driven detection with an event-triggered impedance modulation, enabling accurate alignment and compliant insertion during delicate snap-fit assemblies. Experiments across diverse geometries on a heterogeneous bimanual platform demonstrate high detection accuracy (over 96% recall) and up to a 30% reduction in peak impact forces compared to standard impedance control.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Polyak with optimal thresholding operators for high-dimensional M-estimation</title>
<link>https://arxiv.org/abs/2511.18167</link>
<guid>https://arxiv.org/abs/2511.18167</guid>
<content:encoded><![CDATA[
arXiv:2511.18167v1 Announce Type: cross 
Abstract: We propose and analyze a variant of Sparse Polyak for high dimensional M-estimation problems. Sparse Polyak proposes a novel adaptive step-size rule tailored to suitably estimate the problem's curvature in the high-dimensional setting, guaranteeing that the algorithm's performance does not deteriorate when the ambient dimension increases. However, convergence guarantees can only be obtained by sacrificing solution sparsity and statistical accuracy. In this work, we introduce a variant of Sparse Polyak that retains its desirable scaling properties with respect to the ambient dimension while obtaining sparser and more accurate solutions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEDIC: a network for monitoring data quality in collider experiments</title>
<link>https://arxiv.org/abs/2511.18172</link>
<guid>https://arxiv.org/abs/2511.18172</guid>
<content:encoded><![CDATA[
arXiv:2511.18172v1 Announce Type: cross 
Abstract: Data Quality Monitoring (DQM) is a crucial component of particle physics experiments and ensures that the recorded data is of the highest quality, and suitable for subsequent physics analysis. Due to the extreme environmental conditions, unprecedented data volumes, and the sheer scale and complexity of the detectors, DQM orchestration has become a very challenging task. Therefore, the use of Machine Learning (ML) to automate anomaly detection, improve efficiency, and reduce human error in the process of collecting high-quality data is unavoidable. Since DQM relies on real experimental data, it is inherently tied to the specific detector substructure and technology in operation. In this work, a simulation-driven approach to DQM is proposed, enabling the study and development of data-quality methodologies in a controlled environment. Using a modified version of Delphes -- a fast, multi-purpose detector simulation -- the preliminary realization of a framework is demonstrated which leverages ML to identify detector anomalies as well as localize the malfunctioning components responsible. We introduce MEDIC (Monitoring for Event Data Integrity and Consistency), a neural network designed to learn detector behavior and perform DQM tasks to look for potential faults. Although the present implementation adopts a simplified setup for computational ease, where large detector regions are deliberately deactivated to mimic faults, this work represents an initial step toward a comprehensive ML-based DQM framework. The encouraging results underline the potential of simulation-driven studies as a foundation for developing more advanced, data-driven DQM systems for future particle detectors.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Forecasts of Suicide Attempts for Patients with Little Data</title>
<link>https://arxiv.org/abs/2511.18199</link>
<guid>https://arxiv.org/abs/2511.18199</guid>
<content:encoded><![CDATA[
arXiv:2511.18199v1 Announce Type: cross 
Abstract: Ecological Momentary Assessment provides real-time data on suicidal thoughts and behaviors, but predicting suicide attempts remains challenging due to their rarity and patient heterogeneity. We show that single models fit to all patients perform poorly, while individualized models improve performance but still overfit to patients with limited data. To address this, we introduce Latent Similarity Gaussian Processes (LSGPs) to capture patient heterogeneity, enabling those with little data to leverage similar patients' trends. Preliminary results show promise: even without kernel-design, we outperform all but one baseline while offering a new understanding of patient similarity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProHD: Projection-Based Hausdorff Distance Approximation</title>
<link>https://arxiv.org/abs/2511.18207</link>
<guid>https://arxiv.org/abs/2511.18207</guid>
<content:encoded><![CDATA[
arXiv:2511.18207v1 Announce Type: cross 
Abstract: The Hausdorff distance (HD) is a robust measure of set dissimilarity, but computing it exactly on large, high-dimensional datasets is prohibitively expensive. We propose \textbf{ProHD}, a projection-guided approximation algorithm that dramatically accelerates HD computation while maintaining high accuracy. ProHD identifies a small subset of candidate "extreme" points by projecting the data onto a few informative directions (such as the centroid axis and top principal components) and computing the HD on this subset. This approach guarantees an underestimate of the true HD with a bounded additive error and typically achieves results within a few percent of the exact value. In extensive experiments on image, physics, and synthetic datasets (up to two million points in $D=256$), ProHD runs 10--100$\times$ faster than exact algorithms while attaining 5--20$\times$ lower error than random sampling-based approximations. Our method enables practical HD calculations in scenarios like large vector databases and streaming data, where quick and reliable set distance estimation is needed.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Typing Reinvented: Towards Hands-Free Input via sEMG</title>
<link>https://arxiv.org/abs/2511.18213</link>
<guid>https://arxiv.org/abs/2511.18213</guid>
<content:encoded><![CDATA[
arXiv:2511.18213v1 Announce Type: cross 
Abstract: We explore surface electromyography (sEMG) as a non-invasive input modality for mapping muscle activity to keyboard inputs, targeting immersive typing in next-generation human-computer interaction (HCI). This is especially relevant for spatial computing and virtual reality (VR), where traditional keyboards are impractical. Using attention-based architectures, we significantly outperform the existing convolutional baselines, reducing online generic CER from 24.98% -> 20.34% and offline personalized CER from 10.86% -> 10.10%, while remaining fully causal. We further incorporate a lightweight decoding pipeline with language-model-based correction, demonstrating the feasibility of accurate, real-time muscle-driven text input for future wearable and spatial interfaces.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using MLIR Transform to Design Sliced Convolution Algorithm</title>
<link>https://arxiv.org/abs/2511.18222</link>
<guid>https://arxiv.org/abs/2511.18222</guid>
<content:encoded><![CDATA[
arXiv:2511.18222v1 Announce Type: cross 
Abstract: This paper proposes SConvTransform, a Transform dialect extension that provides operations for optimizing 2D convolutions in MLIR. Its main operation, SConvOp, lowers Linalg convolutions into tiled and packed generic operations through a fully declarative transformation pipeline. The process is guided by a Convolution Slicing Analysis that determines tile sizes and data layout strategies based on input and filter shapes, as well as target architecture parameters. SConvOp handles edge cases by splitting irregular regions and adjusting affine maps where needed. All packing and tiling operations are derived from a parametric set of affine equations, enabling reusable and analyzable transformations. Although functional correctness was the primary goal of this work, the experimental evaluation demonstrates the effectiveness of SConvTransform, achieving good enough performance across different target architectures. Future work will focus on optimizing performance and porting to other target devices. When applied to standard convolution configurations, the generated code achieves up to 60% of peak performance on ARM SME and 67% on Intel AVX512. These results validate the benefit of combining static shape analysis with structured tiling and packing strategies within the MLIR Transform dialect. Furthermore, the modular design of SConvTransform facilitates integration with future extensions, enabling continued optimization of convolution workloads through MLIR's extensible compilation infrastructure.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Agentic AI and Multi-Agent Systems in Smart Manufacturing</title>
<link>https://arxiv.org/abs/2511.18258</link>
<guid>https://arxiv.org/abs/2511.18258</guid>
<content:encoded><![CDATA[
arXiv:2511.18258v1 Announce Type: cross 
Abstract: The convergence of Agentic AI and MAS enables a new paradigm for intelligent decision making in SMS. Traditional MAS architectures emphasize distributed coordination and specialized autonomy, while recent advances in agentic AI driven by LLMs introduce higher order reasoning, planning, and tool orchestration capabilities. This paper presents a hybrid agentic AI and multi agent framework for a Prescriptive Maintenance use case, where LLM based agents provide strategic orchestration and adaptive reasoning, complemented by rule based and SLMs agents performing efficient, domain specific tasks on the edge. The proposed framework adopts a layered architecture that consists of perception, preprocessing, analytics, and optimization layers, coordinated through an LLM Planner Agent that manages workflow decisions and context retention. Specialized agents autonomously handle schema discovery, intelligent feature analysis, model selection, and prescriptive optimization, while a HITL interface ensures transparency and auditability of generated maintenance recommendations. This hybrid design supports dynamic model adaptation, cost efficient maintenance scheduling, and interpretable decision making. An initial proof of concept implementation is validated on two industrial manufacturing datasets. The developed framework is modular and extensible, supporting seamless integration of new agents or domain modules as capabilities evolve. The results demonstrate the system capability to automatically detect schema, adapt preprocessing pipelines, optimize model performance through adaptive intelligence, and generate actionable, prioritized maintenance recommendations. The framework shows promise in achieving improved robustness, scalability, and explainability for RxM in smart manufacturing, bridging the gap between high level agentic reasoning and low level autonomous execution.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScriptViT: Vision Transformer-Based Personalized Handwriting Generation</title>
<link>https://arxiv.org/abs/2511.18307</link>
<guid>https://arxiv.org/abs/2511.18307</guid>
<content:encoded><![CDATA[
arXiv:2511.18307v1 Announce Type: cross 
Abstract: Styled handwriting generation aims to synthesize handwritten text that looks both realistic and aligned with a specific writer's style. While recent approaches involving GAN, transformer and diffusion-based models have made progress, they often struggle to capture the full spectrum of writer-specific attributes, particularly global stylistic patterns that span long-range spatial dependencies. As a result, capturing subtle writer-specific traits such as consistent slant, curvature or stroke pressure, while keeping the generated text accurate is still an open problem. In this work, we present a unified framework designed to address these limitations. We introduce a Vision Transformer-based style encoder that learns global stylistic patterns from multiple reference images, allowing the model to better represent long-range structural characteristics of handwriting. We then integrate these style cues with the target text using a cross-attention mechanism, enabling the system to produce handwritten images that more faithfully reflect the intended style. To make the process more interpretable, we utilize Salient Stroke Attention Analysis (SSAA), which reveals the stroke-level features the model focuses on during style transfer. Together, these components lead to handwriting synthesis that is not only more stylistically coherent, but also easier to understand and analyze.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search</title>
<link>https://arxiv.org/abs/2511.18313</link>
<guid>https://arxiv.org/abs/2511.18313</guid>
<content:encoded><![CDATA[
arXiv:2511.18313v1 Announce Type: cross 
Abstract: Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly-supervised Latent Models for Task-specific Visual-Language Control</title>
<link>https://arxiv.org/abs/2511.18319</link>
<guid>https://arxiv.org/abs/2511.18319</guid>
<content:encoded><![CDATA[
arXiv:2511.18319v1 Announce Type: cross 
Abstract: Autonomous inspection in hazardous environments requires AI agents that can interpret high-level goals and execute precise control. A key capability for such agents is spatial grounding, for example when a drone must center a detected object in its camera view to enable reliable inspection. While large language models provide a natural interface for specifying goals, using them directly for visual control achieves only 58\% success in this task. We envision that equipping agents with a world model as a tool would allow them to roll out candidate actions and perform better in spatially grounded settings, but conventional world models are data and compute intensive. To address this, we propose a task-specific latent dynamics model that learns state-specific action-induced shifts in a shared latent space using only goal-state supervision. The model leverages global action embeddings and complementary training losses to stabilize learning. In experiments, our approach achieves 71\% success and generalizes to unseen images and instructions, highlighting the potential of compact, domain-specific latent dynamics models for spatial alignment in autonomous inspection.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video</title>
<link>https://arxiv.org/abs/2511.18322</link>
<guid>https://arxiv.org/abs/2511.18322</guid>
<content:encoded><![CDATA[
arXiv:2511.18322v1 Announce Type: cross 
Abstract: Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crash-Consistent Checkpointing for AI Training on macOS/APFS</title>
<link>https://arxiv.org/abs/2511.18323</link>
<guid>https://arxiv.org/abs/2511.18323</guid>
<content:encoded><![CDATA[
arXiv:2511.18323v1 Announce Type: cross 
Abstract: Deep learning training relies on periodic checkpoints to recover from failures, but unsafe checkpoint installation can leave corrupted files on disk. This paper presents an experimental study of checkpoint installation protocols and integrity validation for AI training on macOS/APFS. We implement three write modes with increasing durability guarantees: unsafe (baseline, no fsync), atomic_nodirsync (file-level durability via fsync()), and atomic_dirsync (file + directory durability). We design a format-agnostic integrity guard using SHA-256 checksums with automatic rollback. Through controlled experiments including crash injection (430 unsafe-mode trials) and corruption injection (1,600 atomic-mode trials), we demonstrate that the integrity guard detects 99.8-100% of corruptions with zero false positives. Performance overhead is 56.5-108.4% for atomic_nodirsync and 84.2-570.6% for atomic_dirsync relative to the unsafe baseline. Our findings quantify the reliability-performance trade-offs and provide deployment guidance for production AI infrastructure.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-MGF: Multimodal Graph Fusion Network for EEG-fMRI Brain Connectivity Analysis Under Psilocybin</title>
<link>https://arxiv.org/abs/2511.18325</link>
<guid>https://arxiv.org/abs/2511.18325</guid>
<content:encoded><![CDATA[
arXiv:2511.18325v1 Announce Type: cross 
Abstract: Psychedelics, such as psilocybin, reorganise large-scale brain connectivity, yet how these changes are reflected across electrophysiological (electroencephalogram, EEG) and haemodynamic (functional magnetic resonance imaging, fMRI) networks remains unclear. We present Brain-MGF, a multimodal graph fusion network for joint EEG-fMRI connectivity analysis. For each modality, we construct graphs with partial-correlation edges and Pearson-profile node features, and learn subject-level embeddings via graph convolution. An adaptive softmax gate then fuses modalities with sample-specific weights to capture context-dependent contributions. Using the world's largest single-site psilocybin dataset, PsiConnect, Brain-MGF distinguishes psilocybin from no-psilocybin conditions in meditation and rest. Fusion improves over unimodal and non-adaptive variants, achieving 74.0% accuracy and 76.5% F1 score on meditation, and 76.0% accuracy with 85.8% ROC-AUC on rest. UMAP visualisations reveal clearer class separation for fused embeddings. These results indicate that adaptive graph fusion effectively integrates complementary EEG-fMRI information, providing an interpretable framework for characterising psilocybin-induced alterations in large-scale neural organisation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas</title>
<link>https://arxiv.org/abs/2511.18335</link>
<guid>https://arxiv.org/abs/2511.18335</guid>
<content:encoded><![CDATA[
arXiv:2511.18335v1 Announce Type: cross 
Abstract: The ability of Large Language Models (LLMs) to generate structured outputs that follow arbitrary schemas is crucial to a wide range of downstream tasks that require diverse structured representations of results such as information extraction, table generation, and function calling. While modern LLMs excel in generating unstructured responses in natural language, whether this advancement translates to a strong performance on text-to-structure tasks remains unclear. To bridge this gap, we first introduce OmniStruct, a comprehensive benchmark for assessing LLMs' capabilities on diverse text-to-structure tasks such as information extraction, table generation, and function calling. We build OmniStruct by identifying existing datasets across a wide range of tasks that are suitable for a structured answer format, and adapting them under a unified text-to-structure problem setting. To facilitate the development of efficient text-to-structure models, we collect high-quality training data via synthetic task generation. Without using any supervised data for OmniStruct tasks, our experiments demonstrate the possibility of fine-tuning much smaller models on synthetic data into universal structured generation models that can rival the performance of GPT-4o.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KGpipe: Generation and Evaluation of Pipelines for Data Integration into Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.18364</link>
<guid>https://arxiv.org/abs/2511.18364</guid>
<content:encoded><![CDATA[
arXiv:2511.18364v1 Announce Type: cross 
Abstract: Building high-quality knowledge graphs (KGs) from diverse sources requires combining methods for information extraction, data transformation, ontology mapping, entity matching, and data fusion. Numerous methods and tools exist for each of these tasks, but support for combining them into reproducible and effective end-to-end pipelines is still lacking. We present a new framework, KGpipe for defining and executing integration pipelines that can combine existing tools or LLM (Large Language Model) functionality. To evaluate different pipelines and the resulting KGs, we propose a benchmark to integrate heterogeneous data of different formats (RDF, JSON, text) into a seed KG. We demonstrate the flexibility of KGpipe by running and comparatively evaluating several pipelines integrating sources of the same or different formats using selected performance and quality metrics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DHAuDS: A Dynamic and Heterogeneous Audio Benchmark for Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.18421</link>
<guid>https://arxiv.org/abs/2511.18421</guid>
<content:encoded><![CDATA[
arXiv:2511.18421v1 Announce Type: cross 
Abstract: Audio classifiers frequently face domain shift, when models trained on one dataset lose accuracy on data recorded in acoustically different conditions. Previous Test-Time Adaptation (TTA) research in speech and sound analysis often evaluates models under fixed or mismatched noise settings, that fail to mimic real-world variability. To overcome these limitations, this paper presents DHAuDS (Dynamic and Heterogeneous Audio Domain Shift), a benchmark designed to assess TTA approaches under more realistic and diverse acoustic shifts. DHAuDS comprises four standardized benchmarks: UrbanSound8K-C, SpeechCommandsV2-C, VocalSound-C, and ReefSet-C, each constructed with dynamic corruption severity levels and heterogeneous noise types to simulate authentic audio degradation scenarios. The framework defines 14 evaluation criteria for each benchmark (8 for UrbanSound8K-C), resulting in 50 unrepeated criteria (124 experiments) that collectively enable fair, reproducible, and cross-domain comparison of TTA algorithms. Through the inclusion of dynamic and mixed-domain noise settings, DHAuDS offers a consistent and publicly reproducible testbed to support ongoing studies in robust and adaptive audio modeling.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroVascU-Net: A Unified Multi-Scale and Cross-Domain Adaptive Feature Fusion U-Net for Precise 3D Segmentation of Brain Vessels in Contrast-Enhanced T1 MRI</title>
<link>https://arxiv.org/abs/2511.18422</link>
<guid>https://arxiv.org/abs/2511.18422</guid>
<content:encoded><![CDATA[
arXiv:2511.18422v1 Announce Type: cross 
Abstract: Precise 3D segmentation of cerebral vasculature from T1-weighted contrast-enhanced (T1CE) MRI is crucial for safe neurosurgical planning. Manual delineation is time-consuming and prone to inter-observer variability, while current automated methods often trade accuracy for computational cost, limiting clinical use. We present NeuroVascU-Net, the first deep learning architecture specifically designed to segment cerebrovascular structures directly from clinically standard T1CE MRI in neuro-oncology patients, addressing a gap in prior work dominated by TOF-MRA-based approaches. NeuroVascU-Net builds on a dilated U-Net and integrates two specialized modules: a Multi-Scale Contextual Feature Fusion ($MSC^2F$) module at the bottleneck and a Cross-Domain Adaptive Feature Fusion ($CDA^2F$) module at deeper hierarchical layers. $MSC^2F$ captures both local and global information via multi-scale dilated convolutions, while $CDA^2F$ dynamically integrates domain-specific features, enhancing representation while keeping computation low. The model was trained and validated on a curated dataset of T1CE scans from 137 brain tumor biopsy patients, annotated by a board-certified functional neurosurgeon. NeuroVascU-Net achieved a Dice score of 0.8609 and precision of 0.8841, accurately segmenting both major and fine vascular structures. Notably, it requires only 12.4M parameters, significantly fewer than transformer-based models such as Swin U-NetR. This balance of accuracy and efficiency positions NeuroVascU-Net as a practical solution for computer-assisted neurosurgical planning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General Agentic Memory Via Deep Research</title>
<link>https://arxiv.org/abs/2511.18423</link>
<guid>https://arxiv.org/abs/2511.18423</guid>
<content:encoded><![CDATA[
arXiv:2511.18423v1 Announce Type: cross 
Abstract: Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \textbf{general agentic memory (GAM)}. GAM follows the principle of "\textbf{just-in time (JIT) compilation}" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Selection of Heterogeneous Treatment Effect Estimators</title>
<link>https://arxiv.org/abs/2511.18464</link>
<guid>https://arxiv.org/abs/2511.18464</guid>
<content:encoded><![CDATA[
arXiv:2511.18464v1 Announce Type: cross 
Abstract: We study the problem of selecting the best heterogeneous treatment effect (HTE) estimator from a collection of candidates in settings where the treatment effect is fundamentally unobserved. We cast estimator selection as a multiple testing problem and introduce a ground-truth-free procedure based on a cross-fitted, exponentially weighted test statistic. A key component of our method is a two-way sample splitting scheme that decouples nuisance estimation from weight learning and ensures the stability required for valid inference. Leveraging a stability-based central limit theorem, we establish asymptotic familywise error rate control under mild regularity conditions. Empirically, our procedure provides reliable error control while substantially reducing false selections compared with commonly used methods across ACIC 2016, IHDP, and Twins benchmarks, demonstrating that our method is feasible and powerful even without ground-truth treatment effects.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundations of Artificial Intelligence Frameworks: Notion and Limits of AGI</title>
<link>https://arxiv.org/abs/2511.18517</link>
<guid>https://arxiv.org/abs/2511.18517</guid>
<content:encoded><![CDATA[
arXiv:2511.18517v1 Announce Type: cross 
Abstract: Within the limited scope of this paper, we argue that artificial general intelligence cannot emerge from current neural network paradigms regardless of scale, nor is such an approach healthy for the field at present. Drawing on various notions, discussions, present-day developments and observations, current debates and critiques, experiments, and so on in between philosophy, including the Chinese Room Argument and G\"odelian argument, neuroscientific ideas, computer science, the theoretical consideration of artificial intelligence, and learning theory, we address conceptually that neural networks are architecturally insufficient for genuine understanding. They operate as static function approximators of a limited encoding framework - a 'sophisticated sponge' exhibiting complex behaviours without structural richness that constitute intelligence. We critique the theoretical foundations the field relies on and created of recent times; for example, an interesting heuristic as neural scaling law (as an example, arXiv:2001.08361 ) made prominent in a wrong way of interpretation, The Universal Approximation Theorem addresses the wrong level of abstraction and, in parts, partially, the question of current architectures lacking dynamic restructuring capabilities. We propose a framework distinguishing existential facilities (computational substrate) from architectural organization (interpretive structures), and outline principles for what genuine machine intelligence would require, and furthermore, a conceptual method of structuralizing the richer framework on which the principle of neural network system takes hold.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transforming Conditional Density Estimation Into a Single Nonparametric Regression Task</title>
<link>https://arxiv.org/abs/2511.18530</link>
<guid>https://arxiv.org/abs/2511.18530</guid>
<content:encoded><![CDATA[
arXiv:2511.18530v1 Announce Type: cross 
Abstract: We propose a way of transforming the problem of conditional density estimation into a single nonparametric regression task via the introduction of auxiliary samples. This allows leveraging regression methods that work well in high dimensions, such as neural networks and decision trees. Our main theoretical result characterizes and establishes the convergence of our estimator to the true conditional density in the data limit. We develop condensit\'e, a method that implements this approach. We demonstrate the benefit of the auxiliary samples on synthetic data and showcase that condensit\'e can achieve good out-of-the-box results. We evaluate our method on a large population survey dataset and on a satellite imaging dataset. In both cases, we find that condensit\'e matches or outperforms the state of the art and yields conditional densities in line with established findings in the literature on each dataset. Our contribution opens up new possibilities for regression-based conditional density estimation and the empirical results indicate strong promise for applied research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Smoothed Demand Management</title>
<link>https://arxiv.org/abs/2511.18554</link>
<guid>https://arxiv.org/abs/2511.18554</guid>
<content:encoded><![CDATA[
arXiv:2511.18554v1 Announce Type: cross 
Abstract: We introduce and study a class of online problems called online smoothed demand management $(\texttt{OSDM})$, motivated by paradigm shifts in grid integration and energy storage for large energy consumers such as data centers. In $\texttt{OSDM}$, an operator makes two decisions at each time step: an amount of energy to be purchased, and an amount of energy to be delivered (i.e., used for computation). The difference between these decisions charges (or discharges) the operator's energy storage (e.g., a battery). Two types of demand arrive online: base demand, which must be covered at the current time, and flexible demand, which can be satisfied at any time steps before a demand-specific deadline $\Delta_t$. The operator's goal is to minimize a cost (subject to the constraints above) that combines a cost of purchasing energy, a cost for delivering energy (if applicable), and smoothness penalties on the purchasing and delivery rates to discourage fluctuations and encourage ``grid healthy'' decisions. $\texttt{OSDM}$ generalizes several problems in the online algorithms literature while being the first to fully model applications of interest. We propose a competitive algorithm called $\texttt{PAAD}$ (partitioned accounting \& aggregated decisions) and show it achieves the optimal competitive ratio. To overcome the pessimism typical of worst-case analysis, we also propose a novel learning framework that provides guarantees on the worst-case competitive ratio (i.e., to provide robustness against nonstationarity) while allowing end-to-end differentiable learning of the best algorithm on historical instances of the problem. We evaluate our algorithms in a case study of a grid-integrated data center with battery storage, showing that $\texttt{PAAD}$ effectively solves the problem and end-to-end learning achieves substantial performance improvements compared to $\texttt{PAAD}$.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A joint optimization approach to identifying sparse dynamics using least squares kernel collocation</title>
<link>https://arxiv.org/abs/2511.18555</link>
<guid>https://arxiv.org/abs/2511.18555</guid>
<content:encoded><![CDATA[
arXiv:2511.18555v1 Announce Type: cross 
Abstract: We develop an all-at-once modeling framework for learning systems of ordinary differential equations (ODE) from scarce, partial, and noisy observations of the states. The proposed methodology amounts to a combination of sparse recovery strategies for the ODE over a function library combined with techniques from reproducing kernel Hilbert space (RKHS) theory for estimating the state and discretizing the ODE. Our numerical experiments reveal that the proposed strategy leads to significant gains in terms of accuracy, sample efficiency, and robustness to noise, both in terms of learning the equation and estimating the unknown states. This work demonstrates capabilities well beyond existing and widely used algorithms while extending the modeling flexibility of other recent developments in equation discovery.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks</title>
<link>https://arxiv.org/abs/2511.18562</link>
<guid>https://arxiv.org/abs/2511.18562</guid>
<content:encoded><![CDATA[
arXiv:2511.18562v1 Announce Type: cross 
Abstract: Conformal prediction (CP) provides distribution-free, finite-sample coverage guarantees but critically relies on exchangeability, a condition often violated under distribution shift. We study the robustness of split conformal prediction under adversarial perturbations at test time, focusing on both coverage validity and the resulting prediction set size. Our theoretical analysis characterizes how the strength of adversarial perturbations during calibration affects coverage guarantees under adversarial test conditions. We further examine the impact of adversarial training at the model-training stage. Extensive experiments support our theory: (i) Prediction coverage varies monotonically with the calibration-time attack strength, enabling the use of nonzero calibration-time attack to predictably control coverage under adversarial tests; (ii) target coverage can hold over a range of test-time attacks: with a suitable calibration attack, coverage stays within any chosen tolerance band across a contiguous set of perturbation levels; and (iii) adversarial training at the training stage produces tighter prediction sets that retain high informativeness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re(Visiting) Time Series Foundation Models in Finance</title>
<link>https://arxiv.org/abs/2511.18578</link>
<guid>https://arxiv.org/abs/2511.18578</guid>
<content:encoded><![CDATA[
arXiv:2511.18578v1 Announce Type: cross 
Abstract: Financial time series forecasting is central to trading, portfolio optimization, and risk management, yet it remains challenging due to noisy, non-stationary, and heterogeneous data. Recent advances in time series foundation models (TSFMs), inspired by large language models, offer a new paradigm for learning generalizable temporal representations from large and diverse datasets. This paper presents the first comprehensive empirical study of TSFMs in global financial markets. Using a large-scale dataset of daily excess returns across diverse markets, we evaluate zero-shot inference, fine-tuning, and pre-training from scratch against strong benchmark models. We find that off-the-shelf pre-trained TSFMs perform poorly in zero-shot and fine-tuning settings, whereas models pre-trained from scratch on financial data achieve substantial forecasting and economic improvements, underscoring the value of domain-specific adaptation. Increasing the dataset size, incorporating synthetic data augmentation, and applying hyperparameter tuning further enhance performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differential privacy with dependent data</title>
<link>https://arxiv.org/abs/2511.18583</link>
<guid>https://arxiv.org/abs/2511.18583</guid>
<content:encoded><![CDATA[
arXiv:2511.18583v1 Announce Type: cross 
Abstract: Dependent data underlies many statistical studies in the social and health sciences, which often involve sensitive or private information. Differential privacy (DP) and in particular \textit{user-level} DP provide a natural formalization of privacy requirements for processing dependent data where each individual provides multiple observations to the dataset. However, dependence introduced, e.g., through repeated measurements challenges the existing statistical theory under DP-constraints. In \iid{} settings, noisy Winsorized mean estimators have been shown to be minimax optimal for standard (\textit{item-level}) and \textit{user-level} DP estimation of a mean $\mu \in \R^d$. Yet, their behavior on potentially dependent observations has not previously been studied. We fill this gap and show that Winsorized mean estimators can also be used under dependence for bounded and unbounded data, and can lead to asymptotic and finite sample guarantees that resemble their \iid{} counterparts under a weak notion of dependence. For this, we formalize dependence via log-Sobolev inequalities on the joint distribution of observations. This enables us to adapt the stable histogram by Karwa and Vadhan (2018) to a non-\iid{} setting, which we then use to estimate the private projection intervals of the Winsorized estimator. The resulting guarantees for our item-level mean estimator extend to \textit{user-level} mean estimation and transfer to the local model via a randomized response histogram. Using the mean estimators as building blocks, we provide extensions to random effects models, longitudinal linear regression and nonparametric regression. Therefore, our work constitutes a first step towards a systematic study of DP for dependent data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Simulations to Surveys: Domain Adaptation for Galaxy Observations</title>
<link>https://arxiv.org/abs/2511.18590</link>
<guid>https://arxiv.org/abs/2511.18590</guid>
<content:encoded><![CDATA[
arXiv:2511.18590v1 Announce Type: cross 
Abstract: Large photometric surveys will image billions of galaxies, but we currently lack quick, reliable automated ways to infer their physical properties like morphology, stellar mass, and star formation rates. Simulations provide galaxy images with ground-truth physical labels, but domain shifts in PSF, noise, backgrounds, selection, and label priors degrade transfer to real surveys. We present a preliminary domain adaptation pipeline that trains on simulated TNG50 galaxies and evaluates on real SDSS galaxies with morphology labels (elliptical/spiral/irregular). We train three backbones (CNN, $E(2)$-steerable CNN, ResNet-18) with focal loss and effective-number class weighting, and a feature-level domain loss $L_D$ built from GeomLoss (entropic Sinkhorn OT, energy distance, Gaussian MMD, and related metrics). We show that a combination of these losses with an OT-based "top_$k$ soft matching" loss that focuses $L_D$ on the worst-matched source-target pairs can further enhance domain alignment. With Euclidean distance, scheduled alignment weights, and top-$k$ matching, target accuracy (macro F1) rises from $\sim$46% ($\sim$30%) at no adaptation to $\sim$87% ($\sim$62.6%), with a domain AUC near 0.5, indicating strong latent-space mixing.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoder for Position-Assisted Beam Prediction in mmWave ISAC Systems</title>
<link>https://arxiv.org/abs/2511.18594</link>
<guid>https://arxiv.org/abs/2511.18594</guid>
<content:encoded><![CDATA[
arXiv:2511.18594v1 Announce Type: cross 
Abstract: Integrated sensing and communication and millimeter wave (mmWave) have emerged as pivotal technologies for 6G networks. However, the narrow nature of mmWave beams requires precise alignments that typically necessitate large training overhead. This overhead can be reduced by incorporating the position information with beam adjustments. This letter proposes a lightweight autorencoder (LAE) model that addresses the position-assisted beam prediction problem while significantly reducing computational complexity compared to the conventional baseline method, i.e., deep fully connected neural network. The proposed LAE is designed as a three-layer undercomplete network to exploit its dimensionality reduction capabilities and thereby mitigate the computational requirements of the trained model. Simulation results show that the proposed model achieves a similar beam prediction accuracy to the baseline with an 83% complexity reduction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints</title>
<link>https://arxiv.org/abs/2511.18606</link>
<guid>https://arxiv.org/abs/2511.18606</guid>
<content:encoded><![CDATA[
arXiv:2511.18606v1 Announce Type: cross 
Abstract: Latent safety filters extend Hamilton-Jacobi (HJ) reachability to operate on latent state representations and dynamics learned directly from high-dimensional observations, enabling safe visuomotor control under hard-to-model constraints. However, existing methods implement "least-restrictive" filtering that discretely switch between nominal and safety policies, potentially undermining the task performance that makes modern visuomotor policies valuable. While reachability value functions can, in principle, be adapted to be control barrier functions (CBFs) for smooth optimization-based filtering, we theoretically and empirically show that current latent-space learning methods produce fundamentally incompatible value functions. We identify two sources of incompatibility: First, in HJ reachability, failures are encoded via a "margin function" in latent space, whose sign indicates whether or not a latent is in the constraint set. However, representing the margin function as a classifier yields saturated value functions that exhibit discontinuous jumps. We prove that the value function's Lipschitz constant scales linearly with the margin function's Lipschitz constant, revealing that smooth CBFs require smooth margins. Second, reinforcement learning (RL) approximations trained solely on safety policy data yield inaccurate value estimates for nominal policy actions, precisely where CBF filtering needs them. We propose the LatentCBF, which addresses both challenges through gradient penalties that lead to smooth margin functions without additional labeling, and a value-training procedure that mixes data from both nominal and safety policy distributions. Experiments on simulated benchmarks and hardware with a vision-based manipulation policy demonstrate that LatentCBF enables smooth safety filtering while doubling the task-completion rate over prior switching methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Localization Enforced Deep Anomaly Detection Using Fundus Images</title>
<link>https://arxiv.org/abs/2511.18627</link>
<guid>https://arxiv.org/abs/2511.18627</guid>
<content:encoded><![CDATA[
arXiv:2511.18627v1 Announce Type: cross 
Abstract: Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings.
  On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Philosophy and Machine Learning: A Structuralist Framework for Classifying Neural Network Representations</title>
<link>https://arxiv.org/abs/2511.18633</link>
<guid>https://arxiv.org/abs/2511.18633</guid>
<content:encoded><![CDATA[
arXiv:2511.18633v1 Announce Type: cross 
Abstract: Machine learning models increasingly function as representational systems, yet the philosoph- ical assumptions underlying their internal structures remain largely unexamined. This paper develops a structuralist decision framework for classifying the implicit ontological commitments made in machine learning research on neural network representations. Using a modified PRISMA protocol, a systematic review of the last two decades of literature on representation learning and interpretability is conducted. Five influential papers are analysed through three hierarchical criteria derived from structuralist philosophy of science: entity elimination, source of structure, and mode of existence. The results reveal a pronounced tendency toward structural idealism, where learned representations are treated as model-dependent constructions shaped by architec- ture, data priors, and training dynamics. Eliminative and non-eliminative structuralist stances appear selectively, while structural realism is notably absent. The proposed framework clarifies conceptual tensions in debates on interpretability, emergence, and epistemic trust in machine learning, and offers a rigorous foundation for future interdisciplinary work between philosophy of science and machine learning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Health system learning achieves generalist neuroimaging models</title>
<link>https://arxiv.org/abs/2511.18640</link>
<guid>https://arxiv.org/abs/2511.18640</guid>
<content:encoded><![CDATA[
arXiv:2511.18640v1 Announce Type: cross 
Abstract: Frontier artificial intelligence (AI) models, such as OpenAI's GPT-5 and Meta's DINOv3, have advanced rapidly through training on internet-scale public data, yet such systems lack access to private clinical data. Neuroimaging, in particular, is underrepresented in the public domain due to identifiable facial features within MRI and CT scans, fundamentally restricting model performance in clinical medicine. Here, we show that frontier models underperform on neuroimaging tasks and that learning directly from uncurated data generated during routine clinical care at health systems, a paradigm we call health system learning, yields high-performance, generalist neuroimaging models. We introduce NeuroVFM, a visual foundation model trained on 5.24 million clinical MRI and CT volumes using a scalable volumetric joint-embedding predictive architecture. NeuroVFM learns comprehensive representations of brain anatomy and pathology, achieving state-of-the-art performance across multiple clinical tasks, including radiologic diagnosis and report generation. The model exhibits emergent neuroanatomic understanding and interpretable visual grounding of diagnostic findings. When paired with open-source language models through lightweight visual instruction tuning, NeuroVFM generates radiology reports that surpass frontier models in accuracy, clinical triage, and expert preference. Through clinically grounded visual understanding, NeuroVFM reduces hallucinated findings and critical errors, offering safer clinical decision support. These results establish health system learning as a paradigm for building generalist medical AI and provide a scalable framework for clinical foundation models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lean 5.0: A Predictive, Human-AI, and Ethically Grounded Paradigm for Construction Management</title>
<link>https://arxiv.org/abs/2511.18651</link>
<guid>https://arxiv.org/abs/2511.18651</guid>
<content:encoded><![CDATA[
arXiv:2511.18651v1 Announce Type: cross 
Abstract: This paper introduces Lean 5.0, a human-centric evolution of Lean-Digital integration that connects predictive analytics, AI collaboration, and continuous learning within Industry 5.0 and Construction 5.0 contexts. A systematic literature review (2019-2024) and a 12-week empirical validation study demonstrate measurable performance gains, including a 13% increase in Plan Percent Complete (PPC), 22% reduction in rework, and 42% improvement in forecast accuracy. The study adopts a mixed-method Design Science Research (DSR) approach aligned with PRISMA 2020 guidelines. The paper also examines integration with digital twin and blockchain technologies to improve traceability, auditability, and lifecycle transparency. Despite limitations related to sample size, single-case design, and study duration, the findings show that Lean 5.0 provides a transformative paradigm connecting human cognition with predictive control in construction management.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FHE-Agent: Automating CKKS Configuration for Practical Encrypted Inference via an LLM-Guided Agentic Framework</title>
<link>https://arxiv.org/abs/2511.18653</link>
<guid>https://arxiv.org/abs/2511.18653</guid>
<content:encoded><![CDATA[
arXiv:2511.18653v1 Announce Type: cross 
Abstract: Fully Homomorphic Encryption (FHE), particularly the CKKS scheme, is a promising enabler for privacy-preserving MLaaS, but its practical deployment faces a prohibitive barrier: it heavily relies on domain expertise. Configuring CKKS involves a tightly coupled space of ring dimensions, modulus chains, and packing layouts. Without deep cryptographic knowledge to navigate these interactions, practitioners are restricted to compilers that rely on fixed heuristics. These "one-shot" tools often emit rigid configurations that are either severely over-provisioned in latency or fail to find a feasible solution entirely for deeper networks.
  We present FHE-Agent, an agentic framework that automates this expert reasoning process. By coupling a Large Language Model (LLM) controller with a deterministic tool suite, FHE-Agent decomposes the search into global parameter selection and layer-wise bottleneck repair. The agents operate within a multi-fidelity workflow, pruning invalid regimes using cheap static analysis and reserving expensive encrypted evaluations for the most promising candidates.
  We instantiate FHE-Agent on the Orion compiler and evaluate it on standard benchmarks (MLP, LeNet, LoLa) and deeper architectures (AlexNet). FHE-Agent consistently achieves better precision and lower latency than na\"ive search strategies. Crucially, it automatically discovers feasible, 128-bit secure configurations for complex models where baseline heuristics and one-shot prompts fail to produce a valid setup.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Escape, Slow Convergence: Learning Dynamics of Phase Retrieval under Power-Law Data</title>
<link>https://arxiv.org/abs/2511.18661</link>
<guid>https://arxiv.org/abs/2511.18661</guid>
<content:encoded><![CDATA[
arXiv:2511.18661v1 Announce Type: cross 
Abstract: Scaling laws describe how learning performance improves with data, compute, or training time, and have become a central theme in modern deep learning. We study this phenomenon in a canonical nonlinear model: phase retrieval with anisotropic Gaussian inputs whose covariance spectrum follows a power law. Unlike the isotropic case, where dynamics collapse to a two-dimensional system, anisotropy yields a qualitatively new regime in which an infinite hierarchy of coupled equations governs the evolution of the summary statistics. We develop a tractable reduction that reveals a three-phase trajectory: (i) fast escape from low alignment, (ii) slow convergence of the summary statistics, and (iii) spectral-tail learning in low-variance directions. From this decomposition, we derive explicit scaling laws for the mean-squared error, showing how spectral decay dictates convergence times and error curves. Experiments confirm the predicted phases and exponents. These results provide the first rigorous characterization of scaling laws in nonlinear regression with anisotropic data, highlighting how anisotropy reshapes learning dynamics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Deep Equilibrium Models for Imaging Inverse Problems</title>
<link>https://arxiv.org/abs/2511.18667</link>
<guid>https://arxiv.org/abs/2511.18667</guid>
<content:encoded><![CDATA[
arXiv:2511.18667v1 Announce Type: cross 
Abstract: Equivariant imaging (EI) enables training signal reconstruction models without requiring ground truth data by leveraging signal symmetries. Deep equilibrium models (DEQs) are a powerful class of neural networks where the output is a fixed point of a learned operator. However, training DEQs with complex EI losses requires implicit differentiation through fixed-point computations, whose implementation can be challenging. We show that backpropagation can be implemented modularly, simplifying training. Experiments demonstrate that DEQs trained with implicit differentiation outperform those trained with Jacobian-free backpropagation and other baseline methods. Additionally, we find evidence that EI-trained DEQs approximate the proximal map of an invariant prior.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration</title>
<link>https://arxiv.org/abs/2511.18674</link>
<guid>https://arxiv.org/abs/2511.18674</guid>
<content:encoded><![CDATA[
arXiv:2511.18674v1 Announce Type: cross 
Abstract: Large matrix multiplication is a cornerstone of modern machine learning workloads, yet traditional approaches suffer from cubic computational complexity (e.g., $\mathcal{O}(n^3)$ for a matrix of size $n\times n$). We present Low-Rank GEMM, a novel approach that leverages low-rank matrix approximations to achieve sub-quadratic complexity while maintaining hardware-accelerated performance through FP8 precision and intelligent kernel selection. On a NVIDIA RTX 4090, our implementation achieves up to 378 TFLOPS on matrices up to $N=20480$, providing 75\% memory savings and $7.8\times$ speedup over PyTorch FP32 for large matrices. The system automatically adapts to hardware capabilities, selecting optimal decomposition methods (SVD, randomized SVD) and precision levels based on matrix characteristics and available accelerators. Comprehensive benchmarking on NVIDIA RTX 4090 demonstrates that Low-Rank GEMM becomes the fastest approach for matrices $N\geq10240$, surpassing traditional cuBLAS implementations through memory bandwidth optimization rather than computational shortcuts.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Real-Time Anomaly Detection and Industrial Applications</title>
<link>https://arxiv.org/abs/2511.18698</link>
<guid>https://arxiv.org/abs/2511.18698</guid>
<content:encoded><![CDATA[
arXiv:2511.18698v1 Announce Type: cross 
Abstract: This paper presents the design, implementation, and evolution of a comprehensive multimodal room-monitoring system that integrates synchronized video and audio processing for real-time activity recognition and anomaly detection. We describe two iterations of the system: an initial lightweight implementation using YOLOv8, ByteTrack, and the Audio Spectrogram Transformer (AST), and an advanced version that incorporates multi-model audio ensembles, hybrid object detection, bidirectional cross-modal attention, and multi-method anomaly detection. The evolution demonstrates significant improvements in accuracy, robustness, and industrial applicability. The advanced system combines three audio models (AST, Wav2Vec2, and HuBERT) for comprehensive audio understanding, dual object detectors (YOLO and DETR) for improved accuracy, and sophisticated fusion mechanisms for enhanced cross-modal learning. Experimental evaluation shows the system's effectiveness in general monitoring scenarios as well as specialized industrial safety applications, achieving real-time performance on standard hardware while maintaining high accuracy.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dendritic Convolution for Noise Image Recognition</title>
<link>https://arxiv.org/abs/2511.18699</link>
<guid>https://arxiv.org/abs/2511.18699</guid>
<content:encoded><![CDATA[
arXiv:2511.18699v1 Announce Type: cross 
Abstract: In real-world scenarios of image recognition, there exists substantial noise interference. Existing works primarily focus on methods such as adjusting networks or training strategies to address noisy image recognition, and the anti-noise performance has reached a bottleneck. However, little is known about the exploration of anti-interference solutions from a neuronal perspective.This paper proposes an anti-noise neuronal convolution. This convolution mimics the dendritic structure of neurons, integrates the neighborhood interaction computation logic of dendrites into the underlying design of convolutional operations, and simulates the XOR logic preprocessing function of biological dendrites through nonlinear interactions between input features, thereby fundamentally reconstructing the mathematical paradigm of feature extraction. Unlike traditional convolution where noise directly interferes with feature extraction and exerts a significant impact, DDC mitigates the influence of noise by focusing on the interaction of neighborhood information. Experimental results demonstrate that in image classification tasks (using YOLOv11-cls, VGG16, and EfficientNet-B0) and object detection tasks (using YOLOv11, YOLOv8, and YOLOv5), after replacing traditional convolution with the dendritic convolution, the accuracy of the EfficientNet-B0 model on noisy datasets is relatively improved by 11.23%, and the mean Average Precision (mAP) of YOLOv8 is increased by 19.80%. The consistency between the computation method of this convolution and the dendrites of biological neurons enables it to perform significantly better than traditional convolution in complex noisy environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction</title>
<link>https://arxiv.org/abs/2511.18701</link>
<guid>https://arxiv.org/abs/2511.18701</guid>
<content:encoded><![CDATA[
arXiv:2511.18701v1 Announce Type: cross 
Abstract: Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed "consistent" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When and What to Recommend: Joint Modeling of Timing and Content for Active Sequential Recommendation</title>
<link>https://arxiv.org/abs/2511.18717</link>
<guid>https://arxiv.org/abs/2511.18717</guid>
<content:encoded><![CDATA[
arXiv:2511.18717v1 Announce Type: cross 
Abstract: Sequential recommendation models user preferences to predict the next target item. Most existing work is passive, where the system responds only when users open the application, missing chances after closure. We investigate active recommendation, which predicts the next interaction time and actively delivers items. Two challenges: accurately estimating the Time of Interest (ToI) and generating Item of Interest (IoI) conditioned on the predicted ToI. We propose PASRec, a diffusion-based framework that aligns ToI and IoI via a joint objective. Experiments on five benchmarks show superiority over eight state-of-the-art baselines under leave-one-out and temporal splits.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Problem-Oriented Taxonomy of Evaluation Metrics for Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.18739</link>
<guid>https://arxiv.org/abs/2511.18739</guid>
<content:encoded><![CDATA[
arXiv:2511.18739v1 Announce Type: cross 
Abstract: Time series anomaly detection is widely used in IoT and cyber-physical systems, yet its evaluation remains challenging due to diverse application objectives and heterogeneous metric assumptions. This study introduces a problem-oriented framework that reinterprets existing metrics based on the specific evaluation challenges they are designed to address, rather than their mathematical forms or output structures. We categorize over twenty commonly used metrics into six dimensions: 1) basic accuracy-driven evaluation; 2) timeliness-aware reward mechanisms; 3) tolerance to labeling imprecision; 4) penalties reflecting human-audit cost; 5) robustness against random or inflated scores; and 6) parameter-free comparability for cross-dataset benchmarking. Comprehensive experiments are conducted to examine metric behavior under genuine, random, and oracle detection scenarios. By comparing their resulting score distributions, we quantify each metric's discriminative ability -- its capability to distinguish meaningful detections from random noise. The results show that while most event-level metrics exhibit strong separability, several widely used metrics (e.g., NAB, Point-Adjust) demonstrate limited resistance to random-score inflation. These findings reveal that metric suitability must be inherently task-dependent and aligned with the operational objectives of IoT applications. The proposed framework offers a unified analytical perspective for understanding existing metrics and provides practical guidance for selecting or developing more context-aware, robust, and fair evaluation methodologies for time series anomaly detection.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion</title>
<link>https://arxiv.org/abs/2511.18742</link>
<guid>https://arxiv.org/abs/2511.18742</guid>
<content:encoded><![CDATA[
arXiv:2511.18742v1 Announce Type: cross 
Abstract: Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Instability of Minimax Optimal Optimism-Based Bandit Algorithms</title>
<link>https://arxiv.org/abs/2511.18750</link>
<guid>https://arxiv.org/abs/2511.18750</guid>
<content:encoded><![CDATA[
arXiv:2511.18750v1 Announce Type: cross 
Abstract: Statistical inference from data generated by multi-armed bandit (MAB) algorithms is challenging due to their adaptive, non-i.i.d. nature. A classical manifestation is that sample averages of arm rewards under bandit sampling may fail to satisfy a central limit theorem. Lai and Wei's stability condition provides a sufficient, and essentially necessary criterion, for asymptotic normality in bandit problems. While the celebrated Upper Confidence Bound (UCB) algorithm satisfies this stability condition, it is not minimax optimal, raising the question of whether minimax optimality and statistical stability can be achieved simultaneously. In this paper, we analyze the stability properties of a broad class of bandit algorithms that are based on the optimism principle. We establish general structural conditions under which such algorithms violate the Lai-Wei stability criterion. As a consequence, we show that widely used minimax-optimal UCB-style algorithms, including MOSS, Anytime-MOSS, Vanilla-MOSS, ADA-UCB, OC-UCB, KL-MOSS, KL-UCB++, KL-UCB-SWITCH, and Anytime KL-UCB-SWITCH, are unstable. We further complement our theoretical results with numerical simulations demonstrating that, in all these cases, the sample means fail to exhibit asymptotic normality.
  Overall, our findings suggest a fundamental tension between stability and minimax optimal regret, raising the question of whether it is possible to design bandit algorithms that achieve both. Understanding whether such simultaneously stable and minimax optimal strategies exist remains an important open direction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Task Transfer in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.18787</link>
<guid>https://arxiv.org/abs/2511.18787</guid>
<content:encoded><![CDATA[
arXiv:2511.18787v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations</title>
<link>https://arxiv.org/abs/2511.18793</link>
<guid>https://arxiv.org/abs/2511.18793</guid>
<content:encoded><![CDATA[
arXiv:2511.18793v1 Announce Type: cross 
Abstract: Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services and limits their overall business impact. While Speculative Decoding (SD) has been proposed to accelerate the autoregressive generation process, existing implementations introduce new bottlenecks: they typically require separate draft models and model-based verifiers, requiring additional training and increasing the latency overhead. In this paper, we address these challenges with NEZHA, a novel architecture that achieves hyperspeed decoding for GR systems without sacrificing recommendation quality. Specifically, NEZHA integrates a nimble autoregressive draft head directly into the primary model, enabling efficient self-drafting. This design, combined with a specialized input prompt structure, preserves the integrity of sequence-to-sequence generation. Furthermore, to tackle the critical problem of hallucination, a major source of performance degradation, we introduce an efficient, model-free verifier based on a hash set. We demonstrate the effectiveness of NEZHA through extensive experiments on public datasets and have successfully deployed the system on Taobao since October 2025, driving the billion-level advertising revenue and serving hundreds of millions of daily active users.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty of Network Topology with Applications to Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2511.18813</link>
<guid>https://arxiv.org/abs/2511.18813</guid>
<content:encoded><![CDATA[
arXiv:2511.18813v1 Announce Type: cross 
Abstract: Persistent homology (PH) is a crucial concept in computational topology, providing a multiscale topological description of a space. It is particularly significant in topological data analysis, which aims to make statistical inference from a topological perspective. In this work, we introduce a new topological summary for Bayesian neural networks, termed the predictive topological uncertainty (pTU). The proposed pTU measures the uncertainty in the interaction between the model and the inputs. It provides insights from the model perspective: if two samples interact with a model in a similar way, then they are considered identically distributed. We also show that the pTU is insensitive to the model architecture. As an application, pTU is used to solve the out-of-distribution (OOD) detection problem, which is critical to ensure model reliability. Failure to detect OOD input can lead to incorrect and unreliable predictions. To address this issue, we propose a significance test for OOD based on the pTU, providing a statistical framework for this issue. The effectiveness of the framework is validated through various experiments, in terms of its statistical power, sensitivity, and robustness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solution of Incompressible Flow Equations with Physics and Equality Constrained Artificial Neural Networks</title>
<link>https://arxiv.org/abs/2511.18820</link>
<guid>https://arxiv.org/abs/2511.18820</guid>
<content:encoded><![CDATA[
arXiv:2511.18820v1 Announce Type: cross 
Abstract: We present a meshless method for the solution of incompressible Navier-Stokes equations in advection-dominated regimes using physics- and equality-constrained artificial neural networks combined with a conditionally adaptive augmented Lagrangian formulation. A single neural network parameterizes both the velocity and pressure fields, and is trained by minimizing the residual of a Poisson's equation for pressure, constrained by the momentum and continuity equations, together with boundary conditions on the velocity field. No boundary conditions are imposed on the pressure field aside from anchoring the pressure at a point to prevent its unbounded development. The training is performed from scratch without labeled data, relying solely on the governing equations and constraints. To enhance accuracy in advection-dominated flows, we employ a single Fourier feature mapping of the input coordinates. The proposed method is demonstrated for the canonical lid-driven cavity flow up to a Reynolds number of 7,500 and for laminar flow over a circular cylinder with inflow-outflow boundary conditions, achieving excellent agreement with benchmark solutions. We further compare the present formulation against alternative objective-function constructions based on different arrangements of the flow equations, thereby highlighting the algorithmic advantages of the proposed formulation centered around the Poisson's equation for pressure.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification</title>
<link>https://arxiv.org/abs/2511.18826</link>
<guid>https://arxiv.org/abs/2511.18826</guid>
<content:encoded><![CDATA[
arXiv:2511.18826v1 Announce Type: cross 
Abstract: Knowledge distillation has emerged as a powerful technique for model compression, enabling the transfer of knowledge from large teacher networks to compact student models. However, traditional knowledge distillation methods treat all teacher predictions equally, regardless of the teacher's confidence in those predictions. This paper proposes an uncertainty-aware dual-student knowledge distillation framework that leverages teacher prediction uncertainty to selectively guide student learning. We introduce a peer-learning mechanism where two heterogeneous student architectures, specifically ResNet-18 and MobileNetV2, learn collaboratively from both the teacher network and each other. Experimental results on ImageNet-100 demonstrate that our approach achieves superior performance compared to baseline knowledge distillation methods, with ResNet-18 achieving 83.84\% top-1 accuracy and MobileNetV2 achieving 81.46\% top-1 accuracy, representing improvements of 2.04\% and 0.92\% respectively over traditional single-student distillation approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving a Research Problem in Mathematical Statistics with AI Assistance</title>
<link>https://arxiv.org/abs/2511.18828</link>
<guid>https://arxiv.org/abs/2511.18828</guid>
<content:encoded><![CDATA[
arXiv:2511.18828v1 Announce Type: cross 
Abstract: Over the last few months, AI models including large language models have improved greatly. There are now several documented examples where they have helped professional mathematical scientists prove new results, sometimes even helping resolve known open problems. In this short note, we add another example to the list, by documenting how we were able to solve a previously unsolved research problem in robust mathematical statistics with crucial help from GPT-5. Our problem concerns robust density estimation, where the observations are perturbed by Wasserstein-bounded contaminations.In a previous preprint (Chao and Dobriban, 2023, arxiv:2308.01853v2), we have obtained upper and lower bounds on the minimax optimal estimation error; which were, however, not sharp.
  Starting in October 2025, making significant use of GPT-5 Pro, we were able to derive the minimax optimal error rate (reported in version 3 of the above arxiv preprint). GPT-5 provided crucial help along the way, including by suggesting calculations that we did not think of, and techniques that were not familiar to us, such as the dynamic Benamou-Brenier formulation, for key steps in the analysis. Working with GPT-5 took a few weeks of effort, and we estimate that it could have taken several months to get the same results otherwise. At the same time, there are still areas where working with GPT-5 was challenging: it sometimes provided incorrect references, and glossed over details that sometimes took days of work to fill in. We outline our workflow and steps taken to mitigate issues. Overall, our work can serve as additional documentation for a new age of human-AI collaborative work in mathematical science.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Multi-Label Thoracic Disease Diagnosis with Deep Ensemble-Based Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2511.18839</link>
<guid>https://arxiv.org/abs/2511.18839</guid>
<content:encoded><![CDATA[
arXiv:2511.18839v1 Announce Type: cross 
Abstract: The utility of deep learning models, such as CheXNet, in high stakes clinical settings is fundamentally constrained by their purely deterministic nature, failing to provide reliable measures of predictive confidence. This project addresses this critical gap by integrating robust Uncertainty Quantification (UQ) into a high performance diagnostic platform for 14 common thoracic diseases on the NIH ChestX-ray14 dataset. Initial architectural development failed to stabilize performance and calibration using Monte Carlo Dropout (MCD), yielding an unacceptable Expected Calibration Error (ECE) of 0.7588. This technical failure necessitated a rigorous architectural pivot to a high diversity, 9-member Deep Ensemble (DE). This resulting DE successfully stabilized performance and delivered superior reliability, achieving a State-of-the-Art (SOTA) average Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.8559 and an average F1 Score of 0.3857. Crucially, the DE demonstrated superior calibration (Mean ECE of 0.0728 and Negative Log-Likelihood (NLL) of 0.1916) and enabled the reliable decomposition of total uncertainty into its Aleatoric (irreducible data noise) and Epistemic (reducible model knowledge) components, with a mean Epistemic Uncertainty (EU) of 0.0240. These results establish the Deep Ensemble as a trustworthy and explainable platform, transforming the model from a probabilistic tool into a reliable clinical decision support system.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis</title>
<link>https://arxiv.org/abs/2511.18843</link>
<guid>https://arxiv.org/abs/2511.18843</guid>
<content:encoded><![CDATA[
arXiv:2511.18843v1 Announce Type: cross 
Abstract: Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a rigorous, reproducible computational framework for applying neural topic modeling to focus group transcripts, addressing fundamental methodological challenges: hyperparameter sensitivity, model stability, and validation of interpretability. Using BERTopic applied to ten focus groups exploring HPV vaccine perceptions in Tunisia (1,076 utterances), we conducted systematic evaluation across 27 hyperparameter configurations, assessed stability through bootstrap resampling with 30 replicates per configuration, and validated interpretability through formal human evaluation by three domain experts. Our analysis demonstrates substantial sensitivity to hyperparameter choices and reveals that metric selection for stability assessment must align with analytical goals. A hierarchical merging strategy (extracting fine-grained topics for stability then consolidating for interpretability) effectively navigates the stability-coherence tradeoff, achieving coherence of 0.558 compared to 0.539 for direct extraction. Human validation confirmed topic quality with very good inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578). Our framework provides practical guidelines that researchers can adapt to their own qualitative research contexts. All code, data processing scripts, and evaluation protocols are publicly available to support reproduction and extension of this work.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction</title>
<link>https://arxiv.org/abs/2511.18874</link>
<guid>https://arxiv.org/abs/2511.18874</guid>
<content:encoded><![CDATA[
arXiv:2511.18874v1 Announce Type: cross 
Abstract: Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness Meets Privacy: Integrating Differential Privacy and Demographic Parity in Multi-class Classification</title>
<link>https://arxiv.org/abs/2511.18876</link>
<guid>https://arxiv.org/abs/2511.18876</guid>
<content:encoded><![CDATA[
arXiv:2511.18876v1 Announce Type: cross 
Abstract: The increasing use of machine learning in sensitive applications demands algorithms that simultaneously preserve data privacy and ensure fairness across potentially sensitive sub-populations. While privacy and fairness have each been extensively studied, their joint treatment remains poorly understood. Existing research often frames them as conflicting objectives, with multiple studies suggesting that strong privacy notions such as differential privacy inevitably compromise fairness. In this work, we challenge that perspective by showing that differential privacy can be integrated into a fairness-enhancing pipeline with minimal impact on fairness guarantees. We design a postprocessing algorithm, called DP2DP, that enforces both demographic parity and differential privacy. Our analysis reveals that our algorithm converges towards its demographic parity objective at essentially the same rate (up logarithmic factor) as the best non-private methods from the literature. Experiments on both synthetic and real datasets confirm our theoretical results, showing that the proposed algorithm achieves state-of-the-art accuracy/fairness/privacy trade-offs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation</title>
<link>https://arxiv.org/abs/2511.18950</link>
<guid>https://arxiv.org/abs/2511.18950</guid>
<content:encoded><![CDATA[
arXiv:2511.18950v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classification EM-PCA for clustering and embedding</title>
<link>https://arxiv.org/abs/2511.18992</link>
<guid>https://arxiv.org/abs/2511.18992</guid>
<content:encoded><![CDATA[
arXiv:2511.18992v1 Announce Type: cross 
Abstract: The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding, Accelerating, and Improving MeanFlow Training</title>
<link>https://arxiv.org/abs/2511.19065</link>
<guid>https://arxiv.org/abs/2511.19065</guid>
<content:encoded><![CDATA[
arXiv:2511.19065v1 Announce Type: cross 
Abstract: MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling</title>
<link>https://arxiv.org/abs/2511.19067</link>
<guid>https://arxiv.org/abs/2511.19067</guid>
<content:encoded><![CDATA[
arXiv:2511.19067v1 Announce Type: cross 
Abstract: Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Matching via Cost-Regularized Unbalanced Optimal Transport</title>
<link>https://arxiv.org/abs/2511.19075</link>
<guid>https://arxiv.org/abs/2511.19075</guid>
<content:encoded><![CDATA[
arXiv:2511.19075v1 Announce Type: cross 
Abstract: Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting Robust Register Automata from Neural Networks over Data Sequences</title>
<link>https://arxiv.org/abs/2511.19100</link>
<guid>https://arxiv.org/abs/2511.19100</guid>
<content:encoded><![CDATA[
arXiv:2511.19100v1 Announce Type: cross 
Abstract: Automata extraction is a method for synthesising interpretable surrogates for black-box neural models that can be analysed symbolically. Existing techniques assume a finite input alphabet, and thus are not directly applicable to data sequences drawn from continuous domains. We address this challenge with deterministic register automata (DRAs), which extend finite automata with registers that store and compare numeric values. Our main contribution is a framework for robust DRA extraction from black-box models: we develop a polynomial-time robustness checker for DRAs with a fixed number of registers, and combine it with passive and active automata learning algorithms. This combination yields surrogate DRAs with statistical robustness and equivalence guarantees. As a key application, we use the extracted automata to assess the robustness of neural networks: for a given sequence and distance metric, the DRA either certifies local robustness or produces a concrete counterexample. Experiments on recurrent neural networks and transformer architectures show that our framework reliably learns accurate automata and enables principled robustness evaluation. Overall, our results demonstrate that robust DRA extraction effectively bridges neural network interpretability and formal reasoning without requiring white-box access to the underlying network.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation</title>
<link>https://arxiv.org/abs/2511.19147</link>
<guid>https://arxiv.org/abs/2511.19147</guid>
<content:encoded><![CDATA[
arXiv:2511.19147v1 Announce Type: cross 
Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Ranking in Credit-Risk with Qudit-Based Networks</title>
<link>https://arxiv.org/abs/2511.19150</link>
<guid>https://arxiv.org/abs/2511.19150</guid>
<content:encoded><![CDATA[
arXiv:2511.19150v1 Announce Type: cross 
Abstract: In finance, predictive models must balance accuracy and interpretability, particularly in credit risk assessment, where model decisions carry material consequences. We present a quantum neural network (QNN) based on a single qudit, in which both data features and trainable parameters are co-encoded within a unified unitary evolution generated by the full Lie algebra. This design explores the entire Hilbert space while enabling interpretability through the magnitudes of the learned coefficients. We benchmark our model on a real-world, imbalanced credit-risk dataset from Taiwan. The proposed QNN consistently outperforms LR and reaches the results of random forest models in macro-F1 score while preserving a transparent correspondence between learned parameters and input feature importance. To quantify the interpretability of the proposed model, we introduce two complementary metrics: (i) the edit distance between the model's feature ranking and that of LR, and (ii) a feature-poisoning test where selected features are replaced with noise. Results indicate that the proposed quantum model achieves competitive performance while offering a tractable path toward interpretable quantum learning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Robust State Filter Against Unmodeled Process And Measurement Noise</title>
<link>https://arxiv.org/abs/2511.19157</link>
<guid>https://arxiv.org/abs/2511.19157</guid>
<content:encoded><![CDATA[
arXiv:2511.19157v1 Announce Type: cross 
Abstract: This paper introduces a novel Kalman filter framework designed to achieve robust state estimation under both process and measurement noise. Inspired by the Weighted Observation Likelihood Filter (WoLF), which provides robustness against measurement outliers, we applied generalized Bayesian approach to build a framework considering both process and measurement noise outliers.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioArtlas: Computational Clustering of Multi-Dimensional Complexity in Bioart</title>
<link>https://arxiv.org/abs/2511.19162</link>
<guid>https://arxiv.org/abs/2511.19162</guid>
<content:encoded><![CDATA[
arXiv:2511.19162v1 Announce Type: cross 
Abstract: Bioart's hybrid nature spanning art, science, technology, ethics, and politics defies traditional single-axis categorization. I present BioArtlas, analyzing 81 bioart works across thirteen curated dimensions using novel axis-aware representations that preserve semantic distinctions while enabling cross-dimensional comparison. Our codebook-based approach groups related concepts into unified clusters, addressing polysemy in cultural terminology. Comprehensive evaluation of up to 800 representation-space-algorithm combinations identifies Agglomerative clustering at k=15 on 4D UMAP as optimal (silhouette 0.664 +/- 0.008, trustworthiness/continuity 0.805/0.812). The approach reveals four organizational patterns: artist-specific methodological cohesion, technique-based segmentation, temporal artistic evolution, and trans-temporal conceptual affinities. By separating analytical optimization from public communication, I provide rigorous analysis and accessible exploration through an interactive web interface (https://www.bioartlas.com) with the dataset publicly available (https://github.com/joonhyungbae/BioArtlas).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection</title>
<link>https://arxiv.org/abs/2511.19187</link>
<guid>https://arxiv.org/abs/2511.19187</guid>
<content:encoded><![CDATA[
arXiv:2511.19187v1 Announce Type: cross 
Abstract: Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLASH: A Benchmark for Cross-Modal Contradiction Detection</title>
<link>https://arxiv.org/abs/2511.19199</link>
<guid>https://arxiv.org/abs/2511.19199</guid>
<content:encoded><![CDATA[
arXiv:2511.19199v1 Announce Type: cross 
Abstract: Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Architecture Search for Quantum Autoencoders</title>
<link>https://arxiv.org/abs/2511.19246</link>
<guid>https://arxiv.org/abs/2511.19246</guid>
<content:encoded><![CDATA[
arXiv:2511.19246v1 Announce Type: cross 
Abstract: In recent years, machine learning and deep learning have driven advances in domains such as image classification, speech recognition, and anomaly detection by leveraging multi-layer neural networks to model complex data. Simultaneously, quantum computing (QC) promises to address classically intractable problems via quantum parallelism, motivating research in quantum machine learning (QML). Among QML techniques, quantum autoencoders show promise for compressing high-dimensional quantum and classical data. However, designing effective quantum circuit architectures for quantum autoencoders remains challenging due to the complexity of selecting gates, arranging circuit layers, and tuning parameters.
  This paper proposes a neural architecture search (NAS) framework that automates the design of quantum autoencoders using a genetic algorithm (GA). By systematically evolving variational quantum circuit (VQC) configurations, our method seeks to identify high-performing hybrid quantum-classical autoencoders for data reconstruction without becoming trapped in local minima. We demonstrate effectiveness on image datasets, highlighting the potential of quantum autoencoders for efficient feature extraction within a noise-prone, near-term quantum era. Our approach lays a foundation for broader application of genetic algorithms to quantum architecture search, aiming for a robust, automated method that can adapt to varied data and hardware constraints.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimDiff: Simpler Yet Better Diffusion Model for Time Series Point Forecasting</title>
<link>https://arxiv.org/abs/2511.19256</link>
<guid>https://arxiv.org/abs/2511.19256</guid>
<content:encoded><![CDATA[
arXiv:2511.19256v1 Announce Type: cross 
Abstract: Diffusion models have recently shown promise in time series forecasting, particularly for probabilistic predictions. However, they often fail to achieve state-of-the-art point estimation performance compared to regression-based methods. This limitation stems from difficulties in providing sufficient contextual bias to track distribution shifts and in balancing output diversity with the stability and precision required for point forecasts. Existing diffusion-based approaches mainly focus on full-distribution modeling under probabilistic frameworks, often with likelihood maximization objectives, while paying little attention to dedicated strategies for high-accuracy point estimation. Moreover, other existing point prediction diffusion methods frequently rely on pre-trained or jointly trained mature models for contextual bias, sacrificing the generative flexibility of diffusion models.
  To address these challenges, we propose SimDiff, a single-stage, end-to-end framework. SimDiff employs a single unified Transformer network carefully tailored to serve as both denoiser and predictor, eliminating the need for external pre-trained or jointly trained regressors. It achieves state-of-the-art point estimation performance by leveraging intrinsic output diversity and improving mean squared error accuracy through multiple inference ensembling. Key innovations, including normalization independence and the median-of-means estimator, further enhance adaptability and stability. Extensive experiments demonstrate that SimDiff significantly outperforms existing methods in time series point forecasting.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2511.19257</link>
<guid>https://arxiv.org/abs/2511.19257</guid>
<content:encoded><![CDATA[
arXiv:2511.19257v1 Announce Type: cross 
Abstract: With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Psychometric Tests for AI Agents and Their Moduli Space</title>
<link>https://arxiv.org/abs/2511.19262</link>
<guid>https://arxiv.org/abs/2511.19262</guid>
<content:encoded><![CDATA[
arXiv:2511.19262v1 Announce Type: cross 
Abstract: We develop a moduli-theoretic view of psychometric test batteries for AI agents and connect it explicitly to the AAI score developed previously. First, we make precise the notion of an AAI functional on a battery and set out axioms that any reasonable autonomy/general intelligence score should satisfy. Second, we show that the composite index ('AAI-Index') defined previously is a special case of our AAI functional. Third, we introduce the notion of a cognitive core of an agent relative to a battery and define the associated AAI$_{\textrm{core}}$ score as the restriction of an AAI functional to that core. Finally, we use these notions to describe invariants of batteries under evaluation-preserving symmetries and outline how moduli of equivalent batteries are organized.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility</title>
<link>https://arxiv.org/abs/2511.19284</link>
<guid>https://arxiv.org/abs/2511.19284</guid>
<content:encoded><![CDATA[
arXiv:2511.19284v1 Announce Type: cross 
Abstract: This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a "Gatekeeper" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance Guarantees for Quantum Neural Estimation of Entropies</title>
<link>https://arxiv.org/abs/2511.19289</link>
<guid>https://arxiv.org/abs/2511.19289</guid>
<content:encoded><![CDATA[
arXiv:2511.19289v1 Announce Type: cross 
Abstract: Estimating quantum entropies and divergences is an important problem in quantum physics, information theory, and machine learning. Quantum neural estimators (QNEs), which utilize a hybrid classical-quantum architecture, have recently emerged as an appealing computational framework for estimating these measures. Such estimators combine classical neural networks with parametrized quantum circuits, and their deployment typically entails tedious tuning of hyperparameters controlling the sample size, network architecture, and circuit topology. This work initiates the study of formal guarantees for QNEs of measured (R\'enyi) relative entropies in the form of non-asymptotic error risk bounds. We further establish exponential tail bounds showing that the error is sub-Gaussian, and thus sharply concentrates about the ground truth value. For an appropriate sub-class of density operator pairs on a space of dimension $d$ with bounded Thompson metric, our theory establishes a copy complexity of $O(|\Theta(\mathcal{U})|d/\epsilon^2)$ for QNE with a quantum circuit parameter set $\Theta(\mathcal{U})$, which has minimax optimal dependence on the accuracy $\epsilon$. Additionally, if the density operator pairs are permutation invariant, we improve the dimension dependence above to $O(|\Theta(\mathcal{U})|\mathrm{polylog}(d)/\epsilon^2)$. Our theory aims to facilitate principled implementation of QNEs for measured relative entropies and guide hyperparameter tuning in practice.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TorchQuantumDistributed</title>
<link>https://arxiv.org/abs/2511.19291</link>
<guid>https://arxiv.org/abs/2511.19291</guid>
<content:encoded><![CDATA[
arXiv:2511.19291v1 Announce Type: cross 
Abstract: TorchQuantumDistributed (tqd) is a PyTorch-based [Paszke et al., 2019] library for accelerator-agnostic differentiable quantum state vector simulation at scale. This enables studying the behavior of learnable parameterized near-term and fault- tolerant quantum circuits with high qubit counts.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning</title>
<link>https://arxiv.org/abs/2511.19304</link>
<guid>https://arxiv.org/abs/2511.19304</guid>
<content:encoded><![CDATA[
arXiv:2511.19304v1 Announce Type: cross 
Abstract: Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRInTS: Reward Modeling for Long-Horizon Information Seeking</title>
<link>https://arxiv.org/abs/2511.19314</link>
<guid>https://arxiv.org/abs/2511.19314</guid>
<content:encoded><![CDATA[
arXiv:2511.19314v1 Announce Type: cross 
Abstract: Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-throughput validation of phase formability and simulation accuracy of Cantor alloys</title>
<link>https://arxiv.org/abs/2511.19335</link>
<guid>https://arxiv.org/abs/2511.19335</guid>
<content:encoded><![CDATA[
arXiv:2511.19335v1 Announce Type: cross 
Abstract: High-throughput methods enable accelerated discovery of novel materials in complex systems such as high-entropy alloys, which exhibit intricate phase stability across vast compositional spaces. Computational approaches, including Density Functional Theory (DFT) and calculation of phase diagrams (CALPHAD), facilitate screening of phase formability as a function of composition and temperature. However, the integration of computational predictions with experimental validation remains challenging in high-throughput studies. In this work, we introduce a quantitative confidence metric to assess the agreement between predictions and experimental observations, providing a quantitative measure of the confidence of machine learning models trained on either DFT or CALPHAD input in accounting for experimental evidence. The experimental dataset was generated via high-throughput in-situ synchrotron X-ray diffraction on compositionally varied FeNiMnCr alloy libraries, heated from room temperature to ~1000 {\deg}C. Agreement between the observed and predicted phases was evaluated using either temperature-independent phase classification or a model that incorporates a temperature-dependent probability of phase formation. This integrated approach demonstrates where strong overall agreement between computation and experiment exists, while also identifying key discrepancies, particularly in FCC/BCC predictions at Mn-rich regions to inform future model refinement.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence Driven Workflow for Accelerating Design of Novel Photosensitizers</title>
<link>https://arxiv.org/abs/2511.19347</link>
<guid>https://arxiv.org/abs/2511.19347</guid>
<content:encoded><![CDATA[
arXiv:2511.19347v1 Announce Type: cross 
Abstract: The discovery of high-performance photosensitizers has long been hindered by the time-consuming and resource-intensive nature of traditional trial-and-error approaches. Here, we present \textbf{A}I-\textbf{A}ccelerated \textbf{P}hoto\textbf{S}ensitizer \textbf{I}nnovation (AAPSI), a closed-loop workflow that integrates expert knowledge, scaffold-based molecule generation, and Bayesian optimization to accelerate the design of novel photosensitizers. The scaffold-driven generation in AAPSI ensures structural novelty and synthetic feasibility, while the iterative AI-experiment loop accelerates the discovery of novel photosensitizers. AAPSI leverages a curated database of 102,534 photosensitizer-solvent pairs and generate 6,148 synthetically accessible candidates. These candidates are screened via graph transformers trained to predict singlet oxygen quantum yield ($\phi_\Delta$) and absorption maxima ($\lambda_{max}$), following experimental validation. This work generates several novel candidates for photodynamic therapy (PDT), among which the hypocrellin-based candidate HB4Ph exhibits exceptional performance at the Pareto frontier of high quantum yield of singlet oxygen and long absorption maxima among current photosensitizers ($\phi_\Delta$=0.85, $\lambda_{max}$=650nm).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PTF Testing Lower Bounds for Non-Gaussian Component Analysis</title>
<link>https://arxiv.org/abs/2511.19398</link>
<guid>https://arxiv.org/abs/2511.19398</guid>
<content:encoded><![CDATA[
arXiv:2511.19398v1 Announce Type: cross 
Abstract: This work studies information-computation gaps for statistical problems. A common approach for providing evidence of such gaps is to show sample complexity lower bounds (that are stronger than the information-theoretic optimum) against natural models of computation. A popular such model in the literature is the family of low-degree polynomial tests. While these tests are defined in such a way that make them easy to analyze, the class of algorithms that they rule out is somewhat restricted. An important goal in this context has been to obtain lower bounds against the stronger and more natural class of low-degree Polynomial Threshold Function (PTF) tests, i.e., any test that can be expressed as comparing some low-degree polynomial of the data to a threshold. Proving lower bounds against PTF tests has turned out to be challenging. Indeed, we are not aware of any non-trivial PTF testing lower bounds in the literature.
  In this paper, we establish the first non-trivial PTF testing lower bounds for a range of statistical tasks. Specifically, we prove a near-optimal PTF testing lower bound for Non-Gaussian Component Analysis (NGCA). Our NGCA lower bound implies similar lower bounds for a number of other statistical problems. Our proof leverages a connection to recent work on pseudorandom generators for PTFs and recent techniques developed in that context. At the technical level, we develop several tools of independent interest, including novel structural results for analyzing the behavior of low-degree polynomials restricted to random directions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research</title>
<link>https://arxiv.org/abs/2511.19399</link>
<guid>https://arxiv.org/abs/2511.19399</guid>
<content:encoded><![CDATA[
arXiv:2511.19399v1 Announce Type: cross 
Abstract: Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonparametric Instrumental Variable Regression with Observed Covariates</title>
<link>https://arxiv.org/abs/2511.19404</link>
<guid>https://arxiv.org/abs/2511.19404</guid>
<content:encoded><![CDATA[
arXiv:2511.19404v1 Announce Type: cross 
Abstract: We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2511.19417</link>
<guid>https://arxiv.org/abs/2511.19417</guid>
<content:encoded><![CDATA[
arXiv:2511.19417v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</title>
<link>https://arxiv.org/abs/2511.19418</link>
<guid>https://arxiv.org/abs/2511.19418</guid>
<content:encoded><![CDATA[
arXiv:2511.19418v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts</title>
<link>https://arxiv.org/abs/2511.19434</link>
<guid>https://arxiv.org/abs/2511.19434</guid>
<content:encoded><![CDATA[
arXiv:2511.19434v1 Announce Type: cross 
Abstract: Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection</title>
<link>https://arxiv.org/abs/2511.19436</link>
<guid>https://arxiv.org/abs/2511.19436</guid>
<content:encoded><![CDATA[
arXiv:2511.19436v1 Announce Type: cross 
Abstract: We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Description of Corner Cases in Automated Driving: Goals and Challenges</title>
<link>https://arxiv.org/abs/2109.09607</link>
<guid>https://arxiv.org/abs/2109.09607</guid>
<content:encoded><![CDATA[
arXiv:2109.09607v4 Announce Type: replace 
Abstract: Scaling the distribution of automated vehicles requires handling various unexpected and possibly dangerous situations, termed corner cases (CC). Since many modules of automated driving systems are based on machine learning (ML), CC are an essential part of the data for their development. However, there is only a limited amount of CC data in large-scale data collections, which makes them challenging in the context of ML. With a better understanding of CC, offline applications, e.g., dataset analysis, and online methods, e.g., improved performance of automated driving systems, can be improved. While there are knowledge-based descriptions and taxonomies for CC, there is little research on machine-interpretable descriptions. In this extended abstract, we will give a brief overview of the challenges and goals of such a description.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compressing Sensor Data for Remote Assistance of Autonomous Vehicles using Deep Generative Models</title>
<link>https://arxiv.org/abs/2111.03201</link>
<guid>https://arxiv.org/abs/2111.03201</guid>
<content:encoded><![CDATA[
arXiv:2111.03201v3 Announce Type: replace 
Abstract: In the foreseeable future, autonomous vehicles will require human assistance in situations they can not resolve on their own. In such scenarios, remote assistance from a human can provide the required input for the vehicle to continue its operation. Typical sensors used in autonomous vehicles include camera and lidar sensors. Due to the massive volume of sensor data that must be sent in real-time, highly efficient data compression is elementary to prevent an overload of network infrastructure. Sensor data compression using deep generative neural networks has been shown to outperform traditional compression approaches for both image and lidar data, regarding compression rate as well as reconstruction quality. However, there is a lack of research about the performance of generative-neural-network-based compression algorithms for remote assistance. In order to gain insights into the feasibility of deep generative models for usage in remote assistance, we evaluate state-of-the-art algorithms regarding their applicability and identify potential weaknesses. Further, we implement an online pipeline for processing sensor data and demonstrate its performance for remote assistance using the CARLA simulator.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-dimensional multi-view clustering methods</title>
<link>https://arxiv.org/abs/2303.08582</link>
<guid>https://arxiv.org/abs/2303.08582</guid>
<content:encoded><![CDATA[
arXiv:2303.08582v2 Announce Type: replace 
Abstract: Multi-view clustering has been widely used in recent years in comparison to single-view clustering, for clear reasons, as it offers more insights into the data, which has brought with it some challenges, such as how to combine these views or features. Most of recent work in this field focuses mainly on tensor representation instead of treating the data as simple matrices. This permits to deal with the high-order correlation between the data which the based matrix approach struggles to capture. Accordingly, we will examine and compare these approaches, particularly in two categories, namely graph-based clustering and subspace-based clustering. We will conduct and report experiments of the main clustering methods over a benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data</title>
<link>https://arxiv.org/abs/2304.13037</link>
<guid>https://arxiv.org/abs/2304.13037</guid>
<content:encoded><![CDATA[
arXiv:2304.13037v3 Announce Type: replace 
Abstract: An end-to-end machine learning (ML) lifecycle consists of many iterative processes, from data preparation and ML model design to model training and then deploying the trained model for inference. When building an end-to-end lifecycle for an ML problem, many ML pipelines must be designed and executed that produce a huge number of lifecycle versions. Therefore, this paper introduces VeML, a Version management system dedicated to end-to-end ML Lifecycle. Our system tackles several crucial problems that other systems have not solved. First, we address the high cost of building an ML lifecycle, especially for large-scale and high-dimensional dataset. We solve this problem by proposing to transfer the lifecycle of similar datasets managed in our system to the new training data. We design an algorithm based on the core set to compute similarity for large-scale, high-dimensional data efficiently. Another critical issue is the model accuracy degradation by the difference between training data and testing data during the ML lifetime, which leads to lifecycle rebuild. Our system helps to detect this mismatch without getting labeled data from testing data and rebuild the ML lifecycle for a new data version. To demonstrate our contributions, we conduct experiments on real-world, large-scale datasets of driving images and spatiotemporal sensor data and show promising results.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness in Streaming Submodular Maximization over a Matroid Constraint</title>
<link>https://arxiv.org/abs/2305.15118</link>
<guid>https://arxiv.org/abs/2305.15118</guid>
<content:encoded><![CDATA[
arXiv:2305.15118v3 Announce Type: replace 
Abstract: Streaming submodular maximization is a natural model for the task of selecting a representative subset from a large-scale dataset. If datapoints have sensitive attributes such as gender or race, it becomes important to enforce fairness to avoid bias and discrimination. This has spurred significant interest in developing fair machine learning algorithms. Recently, such algorithms have been developed for monotone submodular maximization under a cardinality constraint.
  In this paper, we study the natural generalization of this problem to a matroid constraint. We give streaming algorithms as well as impossibility results that provide trade-offs between efficiency, quality and fairness. We validate our findings empirically on a range of well-known real-world applications: exemplar-based clustering, movie recommendation, and maximum coverage in social networks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PINNsFailureRegion Localization and Refinement through White-box AdversarialAttack</title>
<link>https://arxiv.org/abs/2310.11789</link>
<guid>https://arxiv.org/abs/2310.11789</guid>
<content:encoded><![CDATA[
arXiv:2310.11789v2 Announce Type: replace 
Abstract: Physics-informed neural networks (PINNs) have shown great promise in solving partial differential equations (PDEs). However, vanilla PINNs often face challenges when solving complex PDEs, especially those involving multi-scale behaviors or solutions with sharp or oscillatory characteristics. To precisely and adaptively locate the critical regions that fail in the solving process we propose a sampling strategy grounded in white-box adversarial attacks, referred to as WbAR. WbAR search for failure regions in the direction of the loss gradient, thus directly locating the most critical positions. WbAR generates adversarial samples in a random walk manner and iteratively refines PINNs to guide the model's focus towards dynamically updated critical regions during training. We implement WbAR to the elliptic equation with multi-scale coefficients, Poisson equation with multi-peak solutions, high-dimensional Poisson equations, and Burgers equation with sharp solutions. The results demonstrate that WbAR can effectively locate and reduce failure regions. Moreover, WbAR is suitable for solving complex PDEs, since locating failure regions through adversarial attacks is independent of the size of failure regions or the complexity of the distribution.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel Unlearning in Inherited Model Networks</title>
<link>https://arxiv.org/abs/2408.08493</link>
<guid>https://arxiv.org/abs/2408.08493</guid>
<content:encoded><![CDATA[
arXiv:2408.08493v4 Announce Type: replace 
Abstract: Unlearning is challenging in generic learning frameworks with the continuous growth and updates of models exhibiting complex inheritance relationships. This paper presents a novel unlearning framework that enables fully parallel unlearning among models exhibiting inheritance. We use a chronologically Directed Acyclic Graph (DAG) to capture various unlearning scenarios occurring in model inheritance networks. Central to our framework is the Fisher Inheritance Unlearning (FIUn) method, designed to enable efficient parallel unlearning within the DAG. FIUn utilizes the Fisher Information Matrix (FIM) to assess the significance of model parameters for unlearning tasks and adjusts them accordingly. To handle multiple unlearning requests simultaneously, we propose the Merging-FIM (MFIM) function, which consolidates FIMs from multiple upstream models into a unified matrix. This design supports all unlearning scenarios captured by the DAG, enabling one-shot removal of inherited knowledge while significantly reducing computational overhead. Experiments confirm the effectiveness of our unlearning framework. For single-class tasks, it achieves complete unlearning with 0% accuracy for unlearned labels while maintaining 94.53% accuracy for retained labels. For multi-class tasks, the accuracy is 1.07% for unlearned labels and 84.77% for retained labels. Our framework accelerates unlearning by 99% compared to alternative methods. Code is in https://github.com/MJLee00/Parallel-Unlearning-in-Inherited-Model-Networks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Goal-Conditioned RL Algorithms and Research</title>
<link>https://arxiv.org/abs/2408.11052</link>
<guid>https://arxiv.org/abs/2408.11052</guid>
<content:encoded><![CDATA[
arXiv:2408.11052v4 Announce Type: replace 
Abstract: Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover new behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environment simulations as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (JaxGCRL) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. By utilizing GPU-accelerated replay buffers, environments, and a stable contrastive RL algorithm, we reduce training time by up to $22\times$. Additionally, we assess key design choices in contrastive RL, identifying those that most effectively stabilize and enhance training performance. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in diverse and challenging environments. Website + Code: https://github.com/MichalBortkiewicz/JaxGCRL
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning with Shared Representations: Statistical Rates and Efficient Algorithms</title>
<link>https://arxiv.org/abs/2409.04919</link>
<guid>https://arxiv.org/abs/2409.04919</guid>
<content:encoded><![CDATA[
arXiv:2409.04919v3 Announce Type: replace 
Abstract: Collaborative learning through latent shared feature representations enables heterogeneous clients to train personalized models with improved performance and reduced sample complexity. Despite empirical success and extensive study, the theoretical understanding of such methods remains incomplete, even for representations restricted to low-dimensional linear subspaces. In this work, we establish new upper and lower bounds on the statistical error in learning low-dimensional shared representations across clients. Our analysis captures both statistical heterogeneity (including covariate and concept shifts) and variation in local dataset sizes, aspects often overlooked in prior work. We further extend these results to nonlinear models including logistic regression and one-hidden-layer ReLU networks.
  Specifically, we design a spectral estimator that leverages independent replicas of local averages to approximate the non-convex least-squares solution and derive a nearly matching minimax lower bound. Our estimator achieves the optimal statistical rate when the shared representation is well covered across clients -- i.e., when no direction is severely underrepresented. Our results reveal two distinct phases of the optimal rate: a standard parameter-counting regime and a penalized regime when the number of clients is large or local datasets are small. These findings precisely characterize when collaboration benefits the overall system or individual clients in transfer learning and private fine-tuning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEANuT: Parameter-Efficient Adaptation with Weight-aware Neural Tweakers</title>
<link>https://arxiv.org/abs/2410.01870</link>
<guid>https://arxiv.org/abs/2410.01870</guid>
<content:encoded><![CDATA[
arXiv:2410.01870v3 Announce Type: replace 
Abstract: Fine-tuning large pre-trained foundation models often yields excellent downstream performance but is prohibitively expensive when updating all parameters. Parameter-efficient fine-tuning (PEFT) methods such as LoRA alleviate this by introducing lightweight update modules, yet they commonly rely on weight-agnostic linear approximations, limiting their expressiveness. In this work, we propose PEANuT, a novel PEFT framework that introduces weight-aware neural tweakers, compact neural modules that generate task-adaptive updates conditioned on frozen pre-trained weights. PEANuT provides a flexible yet efficient way to capture complex update patterns without full model tuning. We theoretically show that PEANuT achieves equivalent or greater expressivity than existing linear PEFT methods with comparable or fewer parameters. Extensive experiments across four benchmarks with over twenty datasets demonstrate that PEANuT consistently outperforms strong baselines in both NLP and vision tasks, while maintaining low computational overhead.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Fine-tuning in Approximate Unlearning: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2410.03833</link>
<guid>https://arxiv.org/abs/2410.03833</guid>
<content:encoded><![CDATA[
arXiv:2410.03833v3 Announce Type: replace 
Abstract: Machine Unlearning has emerged as a significant area of research, focusing on `removing' specific subsets of data from a trained model. Fine-tuning (FT) methods have become one of the fundamental approaches for approximating unlearning, as they effectively retain model performance. However, it is consistently observed that naive FT methods struggle to forget the targeted data. In this paper, we present the first theoretical analysis of FT methods for machine unlearning within a linear regression framework, providing a deeper exploration of this phenomenon. Our analysis reveals that while FT models can achieve zero remaining loss, they fail to forget the forgetting data, as the pretrained model retains its influence and the fine-tuning process does not adequately mitigate it. To address this, we propose a novel Retention-Based Masking (RBM) strategy that constructs a weight saliency map based on the remaining dataset, unlike existing methods that focus on the forgetting dataset. Our theoretical analysis demonstrates that RBM not only significantly improves unlearning accuracy (UA) but also ensures higher retaining accuracy (RA) by preserving overlapping features shared between the forgetting and remaining datasets. Experiments on synthetic and real-world datasets validate our theoretical insights, showing that RBM outperforms existing masking approaches in balancing UA, RA, and disparity metrics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Representation Universality: Case Study on Genealogical Representations</title>
<link>https://arxiv.org/abs/2410.08255</link>
<guid>https://arxiv.org/abs/2410.08255</guid>
<content:encoded><![CDATA[
arXiv:2410.08255v2 Announce Type: replace 
Abstract: Motivated by interpretability and reliability, we investigate whether large language models (LLMs) deploy universal geometric structures to encode discrete, graph-structured knowledge. To this end, we present two complementary experimental evidence that might support universality of graph representations. First, on an in-context genealogy Q&amp;A task, we train a cone probe to isolate a tree-like subspace in residual stream activations and use activation patching to verify its causal effect in answering related questions. We validate our findings across five different models. Second, we conduct model stitching experiments across models of diverse architectures and parameter counts (OPT, Pythia, Mistral, and LLaMA, 410 million to 8 billion parameters), quantifying representational alignment via relative degradation in the next-token prediction loss. Generally, we conclude that the lack of ground truth representations of graphs makes it challenging to study how LLMs represent them. Ultimately, improving our understanding of LLM representations could facilitate the development of more interpretable, robust, and controllable AI systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection</title>
<link>https://arxiv.org/abs/2410.14581</link>
<guid>https://arxiv.org/abs/2410.14581</guid>
<content:encoded><![CDATA[
arXiv:2410.14581v4 Announce Type: replace 
Abstract: Attention mechanisms have revolutionized several domains of artificial intelligence, such as natural language processing and computer vision, by enabling models to selectively focus on relevant parts of the input data. While recent work has characterized the optimization dynamics of gradient descent (GD) in attention-based models and the structural properties of its preferred solutions, less is known about more general optimization algorithms such as mirror descent (MD). In this paper, we investigate the convergence properties and implicit biases of a family of MD algorithms tailored for softmax attention mechanisms, with the potential function chosen as the $p$-th power of the $\ell_p$-norm. Specifically, we show that these algorithms converge in direction to a generalized hard-margin SVM with an $\ell_p$-norm objective when applied to a classification problem using a softmax attention model. Notably, our theoretical results reveal that the convergence rate is comparable to that of traditional GD in simpler models, despite the highly nonlinear and nonconvex nature of the present problem. Additionally, we delve into the joint optimization dynamics of the key-query matrix and the decoder, establishing conditions under which this complex joint optimization converges to their respective hard-margin SVM solutions. Lastly, our numerical experiments on real data demonstrate that MD algorithms improve generalization over standard GD and excel in optimal token selection.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI-Powered Plugin for Robust Federated Learning in Heterogeneous IoT Networks</title>
<link>https://arxiv.org/abs/2410.23824</link>
<guid>https://arxiv.org/abs/2410.23824</guid>
<content:encoded><![CDATA[
arXiv:2410.23824v3 Announce Type: replace 
Abstract: Federated learning enables edge devices to collaboratively train a global model while maintaining data privacy by keeping data localized. However, the Non-IID nature of data distribution across devices often hinders model convergence and reduces performance. In this paper, we propose a novel plugin for federated optimization methods that approximates Non-IID data distributions to IID through generative AI-enhanced data augmentation and balanced sampling strategy. The key idea is to synthesize additional data for underrepresented classes on each edge device, leveraging generative AI to create a more balanced dataset across the FL network. Additionally, a balanced sampling approach at the central server selectively includes only the most IID-like devices, accelerating convergence while maximizing the global model's performance. Experimental results validate that our approach significantly improves convergence speed and robustness against data imbalance, establishing a flexible, privacy-preserving FL plugin that is applicable even in data-scarce environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Final-Model-Only Data Attribution with a Unifying View of Gradient-Based Methods</title>
<link>https://arxiv.org/abs/2412.03906</link>
<guid>https://arxiv.org/abs/2412.03906</guid>
<content:encoded><![CDATA[
arXiv:2412.03906v2 Announce Type: replace 
Abstract: Training data attribution (TDA) is concerned with understanding model behavior in terms of the training data. This paper draws attention to the common setting where one has access only to the final trained model, and not the training algorithm or intermediate information from training. We reframe the problem in this "final-model-only" setting as one of measuring sensitivity of the model to training instances. To operationalize this reframing, we propose further training, with appropriate adjustment and averaging, as a gold standard method to measure sensitivity. We then unify existing gradient-based methods for TDA by showing that they all approximate the further training gold standard in different ways. We investigate empirically the quality of these gradient-based approximations to further training, for tabular, image, and text datasets and models. We find that the approximation quality of first-order methods is sometimes high but decays with the amount of further training. In contrast, the approximations given by influence function methods are more stable but surprisingly lower in quality.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration</title>
<link>https://arxiv.org/abs/2412.16216</link>
<guid>https://arxiv.org/abs/2412.16216</guid>
<content:encoded><![CDATA[
arXiv:2412.16216v4 Announce Type: replace 
Abstract: The sparse Mixture-of-Experts (MoE) architecture of large language models (LLMs) confronts an inherent issue of load imbalance arising from the simplistic linear router strategy, which ultimately causes the instability and inefficient learning of LLMs. To address this challenge, we introduce a novel MoE graph-based framework $\textbf{GMoE}$, aimed at enhancing the collaboration among multiple experts. In GMoE, a graph router function is designed to capture the collaboration signals among experts. This enables all experts to dynamically allocate information derived from input data by sharing information with their neighboring experts. Moreover, we put forward two coordination strategies in GMoE: the $\textit{Poisson distribution-based distinction strategy}$ and the $\textit{Normal distribution-based balance strategy}$, to further release the capacity of each expert and increase the model stability in the fine-tuning of LLMs. Specifically, we leverage a parameter-efficient fine-tuning technique, i.e., Low-Rank Adaptation (LoRA), to implement the graph MoE architecture. Extensive experiments on four real-world benchmark datasets demonstrate the effectiveness of GMoE, showing the benefits of facilitating collaborations of multiple experts in LLM fine-tuning. The code of experimental implementation is available at https://github.com/BAI-LAB/GMoE
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Inspired Multi-Level Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.07502</link>
<guid>https://arxiv.org/abs/2501.07502</guid>
<content:encoded><![CDATA[
arXiv:2501.07502v2 Announce Type: replace 
Abstract: Reinforcement learning (RL), a common tool in decision making, learns control policies from various experiences based on the associated cumulative return/rewards without treating them differently. Humans, on the contrary, often learn to distinguish from discrete levels of performance and extract the underlying insights/information (beyond reward signals) towards their decision optimization. For instance, when learning to play tennis, a human player does not treat all unsuccessful attempts equally. Missing the ball completely signals a more severe mistake than hitting it out of bounds (although the cumulative rewards can be similar for both cases). Learning effectively from multi-level experiences is essential in human decision making. This motivates us to develop a novel multi-level RL method that learns from multi-level experiences via extracting multi-level information. At the low level of information extraction, we utilized the existing rating-based reinforcement learning to infer inherent reward signals that illustrate the value of states or state-action pairs accordingly. At the high level of information extraction, we propose to extract important directional information from different-level experiences so that policies can be updated towards desired deviation from these different levels of experiences. Specifically, we propose a new policy loss function that penalizes distribution similarities between the current policy and different-level experiences, and assigns different weights to the penalty terms based on the performance levels. Furthermore, the integration of the two levels towards multi-level RL guides the agent toward policy improvements that benefit both reward improvement and policy improvement, hence yielding a similar learning mechanism as humans.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Potential Prompt Injection Attacks in Federated Military LLMs and Their Mitigation</title>
<link>https://arxiv.org/abs/2501.18416</link>
<guid>https://arxiv.org/abs/2501.18416</guid>
<content:encoded><![CDATA[
arXiv:2501.18416v2 Announce Type: replace 
Abstract: Federated Learning (FL) is increasingly being adopted in military collaborations to develop Large Language Models (LLMs) while preserving data sovereignty. However, prompt injection attacks-malicious manipulations of input prompts-pose new threats that may undermine operational security, disrupt decision-making, and erode trust among allies. This perspective paper highlights four vulnerabilities in federated military LLMs: secret data leakage, free-rider exploitation, system disruption, and misinformation spread. To address these risks, we propose a human-AI collaborative framework with both technical and policy countermeasures. On the technical side, our framework uses red/blue team wargaming and quality assurance to detect and mitigate adversarial behaviors of shared LLM weights. On the policy side, it promotes joint AI-human policy development and verification of security protocols.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Spikes Protect Privacy? Investigating Black-Box Model Inversion Attacks in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2502.05509</link>
<guid>https://arxiv.org/abs/2502.05509</guid>
<content:encoded><![CDATA[
arXiv:2502.05509v2 Announce Type: replace 
Abstract: As machine learning models become integral to security-sensitive applications, concerns over data leakage from adversarial attacks continue to rise. Model Inversion (MI) attacks pose a significant privacy threat by enabling adversaries to reconstruct training data from model outputs. While MI attacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking Neural Networks (SNNs) remain largely unexplored in this context. Due to their event-driven and discrete computations, SNNs introduce fundamental differences in information processing that may offer inherent resistance to such attacks. A critical yet underexplored aspect of this threat lies in black-box settings, where attackers operate through queries without direct access to model parameters or gradients-representing a more realistic adversarial scenario in deployed systems. This work presents the first study of black-box MI attacks on SNNs. We adapt a generative adversarial MI framework to the spiking domain by incorporating rate-based encoding for input transformation and decoding mechanisms for output interpretation. Our results show that SNNs exhibit significantly greater resistance to MI attacks than ANNs, as demonstrated by degraded reconstructions, increased instability in attack convergence, and overall reduced attack effectiveness across multiple evaluation metrics. Further analysis suggests that the discrete and temporally distributed nature of SNN decision boundaries disrupts surrogate modeling, limiting the attacker's ability to approximate the target model.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When, Where and Why to Average Weights?</title>
<link>https://arxiv.org/abs/2502.06761</link>
<guid>https://arxiv.org/abs/2502.06761</guid>
<content:encoded><![CDATA[
arXiv:2502.06761v3 Announce Type: replace 
Abstract: Averaging checkpoints along the training trajectory is a simple yet powerful approach to improve the generalization performance of Machine Learning models and reduce training time. Motivated by these potential gains, and in an effort to fairly and thoroughly benchmark this technique, we present an extensive evaluation of averaging techniques in modern Deep Learning, which we perform using AlgoPerf \citep{dahl_benchmarking_2023}, a large-scale benchmark for optimization algorithms. We investigate whether weight averaging can reduce training time, improve generalization, and replace learning rate decay, as suggested by recent literature. Our evaluation across seven architectures and datasets reveals that averaging significantly accelerates training and yields considerable efficiency gains, at the price of a minimal implementation and memory cost, while mildly improving generalization across all considered workloads. Finally, we explore the relationship between averaging and learning rate annealing and show how to optimally combine the two to achieve the best performances.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resilient Contrastive Pre-training under Non-Stationary Drift</title>
<link>https://arxiv.org/abs/2502.07620</link>
<guid>https://arxiv.org/abs/2502.07620</guid>
<content:encoded><![CDATA[
arXiv:2502.07620v3 Announce Type: replace 
Abstract: The remarkable success of large-scale contrastive pre-training has been largely driven by by vast yet static datasets. However, as the scaling paradigm evolves, this paradigm encounters a fundamental challenge when applied to dynamic data streams characterized by concept drift - unpredictable changes in the underlying data distribution. This paper aims to advance robust pre-training under such non-stationary environments. We begin by revealing that conventional contrastive pre-training methods are highly susceptible to concept drift, resulting in significant substantial bias and instability within the learned feature representations. To systematically analyze these effects, we develop a structural causal model that elucidates how drift acts as a confounder, distorting the learned representations. Based on these causal insights, we propose Resilient Contrastive Pre-training (RCP), a novel method that incorporates causal intervention. RCP formulates a causally-informed objective to mitigate drift-induced biases through targeted interventions. The method is designed for simple and scalable implementation and exhibits notable adaptability, promoting robust and autonomous pre-training on non-stationary data. Comprehensive experiments across various downstream tasks consistently demonstrate that RCP effectively alleviates the detrimental impact of concept drift, yielding more resilient and generalizable representations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Predictions: A Participatory Framework for Multi-Stakeholder Decision-Making</title>
<link>https://arxiv.org/abs/2502.08542</link>
<guid>https://arxiv.org/abs/2502.08542</guid>
<content:encoded><![CDATA[
arXiv:2502.08542v3 Announce Type: replace 
Abstract: Conventional automated decision-support systems often prioritize predictive accuracy, overlooking the complexities of real-world settings where stakeholders' preferences may diverge or conflict. This can lead to outcomes that disadvantage vulnerable groups and erode trust in algorithmic processes. Participatory AI approaches aim to address these issues but remain largely context-specific, limiting their broader applicability and scalability. To address these gaps, we propose a participatory framework that reframes decision-making as a multi-stakeholder learning and optimization problem. Our modular, model-agnostic approach builds on the standard machine learning training pipeline to fine-tune user-provided prediction models and evaluate decision strategies, including compromise functions that mediate stakeholder trade-offs. A synthetic scoring mechanism aggregates user-defined preferences across multiple metrics, ranking strategies and selecting an optimal decision-maker to generate actionable recommendations that jointly optimize performance, fairness, and domain-specific goals. Empirical validation on two high-stakes case studies demonstrates the versatility of the framework and its promise as a more accountable, context-aware alternative to prediction-centric pipelines for socially impactful deployments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction of Clinical Complication Onset using Neural Point Processes</title>
<link>https://arxiv.org/abs/2502.13290</link>
<guid>https://arxiv.org/abs/2502.13290</guid>
<content:encoded><![CDATA[
arXiv:2502.13290v2 Announce Type: replace 
Abstract: Predicting medical events in advance within critical care settings is paramount for patient outcomes and resource management. Utilizing predictive models, healthcare providers can anticipate issues such as cardiac arrest, sepsis, or respiratory failure before they manifest. Recently, there has been a surge in research focusing on forecasting adverse medical event onsets prior to clinical manifestation using machine learning. However, while these models provide temporal prognostic predictions for the occurrence of a specific adverse event of interest within defined time intervals, their interpretability often remains a challenge. In this work, we explore the applicability of neural temporal point processes in the context of adverse event onset prediction, with the aim of explaining clinical pathways and providing interpretable insights. Our experiments span six state-of-the-art neural point processes and six critical care datasets, each focusing on the onset of distinct adverse events. This work represents a novel application class of neural temporal point processes in event prediction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs</title>
<link>https://arxiv.org/abs/2502.15938</link>
<guid>https://arxiv.org/abs/2502.15938</guid>
<content:encoded><![CDATA[
arXiv:2502.15938v2 Announce Type: replace 
Abstract: LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10x decay). In a large-scale empirical study, we show that under an optimal peak LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at compute-optimal dataset sizes. D2Z is superior across a range of model sizes, batch sizes, datasets, and vocabularies. Benefits increase as dataset size increases. Leveraging a novel interpretation of AdamW as an exponential moving average of weight updates, we show how linear D2Z optimally balances the demands of early training (moving away from initial conditions) and late training (averaging over more updates in order to mitigate gradient noise). In experiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP) using D2Z achieves lower loss than when trained for 200 TPP using 10x decay, corresponding to an astonishing 60% compute savings. Models such as Llama2-7B, trained for 286 TPP with 10x decay, could likely have saved a majority of compute by training with D2Z.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causally Reliable Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2503.04363</link>
<guid>https://arxiv.org/abs/2503.04363</guid>
<content:encoded><![CDATA[
arXiv:2503.04363v3 Announce Type: replace 
Abstract: Concept-based models are an emerging paradigm in deep learning that constrains the inference process to operate through human-interpretable variables, facilitating explainability and human interaction. However, these architectures, on par with popular opaque neural models, fail to account for the true causal mechanisms underlying the target phenomena represented in the data. This hampers their ability to support causal reasoning tasks, limits out-of-distribution generalization, and hinders the implementation of fairness constraints. To overcome these issues, we propose Causally reliable Concept Bottleneck Models (C$^2$BMs), a class of concept-based architectures that enforce reasoning through a bottleneck of concepts structured according to a model of the real-world causal mechanisms. We also introduce a pipeline to automatically learn this structure from observational data and unstructured background knowledge (e.g., scientific literature). Experimental evidence suggests that C$^2$BMs are more interpretable, causally reliable, and improve responsiveness to interventions w.r.t. standard opaque and concept-based models, while maintaining their accuracy.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving</title>
<link>https://arxiv.org/abs/2503.06567</link>
<guid>https://arxiv.org/abs/2503.06567</guid>
<content:encoded><![CDATA[
arXiv:2503.06567v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated significant potential across various domains. However, they often struggle with integrating external knowledge and performing complex reasoning, leading to hallucinations and unreliable outputs. Retrieval Augmented Generation (RAG) has emerged as a promising paradigm to mitigate these issues by incorporating external knowledge. Yet, conventional RAG approaches, especially those based on vector similarity, fail to effectively capture relational dependencies and support multi-step reasoning. In this work, we propose CogGRAG, a human cognition-inspired, graph-based RAG framework designed for Knowledge Graph Question Answering (KGQA). CogGRAG models the reasoning process as a tree-structured mind map that decomposes the original problem into interrelated subproblems and explicitly encodes their semantic relationships. This structure not only provides a global view to guide subsequent retrieval and reasoning but also enables self-consistent verification across reasoning paths. The framework operates in three stages: (1) top-down problem decomposition via mind map construction, (2) structured retrieval of both local and global knowledge from external Knowledge Graphs (KGs), and (3) bottom-up reasoning with dual-process self-verification. Unlike previous tree-based decomposition methods such as MindMap or Graph-CoT, CogGRAG unifies problem decomposition, knowledge retrieval, and reasoning under a single graph-structured cognitive framework, allowing early integration of relational knowledge and adaptive verification. Extensive experiments demonstrate that CogGRAG achieves superior accuracy and reliability compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities</title>
<link>https://arxiv.org/abs/2503.14858</link>
<guid>https://arxiv.org/abs/2503.14858</guid>
<content:encoded><![CDATA[
arXiv:2503.14858v3 Announce Type: replace 
Abstract: Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 - 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance. Our experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals. Evaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by $2\times$ - $50\times$, outperforming other goal-conditioned baselines. Increasing the model depth not only increases success rates but also qualitatively changes the behaviors learned. The project webpage and code can be found here: https://wang-kevin3290.github.io/scaling-crl/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Malliavin Calculus for Score-based Diffusion Models</title>
<link>https://arxiv.org/abs/2503.16917</link>
<guid>https://arxiv.org/abs/2503.16917</guid>
<content:encoded><![CDATA[
arXiv:2503.16917v3 Announce Type: replace 
Abstract: We introduce a new framework based on Malliavin calculus to derive exact analytical expressions for the score function $\nabla \log p_t(x)$, i.e., the gradient of the log-density associated with the solution to stochastic differential equations (SDEs). Our approach combines classical integration-by-parts techniques with modern stochastic analysis tools, such as Bismut's formula and Malliavin calculus, and it works for both linear and nonlinear SDEs. In doing so, we establish a rigorous connection between the Malliavin derivative, its adjoint, the Malliavin divergence (Skorokhod integral), and diffusion generative models, thereby providing a systematic method for computing $\nabla \log p_t(x)$. In the linear case, we present a detailed analysis showing that our formula coincides with the analytical score function derived from the solution of the Fokker--Planck equation. For nonlinear SDEs with state-independent diffusion coefficients, we derive a closed-form expression for $\nabla \log p_t(x)$. We evaluate the proposed framework across multiple generative tasks and find that its performance is comparable to state-of-the-art methods. These results can be generalised to broader classes of SDEs, paving the way for new score-based diffusion generative models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Node Embeddings via Neighbor Embeddings</title>
<link>https://arxiv.org/abs/2503.23822</link>
<guid>https://arxiv.org/abs/2503.23822</guid>
<content:encoded><![CDATA[
arXiv:2503.23822v2 Announce Type: replace 
Abstract: Node embeddings are a paradigm in non-parametric graph representation learning, where graph nodes are embedded into a given vector space to enable downstream processing. State-of-the-art node-embedding algorithms, such as DeepWalk and node2vec, are based on random-walk notions of node similarity and on contrastive learning. In this work, we introduce the graph neighbor-embedding (graph NE) framework that directly pulls together embedding vectors of adjacent nodes without relying on any random walks. We show that graph NE strongly outperforms state-of-the-art node-embedding algorithms in terms of local structure preservation. Furthermore, we apply graph NE to the 2D node-embedding problem, obtaining graph t-SNE layouts that also outperform existing graph-layout algorithms.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Lipschitz Bandits</title>
<link>https://arxiv.org/abs/2504.02251</link>
<guid>https://arxiv.org/abs/2504.02251</guid>
<content:encoded><![CDATA[
arXiv:2504.02251v2 Announce Type: replace 
Abstract: The Lipschitz bandit is a key variant of stochastic bandit problems where the expected reward function satisfies a Lipschitz condition with respect to an arm metric space. With its wide-ranging practical applications, various Lipschitz bandit algorithms have been developed, achieving the cumulative regret lower bound of order $\tilde O(T^{(d_z+1)/(d_z+2)})$ over time horizon $T$. Motivated by recent advancements in quantum computing and the demonstrated success of quantum Monte Carlo in simpler bandit settings, we introduce the first quantum Lipschitz bandit algorithms to address the challenges of continuous action spaces and non-linear reward functions. Specifically, we first leverage the elimination-based framework to propose an efficient quantum Lipschitz bandit algorithm named Q-LAE. Next, we present novel modifications to the classical Zooming algorithm, which results in a simple quantum Lipschitz bandit method, Q-Zooming. Both algorithms exploit the computational power of quantum methods to achieve an improved regret bound of $\tilde O(T^{d_z/(d_z+1)})$. Comprehensive experiments further validate our improved theoretical findings, demonstrating superior empirical performance compared to existing Lipschitz bandit methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Learning Dynamics of In-Context Learning in Linear Transformers and Its Application to Non-Linear Transformers</title>
<link>https://arxiv.org/abs/2504.12916</link>
<guid>https://arxiv.org/abs/2504.12916</guid>
<content:encoded><![CDATA[
arXiv:2504.12916v2 Announce Type: replace 
Abstract: Transformer models exhibit remarkable in-context learning (ICL), adapting to novel tasks from examples within their context, yet the underlying mechanisms remain largely mysterious. Here, we provide an exact analytical characterization of ICL emergence by deriving the closed-form stochastic gradient descent (SGD) dynamics for a simplified linear transformer performing regression tasks. Our analysis reveals key properties: (1) a natural separation of timescales directly governed by the input data's covariance structure, leading to staged learning; (2) an exact description of how ICL develops, including fixed points corresponding to learned algorithms and conservation laws constraining the dynamics; and (3) surprisingly nonlinear learning behavior despite the model's linearity. We hypothesize this phenomenology extends to non-linear models. To test this, we introduce theory-inspired macroscopic measures (spectral rank dynamics, subspace stability) and use them to provide mechanistic explanations for (1) the sudden emergence of ICL in attention-only networks and (2) delayed generalization (grokking) in modular arithmetic models. Our work offers an exact dynamical model for ICL and theoretically grounded tools for analyzing complex transformer training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropic Time Schedulers for Generative Diffusion Models</title>
<link>https://arxiv.org/abs/2504.13612</link>
<guid>https://arxiv.org/abs/2504.13612</guid>
<content:encoded><![CDATA[
arXiv:2504.13612v4 Announce Type: replace 
Abstract: The practical performance of generative diffusion models depends on the appropriate choice of the noise scheduling function, which can also be equivalently expressed as a time reparameterization. In this paper, we present a time scheduler that selects sampling points based on entropy rather than uniform time spacing, ensuring that each point contributes an equal amount of information to the final generation. We prove that this time reparameterization does not depend on the initial choice of time. Furthermore, we provide a tractable exact formula to estimate this \emph{entropic time} for a trained model using the training loss without substantial overhead. Alongside the entropic time, inspired by the optimality results, we introduce a rescaled entropic time. In our experiments with mixtures of Gaussian distributions and ImageNet, we show that using the (rescaled) entropic times greatly improves the inference performance of trained models. In particular, we found that the image quality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can be substantially increased by the rescaled entropic time reparameterization without increasing the number of function evaluations, with greater improvements in the few NFEs regime. Code is available at https://github.com/DejanStancevic/Entropic-Time-Schedulers-for-Generative-Diffusion-Models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach</title>
<link>https://arxiv.org/abs/2504.20319</link>
<guid>https://arxiv.org/abs/2504.20319</guid>
<content:encoded><![CDATA[
arXiv:2504.20319v2 Announce Type: replace 
Abstract: Bayesian experimental design (BED) offers a principled framework for optimizing data acquisition by leveraging probabilistic inference. However, practical implementations of BED are often compromised by model discrepancy, i.e., the mismatch between predictive models and true physical systems, which can potentially lead to biased parameter estimates. While data-driven approaches have been recently explored to characterize the model discrepancy, the resulting high-dimensional parameter space poses severe challenges for both Bayesian updating and design optimization. In this work, we propose a hybrid BED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI) that addresses these challenges by providing a computationally efficient, gradient-free alternative to estimate the information gain for high-dimensional network parameters. The AD-EKI allows a differentiable evaluation of the utility function in BED and thus facilitates the use of standard gradient-based methods for design optimization. In the proposed hybrid framework, we iteratively optimize experimental designs, decoupling the inference of low-dimensional physical parameters handled by standard BED methods, from the high-dimensional model discrepancy handled by AD-EKI. The identified optimal designs for the model discrepancy enable us to systematically collect informative data for its calibration. The performance of the proposed method is studied by a classical convection-diffusion BED example, and the hybrid framework enabled by AD-EKI efficiently identifies informative data to calibrate the model discrepancy and robustly infers the unknown physical parameters in the modeled system. Besides addressing the challenges of BED with model discrepancy, AD-EKI also potentially fosters efficient and scalable frameworks in many other areas with bilevel optimization, such as meta-learning and structure optimization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IIKL: Isometric Immersion Kernel Learning with Riemannian Manifold for Geometric Preservation</title>
<link>https://arxiv.org/abs/2505.06288</link>
<guid>https://arxiv.org/abs/2505.06288</guid>
<content:encoded><![CDATA[
arXiv:2505.06288v2 Announce Type: replace 
Abstract: Geometric representation learning in preserving the intrinsic geometric and topological properties for discrete non-Euclidean data is crucial in scientific applications. Previous research generally mapped non-Euclidean discrete data into Euclidean space during representation learning, which may lead to the loss of some critical geometric information. In this paper, we propose a novel Isometric Immersion Kernel Learning (IIKL) method to build Riemannian manifold and isometrically induce Riemannian metric from discrete non-Euclidean data. We prove that Isometric immersion is equivalent to the kernel function in the tangent bundle on the manifold, which explicitly guarantees the invariance of the inner product between vectors in the arbitrary tangent space throughout the learning process, thus maintaining the geometric structure of the original data. Moreover, a novel parameterized learning model based on IIKL is introduced, and an alternating training method for this model is derived using Maximum Likelihood Estimation (MLE), ensuring efficient convergence. Experimental results proved that using the learned Riemannian manifold and its metric, our model preserved the intrinsic geometric representation of data in both 3D and high-dimensional datasets successfully, and significantly improved the accuracy of downstream tasks, such as data reconstruction and classification. It is showed that our method could reduce the inner product invariant loss by more than 90% compared to state-of-the-art (SOTA) methods, also achieved an average 40% improvement in downstream reconstruction accuracy and a 90% reduction in error for geometric metrics involving isometric and conformal.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting Graph Inference with Skyline Explanations</title>
<link>https://arxiv.org/abs/2505.07635</link>
<guid>https://arxiv.org/abs/2505.07635</guid>
<content:encoded><![CDATA[
arXiv:2505.07635v4 Announce Type: replace 
Abstract: Inference queries have been routinely issued to graph machine learning models such as graph neural networks (GNNs) for various network analytical tasks. Nevertheless, GNN outputs are often hard to interpret comprehensively. Existing methods typically conform to individual pre-defined explainability measures (such as fidelity), which often leads to biased, ``one-side'' interpretations. This paper introduces skyline explanation, a new paradigm that interprets GNN outputs by simultaneously optimizing multiple explainability measures of users' interests. (1) We propose skyline explanations as a Pareto set of explanatory subgraphs that dominate others over multiple explanatory measures. We formulate skyline explanation as a multi-criteria optimization problem, and establish its hardness results. (2) We design efficient algorithms with an onion-peeling approach, which strategically prioritizes nodes and removes unpromising edges to incrementally assemble skyline explanations. (3) We also develop an algorithm to diversify the skyline explanations to enrich the comprehensive interpretation. (4) We introduce efficient parallel algorithms with load-balancing strategies to scale skyline explanation for large-scale GNN-based inference. Using real-world and synthetic graphs, we experimentally verify our algorithms' effectiveness and scalability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction</title>
<link>https://arxiv.org/abs/2505.11254</link>
<guid>https://arxiv.org/abs/2505.11254</guid>
<content:encoded><![CDATA[
arXiv:2505.11254v2 Announce Type: replace 
Abstract: The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Grokking a Computational Glass Relaxation?</title>
<link>https://arxiv.org/abs/2505.11411</link>
<guid>https://arxiv.org/abs/2505.11411</guid>
<content:encoded><![CDATA[
arXiv:2505.11411v4 Announce Type: replace 
Abstract: Understanding neural network's (NN) generalizability remains a central question in deep learning research. The special phenomenon of grokking, where NNs abruptly generalize long after the training performance reaches a near-perfect level, offers a unique window to investigate the underlying mechanisms of NNs' generalizability. Here we propose an interpretation for grokking by framing it as a computational glass relaxation: viewing NNs as a physical system where parameters are the degrees of freedom and train loss is the system energy, we find memorization process resembles a rapid cooling of liquid into non-equilibrium glassy state at low temperature and the later generalization is like a slow relaxation towards a more stable configuration. This mapping enables us to sample NNs' Boltzmann entropy (states of density) landscape as a function of training loss and test accuracy. Our experiments in transformers on arithmetic tasks suggests that there is NO entropy barrier in the memorization-to-generalization transition of grokking, challenging previous theory that defines grokking as a first-order phase transition. We identify a high-entropy advantage under grokking, an extension of prior work linking entropy to generalizability but much more significant. Inspired by grokking's far-from-equilibrium nature, we develop a toy optimizer WanD based on Wang-landau molecular dynamics, which can eliminate grokking without any constraints and find high-norm generalizing solutions. This provides strictly-defined counterexamples to theory attributing grokking solely to weight norm evolution towards the Goldilocks zone and also suggests new potential ways for optimizer design.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents</title>
<link>https://arxiv.org/abs/2505.12842</link>
<guid>https://arxiv.org/abs/2505.12842</guid>
<content:encoded><![CDATA[
arXiv:2505.12842v4 Announce Type: replace 
Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70\% over the best-performing baseline while only increasing training time by 4.9\% and testing time by 6.5\%. We also experimentally demonstrate that GEM can improve the step-wise success rate by 9.40\% by requesting assistance from the cloud model when encountering OOD samples. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collapsing Taylor Mode Automatic Differentiation</title>
<link>https://arxiv.org/abs/2505.13644</link>
<guid>https://arxiv.org/abs/2505.13644</guid>
<content:encoded><![CDATA[
arXiv:2505.13644v2 Announce Type: replace 
Abstract: Computing partial differential equation (PDE) operators via nested backpropagation is expensive, yet popular, and severely restricts their utility for scientific machine learning. Recent advances, like the forward Laplacian and randomizing Taylor mode automatic differentiation (AD), propose forward schemes to address this. We introduce an optimization technique for Taylor mode that 'collapses' derivatives by rewriting the computational graph, and demonstrate how to apply it to general linear PDE operators, and randomized Taylor mode. The modifications simply require propagating a sum up the computational graph, which could -- or should -- be done by a machine learning compiler, without exposing complexity to users. We implement our collapsing procedure and evaluate it on popular PDE operators, confirming it accelerates Taylor mode and outperforms nested backpropagation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training</title>
<link>https://arxiv.org/abs/2505.13738</link>
<guid>https://arxiv.org/abs/2505.13738</guid>
<content:encoded><![CDATA[
arXiv:2505.13738v2 Announce Type: replace 
Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs), including learning rate $\eta$ and weight decay $\lambda$. We study scaling laws for HPs: formulas for how to scale HPs as we scale model size N, dataset size D, and batch size B. Recent work suggests the AdamW timescale, $\tau = B/(\eta \lambda D)$, should remain constant across training settings, and we verify the implication that optimal $\lambda$ scales linearly with B, for a fixed N and D. However, as N and D scale, we show optimal $\tau$ obeys a precise power law in the tokens-per-parameter ratio, D/N. This law thus provides a method to accurately predict $\lambda$opt in advance of large-scale training. We also study scaling laws for optimal batch size Bopt (the B enabling lowest loss at a given N,D) and critical batch size Bcrit (the B beyond which further data parallelism becomes ineffective). In contrast to prior work, we find both Bopt and Bcrit scale as power laws in D, independent of model size, N. Finally, we analyze how these findings inform the real-world selection of Pareto-optimal N and D under dual training time and compute objectives. All experiments were run on Cerebras CS-3 systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens</title>
<link>https://arxiv.org/abs/2505.13775</link>
<guid>https://arxiv.org/abs/2505.13775</guid>
<content:encoded><![CDATA[
arXiv:2505.13775v3 Announce Type: replace 
Abstract: Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought'' reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement</title>
<link>https://arxiv.org/abs/2505.20192</link>
<guid>https://arxiv.org/abs/2505.20192</guid>
<content:encoded><![CDATA[
arXiv:2505.20192v2 Announce Type: replace 
Abstract: The integration of large language models (LLMs) with function calling has emerged as a crucial capability for enhancing their practical utility in real-world applications. However, effectively combining reasoning processes with accurate function execution remains a significant challenge. Traditional training approaches often struggle to balance the detailed reasoning steps with the precision of function calls, leading to suboptimal performance. To address these limitations, we introduce FunReason, a novel framework that enhances LLMs' function calling capabilities through an automated data refinement strategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason leverages LLMs' natural reasoning abilities to generate high-quality training examples, focusing on query parseability, reasoning coherence, and function call precision. The SRML approach dynamically balances the contribution of reasoning processes and function call accuracy during training, addressing the inherent trade-off between these two critical aspects. FunReason achieves performance comparable to GPT-4o while effectively mitigating catastrophic forgetting during fine-tuning. FunReason provides a comprehensive solution for enhancing LLMs' function calling capabilities by introducing a balanced training methodology and a data refinement pipeline. For code and dataset, please refer to our repository at GitHub https://github.com/BingguangHao/FunReason
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO</title>
<link>https://arxiv.org/abs/2506.00967</link>
<guid>https://arxiv.org/abs/2506.00967</guid>
<content:encoded><![CDATA[
arXiv:2506.00967v4 Announce Type: replace 
Abstract: Optimization-based power control algorithms are predominantly iterative with high computational complexity, making them impractical for real-time applications in cell-free massive multiple-input multiple-output (CFmMIMO) systems. Learning-based methods have emerged as a promising alternative, and among them, graph neural networks (GNNs) have demonstrated their excellent performance in solving power control problems. However, all existing GNN-based approaches assume ideal orthogonality among pilot sequences for user equipments (UEs), which is unrealistic given that the number of UEs exceeds the available orthogonal pilot sequences in CFmMIMO schemes. Moreover, most learning-based methods assume a fixed number of UEs, whereas the number of active UEs varies over time in practice. Additionally, supervised training necessitates costly computational resources for computing the target power control solutions for a large volume of training samples. To address these issues, we propose a graph attention network for downlink power control in CFmMIMO systems that operates in a self-supervised manner while effectively handling pilot contamination and adapting to a dynamic number of UEs. Experimental results show its effectiveness, even in comparison to the optimal accelerated projected gradient method as a baseline.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Stability of the Jacobian Matrix in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2506.08764</link>
<guid>https://arxiv.org/abs/2506.08764</guid>
<content:encoded><![CDATA[
arXiv:2506.08764v2 Announce Type: replace 
Abstract: Deep neural networks are known to suffer from exploding or vanishing gradients as depth increases, a phenomenon closely tied to the spectral behavior of the input-output Jacobian. Prior work has identified critical initialization schemes that ensure Jacobian stability, but these analyses are typically restricted to fully connected networks with i.i.d. weights. In this work, we go significantly beyond these limitations: we establish a general stability theorem for deep neural networks that accommodates sparsity (such as that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g. induced by training). Our results rely on recent advances in random matrix theory, and provide rigorous guarantees for spectral stability in a much broader class of network models. This extends the theoretical foundation for initialization schemes in modern neural networks with structured and dependent randomness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models</title>
<link>https://arxiv.org/abs/2506.09532</link>
<guid>https://arxiv.org/abs/2506.09532</guid>
<content:encoded><![CDATA[
arXiv:2506.09532v3 Announce Type: replace 
Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders</title>
<link>https://arxiv.org/abs/2506.15881</link>
<guid>https://arxiv.org/abs/2506.15881</guid>
<content:encoded><![CDATA[
arXiv:2506.15881v2 Announce Type: replace 
Abstract: SHallow REcurrent Decoders (SHRED) are effective for system identification and forecasting from sparse sensor measurements. Such models are light-weight and computationally efficient, allowing them to be trained on consumer laptops. SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding respectively. Despite the relatively simple structure of SHRED, they are able to predict chaotic dynamical systems on different physical, spatial, and temporal scales directly from a sparse set of sensor measurements. In this work, we modify SHRED by leveraging transformers (T-SHRED) embedded with symbolic regression for the temporal encoding, circumventing auto-regressive long-term forecasting for physical data. This is achieved through a new sparse identification of nonlinear dynamics (SINDy) attention mechanism into T-SHRED to impose sparsity regularization on the latent space, which also allows for immediate symbolic interpretation. Symbolic regression improves model interpretability by learning and regularizing the dynamics of the latent space during training. We analyze the performance of T-SHRED on three different dynamical systems ranging from low-data to high-data regimes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction</title>
<link>https://arxiv.org/abs/2506.16001</link>
<guid>https://arxiv.org/abs/2506.16001</guid>
<content:encoded><![CDATA[
arXiv:2506.16001v2 Announce Type: replace 
Abstract: Time series forecasting requires architectures that simultaneously achieve three competing objectives: (1) strict temporal causality for reliable predictions, (2) sub-quadratic complexity for practical scalability, and (3) multi-scale pattern recognition for accurate long-horizon forecasting. We introduce AutoHFormer, a hierarchical autoregressive transformer that addresses these challenges through three key innovations: 1) Hierarchical Temporal Modeling: Our architecture decomposes predictions into segment-level blocks processed in parallel, followed by intra-segment sequential refinement. This dual-scale approach maintains temporal coherence while enabling efficient computation. 2) Dynamic Windowed Attention: The attention mechanism employs learnable causal windows with exponential decay, reducing complexity while preserving precise temporal relationships. This design avoids both the anti-causal violations of standard transformers and the sequential bottlenecks of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system is adopted to capture time patterns at multiple scales. It combines fixed oscillating patterns for short-term variations with learnable decay rates for long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X faster training and 6.06X memory reduction compared to PatchTST on PEMS08, while maintaining consistent accuracy across 96-720 step horizons in most of cases. These breakthroughs establish new benchmarks for efficient and precise time series modeling. Implementations of our method and all baselines in hierarchical autoregressive mechanism are available at https://github.com/lizzyhku/Autotime.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SING: SDE Inference via Natural Gradients</title>
<link>https://arxiv.org/abs/2506.17796</link>
<guid>https://arxiv.org/abs/2506.17796</guid>
<content:encoded><![CDATA[
arXiv:2506.17796v2 Announce Type: replace 
Abstract: Latent stochastic differential equation (SDE) models are important tools for the unsupervised discovery of dynamical systems from data, with applications ranging from engineering to neuroscience. In these complex domains, exact posterior inference of the latent state path is typically intractable, motivating the use of approximate methods such as variational inference (VI). However, existing VI methods for inference in latent SDEs often suffer from slow convergence and numerical instability. We propose SDE Inference via Natural Gradients (SING), a method that leverages natural gradient VI to efficiently exploit the underlying geometry of the model and variational posterior. SING enables fast and reliable inference in latent SDE models by approximating intractable integrals and parallelizing computations in time. We provide theoretical guarantees that SING approximately optimizes the intractable, continuous-time objective of interest. Moreover, we demonstrate that better state inference enables more accurate estimation of nonlinear drift functions using, for example, Gaussian process SDE models. SING outperforms prior methods in state inference and drift estimation on a variety of datasets, including a challenging application to modeling neural dynamics in freely behaving animals. Altogether, our results illustrate the potential of SING as a tool for accurate inference in complex dynamical systems, especially those characterized by limited prior knowledge and non-conjugate structure.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Policy Switching for Secure UAV Deconfliction in Adversarial Airspace</title>
<link>https://arxiv.org/abs/2506.21127</link>
<guid>https://arxiv.org/abs/2506.21127</guid>
<content:encoded><![CDATA[
arXiv:2506.21127v2 Announce Type: replace 
Abstract: Autonomous UAV navigation using reinforcement learning (RL) is vulnerable to adversarial attacks that manipulate sensor inputs, potentially leading to unsafe behavior and mission failure. Although robust RL methods provide partial protection, they often struggle to generalize to unseen or out-of-distribution (OOD) attacks due to their reliance on fixed perturbation settings. To address this limitation, we propose a meta-policy switching framework in which a meta-level polic dynamically selects among multiple robust policies to counter unknown adversarial shifts. At the core of this framework lies a discounted Thompson sampling (DTS) mechanism that formulates policy selection as a multi-armed bandit problem, thereby minimizing value distribution shifts via self-induced adversarial observations. We first construct a diverse ensemble of action-robust policies trained under varying perturbation intensities. The DTS-based meta-policy then adaptively selects among these policies online, optimizing resilience against self-induced, piecewise-stationary attacks. Theoretical analysis shows that the DTS mechanism minimizes expected regret, ensuring adaptive robustness to OOD attacks and exhibiting emergent antifragile behavior under uncertainty. Extensive simulations in complex 3D obstacle environments under both white-box (Projected Gradient Descent) and black-box (GPS spoofing) attacks demonstrate significantly improved navigation efficiency and higher conflict free trajectory rates compared to standard robust and vanilla RL baselines, highlighting the practical security and dependability benefits of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedRef: Communication-Efficient Bayesian Fine-Tuning using a Reference Model</title>
<link>https://arxiv.org/abs/2506.23210</link>
<guid>https://arxiv.org/abs/2506.23210</guid>
<content:encoded><![CDATA[
arXiv:2506.23210v4 Announce Type: replace 
Abstract: Federated learning (FL) collaboratively trains artificial intelligence (AI) models to ensure user data privacy. Sharing only model updates generated from local training on client data with the server enhances user data privacy. However, model performance may suffer due to data and system heterogeneity among clients in FL scenarios. Previous studies have proposed model optimization, fine-tuning, and personalization to achieve improved model performance. Despite these efforts, models resulting from FL scenarios often exhibit catastrophic forgetting, which increases the communication and computational costs of clients for model optimization and raises energy consumption. To address these challenges, we propose a reference model-based fine-tuning method for federated learning that overcomes catastrophic forgetting in each round. Our method is derived from Bayesian parameter-efficient transfer learning and includes an proximal term. It employs a reference model that incorporates previous model parameters and reviews previous global features in the model optimization step to mitigate catastrophic forgetting. As a result, our method achieves higher model performance and lower communication and computational costs for clients than existing methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classification of autoimmune diseases from Peripheral blood TCR repertoires by multimodal multi-instance learning</title>
<link>https://arxiv.org/abs/2507.04981</link>
<guid>https://arxiv.org/abs/2507.04981</guid>
<content:encoded><![CDATA[
arXiv:2507.04981v4 Announce Type: replace 
Abstract: T cell receptor (TCR) repertoires encode critical immunological signatures for autoimmune diseases, yet their clinical application remains limited by sequence sparsity and low witness rates. We developed EAMil, a multi-instance deep learning framework that leverages TCR sequencing data to diagnose systemic lupus erythematosus (SLE) and rheumatoid arthritis (RA) with exceptional accuracy. By integrating PrimeSeq feature extraction with ESMonehot encoding and enhanced gate attention mechanisms, our model achieved state-of-the-art performance with AUCs of 98.95% for SLE and 97.76% for RA. EAMil successfully identified disease-associated genes with over 90% concordance with established differential analyses and effectively distinguished disease-specific TCR genes. The model demonstrated robustness in classifying multiple disease categories, utilizing the SLEDAI score to stratify SLE patients by disease severity as well as to diagnose the site of damage in SLE patients, and effectively controlling for confounding factors such as age and gender. This interpretable framework for immune receptor analysis provides new insights for autoimmune disease detection and classification with broad potential clinical applications across immune-mediated conditions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference</title>
<link>https://arxiv.org/abs/2507.06567</link>
<guid>https://arxiv.org/abs/2507.06567</guid>
<content:encoded><![CDATA[
arXiv:2507.06567v2 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language models (LLMs) by activating only a small subset of relevant experts per input. However, the sheer number of expert networks in an MoE model introduces a significant storage burden for an edge device. To address this challenge, we consider a scenario where experts are dispersed across an edge network for distributed inference. Based on the popular Top-$K$ expert selection strategy, we formulate a latency minimization problem by optimizing expert caching on edge servers under storage constraints. When $K=1$, the problem reduces to a monotone submodular maximization problem with knapsack constraints, for which we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee. For the general case where $K \geq 1$, expert co-activation within the same MoE layer introduces non-submodularity, which renders greedy methods ineffective. To tackle this issue, we propose a successive greedy decomposition method to decompose the original problem into a series of subproblems, with each being solved by a dynamic programming approach. Furthermore, we design an accelerated algorithm based on the max-convolution technique to obtain the approximate solution with a provable guarantee in polynomial time. Simulation results on various MoE models demonstrate that our method significantly reduces inference latency compared to existing baselines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General-Purpose Models for the Chemical Sciences: LLMs and Beyond</title>
<link>https://arxiv.org/abs/2507.07456</link>
<guid>https://arxiv.org/abs/2507.07456</guid>
<content:encoded><![CDATA[
arXiv:2507.07456v2 Announce Type: replace 
Abstract: Data-driven techniques have a large potential to transform and accelerate the chemical sciences. However, chemical sciences also pose the unique challenge of very diverse, small, fuzzy datasets that are difficult to leverage in conventional machine learning approaches. A new class of models, which can be summarized under the term general-purpose models (GPMs) such as large language models, has shown the ability to solve tasks they have not been directly trained on, and to flexibly operate with low amounts of data in different formats. In this review, we discuss fundamental building principles of GPMs and review recent and emerging applications of those models in the chemical sciences across the entire scientific process. While many of these applications are still in the prototype phase, we expect that the increasing interest in GPMs will make many of them mature in the coming years.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients</title>
<link>https://arxiv.org/abs/2507.10241</link>
<guid>https://arxiv.org/abs/2507.10241</guid>
<content:encoded><![CDATA[
arXiv:2507.10241v2 Announce Type: replace 
Abstract: Physics-informed machine learning frameworks such as Physics-Informed Neural Networks (PINNs) and Physics-Informed Extreme Learning Machines (PI-ELMs) have shown great promise for solving partial differential equations (PDEs) but struggle with localized sharp gradients and singularly perturbed regimes, PINNs due to spectral bias and PI-ELMs due to their single-shot, non-adaptive formulation. We propose the Kernel-Adaptive Physics-Informed Extreme Learning Machine (KAPI-ELM), which performs Bayesian optimization over a low-dimensional, physically interpretable hyperparameter space governing the distribution of Radial Basis Function (RBF) centers and widths. This converts high-dimensional weight optimization into a low-dimensional distributional search, enabling targeted kernel refinement in regions with sharp gradients while also improving baseline solutions in smooth-flow regimes by tuning RBF supports. KAPI-ELM is validated on benchmark forward and inverse problems (1D convection-diffusion and 2D Poisson) involving PDEs with sharp gradients. It accurately resolves steep layers, improves smooth-solution fidelity, and recovers physical parameters robustly, matching or surpassing advanced methods such as the extended Theory of Functional Connections (X-TFC) with nearly an order of magnitude fewer tunable parameters. An extension to nonlinear problems is demonstrated by a curriculum-based solution of the steady Navier-Stokes equations via successive linearizations, yielding stable solutions for benchmark lid-driven cavity flow up to Re=100. These results indicate that KAPI-ELM provides an efficient and unified approach for forward and inverse PDEs, particularly in challenging sharp-gradient regimes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides</title>
<link>https://arxiv.org/abs/2508.00578</link>
<guid>https://arxiv.org/abs/2508.00578</guid>
<content:encoded><![CDATA[
arXiv:2508.00578v2 Announce Type: replace 
Abstract: Hydrogen atom transfer (HAT) reactions are essential in many biological processes, such as radical migration in damaged proteins, but their mechanistic pathways remain incompletely understood. Simulating HAT is challenging due to the need for quantum chemical accuracy at biologically relevant scales; thus, neither classical force fields nor DFT-based molecular dynamics are applicable. Machine-learned potentials offer an alternative, able to learn potential energy surfaces (PESs) with near-quantum accuracy. However, training these models to generalize across diverse HAT configurations, especially at radical positions in proteins, requires tailored data generation and careful model selection. Here, we systematically generate HAT configurations in peptides to build large datasets using semiempirical methods and DFT. We benchmark three graph neural network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT PESs and indirectly predict reaction barriers from energy predictions. MACE consistently outperforms the others in energy, force, and barrier prediction, achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT barrier predictions. Using molecular dynamics, we show our MACE potential is stable, reactive, and generalizes beyond training data to model HAT barriers in collagen I. This accuracy enables integration of ML potentials into large-scale collagen simulations to compute reaction rates from predicted barriers, advancing mechanistic understanding of HAT and radical migration in peptides. We analyze scaling laws, model transferability, and cost-performance trade-offs, and outline strategies for improvement by combining ML potentials with transition state search algorithms and active learning. Our approach is generalizable to other biomolecular systems, enabling quantum-accurate simulations of chemical reactivity in complex environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAIR-Pruner: Leveraging Tolerance of Difference for Flexible Automatic Layer-Wise Neural Network Pruning</title>
<link>https://arxiv.org/abs/2508.02291</link>
<guid>https://arxiv.org/abs/2508.02291</guid>
<content:encoded><![CDATA[
arXiv:2508.02291v2 Announce Type: replace 
Abstract: Neural network pruning has been widely adopted to reduce the parameter scale of complex neural networks, enabling efficient deployment on resource-limited edge devices. Mainstream pruning methods typically adopt uniform pruning strategies, which tend to cause a substantial performance degradation under high sparsity levels. Recent studies focus on non-uniform layer-wise pruning, but such approaches typically depend on global architecture optimization, which is computational expensive and lacks flexibility. To address these limitations, this paper proposes a novel method named Flexible Automatic Identification and Removal (FAIR)-Pruner, which adaptively determines the sparsity levels of each layer and identifies the units to be pruned. The core of FAIR-Pruner lies in the introduction of a novel indicator, Tolerance of Differences (ToD), designed to balance the importance scores obtained from two complementary perspectives: the architecture-level (Utilization Score) and the task-level (Reconstruction Score). By controlling ToD at preset levels, FAIR-Pruner determines layer-specific thresholds and removes units whose Utilization Scores fall below the corresponding thresholds. Furthermore, by decoupling threshold determination from importance estimation, FAIR-Pruner allows users to flexibly obtain pruned models under varying pruning ratios. Extensive experiments demonstrate that FAIR-Pruner achieves state-of-the-art performance, maintaining higher accuracy even at high compression ratios. Moreover, the ToD based layer-wise pruning ratios can be directly applied to existing powerful importance measurements, thereby improving the performance under uniform-pruning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning</title>
<link>https://arxiv.org/abs/2508.05224</link>
<guid>https://arxiv.org/abs/2508.05224</guid>
<content:encoded><![CDATA[
arXiv:2508.05224v2 Announce Type: replace 
Abstract: Federated learning (FL) enables collaborative model training across distributed clients while preserving data privacy by keeping data local. Traditional FL approaches rely on a centralized, star-shaped topology, where a central server aggregates model updates from clients. However, this architecture introduces several limitations, including a single point of failure, limited personalization, and poor robustness to distribution shifts or vulnerability to malfunctioning clients. Moreover, update selection in centralized FL often relies on low-level parameter differences, which can be unreliable when client data is not independent and identically distributed, and offer clients little control. In this work, we propose a decentralized, peer-to-peer (P2P) FL framework. It leverages the flexibility of the P2P topology to enable each client to identify and aggregate a personalized set of trustworthy and beneficial updates.This framework is the Local Inference Guided Aggregation for Heterogeneous Training Environments to Yield Enhancement Through Agreement and Regularization (LIGHTYEAR). Central to our method is an agreement score, computed on a local validation set, which quantifies the semantic alignment of incoming updates in the function space with respect to the clients reference model. Each client uses this score to select a tailored subset of updates and performs aggregation with a regularization term that further stabilizes the training. Our empirical evaluation across five datasets shows that the proposed approach consistently outperforms both, centralized baselines and existing P2P methods in terms of client-level performance, particularly under adversarial and heterogeneous conditions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)</title>
<link>https://arxiv.org/abs/2508.06251</link>
<guid>https://arxiv.org/abs/2508.06251</guid>
<content:encoded><![CDATA[
arXiv:2508.06251v2 Announce Type: replace 
Abstract: Synthetic data generation is a key technique in modern artificial intelligence, addressing data scarcity, privacy constraints, and the need for diverse datasets in training robust models. In this work, we propose a method for generating privacy-preserving high-quality synthetic tabular data using Tensor Networks, specifically Matrix Product States (MPS). We benchmark the MPS-based generative model against state-of-the-art models such as CTGAN, VAE, and PrivBayes, focusing on both fidelity and privacy-preserving capabilities. To ensure differential privacy (DP), we integrate noise injection and gradient clipping during training, enabling privacy guarantees via R\'enyi Differential Privacy accounting. Across multiple metrics analyzing data fidelity and downstream machine learning task performance, our results show that MPS outperforms classical models, particularly under strict privacy constraints. This work highlights MPS as a promising tool for privacy-aware synthetic data generation. By combining the expressive power of tensor network representations with formal privacy mechanisms, the proposed approach offers an interpretable and scalable alternative for secure data sharing. Its structured design facilitates integration into sensitive domains where both data quality and confidentiality are critical.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Does Stochastic Gradient Descent Slow Down in Low-Precision Training?</title>
<link>https://arxiv.org/abs/2508.07142</link>
<guid>https://arxiv.org/abs/2508.07142</guid>
<content:encoded><![CDATA[
arXiv:2508.07142v3 Announce Type: replace 
Abstract: Low-precision training has become crucial for reducing the computational and memory costs of large-scale deep learning. However, quantizing gradients introduces magnitude shrinkage, which can change how stochastic gradient descent (SGD) converges. In this study, we explore SGD convergence under a gradient shrinkage model, where each stochastic gradient is scaled by a factor \( q_k \in (0,1] \). We show that this shrinkage affect the usual stepsize \( \mu_k \) with an effective stepsize \( \mu_k q_k \), slowing convergence when \( q_{\min} < 1 \). With typical smoothness and bounded-variance assumptions, we prove that low-precision SGD still converges, but at a slower pace set by \( q_{\min} \), and with a higher steady error level due to quantization effects. We analyze theoretically how lower numerical precision slows training by treating it as gradient shrinkage within the standard SGD convergence setup.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring</title>
<link>https://arxiv.org/abs/2508.09527</link>
<guid>https://arxiv.org/abs/2508.09527</guid>
<content:encoded><![CDATA[
arXiv:2508.09527v2 Announce Type: replace 
Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events in ongoing cases based on historical event logs. While Graph Neural Networks (GNNs) are well suited to capture structural dependencies in process data, existing GNN-based PBPM models remain underdeveloped. Most rely either on short prefix subgraphs or global architectures that overlook temporal relevance and transition semantics. We propose a unified, interpretable GNN framework that advances the state of the art along three key axes. First, we compare prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention Networks(GATs) to quantify the performance gap between localized and global modeling. Second, we introduce a novel time decay attention mechanism that constructs dynamic, prediction-centered windows, emphasizing temporally relevant history and suppressing noise. Third, we embed transition type semantics into edge features to enable fine grained reasoning over structurally ambiguous traces. Our architecture includes multilevel interpretability modules, offering diverse visualizations of attention behavior. Evaluated on five benchmarks, the proposed models achieve competitive Top-k accuracy and DL scores without per-dataset tuning. By addressing architectural, temporal, and semantic gaps, this work presents a robust, generalizable, and explainable solution for next event prediction in PBPM.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts</title>
<link>https://arxiv.org/abs/2508.10123</link>
<guid>https://arxiv.org/abs/2508.10123</guid>
<content:encoded><![CDATA[
arXiv:2508.10123v2 Announce Type: replace 
Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In standard ReFT frameworks, a behavior model generates multiple completions with answers per problem, for the answer to be then scored by a reward function. While such RL post-training methods demonstrate significant performance improvements across challenging reasoning domains, the computational cost of generating completions during training with multiple inference steps makes the training cost non-trivial. To address this, we draw inspiration from off-policy RL, and speculative decoding to introduce a novel ReFT framework, dubbed Nested-ReFT, where a subset of layers of the target model acts as the behavior model to generate off-policy completions during training. The behavior model configured with dynamic layer skipping per batch during training decreases the inference cost compared to the standard ReFT frameworks. Our theoretical analysis shows that Nested-ReFT yields unbiased gradient estimates with controlled variance. Our empirical analysis demonstrates improved computational efficiency measured as tokens/sec across multiple math reasoning benchmarks and model sizes. Additionally, we explore three variants of bias mitigation to minimize the off-policyness in the gradient updates that allows for maintaining performance that matches the baseline ReFT performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cost-Aware Contrastive Routing for LLMs</title>
<link>https://arxiv.org/abs/2508.12491</link>
<guid>https://arxiv.org/abs/2508.12491</guid>
<content:encoded><![CDATA[
arXiv:2508.12491v3 Announce Type: replace 
Abstract: We study cost-aware routing for large language models across diverse and dynamic pools of models. Existing approaches often overlook prompt-specific context, rely on expensive model profiling, assume a fixed set of experts, or use inefficient trial-and-error strategies. We introduce Cost-Spectrum Contrastive Routing (CSCR), a lightweight framework that maps both prompts and models into a shared embedding space to enable fast, cost-sensitive selection. CSCR uses compact, fast-to-compute logit footprints for open-source models and perplexity fingerprints for black-box APIs. A contrastive encoder is trained to favor the cheapest accurate expert within adaptive cost bands. At inference time, routing reduces to a single k-NN lookup via a FAISS index, requiring no retraining when the expert pool changes and enabling microsecond latency. Across multiple benchmarks, CSCR consistently outperforms baselines, improving the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen LLMs and out-of-distribution prompts.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Protein-Ligand Binding in Hyperbolic Space</title>
<link>https://arxiv.org/abs/2508.15480</link>
<guid>https://arxiv.org/abs/2508.15480</guid>
<content:encoded><![CDATA[
arXiv:2508.15480v2 Announce Type: replace 
Abstract: Protein-ligand binding prediction is central to virtual screening and affinity ranking, two fundamental tasks in drug discovery. While recent retrieval-based methods embed ligands and protein pockets into Euclidean space for similarity-based search, the geometry of Euclidean embeddings often fails to capture the hierarchical structure and fine-grained affinity variations intrinsic to molecular interactions. In this work, we propose HypSeek, a hyperbolic representation learning framework that embeds ligands, protein pockets, and sequences into Lorentz-model hyperbolic space. By leveraging the exponential geometry and negative curvature of hyperbolic space, HypSeek enables expressive, affinity-sensitive embeddings that can effectively model both global activity and subtle functional differences-particularly in challenging cases such as activity cliffs, where structurally similar ligands exhibit large affinity gaps. Our mode unifies virtual screening and affinity ranking in a single framework, introducing a protein-guided three-tower architecture to enhance representational structure. HypSeek improves early enrichment in virtual screening on DUD-E from 42.63 to 51.44 (+20.7%) and affinity ranking correlation on JACS from 0.5774 to 0.7239 (+25.4%), demonstrating the benefits of hyperbolic geometry across both tasks and highlighting its potential as a powerful inductive bias for protein-ligand modeling.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speech Foundation Models Generalize to Time Series Tasks from Wearable Sensor Data</title>
<link>https://arxiv.org/abs/2509.00221</link>
<guid>https://arxiv.org/abs/2509.00221</guid>
<content:encoded><![CDATA[
arXiv:2509.00221v3 Announce Type: replace 
Abstract: Both speech and sensor time series data encode information in both the time- and frequency- domains, like spectral powers and waveform shapelets. We show that speech foundation models learn representations that generalize beyond the speech domain and achieve state-of-the-art performance on diverse time-series tasks from wearable sensors. Probes trained on features extracted from HuBERT and wav2vec 2.0 outperform those extracted from self-supervised models trained directly on modality-specific datasets for mood classification, arrhythmia detection, and activity classification tasks. We find that the convolutional feature encoders of speech models are particularly relevant for wearable sensor applications. The proposed approach enhances performance on data-scarce time-series tasks using simple probing methods. This work takes a step toward developing generalized time-series models that unify speech and sensor modalities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Counterfactual Labels for Efficient Conformal Counterfactual Inference</title>
<link>https://arxiv.org/abs/2509.04112</link>
<guid>https://arxiv.org/abs/2509.04112</guid>
<content:encoded><![CDATA[
arXiv:2509.04112v2 Announce Type: replace 
Abstract: This work addresses the problem of constructing reliable prediction intervals for individual counterfactual outcomes. Existing conformal counterfactual inference (CCI) methods provide marginal coverage guarantees but often produce overly conservative intervals, particularly under treatment imbalance when counterfactual samples are scarce. We introduce synthetic data-powered CCI (SP-CCI), a new framework that augments the calibration set with synthetic counterfactual labels generated by a pre-trained counterfactual model. To ensure validity, SP-CCI incorporates synthetic samples into a conformal calibration procedure based on risk-controlling prediction sets (RCPS) with a debiasing step informed by prediction-powered inference (PPI). We prove that SP-CCI achieves tighter prediction intervals while preserving marginal coverage, with theoretical guarantees under both exact and approximate importance weighting. Empirical results on different datasets confirm that SP-CCI consistently reduces interval width compared to standard CCI across all settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Scaling Laws for Deep Regression</title>
<link>https://arxiv.org/abs/2509.10000</link>
<guid>https://arxiv.org/abs/2509.10000</guid>
<content:encoded><![CDATA[
arXiv:2509.10000v2 Announce Type: replace 
Abstract: Neural scaling laws--power-law relationships between generalization errors and characteristics of deep learning models--are vital tools for developing reliable models while managing limited resources. Although the success of large language models highlights the importance of these laws, their application to deep regression models remains largely unexplored. Here, we empirically investigate neural scaling laws in deep regression using a parameter estimation model for twisted van der Waals magnets. We observe power-law relationships between the loss and both training dataset size and model capacity across a wide range of values, employing various architectures--including fully connected networks, residual networks, and vision transformers. Furthermore, the scaling exponents governing these relationships range from 1 to 2, with specific values depending on the regressed parameters and model details. The consistent scaling behaviors and their large scaling exponents suggest that the performance of deep regression models can improve substantially with increasing data size.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition</title>
<link>https://arxiv.org/abs/2509.10729</link>
<guid>https://arxiv.org/abs/2509.10729</guid>
<content:encoded><![CDATA[
arXiv:2509.10729v2 Announce Type: replace 
Abstract: Sensor data streams provide valuable information around activities and context for downstream applications, though integrating complementary information can be challenging. We show that large language models (LLMs) can be used for late fusion for activity classification from audio and motion time series data. We curated a subset of data for diverse activity recognition across contexts (e.g., household activities, sports) from the Ego4D dataset. Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores significantly above chance, with no task-specific training. Zero-shot classification via LLM-based fusion from modality-specific models can enable multimodal temporal applications where there is limited aligned training data for learning a shared embedding space. Additionally, LLM-based fusion can enable model deploying without requiring additional memory and computation for targeted application-specific multimodal models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KANO: Kolmogorov-Arnold Neural Operator</title>
<link>https://arxiv.org/abs/2509.16825</link>
<guid>https://arxiv.org/abs/2509.16825</guid>
<content:encoded><![CDATA[
arXiv:2509.16825v3 Announce Type: replace 
Abstract: We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural operator jointly parameterized by both spectral and spatial bases with intrinsic symbolic interpretability. We theoretically demonstrate that KANO overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO remains expressive over generic position-dependent dynamics (variable coefficient PDEs) for any physical input, whereas FNO stays practical only for spectrally sparse operators and strictly imposes a fast-decaying input Fourier tail. We verify our claims empirically on position-dependent differential operators, for which KANO robustly generalizes but FNO fails to. In the quantum Hamiltonian learning benchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic representations accurate to the fourth decimal place in coefficients and attains $\approx 6\times10^{-6}$ state infidelity from projective measurement data, substantially outperforming that of the FNO trained with ideal full wave function data, $\approx 1.5\times10^{-2}$, by orders of magnitude.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MDBench: Benchmarking Data-Driven Methods for Model Discovery</title>
<link>https://arxiv.org/abs/2509.20529</link>
<guid>https://arxiv.org/abs/2509.20529</guid>
<content:encoded><![CDATA[
arXiv:2509.20529v2 Announce Type: replace 
Abstract: Model discovery aims to uncover governing differential equations of dynamical systems directly from experimental data. Benchmarking such methods is essential for tracking progress and understanding trade-offs in the field. While prior efforts have focused mostly on identifying single equations, typically framed as symbolic regression, there remains a lack of comprehensive benchmarks for discovering dynamical models. To address this, we introduce MDBench, an open-source benchmarking framework for evaluating model discovery methods on dynamical systems. MDBench assesses 12 algorithms on 14 partial differential equations (PDEs) and 63 ordinary differential equations (ODEs) under varying levels of noise. Evaluation metrics include derivative prediction accuracy, model complexity, and equation fidelity. We also introduce seven challenging PDE systems from fluid dynamics and thermodynamics, revealing key limitations in current methods. Our findings illustrate that linear methods and genetic programming methods achieve the lowest prediction error for PDEs and ODEs, respectively. Moreover, linear models are in general more robust against noise. MDBench accelerates the advancement of model discovery methods by offering a rigorous, extensible benchmarking framework and a rich, diverse collection of dynamical system datasets, enabling systematic evaluation, comparison, and improvement of equation accuracy and robustness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Scaling Laws of Feature Emergence from Learning Dynamics of Grokking</title>
<link>https://arxiv.org/abs/2509.21519</link>
<guid>https://arxiv.org/abs/2509.21519</guid>
<content:encoded><![CDATA[
arXiv:2509.21519v4 Announce Type: replace 
Abstract: While the phenomenon of grokking, i.e., delayed generalization, has been studied extensively, it remains an open problem whether there is a mathematical framework that characterizes what kind of features will emerge, how and in which conditions it happens, and is closely related to the gradient dynamics of the training, for complex structured inputs. We propose a novel framework, named $\mathbf{Li}_2$, that captures three key stages for the grokking behavior of 2-layer nonlinear networks: (I) Lazy learning, (II) independent feature learning and (III) interactive feature learning. At the lazy learning stage, top layer overfits to random hidden representation and the model appears to memorize. Thanks to lazy learning and weight decay, the backpropagated gradient $G_F$ from the top layer now carries information about the target label, with a specific structure that enables each hidden node to learn their representation independently. Interestingly, the independent dynamics follows exactly the gradient ascent of an energy function $E$, and its local maxima are precisely the emerging features. We study whether these local-optima induced features are generalizable, their representation power, and how they change on sample size, in group arithmetic tasks. When hidden nodes start to interact in the later stage of learning, we provably show how $G_F$ changes to focus on missing features that need to be learned. Our study sheds lights on roles played by key hyperparameters such as weight decay, learning rate and sample sizes in grokking, leads to provable scaling laws of feature emergence, memorization and generalization, and reveals why recent optimizers such as Muon can be effective, from the first principles of gradient dynamics. Our analysis can be extended to multi-layers. The code is available at https://github.com/yuandong-tian/understanding/tree/main/ssl/real-dataset/cogo.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data</title>
<link>https://arxiv.org/abs/2509.22850</link>
<guid>https://arxiv.org/abs/2509.22850</guid>
<content:encoded><![CDATA[
arXiv:2509.22850v3 Announce Type: replace 
Abstract: Adversarial robustness in structured data remains an underexplored frontier compared to vision and language domains. In this work, we introduce a novel black-box, decision-based adversarial attack tailored for tabular data. Our approach combines gradient-free direction estimation with an iterative boundary search, enabling efficient navigation of discrete and continuous feature spaces under minimal oracle access. Extensive experiments demonstrate that our method successfully compromises nearly the entire test set across diverse models, ranging from classical machine learning classifiers to large language model (LLM)-based pipelines. Remarkably, the attack achieves success rates consistently above 90%, while requiring only a small number of queries per instance. These results highlight the critical vulnerability of tabular models to adversarial perturbations, underscoring the urgent need for stronger defenses in real-world decision-making systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Posterior Collapse as a Phase Transition in Variational Autoencoders</title>
<link>https://arxiv.org/abs/2510.01621</link>
<guid>https://arxiv.org/abs/2510.01621</guid>
<content:encoded><![CDATA[
arXiv:2510.01621v2 Announce Type: replace 
Abstract: We investigate the phenomenon of posterior collapse in variational autoencoders (VAEs) from the perspective of statistical physics, and reveal that it constitutes a phase transition governed jointly by data structure and model hyper-parameters. By analyzing the stability of the trivial solution associated with posterior collapse, we identify a critical hyper-parameter threshold. In particular, we derive an explicit criterion for the onset of collapse: posterior collapse occurs when the decoder variance exceeds the largest eigenvalue of the data covariance matrix. This critical boundary, separating meaningful latent inference from collapse, is characterized by a discontinuity in the KL divergence between the approximate posterior and the prior distribution, where the KL divergence and its derivatives exhibit clear non-analytic behavior. We validate this critical behavior on both synthetic and real-world datasets, confirming the existence of a phase transition. The experimental results align well with our theoretical predictions, demonstrating the robustness of our collapse criterion across various VAE architectures. Our stability-based analysis demonstrate that posterior collapse is not merely an optimization failure, but rather an emerging phase transition arising from the interplay between data structure and variational constraints. This perspective offers new insights into the trainability and representational capacity of deep generative models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Grained GRPO for Precise Preference Alignment in Flow Models</title>
<link>https://arxiv.org/abs/2510.01982</link>
<guid>https://arxiv.org/abs/2510.01982</guid>
<content:encoded><![CDATA[
arXiv:2510.01982v3 Announce Type: replace 
Abstract: The incorporation of online reinforcement learning (RL) into diffusion and flow-based generative models has recently gained attention as a powerful paradigm for aligning model behavior with human preferences. By leveraging stochastic sampling via Stochastic Differential Equations (SDEs) during the denoising phase, these models can explore a variety of denoising trajectories, enhancing the exploratory capacity of RL. However, despite their ability to discover potentially high-reward samples, current approaches often struggle to effectively align with preferences due to the sparsity and narrowness of reward feedback. To overcome this limitation, we introduce a novel framework called Granular-GRPO (G$^2$RPO), which enables fine-grained and comprehensive evaluation of sampling directions in the RL training of flow models. Specifically, we propose a Singular Stochastic Sampling mechanism that supports step-wise stochastic exploration while ensuring strong correlation between injected noise and reward signals, enabling more accurate credit assignment to each SDE perturbation. Additionally, to mitigate the bias introduced by fixed-granularity denoising, we design a Multi-Granularity Advantage Integration module that aggregates advantages computed across multiple diffusion scales, resulting in a more robust and holistic assessment of sampling trajectories. Extensive experiments on various reward models, including both in-domain and out-of-domain settings, demonstrate that our G$^2$RPO outperforms existing flow-based GRPO baselines, highlighting its effectiveness and generalization capability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification</title>
<link>https://arxiv.org/abs/2510.02779</link>
<guid>https://arxiv.org/abs/2510.02779</guid>
<content:encoded><![CDATA[
arXiv:2510.02779v2 Announce Type: replace 
Abstract: Recent advances have significantly improved our understanding of the generalization performance of gradient descent (GD) methods in deep neural networks. A natural and fundamental question is whether GD can achieve generalization rates comparable to the minimax optimal rates established in the kernel setting. Existing results either yield suboptimal rates of $O(1/\sqrt{n})$, or focus on networks with smooth activation functions, incurring exponential dependence on network depth $L$. In this work, we establish optimal generalization rates for GD with deep ReLU networks by carefully trading off optimization and generalization errors, achieving only polynomial dependence on depth. Specifically, under the assumption that the data are NTK separable from the margin $\gamma$, we prove an excess risk rate of $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$, which aligns with the optimal SVM-type rate $\widetilde{O}(1 / (n \gamma^2))$ up to depth-dependent factors. A key technical contribution is our novel control of activation patterns near a reference model, enabling a sharper Rademacher complexity bound for deep ReLU networks trained with gradient descent.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindCraft: How Concept Trees Take Shape In Deep Models</title>
<link>https://arxiv.org/abs/2510.03265</link>
<guid>https://arxiv.org/abs/2510.03265</guid>
<content:encoded><![CDATA[
arXiv:2510.03265v2 Announce Type: replace 
Abstract: Large-scale foundation models demonstrate strong performance across language, vision, and reasoning tasks. However, how they internally structure and stabilize concepts remains elusive. Inspired by causal inference, we introduce the MindCraft framework built upon Concept Trees. By applying spectral decomposition at each layer and linking principal directions into branching Concept Paths, Concept Trees reconstruct the hierarchical emergence of concepts, revealing exactly when they diverge from shared representations into linearly separable subspaces. Empirical evaluations across diverse scenarios across disciplines, including medical diagnosis, physics reasoning, and political decision-making, show that Concept Trees recover semantic hierarchies, disentangle latent concepts, and can be widely applied across multiple domains. The Concept Tree establishes a widely applicable and powerful framework that enables in-depth analysis of conceptual representations in deep models, marking a significant step forward in the foundation of interpretable AI.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting-based Biomedical Time-series Data Synthesis for Open Data and Robust AI</title>
<link>https://arxiv.org/abs/2510.04622</link>
<guid>https://arxiv.org/abs/2510.04622</guid>
<content:encoded><![CDATA[
arXiv:2510.04622v2 Announce Type: replace 
Abstract: The limited data availability due to strict privacy regulations and significant resource demands severely constrains biomedical time-series AI development, which creates a critical gap between data requirements and accessibility. Synthetic data generation presents a promising solution by producing artificial datasets that maintain the statistical properties of real biomedical time-series data without compromising patient confidentiality. While GANs, VAEs, and diffusion models capture global data distributions, forecasting models offer inductive biases tailored for sequential dynamics. We propose a framework for synthetic biomedical time-series data generation based on recent forecasting models that accurately replicates complex electrophysiological signals such as EEG and EMG with high fidelity. These synthetic datasets can be freely shared for open AI development and consistently improve downstream model performance. Numerical results on sleep-stage classification show up to a 3.71\% performance gain with augmentation and a 91.00\% synthetic-only accuracy that surpasses the real-data-only baseline.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>