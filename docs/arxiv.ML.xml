<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>Learning functions through Diffusion Maps</title>
<link>https://arxiv.org/abs/2509.03758</link>
<guid>https://arxiv.org/abs/2509.03758</guid>
<content:encoded><![CDATA[
<div> Keywords: data-driven method, smooth manifolds, Diffusion Maps framework, dimensionality reduction, online updating mechanism

Summary:
A data-driven method for approximating real-valued functions on smooth manifolds is proposed, leveraging the Diffusion Maps framework. By extending the function to the ambient space using diffusion geometry and the Laplace-Beltrami operator, accurate approximations can be made. To deal with high-dimensional data, a dimensionality reduction strategy based on the low-rank structure of the distance matrix is introduced through singular value decomposition (SVD). An online updating mechanism allows for efficient integration of new data, enhancing scalability and reducing computational costs. Numerical experiments, including sparse CT reconstruction, highlight the superior performance of this methodology over traditional feedforward neural networks and interpolation methods in terms of accuracy and efficiency. <div>
arXiv:2509.03758v2 Announce Type: replace 
Abstract: We propose a data-driven method for approximating real-valued functions on smooth manifolds, building on the Diffusion Maps framework under the manifold hypothesis. Given pointwise evaluations of a function, the method constructs a smooth extension to the ambient space by exploiting diffusion geometry and its connection to the heat equation and the Laplace-Beltrami operator.
  To address the computational challenges of high-dimensional data, we introduce a dimensionality reduction strategy based on the low-rank structure of the distance matrix, revealed via singular value decomposition (SVD). In addition, we develop an online updating mechanism that enables efficient incorporation of new data, thereby improving scalability and reducing computational cost.
  Numerical experiments, including applications to sparse CT reconstruction, demonstrate that the proposed methodology outperforms classical feedforward neural networks and interpolation methods in terms of both accuracy and efficiency.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Uncertainty Estimation using Variance-Gated Distributions</title>
<link>https://arxiv.org/abs/2509.08846</link>
<guid>https://arxiv.org/abs/2509.08846</guid>
<content:encoded><![CDATA[
<div> Bayesian models, uncertainty quantification, neural networks, signal-to-noise ratio, ensemble learning<br />
Summary:<br />
This article explores uncertainty quantification in neural networks, crucial for high-risk decision-making. It questions the traditional additive decomposition of predictive uncertainty into epistemic and aleatoric components. A new framework based on the signal-to-noise ratio of class probability distributions is proposed for uncertainty estimation and decomposition. The framework includes a variance-gated measure that scales predictions by a confidence factor from ensembles. The study discusses the concept of a collapse in the diversity of committee machines, shedding light on the limitations of current uncertainty quantification methods in neural networks. <div>
arXiv:2509.08846v1 Announce Type: new 
Abstract: Evaluation of per-sample uncertainty quantification from neural networks is essential for decision-making involving high-risk applications. A common approach is to use the predictive distribution from Bayesian or approximation models and decompose the corresponding predictive uncertainty into epistemic (model-related) and aleatoric (data-related) components. However, additive decomposition has recently been questioned. In this work, we propose an intuitive framework for uncertainty estimation and decomposition based on the signal-to-noise ratio of class probability distributions across different model predictions. We introduce a variance-gated measure that scales predictions by a confidence factor derived from ensembles. We use this measure to discuss the existence of a collapse in the diversity of committee machines.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications</title>
<link>https://arxiv.org/abs/2509.08911</link>
<guid>https://arxiv.org/abs/2509.08911</guid>
<content:encoded><![CDATA[
<div> regret bound, matrix LEA, online learning, quantum relative entropy, Laplace transform<br />
Summary:<br />
- The paper introduces an improved algorithm for the Matrix Multiplicative Weight Update (MMWU) in online learning, achieving an optimal regret bound in the matrix Learning from Expert Advice (LEA) problem. 
- The algorithm's computational complexity is the same as MMWU, making the improved regret bound "free" to attain. 
- A novel potential-based framework for matrix LEA is developed, with a new Jensen's trace inequality allowing the application of general potential functions beyond exponential. 
- The algorithm, based on the imaginary error function, outperforms the state of the art in learning quantum states with depolarization noise, random quantum states, and Gibbs states. 
- Additionally, the algorithm can predict nonlinear quantum properties like purity, quantum virtual cooling, and R\'{e}nyi-$2$ correlation by applying it to linearized convex losses. 
<br /><br />Summary: <div>
arXiv:2509.08911v1 Announce Type: new 
Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning algorithm with numerous applications. Applied to the matrix version of the Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex, it is well known that MMWU achieves the minimax-optimal regret bound of $O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present an improved algorithm achieving the instance-optimal regret bound of $O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret, $I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum relative entropy. Furthermore, our algorithm has the same computational complexity as MMWU, indicating that the improvement in the regret bound is ``free''.
  Technically, we first develop a general potential-based framework for matrix LEA, with MMWU being its special case induced by the standard exponential potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace inequality built on a Laplace transform technique, which allows the application of general potential functions beyond exponential to matrix LEA. Our algorithm is finally induced by an optimal potential function from the vector LEA problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and explore the applications of our algorithm in quantum learning theory. We show that it outperforms the state of the art for learning quantum states corrupted by depolarization noise, random quantum states, and Gibbs states. In addition, applying our algorithm to linearized convex losses enables predicting nonlinear quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$ correlation.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates</title>
<link>https://arxiv.org/abs/2509.08933</link>
<guid>https://arxiv.org/abs/2509.08933</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, adversarial corruption, Q-learning, robustness, asynchronous sampling

Summary:
The article addresses the issue of learning optimal policies in a reinforcement learning setting with adversarial corruption in the reward signal. A new variant of the Q-learning algorithm is proposed to effectively handle corrupted rewards. The algorithm demonstrates robustness under the asynchronous sampling model, showing a convergence rate similar to non-adversarial cases with an additional term related to the fraction of corrupted samples. Additionally, an information-theoretic lower bound highlights the inevitability of the corruption term in the upper bounds. A variant of the algorithm is introduced that does not require prior knowledge of true reward distributions, utilizing a refined Azuma-Hoeffding inequality for almost-martingales. These contributions offer the first finite-time robustness guarantees for asynchronous Q-learning, filling a significant gap in robust reinforcement learning. <div>
arXiv:2509.08933v1 Announce Type: new 
Abstract: We consider the problem of learning the optimal policy in a discounted, infinite-horizon reinforcement learning (RL) setting where the reward signal is subject to adversarial corruption. Such corruption, which may arise from extreme noise, sensor faults, or malicious attacks, can severely degrade the performance of classical algorithms such as Q-learning. To address this challenge, we propose a new provably robust variant of the Q-learning algorithm that operates effectively even when a fraction of the observed rewards are arbitrarily perturbed by an adversary. Under the asynchronous sampling model with time-correlated data, we establish that despite adversarial corruption, the finite-time convergence rate of our algorithm matches that of existing results for the non-adversarial case, up to an additive term proportional to the fraction of corrupted samples. Moreover, we derive an information-theoretic lower bound revealing that the additive corruption term in our upper bounds is unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge of the statistics of the true reward distributions. The analysis of this setting is particularly challenging and is enabled by carefully exploiting a refined Azuma-Hoeffding inequality for almost-martingales, a technical tool that might be of independent interest. Collectively, our contributions provide the first finite-time robustness guarantees for asynchronous Q-learning, bridging a significant gap in robust RL.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty</title>
<link>https://arxiv.org/abs/2509.08942</link>
<guid>https://arxiv.org/abs/2509.08942</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, data distribution, Wasserstein-based optimization, worst-group performance, real-world data

Summary: 
This article introduces a novel framework for improving the performance of machine learning models in applications with multiple heterogeneous data sources. Standard ML methods often learn spurious correlations, leading to suboptimal performance for underrepresented groups. The proposed framework utilizes Wasserstein-based distributionally robust optimization to address distributional uncertainties within each group while aiming to enhance the worst-group performance. The authors present a gradient descent-ascent algorithm to solve the optimization problem and offer convergence results. Real-world data validation demonstrates the effectiveness of the method in mitigating biases and improving model performance across diverse groups. <div>
arXiv:2509.08942v1 Announce Type: new 
Abstract: The performance of machine learning (ML) models critically depends on the quality and representativeness of the training data. In applications with multiple heterogeneous data generating sources, standard ML methods often learn spurious correlations that perform well on average but degrade performance for atypical or underrepresented groups. Prior work addresses this issue by optimizing the worst-group performance. However, these approaches typically assume that the underlying data distributions for each group can be accurately estimated using the training data, a condition that is frequently violated in noisy, non-stationary, and evolving environments. In this work, we propose a novel framework that relies on Wasserstein-based distributionally robust optimization (DRO) to account for the distributional uncertainty within each group, while simultaneously preserving the objective of improving the worst-group performance. We develop a gradient descent-ascent algorithm to solve the proposed DRO problem and provide convergence results. Finally, we validate the effectiveness of our method on real-world data.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis</title>
<link>https://arxiv.org/abs/2509.08961</link>
<guid>https://arxiv.org/abs/2509.08961</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiovascular diseases, ECG analysis, Automated classification, Convolutional Block Attention Module, Graph Attention Networks 

Summary: 
FoundationalECGNet is introduced as a novel framework for automated ECG classification to address challenges in detecting cardiac abnormalities. The model utilizes advanced techniques such as dual-stage denoising, Convolutional Block Attention Module, Graph Attention Networks, and Time Series Transformers to capture spatial and temporal dependencies in multi-channel ECG signals. It achieves a high F1-score of 99% for distinguishing between Normal and Abnormal ECG signals, as well as state-of-the-art performance in multi-class disease detection. Additionally, the model provides risk level estimations to support clinical decision-making, making it a scalable, interpretable, and generalizable solution for automated ECG analysis. The potential of FoundationalECGNet lies in its ability to enhance diagnostic precision and improve patient outcomes in healthcare settings.<br /><br />Summary: <div>
arXiv:2509.08961v1 Announce Type: new 
Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide, underscoring the importance of accurate and scalable diagnostic systems. Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities, yet challenges such as noise, class imbalance, and dataset heterogeneity limit current methods. To address these issues, we propose FoundationalECGNet, a foundational framework for automated ECG classification. The model integrates a dual-stage denoising by Morlet and Daubechies wavelets transformation, Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT), and Time Series Transformers (TST) to jointly capture spatial and temporal dependencies in multi-channel ECG signals. FoundationalECGNet first distinguishes between Normal and Abnormal ECG signals, and then classifies the Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal classification and shows state-of-the-art performance in multi-class disease detection, including a 99% F1-score for Conduction Disorders and Hypertrophy, as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides risk level estimations to facilitate clinical decision-making. In conclusion, FoundationalECGNet represents a scalable, interpretable, and generalizable solution for automated ECG analysis, with the potential to improve diagnostic precision and patient outcomes in healthcare settings. We'll share the code after acceptance.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value bounds and Convergence Analysis for Averages of LRP attributions</title>
<link>https://arxiv.org/abs/2509.08963</link>
<guid>https://arxiv.org/abs/2509.08963</guid>
<content:encoded><![CDATA[
<div> Keywords: Layer-wise relevance propagation, attribution methods, gradient matrices, singular values, convergence

Summary:
In this study, the authors analyze Layer-wise relevance propagation (LRP)-type attribution methods by representing them as modified gradient matrices. They establish an analogy with matrix multiplications of Jacobi-matrices, relating to the chain rule of differentiation. The authors derive upper bounds for singular values and component-wise bounds for attribution map values to shed light on the distribution of attribution values. By applying these component-wise bounds, they identify multiplicative constants that govern the convergence of empirical means of attributions to expectations of attribution maps. This finding has implications for scenarios involving non-geometric data augmentations and Smoothgrad-type attribution methods. Notably, the analysis reveals that the constants for LRP-beta are independent of weight norms, distinguishing it from gradient-based methods and LRP-epsilon.

<br /><br />Summary: <div>
arXiv:2509.08963v1 Announce Type: new 
Abstract: We analyze numerical properties of Layer-wise relevance propagation (LRP)-type attribution methods by representing them as a product of modified gradient matrices. This representation creates an analogy to matrix multiplications of Jacobi-matrices which arise from the chain rule of differentiation. In order to shed light on the distribution of attribution values, we derive upper bounds for singular values. Furthermore we derive component-wise bounds for attribution map values. As a main result, we apply these component-wise bounds to obtain multiplicative constants. These constants govern the convergence of empirical means of attributions to expectations of attribution maps. This finding has important implications for scenarios where multiple non-geometric data augmentations are applied to individual test samples, as well as for Smoothgrad-type attribution methods. In particular, our analysis reveals that the constants for LRP-beta remain independent of weight norms, a significant distinction from both gradient-based methods and LRP-epsilon.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling</title>
<link>https://arxiv.org/abs/2509.08980</link>
<guid>https://arxiv.org/abs/2509.08980</guid>
<content:encoded><![CDATA[
<div> carbon emissions, machine learning models, Federated Learning, training scheduling, emission savings

Summary:
- The paper explores reducing carbon emissions in large-scale machine learning models through Federated Learning (FL).
- Carbon-aware client selection and training scheduling are investigated to leverage regional and temporal variations in Carbon Intensity (CI).
- A carbon-aware scheduling policy is proposed that utilizes slack time to allow clients to defer local training rounds to lower-carbon periods, resulting in emission savings.
- The performance trade-offs of the scheduling policy are analyzed, taking into account statistical heterogeneity among clients, selection bias in participation, and temporal correlation in model updates.
- A carbon-aware scheduler is developed, integrating slack time, $\alpha$-fair carbon allocation, and a global fine-tuning phase, which outperforms slack-agnostic baselines in terms of model accuracy under various carbon budgets, with significant gains observed under strict carbon constraints.

<br /><br />Summary: <div>
arXiv:2509.08980v1 Announce Type: new 
Abstract: Training large-scale machine learning models incurs substantial carbon emissions. Federated Learning (FL), by distributing computation across geographically dispersed clients, offers a natural framework to leverage regional and temporal variations in Carbon Intensity (CI). This paper investigates how to reduce emissions in FL through carbon-aware client selection and training scheduling. We first quantify the emission savings of a carbon-aware scheduling policy that leverages slack time -- permitting a modest extension of the training duration so that clients can defer local training rounds to lower-carbon periods. We then examine the performance trade-offs of such scheduling which stem from statistical heterogeneity among clients, selection bias in participation, and temporal correlation in model updates. To leverage these trade-offs, we construct a carbon-aware scheduler that integrates slack time, $\alpha$-fair carbon allocation, and a global fine-tuning phase. Experiments on real-world CI data show that our scheduler outperforms slack-agnostic baselines, achieving higher model accuracy across a wide range of carbon budgets, with especially strong gains under tight carbon constraints.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers</title>
<link>https://arxiv.org/abs/2509.08988</link>
<guid>https://arxiv.org/abs/2509.08988</guid>
<content:encoded><![CDATA[
<div> Keywords: Spin coating, Polymer thin films, Optimization, Explainable AI, Pareto front learning <br />
<br />
Summary: 
The article introduces a framework that combines an active Pareto front learning algorithm, PyePAL, with visualization and explainable AI techniques to optimize the processing parameters for spin coating polymer thin films. The algorithm uses Gaussian processes to predict hardness and elasticity values based on design variables such as spin speed, dilution, and polymer mixture, guiding sample selection towards promising areas in the design space. By incorporating UMAP for visualization and fuzzy linguistic summaries for interpretation, the framework allows for a better understanding of the high-dimensional design space. Experimental results show that the method effectively identifies optimal polymer designs, with the visual and linguistic explanations aiding in expert analysis and knowledge discovery. <div>
arXiv:2509.08988v1 Announce Type: new 
Abstract: Spin coating polymer thin films to achieve specific mechanical properties is inherently a multi-objective optimization problem. We present a framework that integrates an active Pareto front learning algorithm (PyePAL) with visualization and explainable AI techniques to optimize processing parameters. PyePAL uses Gaussian process models to predict objective values (hardness and elasticity) from the design variables (spin speed, dilution, and polymer mixture), guiding the adaptive selection of samples toward promising regions of the design space. To enable interpretable insights into the high-dimensional design space, we utilize UMAP (Uniform Manifold Approximation and Projection) for two-dimensional visualization of the Pareto front exploration. Additionally, we incorporate fuzzy linguistic summaries, which translate the learned relationships between process parameters and performance objectives into linguistic statements, thus enhancing the explainability and understanding of the optimization results. Experimental results demonstrate that our method efficiently identifies promising polymer designs, while the visual and linguistic explanations facilitate expert-driven analysis and knowledge discovery.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast attention mechanisms: a tale of parallelism</title>
<link>https://arxiv.org/abs/2509.09001</link>
<guid>https://arxiv.org/abs/2509.09001</guid>
<content:encoded><![CDATA[
<div> Transformers, Approximate Nearest Neighbor Attention, MPC algorithms, scalability, expressive power <br />
<br />
Summary: Transformers with Approximate Nearest Neighbor Attention (ANNA) offer an efficient attention mechanism with sub-quadratic time complexity, making them highly scalable. They maintain the expressive power of standard attention, matching the capabilities of Massively Parallel Computation (MPC) algorithms. ANNA-transformers can effectively solve key reasoning tasks like Match2 and $k$-hop with near-optimal depth. Through the MPC framework, it is proven that constant-depth ANNA-transformers can simulate constant-depth low-rank transformers. This provides a unified approach to understanding a wide range of efficient attention approximations. <div>
arXiv:2509.09001v1 Announce Type: new 
Abstract: Transformers have the representational capacity to simulate Massively Parallel Computation (MPC) algorithms, but they suffer from quadratic time complexity, which severely limits their scalability. We introduce an efficient attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the expressive power previously established for standard attention in terms of matching the capabilities of MPC algorithms, and (2) can solve key reasoning tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC framework, we further prove that constant-depth ANNA-transformers can simulate constant-depth low-rank transformers, thereby providing a unified way to reason about a broad class of efficient attention approximations.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison</title>
<link>https://arxiv.org/abs/2509.09009</link>
<guid>https://arxiv.org/abs/2509.09009</guid>
<content:encoded><![CDATA[
<div> Transformer models, research baselines, multiple model scales, open reference datasets, standardized benchmarks <br />
<br />
Summary: 
The open-sci-ref project introduces a family of dense transformer models trained as research baselines on varying scales and datasets. These models, ranging from 0.13B to 1.7B parameters, were evaluated on multiple standardized benchmarks to set reference points for researchers to assess training approaches. Training on the NemoTron-CC HQ dataset consistently outperformed other datasets like DCLM-baseline and FineWeb-Edu. The release includes intermediate training checkpoints, logs, code, and downstream evaluations to facilitate reproducibility and comparison of training procedures. Researchers can now compare training dynamics, scaling trends, and model quality across different datasets using the established reference baselines. <div>
arXiv:2509.09009v1 Announce Type: new 
Abstract: We introduce open-sci-ref, a family of dense transformer models trained as research baselines across multiple model (0.13B to 1.7B parameters) and token scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on various standardized benchmarks, our training runs set establishes reference points that enable researchers to assess the sanity and quality of alternative training approaches across scales and datasets. Intermediate checkpoints allow comparison and studying of the training dynamics. The established reference baselines allow training procedures to be compared through their scaling trends, aligning them on a common compute axis. Comparison of open reference datasets reveals that training on NemoTron-CC HQ consistently outperforms other reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to intermediate training checkpoints, the release includes logs, code, and downstream evaluations to simplify reproduction, standardize comparison, and facilitate future research.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Context-Conditioned Anomaly Detection for Tabular Data</title>
<link>https://arxiv.org/abs/2509.09030</link>
<guid>https://arxiv.org/abs/2509.09030</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, tabular data, deep learning, context modeling, autoencoder

Summary:
- Anomaly detection is crucial in sectors like cybersecurity and finance, particularly with large-scale tabular data.
- Unsupervised anomaly detection, where labeled anomalies are unavailable, presents a significant challenge.
- Real-world tabular data often have diverse contexts, such as different users, where globally rare events can be normal under specific contexts.
- Existing deep learning methods struggle to handle these contextual nuances, leading to performance degradation in anomaly detection.
- The proposed context-conditional anomaly detection framework leverages context features and models conditional data distribution using a deep autoencoder. Extensive experiments on various tabular benchmark datasets demonstrate superior performance compared to state-of-the-art methods, highlighting the significance of considering context in accurately distinguishing anomalous instances from normal ones.

<br /><br />Summary: <div>
arXiv:2509.09030v1 Announce Type: new 
Abstract: Anomaly detection is critical in domains such as cybersecurity and finance, especially when working with large-scale tabular data. Yet, unsupervised anomaly detection -- where no labeled anomalies are available -- remains a significant challenge. Although various deep learning methods have been proposed to model a dataset's joint distribution, real-world tabular data often contain heterogeneous contexts (e.g., different users), making globally rare events normal under certain contexts. Consequently, relying on a single global distribution can overlook these contextual nuances, degrading detection performance. In this paper, we present a context-conditional anomaly detection framework tailored for tabular datasets. Our approach automatically identifies context features and models the conditional data distribution using a simple deep autoencoder. Extensive experiments on multiple tabular benchmark datasets demonstrate that our method outperforms state-of-the-art approaches, underscoring the importance of context in accurately distinguishing anomalous from normal instances.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoWE : A Mixture of Weather Experts</title>
<link>https://arxiv.org/abs/2509.09052</link>
<guid>https://arxiv.org/abs/2509.09052</guid>
<content:encoded><![CDATA[
<div> Mixture of Experts, Data-driven weather models, Vision Transformer, Forecasting, Root Mean Squared Error<br />
Summary:<br />
This paper introduces a Mixture of Experts (MoWE) approach for data-driven weather prediction. The model combines outputs from existing models using a Vision Transformer-based gating network that dynamically weights expert contributions based on forecast lead time. The synthesized deterministic forecast generated by MoWE outperforms individual expert models in terms of Root Mean Squared Error (RMSE), achieving up to a 10% lower RMSE than the best-performing AI weather model on a 2-day forecast horizon. The approach is computationally efficient, requiring significantly lower resources compared to training individual models. By optimally combining high-quality forecast models, the MoWE paradigm offers a scalable strategy to advance the state of the art in data-driven weather prediction.<br /> <div>
arXiv:2509.09052v1 Announce Type: new 
Abstract: Data-driven weather models have recently achieved state-of-the-art performance, yet progress has plateaued in recent years. This paper introduces a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these limitations, not by creating a new forecaster, but by optimally combining the outputs of existing models. The MoWE model is trained with significantly lower computational resources than the individual experts. Our model employs a Vision Transformer-based gating network that dynamically learns to weight the contributions of multiple "expert" models at each grid point, conditioned on forecast lead time. This approach creates a synthesized deterministic forecast that is more accurate than any individual component in terms of Root Mean Squared Error (RMSE). Our results demonstrate the effectiveness of this method, achieving up to a 10% lower RMSE than the best-performing AI weather model on a 2-day forecast horizon, significantly outperforming individual experts as well as a simple average across experts. This work presents a computationally efficient and scalable strategy to push the state of the art in data-driven weather prediction by making the most out of leading high-quality forecast models.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management</title>
<link>https://arxiv.org/abs/2509.09053</link>
<guid>https://arxiv.org/abs/2509.09053</guid>
<content:encoded><![CDATA[
<div> Keywords: renewable energy, distributed energy resources, machine learning, power system protection, dataset quality <br />
Summary: 
The scoping review discusses the integration of renewable and distributed energy resources in power systems and the challenges it poses to conventional protection schemes. It synthesizes recent literature on the application of machine learning (ML) in power system protection and disturbance management, highlighting the need for standardized reporting practices. While ML models show high accuracy on simulated datasets, their performance in real-world conditions needs further validation. The review identifies gaps in research such as the lack of real-world validation, limited robustness testing, and inadequate consideration of deployment feasibility. To address these challenges, the review introduces a ML-oriented taxonomy for protection tasks and advocates for comprehensive dataset documentation, methodological transparency, and consistent evaluation protocols. Future research should focus on public benchmark datasets, realistic validation methods, and advanced ML architectures to facilitate the practical deployment of ML-based protection in dynamic and decentralized power systems.<br /> 
Summary: <div>
arXiv:2509.09053v1 Announce Type: new 
Abstract: The integration of renewable and distributed energy resources reshapes modern power systems, challenging conventional protection schemes. This scoping review synthesizes recent literature on machine learning (ML) applications in power system protection and disturbance management, following the PRISMA for Scoping Reviews framework. Based on over 100 publications, three key objectives are addressed: (i) assessing the scope of ML research in protection tasks; (ii) evaluating ML performance across diverse operational scenarios; and (iii) identifying methods suitable for evolving grid conditions. ML models often demonstrate high accuracy on simulated datasets; however, their performance under real-world conditions remains insufficiently validated. The existing literature is fragmented, with inconsistencies in methodological rigor, dataset quality, and evaluation metrics. This lack of standardization hampers the comparability of results and limits the generalizability of findings. To address these challenges, this review introduces a ML-oriented taxonomy for protection tasks, resolves key terminological inconsistencies, and advocates for standardized reporting practices. It further provides guidelines for comprehensive dataset documentation, methodological transparency, and consistent evaluation protocols, aiming to improve reproducibility and enhance the practical relevance of research outcomes. Critical gaps remain, including the scarcity of real-world validation, insufficient robustness testing, and limited consideration of deployment feasibility. Future research should prioritize public benchmark datasets, realistic validation methods, and advanced ML architectures. These steps are essential to move ML-based protection from theoretical promise to practical deployment in increasingly dynamic and decentralized power systems.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition</title>
<link>https://arxiv.org/abs/2509.09070</link>
<guid>https://arxiv.org/abs/2509.09070</guid>
<content:encoded><![CDATA[
<div> Keywords: explainable AI, XAI frameworks, STRIDE, functional decomposition, Reproducing Kernel Hilbert Space<br />
<br />
Summary: STRIDE is a new explainable AI framework that addresses key limitations faced by existing XAI frameworks. It focuses on avoiding the exponential cost of reasoning over feature subsets and provides a more expressive way of summarizing effects. The framework frames explanation as a subset-enumeration-free, orthogonal functional decomposition in a Reproducing Kernel Hilbert Space. Instead of scalar attributions, STRIDE computes functional components, allowing for both local and global views. It offers speedups ranging from 0.6 to 9.7 times on public tabular benchmarks while maintaining high fidelity and substantial rank agreement. The approach is model-agnostic and supported by theoretical results on orthogonality and L^2 convergence. STRIDE also enables unique diagnostics like 'component surgery' to quantify the impact of specific interactions. Overall, STRIDE complements scalar attribution methods by providing a structured functional perspective in explainable AI.<br /><br />Summary: <div>
arXiv:2509.09070v1 Announce Type: new 
Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the exponential cost of reasoning over feature subsets and the reduced expressiveness of summarizing effects as single scalar values. We present STRIDE, a scalable framework that aims to mitigate both issues by framing explanation as a subset-enumeration-free, orthogonal functional decomposition in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on scalar attributions, STRIDE computes functional components f_S(x_S) via an analytical projection scheme based on a recursive kernel-centering procedure, avoiding explicit subset enumeration. In the tabular setups we study, the approach is model-agnostic, provides both local and global views, and is supported by theoretical results on orthogonality and L^2 convergence under stated assumptions. On public tabular benchmarks in our environment, we observed speedups ranging from 0.6 times (slower than TreeSHAP on a small dataset) to 9.7 times (California), with a median approximate 3.0 times across 10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and substantial rank agreement on most datasets. Overall, STRIDE complements scalar attribution methods by offering a structured functional perspective, enabling novel diagnostics like 'component surgery' to quantitatively measure the impact of specific interactions within our experimental scope.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations</title>
<link>https://arxiv.org/abs/2509.09073</link>
<guid>https://arxiv.org/abs/2509.09073</guid>
<content:encoded><![CDATA[
<div> Rashomon Ensemble, machine learning, model selection, generalization, diverse solutions <br />
<br />
Summary: The Rashomon Ensemble method addresses the challenge of selecting models that generalize well in machine learning. The Rashomon Effect, where multiple models perform similarly well for a given problem due to diverse data patterns, is common in real-world scenarios like manufacturing and medical diagnosis. The method strategically selects models from diverse high-performing solutions, grouping them based on performance and explanations to maximize diversity while maintaining accuracy. This approach ensures that each model covers a distinct region of the solution space, improving robustness to distribution shifts and unseen data variations. Validation on real-world datasets shows significant AUROC improvements, particularly in scenarios with a large Rashomon ratio. Businesses can benefit from the robustness, practicality, and effectiveness of the Rashomon Ensemble in various applications. <br /><br /> <div>
arXiv:2509.09073v1 Announce Type: new 
Abstract: Creating models from past observations and ensuring their effectiveness on new data is the essence of machine learning. However, selecting models that generalize well remains a challenging task. Related to this topic, the Rashomon Effect refers to cases where multiple models perform similarly well for a given learning problem. This often occurs in real-world scenarios, like the manufacturing process or medical diagnosis, where diverse patterns in data lead to multiple high-performing solutions. We propose the Rashomon Ensemble, a method that strategically selects models from these diverse high-performing solutions to improve generalization. By grouping models based on both their performance and explanations, we construct ensembles that maximize diversity while maintaining predictive accuracy. This selection ensures that each model covers a distinct region of the solution space, making the ensemble more robust to distribution shifts and variations in unseen data. We validate our approach on both open and proprietary collaborative real-world datasets, demonstrating up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large. Additionally, we demonstrate tangible benefits for businesses in various real-world applications, highlighting the robustness, practicality, and effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An entropy formula for the Deep Linear Network</title>
<link>https://arxiv.org/abs/2509.09088</link>
<guid>https://arxiv.org/abs/2509.09088</guid>
<content:encoded><![CDATA[
<div> DLN, Riemannian geometry, group actions, overparametrization, Boltzmann entropy<br />
Summary: The article investigates the Riemannian geometry of Deep Linear Networks (DLN) to develop a thermodynamic framework for understanding the learning process. Group actions are employed to analyze overparametrization in the DLN, while Riemannian submersion from parameter space to observable space is utilized. By foliating the balanced manifold in parameter space with group orbits, a Boltzmann entropy is defined and computed. The Riemannian geometry on the space of observables is shown to be derived from the balanced manifold through Riemannian submersion. A key technical aspect involves constructing an orthonormal basis for the tangent space of the balanced manifold using Jacobi matrix theory. This study provides insights into the geometric properties of DLN and contributes to a deeper understanding of its learning dynamics.<br /><br /> <div>
arXiv:2509.09088v1 Announce Type: new 
Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a foundation for a thermodynamic description of the learning process. The main tools are the use of group actions to analyze overparametrization and the use of Riemannian submersion from the space of parameters to the space of observables. The foliation of the balanced manifold in the parameter space by group orbits is used to define and compute a Boltzmann entropy. We also show that the Riemannian geometry on the space of observables defined in [2] is obtained by Riemannian submersion of the balanced manifold. The main technical step is an explicit construction of an orthonormal basis for the tangent space of the balanced manifold using the theory of Jacobi matrices.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models</title>
<link>https://arxiv.org/abs/2509.09119</link>
<guid>https://arxiv.org/abs/2509.09119</guid>
<content:encoded><![CDATA[
<div> LoRA, Parameter-Efficient Fine-Tuning, Low-Rank Adaptation, Sensitivity-LoRA, Large Language Models 

Summary:
Sensitivity-LoRA is a novel fine-tuning method for Large Language Models that dynamically allocates ranks to weight matrices based on their global and local sensitivities. By leveraging second-order derivatives, Sensitivity-LoRA efficiently captures weight sensitivity, enabling optimal rank allocation with minimal computational overhead. This approach addresses limitations of existing rank allocation techniques, providing robust effectiveness, efficiency, and stability across diverse tasks and benchmarks. Sensitivity-LoRA outperforms uniform rank allocation methods by adapting to the specific needs of each incremental matrix. This advancement in fine-tuning techniques offers practical applications in resource-constrained environments, enhancing the adaptability and performance of specialized tasks for Large Language Models.<br /><br />Summary: <div>
arXiv:2509.09119v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have transformed both everyday life and scientific research. However, adapting LLMs from general-purpose models to specialized tasks remains challenging, particularly in resource-constrained environments. Low-Rank Adaptation (LoRA), a prominent method within Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to LLMs by approximating model weight updates using low-rank decomposition. However, LoRA is limited by its uniform rank ( r ) allocation to each incremental matrix, and existing rank allocation techniques aimed at addressing this issue remain computationally inefficient, complex, and unstable, hindering practical applications. To address these limitations, we propose Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates ranks to weight matrices based on both their global and local sensitivities. It leverages the second-order derivatives (Hessian Matrix) of the loss function to effectively capture weight sensitivity, enabling optimal rank allocation with minimal computational overhead. Our experimental results have demonstrated robust effectiveness, efficiency and stability of Sensitivity-LoRA across diverse tasks and benchmarks.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction</title>
<link>https://arxiv.org/abs/2509.09128</link>
<guid>https://arxiv.org/abs/2509.09128</guid>
<content:encoded><![CDATA[
<div> Granger Causality, PCMCI+, deep learning, Arctic Sea Ice Extent, causal feature selection <br />
<br />
Summary: 
A new causality-aware deep learning framework is introduced to improve machine learning models by distinguishing genuine causal relationships from spurious associations. By incorporating Multivariate Granger Causality and PCMCI+ for causal feature selection within a hybrid neural architecture, the framework enhances interpretability, generalizability, and computational efficiency in predicting Arctic Sea Ice Extent dynamics. Through experiments utilizing 43 years of data, the method identifies causally influential predictors, prioritizes direct causes of SIE dynamics, and reduces unnecessary features. The inclusion of causal inputs leads to improved prediction accuracy and interpretability across various lead times. This framework, while demonstrated on Arctic SIE forecasting, is adaptable to other dynamic, high-dimensional domains, offering a scalable approach that enhances theoretical foundations and practical performance in causality-informed predictive modeling. <br /><br /> <div>
arXiv:2509.09128v1 Announce Type: new 
Abstract: Conventional machine learning and deep learning models typically rely on correlation-based learning, which often fails to distinguish genuine causal relationships from spurious associations, limiting their robustness, interpretability, and ability to generalize. To overcome these limitations, we introduce a causality-aware deep learning framework that integrates Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily and monthly resolutions, the proposed method identifies causally influential predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary features, and enhances computational efficiency. Experimental results show that incorporating causal inputs leads to improved prediction accuracy and interpretability across varying lead times. While demonstrated on Arctic SIE forecasting, the framework is broadly applicable to other dynamic, high-dimensional domains, offering a scalable approach that advances both the theoretical foundations and practical performance of causality-informed predictive modeling.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.09135</link>
<guid>https://arxiv.org/abs/2509.09135</guid>
<content:encoded><![CDATA[
<div> CTRL, Continuous-time RL, CT-MARL, Physics-informed neural networks, Value Gradient Iteration <br />
Summary: 
The paper introduces a Continuous-Time Multi-Agent Reinforcement Learning (CT-MARL) framework using Physics-Informed Neural Networks (PINNs) to approximate Hamilton-Jacobi-Bellman (HJB) based value functions in high-dimensional systems. By aligning value learning with value-gradient learning through a Value Gradient Iteration (VGI) module, the method refines value gradients along trajectories to improve gradient fidelity. This results in more accurate values and enhanced policy learning. Evaluation on continuous-time versions of standard benchmarks like Multi-Agent Particle Environment (MPE) and Multi-Agent MuJoCo demonstrates that the proposed approach consistently outperforms existing continuous-time RL methods and effectively scales to complex multi-agent dynamics. <div>
arXiv:2509.09135v1 Announce Type: new 
Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient fidelity, in turn yielding more accurate values and stronger policy learning. We evaluate our method using continuous-time variants of standard benchmarks, including multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous-time RL baselines and scales to complex multi-agent dynamics.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peering Partner Recommendation for ISPs using Machine Learning</title>
<link>https://arxiv.org/abs/2509.09146</link>
<guid>https://arxiv.org/abs/2509.09146</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Peering Relationships, Internet Service Providers, XGBoost, Efficiency

Summary: 
Internet service providers (ISPs) rely on connecting with other ISPs through transit services or direct peering relationships via Internet exchange points (IXPs) for global connectivity. Peering allows for ISP-specific optimizations but can be a complex process. This study explores using machine learning (ML) models to predict whether an ISP pair should peer based on publicly available data. By gathering data from databases like PeeringDB and CAIDA, the study evaluates tree-based, neural network-based, and transformer-based ML models for predicting peering relationships. The XGBoost model trained with public data achieved a 98% accuracy rate in predicting peering partners, showing resilience to variations in data. The findings suggest that ISPs can automate the peering partner selection process using ML, leading to a more efficient and optimized Internet ecosystem. 

Summary: <div>
arXiv:2509.09146v1 Announce Type: new 
Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide global connectivity services to their users. To ensure global connectivity, ISPs can either use transit service(s) or establish direct peering relationships between themselves via Internet exchange points (IXPs). Peering offers more room for ISP-specific optimizations and is preferred, but it often involves a lengthy and complex process. Automating peering partner selection can enhance efficiency in the global Internet ecosystem. We explore the use of publicly available data on ISPs to develop a machine learning (ML) model that can predict whether an ISP pair should peer or not. At first, we explore public databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we evaluate the performance of three broad types of ML models for predicting peering relationships: tree-based, neural network-based, and transformer-based. Among these, we observe that tree-based models achieve the highest accuracy and efficiency in our experiments. The XGBoost model trained with publicly available data showed promising performance, with a 98% accuracy rate in predicting peering partners. In addition, the model demonstrated great resilience to variations in time, space, and missing data. We envision that ISPs can adopt our method to fully automate the peering partner selection process, thus transitioning to a more efficient and optimized Internet ecosystem.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HISPASpoof: A New Dataset For Spanish Speech Forensics</title>
<link>https://arxiv.org/abs/2509.09155</link>
<guid>https://arxiv.org/abs/2509.09155</guid>
<content:encoded><![CDATA[
<div> Keywords: Zero-shot Voice Cloning, Text-to-Speech, Spanish, Synthetic speech detection, Speech forensics 

Summary: 
HISPASpoof is introduced as the first large-scale Spanish dataset aimed at detection and attribution of synthetic speech in order to address the underrepresentation of Spanish in speech forensics. The dataset includes real speech from public corpora with a variety of accents and synthetic speech generated using zero-shot TTS systems. Evaluation of five methods reveals that detectors trained on English are not effective for Spanish, but training on HISPASpoof greatly improves detection. Additionally, the dataset allows for the evaluation of synthetic speech attribution, identifying the origin of generated speech. HISPASpoof serves as a crucial benchmark for the advancement of reliable and inclusive speech forensics in Spanish.<br /><br />Summary: <div>
arXiv:2509.09155v1 Announce Type: new 
Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced rapidly, enabling the generation of highly realistic synthetic speech and raising serious concerns about their misuse. While numerous detectors have been developed for English and Chinese, Spanish-spoken by over 600 million people worldwide-remains underrepresented in speech forensics. To address this gap, we introduce HISPASpoof, the first large-scale Spanish dataset designed for synthetic speech detection and attribution. It includes real speech from public corpora across six accents and synthetic speech generated with six zero-shot TTS systems. We evaluate five representative methods, showing that detectors trained on English fail to generalize to Spanish, while training on HISPASpoof substantially improves detection. We also evaluate synthetic speech attribution performance on HISPASpoof, i.e., identifying the generation method of synthetic speech. HISPASpoof thus provides a critical benchmark for advancing reliable and inclusive speech forensics in Spanish.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication</title>
<link>https://arxiv.org/abs/2509.09168</link>
<guid>https://arxiv.org/abs/2509.09168</guid>
<content:encoded><![CDATA[
<div> transformer models, semantic communication, adaptive token merging, pretrained vision transformers, Bayesian optimization  
Summary:  
- Large-scale transformer models are powerful for semantic communication in edge devices but face computational barriers in 6G networks.
- A training-free framework for adaptive token merging in pretrained vision transformers is proposed to reduce inference time and resource usage.
- Multi-objective optimization is used to balance accuracy and computational cost by selecting per-layer merging proportions.
- Gaussian process-based Bayesian optimization is employed to create optimal configurations, allowing runtime adaptation to dynamic requirements and channel conditions.
- Experimental results show outperformance of the proposed method in reducing floating-point operations while maintaining competitive accuracy across varying signal-to-noise ratio conditions. Adaptive policies adjusting merging aggressiveness in response to channel quality are effective in trading off latency and semantic fidelity on demand. This approach offers a scalable and efficient deployment for transformer-based semantic communication in future edge intelligence systems.   
<br /><br /> <div>
arXiv:2509.09168v1 Announce Type: new 
Abstract: Large-scale transformer models have emerged as a powerful tool for semantic communication systems, enabling edge devices to extract rich representations for robust inference across noisy wireless channels. However, their substantial computational demands remain a major barrier to practical deployment in resource-constrained 6G networks. In this paper, we present a training-free framework for adaptive token merging in pretrained vision transformers to jointly reduce inference time and transmission resource usage. We formulate the selection of per-layer merging proportions as a multi-objective optimization problem to balance accuracy and computational cost. We employ Gaussian process-based Bayesian optimization to construct a Pareto frontier of optimal configurations, enabling flexible runtime adaptation to dynamic application requirements and channel conditions. Extensive experiments demonstrate that our method consistently outperforms other baselines and achieves significant reductions in floating-point operations while maintaining competitive accuracy across a wide range of signal-to-noise ratio (SNR) conditions. Additional results highlight the effectiveness of adaptive policies that adjust merging aggressiveness in response to channel quality, providing a practical mechanism to trade off latency and semantic fidelity on demand. These findings establish a scalable and efficient approach for deploying transformer-based semantic communication in future edge intelligence systems.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning</title>
<link>https://arxiv.org/abs/2509.09176</link>
<guid>https://arxiv.org/abs/2509.09176</guid>
<content:encoded><![CDATA[
<div> Quantum-inspired neural networks, deep reinforcement learning, financial trading, trading agent, USD/TWD<br />
<br />
Summary: Quantum-inspired neural networks and deep reinforcement learning are combined to create a trading agent for USD/TWD. The agent integrates Quantum Long Short-Term Memory (QLSTM) for short-term trend prediction with Quantum Asynchronous Advantage Actor-Critic (QA3C) for trading. Trained on data from 2000-01-01 to 2025-04-30, the agent achieves an 11.87% return over around 5 years with a 0.92% max drawdown, outperforming currency ETFs. The article details state design, reward function, and multi-core training. Results show hybrid models provide competitive FX trading performance. Implications include QLSTM's effectiveness for small-profit trades with tight risk and potential future enhancements. Key hyperparameters include QLSTM sequence length of 4 and 8 QA3C workers. Limitations include classical quantum simulation and a simplified strategy. This article represents the views of the authors and does not constitute investment advice from Wells Fargo. <div>
arXiv:2509.09176v1 Announce Type: new 
Abstract: The convergence of quantum-inspired neural networks and deep reinforcement learning offers a promising avenue for financial trading. We implemented a trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM) for short-term trend prediction with Quantum Asynchronous Advantage Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the long-only agent achieves 11.87\% return over around 5 years with 0.92\% max drawdown, outperforming several currency ETFs. We detail state design (QLSTM features and indicators), reward function for trend-following/risk control, and multi-core training. Results show hybrid models yield competitive FX trading performance. Implications include QLSTM's effectiveness for small-profit trades with tight risk and future enhancements. Key hyperparameters: QLSTM sequence length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and simplified strategy. \footnote{The views expressed in this article are those of the authors and do not represent the views of Wells Fargo. This article is for informational purposes only. Nothing contained in this article should be construed as investment advice. Wells Fargo makes no express or implied warranties and expressly disclaims all legal, tax, and accounting implications related to this article.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL</title>
<link>https://arxiv.org/abs/2509.09177</link>
<guid>https://arxiv.org/abs/2509.09177</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, LLMs, sequence-level, importance-sampling, Fair Sequence Policy Optimization <br />
Summary: <br />
The article introduces FSPO, a sequence-level reinforcement learning method for LLMs that enforces length-fair clipping directly in the importance-sampling space. It addresses the mismatch found in existing sequence-level RL methods when applying PPO/GRPO-style clipping to sequences, which can distort the effective objective. Theoretical analysis formalizes length fairness through the Length Reweighting Error (LRE) and proves the effectiveness of FSPO in maintaining a directional cosine guarantee between clipped and true updates. FSPO introduces a Gaussian-motivated remedy by clipping the sequence log-IS ratio with a band that includes a KL-corrected drift term and scales with the square root of the sequence length. Empirical results show that FSPO flattens clip rates across length bins, stabilizes training, and outperforms all baselines across multiple evaluation datasets. <div>
arXiv:2509.09177v1 Announce Type: new 
Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level reinforcement learning method for LLMs that enforces length-fair clipping directly in the importance-sampling (IS) weight space. We revisit sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping is transplanted to sequences: a fixed clip range systematically reweights short vs. long responses, distorting the effective objective. Theoretically, we formalize length fairness via a Length Reweighting Error (LRE) and prove that small LRE yields a directional cosine guarantee between the clipped and true updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the sequence log-IS ratio with a band that applies a KL-corrected drift term and scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins, stabilizes training, and outperforms all baselines across multiple evaluation datasets.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Statistical Similarity Trap in Extreme Convection Detection</title>
<link>https://arxiv.org/abs/2509.09195</link>
<guid>https://arxiv.org/abs/2509.09195</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, weather models, Statistical Similarity Trap, extreme convection detection, DART framework

Summary: 
1. Current evaluation metrics for deep learning weather models incentivize blurry predictions, leading to the "Statistical Similarity Trap" and missing rare, high-impact events.
2. The DART framework addresses the challenge of transforming coarse atmospheric forecasts into high-resolution satellite brightness temperature fields optimized for extreme convection detection.
3. Removing Integrated Water Vapor Transport paradoxically improves extreme convection detection by 270%, showcasing the limitations of certain commonly used metrics in atmospheric analysis.
4. The dual-decoder architecture of DART allows for operational flexibility and more accurate predictions compared to existing baselines.
5. Real-world validation of DART during the August 2023 Chittagong flooding disaster demonstrates its effectiveness in extreme weather preparedness.

<br /><br />Summary: <div>
arXiv:2509.09195v1 Announce Type: new 
Abstract: Current evaluation metrics for deep learning weather models create a "Statistical Similarity Trap", rewarding blurry predictions while missing rare, high-impact events. We provide quantitative evidence of this trap, showing sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous convection detection. We introduce DART (Dual Architecture for Regression Tasks), a framework addressing the challenge of transforming coarse atmospheric forecasts into high-resolution satellite brightness temperature fields optimized for extreme convection detection (below 220 K). DART employs dual-decoder architecture with explicit background/extreme decomposition, physically motivated oversampling, and task-specific loss functions. We present four key findings: (1) empirical validation of the Statistical Similarity Trap across multiple sophisticated baselines; (2) the "IVT Paradox", removing Integrated Water Vapor Transport, widely regarded as essential for atmospheric river analysis, improves extreme convection detection by 270%; (3) architectural necessity demonstrated through operational flexibility (DART achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent CSI), and (4) real-world validation with the August 2023 Chittagong flooding disaster as a case study. To our knowledge, this is the first work to systematically address this hybrid conversion-segmentation-downscaling task, with no direct prior benchmarks identified in existing literature. Our validation against diverse statistical and deep learning baselines sufficiently demonstrates DART's specialized design. The framework enables precise operational calibration through beta-tuning, trains in under 10 minutes on standard hardware, and integrates seamlessly with existing meteorological workflows, demonstrating a pathway toward trustworthy AI for extreme weather preparedness.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.09208</link>
<guid>https://arxiv.org/abs/2509.09208</guid>
<content:encoded><![CDATA[
<div> Constrained Reinforcement Learning, Adaptive Incentive Mechanism, Incrementally Penalized Proximal Policy Optimization, Benchmark Environment, Theoretical Guarantees
Summary:
Incrementally Penalized Proximal Policy Optimization (IP3O) is proposed to address the challenge of balancing reward maximization and constraint satisfaction in continuous control settings. The algorithm integrates an adaptive incentive mechanism to enforce a progressively increasing penalty before reaching constraint boundaries, improving stability during training. Empirical evaluation on benchmark environments shows the efficacy of IP3O compared to state-of-the-art Safe RL algorithms. Theoretical guarantees are provided through a bound on the worst-case error of optimality achieved by the algorithm. IP3O offers a practical solution for enhancing training performance and ensuring adherence to constraint limits in Constrained Reinforcement Learning tasks.
<br /><br />Summary: <div>
arXiv:2509.09208v1 Announce Type: new 
Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while adhering to predefined constraint limits, which represent domain-specific safety requirements. In continuous control settings, where learning agents govern system actions, balancing the trade-off between reward maximization and constraint satisfaction remains a significant challenge. Policy optimization methods often exhibit instability near constraint boundaries, resulting in suboptimal training performance. To address this issue, we introduce a novel approach that integrates an adaptive incentive mechanism in addition to the reward structure to stay within the constraint bound before approaching the constraint boundary. Building on this insight, we propose Incrementally Penalized Proximal Policy Optimization (IP3O), a practical algorithm that enforces a progressively increasing penalty to stabilize training dynamics. Through empirical evaluation on benchmark environments, we demonstrate the efficacy of IP3O compared to the performance of state-of-the-art Safe RL algorithms. Furthermore, we provide theoretical guarantees by deriving a bound on the worst-case error of the optimality achieved by our algorithm.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach</title>
<link>https://arxiv.org/abs/2509.09214</link>
<guid>https://arxiv.org/abs/2509.09214</guid>
<content:encoded><![CDATA[
<div> indicators, agro-tourism, growth, machine learning, LASSO <br />
<br />
Summary: <br />
Agro-tourism is an economic model that supports rural development by diversifying income sources and preserving cultural heritage. This study identifies key indicators for agro-tourism growth through literature review and machine learning techniques. The Least Absolute Shrinkage and Selection Operator (LASSO) method, along with classifiers like Logistic Regression and Random Forest, is used to predict growth. LR model shows 98% accuracy in 70-30% data split, and 99% in 80-20% split. RF and XGBoost also perform well. These findings can guide strategies for enhancing agro-tourism. <br /> <div>
arXiv:2509.09214v1 Announce Type: new 
Abstract: Agro-tourism serves as a strategic economic model designed to facilitate rural development by diversifying income streams for local communities like farmers while promoting the conservation of indigenous cultural heritage and traditional agricultural practices. As a very booming subdomain of tourism, there is a need to study the strategies for the growth of Agro-tourism in detail. The current study has identified the important indicators for the growth and enhancement of agro-tourism. The study is conducted in two phases: identification of the important indicators through a comprehensive literature review and in the second phase state-of-the-art techniques were used to identify the important indicators for the growth of agro-tourism. The indicators are also called features synonymously, the machine learning models for feature selection were applied and it was observed that the Least Absolute Shrinkage and Selection Operator (LASSO) method combined with, the machine Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT), Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were used to suggest the growth of the agro-tourism. The results show that with the LASSO method, LR model gives the highest classification accuracy of 98% in 70-30% train-test data followed by RF with 95% accuracy. Similarly, in the 80-20% train-test data LR maintains the highest accuracy at 99%, while DT and XGBoost follow with 97% accuracy.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement</title>
<link>https://arxiv.org/abs/2509.09219</link>
<guid>https://arxiv.org/abs/2509.09219</guid>
<content:encoded><![CDATA[
<div> Framework, data abstraction, graph neural networks, reinforcement learning, inductive policy functions

Summary:
The framework Vejde combines data abstraction, graph neural networks, and reinforcement learning to generate inductive policy functions for decision problems with structured states. MDP states are represented as databases of entity facts, converted into bipartite graphs, and mapped to latent states through neural message passing. This factored representation enables Vejde agents to handle problems of varying size and structure. Testing on eight problem domains in RDDL with ten instances each, Vejde agents trained with supervised and reinforcement learning demonstrated good policy generalization to unseen instances without a significant score loss. Moreover, Vejde agents scored close to instance-specific MLP agents on the unseen test instances. Overall, Vejde's inductive approach shows promise in effectively addressing decision problems with richly structured states. 

<br /><br />Summary: <div>
arXiv:2509.09219v1 Announce Type: new 
Abstract: We present and evaluate Vejde; a framework which combines data abstraction, graph neural networks and reinforcement learning to produce inductive policy functions for decision problems with richly structured states, such as object classes and relations. MDP states are represented as data bases of facts about entities, and Vejde converts each state to a bipartite graph, which is mapped to latent states through neural message passing. The factored representation of both states and actions allows Vejde agents to handle problems of varying size and structure. We tested Vejde agents on eight problem domains defined in RDDL, with ten problem instances each, where policies were trained using both supervised and reinforcement learning. To test policy generalization, we separate problem instances in two sets, one for training and the other solely for testing. Test results on unseen instances for the Vejde agents were compared to MLP agents trained on each problem instance, as well as the online planning algorithm Prost. Our results show that Vejde policies in average generalize to the test instances without a significant loss in score. Additionally, the inductive agents received scores on unseen test instances that on average were close to the instance-specific MLP agents.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructing a Question-Answering Simulator through the Distillation of LLMs</title>
<link>https://arxiv.org/abs/2509.09226</link>
<guid>https://arxiv.org/abs/2509.09226</guid>
<content:encoded><![CDATA[
<div> Keywords: question-answering simulator, educational recommender systems, LLM-free methods, LLM-based methods, LDSim<br />
<br />
Summary: 
The article introduces the concept of a question-answering (QA) simulator that mimics student learning behaviors and predicts the correctness of their responses. It discusses two categories of solutions for predicting correctness based on QA history: LLM-free methods and LLM-based methods. While LLM-free methods offer fast inference, they often result in suboptimal performance. On the other hand, LLM-based methods leverage domain knowledge and reasoning capabilities to enhance predictions at the cost of slower inference speed and higher GPU memory consumption. The paper presents a novel method called LLM Distillation based Simulator (LDSim), which distills domain knowledge and reasoning from an LLM to improve prediction performance. Experimental results show that LDSim outperforms existing methods on both simulation and knowledge tracing tasks. The code for LDSim is publicly available for further research and development. <br /><br />Summary: <div>
arXiv:2509.09226v1 Announce Type: new 
Abstract: The question-answering (QA) simulator is a model that mimics real student learning behaviors and predicts their correctness of their responses to questions. QA simulators enable educational recommender systems (ERS) to collect large amounts of training data without interacting with real students, thereby preventing harmful recommendations made by an undertrained ERS from undermining actual student learning. Given the QA history, there are two categories of solutions to predict the correctness, conducting the simulation: (1) LLM-free methods, which apply a traditional sequential model to transfer the QA history into a vector representation first, and make predictions based on the representation; (2) LLM-based methods, which leverage the domain knowledge and reasoning capability of LLM to enhence the prediction. LLM-free methods offer fast inference but generally yield suboptimal performance. In contrast, most LLM-based methods achieve better results, but at the cost of slower inference speed and higher GPU memory consumption. In this paper, we propose a method named LLM Distillation based Simulator (LDSim), which distills domain knowledge and reasoning capability from an LLM to better assist prediction, thereby improving simulation performance. Extensive experiments demonstrate that our LDSim achieves strong results on both the simulation task and the knowledge tracing (KT) task. Our code is publicly available at https://anonymous.4open.science/r/LDSim-05A9.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis</title>
<link>https://arxiv.org/abs/2509.09251</link>
<guid>https://arxiv.org/abs/2509.09251</guid>
<content:encoded><![CDATA[
<div> encoder, meta-learning, fault diagnosis, few-shot learning, rotating machinery<br />
Summary:<br />
The article introduces a Multi-Attention Meta Transformer method for few-shot unsupervised fault diagnosis in rotating machinery. Traditional fault diagnosis methods require a large amount of labeled data, which is often challenging to obtain in practical industrial settings. The proposed MMT-FD framework addresses this issue by extracting fault representations from unlabeled data and demonstrating strong generalization capabilities. The framework combines a time-frequency domain encoder with a meta-learning network to classify and generalize fault diagnosis across different types of mechanical equipment. By fine-tuning the model with a limited amount of labeled data and conducting contrastive learning iterations, the MMT-FD model achieves high fault diagnosis accuracy with only 1% of labeled sample data. Experimental results on bearing fault datasets and rotor test bench data showcase the robustness and efficiency of the proposed framework. <br /><br />Summary: <div>
arXiv:2509.09251v1 Announce Type: new 
Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually requires a large amount of labeled sample data. However, in practical industrial applications, acquiring enough data is both challenging and expensive in terms of time and cost. Moreover, different types of rotating mechanical equipment with different unique mechanical properties, require separate training of diagnostic models for each case. To address the challenges of limited fault samples and the lack of generalizability in prediction models for practical engineering applications, we propose a Multi-Attention Meta Transformer method for few-shot unsupervised rotating machinery fault diagnosis (MMT-FD). This framework extracts potential fault representations from unlabeled data and demonstrates strong generalization capabilities, making it suitable for diagnosing faults across various types of mechanical equipment. The MMT-FD framework integrates a time-frequency domain encoder and a meta-learning generalization model. The time-frequency domain encoder predicts status representations generated through random augmentations in the time-frequency domain. These enhanced data are then fed into a meta-learning network for classification and generalization training, followed by fine-tuning using a limited amount of labeled data. The model is iteratively optimized using a small number of contrastive learning iterations, resulting in high efficiency. To validate the framework, we conducted experiments on a bearing fault dataset and rotor test bench data. The results demonstrate that the MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled sample data, exhibiting robust generalization capabilities.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents</title>
<link>https://arxiv.org/abs/2509.09265</link>
<guid>https://arxiv.org/abs/2509.09265</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-horizon tasks, Large Language Models (LLMs), Entropy-Modulated Policy Gradients (EMPG), reinforcement learning, future clarity bonus

Summary:
In the field of long-horizon tasks, where sparse rewards make credit assignment challenging for agents based on Large Language Models (LLMs), a new framework called Entropy-Modulated Policy Gradients (EMPG) has been introduced. The framework addresses the inherent coupling between policy gradients and entropy in LLM learning dynamics, leading to inefficient updates. EMPG re-calibrates the learning signal based on uncertainty and task outcome, amplifying updates for confident correct actions, penalizing confident errors, and stabilizing exploration by attenuating updates from uncertain steps. Additionally, a bonus term for future clarity encourages agents to find more predictable solution paths. Through experiments on challenging tasks, including WebShop, ALFWorld, and Deep Search, EMPG demonstrates significant performance improvements over strong policy gradient baselines. The results highlight the effectiveness of EMPG in enhancing learning efficiency and achieving better task performance. 

<br /><br />Summary: <div>
arXiv:2509.09265v1 Announce Type: new 
Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations</title>
<link>https://arxiv.org/abs/2509.09278</link>
<guid>https://arxiv.org/abs/2509.09278</guid>
<content:encoded><![CDATA[
<div> Agent-based models, Cellular Automata, reaction-diffusion systems, data-driven discovery, emergent dynamics<br />
Summary:<br />
The study focuses on learning Soft Artificial Life models from observed data in reaction-diffusion systems without prior knowledge of the underlying physics. The Data-driven Rulesets for Soft Artificial Life (DRSALife) model is utilized to accurately represent emergent dynamics in various systems. The research successfully identifies the structure and parameters of the underlying partial differential equations (PDEs) representing the dynamics. Despite using noisy and sparse datasets, the learned models predict emergent dynamics with 74% accuracy and show robust performance against Gaussian noise and temporal sparsity. This approach provides a promising tool for system identification and learning emergent behaviors in complex systems, with implications for fields like neuroscience, ecology, and epidemiology.<br /> 
Summary: <div>
arXiv:2509.09278v1 Announce Type: new 
Abstract: Data-driven discovery of emergent dynamics is gaining popularity, particularly in the context of reaction-diffusion systems. These systems are widely studied across various fields, including neuroscience, ecology, epidemiology, and several other subject areas that deal with emergent dynamics. A current challenge in the discovery process relates to system identification when there is no prior knowledge of the underlying physics. We attempt to address this challenge by learning Soft Artificial Life (Soft ALife) models, such as Agent-based and Cellular Automata (CA) models, from observed data for reaction-diffusion systems. In this paper, we present findings on the applicability of a conceptual framework, the Data-driven Rulesets for Soft Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately represent emergent dynamics in a reaction-diffusion system from observed data. This model has demonstrated promising results for Elementary CA Rule 30, Game of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is one of the few studies that explore machine-based Soft ALife ruleset learning and system identification for reaction-diffusion dynamics without any prior knowledge of the underlying physics. Moreover, we provide comprehensive findings from experiments investigating the potential effects of using noisy and sparse observed datasets on learning emergent dynamics. Additionally, we successfully identify the structure and parameters of the underlying partial differential equations (PDEs) representing these dynamics. Experimental results demonstrate that the learned models are able to predict the emergent dynamics with good accuracy (74%) and exhibit quite robust performance when subjected to Gaussian noise and temporal sparsity.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts</title>
<link>https://arxiv.org/abs/2509.09337</link>
<guid>https://arxiv.org/abs/2509.09337</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Structural Expressiveness, Subgraph Patterns, Mixture of Subgraph Experts, Representation Learning

Summary:
The paper introduces the Mixture of Subgraph Experts (MoSE) framework to improve the structural expressiveness of Graph Neural Networks (GNNs) in capturing complex, high-order subgraph patterns. MoSE utilizes anonymous walks to extract informative subgraphs and dynamically routes them to specialized experts, allowing for flexibility and interpretability in capturing diverse subgraph structures. The theoretical analysis shows that MoSE is more powerful than the Subgraph Weisfeiler-Lehman Test. Experimental results demonstrate that MoSE outperforms existing methods and provides interpretable insights into learned structural patterns. The framework addresses limitations in current methods by enhancing flexibility in capturing subgraph patterns and improving model performance across various graph tasks. Visualizations of learned subgraph experts further illustrate the effectiveness and interpretability of the MoSE framework. 

<br /><br />Summary: <div>
arXiv:2509.09337v1 Announce Type: new 
Abstract: While graph neural networks (GNNs) have achieved great success in learning from graph-structured data, their reliance on local, pairwise message passing restricts their ability to capture complex, high-order subgraph patterns. leading to insufficient structural expressiveness. Recent efforts have attempted to enhance structural expressiveness by integrating random walk kernels into GNNs. However, these methods are inherently designed for graph-level tasks, which limits their applicability to other downstream tasks such as node classification. Moreover, their fixed kernel configurations hinder the model's flexibility in capturing diverse subgraph structures. To address these limitations, this paper proposes a novel Mixture of Subgraph Experts (MoSE) framework for flexible and expressive subgraph-based representation learning across diverse graph tasks. Specifically, MoSE extracts informative subgraphs via anonymous walks and dynamically routes them to specialized experts based on structural semantics, enabling the model to capture diverse subgraph patterns with improved flexibility and interpretability. We further provide a theoretical analysis of MoSE's expressivity within the Subgraph Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL. Extensive experiments, together with visualizations of learned subgraph experts, demonstrate that MoSE not only outperforms competitive baselines but also provides interpretable insights into structural patterns learned by the model.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Non-Linear Correlations via Polynomial Regression</title>
<link>https://arxiv.org/abs/2509.09380</link>
<guid>https://arxiv.org/abs/2509.09380</guid>
<content:encoded><![CDATA[
<div> Hirschfeld-Gebelein-Rényi, correlation coefficient, algorithmic fairness, machine learning, polynomial kernels
Summary: 
The article introduces a novel computational approach for estimating the Hirschfeld-Gebelein-Rényi (HGR) correlation coefficient, which extends beyond linear correlations. Previous methods for estimating HGR in a differentiable manner raised concerns about bias-variance trade-off and robustness in real-world applications. The new approach relies on user-configurable polynomial kernels, offering greater robustness and faster computation while maintaining effectiveness. It addresses technical concerns and provides a reliable option for constrained machine learning applications, where HGR can serve as a loss regularizer. Additionally, experimental analysis validates the applicability of the approach, showing that its computation yields an insightful subgradient for loss regularization.
<br /><br />Summary: <div>
arXiv:2509.09380v1 Announce Type: new 
Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension of Pearson's correlation that is not limited to linear correlations, with potential applications in algorithmic fairness, scientific analysis, and causal discovery. Recently, novel algorithms to estimate HGR in a differentiable manner have been proposed to facilitate its use as a loss regularizer in constrained machine learning applications. However, the inherent uncomputability of HGR requires a bias-variance trade-off, which can possibly compromise the robustness of the proposed methods, hence raising technical concerns if applied in real-world scenarios. We introduce a novel computational approach for HGR that relies on user-configurable polynomial kernels, offering greater robustness compared to previous methods and featuring a faster yet almost equally effective restriction. Our approach provides significant advantages in terms of robustness and determinism, making it a more reliable option for real-world applications. Moreover, we present a brief experimental analysis to validate the applicability of our approach within a constrained machine learning framework, showing that its computation yields an insightful subgradient that can serve as a loss regularizer.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization</title>
<link>https://arxiv.org/abs/2509.09387</link>
<guid>https://arxiv.org/abs/2509.09387</guid>
<content:encoded><![CDATA[
<div> MetaLLMiX; hyperparameter optimization; deep learning; LLM; medical imaging 

Summary:
MetaLLMiX is a zero-shot hyperparameter optimization framework that combines meta-learning, explainable AI, and efficient large language models (LLMs) reasoning. It leverages historical experiment outcomes with SHAP explanations to recommend optimal hyperparameters and pretrained models without additional trials. The framework also employs an LLM-as-judge evaluation to control output format, accuracy, and completeness. Experiments on eight medical imaging datasets using nine open-source lightweight LLMs demonstrate that MetaLLMiX achieves competitive or superior performance to traditional hyperparameter optimization (HPO) methods while significantly reducing computational cost. Local deployment of MetaLLMiX outperforms previous API-based approaches, achieving optimal results on the majority of tasks, with response time reductions of 99.6-99.9% and the fastest training times on six datasets (2.4-15.7x faster), while maintaining accuracy within 1-5% of best-performing baselines. 

<br /><br />Summary: <div>
arXiv:2509.09387v1 Announce Type: new 
Abstract: Effective model and hyperparameter selection remains a major challenge in deep learning, often requiring extensive expertise and computation. While AutoML and large language models (LLMs) promise automation, current LLM-based approaches rely on trial and error and expensive APIs, which provide limited interpretability and generalizability. We propose MetaLLMiX, a zero-shot hyperparameter optimization framework combining meta-learning, explainable AI, and efficient LLM reasoning. By leveraging historical experiment outcomes with SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained models without additional trials. We further employ an LLM-as-judge evaluation to control output format, accuracy, and completeness. Experiments on eight medical imaging datasets using nine open-source lightweight LLMs show that MetaLLMiX achieves competitive or superior performance to traditional HPO methods while drastically reducing computational cost. Our local deployment outperforms prior API-based approaches, achieving optimal results on 5 of 8 tasks, response time reductions of 99.6-99.9%, and the fastest training times on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of best-performing baselines.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2509.09396</link>
<guid>https://arxiv.org/abs/2509.09396</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, self-explanation, counterfactual explanations, validity, minimality

Summary:
Language models (LLMs) must be able to explain their decisions in natural language to collaborate effectively with humans. This study focuses on self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input to achieve a different outcome. Evaluation shows that while LLMs can produce valid SCEs, they often lack minimality, offering little insight into their decision-making process. When asked to generate minimal counterfactuals, LLMs make excessively small edits that do not alter predictions. This trade-off between validity and minimality persists across different LLMs, datasets, and evaluation scenarios. The findings suggest that using SCEs as explainability tools for LLMs may be ineffective or even misleading. Concerns arise about the reliability of self-explanations in high-stakes settings and their impact on downstream decision-making. <div>
arXiv:2509.09396v1 Announce Type: new 
Abstract: To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping</title>
<link>https://arxiv.org/abs/2509.09408</link>
<guid>https://arxiv.org/abs/2509.09408</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, geostatistics, soil properties, spatial mapping, precision agriculture<br />
Summary:<br />
Machine learning and geostatistics are traditionally distinct frameworks for predicting and mapping soil properties. A new hybrid approach, termed 'kriging prior regression' (KpR), integrates spatial context into machine learning by incorporating spatial lag features from ordinary kriging. Evaluation using the TabPFN model on six fieldscale datasets showed that KpR with TabPFN outperformed other spatial techniques and non-spatial machine learning algorithms. It provided improved uncertainty estimates and more accurate predictions, enhancing the average R2 by approximately 30%. TabPFN demonstrated strong performance, especially in tasks with limited data, while KpR compensated for weak feature-property relationships when proximal soil sensing data were scarce. The combination of KpR with TabPFN proved to be a robust and versatile framework for digital soil mapping in precision agriculture. Its effectiveness lies in leveraging the strengths of both geostatistics and machine learning, offering enhanced predictive capabilities for soil properties. <br /><br />Summary: <div>
arXiv:2509.09408v1 Announce Type: new 
Abstract: Machine learning and geostatistics are two fundamentally different frameworks for predicting and spatially mapping soil properties. Geostatistics leverages the spatial structure of soil properties, while machine learning captures the relationship between available environmental features and soil properties. We propose a hybrid framework that enriches ML with spatial context through engineering of 'spatial lag' features from ordinary kriging. We call this approach 'kriging prior regression' (KpR), as it follows the inverse logic of regression kriging. To evaluate this approach, we assessed both the point and probabilistic prediction performance of KpR, using the TabPFN model across six fieldscale datasets from LimeSoDa. These datasets included soil organic carbon, clay content, and pH, along with features derived from remote sensing and in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable uncertainty estimates and more accurate predictions in comparison to several other spatial techniques (e.g., regression/residual kriging with TabPFN), as well as to established non-spatial machine learning algorithms (e.g., random forest). Most notably, it significantly improved the average R2 by around 30% compared to machine learning algorithms without spatial context. This improvement was due to the strong prediction performance of the TabPFN algorithm itself and the complementary spatial information provided by KpR features. TabPFN is particularly effective for prediction tasks with small sample sizes, common in precision agriculture, whereas KpR can compensate for weak relationships between sensing features and soil properties when proximal soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a very robust and versatile modelling framework for digital soil mapping in precision agriculture.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples</title>
<link>https://arxiv.org/abs/2509.09413</link>
<guid>https://arxiv.org/abs/2509.09413</guid>
<content:encoded><![CDATA[
<div> Algorithm, Microbiome, Co-occurrence network, Spatial dynamics, Temporal dynamics

Summary:
The study introduces a new algorithm called fuser for analyzing microbial associations in microbiome communities by considering spatial and temporal dynamics. The Same-All Cross-validation (SAC) framework is proposed to evaluate algorithm performance in predicting microbial associations within the same environmental niche as well as across different niches. The fuser algorithm retains subsample-specific signals while sharing relevant information across environments during training, generating environment-specific predictive networks. Results show that fuser achieves comparable predictive performance to existing algorithms within homogeneous environments and reduces test error in cross-environment scenarios. This novel approach addresses the limitations of conventional algorithms by capturing dynamic microbial processes and adapting to varying ecological conditions.<br /><br />Summary: <div>
arXiv:2509.09413v1 Announce Type: new 
Abstract: Co-occurrence network inference algorithms have significantly advanced our understanding of microbiome communities. However, these algorithms typically analyze microbial associations within samples collected from a single environmental niche, often capturing only static snapshots rather than dynamic microbial processes. Previous studies have commonly grouped samples from different environmental niches together without fully considering how microbial communities adapt their associations when faced with varying ecological conditions. Our study addresses this limitation by explicitly investigating both spatial and temporal dynamics of microbial communities. We analyzed publicly available microbiome abundance data across multiple locations and time points, to evaluate algorithm performance in predicting microbial associations using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates algorithms in two distinct scenarios: training and testing within the same environmental niche (Same), and training and testing on combined data from multiple environmental niches (All). To overcome the limitations of conventional algorithms, we propose fuser, an algorithm that, while not entirely new in machine learning, is novel for microbiome community network inference. It retains subsample-specific signals while simultaneously sharing relevant information across environments during training. Unlike standard approaches that infer a single generalized network from combined data, fuser generates distinct, environment-specific predictive networks. Our results demonstrate that fuser achieves comparable predictive performance to existing algorithms such as glmnet when evaluated within homogeneous environments (Same), and notably reduces test error compared to baseline algorithms in cross-environment (All) scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation</title>
<link>https://arxiv.org/abs/2509.09451</link>
<guid>https://arxiv.org/abs/2509.09451</guid>
<content:encoded><![CDATA[
<div> molecular graph generation, material discovery, drug discovery, score-based techniques, controllability <br />
<br />
Summary: 
The article introduces a Composable Score-based Graph Diffusion model (CSGD) that enhances controllable molecular graph generation for material and drug discovery by using concrete scores. This model enables flexible manipulation of conditional guidance through Composable Guidance (CoG) and Probability Calibration (PC) techniques. Empirical results on molecular datasets demonstrate that CSGD outperforms existing methods, showing a 15.3% average improvement in controllability while maintaining validity and fidelity. The model's score-based approach allows for fine-grained control over conditions during sampling and adjusts transition probabilities to address train-test mismatches. Overall, CSGD showcases state-of-the-art performance in multi-property molecular design, showcasing the practical advantages of score-based modeling for discrete graph generation. <br /><br />Summary: <div>
arXiv:2509.09451v1 Announce Type: new 
Abstract: Controllable molecular graph generation is essential for material and drug discovery, where generated molecules must satisfy diverse property constraints. While recent advances in graph diffusion models have improved generation quality, their effectiveness in multi-conditional settings remains limited due to reliance on joint conditioning or continuous relaxations that compromise fidelity. To address these limitations, we propose Composable Score-based Graph Diffusion model (CSGD), the first model that extends score matching to discrete graphs via concrete scores, enabling flexible and principled manipulation of conditional guidance. Building on this foundation, we introduce two score-based techniques: Composable Guidance (CoG), which allows fine-grained control over arbitrary subsets of conditions during sampling, and Probability Calibration (PC), which adjusts estimated transition probabilities to mitigate train-test mismatches. Empirical results on four molecular datasets show that CSGD achieves state-of-the-art performance, with a 15.3% average improvement in controllability over prior methods, while maintaining high validity and distributional fidelity. Our findings highlight the practical advantages of score-based modeling for discrete graph generation and its capacity for flexible, multi-property molecular design.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer</title>
<link>https://arxiv.org/abs/2509.09458</link>
<guid>https://arxiv.org/abs/2509.09458</guid>
<content:encoded><![CDATA[
<div> deep learning, urban water dynamics, forecasting, endogenous variables, exogenous factors

Summary: 
- The work introduces a deep learning model, AquaCast, for forecasting urban water dynamics that incorporates both endogenous variables and exogenous factors.
- AquaCast captures inter-variable and temporal dependencies across all inputs, focusing on forecasting endogenous variables.
- Exogenous inputs are fused via an embedding layer, eliminating the need to forecast them and enhancing short-term influence handling.
- Evaluation on LausanneCity dataset shows state-of-the-art performance with endogenous variables and improved performance with the inclusion of exogenous variables and forecast reports.
- Generalization and scalability testing on three synthesized datasets confirm the model consistently outperforms existing baselines, providing accurate forecasts across real and synthetic datasets. 

<br /><br />Summary: <div>
arXiv:2509.09458v1 Announce Type: new 
Abstract: This work addresses the challenge of forecasting urban water dynamics by developing a multi-input, multi-output deep learning model that incorporates both endogenous variables (e.g., water height or discharge) and exogenous factors (e.g., precipitation history and forecast reports). Unlike conventional forecasting, the proposed model, AquaCast, captures both inter-variable and temporal dependencies across all inputs, while focusing forecast solely on endogenous variables. Exogenous inputs are fused via an embedding layer, eliminating the need to forecast them and enabling the model to attend to their short-term influences more effectively. We evaluate our approach on the LausanneCity dataset, which includes measurements from four urban drainage sensors, and demonstrate state-of-the-art performance when using only endogenous variables. Performance also improves with the inclusion of exogenous variables and forecast reports. To assess generalization and scalability, we additionally test the model on three large-scale synthesized datasets, generated from MeteoSwiss records, the Lorenz Attractors model, and the Random Fields model, each representing a different level of temporal complexity across 100 nodes. The results confirm that our model consistently outperforms existing baselines and maintains a robust and accurate forecast across both real and synthetic datasets.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings</title>
<link>https://arxiv.org/abs/2509.09470</link>
<guid>https://arxiv.org/abs/2509.09470</guid>
<content:encoded><![CDATA[
<div> Keywords: academia, literature, AI agent, geographic regions, conference proceedings<br />
<br />
Summary: 
The paper introduces a novel automated system, utilizing an AI agent named 'Agent-E', that streamlines the process of scholarly discovery by identifying papers from specific geographic regions within conference proceedings and executing predefined actions. The system was validated on 586 papers from five conferences, achieving a recall rate of 100% and an accuracy of 99.4%. This showcases the potential of task-oriented AI agents in not only filtering information but also actively participating in and accelerating academic workflows. Through its successful demonstration, the system presents a promising solution to the time-consuming manual effort required in academia and highlights the benefits of leveraging AI technologies for efficient research and collaboration in the academic community.<br /><br />Summary: <div>
arXiv:2509.09470v1 Announce Type: new 
Abstract: Keeping pace with the rapid growth of academia literature presents a significant challenge for researchers, funding bodies, and academic societies. To address the time-consuming manual effort required for scholarly discovery, we present a novel, fully automated system that transitions from data discovery to direct action. Our pipeline demonstrates how a specialized AI agent, 'Agent-E', can be tasked with identifying papers from specific geographic regions within conference proceedings and then executing a Robotic Process Automation (RPA) to complete a predefined action, such as submitting a nomination form. We validated our system on 586 papers from five different conferences, where it successfully identified every target paper with a recall of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the potential of task-oriented AI agents to not only filter information but also to actively participate in and accelerate the workflows of the academic community.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting</title>
<link>https://arxiv.org/abs/2509.09474</link>
<guid>https://arxiv.org/abs/2509.09474</guid>
<content:encoded><![CDATA[
<div> Temporal Knowledge Graph Forecasting, Explainable Method, Temporal Rules, Performance Evaluation, Interpretable Predictions
Summary: 
The article introduces a fully explainable method for temporal knowledge graph (TKG) forecasting, utilizing temporal rules to predict future events. The approach learns four types of rules with a confidence function that considers recency and frequency. Evaluated on nine datasets, the method outperforms eight state-of-the-art models and two baselines, matching or surpassing their performance. Additionally, the predictions made by the method are fully interpretable, providing insights into the reasoning behind the forecasts. By incorporating both recency and frequency in the confidence function, the approach achieves accurate predictions of TKGs. <div>
arXiv:2509.09474v1 Announce Type: new 
Abstract: We address the task of temporal knowledge graph (TKG) forecasting by introducing a fully explainable method based on temporal rules. Motivated by recent work proposing a strong baseline using recurrent facts, our approach learns four simple types of rules with a confidence function that considers both recency and frequency. Evaluated on nine datasets, our method matches or surpasses the performance of eight state-of-the-art models and two baselines, while providing fully interpretable predictions.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Utility and Privacy: Dynamically Private SGD with Random Projection</title>
<link>https://arxiv.org/abs/2509.09485</link>
<guid>https://arxiv.org/abs/2509.09485</guid>
<content:encoded><![CDATA[
<div> privacy leakage, stochastic optimization, differentially private SGD, dynamic differential privacy, random projection

Summary: 
The article introduces the Dynamically Differentially Private Projected SGD (D2P2-SGD) optimizer as a solution to privacy concerns in stochastic optimization for machine learning. By combining dynamic differential privacy (DDP) with automatic gradient clipping and random projection with SGD, D2P2-SGD allows for dynamic adjustment of the tradeoff between model utility and privacy. The optimizer shows provably sub-linear convergence rates across various objective functions, matching the best available rate. The theoretical analysis suggests that DDP improves utility at the expense of privacy, while random projection facilitates more efficient model learning. Extensive experiments on different datasets demonstrate that D2P2-SGD significantly enhances model accuracy while preserving privacy. The code for D2P2-SGD is also available for use. <br /><br />Summary: <div>
arXiv:2509.09485v1 Announce Type: new 
Abstract: Stochastic optimization is a pivotal enabler in modern machine learning, producing effective models for various tasks. However, several existing works have shown that model parameters and gradient information are susceptible to privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy concerns, its static noise mechanism impacts the error bounds for model performance. Additionally, with the exponential increase in model parameters, efficient learning of these models using stochastic optimizers has become more challenging. To address these concerns, we introduce the Dynamically Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we combine two important ideas: (i) dynamic differential privacy (DDP) with automatic gradient clipping and (ii) random projection with SGD, allowing dynamic adjustment of the tradeoff between utility and privacy of the model. It exhibits provably sub-linear convergence rates across different objective functions, matching the best available rate. The theoretical analysis further suggests that DDP leads to better utility at the cost of privacy, while random projection enables more efficient model learning. Extensive experiments across diverse datasets show that D2P2-SGD remarkably enhances accuracy while maintaining privacy. Our code is available here.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIPES: A Meta-dataset of Machine Learning Pipelines</title>
<link>https://arxiv.org/abs/2509.09512</link>
<guid>https://arxiv.org/abs/2509.09512</guid>
<content:encoded><![CDATA[
<div> meta-learning, algorithm selection problem, OpenML, pipelines, diversity

Summary:
The article introduces PIPES, a solution to the Algorithm Selection Problem in machine learning by leveraging meta-learning and utilizing experiments from OpenML. OpenML, while providing a vast collection of machine learning experiments, lacks diversity and completeness in pipeline exploration, particularly in data preprocessing steps. PIPES addresses this limitation by storing detailed results of experiments involving multiple pipelines applied to 300 datasets. It aims to represent all combinations of selected techniques, ensuring diversity and completeness in the analysis. The included information covers pipeline blocks, training/testing times, predictions, performances, and error messages. PIPES offers a comprehensive collection of results for researchers to conduct analyses across various pipelines and datasets, with the potential for further expansion to support the meta-learning community. Researchers can access the data, code, supplementary material, and experiments on the provided GitHub repository. 

<br /><br />Summary: <div>
arXiv:2509.09512v1 Announce Type: new 
Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face the challenge of high computational costs associated with evaluating various algorithms' performances on a given dataset. To mitigate this cost, the meta-learning field can leverage previously executed experiments shared in online repositories such as OpenML. OpenML provides an extensive collection of machine learning experiments. However, an analysis of OpenML's records reveals limitations. It lacks diversity in pipelines, specifically when exploring data preprocessing steps/blocks, such as scaling or imputation, resulting in limited representation. Its experiments are often focused on a few popular techniques within each pipeline block, leading to an imbalanced sample. To overcome the observed limitations of OpenML, we propose PIPES, a collection of experiments involving multiple pipelines designed to represent all combinations of the selected sets of techniques, aiming at diversity and completeness. PIPES stores the results of experiments performed applying 9,408 pipelines to 300 datasets. It includes detailed information on the pipeline blocks, training and testing times, predictions, performances, and the eventual error messages. This comprehensive collection of results allows researchers to perform analyses across diverse and representative pipelines and datasets. PIPES also offers potential for expansion, as additional data and experiments can be incorporated to support the meta-learning community further. The data, code, supplementary material, and all experiments can be found at https://github.com/cynthiamaia/PIPES.git.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cough Classification using Few-Shot Learning</title>
<link>https://arxiv.org/abs/2509.09515</link>
<guid>https://arxiv.org/abs/2509.09515</guid>
<content:encoded><![CDATA[
<div> few-shot learning, respiratory sound classification, COVID-19, Flu, healthy conditions

Summary: 
This paper explores the use of few-shot learning for classifying respiratory sounds, specifically focusing on detecting cough-based indicators of COVID-19, Flu, and healthy conditions. By utilizing Prototypical Networks with spectrogram representations of cough sounds, the study aims to achieve high accuracy with limited labeled data. The results show that few-shot learning models can achieve competitive accuracy, with a multi-class model reaching 74.87% accuracy using only 15 support examples per class. Binary classification models also perform well, with over 70% accuracy across all class pairs. Class-wise analysis reveals Flu as the most distinguishable class and Healthy as the most challenging. Statistical tests indicate no significant performance difference between binary and multi-class models, supporting the practicality of multi-class classification in this context. These findings demonstrate the potential of applying few-shot learning in medical diagnostics, particularly in scenarios where large labeled datasets are not readily available. 

<br /><br />Summary: <div>
arXiv:2509.09515v1 Announce Type: new 
Abstract: This paper investigates the effectiveness of few-shot learning for respiratory sound classification, focusing on coughbased detection of COVID-19, Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram representations of cough sounds to address the challenge of limited labeled data. Our study evaluates whether few-shot learning can enable models to achieve performance comparable to traditional deep learning approaches while using significantly fewer training samples. Additionally, we compare multi-class and binary classification models to assess whether multi-class models can perform comparably to their binary counterparts. Experimental findings show that few-shot learning models can achieve competitive accuracy. Our model attains 74.87% accuracy in multi-class classification with only 15 support examples per class, while binary classification achieves over 70% accuracy across all class pairs. Class-wise analysis reveals Flu as the most distinguishable class, and Healthy as the most challenging. Statistical tests (paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant performance difference between binary and multiclass models, supporting the viability of multi-class classification in this setting. These results highlight the feasibility of applying few-shot learning in medical diagnostics, particularly when large labeled datasets are unavailable.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning</title>
<link>https://arxiv.org/abs/2509.09534</link>
<guid>https://arxiv.org/abs/2509.09534</guid>
<content:encoded><![CDATA[
<div> Federated Learning, ProDiGy, Byzantine-robust, Adversarial attacks, Data heterogeneity 
Summary: 
ProDiGy is a new Byzantine-robust Federated Learning (FL) algorithm that addresses vulnerabilities to adversarial attacks in distributed learning, particularly under data heterogeneity. It introduces a dual scoring system based on client gradients' proximity and dissimilarity, outperforming existing defenses in various scenarios. When clients' data deviate from an identical and independent distribution (IID), ProDiGy maintains strong defense capabilities and model accuracy where other mechanisms fail. The algorithm's success lies in promoting natural similarity among honest clients while detecting suspicious uniformity as potential attack indicators. Its effectiveness in combating adversarial threats highlights the importance of considering both cooperative and divergent behavior among clients in FL settings. 
<br /><br />Summary: <div>
arXiv:2509.09534v1 Announce Type: new 
Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed learning. Despite its many advantages, FL remains vulnerable to adversarial attacks, especially under data heterogeneity. We propose a new Byzantine-robust FL algorithm called ProDiGy. The key novelty lies in evaluating the client gradients using a joint dual scoring system based on the gradients' proximity and dissimilarity. We demonstrate through extensive numerical experiments that ProDiGy outperforms existing defenses in various scenarios. In particular, when the clients' data do not follow an IID distribution, while other defense mechanisms fail, ProDiGy maintains strong defense capabilities and model accuracy. These findings highlight the effectiveness of a dual perspective approach that promotes natural similarity among honest clients while detecting suspicious uniformity as a potential indicator of an attack.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication</title>
<link>https://arxiv.org/abs/2509.09597</link>
<guid>https://arxiv.org/abs/2509.09597</guid>
<content:encoded><![CDATA[
<div> Graph alignment, unsupervised methods, node distinctiveness, latent spaces, dual-pass encoder <br />
Summary: 
The article presents a novel graph alignment framework that addresses the limitations of existing unsupervised methods. The proposed method enhances node distinctiveness by using a dual-pass encoder that combines low-pass and high-pass spectral filters. This allows for structure-aware and highly discriminative embeddings. To tackle latent space misalignment, a geometry-aware functional map module is incorporated to ensure consistent geometric relationships across different representations. Extensive experiments on graph benchmarks show that the method outperforms existing baselines, demonstrating robustness to structural inconsistencies and challenging alignment scenarios. Furthermore, the framework is evaluated on vision-language benchmarks, showcasing its ability to generalize beyond graph domains for unsupervised alignment of vision and language representations. Overall, the proposed approach provides a promising solution for improving graph alignment accuracy and generalization to diverse domains. <br /><br />Summary: <div>
arXiv:2509.09597v1 Announce Type: new 
Abstract: Graph alignment-the problem of identifying corresponding nodes across multiple graphs-is fundamental to numerous applications. Most existing unsupervised methods embed node features into latent representations to enable cross-graph comparison without ground-truth correspondences. However, these methods suffer from two critical limitations: the degradation of node distinctiveness due to oversmoothing in GNN-based embeddings, and the misalignment of latent spaces across graphs caused by structural noise, feature heterogeneity, and training instability, ultimately leading to unreliable node correspondences. We propose a novel graph alignment framework that simultaneously enhances node distinctiveness and enforces geometric consistency across latent spaces. Our approach introduces a dual-pass encoder that combines low-pass and high-pass spectral filters to generate embeddings that are both structure-aware and highly discriminative. To address latent space misalignment, we incorporate a geometry-aware functional map module that learns bijective and isometric transformations between graph embeddings, ensuring consistent geometric relationships across different representations. Extensive experiments on graph benchmarks demonstrate that our method consistently outperforms existing unsupervised alignment baselines, exhibiting superior robustness to structural inconsistencies and challenging alignment scenarios. Additionally, comprehensive evaluation on vision-language benchmarks using diverse pretrained models shows that our framework effectively generalizes beyond graph domains, enabling unsupervised alignment of vision and language representations.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics</title>
<link>https://arxiv.org/abs/2509.09599</link>
<guid>https://arxiv.org/abs/2509.09599</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, emulator, spatio-temporal systems, stochastic, chaotic

Summary:
Deep learning emulator is created for stochastic and chaotic spatio-temporal systems, conditioned on PDE parameter values. Pre-trained on a single parameter domain, it is fine-tuned on diverse datasets for generalization across various parameter values. The network incorporates local attention mechanisms for handling varying domain sizes and resolutions efficiently. Demonstrated on Kuramoto-Sivashinsky equation and beta-plane turbulence, the emulator captures phenomena at interpolated parameter values. It enables faster exploration of parameter space compared to numerical integration and provides uncertainty quantification for statistical study of rare events. The model showcases significant computational speed-ups, facilitating efficient parameter space exploration and rare event analysis.Probabilistic variant of the emulator can provide uncertainty quantification for studying rare events. <div>
arXiv:2509.09599v1 Announce Type: new 
Abstract: We present a deep learning emulator for stochastic and chaotic spatio-temporal systems, explicitly conditioned on the parameter values of the underlying partial differential equations (PDEs). Our approach involves pre-training the model on a single parameter domain, followed by fine-tuning on a smaller, yet diverse dataset, enabling generalisation across a broad range of parameter values. By incorporating local attention mechanisms, the network is capable of handling varying domain sizes and resolutions. This enables computationally efficient pre-training on smaller domains while requiring only a small additional dataset to learn how to generalise to larger domain sizes. We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky equation and stochastically-forced beta-plane turbulence, showcasing its ability to capture phenomena at interpolated parameter values. The emulator provides significant computational speed-ups over conventional numerical integration, facilitating efficient exploration of parameter space, while a probabilistic variant of the emulator provides uncertainty quantification, allowing for the statistical study of rare events.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance</title>
<link>https://arxiv.org/abs/2509.09611</link>
<guid>https://arxiv.org/abs/2509.09611</guid>
<content:encoded><![CDATA[
<div> Reduced Basis Neural Operator, PDEs, machine learning, physics-informed, discretization invariance <br />
<br />
Summary: 
The Reduced Basis Neural Operator (ReBaNO) is introduced as a data-lean algorithm for solving multiple distinct input partial differential equations (PDEs). It utilizes a greedy algorithm to build its network structure offline, incorporating task-specific activation functions through knowledge distillation. ReBaNO demonstrates superior performance compared to existing operator learning algorithms such as PCA-Net, DeepONet, FNO, and CNO, by minimizing the generalization gap and achieving strict discretization invariance for both in- and out-of-distribution tests. The compact architecture of ReBaNO requires minimal computational cost while embedding physics, making it a promising approach for efficient and accurate solution of PDEs. <br /> <div>
arXiv:2509.09611v1 Announce Type: new 
Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct inputs. Inspired by the Reduced Basis Method and the recently introduced Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a mathematically rigorous greedy algorithm to build its network structure offline adaptively from the ground up. Knowledge distillation via task-specific activation function allows ReBaNO to have a compact architecture requiring minimal computational cost online while embedding physics. In comparison to state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO, and CNO, numerical results demonstrate that ReBaNO significantly outperforms them in terms of eliminating/shrinking the generalization gap for both in- and out-of-distribution tests and being the only operator learning algorithm achieving strict discretization invariance.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Concept Drift through the Evolution of Group Counterfactuals</title>
<link>https://arxiv.org/abs/2509.09616</link>
<guid>https://arxiv.org/abs/2509.09616</guid>
<content:encoded><![CDATA[
<div> Keywords: concept drift, machine learning, counterfactual explanations, temporal evolution, interpretability

Summary:<br />
The paper introduces a methodology to explain concept drift in machine learning models operating in dynamic environments. It focuses on tracking the temporal evolution of group-based counterfactual explanations (GCEs) to understand changes in the model's decision-making logic. By analyzing shifts in cluster centroids and associated action vectors before and after a drift, the approach provides interpretable insights into structural changes in the decision boundary and underlying rationale. The analysis is operationalized within a three-layer framework that considers distributional shifts in the data layer, prediction disagreement in the model layer, and evolving GCEs in the explanation layer. This holistic view enables a more comprehensive diagnosis of concept drift, allowing for the differentiation of root causes such as spatial data shifts or re-labeling of concepts. <div>
arXiv:2509.09616v1 Announce Type: new 
Abstract: Machine learning models in dynamic environments often suffer from concept drift, where changes in the data distribution degrade performance. While detecting this drift is a well-studied topic, explaining how and why the model's decision-making logic changes still remains a significant challenge. In this paper, we introduce a novel methodology to explain concept drift by analyzing the temporal evolution of group-based counterfactual explanations (GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their associated counterfactual action vectors before and after a drift. These evolving GCEs act as an interpretable proxy, revealing structural changes in the model's decision boundary and its underlying rationale. We operationalize this analysis within a three-layer framework that synergistically combines insights from the data layer (distributional shifts), the model layer (prediction disagreement), and our proposed explanation layer. We show that such holistic view allows for a more comprehensive diagnosis of drift, making it possible to distinguish between different root causes, such as a spatial data shift versus a re-labeling of concepts.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction</title>
<link>https://arxiv.org/abs/2509.09619</link>
<guid>https://arxiv.org/abs/2509.09619</guid>
<content:encoded><![CDATA[
arXiv:2509.09619v1 Announce Type: new 
Abstract: Molecular property prediction using deep learning (DL) models has accelerated drug and materials discovery, but the resulting DL models often lack interpretability, hindering their adoption by chemists. This work proposes developing molecule representations using the concept of Functional Groups (FG) in chemistry. We introduce the Functional Group Representation (FGR) framework, a novel approach to encoding molecules based on their fundamental chemical substructures. Our method integrates two types of functional groups: those curated from established chemical knowledge (FG), and those mined from a large molecular corpus using sequential pattern mining (MFG). The resulting FGR framework encodes molecules into a lower-dimensional latent space by leveraging pre-training on a large dataset of unlabeled molecules. Furthermore, the proposed framework allows the inclusion of 2D structure-based descriptors of molecules. We demonstrate that the FGR framework achieves state-of-the-art performance on a diverse range of 33 benchmark datasets spanning physical chemistry, biophysics, quantum mechanics, biological activity, and pharmacokinetics while enabling chemical interpretability. Crucially, the model's representations are intrinsically aligned with established chemical principles, allowing chemists to directly link predicted properties to specific functional groups and facilitating novel insights into structure-property relationships. Our work presents a significant step toward developing high-performing, chemically interpretable DL models for molecular discovery.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management</title>
<link>https://arxiv.org/abs/2509.09655</link>
<guid>https://arxiv.org/abs/2509.09655</guid>
<content:encoded><![CDATA[
arXiv:2509.09655v1 Announce Type: new 
Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning (FG-FARL), an offline RL procedure that calibrates per-group safety thresholds to reduce harm while equalizing a chosen fairness target (coverage or harm) across protected subgroups. Using de-identified longitudinal trajectories from a Medicaid population health management program, we evaluate FG-FARL against behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global conformal safety baseline). We report off-policy value estimates with bootstrap 95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL achieves comparable value to baselines while improving fairness metrics, demonstrating a practical path to safer and more equitable decision support.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms</title>
<link>https://arxiv.org/abs/2509.09679</link>
<guid>https://arxiv.org/abs/2509.09679</guid>
<content:encoded><![CDATA[
arXiv:2509.09679v1 Announce Type: new 
Abstract: Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} = (\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. We propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$ entries that are non-differentiable and prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \log n)$ computational complexity with only $\frac{n \log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals</title>
<link>https://arxiv.org/abs/2509.08830</link>
<guid>https://arxiv.org/abs/2509.08830</guid>
<content:encoded><![CDATA[
arXiv:2509.08830v1 Announce Type: cross 
Abstract: In clinical settings, monitoring hemodynamics is crucial for managing patient prognosis, necessitating the integrated analysis of multiple physiological signals. While recent research has analyzed single signals such as electrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a proposal for an approach that encompasses the complex signal analysis required in actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul National University hospital PHYsiological signal Masked representation learning) model extracts physiological features reflecting the electrical, pressure, and fluid characteristics of the cardiac cycle in the process of restoring three masked physiological signals based on self-supervised learning (SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing multiple physical characteristics, the model can extract more enriched features only using non-invasive signals. We evaluated the model's performance in clinical downstream tasks such as hypotension, stroke volume, systolic blood pressure, diastolic blood pressure, and age prediction. Our results showed that the SNUPHY-M significantly outperformed supervised or SSL models, especially in prediction tasks using non-invasive signals. To the best of our knowledge, SNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis involving ECG, PPG, and ABP signals. This approach effectively supports clinical decision-making and enables precise diagnostics, contributing significantly to the early diagnosis and management of hemodynamics without invasiveness.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs</title>
<link>https://arxiv.org/abs/2509.08847</link>
<guid>https://arxiv.org/abs/2509.08847</guid>
<content:encoded><![CDATA[
arXiv:2509.08847v1 Announce Type: cross 
Abstract: This paper presents a novel framework for automated game template generation by transforming Game Design Documents (GDDs) into functional Unity game prototypes using Natural Language Processing (NLP) and multi-modal Large Language Models (LLMs). We introduce an end-to-end system that parses GDDs, extracts structured game specifications, and synthesizes Unity-compatible C# code that implements the core mechanics, systems, and architecture defined in the design documentation. Our approach combines a fine-tuned LLaMA-3 model specialized for Unity code generation with a custom Unity integration package that streamlines the implementation process. Evaluation results demonstrate significant improvements over baseline models, with our fine-tuned model achieving superior performance (4.8/5.0 average score) compared to state-of-the-art LLMs across compilation success, GDD adherence, best practices adoption, and code modularity metrics. The generated templates demonstrate high adherence to GDD specifications across multiple game genres. Our system effectively addresses critical gaps in AI-assisted game development, positioning LLMs as valuable tools in streamlining the transition from game design to implementation.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe and Certifiable AI Systems: Concepts, Challenges, and Lessons Learned</title>
<link>https://arxiv.org/abs/2509.08852</link>
<guid>https://arxiv.org/abs/2509.08852</guid>
<content:encoded><![CDATA[
arXiv:2509.08852v1 Announce Type: cross 
Abstract: There is an increasing adoption of artificial intelligence in safety-critical applications, yet practical schemes for certifying that AI systems are safe, lawful and socially acceptable remain scarce. This white paper presents the T\"UV AUSTRIA Trusted AI framework an end-to-end audit catalog and methodology for assessing and certifying machine learning systems. The audit catalog has been in continuous development since 2019 in an ongoing collaboration with scientific partners. Building on three pillars - Secure Software Development, Functional Requirements, and Ethics & Data Privacy - the catalog translates the high-level obligations of the EU AI Act into specific, testable criteria. Its core concept of functional trustworthiness couples a statistically defined application domain with risk-based minimum performance requirements and statistical testing on independently sampled data, providing transparent and reproducible evidence of model quality in real-world settings. We provide an overview of the functional requirements that we assess, which are oriented on the lifecycle of an AI system. In addition, we share some lessons learned from the practical application of the audit catalog, highlighting common pitfalls we encountered, such as data leakage scenarios, inadequate domain definitions, neglect of biases, or a lack of distribution drift controls. We further discuss key aspects of certifying AI systems, such as robustness, algorithmic fairness, or post-certification requirements, outlining both our current conclusions and a roadmap for future research. In general, by aligning technical best practices with emerging European standards, the approach offers regulators, providers, and users a practical roadmap for legally compliant, functionally trustworthy, and certifiable AI systems.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralising LLM Alignment: A Case for Context, Pluralism, and Participation</title>
<link>https://arxiv.org/abs/2509.08858</link>
<guid>https://arxiv.org/abs/2509.08858</guid>
<content:encoded><![CDATA[
arXiv:2509.08858v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) alignment methods have been credited with the commercial success of products like ChatGPT, given their role in steering LLMs towards user-friendly outputs. However, current alignment techniques predominantly mirror the normative preferences of a narrow reference group, effectively imposing their values on a wide user base. Drawing on theories of the power/knowledge nexus, this work argues that current alignment practices centralise control over knowledge production and governance within already influential institutions. To counter this, we propose decentralising alignment through three characteristics: context, pluralism, and participation. Furthermore, this paper demonstrates the critical importance of delineating the context-of-use when shaping alignment practices by grounding each of these features in concrete use cases. This work makes the following contributions: (1) highlighting the role of context, pluralism, and participation in decentralising alignment; (2) providing concrete examples to illustrate these strategies; and (3) demonstrating the nuanced requirements associated with applying alignment across different contexts of use. Ultimately, this paper positions LLM alignment as a potential site of resistance against epistemic injustice and the erosion of democratic processes, while acknowledging that these strategies alone cannot substitute for broader societal changes.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WarpPINN-fibers: improved cardiac strain estimation from cine-MR with physics-informed neural networks</title>
<link>https://arxiv.org/abs/2509.08872</link>
<guid>https://arxiv.org/abs/2509.08872</guid>
<content:encoded><![CDATA[
arXiv:2509.08872v1 Announce Type: cross 
Abstract: The contractile motion of the heart is strongly determined by the distribution of the fibers that constitute cardiac tissue. Strain analysis informed with the orientation of fibers allows to describe several pathologies that are typically associated with impaired mechanics of the myocardium, such as cardiovascular disease. Several methods have been developed to estimate strain-derived metrics from traditional imaging techniques. However, the physical models underlying these methods do not include fiber mechanics, restricting their capacity to accurately explain cardiac function. In this work, we introduce WarpPINN-fibers, a physics-informed neural network framework to accurately obtain cardiac motion and strains enhanced by fiber information. We train our neural network to satisfy a hyper-elastic model and promote fiber contraction with the goal to predict the deformation field of the heart from cine magnetic resonance images. For this purpose, we build a loss function composed of three terms: a data-similarity loss between the reference and the warped template images, a regularizer enforcing near-incompressibility of cardiac tissue and a fiber-stretch penalization that controls strain in the direction of synthetically produced fibers. We show that our neural network improves the former WarpPINN model and effectively controls fiber stretch in a synthetic phantom experiment. Then, we demonstrate that WarpPINN-fibers outperforms alternative methodologies in landmark-tracking and strain curve prediction for a cine-MRI benchmark with a cohort of 15 healthy volunteers. We expect that our method will enable a more precise quantification of cardiac strains through accurate deformation fields that are consistent with fiber physiology, without requiring imaging techniques more sophisticated than MRI.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures</title>
<link>https://arxiv.org/abs/2509.08926</link>
<guid>https://arxiv.org/abs/2509.08926</guid>
<content:encoded><![CDATA[
arXiv:2509.08926v1 Announce Type: cross 
Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise, which typically leads to significant performance degradation. We address this challenge by reframing Re-ID as a supervised image similarity task and adopting a Siamese network architecture trained to capture discriminative pairwise relationships. Central to our approach is a novel statistical outlier detection (OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier Detection), which models the distribution of cosine similarities between embedding pairs using a two-component Beta distribution mixture model. We establish a novel identifiability result for mixtures of two Beta distributions, ensuring that our learning task is well-posed.The proposed OD step complements the Re-ID architecture combining binary cross-entropy, contrastive, and cosine embedding losses that jointly optimize feature-level similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance compared to the state-of-the-art methods across various noise levels (10-30\%), demonstrating both robustness and broad applicability in noisy Re-ID scenarios. The implementation of Beta-SOD is available at: https://github.com/waqar3411/Beta-SOD
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities</title>
<link>https://arxiv.org/abs/2509.08950</link>
<guid>https://arxiv.org/abs/2509.08950</guid>
<content:encoded><![CDATA[
arXiv:2509.08950v1 Announce Type: cross 
Abstract: Powerful artificial intelligence (AI) tools that have emerged in recent years -- including large language models, automated coding assistants, and advanced image and speech generation technologies -- are the result of monumental human achievements. These breakthroughs reflect mastery across multiple technical disciplines and the resolution of significant technological challenges. However, some of the most profound challenges may still lie ahead. These challenges are not purely technical but pertain to the fair and responsible use of AI in ways that genuinely improve the global human condition. This article explores one promising application aligned with that vision: the use of AI tools to facilitate and enhance education, with a specific focus on signal processing (SP). It presents two interrelated perspectives: identifying and addressing technical limitations, and applying AI tools in practice to improve educational experiences. Primers are provided on several core technical issues that arise when using AI in educational settings, including how to ensure fairness and inclusivity, handle hallucinated outputs, and achieve efficient use of resources. These and other considerations -- such as transparency, explainability, and trustworthiness -- are illustrated through the development of an immersive, structured, and reliable "smart textbook." The article serves as a resource for researchers and educators seeking to advance AI's role in engineering education.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convexity of Optimization Curves: Local Sharp Thresholds, Robustness Impossibility, and New Counterexamples</title>
<link>https://arxiv.org/abs/2509.08954</link>
<guid>https://arxiv.org/abs/2509.08954</guid>
<content:encoded><![CDATA[
arXiv:2509.08954v1 Announce Type: cross 
Abstract: We study when the \emph{optimization curve} of first-order methods -- the sequence \${f(x\_n)}*{n\ge0}\$ produced by constant-stepsize iterations -- is convex, equivalently when the forward differences \$f(x\_n)-f(x*{n+1})\$ are nonincreasing. For gradient descent (GD) on convex \$L\$-smooth functions, the curve is convex for all stepsizes \$\eta \le 1.75/L\$, and this threshold is tight. Moreover, gradient norms are nonincreasing for all \$\eta \le 2/L\$, and in continuous time (gradient flow) the curve is always convex. These results complement and refine the classical smooth convex optimization toolbox, connecting discrete and continuous dynamics as well as worst-case analyses.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed waveform inversion using pretrained wavefield neural operators</title>
<link>https://arxiv.org/abs/2509.08967</link>
<guid>https://arxiv.org/abs/2509.08967</guid>
<content:encoded><![CDATA[
arXiv:2509.08967v1 Announce Type: cross 
Abstract: Full waveform inversion (FWI) is crucial for reconstructing high-resolution subsurface models, but it is often hindered, considering the limited data, by its null space resulting in low-resolution models, and more importantly, by its computational cost, especially if needed for real-time applications. Recent attempts to accelerate FWI using learned wavefield neural operators have shown promise in efficiency and differentiability, but typically suffer from noisy and unstable inversion performance. To address these limitations, we introduce a novel physics-informed FWI framework to enhance the inversion in accuracy while maintaining the efficiency of neural operator-based FWI. Instead of relying only on the L2 norm objective function via automatic differentiation, resulting in noisy model reconstruction, we integrate a physics constraint term in the loss function of FWI, improving the quality of the inverted velocity models. Specifically, starting with an initial model to simulate wavefields and then evaluating the loss over how much the resulting wavefield obeys the physical laws (wave equation) and matches the recorded data, we achieve a reduction in noise and artifacts. Numerical experiments using the OpenFWI and Overthrust models demonstrate our method's superior performance, offering cleaner and more accurate subsurface velocity than vanilla approaches. Considering the efficiency of the approach compared to FWI, this advancement represents a significant step forward in the practical application of FWI for real-time subsurface monitoring.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models</title>
<link>https://arxiv.org/abs/2509.08972</link>
<guid>https://arxiv.org/abs/2509.08972</guid>
<content:encoded><![CDATA[
arXiv:2509.08972v1 Announce Type: cross 
Abstract: The increasing reliance on generative AI models has accelerated the generation rate of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030. This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data leads to a phenomenon known as model collapse, where model performance degrades over generations of training, eventually rendering the models ineffective. Although prior studies have explored the causes and detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data as a key driver of collapse. Building on this observation, we propose a confidence-aware loss function that downweights high-confidence predictions during training. We introduce a novel loss function we call Truncated Cross Entropy (TCE). We demonstrate that TCE significantly delays model collapse in recursive training.
  We provide a model-agnostic framework that links the loss function design to model collapse mitigation and validate our approach both theoretically and empirically, showing that it can extend the model's fidelity interval before collapse by more than 2.3x. Finally, we show that our method generalizes across modalities. These findings suggest that the design of loss functions provides a simple yet powerful tool for preserving the quality of generative models in the era of increasing synthetic data.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data</title>
<link>https://arxiv.org/abs/2509.09018</link>
<guid>https://arxiv.org/abs/2509.09018</guid>
<content:encoded><![CDATA[
arXiv:2509.09018v1 Announce Type: cross 
Abstract: A sleep forecast allows individuals and healthcare providers to anticipate and proactively address factors influencing restful rest, ultimately improving mental and physical well-being. This work presents an adaptive spatial and temporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model combines convolutional layers to capture spatial feature interactions between multiple features and recurrent neural network layers to handle longer-term temporal health-related data. A domain classifier is further integrated to generalize across different subjects. We conducted several experiments using five input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes (1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline models, achieving its lowest RMSE (0.282) with a seven-day input window and a one-day predicting window. Moreover, the method maintained strong performance even when forecasting multiple days into the future, demonstrating its versatility for real-world applications. Visual comparisons reveal that the model accurately tracks both the overall sleep score level and daily fluctuations. These findings prove that the proposed framework provides a robust and adaptable solution for personalized sleep forecasting using sparse data from commercial wearable devices and domain adaptation techniques.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative quantum advantage for classical and quantum problems</title>
<link>https://arxiv.org/abs/2509.09033</link>
<guid>https://arxiv.org/abs/2509.09033</guid>
<content:encoded><![CDATA[
arXiv:2509.09033v1 Announce Type: cross 
Abstract: Recent breakthroughs in generative machine learning, powered by massive computational resources, have demonstrated unprecedented human-like capabilities. While beyond-classical quantum experiments can generate samples from classically intractable distributions, their complexity has thwarted all efforts toward efficient learning. This challenge has hindered demonstrations of generative quantum advantage: the ability of quantum computers to learn and generate desired outputs substantially better than classical computers. We resolve this challenge by introducing families of generative quantum models that are hard to simulate classically, are efficiently trainable, exhibit no barren plateaus or proliferating local minima, and can learn to generate distributions beyond the reach of classical computers. Using a $68$-qubit superconducting quantum processor, we demonstrate these capabilities in two scenarios: learning classically intractable probability distributions and learning quantum circuits for accelerated physical simulation. Our results establish that both learning and sampling can be performed efficiently in the beyond-classical regime, opening new possibilities for quantum-enhanced generative models with provable advantage.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Community Detection Methods in Performance Variations of Graph Mining Tasks</title>
<link>https://arxiv.org/abs/2509.09045</link>
<guid>https://arxiv.org/abs/2509.09045</guid>
<content:encoded><![CDATA[
arXiv:2509.09045v1 Announce Type: cross 
Abstract: In real-world scenarios, large graphs represent relationships among entities in complex systems. Mining these large graphs often containing millions of nodes and edges helps uncover structural patterns and meaningful insights. Dividing a large graph into smaller subgraphs facilitates complex system analysis by revealing local information. Community detection extracts clusters or communities of graphs based on statistical methods and machine learning models using various optimization techniques. Structure based community detection methods are more suitable for applying to graphs because they do not rely heavily on rich node or edge attribute information. The features derived from these communities can improve downstream graph mining tasks, such as link prediction and node classification. In real-world applications, we often lack ground truth community information. Additionally, there is neither a universally accepted gold standard for community detection nor a single method that is consistently optimal across diverse applications. In many cases, it is unclear how practitioners select community detection methods, and choices are often made without explicitly considering their potential impact on downstream tasks. In this study, we investigate whether the choice of community detection algorithm significantly influences the performance of downstream applications. We propose a framework capable of integrating various community detection methods to systematically evaluate their effects on downstream task outcomes. Our comparative analysis reveals that specific community detection algorithms yield superior results in certain applications, highlighting that method selection substantially affects performance.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M</title>
<link>https://arxiv.org/abs/2509.09055</link>
<guid>https://arxiv.org/abs/2509.09055</guid>
<content:encoded><![CDATA[
arXiv:2509.09055v1 Announce Type: cross 
Abstract: This research investigates the effectiveness of alignment techniques, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a combined SFT+DPO approach on improving the safety and helpfulness of the OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset, we train and evaluate four models: the base OPT350M, an SFT model, a DPO model, and a model trained with both SFT and DPO. We introduce three key evaluation metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined Alignment Score (CAS), all derived from reward model outputs. The results show that while SFT outperforms DPO, The combined SFT+DPO model outperforms all others across all metrics, demonstrating the complementary nature of these techniques. Our findings also highlight challenges posed by noisy data, limited GPU resources, and training constraints. This study offers a comprehensive view of how fine-tuning strategies affect model alignment and provides a foundation for more robust alignment pipelines in future work.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning</title>
<link>https://arxiv.org/abs/2509.09074</link>
<guid>https://arxiv.org/abs/2509.09074</guid>
<content:encoded><![CDATA[
arXiv:2509.09074v1 Announce Type: cross 
Abstract: In this work, we propose a novel flow field-based motion planning method that drives a robot from any initial state to a desired reference trajectory such that it converges to the trajectory's end point. Despite demonstrated efficacy in using Koopman operator theory for modeling dynamical systems, Koopman does not inherently enforce convergence to desired trajectories nor to specified goals -- a requirement when learning from demonstrations (LfD). We present KoopMotion which represents motion flow fields as dynamical systems, parameterized by Koopman Operators to mimic desired trajectories, and leverages the divergence properties of the learnt flow fields to obtain smooth motion fields that converge to a desired reference trajectory when a robot is placed away from the desired trajectory, and tracks the trajectory until the end point. To demonstrate the effectiveness of our approach, we show evaluations of KoopMotion on the LASA human handwriting dataset and a 3D manipulator end-effector trajectory dataset, including spectral analysis. We also perform experiments on a physical robot, verifying KoopMotion on a miniature autonomous surface vehicle operating in a non-static fluid flow environment. Our approach is highly sample efficient in both space and time, requiring only 3\% of the LASA dataset to generate dense motion plans. Additionally, KoopMotion provides a significant improvement over baselines when comparing metrics that measure spatial and temporal dynamics modeling efficacy.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable extensions to given-data Sobol' index estimators</title>
<link>https://arxiv.org/abs/2509.09078</link>
<guid>https://arxiv.org/abs/2509.09078</guid>
<content:encoded><![CDATA[
arXiv:2509.09078v1 Announce Type: cross 
Abstract: Given-data methods for variance-based sensitivity analysis have significantly advanced the feasibility of Sobol' index computation for computationally expensive models and models with many inputs. However, the limitations of existing methods still preclude their application to models with an extremely large number of inputs. In this work, we present practical extensions to the existing given-data Sobol' index method, which allow variance-based sensitivity analysis to be efficiently performed on large models such as neural networks, which have $>10^4$ parameterizable inputs. For models of this size, holding all input-output evaluations simultaneously in memory -- as required by existing methods -- can quickly become impractical. These extensions also support nonstandard input distributions with many repeated values, which are not amenable to equiprobable partitions employed by existing given-data methods.
  Our extensions include a general definition of the given-data Sobol' index estimator with arbitrary partition, a streaming algorithm to process input-output samples in batches, and a heuristic to filter out small indices that are indistinguishable from zero indices due to statistical noise. We show that the equiprobable partition employed in existing given-data methods can introduce significant bias into Sobol' index estimates even at large sample sizes and provide numerical analyses that demonstrate why this can occur. We also show that our streaming algorithm can achieve comparable accuracy and runtimes with lower memory requirements, relative to current methods which process all samples at once. We demonstrate our novel developments on two application problems in neural network modeling.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CryptGNN: Enabling Secure Inference for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.09107</link>
<guid>https://arxiv.org/abs/2509.09107</guid>
<content:encoded><![CDATA[
arXiv:2509.09107v1 Announce Type: cross 
Abstract: We present CryptGNN, a secure and effective inference solution for third-party graph neural network (GNN) models in the cloud, which are accessed by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its secure message passing and feature transformation layers using distributed secure multi-party computation (SMPC) techniques. CryptGNN protects the client's input data and graph structure from the cloud provider and the third-party model owner, and it protects the model parameters from the cloud provider and the clients. CryptGNN works with any number of SMPC parties, does not require a trusted server, and is provably secure even if P-1 out of P parties in the cloud collude. Theoretical analysis and empirical experiments demonstrate the security and efficiency of CryptGNN.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Understanding by Design: How Datasets Shape Architectures and Insights</title>
<link>https://arxiv.org/abs/2509.09151</link>
<guid>https://arxiv.org/abs/2509.09151</guid>
<content:encoded><![CDATA[
arXiv:2509.09151v1 Announce Type: cross 
Abstract: Video understanding has advanced rapidly, fueled by increasingly complex datasets and powerful architectures. Yet existing surveys largely classify models by task or family, overlooking the structural pressures through which datasets guide architectural evolution. This survey is the first to adopt a dataset-driven perspective, showing how motion complexity, temporal span, hierarchical composition, and multimodal richness impose inductive biases that models should encode. We reinterpret milestones, from two-stream and 3D CNNs to sequential, transformer, and multimodal foundation models, as concrete responses to these dataset-driven pressures. Building on this synthesis, we offer practical guidance for aligning model design with dataset invariances while balancing scalability and task demands. By unifying datasets, inductive biases, and architectures into a coherent framework, this survey provides both a comprehensive retrospective and a prescriptive roadmap for advancing general-purpose video understanding.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Optimization of Stochastic Black-Box Functions with Arbitrary Noise Distributions using Wilson Score Kernel Density Estimation</title>
<link>https://arxiv.org/abs/2509.09238</link>
<guid>https://arxiv.org/abs/2509.09238</guid>
<content:encoded><![CDATA[
arXiv:2509.09238v1 Announce Type: cross 
Abstract: Many optimization problems in robotics involve the optimization of time-expensive black-box functions, such as those involving complex simulations or evaluation of real-world experiments. Furthermore, these functions are often stochastic as repeated experiments are subject to unmeasurable disturbances. Bayesian optimization can be used to optimize such methods in an efficient manner by deploying a probabilistic function estimator to estimate with a given confidence so that regions of the search space can be pruned away. Consequently, the success of the Bayesian optimization depends on the function estimator's ability to provide informative confidence bounds. Existing function estimators require many function evaluations to infer the underlying confidence or depend on modeling of the disturbances. In this paper, it is shown that the confidence bounds provided by the Wilson Score Kernel Density Estimator (WS-KDE) are applicable as excellent bounds to any stochastic function with an output confined to the closed interval [0;1] regardless of the distribution of the output. This finding opens up the use of WS-KDE for stable global optimization on a wider range of cost functions. The properties of WS-KDE in the context of Bayesian optimization are demonstrated in simulation and applied to the problem of automated trap design for vibrational part feeders.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning</title>
<link>https://arxiv.org/abs/2509.09284</link>
<guid>https://arxiv.org/abs/2509.09284</guid>
<content:encoded><![CDATA[
arXiv:2509.09284v1 Announce Type: cross 
Abstract: Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS-derived trajectories, traditionally used for training value or reward models, can be repurposed to improve policy optimization in preference-based reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables preference-consistent policy learning without value networks. We propose a staged GRPO training paradigm where completions are derived from partially revealed MCTS rollouts, introducing a novel tree-structured setting for advantage estimation. This leads to a rich class of prefix-conditioned reward signals, which we analyze theoretically and empirically. Our initial results indicate that while structured advantage estimation can stabilize updates and better reflect compositional reasoning quality, challenges such as advantage saturation and reward signal collapse remain. We propose heuristic and statistical solutions to mitigate these issues and discuss open challenges for learning under staged or tree-like reward structures.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception</title>
<link>https://arxiv.org/abs/2509.09297</link>
<guid>https://arxiv.org/abs/2509.09297</guid>
<content:encoded><![CDATA[
arXiv:2509.09297v1 Announce Type: cross 
Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object detection under real-world conditions. Traditional closed-set detectors degrade significantly under domain shifts and flight data corruption, posing risks to safety-critical applications. We propose a novel, model-agnostic open-set detection framework designed specifically for embedding-based detectors. The method explicitly handles unknown object rejection while maintaining robustness against corrupted flight data. It estimates semantic uncertainty via entropy modeling in the embedding space and incorporates spectral normalization and temperature scaling to enhance open-set discrimination. We validate our approach on the challenging AOT aerial benchmark and through extensive real-world flight tests. Comprehensive ablation studies demonstrate consistent improvements over baseline methods, achieving up to a 10\% relative AUROC gain compared to standard YOLO-based detectors. Additionally, we show that background rejection further strengthens robustness without compromising detection accuracy, making our solution particularly well-suited for reliable UAV perception in dynamic air-to-air environments.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment</title>
<link>https://arxiv.org/abs/2509.09327</link>
<guid>https://arxiv.org/abs/2509.09327</guid>
<content:encoded><![CDATA[
arXiv:2509.09327v1 Announce Type: cross 
Abstract: Automated surgical skill assessment (SSA) is a central task in surgical computer vision. Developing robust SSA models is challenging due to the scarcity of skill annotations, which are time-consuming to produce and require expert consensus. Few-shot learning (FSL) offers a scalable alternative enabling model development with minimal supervision, though its success critically depends on effective pre-training. While widely studied for several surgical downstream tasks, pre-training has remained largely unexplored in SSA. In this work, we formulate SSA as a few-shot task and investigate how self-supervised pre-training strategies affect downstream few-shot SSA performance. We annotate a publicly available robotic surgery dataset with Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate various pre-training sources across three few-shot settings. We quantify domain similarity and analyze how domain gap and the inclusion of procedure-specific data into pre-training influence transferability. Our results show that small but domain-relevant datasets can outperform large scale, less aligned ones, achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot settings, respectively. Moreover, incorporating procedure-specific data into pre-training with a domain-relevant external dataset significantly boosts downstream performance, with an average gain of +1.22% in accuracy and +2.28% in F1-score; however, applying the same strategy with less similar but large-scale sources can instead lead to performance degradation. Code and models are available at https://github.com/anastadimi/ssa-fsl.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-degree lower bounds via almost orthonormal bases</title>
<link>https://arxiv.org/abs/2509.09353</link>
<guid>https://arxiv.org/abs/2509.09353</guid>
<content:encoded><![CDATA[
arXiv:2509.09353v1 Announce Type: cross 
Abstract: Low-degree polynomials have emerged as a powerful paradigm for providing evidence of statistical-computational gaps across a variety of high-dimensional statistical models [Wein25]. For detection problems -- where the goal is to test a planted distribution $\mathbb{P}'$ against a null distribution $\mathbb{P}$ with independent components -- the standard approach is to bound the advantage using an $\mathbb{L}^2(\mathbb{P})$-orthonormal family of polynomials. However, this method breaks down for estimation tasks or more complex testing problems where $\mathbb{P}$ has some planted structures, so that no simple $\mathbb{L}^2(\mathbb{P})$-orthogonal polynomial family is available. To address this challenge, several technical workarounds have been proposed [SW22,SW25], though their implementation can be delicate. In this work, we propose a more direct proof strategy. Focusing on random graph models, we construct a basis of polynomials that is almost orthonormal under $\mathbb{P}$, in precisely those regimes where statistical-computational gaps arise. This almost orthonormal basis not only yields a direct route to establishing low-degree lower bounds, but also allows us to explicitly identify the polynomials that optimize the low-degree criterion. This, in turn, provides insights into the design of optimal polynomial-time algorithms. We illustrate the effectiveness of our approach by recovering known low-degree lower bounds, and establishing new ones for problems such as hidden subcliques, stochastic block models, and seriation models.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation</title>
<link>https://arxiv.org/abs/2509.09362</link>
<guid>https://arxiv.org/abs/2509.09362</guid>
<content:encoded><![CDATA[
arXiv:2509.09362v1 Announce Type: cross 
Abstract: A key challenge in scientific machine learning is solving partial differential equations (PDEs) on complex domains, where the curved geometry complicates the approximation of functions and their derivatives required by differential operators. This paper establishes the first simultaneous approximation theory for deep neural networks on manifolds. We prove that a constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property that plays a crucial role in controlling generalization error--can approximate any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for $k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero parameters, a rate that overcomes the curse of dimensionality by depending only on the intrinsic dimension $d$. These results readily extend to functions in H\"older-Zygmund spaces. We complement this result with a matching lower bound, proving our construction is nearly optimal by showing the required number of parameters matches up to a logarithmic factor. Our proof of the lower bound introduces novel estimates for the Vapnik-Chervonenkis dimension and pseudo-dimension of the network's high-order derivative classes. These complexity bounds provide a theoretical cornerstone for learning PDEs on manifolds involving derivatives. Our analysis reveals that the network architecture leverages a sparse structure to efficiently exploit the manifold's low-dimensional geometry.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation-Aware Distributionally Robust Optimization: A Knowledge Transfer Framework</title>
<link>https://arxiv.org/abs/2509.09371</link>
<guid>https://arxiv.org/abs/2509.09371</guid>
<content:encoded><![CDATA[
arXiv:2509.09371v1 Announce Type: cross 
Abstract: We propose REpresentation-Aware Distributionally Robust Estimation (READ), a novel framework for Wasserstein distributionally robust learning that accounts for predictive representations when guarding against distributional shifts. Unlike classical approaches that treat all feature perturbations equally, READ embeds a multidimensional alignment parameter into the transport cost, allowing the model to differentially discourage perturbations along directions associated with informative representations. This yields robustness to feature variation while preserving invariant structure. Our first contribution is a theoretical foundation: we show that seminorm regularizations for linear regression and binary classification arise as Wasserstein distributionally robust objectives, thereby providing tractable reformulations of READ and unifying a broad class of regularized estimators under the DRO lens. Second, we adopt a principled procedure for selecting the Wasserstein radius using the techniques of robust Wasserstein profile inference. This further enables the construction of valid, representation-aware confidence regions for model parameters with distinct geometric features. Finally, we analyze the geometry of READ estimators as the alignment parameters vary and propose an optimization algorithm to estimate the projection of the global optimum onto this solution surface. This procedure selects among equally robust estimators while optimally constructing a representation structure. We conclude by demonstrating the effectiveness of our framework through extensive simulations and a real-world study, providing a powerful robust estimation grounded in learning representation.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Concentration for Self-Supervised Dense Representations Learning</title>
<link>https://arxiv.org/abs/2509.09429</link>
<guid>https://arxiv.org/abs/2509.09429</guid>
<content:encoded><![CDATA[
arXiv:2509.09429v1 Announce Type: cross 
Abstract: Recent advances in image-level self-supervised learning (SSL) have made significant progress, yet learning dense representations for patches remains challenging. Mainstream methods encounter an over-dispersion phenomenon that patches from the same instance/category scatter, harming downstream performance on dense tasks. This work reveals that image-level SSL avoids over-dispersion by involving implicit semantic concentration. Specifically, the non-strict spatial alignment ensures intra-instance consistency, while shared patterns, i.e., similar parts of within-class instances in the input space, ensure inter-image consistency. Unfortunately, these approaches are infeasible for dense SSL due to their spatial sensitivity and complicated scene-centric data. These observations motivate us to explore explicit semantic concentration for dense SSL. First, to break the strict spatial alignment, we propose to distill the patch correspondences. Facing noisy and imbalanced pseudo labels, we propose a noise-tolerant ranking loss. The core idea is extending the Average Precision (AP) loss to continuous targets, such that its decision-agnostic and adaptive focusing properties prevent the student model from being misled. Second, to discriminate the shared patterns from complicated scenes, we propose the object-aware filter to map the output space to an object-based space. Specifically, patches are represented by learnable prototypes of objects via cross-attention. Last but not least, empirical studies across various tasks soundly support the effectiveness of our method. Code is available in https://github.com/KID-7391/CoTAP.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Database Views as Explanations for Relational Deep Learning</title>
<link>https://arxiv.org/abs/2509.09482</link>
<guid>https://arxiv.org/abs/2509.09482</guid>
<content:encoded><![CDATA[
arXiv:2509.09482v1 Announce Type: cross 
Abstract: In recent years, there has been significant progress in the development of deep learning models over relational databases, including architectures based on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph transformers. In effect, such architectures state how the database records and links (e.g., foreign-key references) translate into a large, complex numerical expression, involving numerous learnable parameters. This complexity makes it hard to explain, in human-understandable terms, how a model uses the available data to arrive at a given prediction. We present a novel framework for explaining machine-learning models over relational databases, where explanations are view definitions that highlight focused parts of the database that mostly contribute to the model's prediction. We establish such global abductive explanations by adapting the classic notion of determinacy by Nash, Segoufin, and Vianu (2010). In addition to tuning the tradeoff between determinacy and conciseness, the framework allows controlling the level of granularity by adopting different fragments of view definitions, such as ones highlighting whole columns, foreign keys between tables, relevant groups of tuples, and so on. We investigate the realization of the framework in the case of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive search over the space of all databases. We propose techniques that are model-agnostic, and others that are tailored to hetero-GNNs via the notion of learnable masking. Our approach is evaluated through an extensive empirical study on the RelBench collection, covering a variety of domains and different record-level tasks. The results demonstrate the usefulness of the proposed explanations, as well as the efficiency of their generation.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection</title>
<link>https://arxiv.org/abs/2509.09495</link>
<guid>https://arxiv.org/abs/2509.09495</guid>
<content:encoded><![CDATA[
arXiv:2509.09495v1 Announce Type: cross 
Abstract: Deepfakes, synthetic media created using advanced AI techniques, have intensified the spread of misinformation, particularly in politically sensitive contexts. Existing deepfake detection datasets are often limited, relying on outdated generation methods, low realism, or single-face imagery, restricting the effectiveness for general synthetic image detection. By analyzing social media posts, we identify multiple modalities through which deepfakes propagate misinformation. Furthermore, our human perception study demonstrates that recently developed proprietary models produce synthetic images increasingly indistinguishable from real ones, complicating accurate identification by the general public. Consequently, we present a comprehensive, politically-focused dataset specifically crafted for benchmarking detection against modern generative models. This dataset contains three million real images paired with descriptive captions, which are used for generating 963k corresponding high-quality synthetic images from a mix of proprietary and open-source models. Recognizing the continual evolution of generative techniques, we introduce an innovative crowdsourced adversarial platform, where participants are incentivized to generate and submit challenging synthetic images. This ongoing community-driven initiative ensures that deepfake detection methods remain robust and adaptive, proactively safeguarding public discourse from sophisticated misinformation threats.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner</title>
<link>https://arxiv.org/abs/2509.09513</link>
<guid>https://arxiv.org/abs/2509.09513</guid>
<content:encoded><![CDATA[
arXiv:2509.09513v1 Announce Type: cross 
Abstract: The diffusion MRI Neurite Exchange Imaging model offers a promising framework for probing gray matter microstructure by estimating parameters such as compartment sizes, diffusivities, and inter-compartmental water exchange time. However, existing protocols require long scan times. This study proposes a reduced acquisition scheme for the Connectome 2.0 scanner that preserves model accuracy while substantially shortening scan duration. We developed a data-driven framework using explainable artificial intelligence with a guided recursive feature elimination strategy to identify an optimal 8-feature subset from a 15-feature protocol. The performance of this optimized protocol was validated in vivo and benchmarked against the full acquisition and alternative reduction strategies. Parameter accuracy, preservation of anatomical contrast, and test-retest reproducibility were assessed. The reduced protocol yielded parameter estimates and cortical maps comparable to the full protocol, with low estimation errors in synthetic data and minimal impact on test-retest variability. Compared to theory-driven and heuristic reduction schemes, the optimized protocol demonstrated superior robustness, reducing the deviation in water exchange time estimates by over two-fold. In conclusion, this hybrid optimization framework enables viable imaging of neurite exchange in 14 minutes without loss of parameter fidelity. This approach supports the broader application of exchange-sensitive diffusion magnetic resonance imaging in neuroscience and clinical research, and offers a generalizable method for designing efficient acquisition protocols in biophysical parameter mapping.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning</title>
<link>https://arxiv.org/abs/2509.09524</link>
<guid>https://arxiv.org/abs/2509.09524</guid>
<content:encoded><![CDATA[
arXiv:2509.09524v1 Announce Type: cross 
Abstract: This system paper presents the DeMeVa team's approaches to the third edition of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et al., 2025). We explore two directions: in-context learning (ICL) with large language models, where we compare example sampling strategies; and label distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we evaluate several fine-tuning methods. Our contributions are twofold: (1) we show that ICL can effectively predict annotator-specific annotations (perspectivist annotations), and that aggregating these predictions into soft labels yields competitive performance; and (2) we argue that LDL methods are promising for soft label predictions and merit further exploration by the perspectivist community.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates</title>
<link>https://arxiv.org/abs/2509.09550</link>
<guid>https://arxiv.org/abs/2509.09550</guid>
<content:encoded><![CDATA[
arXiv:2509.09550v1 Announce Type: cross 
Abstract: Neural Audio Codecs (NACs) have become increasingly adopted in speech processing tasks due to their excellent rate-distortion performance and compatibility with Large Language Models (LLMs) as discrete feature representations for audio generation. While most existing codecs rely on Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has recently emerged as a compelling alternative that simplifies training and natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC, and show that FSQ encodes baked-in redundancy which produces an encoding which is robust when transmitted through noisy channels. First, through an encoder distillation experiment, we show that two different encoders can learn to encode identical audio into vastly different code sequences whilst maintaining comparable reconstruction quality with the same quantizer and decoder. Second, we demonstrate that FSQ has vastly superior bit-level perturbation robustness by comparing the performance of RVQ and FSQ codecs when simulating the transmission of code sequences through a noisy channel.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution</title>
<link>https://arxiv.org/abs/2509.09560</link>
<guid>https://arxiv.org/abs/2509.09560</guid>
<content:encoded><![CDATA[
arXiv:2509.09560v1 Announce Type: cross 
Abstract: Embodied AI systems operate in dynamic environments, requiring seamless integration of perception and generation modules to process high-frequency input and output demands. Traditional sequential computation patterns, while effective in ensuring accuracy, face significant limitations in achieving the necessary "thinking" frequency for real-world applications. In this work, we present Auras, an algorithm-system co-designed inference framework to optimize the inference frequency of embodied AI agents. Auras disaggregates the perception and generation and provides controlled pipeline parallelism for them to achieve high and stable throughput. Faced with the data staleness problem that appears when the parallelism is increased, Auras establishes a public context for perception and generation to share, thereby promising the accuracy of embodied agents. Experimental results show that Auras improves throughput by 2.54x on average while achieving 102.7% of the original accuracy, demonstrating its efficacy in overcoming the constraints of sequential computation and providing high throughput.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets</title>
<link>https://arxiv.org/abs/2509.09564</link>
<guid>https://arxiv.org/abs/2509.09564</guid>
<content:encoded><![CDATA[
arXiv:2509.09564v1 Announce Type: cross 
Abstract: Supervised machine learning techniques rely on labeled data to achieve high task performance, but this requires the labels to capture some meaningful differences in the underlying data structure. For training network intrusion detection algorithms, most datasets contain a series of attack classes and a single large benign class which captures all non-attack network traffic. A review of intrusion detection papers and guides that explicitly state their data preprocessing steps identified that the majority took the labeled categories of the dataset at face value when training their algorithms. The present paper evaluates the structure of benign traffic in several common intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and determines whether there are meaningful sub-categories within this traffic which may improve overall multi-classification performance using common machine learning techniques. We present an overview of some unsupervised clustering techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they differentially cluster the benign traffic space.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking</title>
<link>https://arxiv.org/abs/2509.09583</link>
<guid>https://arxiv.org/abs/2509.09583</guid>
<content:encoded><![CDATA[
arXiv:2509.09583v1 Announce Type: cross 
Abstract: Social connection is a vital part of learning, yet online course environments present barriers to the organic formation of social groups. SAMI offers one solution by facilitating student connections, but its effectiveness is constrained by an incomplete Theory of Mind, limiting its ability to create an effective mental model of a student. One facet of this is its inability to intuit personality, which may influence the relevance of its recommendations. To explore this, we propose a personality detection model utilizing GPTs zero-shot capability to infer Big-Five personality traits from forum introduction posts, often encouraged in online courses. We benchmark its performance against established models, demonstrating its efficacy in this task. Furthermore, we integrate this model into SAMIs entity-based matchmaking system, enabling personality-informed social recommendations. Initial integration suggests personality traits can complement existing matching factors, though additional evaluation is required to determine their full impact on student engagement and match quality.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObjectReact: Learning Object-Relative Control for Visual Navigation</title>
<link>https://arxiv.org/abs/2509.09594</link>
<guid>https://arxiv.org/abs/2509.09594</guid>
<content:encoded><![CDATA[
arXiv:2509.09594v1 Announce Type: cross 
Abstract: Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an "image-relative" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning "object-relative" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a "relative" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed "ObjectReact", conditioned directly on a high-level "WayObject Costmap" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations</title>
<link>https://arxiv.org/abs/2509.09651</link>
<guid>https://arxiv.org/abs/2509.09651</guid>
<content:encoded><![CDATA[
arXiv:2509.09651v1 Announce Type: cross 
Abstract: We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering MoE LLMs via Expert (De)Activation</title>
<link>https://arxiv.org/abs/2509.09660</link>
<guid>https://arxiv.org/abs/2509.09660</guid>
<content:encoded><![CDATA[
arXiv:2509.09660v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token through a subset of specialized Feed-Forward Networks (FFN), known as experts. We present SteerMoE, a framework for steering MoE models by detecting and controlling behavior-linked experts. Our detection method identifies experts with distinct activation patterns across paired inputs exhibiting contrasting behaviors. By selectively (de)activating such experts during inference, we control behaviors like faithfulness and safety without retraining or modifying weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to +20% and faithfulness by +27%. In adversarial attack mode, it drops safety by -41% alone, and -100% when combined with existing jailbreak methods, bypassing all safety guardrails and exposing a new dimension of alignment faking hidden within experts.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.09674</link>
<guid>https://arxiv.org/abs/2509.09674</guid>
<content:encoded><![CDATA[
arXiv:2509.09674v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$ on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models</title>
<link>https://arxiv.org/abs/2509.09675</link>
<guid>https://arxiv.org/abs/2509.09675</guid>
<content:encoded><![CDATA[
arXiv:2509.09675v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Relationship Between Adversarial Robustness and Decision Region in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2207.03400</link>
<guid>https://arxiv.org/abs/2207.03400</guid>
<content:encoded><![CDATA[
arXiv:2207.03400v2 Announce Type: replace 
Abstract: In general, Deep Neural Networks (DNNs) are evaluated by the generalization performance measured on unseen data excluded from the training phase. Along with the development of DNNs, the generalization performance converges to the state-of-the-art and it becomes difficult to evaluate DNNs solely based on this metric. The robustness against adversarial attack has been used as an additional metric to evaluate DNNs by measuring their vulnerability. However, few studies have been performed to analyze the adversarial robustness in terms of the geometry in DNNs. In this work, we perform an empirical study to analyze the internal properties of DNNs that affect model robustness under adversarial attacks. In particular, we propose the novel concept of the Populated Region Set (PRS), where training samples are populated more frequently, to represent the internal properties of DNNs in a practical setting. From systematic experiments with the proposed concept, we provide empirical evidence to validate that a low PRS ratio has a strong relationship with the adversarial robustness of DNNs. We also devise PRS regularizer leveraging the characteristics of PRS to improve the adversarial robustness without adversarial training.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Inventory Networks: Toward Reliable Policy Optimization</title>
<link>https://arxiv.org/abs/2306.11246</link>
<guid>https://arxiv.org/abs/2306.11246</guid>
<content:encoded><![CDATA[
arXiv:2306.11246v3 Announce Type: replace 
Abstract: We argue that inventory management presents unique opportunities for the reliable application of deep reinforcement learning (DRL). To enable this, we emphasize and test two complementary techniques. The first is Hindsight Differentiable Policy Optimization (HDPO), which uses pathwise gradients from offline counterfactual simulations to directly and efficiently optimize policy performance. Unlike standard policy gradient methods that rely on high-variance score-function estimators, HDPO computes gradients by differentiating through the known system dynamics. Via extensive benchmarking, we show that HDPO recovers near-optimal policies in settings with known or bounded optima, is more robust than variants of the REINFORCE algorithm, and significantly outperforms generalized newsvendor heuristics on problems using real time series data. Our second technique aligns neural policy architectures with the topology of the inventory network. We exploit Graph Neural Networks (GNNs) as a natural inductive bias for encoding supply chain structure, demonstrate that they can represent optimal and near-optimal policies in two theoretical settings, and empirically show that they reduce data requirements across six diverse inventory problems. A key obstacle to progress in this area is the lack of standardized benchmark problems. To address this gap, we open-source a suite of benchmark environments, along with our full codebase, to promote transparency and reproducibility. All resources are available at github.com/MatiasAlvo/Neural_inventory_control.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry and Stability of Supervised Learning Problems</title>
<link>https://arxiv.org/abs/2403.01660</link>
<guid>https://arxiv.org/abs/2403.01660</guid>
<content:encoded><![CDATA[
arXiv:2403.01660v2 Announce Type: replace 
Abstract: We introduce a notion of distance between supervised learning problems, which we call the Risk distance. This distance, inspired by optimal transport, facilitates stability results; one can quantify how seriously issues like sampling bias, noise, limited data, and approximations might change a given problem by bounding how much these modifications can move the problem under the Risk distance. With the distance established, we explore the geometry of the resulting space of supervised learning problems, providing explicit geodesics and proving that the set of classification problems is dense in a larger class of problems. We also provide two variants of the Risk distance: one that incorporates specified weights on a problem's predictors, and one that is more sensitive to the contours of a problem's risk landscape.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attribution Regularization for Multimodal Paradigms</title>
<link>https://arxiv.org/abs/2404.02359</link>
<guid>https://arxiv.org/abs/2404.02359</guid>
<content:encoded><![CDATA[
arXiv:2404.02359v3 Announce Type: replace 
Abstract: Multimodal machine learning has gained significant attention in recent years due to its potential for integrating information from multiple modalities to enhance learning and decision-making processes. However, it is commonly observed that unimodal models outperform multimodal models, despite the latter having access to richer information. Additionally, the influence of a single modality often dominates the decision-making process, resulting in suboptimal performance. This research project aims to address these challenges by proposing a novel regularization term that encourages multimodal models to effectively utilize information from all modalities when making decisions. The focus of this project lies in the video-audio domain, although the proposed regularization technique holds promise for broader applications in embodied AI research, where multiple modalities are involved. By leveraging this regularization term, the proposed approach aims to mitigate the issue of unimodal dominance and improve the performance of multimodal machine learning systems. Through extensive experimentation and evaluation, the effectiveness and generalizability of the proposed technique will be assessed. The findings of this research project have the potential to significantly contribute to the advancement of multimodal machine learning and facilitate its application in various domains, including multimedia analysis, human-computer interaction, and embodied AI research.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaWaveNet: Adaptive Wavelet Network for Time Series Analysis</title>
<link>https://arxiv.org/abs/2405.11124</link>
<guid>https://arxiv.org/abs/2405.11124</guid>
<content:encoded><![CDATA[
arXiv:2405.11124v2 Announce Type: replace 
Abstract: Time series data analysis is a critical component in various domains such as finance, healthcare, and meteorology. Despite the progress in deep learning for time series analysis, there remains a challenge in addressing the non-stationary nature of time series data. Traditional models, which are built on the assumption of constant statistical properties over time, often struggle to capture the temporal dynamics in realistic time series, resulting in bias and error in time series analysis. This paper introduces the Adaptive Wavelet Network (AdaWaveNet), a novel approach that employs Adaptive Wavelet Transformation for multi-scale analysis of non-stationary time series data. AdaWaveNet designed a lifting scheme-based wavelet decomposition and construction mechanism for adaptive and learnable wavelet transforms, which offers enhanced flexibility and robustness in analysis. We conduct extensive experiments on 10 datasets across 3 different tasks, including forecasting, imputation, and a newly established super-resolution task. The evaluations demonstrate the effectiveness of AdaWaveNet over existing methods in all three tasks, which illustrates its potential in various real-world applications.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Multiple Descents in Unsupervised Autoencoders</title>
<link>https://arxiv.org/abs/2406.11703</link>
<guid>https://arxiv.org/abs/2406.11703</guid>
<content:encoded><![CDATA[
arXiv:2406.11703v3 Announce Type: replace 
Abstract: The phenomenon of double descent has challenged the traditional bias-variance trade-off in supervised learning but remains unexplored in unsupervised learning, with some studies arguing for its absence. In this study, we first demonstrate analytically that double descent does not occur in linear unsupervised autoencoders (AEs). In contrast, we show for the first time that both double and triple descent can be observed with nonlinear AEs across various data models and architectural designs. We examine the effects of partial sample and feature noise and highlight the importance of bottleneck size in influencing the double descent curve. Through extensive experiments on both synthetic and real datasets, we uncover model-wise, epoch-wise, and sample-wise double descent across several data types and architectures. Our findings indicate that over-parameterized models not only improve reconstruction but also enhance performance in downstream tasks such as anomaly detection and domain adaptation, highlighting their practical value in complex real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering physical laws with parallel symbolic enumeration</title>
<link>https://arxiv.org/abs/2407.04405</link>
<guid>https://arxiv.org/abs/2407.04405</guid>
<content:encoded><![CDATA[
arXiv:2407.04405v4 Announce Type: replace 
Abstract: Symbolic regression plays a crucial role in modern scientific research thanks to its capability of discovering concise and interpretable mathematical expressions from data. A key challenge lies in the search for parsimonious and generalizable mathematical formulas, in an infinite search space, while intending to fit the training data. Existing algorithms have faced a critical bottleneck of accuracy and efficiency over a decade when handling problems of complexity, which essentially hinders the pace of applying symbolic regression for scientific exploration across interdisciplinary domains. To this end, we introduce parallel symbolic enumeration (PSE) to efficiently distill generic mathematical expressions from limited data. Experiments show that PSE achieves higher accuracy and faster computation compared to the state-of-the-art baseline algorithms across over 200 synthetic and experimental problem sets (e.g., improving the recovery accuracy by up to 99% and reducing runtime by an order of magnitude). PSE represents an advance in accurate and efficient data-driven discovery of symbolic, interpretable models (e.g., underlying physical laws), and improves the scalability of symbolic learning.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Disentanglement under Dependent Factors of Variation</title>
<link>https://arxiv.org/abs/2408.07016</link>
<guid>https://arxiv.org/abs/2408.07016</guid>
<content:encoded><![CDATA[
arXiv:2408.07016v2 Announce Type: replace 
Abstract: Representation learning is an approach that allows to discover and extract the factors of variation from the data. Intuitively, a representation is said to be disentangled if it separates the different factors of variation in a way that is understandable to humans. Definitions of disentanglement and metrics to measure it usually assume that the factors of variation are independent of each other. However, this is generally false in the real world, which limits the use of these definitions and metrics to very specific and unrealistic scenarios. In this paper we give a definition of disentanglement based on information theory that is also valid when the factors of variation are not independent. Furthermore, we relate this definition to the Information Bottleneck Method. Finally, we propose a method to measure the degree of disentanglement from the given definition that works when the factors of variation are not independent. We show through different experiments that the method proposed in this paper correctly measures disentanglement with non-independent factors of variation, while other methods fail in this scenario.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Large Language Models in Your Pockets: Performance Study on COTS Mobile Devices</title>
<link>https://arxiv.org/abs/2410.03613</link>
<guid>https://arxiv.org/abs/2410.03613</guid>
<content:encoded><![CDATA[
arXiv:2410.03613v3 Announce Type: replace 
Abstract: As large language models (LLMs) increasingly integrate into every aspect of our work and daily lives, there are growing concerns about user privacy, which push the trend toward local deployment of these models. There are a number of lightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on smartphones, providing users with greater control over their personal data. As a rapidly emerging application, we are concerned about their performance on commercial-off-the-shelf mobile devices. To fully understand the current landscape of LLM deployment on mobile platforms, we conduct a comprehensive measurement study on mobile devices. We evaluate both metrics that affect user experience, including token throughput, latency, and battery consumption, as well as factors critical to developers, such as resource utilization, DVFS strategies, and inference engines. In addition, we provide a detailed analysis of how these hardware capabilities and system dynamics affect on-device LLM performance, which may help developers identify and address bottlenecks for mobile LLM applications. We also provide comprehensive comparisons across the mobile system-on-chips (SoCs) from major vendors, highlighting their performance differences in handling LLM workloads. We hope that this study can provide insights for both the development of on-device LLMs and the design for future mobile system architecture.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor-Based Foundations of Ordinary Least Squares and Neural Network Regression Models</title>
<link>https://arxiv.org/abs/2411.12873</link>
<guid>https://arxiv.org/abs/2411.12873</guid>
<content:encoded><![CDATA[
arXiv:2411.12873v5 Announce Type: replace 
Abstract: This article introduces a novel approach to the mathematical development of Ordinary Least Squares and Neural Network regression models, diverging from traditional methods in current Machine Learning literature. By leveraging Tensor Analysis and fundamental matrix computations, the theoretical foundations of both models are meticulously detailed and extended to their complete algorithmic forms. The study culminates in the presentation of three algorithms, including a streamlined version of the Backpropagation Algorithm for Neural Networks, illustrating the benefits of this new mathematical approach.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication Compression for Distributed Learning without Control Variates</title>
<link>https://arxiv.org/abs/2412.04538</link>
<guid>https://arxiv.org/abs/2412.04538</guid>
<content:encoded><![CDATA[
arXiv:2412.04538v2 Announce Type: replace 
Abstract: Distributed learning algorithms, such as the ones employed in Federated Learning (FL), require communication compression to reduce the cost of client uploads. The compression methods used in practice are often biased, making error feedback necessary both to achieve convergence under aggressive compression and to provide theoretical convergence guarantees. However, error feedback requires client-specific control variates, creating two key challenges: it violates privacy-preserving principles and demands stateful clients. In this paper, we propose Compressed Aggregate Feedback (CAFe), a novel distributed learning framework that allows highly compressible client updates by exploiting past aggregated updates, and does not require control variates. We consider Distributed Gradient Descent (DGD) as a representative algorithm and analytically prove CAFe's superiority to Distributed Compressed Gradient Descent (DCGD) with biased compression in the non-convex regime with bounded gradient dissimilarity. Experimental results confirm that CAFe outperforms existing distributed learning compression schemes.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Simplicity and Sophistication using GLinear: A Novel Architecture for Enhanced Time Series Prediction</title>
<link>https://arxiv.org/abs/2501.01087</link>
<guid>https://arxiv.org/abs/2501.01087</guid>
<content:encoded><![CDATA[
arXiv:2501.01087v4 Announce Type: replace 
Abstract: Time Series Forecasting (TSF) is an important application across many fields. There is a debate about whether Transformers, despite being good at understanding long sequences, struggle with preserving temporal relationships in time series data. Recent research suggests that simpler linear models might outperform or at least provide competitive performance compared to complex Transformer-based models for TSF tasks. In this paper, we propose a novel data-efficient architecture, \textit{Gaussian-activated Linear model (GLinear)}, for multivariate TSF that exploits periodic patterns to provide better accuracy. It achieves higher prediction accuracy while requiring less historical data than other state-of-the-art linear predictors. Four different datasets (ETTh1, Electricity, Traffic, and Weather) are used to evaluate the performance of the proposed predictor. A performance comparison with state-of-the-art linear architectures (such as NLinear, DLinear, and RLinear) and transformer-based time series predictors (Autoformer) shows that the GLinear, despite being data efficient, outperforms the existing architectures in most cases of multivariate TSF while being competitive in others. We hope that the proposed GLinear model opens new fronts of research and development of simpler and more sophisticated architectures for data and computationally efficient time-series analysis. The source code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings</title>
<link>https://arxiv.org/abs/2501.08219</link>
<guid>https://arxiv.org/abs/2501.08219</guid>
<content:encoded><![CDATA[
arXiv:2501.08219v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing (NLP) tasks, leading to widespread adoption in both research and industry. However, their inference workloads are computationally and energy intensive, raising concerns about sustainability and environmental impact. As LLMs continue to scale, it becomes essential to identify and optimize the factors that influence their runtime efficiency without compromising performance. In this work, we systematically investigate the energy-performance trade-offs of LLMs during inference. We benchmark models of varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1, LLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question answering, commonsense reasoning, and factual generation. We analyze the effect of input characteristics, such as sequence length, entropy, named entity density and so on. Furthermore, we examine the impact of hardware-level optimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring how different GPU clock settings affect latency and power consumption. Our empirical findings show that model architecture, input complexity, and clock configuration significantly influence inference efficiency. By correlating input features with energy metrics and evaluating DVFS behavior, we identify practical strategies that reduce energy consumption by up to 30% while preserving model quality. This study provides actionable insights for designing energy-efficient and sustainable LLM inference systems.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Optimal Sample Complexity in Reward-Free Kernel-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.07715</link>
<guid>https://arxiv.org/abs/2502.07715</guid>
<content:encoded><![CDATA[
arXiv:2502.07715v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) problems are being considered under increasingly more complex structures. While tabular and linear models have been thoroughly explored, the analytical study of RL under nonlinear function approximation, especially kernel-based models, has recently gained traction for their strong representational capacity and theoretical tractability. In this context, we examine the question of statistical efficiency in kernel-based RL within the reward-free RL framework, specifically asking: how many samples are required to design a near-optimal policy? Existing work addresses this question under restrictive assumptions about the class of kernel functions. We first explore this question by assuming a generative model, then relax this assumption at the cost of increasing the sample complexity by a factor of H, the length of the episode. We tackle this fundamental problem using a broad class of kernels and a simpler algorithm compared to prior work. Our approach derives new confidence intervals for kernel ridge regression, specific to our RL setting, which may be of broader applicability. We further validate our theoretical findings through simulations.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Non-Acyclic GFlowNets in Discrete Environments</title>
<link>https://arxiv.org/abs/2502.07735</link>
<guid>https://arxiv.org/abs/2502.07735</guid>
<content:encoded><![CDATA[
arXiv:2502.07735v3 Announce Type: replace 
Abstract: Generative Flow Networks (GFlowNets) are a family of generative models that learn to sample objects from a given probability distribution, potentially known up to a normalizing constant. Instead of working in the object space, GFlowNets proceed by sampling trajectories in an appropriately constructed directed acyclic graph environment, greatly relying on the acyclicity of the graph. In our paper, we revisit the theory that relaxes the acyclicity assumption and present a simpler theoretical framework for non-acyclic GFlowNets in discrete environments. Moreover, we provide various novel theoretical insights related to training with fixed backward policies, the nature of flow functions, and connections between entropy-regularized RL and non-acyclic GFlowNets, which naturally generalize the respective concepts and theoretical results from the acyclic setting. In addition, we experimentally re-examine the concept of loss stability in non-acyclic GFlowNet training, as well as validate our own theoretical findings.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive kernel predictors from feature-learning infinite limits of neural networks</title>
<link>https://arxiv.org/abs/2502.07998</link>
<guid>https://arxiv.org/abs/2502.07998</guid>
<content:encoded><![CDATA[
arXiv:2502.07998v2 Announce Type: replace 
Abstract: Previous influential work showed that infinite width limits of neural networks in the lazy training regime are described by kernel machines. Here, we show that neural networks trained in the rich, feature learning infinite-width regime in two different settings are also described by kernel machines, but with data-dependent kernels. For both cases, we provide explicit expressions for the kernel predictors and prescriptions to numerically calculate them. To derive the first predictor, we study the large-width limit of feature-learning Bayesian networks, showing how feature learning leads to task-relevant adaptation of layer kernels and preactivation densities. The saddle point equations governing this limit result in a min-max optimization problem that defines the kernel predictor. To derive the second predictor, we study gradient flow training of randomly initialized networks trained with weight decay in the infinite-width limit using dynamical mean field theory (DMFT). The fixed point equations of the arising DMFT defines the task-adapted internal representations and the kernel predictor. We compare our kernel predictors to kernels derived from lazy regime and demonstrate that our adaptive kernels achieve lower test loss on benchmark datasets.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOLLM: Multi-Objective Large Language Model for Molecular Design -- Optimizing with Experts</title>
<link>https://arxiv.org/abs/2502.12845</link>
<guid>https://arxiv.org/abs/2502.12845</guid>
<content:encoded><![CDATA[
arXiv:2502.12845v3 Announce Type: replace 
Abstract: Molecular design plays a critical role in advancing fields such as drug discovery, materials science, and chemical engineering. This work introduces the Multi-Objective Large Language Model for Molecular Design (MOLLM), a novel framework that combines domain-specific knowledge with the adaptability of large language models to optimize molecular properties across multiple objectives. Leveraging in-context learning and multi-objective optimization, MOLLM achieves superior performance and innovation, consistently surpassing state-of-the-art (SOTA) methods. We significantly improve the efficiency of our framework, making it 14 times faster and substantially more cost-effective without compromising performance compared to the latest similar work. Our results demonstrate that MOLLM consistently outperforms SOTA models across experiments and excels on the PMO benchmark. In addition, we provide extensive ablation studies and analysis to evaluate the effectiveness of each component and the quality of the output molecules.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vector-Quantized Foundation Model for Patient Behavior Monitoring</title>
<link>https://arxiv.org/abs/2503.15221</link>
<guid>https://arxiv.org/abs/2503.15221</guid>
<content:encoded><![CDATA[
arXiv:2503.15221v3 Announce Type: replace 
Abstract: Foundation models have achieved remarkable success across various domains, yet their adoption in healthcare remains limited. While significant advances have been made in medical imaging, genetic biomarkers, and time series from electronic health records, the potential of foundation models for patient behavior monitoring through personal digital devices remains underexplored. The data generated by these devices are inherently heterogeneous, multisource, and often exhibit high rates of missing data, posing unique challenges. This paper introduces a novel foundation model based on a modified vector quantized variational autoencoder, specifically designed to process real-world data from smartphones and wearable devices. We leveraged the discrete latent representation of this model to effectively perform two downstream tasks, suicide risk assessment and emotional state prediction, on different held-out clinical cohorts without the need of fine-tuning. We also highlight the existence of a trade-off between discrete and continuous latent structures, suggesting that hybrid models may be optimal for balancing accuracy across various supervised and unsupervised tasks.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variance-Aware Noisy Training: Hardening DNNs against Unstable Analog Computations</title>
<link>https://arxiv.org/abs/2503.16183</link>
<guid>https://arxiv.org/abs/2503.16183</guid>
<content:encoded><![CDATA[
arXiv:2503.16183v2 Announce Type: replace 
Abstract: The disparity between the computational demands of deep learning and the capabilities of compute hardware is expanding drastically. Although deep learning achieves remarkable performance in countless tasks, its escalating requirements for computational power and energy consumption surpass the sustainable limits of even specialized neural processing units, including the Apple Neural Engine and NVIDIA TensorCores. This challenge is intensified by the slowdown in CMOS scaling.
  Analog computing presents a promising alternative, offering substantial improvements in energy efficiency by directly manipulating physical quantities such as current, voltage, charge, or photons. However, it is inherently vulnerable to manufacturing variations, nonlinearities, and noise, leading to degraded prediction accuracy. One of the most effective techniques for enhancing robustness, Noisy Training, introduces noise during the training phase to reinforce the model against disturbances encountered during inference. Although highly effective, its performance degrades in real-world environments where noise characteristics fluctuate due to external factors such as temperature variations and temporal drift.
  This study underscores the necessity of Noisy Training while revealing its fundamental limitations in the presence of dynamic noise. To address these challenges, we propose Variance-Aware Noisy Training, a novel approach that mitigates performance degradation by incorporating noise schedules which emulate the evolving noise conditions encountered during inference. Our method substantially improves model robustness, without training overhead. We demonstrate a significant increase in robustness, from 79.3\% with conventional Noisy Training to 97.6\% with Variance-Aware Noisy Training on CIFAR-10 and from 32.4\% to 99.7\% on Tiny ImageNet.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review</title>
<link>https://arxiv.org/abs/2504.18544</link>
<guid>https://arxiv.org/abs/2504.18544</guid>
<content:encoded><![CDATA[
arXiv:2504.18544v2 Announce Type: replace 
Abstract: Generating synthetic tabular data can be challenging, however evaluation of their quality is just as challenging, if not more. This systematic review sheds light on the critical importance of rigorous evaluation of synthetic health data to ensure reliability, relevance, and their appropriate use. Based on screening of 1766 papers and a detailed review of 101 papers we identified key challenges, including lack of consensus on evaluation methods, improper use of evaluation metrics, limited input from domain experts, inadequate reporting of dataset characteristics, and limited reproducibility of results. In response, we provide several guidelines on the generation and evaluation of synthetic data, to allow the community to unlock and fully harness the transformative potential of synthetic data and accelerate innovation.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence Analysis of Asynchronous Federated Learning with Gradient Compression for Non-Convex Optimization</title>
<link>https://arxiv.org/abs/2504.19903</link>
<guid>https://arxiv.org/abs/2504.19903</guid>
<content:encoded><![CDATA[
arXiv:2504.19903v2 Announce Type: replace 
Abstract: Gradient compression is an effective technique for reducing communication overhead in federated learning (FL), and error feedback (EF) is widely adopted to remedy the compression errors. However, in asynchronous FL settings-which inherently face three major challenges: asynchronous delay, data heterogeneity, and flexible client participation-the complex interactions among these system/statistical constraints and compression/EF mechanisms remain poorly understood theoretically. There is a significant lack of systematic convergence analysis that adequately captures these complex couplings. In this paper, we fill this gap by analyzing the convergence behaviors of FL under different frameworks. We first consider a basic asynchronous FL framework AsynFL, and establish an improved convergence analysis that relies on fewer assumptions and yields a superior convergence rate than prior studies. Then, we consider a variant framework with gradient compression, AsynFLC. We derive sufficient conditions for its convergence, indicating the nonlinear interaction between asynchronous delay and compression rate. Our analysis further demonstrates how asynchronous delay and data heterogeneity jointly amplify compression-induced errors, thereby hindering convergence. Furthermore, we study the convergence of AsynFLC-EF, the framework that further integrates EF. We prove that EF can effectively reduce the variance of gradient estimation despite asynchronous delays, which enables AsynFLC-EF to match the convergence rate of AsynFL. We also show that the impact of asynchronous delay and flexible participation on EF is limited to slowing down the higher-order convergence term. Experimental results substantiate our analytical findings very well.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Query Network for Efficient Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.12917</link>
<guid>https://arxiv.org/abs/2505.12917</guid>
<content:encoded><![CDATA[
arXiv:2505.12917v2 Announce Type: replace 
Abstract: Sufficiently modeling the correlations among variables (aka channels) is crucial for achieving accurate multivariate time series forecasting (MTSF). In this paper, we propose a novel technique called Temporal Query (TQ) to more effectively capture multivariate correlations, thereby improving model performance in MTSF tasks. Technically, the TQ technique employs periodically shifted learnable vectors as queries in the attention mechanism to capture global inter-variable patterns, while the keys and values are derived from the raw input data to encode local, sample-level correlations. Building upon the TQ technique, we develop a simple yet efficient model named Temporal Query Network (TQNet), which employs only a single-layer attention mechanism and a lightweight multi-layer perceptron (MLP). Extensive experiments demonstrate that TQNet learns more robust multivariate correlations, achieving state-of-the-art forecasting accuracy across 12 challenging real-world datasets. Furthermore, TQNet achieves high efficiency comparable to linear-based methods even on high-dimensional datasets, balancing performance and computational cost. The code is available at: https://github.com/ACAT-SCUT/TQNet.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Influence Functions with Flat Validation Minima</title>
<link>https://arxiv.org/abs/2505.19097</link>
<guid>https://arxiv.org/abs/2505.19097</guid>
<content:encoded><![CDATA[
arXiv:2505.19097v2 Announce Type: replace 
Abstract: The Influence Function (IF) is a widely used technique for assessing the impact of individual training samples on model predictions. However, existing IF methods often fail to provide reliable influence estimates in deep neural networks, particularly when applied to noisy training data. This issue does not stem from inaccuracies in parameter change estimation, which has been the primary focus of prior research, but rather from deficiencies in loss change estimation, specifically due to the sharpness of validation risk. In this work, we establish a theoretical connection between influence estimation error, validation set risk, and its sharpness, underscoring the importance of flat validation minima for accurate influence estimation. Furthermore, we introduce a novel estimation form of Influence Function specifically designed for flat validation minima. Experimental results across various tasks validate the superiority of our approach.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crack Path Prediction with Operator Learning using Discrete Particle System data Generation</title>
<link>https://arxiv.org/abs/2506.01976</link>
<guid>https://arxiv.org/abs/2506.01976</guid>
<content:encoded><![CDATA[
arXiv:2506.01976v2 Announce Type: replace 
Abstract: Accurately modeling crack propagation is critical for predicting failure in engineering materials and structures, where small cracks can rapidly evolve and cause catastrophic damage. The interaction of cracks with discontinuities, such as holes, significantly affects crack deflection and arrest. Recent developments in discrete particle systems with multibody interactions based on constitutive behavior have demonstrated the ability to capture crack nucleation and evolution without relying on continuum assumptions. In this work, we use data from Constitutively Informed Particle Dynamics (CPD) simulations to train operator learning models, specifically Deep Operator Networks (DeepONets), which learn mappings between function spaces instead of finite-dimensional vectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for predicting time-evolving crack propagation in specimens with varying geometries. Three representative cases are studied: (i) varying notch height without active fracture; and (ii) and (iii) combinations of notch height and hole radius where dynamic fracture occurs on irregular discrete meshes. The models are trained using geometric inputs in the branch network and spatial-temporal coordinates in the trunk network. Results show that Fusion DeepONet consistently outperforms the vanilla variant, with more accurate predictions especially in non-fracturing cases. Fracture-driven scenarios involving displacement and crack evolution remain more challenging. These findings highlight the potential of Fusion DeepONet to generalize across complex, geometry-varying, and time-dependent crack propagation phenomena.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Estimation by Human Perception versus Neural Models</title>
<link>https://arxiv.org/abs/2506.15850</link>
<guid>https://arxiv.org/abs/2506.15850</guid>
<content:encoded><![CDATA[
arXiv:2506.15850v2 Announce Type: replace 
Abstract: Modern neural networks (NNs) often achieve high predictive accuracy but are poorly calibrated, producing overconfident predictions even when wrong. This miscalibration poses serious challenges in applications where reliable uncertainty estimates are critical. In this work, we investigate how human perceptual uncertainty compares to uncertainty estimated by NNs. Using three vision benchmarks annotated with both human disagreement and crowdsourced confidence, we assess the correlation between model-predicted uncertainty and human-perceived uncertainty. Our results show that current methods only weakly align with human intuition, with correlations varying significantly across tasks and uncertainty metrics. Notably, we find that incorporating human-derived soft labels into the training process can improve calibration without compromising accuracy. These findings reveal a persistent gap between model and human uncertainty and highlight the potential of leveraging human insights to guide the development of more trustworthy AI systems.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept</title>
<link>https://arxiv.org/abs/2507.01080</link>
<guid>https://arxiv.org/abs/2507.01080</guid>
<content:encoded><![CDATA[
arXiv:2507.01080v2 Announce Type: replace 
Abstract: Emergency departments struggle with persistent triage errors, especially undertriage and overtriage, which are aggravated by growing patient volumes and staff shortages. This study evaluated three AI models [TRIAGEMASTER (NLP), URGENTIAPARSE (LLM), and EMERGINET (JEPA)] against the FRENCH triage scale and nurse practice, using seven months of adult triage data from Roger Salengro Hospital in Lille, France. Among the models, the LLM-based URGENTIAPARSE consistently outperformed both AI alternatives and nurse triage, achieving the highest accuracy (F1-score 0.900, AUC-ROC 0.879) and superior performance in predicting hospitalization needs (GEMSA). Its robustness across structured data and raw transcripts highlighted the advantage of LLM architectures in abstracting patient information. Overall, the findings suggest that integrating LLM-based AI into emergency department workflows could significantly enhance patient safety and operational efficiency, though successful adoption will depend on addressing limitations and ensuring ethical transparency.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.20999</link>
<guid>https://arxiv.org/abs/2507.20999</guid>
<content:encoded><![CDATA[
arXiv:2507.20999v2 Announce Type: replace 
Abstract: Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or layer-wise allocation rather than explicitly tailoring data and parameters to different response demands. Inspired by "Thinking, Fast and Slow," which characterizes two distinct modes of thought-System 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we draw an analogy that different "subregions" of an LLM's parameters might similarly specialize for tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework that partitions both data and parameters by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically, we classify task data via multi-model role-playing and voting, and partition parameters based on importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show that the two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or surpassing SOTA PEFT baselines.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA</title>
<link>https://arxiv.org/abs/2508.09146</link>
<guid>https://arxiv.org/abs/2508.09146</guid>
<content:encoded><![CDATA[
arXiv:2508.09146v4 Announce Type: replace 
Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still incurs poor throughput performance under dynamic channel environments. Recent model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply optimize backoff strategies under a known and fixed node density, still leading to a large throughput loss due to inaccurate node density estimation. This paper is the first to propose LLM transformer-based in-context learning (ICL) theory for optimizing channel access. We design a transformer-based ICL optimizer to pre-collect collision-threshold data examples and a query collision case. They are constructed as a prompt as the input for the transformer to learn the pattern, which then generates a predicted contention window threshold (CWT). To train the transformer for effective ICL, we develop an efficient algorithm and guarantee a near-optimal CWT prediction within limited training steps. As it may be hard to gather perfect data examples for ICL in practice, we further extend to allow erroneous data input in the prompt. We prove that our optimizer maintains minimal prediction and throughput deviations from the optimal values. Experimental results on NS-3 further demonstrate our approach's fast convergence and near-optimal throughput over existing model-based and DRL-based approaches under unknown node densities.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models</title>
<link>https://arxiv.org/abs/2508.19441</link>
<guid>https://arxiv.org/abs/2508.19441</guid>
<content:encoded><![CDATA[
arXiv:2508.19441v2 Announce Type: replace 
Abstract: Partial differential equations (PDEs) underpin the modeling of many natural and engineered systems. It can be convenient to express such models as neural PDEs rather than using traditional numerical PDE solvers by replacing part or all of the PDE's governing equations with a neural network representation. Neural PDEs are often easier to differentiate, linearize, reduce, or use for uncertainty quantification than the original numerical solver. They are usually trained on solution trajectories obtained by long time integration of the PDE solver. Here we propose a more sample-efficient data-augmentation strategy for generating neural PDE training data from a computer model by space-filling sampling of local "stencil" states. This approach removes a large degree of spatiotemporal redundancy present in trajectory data and oversamples states that may be rarely visited but help the neural PDE generalize across the state space. We demonstrate that accurate neural PDE stencil operators can be learned from synthetic training data generated by the computational equivalent of 10 timesteps' worth of numerical simulation. Accuracy is further improved if we assume access to a single full-trajectory simulation from the computer model, which is typically available in practice. Across several PDE systems, we show that our data-augmented synthetic stencil data yield better trained neural stencil operators, with clear performance gains compared with naively sampled stencil data from simulation trajectories.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DivMerge: A divergence-based model merging method for multi-tasking</title>
<link>https://arxiv.org/abs/2509.02108</link>
<guid>https://arxiv.org/abs/2509.02108</guid>
<content:encoded><![CDATA[
arXiv:2509.02108v2 Announce Type: replace 
Abstract: Multi-task learning (MTL) is often achieved by merging datasets before fine-tuning, but the growing availability of fine-tuned models has led to new approaches such as model merging via task arithmetic. A major challenge in this setting is task interference, which worsens as the number of tasks increases. We propose a method that merges models trained on different tasks into a single model, maintaining strong performance across all tasks. Our approach leverages Jensen-Shannon divergence to guide the merging process without requiring additional labelled data, and automatically balances task importance. Unlike existing methods, our approach remains robust as the number of tasks grows and consistently outperforms prior work.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Augmentation in Images using Language</title>
<link>https://arxiv.org/abs/2404.02353</link>
<guid>https://arxiv.org/abs/2404.02353</guid>
<content:encoded><![CDATA[
arXiv:2404.02353v3 Announce Type: replace-cross 
Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data</title>
<link>https://arxiv.org/abs/2405.14492</link>
<guid>https://arxiv.org/abs/2405.14492</guid>
<content:encoded><![CDATA[
arXiv:2405.14492v4 Announce Type: replace-cross 
Abstract: Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, full-scale approximations (FSAs) combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce computational costs in calculating likelihoods, gradients, and predictive distributions with FSAs. In particular, we introduce a novel preconditioner and show theoretically and empirically that it accelerates the conjugate gradient method's convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Furthermore, we introduce an accurate and fast way to calculate predictive variances using stochastic simulation and iterative methods. In addition, we show how our newly proposed FITC preconditioner can also be used in iterative methods for Vecchia approximations. In our experiments, it outperforms existing state-of-the-art preconditioners for Vecchia approximations. All methods are implemented in a free C++ software library with high-level Python and R packages.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepVoting: Learning and Fine-Tuning Voting Rules with Canonical Embeddings</title>
<link>https://arxiv.org/abs/2408.13630</link>
<guid>https://arxiv.org/abs/2408.13630</guid>
<content:encoded><![CDATA[
arXiv:2408.13630v2 Announce Type: replace-cross 
Abstract: Aggregating agent preferences into a collective decision is an important step in many problems (e.g., hiring, elections, peer review) and across areas of computer science (e.g., reinforcement learning, recommender systems). As Social Choice Theory has shown, the problem of designing aggregation rules with specific sets of properties (axioms) can be difficult, or provably impossible in some cases. Instead of designing algorithms by hand, one can learn aggregation rules, particularly voting rules, from data. However, prior work in this area has required extremely large models or been limited by the choice of preference representation, i.e., embedding. We recast the problem of designing voting rules with desirable properties into one of learning probabilistic functions that output distributions over a set of candidates. Specifically, we use neural networks to learn probabilistic social choice functions. Using standard embeddings from the social choice literature we show that preference profile encoding has significant impact on the efficiency and ability of neural networks to learn rules, allowing us to learn rules faster and with smaller networks than previous work. Moreover, we show that our learned rules can be fine-tuned using axiomatic properties to create novel voting rules and make them resistant to specific types of "attack". Namely, we fine-tune rules to resist a probabilistic version of the No Show Paradox.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sigma Flows for Image and Data Labeling and Learning Structured Prediction</title>
<link>https://arxiv.org/abs/2408.15946</link>
<guid>https://arxiv.org/abs/2408.15946</guid>
<content:encoded><![CDATA[
arXiv:2408.15946v2 Announce Type: replace-cross 
Abstract: This paper introduces the sigma flow model for the prediction of structured labelings of data observed on Riemannian manifolds, including Euclidean image domains as special case. The approach combines the Laplace-Beltrami framework for image denoising and enhancement, introduced by Sochen, Kimmel and Malladi about 25 years ago, and the assignment flow approach introduced and studied by the authors.
  The sigma flow arises as Riemannian gradient flow of generalized harmonic energies and thus is governed by a nonlinear geometric PDE which determines a harmonic map from a closed Riemannian domain manifold to a statistical manifold, equipped with the Fisher-Rao metric from information geometry. A specific ingredient of the sigma flow is the mutual dependency of the Riemannian metric of the domain manifold on the evolving state. This makes the approach amenable to machine learning in a specific way, by realizing this dependency through a mapping with compact time-variant parametrization that can be learned from data. Proof of concept experiments demonstrate the expressivity of the sigma flow model and prediction performance.
  Structural similarities to transformer network architectures and networks generated by the geometric integration of sigma flows are pointed out, which highlights the connection to deep learning and, conversely, may stimulate the use of geometric design principles for structured prediction in other areas of scientific machine learning.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Different Research Communities: Authorship Network</title>
<link>https://arxiv.org/abs/2409.00081</link>
<guid>https://arxiv.org/abs/2409.00081</guid>
<content:encoded><![CDATA[
arXiv:2409.00081v2 Announce Type: replace-cross 
Abstract: Google Scholar is one of the top search engines to access research articles across multiple disciplines for scholarly literature. Google scholar advance search option gives the privilege to extract articles based on phrases, publishers name, authors name, time duration etc. In this work, we collected Google Scholar data (2000-2021) for two different research domains in computer science: Data Mining and Software Engineering. The scholar database resources are powerful for network analysis, data mining, and identify links between authors via authorship network. We examined coauthor-ship network for each domain and studied their network structure. Extensive experiments are performed to analyze publications trend and identifying influential authors and affiliated organizations for each domain. The network analysis shows that the networks features are distinct from one another and exhibit small communities within the influential authors of a particular domain.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Average Causal Effect Estimation in DAGs with Hidden Variables: Beyond Back-Door and Front-Door Criteria</title>
<link>https://arxiv.org/abs/2409.03962</link>
<guid>https://arxiv.org/abs/2409.03962</guid>
<content:encoded><![CDATA[
arXiv:2409.03962v2 Announce Type: replace-cross 
Abstract: The identification theory for causal effects in directed acyclic graphs (DAGs) with hidden variables is well established, but methods for estimating and inferring functionals that extend beyond the g-formula remain underdeveloped. Previous studies have introduced semiparametric estimators for such functionals in a broad class of DAGs with hidden variables. While these estimators exhibit desirable statistical properties such as double robustness in certain cases, they also face significant limitations. Notably, they encounter substantial computational challenges, particularly involving density estimation and numerical integration for continuous variables, and their estimates may fall outside the parameter space of the target estimand. Additionally, the asymptotic properties of these estimators is underexplored, especially when integrating flexible statistical and machine learning models for nuisance functional estimations. This paper addresses these challenges by introducing novel one-step corrected plug-in and targeted minimum loss-based estimators of causal effects for a class of hidden variable DAGs that go beyond classical back-door and front-door criteria (known as the treatment primal fixability criterion in prior literature). These estimators leverage data-adaptive machine learning algorithms to minimize modeling assumptions while ensuring key statistical properties including double robustness, efficiency, boundedness within the target parameter space, and asymptotic linearity under $L^2(P)$-rate conditions for nuisance functional estimates that yield root-n consistent causal effect estimates. To ensure our estimation methods are accessible in practice, we provide the flexCausal package in R.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Neural Contractive Dynamical Systems: On Multiple Tasks and Riemannian Safety Regions</title>
<link>https://arxiv.org/abs/2411.11405</link>
<guid>https://arxiv.org/abs/2411.11405</guid>
<content:encoded><![CDATA[
arXiv:2411.11405v3 Announce Type: replace-cross 
Abstract: Stability guarantees are crucial when ensuring that a fully autonomous robot does not take undesirable or potentially harmful actions. We recently proposed the Neural Contractive Dynamical Systems (NCDS), which is a neural network architecture that guarantees contractive stability. With this, learning-from-demonstrations approaches can trivially provide stability guarantees. However, our early work left several unanswered questions, which we here address. Beyond providing an in-depth explanation of NCDS, this paper extends the framework with more careful regularization, a conditional variant of the framework for handling multiple tasks, and an uncertainty-driven approach to latent obstacle avoidance. Experiments verify that the developed system has the flexibility of ordinary neural networks while providing the stability guarantees needed for autonomous robotics.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics consistent machine learning framework for inverse modeling with applications to ICF capsule implosions</title>
<link>https://arxiv.org/abs/2412.20192</link>
<guid>https://arxiv.org/abs/2412.20192</guid>
<content:encoded><![CDATA[
arXiv:2412.20192v2 Announce Type: replace-cross 
Abstract: In high energy density physics (HEDP) and inertial confinement fusion (ICF), predictive modeling is complicated by uncertainty in parameters that characterize various aspects of the modeled system, such as those characterizing material properties, equation of state (EOS), opacities, and initial conditions. Typically, however, these parameters are not directly observable. What is observed instead is a time sequence of radiographic projections using X-rays. In this work, we define a set of sparse hydrodynamic features derived from the outgoing shock profile and outer material edge, which can be obtained from radiographic measurements, to directly infer such parameters. Our machine learning (ML)-based methodology involves a pipeline of two architectures, a radiograph-to-features network (R2FNet) and a features-to-parameters network (F2PNet), that are trained independently and later combined to approximate a posterior distribution for the parameters from radiographs. We show that the estimated parameters can be used in a hydrodynamics code to obtain density fields and hydrodynamic shock and outer edge features that are consistent with the data. Finally, we demonstrate that features resulting from an unknown EOS model can be successfully mapped onto parameters of a chosen analytical EOS model, implying that network predictions are learning physics, with a degree of invariance to the underlying choice of EOS model.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination</title>
<link>https://arxiv.org/abs/2501.06058</link>
<guid>https://arxiv.org/abs/2501.06058</guid>
<content:encoded><![CDATA[
arXiv:2501.06058v5 Announce Type: replace-cross 
Abstract: Recent advances have enabled heterogeneous multi-robot teams to learn complex and effective coordination skills. However, existing neural architectures that support heterogeneous teaming tend to force a trade-off between expressivity and efficiency. Shared-parameter designs prioritize sample efficiency by enabling a single network to be shared across all or a pre-specified subset of robots (via input augmentations), but tend to limit behavioral diversity. In contrast, recent designs employ a separate policy for each robot, enabling greater diversity and expressivity at the cost of efficiency and generalization. Our key insight is that such tradeoffs can be avoided by viewing these design choices as ends of a broad spectrum. Inspired by recent work in transfer and meta learning, and building on prior work in multi-robot task allocation, we propose Capability-Aware Shared Hypernetworks (CASH), a soft weight sharing architecture that uses hypernetworks to efficiently learn a flexible shared policy that dynamically adapts to each robot post-training. By explicitly encoding the impact of robot capabilities (e.g., speed and payload) on collective behavior, CASH enables zero-shot generalization to unseen robots or team compositions. Our experiments involve multiple heterogeneous tasks, three learning paradigms (imitation learning, value-based, and policy-gradient RL), and SOTA multi-robot simulation (JaxMARL) and hardware (Robotarium) platforms. Across all conditions, we find that CASH generates appropriately-diverse behaviors and consistently outperforms baseline architectures in terms of performance and sample efficiency during both training and zero-shot generalization, all with 60%-80% fewer learnable parameters.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models</title>
<link>https://arxiv.org/abs/2502.02787</link>
<guid>https://arxiv.org/abs/2502.02787</guid>
<content:encoded><![CDATA[
arXiv:2502.02787v2 Announce Type: replace-cross 
Abstract: The widespread adoption of large language models (LLMs) necessitates reliable methods to detect LLM-generated text. We introduce SimMark, a robust sentence-level watermarking algorithm that makes LLMs' outputs traceable without requiring access to model internals, making it compatible with both open and API-based LLMs. By leveraging the similarity of semantic sentence embeddings combined with rejection sampling to embed detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while maintaining the text quality and fluency.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds</title>
<link>https://arxiv.org/abs/2502.05857</link>
<guid>https://arxiv.org/abs/2502.05857</guid>
<content:encoded><![CDATA[
arXiv:2502.05857v3 Announce Type: replace-cross 
Abstract: Learning an agent model that behaves like humans-capable of jointly perceiving the environment, predicting the future, and taking actions from a first-person perspective-is a fundamental challenge in computer vision. Existing methods typically train separate models for these abilities, which fail to capture their intrinsic relationships and prevent them from learning from each other. Inspired by how humans learn through the perception-action loop, we propose EgoAgent, a unified agent model that simultaneously learns to represent, predict, and act within a single transformer. EgoAgent explicitly models the causal and temporal dependencies among these abilities by formulating the task as an interleaved sequence of states and actions. It further introduces a joint embedding-action-prediction architecture with temporally asymmetric predictor and observer branches, enabling synergistic optimization across all three capabilities. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction demonstrate the superiority of our method. The code and trained models will be publicly available at https://github.com/zju3dv/EgoAgent.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions</title>
<link>https://arxiv.org/abs/2503.16505</link>
<guid>https://arxiv.org/abs/2503.16505</guid>
<content:encoded><![CDATA[
arXiv:2503.16505v3 Announce Type: replace-cross 
Abstract: Limited large-scale evaluations exist for facilitation strategies of online discussions due to significant costs associated with human involvement. An effective solution is synthetic discussion simulations using Large Language Models (LLMs) to create initial pilot experiments. We propose design principles based on existing methodologies for synthetic discussion generation. Based on these principles, we propose a simple, generalizable, LLM-driven methodology to prototype the development of LLM facilitators by generating synthetic data without human involvement, and which surpasses current baselines. We use our methodology to test whether current Social Science strategies for facilitation can improve the performance of LLM facilitators. We find that, while LLM facilitators significantly improve synthetic discussions, there is no evidence that the application of these strategies leads to further improvements in discussion quality. In an effort to aid research in the field of facilitation, we release a large, publicly available dataset containing LLM-generated and LLM-annotated discussions using multiple open-source models. This dataset can be used for LLM facilitator finetuning as well as behavioral analysis of current out-of-the-box LLMs in the task. We also release an open-source python framework that efficiently implements our methodology at great scale.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems</title>
<link>https://arxiv.org/abs/2503.20507</link>
<guid>https://arxiv.org/abs/2503.20507</guid>
<content:encoded><![CDATA[
arXiv:2503.20507v3 Announce Type: replace-cross 
Abstract: Hybrid storage systems (HSS) integrate multiple storage devices with diverse characteristics to deliver high performance and capacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which dynamically rearranges stored data (i.e., prefetches hot data and evicts cold data) across the devices to sustain high HSS performance. Prior works optimize either data placement or data migration in isolation, which leads to suboptimal HSS performance. Unfortunately, no prior work tries to optimize both policies together.
  Our goal is to design a holistic data-management technique that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS, and thus significantly improve system performance. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data-management technique that employs two lightweight autonomous RL agents, a data-placement agent and a data-migration agent, that adapt their policies for the current workload and HSS configuration while coordinating with each other to improve overall HSS performance.
  We evaluate Harmonia on real HSS configurations with up to four heterogeneous storage devices and seventeen data-intensive workloads. On performance-optimized (cost-optimized) HSS with two storage devices, Harmonia outperforms the best-performing prior approach by 49.5% (31.7%) on average. On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%) on average. Harmonia's performance benefits come with low latency (240ns for inference) and storage overheads (206 KiB in DRAM for both RL agents combined). We will open-source Harmonia's implementation to aid future research on HSS.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWI: Speaking with Intent in Large Language Models</title>
<link>https://arxiv.org/abs/2503.21544</link>
<guid>https://arxiv.org/abs/2503.21544</guid>
<content:encoded><![CDATA[
arXiv:2503.21544v3 Announce Type: replace-cross 
Abstract: Intent, typically clearly formulated and planned, functions as a cognitive framework for communication and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and action. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on text summarization, multi-task question answering, and mathematical reasoning benchmarks consistently demonstrate the effectiveness and generalizability of Speaking with Intent over direct generation without explicit intent. Further analysis corroborates the generalizability of SWI under different experimental settings. Moreover, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. The promising results in enhancing LLMs with explicit intents pave a new avenue for boosting LLMs' generation and reasoning abilities with cognitive notions.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Assisted Machine Learning Models for Enhanced Weather Prediction</title>
<link>https://arxiv.org/abs/2503.23408</link>
<guid>https://arxiv.org/abs/2503.23408</guid>
<content:encoded><![CDATA[
arXiv:2503.23408v2 Announce Type: replace-cross 
Abstract: Quantum Machine Learning (QML) presents as a revolutionary approach to weather forecasting by using quantum computing to improve predictive modeling capabilities. In this study, we apply QML models, including Quantum Gated Recurrent Units (QGRUs), Quantum Neural Networks (QNNs), Quantum Long Short-Term Memory(QLSTM), Variational Quantum Circuits(VQCs), and Quantum Support Vector Machines(QSVMs), to analyze meteorological time-series data from the ERA5 dataset. Our methodology includes preprocessing meteorological features, implementing QML architectures for both classification and regression tasks. The results demonstrate that QML models can achieve reasonable accuracy in both prediction and classification tasks, particularly in binary classification. However, challenges such as quantum hardware limitations and noise affect scalability and generalization. This research provides insights into the feasibility of QML for weather prediction, paving the way for further exploration of hybrid quantum-classical frameworks to enhance meteorological forecasting.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B</title>
<link>https://arxiv.org/abs/2504.00132</link>
<guid>https://arxiv.org/abs/2504.00132</guid>
<content:encoded><![CDATA[
arXiv:2504.00132v3 Announce Type: replace-cross 
Abstract: In-Context Learning (ICL) is an intriguing ability of large language models (LLMs). Despite a substantial amount of work on its behavioral aspects and how it emerges in miniature setups, it remains unclear which mechanism assembles task information from the individual examples in a fewshot prompt. We use causal interventions to identify information flow in Gemma-2 2B for five naturalistic ICL tasks. We find that the model infers task information using a two-step strategy we call contextualize-then-aggregate: In the lower layers, the model builds up representations of individual fewshot examples, which are contextualized by preceding examples through connections between fewshot input and output tokens across the sequence. In the higher layers, these representations are aggregated to identify the task and prepare prediction of the next output. The importance of the contextualization step differs between tasks, and it may become more important in the presence of ambiguous examples. Overall, by providing rigorous causal analysis, our results shed light on the mechanisms through which ICL happens in language models.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACE: A Security Architecture for LLM-Integrated App Systems</title>
<link>https://arxiv.org/abs/2504.20984</link>
<guid>https://arxiv.org/abs/2504.20984</guid>
<content:encoded><![CDATA[
arXiv:2504.20984v3 Announce Type: replace-cross 
Abstract: LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution.
  In this work, we identify new attacks impacting the integrity of planning, as well as the integrity and availability of execution in LLM-integrated apps, and demonstrate them against IsolateGPT, a recent solution designed to mitigate attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new secure architecture for LLM-integrated app systems that provides security guarantees for system planning and execution. Specifically, ACE decouples planning into two phases by first creating an abstract execution plan using only trusted information, and then mapping the abstract plan to a concrete plan using installed system apps. We verify that the plans generated by our system satisfy user-specified secure information flow constraints via static analysis on the structured plan output. During execution, ACE enforces data and capability barriers between apps, and ensures that the execution is conducted according to the trusted abstract plan. We show experimentally that ACE is secure against attacks from the InjecAgent and Agent Security Bench benchmarks for indirect prompt injection, and our newly introduced attacks. We also evaluate the utility of ACE in realistic environments, using the Tool Usage suite from the LangChain benchmark. Our architecture represents a significant advancement towards hardening LLM-based systems using system security principles.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07815</link>
<guid>https://arxiv.org/abs/2505.07815</guid>
<content:encoded><![CDATA[
arXiv:2505.07815v3 Announce Type: replace-cross 
Abstract: Exploration is essential for general-purpose robotic learning, especially in open-ended environments where dense rewards, explicit goals, or task-specific supervision are scarce. Vision-language models (VLMs), with their semantic reasoning over objects, spatial relations, and potential outcomes, present a compelling foundation for generating high-level exploratory behaviors. However, their outputs are often ungrounded, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity. Human exploration is often driven by the desire to discover novel scene configurations and to deepen understanding of the environment. Similarly, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE enables more diverse and meaningful exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the entropy of visited states. Moreover, the collected experience supports downstream learning, producing policies that closely match or exceed the performance of those trained on human-collected demonstrations.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Optimizing Machine Learning Potential Assisted Automated Workflow for Highly Efficient Complex Systems Material Design</title>
<link>https://arxiv.org/abs/2505.08159</link>
<guid>https://arxiv.org/abs/2505.08159</guid>
<content:encoded><![CDATA[
arXiv:2505.08159v2 Announce Type: replace-cross 
Abstract: Machine learning interatomic potentials have revolutionized complex materials design by enabling rapid exploration of material configurational spaces via crystal structure prediction with ab initio accuracy. However, critical challenges persist in ensuring robust generalization to unknown structures and minimizing the requirement for substantial expert knowledge and time-consuming manual interventions. Here, we propose an automated crystal structure prediction framework built upon the attention-coupled neural networks potential to address these limitations. The generalizability of the potential is achieved by sampling regions across the local minima of the potential energy surface, where the self-evolving pipeline autonomously refines the potential iteratively while minimizing human intervention. The workflow is validated on Mg-Ca-H ternary and Be-P-N-O quaternary systems by exploring nearly 10 million configurations, demonstrating substantial speedup compared to first-principles calculations. These results underscore the effectiveness of our approach in accelerating the exploration and discovery of complex multi-component functional materials.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring entropy production in many-body systems using nonequilibrium MaxEnt</title>
<link>https://arxiv.org/abs/2505.10444</link>
<guid>https://arxiv.org/abs/2505.10444</guid>
<content:encoded><![CDATA[
arXiv:2505.10444v3 Announce Type: replace-cross 
Abstract: We propose a method for inferring entropy production (EP) in high-dimensional stochastic systems, including many-body systems and non-Markovian systems with long memory. Standard techniques for estimating EP become intractable in such systems due to computational and statistical limitations. We infer trajectory-level EP and lower bounds on average EP by exploiting a nonequilibrium analogue of the Maximum Entropy principle, along with convex duality. Our approach uses only samples of trajectory observables, such as spatiotemporal correlations. It does not require reconstruction of high-dimensional probability distributions or rate matrices, nor impose any special assumptions such as discrete states or multipartite dynamics. In addition, it may be used to compute a hierarchical decomposition of EP, reflecting contributions from different interaction orders, and it has an intuitive physical interpretation as a "thermodynamic uncertainty relation." We demonstrate its numerical performance on a disordered nonequilibrium spin model with 1000 spins and a large neural spike-train dataset.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Jump Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.15557</link>
<guid>https://arxiv.org/abs/2505.15557</guid>
<content:encoded><![CDATA[
arXiv:2505.15557v2 Announce Type: replace-cross 
Abstract: Gaussian processes (GPs) furnish accurate nonlinear predictions with well-calibrated uncertainty. However, the typical GP setup has a built-in stationarity assumption, making it ill-suited for modeling data from processes with sudden changes, or "jumps" in the output variable. The "jump GP" (JGP) was developed for modeling data from such processes, combining local GPs and latent "level" variables under a joint inferential framework. But joint modeling can be fraught with difficulty. We aim to simplify by suggesting a more modular setup, eschewing joint inference but retaining the main JGP themes: (a) learning optimal neighborhood sizes that locally respect manifolds of discontinuity; and (b) a new cluster-based (latent) feature to capture regions of distinct output levels on both sides of the manifold. We show that each of (a) and (b) separately leads to dramatic improvements when modeling processes with jumps. In tandem (but without requiring joint inference) that benefit is compounded, as illustrated on real and synthetic benchmark examples from the recent literature.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics</title>
<link>https://arxiv.org/abs/2505.19317</link>
<guid>https://arxiv.org/abs/2505.19317</guid>
<content:encoded><![CDATA[
arXiv:2505.19317v4 Announce Type: replace-cross 
Abstract: Although popularized AI fairness metrics, e.g., demographic parity, have uncovered bias in AI-assisted decision-making outcomes, they do not consider how much effort one has spent to get to where one is today in the input feature space. However, the notion of effort is important in how Philosophy and humans understand fairness. We propose a philosophy-informed approach to conceptualize and evaluate Effort-aware Fairness (EaF), grounded in the concept of Force, which represents the temporal trajectory of predictive features coupled with inertia. Besides theoretical formulation, our empirical contributions include: (1) a pre-registered human subjects experiment, which shows that for both stages of the (individual) fairness evaluation process, people consider the temporal trajectory of a predictive feature more than its aggregate value; (2) pipelines to compute Effort-aware Individual/Group Fairness in the criminal justice and personal finance contexts. Our work may enable AI model auditors to uncover and potentially correct unfair decisions against individuals who have spent significant efforts to improve but are still stuck with systemic disadvantages outside their control.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.19455</link>
<guid>https://arxiv.org/abs/2505.19455</guid>
<content:encoded><![CDATA[
arXiv:2505.19455v2 Announce Type: replace-cross 
Abstract: Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs) has achieved promising progress by leveraging prompt tuning to enable continual multi-modal learning. However, most existing methods adopt cross-modal prompt isolation, constructing visual and textual prompts separately, which exacerbates modality imbalance and leads to degraded performance over time. To tackle this issue, we propose MM-Prompt, a novel framework incorporating cross-modal prompt query and cross-modal prompt recovery. The former enables balanced prompt selection by incorporating cross-modal signals during query formation, while the latter promotes joint prompt reconstruction through iterative cross-modal interactions, guided by an alignment loss to prevent representational drift. Extensive experiments show that MM-Prompt surpasses prior approaches in accuracy and knowledge retention, while maintaining balanced modality engagement throughout continual learning.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Optimization Accelerator Framework for Multistate Ising Problems</title>
<link>https://arxiv.org/abs/2505.20250</link>
<guid>https://arxiv.org/abs/2505.20250</guid>
<content:encoded><![CDATA[
arXiv:2505.20250v2 Announce Type: replace-cross 
Abstract: Ising Machines are emerging hardware architectures that efficiently solve NP-Hard combinatorial optimization problems. Generally, combinatorial problems are transformed into quadratic unconstrained binary optimization (QUBO) form, but this transformation often complicates the solution landscape, degrading performance, especially for multi-state problems. To address this challenge, we model spin interactions as generalized boolean logic function to significantly reduce the exploration space. We demonstrate the effectiveness of our approach on graph coloring problem using probabilistic Ising solvers, achieving similar accuracy compared to state-of-the-art heuristics and machine learning algorithms. It also shows significant improvement over state-of-the-art QUBO-based Ising solvers, including probabilistic Ising and simulated bifurcation machines. We also design 1024-neuron all-to-all connected probabilistic Ising accelerator on FPGA with the proposed approach that shows ~10000x performance acceleration compared to GPU-based Tabucol heuristics and reducing physical neurons by 1.5-4x over baseline Ising frameworks. Thus, this work establishes superior efficiency, scalability and solution quality for multi-state optimization problems.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Graph Neural Networks for Robustness in Olfaction Sensors and Datasets</title>
<link>https://arxiv.org/abs/2506.00455</link>
<guid>https://arxiv.org/abs/2506.00455</guid>
<content:encoded><![CDATA[
arXiv:2506.00455v3 Announce Type: replace-cross 
Abstract: Robotic odour source localization (OSL) is a critical capability for autonomous systems operating in complex environments. However, current OSL methods often suffer from ambiguities, particularly when robots misattribute odours to incorrect objects due to limitations in olfactory datasets and sensor resolutions. To address this challenge, we introduce a novel machine learning method using diffusion-based molecular generation to enhance odour localization accuracy that can be used by itself or with automated olfactory dataset construction pipelines. This generative process of our diffusion model expands the chemical space beyond the limitations of both current olfactory datasets and training methods, enabling the identification of potential odourant molecules not previously documented. The generated molecules can then be more accurately validated using advanced olfactory sensors, enabling them to detect more compounds and inform better hardware design. By integrating visual analysis, language processing, and molecular generation, our framework enhances the ability of olfaction-vision models on robots to accurately associate odours with their correct sources, thereby improving navigation and decision-making through better sensor selection for a target compound in critical applications such as explosives detection, narcotics screening, and search and rescue. Our methodology represents a foundational advancement in the field of artificial olfaction, offering a scalable solution to challenges posed by limited olfactory data and sensor ambiguities. Code and data are made available to the community at the following URL: https://github.com/KordelFranceTech/OlfactionVisionLanguage-Dataset.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs for sensory-motor control: Combining in-context and iterative learning</title>
<link>https://arxiv.org/abs/2506.04867</link>
<guid>https://arxiv.org/abs/2506.04867</guid>
<content:encoded><![CDATA[
arXiv:2506.04867v2 Announce Type: replace-cross 
Abstract: We propose a method that enables large language models (LLMs) to control embodied agents by directly mapping continuous observation vectors to continuous action vectors. At the outset, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. The approach proves effective with relatively compact models such as Gpt-oss:120b and Qwen2.5:72b. In most cases, it successfully identifies optimal or near-optimal solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A User-Centric, Privacy-Preserving, and Verifiable Ecosystem for Personal Data Management and Utilization</title>
<link>https://arxiv.org/abs/2506.22606</link>
<guid>https://arxiv.org/abs/2506.22606</guid>
<content:encoded><![CDATA[
arXiv:2506.22606v2 Announce Type: replace-cross 
Abstract: In the current paradigm of digital personalized services, the centralized management of personal data raises significant privacy concerns, security vulnerabilities, and diminished individual autonomy over sensitive information. Despite their efficiency, traditional centralized architectures frequently fail to satisfy rigorous privacy requirements and expose users to data breaches and unauthorized access risks. This pressing challenge calls for a fundamental paradigm shift in methodologies for collecting, storing, and utilizing personal data across diverse sectors, including education, healthcare, and finance.
  This paper introduces a novel decentralized, privacy-preserving architecture that handles heterogeneous personal information, ranging from educational credentials to health records and financial data. Unlike traditional models, our system grants users complete data ownership and control, allowing them to selectively share information without compromising privacy. The architecture's foundation comprises advanced privacy-enhancing technologies, including secure enclaves and federated learning, enabling secure computation, verification, and data sharing. The system supports diverse functionalities, including local computation, model training, and privacy-preserving data sharing, while ensuring data credibility and robust user privacy.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound</title>
<link>https://arxiv.org/abs/2506.23538</link>
<guid>https://arxiv.org/abs/2506.23538</guid>
<content:encoded><![CDATA[
arXiv:2506.23538v2 Announce Type: replace-cross 
Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage, preterm birth, and an increased risk of pregnancy complications. Compared to traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane, providing a clear visualization of the uterine morphology for assessing CUAs accurately. In this paper, we propose an intelligent system for simultaneous automated plane localization and CUA diagnosis. Our highlights are: 1) we develop a denoising diffusion model with local (plane) and global (volume/text) guidance, using an adaptive weighting strategy to optimize attention allocation to different conditions; 2) we introduce a reinforcement learning-based framework with unsupervised rewards to extract the key slice summary from redundant sequences, fully integrating information across multiple planes to reduce learning difficulty; 3) we provide text-driven uncertainty modeling for coarse prediction, and leverage it to adjust the classification probability for overall performance improvement. Extensive experiments on a large 3D uterine US dataset show the efficacy of our method, in terms of plane localization and CUA diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Euclidean Distance Deflation Under High-Dimensional Heteroskedastic Noise</title>
<link>https://arxiv.org/abs/2507.18520</link>
<guid>https://arxiv.org/abs/2507.18520</guid>
<content:encoded><![CDATA[
arXiv:2507.18520v2 Announce Type: replace-cross 
Abstract: Pairwise Euclidean distance calculation is a fundamental step in many machine learning and data analysis algorithms. In real-world applications, however, these distances are frequently distorted by heteroskedastic noise$\unicode{x2014}$a prevalent form of inhomogeneous corruption characterized by variable noise magnitudes across data observations. Such noise inflates the computed distances in a nontrivial way, leading to misrepresentations of the underlying data geometry. In this work, we address the tasks of estimating the noise magnitudes per observation and correcting the pairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly, we show that in general high-dimensional settings and without assuming prior knowledge on the clean data structure or noise distribution, both tasks can be performed reliably, even when the noise levels vary considerably. Specifically, we develop a principled, hyperparameter-free approach that jointly estimates the noise magnitudes and corrects the distances. We provide theoretical guarantees for our approach, establishing probabilistic bounds on the estimation errors of both noise magnitudes and distances. These bounds, measured in the normalized $\ell_1$ norm, converge to zero at polynomial rates as both feature dimension and dataset size increase. Experiments on synthetic datasets demonstrate that our method accurately estimates distances in challenging regimes, significantly improving the robustness of subsequent distance-based computations. Notably, when applied to single-cell RNA sequencing data, our method yields noise magnitude estimates consistent with an established prototypical model, enabling accurate nearest neighbor identification that is fundamental to many downstream analyses.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2507.23682</link>
<guid>https://arxiv.org/abs/2507.23682</guid>
<content:encoded><![CDATA[
arXiv:2507.23682v2 Announce Type: replace-cross 
Abstract: Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uniform convergence for Gaussian kernel ridge regression</title>
<link>https://arxiv.org/abs/2508.11274</link>
<guid>https://arxiv.org/abs/2508.11274</guid>
<content:encoded><![CDATA[
arXiv:2508.11274v2 Announce Type: replace-cross 
Abstract: This paper establishes the first polynomial convergence rates for Gaussian kernel ridge regression (KRR) with a fixed hyperparameter in both the uniform and the $L^{2}$-norm. The uniform convergence result closes a gap in the theoretical understanding of KRR with the Gaussian kernel, where no such rates were previously known. In addition, we prove a polynomial $L^{2}$-convergence rate in the case, where the Gaussian kernel's width parameter is fixed. This also contributes to the broader understanding of smooth kernels, for which previously only sub-polynomial $L^{2}$-rates were known in similar settings. Together, these results provide new theoretical justification for the use of Gaussian KRR with fixed hyperparameters in nonparametric regression.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Information Dynamics of Generative Diffusion</title>
<link>https://arxiv.org/abs/2508.19897</link>
<guid>https://arxiv.org/abs/2508.19897</guid>
<content:encoded><![CDATA[
arXiv:2508.19897v3 Announce Type: replace-cross 
Abstract: Generative diffusion models have emerged as a powerful class of models in machine learning, yet a unified theoretical understanding of their operation is still developing. This paper provides an integrated perspective on generative diffusion by connecting their dynamic, information-theoretic, and thermodynamic properties under a unified mathematical framework. We demonstrate that the rate of conditional entropy production during generation (i.e. the generative bandwidth) is directly governed by the expected divergence of the score function's vector field. This divergence, in turn, is linked to the branching of trajectories and generative bifurcations, which we characterize as symmetry-breaking phase transitions in the energy landscape. This synthesis offers a powerful insight: the process of generation is fundamentally driven by the controlled, noise-induced breaking of (approximate) symmetries, where peaks in information transfer correspond to critical transitions between possible outcomes. The score function acts as a dynamic non-linear filter that regulates the bandwidth of the noise by suppressing fluctuations that are incompatible with the data.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurGBSA: Learning Representations From Molecular Dynamics Simulations</title>
<link>https://arxiv.org/abs/2509.03084</link>
<guid>https://arxiv.org/abs/2509.03084</guid>
<content:encoded><![CDATA[
arXiv:2509.03084v2 Announce Type: replace-cross 
Abstract: Self-supervised pretraining from static structures of drug-like compounds and proteins enable powerful learned feature representations. Learned features demonstrate state of the art performance on a range of predictive tasks including molecular properties, structure generation, and protein-ligand interactions. The majority of approaches are limited by their use of static structures and it remains an open question, how best to use atomistic molecular dynamics (MD) simulations to develop more generalized models to improve prediction accuracy for novel molecular structures. We present SURrogate mmGBSA (SurGBSA) as a new modeling approach for MD-based representation learning, which learns a surrogate function of the Molecular Mechanics Generalized Born Surface Area (MMGBSA). We show for the first time the benefits of physics-informed pre-training to train a surrogate MMGBSA model on a collection of over 1.4 million 3D trajectories collected from MD simulations of the CASF-2016 benchmark. SurGBSA demonstrates a dramatic 27,927x speedup versus a traditional physics-based single-point MMGBSA calculation while nearly matching single-point MMGBSA accuracy on the challenging pose ranking problem for identification of the correct top pose (-0.4% difference). Our work advances the development of molecular foundation models by showing model improvements when training on MD simulations. Models, code and training data are made publicly available.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Guide to Differential Privacy: From Theory to User Expectations</title>
<link>https://arxiv.org/abs/2509.03294</link>
<guid>https://arxiv.org/abs/2509.03294</guid>
<content:encoded><![CDATA[
arXiv:2509.03294v2 Announce Type: replace-cross 
Abstract: The increasing availability of personal data has enabled significant advances in fields such as machine learning, healthcare, and cybersecurity. However, this data abundance also raises serious privacy concerns, especially in light of powerful re-identification attacks and growing legal and ethical demands for responsible data use. Differential privacy (DP) has emerged as a principled, mathematically grounded framework for mitigating these risks. This review provides a comprehensive survey of DP, covering its theoretical foundations, practical mechanisms, and real-world applications. It explores key algorithmic tools and domain-specific challenges - particularly in privacy-preserving machine learning and synthetic data generation. The report also highlights usability issues and the need for improved communication and transparency in DP systems. Overall, the goal is to support informed adoption of DP by researchers and practitioners navigating the evolving landscape of data privacy.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization</title>
<link>https://arxiv.org/abs/2509.07993</link>
<guid>https://arxiv.org/abs/2509.07993</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake detection, continual learning, visual manipulation, real-time performance, non-universal deepfake distribution hypothesis

Summary: 
The article presents a novel approach to deepfake detection by reframing it as a continual learning problem. The proposed framework efficiently adapts to emerging deepfake technologies while retaining knowledge of past generators. By simulating the chronological evolution of deepfake technologies over a 7-year period, the framework aims to provide real-world applicable solutions. Two novel metrics, Continual AUC (C-AUC) and Forward Transfer AUC (FWT-AUC), are introduced to evaluate historical performance and future generalization, respectively. The framework showcases efficient adaptation and retention of knowledge, significantly faster than full retraining methods. However, the study also reveals challenges in generalizing current approaches to future generators due to unique characteristics of each generator. The findings lead to the development of the Non-Universal Deepfake Distribution Hypothesis. The code for the framework will be released upon acceptance of the paper. 

<br /><br />Summary: <div>
arXiv:2509.07993v1 Announce Type: new 
Abstract: The rapid evolution of deepfake generation technologies poses critical challenges for detection systems, as non-continual learning methods demand frequent and expensive retraining. We reframe deepfake detection (DFD) as a Continual Learning (CL) problem, proposing an efficient framework that incrementally adapts to emerging visual manipulation techniques while retaining knowledge of past generators. Our framework, unlike prior approaches that rely on unreal simulation sequences, simulates the real-world chronological evolution of deepfake technologies in extended periods across 7 years. Simultaneously, our framework builds upon lightweight visual backbones to allow for the real-time performance of DFD systems. Additionally, we contribute two novel metrics: Continual AUC (C-AUC) for historical performance and Forward Transfer AUC (FWT-AUC) for future generalization. Through extensive experimentation (over 600 simulations), we empirically demonstrate that while efficient adaptation (+155 times faster than full retraining) and robust retention of historical knowledge is possible, the generalization of current approaches to future generators without additional training remains near-random (FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing generator. Such observations are the foundation of our newly proposed Non-Universal Deepfake Distribution Hypothesis.
  \textbf{Code will be released upon acceptance.}
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Far Are We from True Unlearnability?</title>
<link>https://arxiv.org/abs/2509.08058</link>
<guid>https://arxiv.org/abs/2509.08058</guid>
<content:encoded><![CDATA[
<div> data, unlearnable methods, model optimization, loss landscape, Sharpness-Aware Learnability<br />
Summary:<br />
The article discusses the importance of high-quality data in training large models and the risks of using unauthorized data. It explores the concept of unlearnable examples (UEs) generated by unlearnable methods to disrupt training availability. Surprisingly, UEs perform well in certain tasks like on the Taskonomy dataset, questioning the true unlearnability of examples. The article delves into model optimization to understand the convergence differences between clean and poisoned models, identifying critical parameter optimization paths in the loss landscape. It introduces Sharpness-Aware Learnability (SAL) to quantify parameter unlearnability and proposes the Unlearnable Distance (UD) to measure data unlearnability based on SAL distribution. Benchmark tests on existing unlearnable methods using UD showcase the limitations of current unlearnable methods, encouraging awareness within the research community. <div>
arXiv:2509.08058v1 Announce Type: new 
Abstract: High-quality data plays an indispensable role in the era of large models, but the use of unauthorized data for model training greatly damages the interests of data owners. To overcome this threat, several unlearnable methods have been proposed, which generate unlearnable examples (UEs) by compromising the training availability of data. Clearly, due to unknown training purposes and the powerful representation learning capabilities of existing models, these data are expected to be unlearnable for models across multiple tasks, i.e., they will not help improve the model's performance. However, unexpectedly, we find that on the multi-task dataset Taskonomy, UEs still perform well in tasks such as semantic segmentation, failing to exhibit cross-task unlearnability. This phenomenon leads us to question: How far are we from attaining truly unlearnable examples? We attempt to answer this question from the perspective of model optimization. To this end, we observe the difference in the convergence process between clean and poisoned models using a simple model architecture. Subsequently, from the loss landscape we find that only a part of the critical parameter optimization paths show significant differences, implying a close relationship between the loss landscape and unlearnability. Consequently, we employ the loss landscape to explain the underlying reasons for UEs and propose Sharpness-Aware Learnability (SAL) to quantify the unlearnability of parameters based on this explanation. Furthermore, we propose an Unlearnable Distance (UD) to measure the unlearnability of data based on the SAL distribution of parameters in clean and poisoned models. Finally, we conduct benchmark tests on mainstream unlearnable methods using the proposed UD, aiming to promote community awareness of the capability boundaries of existing unlearnable methods.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEL: A Novel Model Linking Knowledge Graph entities to News Mentions</title>
<link>https://arxiv.org/abs/2509.08086</link>
<guid>https://arxiv.org/abs/2509.08086</guid>
<content:encoded><![CDATA[
<div> neural network, entity linking, Knowledge Graphs, natural language processing, News Analytics
Summary:
JEL is a novel neural network-based model for entity linking that surpasses current state-of-the-art models. Entity linking is crucial in connecting mentions in textual sources to entities in knowledge graphs, benefiting various applications such as News Analytics. JPMorgan utilizes news analytics solutions for multiple teams within the firm, with over $2 million spent annually on external vendor costs. By linking unstructured news text with knowledge graphs, entity linking simplifies access to curated data, enhancing users' daily tasks. <div>
arXiv:2509.08086v1 Announce Type: new 
Abstract: We present JEL, a novel computationally efficient end-to-end multi-neural network based entity linking model, which beats current state-of-art model. Knowledge Graphs have emerged as a compelling abstraction for capturing critical relationships among the entities of interest and integrating data from multiple heterogeneous sources. A core problem in leveraging a knowledge graph is linking its entities to the mentions (e.g., people, company names) that are encountered in textual sources (e.g., news, blogs., etc) correctly, since there are thousands of entities to consider for each mention. This task of linking mentions and entities is referred as Entity Linking (EL). It is a fundamental task in natural language processing and is beneficial in various uses cases, such as building a New Analytics platform. News Analytics, in JPMorgan, is an essential task that benefits multiple groups across the firm. According to a survey conducted by the Innovation Digital team 1 , around 25 teams across the firm are actively looking for news analytics solutions, and more than \$2 million is being spent annually on external vendor costs. Entity linking is critical for bridging unstructured news text with knowledge graphs, enabling users access to vast amounts of curated data in a knowledge graph and dramatically facilitating their daily work.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Assessment Strategies for Generative AI Applications in Healthcare</title>
<link>https://arxiv.org/abs/2509.08087</link>
<guid>https://arxiv.org/abs/2509.08087</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative artificial intelligence, medical enterprise, evaluation strategies, performance assessment, healthcare. 

Summary: 
Generative artificial intelligence (GenAI) is a burgeoning field within artificial intelligence that finds applications in the medical domain. Assessing the performance of GenAI applications in healthcare requires a thorough understanding of the clinical task and the variability in performance in real-world clinical settings. Current evaluation methods often rely on quantitative benchmarks, which may suffer from limitations such as train-to-the-test overfitting. To address these challenges, there is growing interest in evaluation strategies that leverage human expertise and cost-effective computational models as evaluators. This shift towards more holistic evaluation approaches aims to improve the generalizability of results across various task and data distributions. In this article, the authors discuss state-of-the-art methodologies for evaluating the performance of GenAI applications in healthcare and medical devices. <div>
arXiv:2509.08087v1 Announce Type: new 
Abstract: Generative artificial intelligence (GenAI) represent an emerging paradigm within artificial intelligence, with applications throughout the medical enterprise. Assessing GenAI applications necessitates a comprehensive understanding of the clinical task and awareness of the variability in performance when implemented in actual clinical environments. Presently, a prevalent method for evaluating the performance of generative models relies on quantitative benchmarks. Such benchmarks have limitations and may suffer from train-to-the-test overfitting, optimizing performance for a specified test set at the cost of generalizability across other task and data distributions. Evaluation strategies leveraging human expertise and utilizing cost-effective computational models as evaluators are gaining interest. We discuss current state-of-the-art methodologies for assessing the performance of GenAI applications in healthcare and medical devices.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning</title>
<link>https://arxiv.org/abs/2509.08089</link>
<guid>https://arxiv.org/abs/2509.08089</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Distributed Learning, Machine Learning Model, Backdoor Attacks, Malicious Clients <br />
Summary: <br />
- Distributed learning technique Federated Learning involves multiple clients collaborating to train a machine learning model. 
- Malicious clients can exploit distributed settings to conduct backdoor attacks by embedding malicious behaviors that are triggered during inference.
- Existing defenses against backdoor attacks have not been fully effective, especially against adaptive attackers. 
- A new adaptive adversary has been developed in this work, showcasing superior capabilities in breaching state-of-the-art defenses with minimal malicious clients. 
- The defense approach Hammer and Anvil combines two orthogonal principles to create a robust defense strategy, with its best iteration Krum+ successfully fending off attacks from the new adaptive adversary and existing sophisticated attacks. <div>
arXiv:2509.08089v1 Announce Type: new 
Abstract: Federated Learning is a distributed learning technique in which multiple clients cooperate to train a machine learning model. Distributed settings facilitate backdoor attacks by malicious clients, who can embed malicious behaviors into the model during their participation in the training process. These malicious behaviors are activated during inference by a specific trigger. No defense against backdoor attacks has stood the test of time, especially against adaptive attackers, a powerful but not fully explored category of attackers. In this work, we first devise a new adaptive adversary that surpasses existing adversaries in capabilities, yielding attacks that only require one or two malicious clients out of 20 to break existing state-of-the-art defenses. Then, we present Hammer and Anvil, a principled defense approach that combines two defenses orthogonal in their underlying principle to produce a combined defense that, given the right set of parameters, must succeed against any attack. We show that our best combined defense, Krum+, is successful against our new adaptive adversary and state-of-the-art attacks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography</title>
<link>https://arxiv.org/abs/2509.08116</link>
<guid>https://arxiv.org/abs/2509.08116</guid>
<content:encoded><![CDATA[
<div> Contrastive learning, self-supervised learning, ECG analysis, artificial intelligence, arrhythmia classification <br />
<br />
Summary: PhysioCLR is a novel self-supervised learning framework for ECG analysis that incorporates physiological cues to improve arrhythmia classification. By integrating ECG-specific augmentations and a hybrid loss function, PhysioCLR enhances the quality of learned representations. The method outperforms existing baselines across multiple datasets, showcasing its robust generalization abilities. PhysioCLR's innovative approach of embedding physiological knowledge into contrastive learning enables the model to learn clinically relevant ECG features. This promising method demonstrates the potential of physiology-informed self-supervised learning in improving the efficiency and effectiveness of ECG diagnostics. <div>
arXiv:2509.08116v1 Announce Type: new 
Abstract: Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heart conditions; however, the effectiveness of artificial intelligence (AI)-based ECG analysis is often hindered by the limited availability of labeled data. Self-supervised learning (SSL) can address this by leveraging large-scale unlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive Learning Representation for ECG), a physiology-aware contrastive learning framework that incorporates domain-specific priors to enhance the generalizability and clinical relevance of ECG-based arrhythmia classification. Methods: During pretraining, PhysioCLR learns to bring together embeddings of samples that share similar clinically relevant features while pushing apart those that are dissimilar. Unlike existing methods, our method integrates ECG physiological similarity cues into contrastive learning, promoting the learning of clinically meaningful representations. Additionally, we introduce ECG- specific augmentations that preserve the ECG category post augmentation and propose a hybrid loss function to further refine the quality of learned representations. Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia, for multilabel ECG diagnoses, as well as a private ICU dataset labeled for binary classification. Across the Chapman, Georgia, and private cohorts, PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline, underscoring its robust cross-dataset generalization. Conclusion: By embedding physiological knowledge into contrastive learning, PhysioCLR enables the model to learn clinically meaningful and transferable ECG eatures. Significance: PhysioCLR demonstrates the potential of physiology-informed SSL to offer a promising path toward more effective and label-efficient ECG diagnostics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization Methods and Software for Federated Learning</title>
<link>https://arxiv.org/abs/2509.08120</link>
<guid>https://arxiv.org/abs/2509.08120</guid>
<content:encoded><![CDATA[
<div> challenges, Federated Learning, communication, privacy concerns, algorithm design<br />
<br />
Summary: In this thesis, the author identifies five key challenges in Federated Learning (FL) - the heterogeneity of data and devices, communication issues, privacy concerns for clients, translating theoretical methods into efficient real-world implementations, and adapting practical implementation aspects back into theoretical algorithm design. FL, a decentralized machine learning paradigm involving collaboration among multiple clients, faces unique challenges due to its less controlled settings. The author proposes novel approaches to address these challenges, bridging theoretical advancements and practical implementations to enhance FL algorithms and systems. By exploring the complexities of translating theoretical methods into real-world applications and vice versa, the author sheds light on new dimensions of algorithms and their underlying mechanics, providing guidance for researchers in navigating FL's intricacies. This work serves as a valuable resource for advancing FL algorithms and systems, offering insights into the practical implementation of theoretical advances. <br /> <div>
arXiv:2509.08120v1 Announce Type: new 
Abstract: Federated Learning (FL) is a novel, multidisciplinary Machine Learning paradigm where multiple clients, such as mobile devices, collaborate to solve machine learning problems. Initially introduced in Kone{\v{c}}n{\'y} et al. (2016a,b); McMahan et al. (2017), FL has gained further attention through its inclusion in the National AI Research and Development Strategic Plan (2023 Update) of the United States (Science and on Artificial Intelligence, 2023). The FL training process is inherently decentralized and often takes place in less controlled settings compared to data centers, posing unique challenges distinct from those in fully controlled environments. In this thesis, we identify five key challenges in Federated Learning and propose novel approaches to address them. These challenges arise from the heterogeneity of data and devices, communication issues, and privacy concerns for clients in FL training. Moreover, even well-established theoretical advances in FL require diverse forms of practical implementation to enhance their real-world applicability. Our contributions advance FL algorithms and systems, bridging theoretical advancements and practical implementations. More broadly, our work serves as a guide for researchers navigating the complexities of translating theoretical methods into efficient real-world implementations and software. Additionally, it offers insights into the reverse process of adapting practical implementation aspects back into theoretical algorithm design. This reverse process is particularly intriguing, as the practical perspective compels us to examine the underlying mechanics and flexibilities of algorithms more deeply, often uncovering new dimensions of the algorithms under study.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Learning Enhanced Credibility Transformer</title>
<link>https://arxiv.org/abs/2509.08122</link>
<guid>https://arxiv.org/abs/2509.08122</guid>
<content:encoded><![CDATA[
<div> Credibility Transformer, Transformer architecture, predictive performance, in-context learning, risk patterns. 
Summary: 
The article introduces a network architecture called the Credibility Transformer, which enhances the classical Transformer architecture by incorporating a credibility mechanism to improve model learning and prediction accuracy. The Credibility Transformer learns credibilitized CLS tokens as representations of input features. The paper proposes augmenting this architecture with an in-context learning mechanism, where a context batch of similar instances is used to enhance the CLS token representations by providing additional information and fine-tuning. Empirical results show that in-context learning improves predictive accuracy by adapting to similar risk patterns and enables the model to generalize to new instances, such as those with different feature levels in categorical covariates. This approach is demonstrated by considering the example of a new vehicle model introduced by a car manufacturer. 
<br /><br />Summary: <div>
arXiv:2509.08122v1 Announce Type: new 
Abstract: The starting point of our network architecture is the Credibility Transformer which extends the classical Transformer architecture by a credibility mechanism to improve model learning and predictive performance. This Credibility Transformer learns credibilitized CLS tokens that serve as learned representations of the original input features. In this paper we present a new paradigm that augments this architecture by an in-context learning mechanism, i.e., we increase the information set by a context batch consisting of similar instances. This allows the model to enhance the CLS token representations of the instances by additional in-context information and fine-tuning. We empirically verify that this in-context learning enhances predictive accuracy by adapting to similar risk patterns. Moreover, this in-context learning also allows the model to generalize to new instances which, e.g., have feature levels in the categorical covariates that have not been present when the model was trained -- for a relevant example, think of a new vehicle model which has just been developed by a car manufacturer.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>torchmil: A PyTorch-based library for deep Multiple Instance Learning</title>
<link>https://arxiv.org/abs/2509.08129</link>
<guid>https://arxiv.org/abs/2509.08129</guid>
<content:encoded><![CDATA[
<div> Keywords: Multiple Instance Learning, torchmil, PyTorch, open-source, benchmark datasets

Summary:<br />
The article introduces torchmil, an open-source Python library designed to support Multiple Instance Learning (MIL) using the PyTorch framework. MIL is particularly useful in weakly supervised learning scenarios where fine-grained annotations are not available. The lack of standardized tools in the field of deep MIL methods has hindered reproducibility and accessibility. torchmil aims to address this issue by providing a unified, modular, and extensible framework for developing, evaluating, and comparing MIL models. The library includes basic building blocks for MIL models, a standardized data format, and a curated collection of benchmark datasets and models. Comprehensive documentation and tutorials are available to support both practitioners and researchers, with the goal of accelerating progress in MIL and lowering the entry barrier for new users.<br /><br />Summary: <div>
arXiv:2509.08129v1 Announce Type: new 
Abstract: Multiple Instance Learning (MIL) is a powerful framework for weakly supervised learning, particularly useful when fine-grained annotations are unavailable. Despite growing interest in deep MIL methods, the field lacks standardized tools for model development, evaluation, and comparison, which hinders reproducibility and accessibility. To address this, we present torchmil, an open-source Python library built on PyTorch. torchmil offers a unified, modular, and extensible framework, featuring basic building blocks for MIL models, a standardized data format, and a curated collection of benchmark datasets and models. The library includes comprehensive documentation and tutorials to support both practitioners and researchers. torchmil aims to accelerate progress in MIL and lower the entry barrier for new users. Available at https://torchmil.readthedocs.io.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital</title>
<link>https://arxiv.org/abs/2509.08140</link>
<guid>https://arxiv.org/abs/2509.08140</guid>
<content:encoded><![CDATA[
<div> Keywords: rare-event prediction, large language models, multi-model machine learning, venture capital, feature sensitivity analysis

Summary:
This paper introduces a framework for predicting rare, high-impact outcomes by combining large language models with a multi-model machine learning architecture. The approach leverages black-box models and interpretable decision-making to predict success likelihood and rare-event occurrences. In the context of Venture Capital, where early-stage startup evaluation is challenging, the model demonstrates strong performance in predicting success with precision significantly higher than random classification. The model identifies key success drivers such as the startup's category list, number of founders, education level, and domain expertise, providing valuable insights for decision-makers in the VC domain. Feature sensitivity analysis highlights the importance of these factors in predicting the likelihood of success, showcasing the effectiveness of the proposed framework in predicting rare events and informing strategic investment decisions. 

<br /><br />Summary: <div>
arXiv:2509.08140v1 Announce Type: new 
Abstract: This paper presents a framework for predicting rare, high-impact outcomes by integrating large language models (LLMs) with a multi-model machine learning (ML) architecture. The approach combines the predictive strength of black-box models with the interpretability required for reliable decision-making. We use LLM-powered feature engineering to extract and synthesize complex signals from unstructured data, which are then processed within a layered ensemble of models including XGBoost, Random Forest, and Linear Regression. The ensemble first produces a continuous estimate of success likelihood, which is then thresholded to produce a binary rare-event prediction. We apply this framework to the domain of Venture Capital (VC), where investors must evaluate startups with limited and noisy early-stage data. The empirical results show strong performance: the model achieves precision between 9.8X and 11.1X the random classifier baseline in three independent test subsets. Feature sensitivity analysis further reveals interpretable success drivers: the startup's category list accounts for 15.6% of predictive influence, followed by the number of founders, while education level and domain expertise contribute smaller yet consistent effects.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMM-fair: An Interactive Toolkit for Exploring and Operationalizing Multi-Fairness Trade-offs</title>
<link>https://arxiv.org/abs/2509.08156</link>
<guid>https://arxiv.org/abs/2509.08156</guid>
<content:encoded><![CDATA[
<div> Open-source toolkit, multi-dimensional fairness, boosting-based ensemble, diverse fairness violations, intersectional biases<br />
<br />
Summary:
The article introduces mmm-fair, an open-source toolkit designed for fairness-aware classification in machine learning. It aims to balance performance and fairness, particularly addressing intersectional biases and conflicting fairness definitions. The toolkit utilizes boosting-based ensemble approaches to optimize model weights dynamically, minimizing classification errors and diverse fairness violations simultaneously. mmm-fair allows for flexible multi-objective optimization, empowering users to deploy context-specific models. It also features in-depth multi-attribute fairness analysis, interactive Pareto exploration for model selection, and custom fairness constraint definition. The toolkit incorporates a no-code, chat-based interface and LLM-powered explanations for user-friendly interaction. mmm-fair stands out by combining various fairness-related features in a single tool, facilitating the identification of intersectional biases often overlooked by existing methods. <div>
arXiv:2509.08156v1 Announce Type: new 
Abstract: Fairness-aware classification requires balancing performance and fairness, often intensified by intersectional biases. Conflicting fairness definitions further complicate the task, making it difficult to identify universally fair solutions. Despite growing regulatory and societal demands for equitable AI, popular toolkits offer limited support for exploring multi-dimensional fairness and related trade-offs. To address this, we present mmm-fair, an open-source toolkit leveraging boosting-based ensemble approaches that dynamically optimizes model weights to jointly minimize classification errors and diverse fairness violations, enabling flexible multi-objective optimization. The system empowers users to deploy models that align with their context-specific needs while reliably uncovering intersectional biases often missed by state-of-the-art methods. In a nutshell, mmm-fair uniquely combines in-depth multi-attribute fairness, multi-objective optimization, a no-code, chat-based interface, LLM-powered explanations, interactive Pareto exploration for model selection, custom fairness constraint definition, and deployment-ready models in a single open-source toolkit, a combination rarely found in existing fairness tools. Demo walkthrough available at: https://youtu.be/_rcpjlXFqkw.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation</title>
<link>https://arxiv.org/abs/2509.08163</link>
<guid>https://arxiv.org/abs/2509.08163</guid>
<content:encoded><![CDATA[
<div> fairness, regression, multiple protected attributes, distance covariance, demographic parity  
Summary:  
- The study addresses the issue of fairness in regression tasks, extending beyond binary classification to include continuous attributes like age.  
- A distance covariance regularization framework is proposed to reduce the association between model predictions and protected attributes, focusing on demographic parity.  
- The framework captures both linear and nonlinear dependencies and can handle multiple protected attributes simultaneously.  
- Two multivariate dependence measures, joint distance covariance (JdCov) and concatenated distance covariance (CCdCov), are introduced to combat fairness gerrymandering among intersectional subgroups.  
- Calibration of regularization strength, including a method based on Jensen-Shannon divergence, is discussed for quantifying dissimilarities in prediction distributions across groups.  
<br /><br />Summary: <div>
arXiv:2509.08163v1 Announce Type: new 
Abstract: Ensuring equitable treatment (fairness) across protected attributes (such as gender or ethnicity) is a critical issue in machine learning. Most existing literature focuses on binary classification, but achieving fairness in regression tasks-such as insurance pricing or hiring score assessments-is equally important. Moreover, anti-discrimination laws also apply to continuous attributes, such as age, for which many existing methods are not applicable. In practice, multiple protected attributes can exist simultaneously; however, methods targeting fairness across several attributes often overlook so-called "fairness gerrymandering", thereby ignoring disparities among intersectional subgroups (e.g., African-American women or Hispanic men). In this paper, we propose a distance covariance regularisation framework that mitigates the association between model predictions and protected attributes, in line with the fairness definition of demographic parity, and that captures both linear and nonlinear dependencies. To enhance applicability in the presence of multiple protected attributes, we extend our framework by incorporating two multivariate dependence measures based on distance covariance: the previously proposed joint distance covariance (JdCov) and our novel concatenated distance covariance (CCdCov), which effectively address fairness gerrymandering in both regression and classification tasks involving protected attributes of various types. We discuss and illustrate how to calibrate regularisation strength, including a method based on Jensen-Shannon divergence, which quantifies dissimilarities in prediction distributions across groups. We apply our framework to the COMPAS recidivism dataset and a large motor insurance claims dataset.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments</title>
<link>https://arxiv.org/abs/2509.08176</link>
<guid>https://arxiv.org/abs/2509.08176</guid>
<content:encoded><![CDATA[
<div> Concept drift, online learning, data stream mining, transfer learning, non-stationary environments <br /> 
<br /> 
Summary: 
Concept drift poses a challenge in online learning by affecting the accuracy of data stream mining systems. Existing approaches leverage data streams from multiple sources to address concept drift, assuming similarity between source and target concepts. However, in many real-world scenarios, this assumption may not hold true. To tackle this issue, a new method called MARLINE is proposed. MARLINE utilizes knowledge from various data sources in non-stationary environments, even when source and target concepts do not align. This is achieved by mapping the target concept to each source concept space, allowing multiple source sub-classifiers to contribute to predicting the target concept within an ensemble. Experimental results on synthetic and real-world datasets demonstrate the superior accuracy of MARLINE compared to state-of-the-art data stream learning techniques. <div>
arXiv:2509.08176v1 Announce Type: new 
Abstract: Concept drift is a major problem in online learning due to its impact on the predictive performance of data stream mining systems. Recent studies have started exploring data streams from different sources as a strategy to tackle concept drift in a given target domain. These approaches make the assumption that at least one of the source models represents a concept similar to the target concept, which may not hold in many real-world scenarios. In this paper, we propose a novel approach called Multi-source mApping with tRansfer LearnIng for Non-stationary Environments (MARLINE). MARLINE can benefit from knowledge from multiple data sources in non-stationary environments even when source and target concepts do not match. This is achieved by projecting the target concept to the space of each source concept, enabling multiple source sub-classifiers to contribute towards the prediction of the target concept as part of an ensemble. Experiments on several synthetic and real-world datasets show that MARLINE was more accurate than several state-of-the-art data stream learning approaches.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Domain Mixed Unit: A New Neural Arithmetic Layer</title>
<link>https://arxiv.org/abs/2509.08180</link>
<guid>https://arxiv.org/abs/2509.08180</guid>
<content:encoded><![CDATA[
<div> neural arithmetic unit, Domain Mixed Unit, NALM Benchmark, addition, subtraction
Summary: 
The Domain Mixed Unit (DMU) is a novel neural arithmetic unit that incorporates a single parameter gate to blend log-space and linear-space representations during addition (DMU add) or subtraction (DMU sub). It introduces two initializations: one for addition and multiplication, and another for subtraction and division. The DMU demonstrates superior performance on the NALM Benchmark, outperforming other units by achieving the highest percentage of solved tasks across all seeds, particularly excelling in multiplication and division. The DMU's code is openly available on GitHub for further exploration and integration into the NALM benchmark as a pull request. <div>
arXiv:2509.08180v1 Announce Type: new 
Abstract: The Domain Mixed Unit (DMU) is a new neural arithmetic unit that learns a single parameter gate that mixes between log-space and linear-space representations while performing either addition (DMU add) or subtraction (DMU sub). Two initializations are proposed for the DMU: one covering addition and multiplication, and another covering subtraction and division. The DMU achieves state-of-the-art performance on the NALM Benchmark, a dataset designed to test the ability of neural arithmetic units to generalize arithmetic operations, specifically performing with the highest percentage solved over all seeds on multiplication and division. The DMU will be submitted as a pull request to the open-source NALM benchmark, and its code is available on GitHub at https://github.com/marict?tab=repositories
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Label Transfer Learning in Non-Stationary Data Streams</title>
<link>https://arxiv.org/abs/2509.08181</link>
<guid>https://arxiv.org/abs/2509.08181</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-label data streams, transfer learning, label drift, non-stationary environments, pairwise label dependencies

Summary:
- The study focuses on addressing label concept drift in multi-label data streams in non-stationary environments.
- The proposed methods, BR-MARLENE and BRPW-MARLENE, aim to transfer knowledge between related labels to enhance multi-label classification performance.
- BR-MARLENE leverages knowledge from different labels in source and target streams, while BRPW-MARLENE specifically models and transfers pairwise label dependencies to improve learning outcomes.
- Comprehensive experiments demonstrate that both methods outperform existing approaches in non-stationary settings, showcasing the effectiveness of inter-label knowledge transfer for enhanced predictive performance.
- The research highlights the significance of multi-label transfer learning in adapting to label drift and improving classification accuracy in dynamic data stream environments.

<br /><br />Summary: <div>
arXiv:2509.08181v1 Announce Type: new 
Abstract: Label concepts in multi-label data streams often experience drift in non-stationary environments, either independently or in relation to other labels. Transferring knowledge between related labels can accelerate adaptation, yet research on multi-label transfer learning for data streams remains limited. To address this, we propose two novel transfer learning methods: BR-MARLENE leverages knowledge from different labels in both source and target streams for multi-label classification; BRPW-MARLENE builds on this by explicitly modelling and transferring pairwise label dependencies to enhance learning performance. Comprehensive experiments show that both methods outperform state-of-the-art multi-label stream approaches in non-stationary environments, demonstrating the effectiveness of inter-label knowledge transfer for improved predictive performance.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Induction Heads: How Transformers Select Causal Structures In Context</title>
<link>https://arxiv.org/abs/2509.08184</link>
<guid>https://arxiv.org/abs/2509.08184</guid>
<content:encoded><![CDATA[
<div> Framework, Transformers, Induction Heads, Causal Structures, Selective Induction Heads

Summary:
The article introduces a novel framework for transformers to handle dynamic causal structures. It showcases the formation of Selective Induction Heads, a new circuit that enables transformers to choose the appropriate causal structure in-context. By using interleaved Markov chains with different lags, transformers can predict the next token by identifying the correct lag and copying the corresponding token from the past. The study provides a detailed construction of a 3-layer transformer with the selective induction head mechanism and proves its convergence to the maximum likelihood solution theoretically. This approach advances the understanding of how transformers learn and utilize causal structures, offering insights into their functionality and interpretability.<br /><br />Summary: <div>
arXiv:2509.08184v1 Announce Type: new 
Abstract: Transformers have exhibited exceptional capabilities in sequence modeling tasks, leveraging self-attention and in-context learning. Critical to this success are induction heads, attention circuits that enable copying tokens based on their previous occurrences. In this work, we introduce a novel framework that showcases transformers' ability to dynamically handle causal structures. Existing works rely on Markov Chains to study the formation of induction heads, revealing how transformers capture causal dependencies and learn transition probabilities in-context. However, they rely on a fixed causal structure that fails to capture the complexity of natural languages, where the relationship between tokens dynamically changes with context. To this end, our framework varies the causal structure through interleaved Markov chains with different lags while keeping the transition probabilities fixed. This setting unveils the formation of Selective Induction Heads, a new circuit that endows transformers with the ability to select the correct causal structure in-context. We empirically demonstrate that transformers learn this mechanism to predict the next token by identifying the correct lag and copying the corresponding token from the past. We provide a detailed construction of a 3-layer transformer to implement the selective induction head, and a theoretical analysis proving that this mechanism asymptotically converges to the maximum likelihood solution. Our findings advance the understanding of how transformers select causal structures, providing new insights into their functioning and interpretability.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis</title>
<link>https://arxiv.org/abs/2509.08188</link>
<guid>https://arxiv.org/abs/2509.08188</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, artifacts, generative models, augmentation, synthesis

Summary:
In this study, the researchers explore the use of modern generative models to synthesize realistic artifact segments in EEG data for augmentation and stress-testing purposes. They compare the performance of a conditional WGAN-GP model with a projection discriminator to a 1D denoising diffusion model with classifier-free guidance. The evaluation is based on fidelity metrics like band-power deltas and channel-covariance distance, specificity through class-conditional recovery, and utility in augmentation effects on artifact recognition. The results show that the WGAN-GP model achieves closer spectral alignment with real data, while both models exhibit limitations in class-conditional recovery. This study provides a reproducible pipeline for EEG artifact synthesis and highlights areas for future improvement, such as stronger conditioning and coverage in generative models. <div>
arXiv:2509.08188v1 Announce Type: new 
Abstract: Artifacts in electroencephalography (EEG) -- muscle, eye movement, electrode, chewing, and shiver -- confound automated analysis yet are costly to label at scale. We study whether modern generative models can synthesize realistic, label-aware artifact segments suitable for augmentation and stress-testing. Using the TUH EEG Artifact (TUAR) corpus, we curate subject-wise splits and fixed-length multi-channel windows (e.g., 250 samples) with preprocessing tailored to each model (per-window min--max for adversarial training; per-recording/channel $z$-score for diffusion). We compare a conditional WGAN-GP with a projection discriminator to a 1D denoising diffusion model with classifier-free guidance, and evaluate along three axes: (i) fidelity via Welch band-power deltas ($\Delta\delta,\ \Delta\theta,\ \Delta\alpha,\ \Delta\beta$), channel-covariance Frobenius distance, autocorrelation $L_2$, and distributional metrics (MMD/PRD); (ii) specificity via class-conditional recovery with lightweight $k$NN/classifiers; and (iii) utility via augmentation effects on artifact recognition. In our setting, WGAN-GP achieves closer spectral alignment and lower MMD to real data, while both models exhibit weak class-conditional recovery, limiting immediate augmentation gains and revealing opportunities for stronger conditioning and coverage. We release a reproducible pipeline -- data manifests, training configurations, and evaluation scripts -- to establish a baseline for EEG artifact synthesis and to surface actionable failure modes for future work.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rollout-LaSDI: Enhancing the long-term accuracy of Latent Space Dynamics</title>
<link>https://arxiv.org/abs/2509.08191</link>
<guid>https://arxiv.org/abs/2509.08191</guid>
<content:encoded><![CDATA[
<div> flexible, high-order, inexpensive, finite-difference scheme, Rollout loss <br />
Summary: <br />
The article introduces an approach aimed at solving complex partial differential equations efficiently. It focuses on the development of reduced-order models (ROMs) that can handle parameterized families of PDEs effectively. The key contributions include the introduction of a flexible, high-order, and cost-effective finite-difference scheme, as well as the proposal of a Rollout loss function to improve the predictive power of ROMs over long time horizons. By combining these elements, the approach aims to enhance the accuracy of predictions made by ROMs, particularly in the context of the 2D Burgers equation. This advancement in numerical methods could have significant implications for the physical sciences, where solving PDEs is crucial but computationally expensive. <div>
arXiv:2509.08191v1 Announce Type: new 
Abstract: Solving complex partial differential equations is vital in the physical sciences, but often requires computationally expensive numerical methods. Reduced-order models (ROMs) address this by exploiting dimensionality reduction to create fast approximations. While modern ROMs can solve parameterized families of PDEs, their predictive power degrades over long time horizons. We address this by (1) introducing a flexible, high-order, yet inexpensive finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to make accurate predictions over arbitrary time horizons. We demonstrate our approach on the 2D Burgers equation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prescribe-then-Select: Adaptive Policy Selection for Contextual Stochastic Optimization</title>
<link>https://arxiv.org/abs/2509.08194</link>
<guid>https://arxiv.org/abs/2509.08194</guid>
<content:encoded><![CDATA[
<div> policy selection, contextual stochastic optimization, feasibility constraints, Prescribe-then-Select, Optimal Policy Trees

Summary:
Prescribe-then-Select (PS) addresses the policy selection problem in contextual stochastic optimization (CSO) by first constructing a library of candidate policies and then using a data-driven meta-policy to select the best policy for the given covariates. The meta-policy is implemented using ensembles of Optimal Policy Trees trained via cross-validation on the training set, ensuring optimal policy choice. In benchmark CSO problems like single-stage newsvendor and two-stage shipment planning, PS outperforms individual policies in heterogeneous covariate spaces and converges to the dominant policy in homogeneous regions. The framework consistently performs better than the best single policy across different scenarios, showcasing its effectiveness in diverse CSO settings. The code to reproduce the results of the study is available for reference, providing transparency and reproducibility in the research findings. <br /><br />Summary: <div>
arXiv:2509.08194v1 Announce Type: new 
Abstract: We address the problem of policy selection in contextual stochastic optimization (CSO), where covariates are available as contextual information and decisions must satisfy hard feasibility constraints. In many CSO settings, multiple candidate policies--arising from different modeling paradigms--exhibit heterogeneous performance across the covariate space, with no single policy uniformly dominating. We propose Prescribe-then-Select (PS), a modular framework that first constructs a library of feasible candidate policies and then learns a meta-policy to select the best policy for the observed covariates. We implement the meta-policy using ensembles of Optimal Policy Trees trained via cross-validation on the training set, making policy choice entirely data-driven. Across two benchmark CSO problems--single-stage newsvendor and two-stage shipment planning--PS consistently outperforms the best single policy in heterogeneous regimes of the covariate space and converges to the dominant policy when such heterogeneity is absent. All the code to reproduce the results can be found at https://anonymous.4open.science/r/Prescribe-then-Select-TMLR.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sketched Gaussian Mechanism for Private Federated Learning</title>
<link>https://arxiv.org/abs/2509.08195</link>
<guid>https://arxiv.org/abs/2509.08195</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, sketching, Gaussian mechanism, differential privacy, optimization convergence

Summary:
In this article, the concept of Sketched Gaussian Mechanism (SGM) is introduced, combining sketching and the Gaussian mechanism for privacy in federated learning. A joint analysis of SGM's privacy guarantee using R\'enyi-DP tools reveals that SGM can offer significantly stronger privacy than the original Gaussian mechanism, with the privacy level being proportional to $1/\sqrt{b}$ for a fixed noise magnitude. The application of SGM to federated learning with gradient descent or adaptive server optimizers shows promising theoretical results on optimization convergence with a logarithmic dependence on the number of parameters. Experimental results demonstrate that SGM-based federated learning performs competitively with non-sketching private FL variants and can outperform them in certain scenarios. Additionally, employing adaptive optimization at the server enhances empirical performance while upholding the privacy guarantees. 

<br /><br />Summary: <div>
arXiv:2509.08195v1 Announce Type: new 
Abstract: Communication cost and privacy are two major considerations in federated learning (FL). For communication cost, gradient compression by sketching the clients' transmitted model updates is often used for reducing per-round communication. For privacy, the Gaussian mechanism (GM), which consists of clipping updates and adding Gaussian noise, is commonly used to guarantee client-level differential privacy. Existing literature on private FL analyzes privacy of sketching and GM in an isolated manner, illustrating that sketching provides privacy determined by the sketching dimension and that GM has to supply any additional desired privacy.
  In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which directly combines sketching and the Gaussian mechanism for privacy. Using R\'enyi-DP tools, we present a joint analysis of SGM's overall privacy guarantee, which is significantly more flexible and sharper compared to isolated analysis of sketching and GM privacy. In particular, we prove that the privacy level of SGM for a fixed noise magnitude is proportional to $1/\sqrt{b}$, where $b$ is the sketching dimension, indicating that (for moderate $b$) SGM can provide much stronger privacy guarantees than the original GM under the same noise budget. We demonstrate the application of SGM to FL with either gradient descent or adaptive server optimizers, and establish theoretical results on optimization convergence, which exhibits only a logarithmic dependence on the number of parameters $d$. Experimental results confirm that at the same privacy level, SGM based FL is at least competitive with non-sketching private FL variants and outperforms them in some settings. Moreover, using adaptive optimization at the server improves empirical performance while maintaining the privacy guarantees.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Distribution Distillation for Self-Supervised Human Activity Recognition</title>
<link>https://arxiv.org/abs/2509.08225</link>
<guid>https://arxiv.org/abs/2509.08225</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Activity Recognition, deep learning, Ensemble Distribution Distillation, self-supervised learning, robustness<br />
Summary: This paper introduces a novel approach, Ensemble Distribution Distillation (EDD), within a self-supervised learning framework for Human Activity Recognition (HAR). By utilizing unlabeled data and a partially supervised training strategy, the method enhances predictive accuracy, provides robust estimates of uncertainty, and increases robustness against adversarial perturbations. This improved reliability in real-world scenarios without adding complexity to inference. The study includes the development of a self-supervised EDD framework, a unique data augmentation technique for HAR, and empirical evidence of the method's effectiveness in boosting robustness and reliability.<br /><br /> <div>
arXiv:2509.08225v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) has seen significant advancements with the adoption of deep learning techniques, yet challenges remain in terms of data requirements, reliability and robustness. This paper explores a novel application of Ensemble Distribution Distillation (EDD) within a self-supervised learning framework for HAR aimed at overcoming these challenges. By leveraging unlabeled data and a partially supervised training strategy, our approach yields an increase in predictive accuracy, robust estimates of uncertainty, and substantial increases in robustness against adversarial perturbation; thereby significantly improving reliability in real-world scenarios without increasing computational complexity at inference. We demonstrate this with an evaluation on several publicly available datasets. The contributions of this work include the development of a self-supervised EDD framework, an innovative data augmentation technique designed for HAR, and empirical validation of the proposed method's effectiveness in increasing robustness and reliability.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization</title>
<link>https://arxiv.org/abs/2509.08233</link>
<guid>https://arxiv.org/abs/2509.08233</guid>
<content:encoded><![CDATA[
<div> Keywords: distributed learning, federated learning, model compression, local training, privacy preservation

Summary:
This dissertation focuses on improving communication efficiency in distributed and federated learning by exploring strategies such as model compression, local training, and personalization. The research introduces a unified framework for compression operators with convergence guarantees and adaptive local training strategies that incorporate personalized objectives to accelerate convergence. The Scafflix approach strikes a balance between global and personalized objectives, delivering superior performance in both IID and non-IID settings. Privacy-preserving pruning frameworks like Cohort-Squeeze and SymWanda are introduced to optimize sparsity and minimize communication costs, with SymWanda enhancing robustness under high sparsity without retraining. Through extensive experiments on benchmarks and large-scale language models, the study demonstrates favorable trade-offs among accuracy, convergence, and communication efficiency, providing theoretical and practical insights for scalable and efficient distributed learning.<br /><br />Summary: <div>
arXiv:2509.08233v1 Announce Type: new 
Abstract: Distributed and federated learning are essential paradigms for training models across decentralized data sources while preserving privacy, yet communication overhead remains a major bottleneck. This dissertation explores strategies to improve communication efficiency, focusing on model compression, local training, and personalization. We establish a unified framework for biased and unbiased compression operators with convergence guarantees, then propose adaptive local training strategies that incorporate personalization to accelerate convergence and mitigate client drift. In particular, Scafflix balances global and personalized objectives, achieving superior performance under both IID and non-IID settings. We further introduce privacy-preserving pruning frameworks that optimize sparsity while minimizing communication costs, with Cohort-Squeeze leveraging hierarchical aggregation to reduce cross-device overhead. Finally, SymWanda, a symmetric post-training pruning method, enhances robustness under high sparsity and maintains accuracy without retraining. Extensive experiments on benchmarks and large-scale language models demonstrate favorable trade-offs among accuracy, convergence, and communication, offering theoretical and practical insights for scalable, efficient distributed learning.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The CRITICAL Records Integrated Standardization Pipeline (CRISP): End-to-End Processing of Large-scale Multi-institutional OMOP CDM Data</title>
<link>https://arxiv.org/abs/2509.08247</link>
<guid>https://arxiv.org/abs/2509.08247</guid>
<content:encoded><![CDATA[
<div> Keywords: critical care, EHR datasets, CRITICAL dataset, data harmonization, ML-ready datasets

Summary: 
The CRITICAL dataset introduces a new and diverse perspective on critical care EHR data, encompassing 1.95 billion records from over 371,000 patients across multiple institutions. CRITICAL captures complete patient journeys across various care settings, offering ample opportunities for developing predictive models and advancing health equity research. However, the complexity of this multi-institutional resource necessitates sophisticated data harmonization techniques. CRISP is presented as a solution to unlock the full potential of CRITICAL, offering transparent data quality management, cross-vocabulary mapping, fast processing capabilities, and baseline model benchmarks. By providing a processing pipeline and detailed documentation, CRISP saves researchers time and democratizes access to large-scale critical care data, allowing them to focus on advancing clinical AI. 

<br /><br />Summary: <div>
arXiv:2509.08247v1 Announce Type: new 
Abstract: While existing critical care EHR datasets such as MIMIC and eICU have enabled significant advances in clinical AI research, the CRITICAL dataset opens new frontiers by providing extensive scale and diversity -- containing 1.95 billion records from 371,365 patients across four geographically diverse CTSA institutions. CRITICAL's unique strength lies in capturing full-spectrum patient journeys, including pre-ICU, ICU, and post-ICU encounters across both inpatient and outpatient settings. This multi-institutional, longitudinal perspective creates transformative opportunities for developing generalizable predictive models and advancing health equity research. However, the richness of this multi-site resource introduces substantial complexity in data harmonization, with heterogeneous collection practices and diverse vocabulary usage patterns requiring sophisticated preprocessing approaches.
  We present CRISP to unlock the full potential of this valuable resource. CRISP systematically transforms raw Observational Medical Outcomes Partnership Common Data Model data into ML-ready datasets through: (1) transparent data quality management with comprehensive audit trails, (2) cross-vocabulary mapping of heterogeneous medical terminologies to unified SNOMED-CT standards, with deduplication and unit standardization, (3) modular architecture with parallel optimization enabling complete dataset processing in $<$1 day even on standard computing hardware, and (4) comprehensive baseline model benchmarks spanning multiple clinical prediction tasks to establish reproducible performance standards. By providing processing pipeline, baseline implementations, and detailed transformation documentation, CRISP saves researchers months of preprocessing effort and democratizes access to large-scale multi-institutional critical care data, enabling them to focus on advancing clinical AI.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning</title>
<link>https://arxiv.org/abs/2509.08255</link>
<guid>https://arxiv.org/abs/2509.08255</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Catastrophic Forgetting, fine-tuning, pruning, downstream tasks

Summary:
Large language models (LLMs) have shown impressive capabilities in various downstream tasks but struggle with Catastrophic Forgetting (CF) during fine-tuning. This paper introduces the Forgetting-Aware Pruning Metric (FAPM) as a novel approach to mitigate CF while maintaining task performance. By analyzing the overlap between task vectors and pre-trained model parameters, FAPM quantifies CF using a metric that guides pruning criteria without altering the training process or model architecture. Across a range of datasets, including natural language inference and question answering, FAPM effectively limits CF to 0.25% while achieving a high accuracy of 99.67% on downstream tasks. The code for reproducing these results is provided for further exploration. 

<br /><br />Summary: <div>
arXiv:2509.08255v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have shown impressive capabilities in various downstream tasks but typically face Catastrophic Forgetting (CF) during fine-tuning. In this paper, we propose the Forgetting-Aware Pruning Metric (FAPM), a novel pruning-based approach to balance CF and downstream task performance. Our investigation reveals that the degree to which task vectors (i.e., the subtraction of pre-trained weights from the weights fine-tuned on downstream tasks) overlap with pre-trained model parameters is a critical factor for CF. Based on this finding, FAPM employs the ratio of the task vector to pre-trained model parameters as a metric to quantify CF, integrating this measure into the pruning criteria. Importantly, FAPM does not necessitate modifications to the training process or model architecture, nor does it require any auxiliary data. We conducted extensive experiments across eight datasets, covering natural language inference, General Q&amp;A, Medical Q&amp;A, Math Q&amp;A, reading comprehension, and cloze tests. The results demonstrate that FAPM limits CF to just 0.25\% while maintaining 99.67\% accuracy on downstream tasks. We provide the code to reproduce our results.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Physics Reasoning and Performance Taxonomy in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.08270</link>
<guid>https://arxiv.org/abs/2509.08270</guid>
<content:encoded><![CDATA[
<div> Physics, Vision-Language Models, Reasoning, Evaluation, Framework

Summary:
- Introduction of a novel framework to evaluate Vision-Language Models (VLMs) on their understanding of 2D physics.
- The framework includes a scenario generator with over 400 problems in Projectile Motion, Collision Dynamics, Mechanics, and Fluid Dynamics.
- Evaluation of four state-of-the-art VLMs shows a correlation between model scale and reasoning ability, with Qwen2.5-VL-7B performing the best with a score of 0.815.
- Models excel at formulaic problems but struggle with abstract spatial reasoning tasks.
- The framework aims to democratize the study of scientific reasoning in VLMs and provide insights into their capabilities and limitations.

<br /><br />Summary: <div>
arXiv:2509.08270v1 Announce Type: new 
Abstract: As Vision-Language Models (VLMs) grow in sophistication, their ability to perform reasoning is coming under increasing supervision. While they excel at many tasks, their grasp of fundamental scientific principles, such as physics, remains an underexplored frontier. To reflect the advancements in these capabilities, we introduce a novel and accessible framework designed to rigorously evaluate VLMs on their understanding of 2D physics. Our framework features a pragmatic scenario generator that creates a diverse testbed of over 400 problems across four core domains: Projectile Motion, Collision Dynamics, Mechanics, and Fluid Dynamics. Through comprehensive evaluation of four state-of-the-art VLMs, we demonstrate a strong correlation between model scale and reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving an overall score of 0.815. We find that while models excel at formulaic problems, they struggle significantly with domains requiring abstract spatial reasoning. By designing this framework, we aim to democratize the study of scientific reasoning in VLMs and foster deeper insights into their capabilities and limitations.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Rainfall Forecasting from Multiple Geographical Models Using Matrix Profile and Ensemble Learning</title>
<link>https://arxiv.org/abs/2509.08277</link>
<guid>https://arxiv.org/abs/2509.08277</guid>
<content:encoded><![CDATA[
<div> Matrix Profile-based Weighted Ensemble, rainfall forecasting, Vietnam, regime-switching framework, covariant dependencies
<br />
Summary: 
The study introduces a novel Matrix Profile-based Weighted Ensemble (MPWE) method for rainfall forecasting in Vietnam, where accurate predictions are crucial for flood management and disaster preparedness due to the diverse climatic conditions. MPWE dynamically captures covariant dependencies among multiple geographical model forecasts and incorporates redundancy-aware weighting to balance contributions. Evaluation using data from eight major basins in Vietnam shows that MPWE outperforms geographical models and ensemble baselines in terms of accuracy and stability across different forecast horizons. The results demonstrate lower mean and standard deviation of prediction errors with MPWE, indicating improved forecasting performance. <div>
arXiv:2509.08277v1 Announce Type: new 
Abstract: Rainfall forecasting in Vietnam is highly challenging due to its diverse climatic conditions and strong geographical variability across river basins, yet accurate and reliable forecasts are vital for flood management, hydropower operation, and disaster preparedness. In this work, we propose a Matrix Profile-based Weighted Ensemble (MPWE), a regime-switching framework that dynamically captures covariant dependencies among multiple geographical model forecasts while incorporating redundancy-aware weighting to balance contributions across models. We evaluate MPWE using rainfall forecasts from eight major basins in Vietnam, spanning five forecast horizons (1 hour and accumulated rainfall over 12, 24, 48, 72, and 84 hours). Experimental results show that MPWE consistently achieves lower mean and standard deviation of prediction errors compared to geographical models and ensemble baselines, demonstrating both improved accuracy and stability across basins and horizons.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\emph{FoQuS}: A Forgetting-Quality Coreset Selection Framework for Automatic Modulation Recognition</title>
<link>https://arxiv.org/abs/2509.08300</link>
<guid>https://arxiv.org/abs/2509.08300</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, Automatic Modulation Recognition, FoQuS, Training optimization, Data efficiency <br />
Summary:<br /> 
The article introduces a novel approach called FoQuS for Automatic Modulation Recognition (AMR) model training optimization. By selecting a representative coreset from the original dataset based on training dynamics, FoQuS significantly reduces the time and energy consumption associated with repeated training. This method helps maintain high recognition accuracy and ensures good cross-architecture generalization on multiple AMR datasets. The approach records the prediction trajectory of each sample during full-dataset training and constructs three importance metrics to efficiently select the coreset. Experiments demonstrate that FoQuS achieves impressive results using only a fraction (1%-30%) of the original data. This innovative solution addresses the challenges of developing new AMR models and performing hyperparameter tuning by improving data efficiency and reducing training overhead. <br /><br />Summary: <div>
arXiv:2509.08300v1 Announce Type: new 
Abstract: Deep learning-based Automatic Modulation Recognition (AMR) model has made significant progress with the support of large-scale labeled data. However, when developing new models or performing hyperparameter tuning, the time and energy consumption associated with repeated training using massive amounts of data are often unbearable. To address the above challenges, we propose \emph{FoQuS}, which approximates the effect of full training by selecting a coreset from the original dataset, thereby significantly reducing training overhead. Specifically, \emph{FoQuS} records the prediction trajectory of each sample during full-dataset training and constructs three importance metrics based on training dynamics. Experiments show that \emph{FoQuS} can maintain high recognition accuracy and good cross-architecture generalization on multiple AMR datasets using only 1\%-30\% of the original data.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvolKV: Evolutionary KV Cache Compression for LLM Inference</title>
<link>https://arxiv.org/abs/2509.08315</link>
<guid>https://arxiv.org/abs/2509.08315</guid>
<content:encoded><![CDATA[
<div> evolutionary search, cache compression, task performance, memory efficiency, key-value cache <br />
Summary:
Existing key-value cache compression methods often rely on heuristics, leading to suboptimal generalization due to neglecting interplays between feature patterns and task performance. The EvolKV framework proposed in this paper addresses this issue by dynamically optimizing layer-wise cache allocation to enhance memory efficiency and task performance. By treating cache allocation as a multi-objective optimization problem and utilizing evolutionary search, EvolKV significantly outperforms baseline methods across various tasks, achieving up to 7 percentage points improvement on GSM8K. Remarkably, EvolKV excels in code completion task even with only 1.5% of the original budget, showcasing the potential of learned compression strategies for efficient key-value cache allocation. <br /><br />Summary: <div>
arXiv:2509.08315v1 Announce Type: new 
Abstract: Existing key-value (KV) cache compression methods typically rely on heuristics, such as uniform cache allocation across layers or static eviction policies, however, they ignore the critical interplays among layer-specific feature patterns and task performance, which can lead to degraded generalization. In this paper, we propose EvolKV, an adaptive framework for layer-wise, task-driven KV cache compression that jointly optimizes the memory efficiency and task performance. By reformulating cache allocation as a multi-objective optimization problem, EvolKV leverages evolutionary search to dynamically configure layer budgets while directly maximizing downstream performance. Extensive experiments on 11 tasks demonstrate that our approach outperforms all baseline methods across a wide range of KV cache budgets on long-context tasks and surpasses heuristic baselines by up to 7 percentage points on GSM8K. Notably, EvolKV achieves superior performance over the full KV cache setting on code completion while utilizing only 1.5% of the original budget, suggesting the untapped potential in learned compression strategies for KV cache budget allocation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing</title>
<link>https://arxiv.org/abs/2509.08329</link>
<guid>https://arxiv.org/abs/2509.08329</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Student-Teacher Architecture, Convergence Dynamics, Advice Reuse

Summary:
Reinforcement Learning (RL) algorithms often have slow training times, especially in complex environments with sparse rewards. This study explores the use of pre-trained Large Language Models (LLMs) as tutors in a student-teacher architecture with RL algorithms to speed up convergence. By reusing LLM-generated guidance, the study aims to accelerate training while maintaining optimal performance. The results from 54 different configurations across various RL algorithms, LLM tutors, and environments show that LLM tutoring significantly accelerates RL convergence without sacrificing performance. However, the reuse of LLM advice can lead to less stable convergence dynamics. Overall, LLM tutoring proves to be effective in improving convergence but is influenced by the specific task, RL algorithm, and LLM model combination.<br /><br />Summary: <div>
arXiv:2509.08329v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) algorithms often require long training to become useful, especially in complex environments with sparse rewards. While techniques like reward shaping and curriculum learning exist to accelerate training, these are often extremely specific and require the developer's professionalism and dedicated expertise in the problem's domain. Tackling this challenge, in this study, we explore the effectiveness of pre-trained Large Language Models (LLMs) as tutors in a student-teacher architecture with RL algorithms, hypothesizing that LLM-generated guidance allows for faster convergence. In particular, we explore the effectiveness of reusing the LLM's advice on the RL's convergence dynamics. Through an extensive empirical examination, which included 54 configurations, varying the RL algorithm (DQN, PPO, A2C), LLM tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack, Snake, Connect Four), our results demonstrate that LLM tutoring significantly accelerates RL convergence while maintaining comparable optimal performance. Furthermore, the advice reuse mechanism shows a further improvement in training duration but also results in less stable convergence dynamics. Our findings suggest that LLM tutoring generally improves convergence, and its effectiveness is sensitive to the specific task, RL algorithm, and LLM model combination.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Mixture-of-Expert Inference with Adaptive Expert Split Mechanism</title>
<link>https://arxiv.org/abs/2509.08342</link>
<guid>https://arxiv.org/abs/2509.08342</guid>
<content:encoded><![CDATA[
<div> Efficient MoE Inference, Expert Split Mechanism, GPU Memory Reduction, Cache Configuration, Adaptive Algorithm <br />
Summary: <br />
The paper introduces MoEpic, an efficient system for Mixture-of-Experts (MoE) inference in large language models (LLMs). MoE LLMs typically have heavy GPU memory demands, hindering their widespread adoption. MoEpic tackles this issue by offloading expert parameters to CPU RAM, reducing VRAM requirements during inference. It implements a novel expert split mechanism where each expert is divided into top and bottom segments to optimize caching. MoEpic predicts and prefetches activated experts for the next layer, reducing loading time and enabling transfer-computation overlap. An adaptive algorithm based on fixed-point iteration helps configure the cache for optimal performance. Experiments on various MoE LLMs show that MoEpic significantly reduces GPU cost and inference latency compared to existing methods. <div>
arXiv:2509.08342v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) has emerged as a promising architecture for modern large language models (LLMs). However, massive parameters impose heavy GPU memory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs. Offloading the expert parameters to CPU RAM offers an effective way to alleviate the VRAM requirements for MoE inference. Existing approaches typically cache a small subset of experts in VRAM and dynamically prefetch experts from RAM during inference, leading to significant degradation in inference speed due to the poor cache hit rate and substantial expert loading latency. In this work, we propose MoEpic, an efficient MoE inference system with a novel expert split mechanism. Specifically, each expert is vertically divided into two segments: top and bottom. MoEpic caches the top segment of hot experts, so that more experts will be stored under the limited VRAM budget, thereby improving the cache hit rate. During each layer's inference, MoEpic predicts and prefetches the activated experts for the next layer. Since the top segments of cached experts are exempt from fetching, the loading time is reduced, which allows efficient transfer-computation overlap. Nevertheless, the performance of MoEpic critically depends on the cache configuration (i.e., each layer's VRAM budget and expert split ratio). To this end, we propose a divide-and-conquer algorithm based on fixed-point iteration for adaptive cache configuration. Extensive experiments on popular MoE LLMs demonstrate that MoEpic can save about half of the GPU cost, while lowering the inference latency by about 37.51%-65.73% compared to the baselines.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction Loss Guided Decision-Focused Learning</title>
<link>https://arxiv.org/abs/2509.08359</link>
<guid>https://arxiv.org/abs/2509.08359</guid>
<content:encoded><![CDATA[
<div> Prediction-focused learning, Decision-focused learning, Uncertainty, Optimization, Gradient perturbation<br />
<br />
Summary:<br />
Decision-making under uncertainty involves predicting unknown parameters and optimizing decisions. Traditional prediction-focused learning (PFL) and decision-focused learning (DFL) address these stages separately or jointly. We introduce a method combining PFL and DFL by perturbing the decision loss gradient with the prediction loss gradient. This approach guides the decision loss optimization towards higher decision quality without additional training. The method ensures stable convergence and Pareto stationary points under mild assumptions. In empirical tests on stochastic optimization problems, our method outperforms existing baselines by achieving lower regret and stable training. It offers a promising solution for optimizing decision quality in various scenarios where PFL or DFL alone face challenges. <br /> <div>
arXiv:2509.08359v1 Announce Type: new 
Abstract: Decision-making under uncertainty is often considered in two stages: predicting the unknown parameters, and then optimizing decisions based on predictions. While traditional prediction-focused learning (PFL) treats these two stages separately, decision-focused learning (DFL) trains the predictive model by directly optimizing the decision quality in an end-to-end manner. However, despite using exact or well-approximated gradients, vanilla DFL often suffers from unstable convergence due to its flat-and-sharp loss landscapes. In contrast, PFL yields more stable optimization, but overlooks the downstream decision quality. To address this, we propose a simple yet effective approach: perturbing the decision loss gradient using the prediction loss gradient to construct an update direction. Our method requires no additional training and can be integrated with any DFL solvers. Using the sigmoid-like decaying parameter, we let the prediction loss gradient guide the decision loss gradient to train a predictive model that optimizes decision quality. Also, we provide a theoretical convergence guarantee to Pareto stationary point under mild assumptions. Empirically, we demonstrate our method across three stochastic optimization problems, showing promising results compared to other baselines. We validate that our approach achieves lower regret with more stable training, even in situations where either PFL or DFL struggles.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models</title>
<link>https://arxiv.org/abs/2509.08372</link>
<guid>https://arxiv.org/abs/2509.08372</guid>
<content:encoded><![CDATA[
<div> Federated Learning, FFREEDA, Class Imbalanced FFREEDA, Vision Foundation Model, Feature Extractors <br />
Summary: <br />
- The study focuses on Federated Source-Free Domain Adaptation (FFREEDA) and extends it to a more complex setting called Class Imbalanced FFREEDA (CI-FFREEDA).
- CI-FFREEDA considers class imbalances and label shifts in both the source and target domains.
- The research suggests that improving feature extractors within the network is more beneficial than enhancing aggregation and domain adaptation methods.
- By replacing the FFREEDA backbone with a frozen Vision Foundation Model (VFM), overall accuracy is improved without extensive parameter tuning, reducing computational and communication costs.
- Experimental results demonstrate that VFMs effectively address domain gaps, class imbalances, and non-IID-ness among target clients, emphasizing the importance of strong feature extractors in real-world Federated Learning. <div>
arXiv:2509.08372v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a framework for training models collaboratively while preserving data privacy of each client. Recently, research has focused on Federated Source-Free Domain Adaptation (FFREEDA), a more realistic scenario wherein client-held target domain data remains unlabeled, and the server can access source domain data only during pre-training. We extend this framework to a more complex and realistic setting: Class Imbalanced FFREEDA (CI-FFREEDA), which takes into account class imbalances in both the source and target domains, as well as label shifts between source and target and among target clients. The replication of existing methods in our experimental setup lead us to rethink the focus from enhancing aggregation and domain adaptation methods to improving the feature extractors within the network itself. We propose replacing the FFREEDA backbone with a frozen vision foundation model (VFM), thereby improving overall accuracy without extensive parameter tuning and reducing computational and communication costs in federated learning. Our experimental results demonstrate that VFMs effectively mitigate the effects of domain gaps, class imbalances, and even non-IID-ness among target clients, suggesting that strong feature extractors, not complex adaptation or FL methods, are key to success in the real-world FL.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Decoding Methods for Language Models on Encrypted Data</title>
<link>https://arxiv.org/abs/2509.08383</link>
<guid>https://arxiv.org/abs/2509.08383</guid>
<content:encoded><![CDATA[
<div> HE-friendly argmax algorithm, efficient stochastic decoding, privacy-preserving, polynomial, secure text generation <br />
Summary: 
The article introduces cutmax, an algorithm that enables practical greedy decoding under encryption for large language models. It also presents the first HE-compatible nucleus sampling method, allowing for efficient stochastic decoding with privacy guarantees. Both techniques are polynomial, supporting efficient inference in secure settings. Additionally, the algorithms are differentiable, enabling gradient-based sequence-level optimization. The article provides theoretical guarantees for cutmax, demonstrating its rapid convergence to a unique fixed point. Evaluations on realistic LLM outputs show significant latency reductions compared to baselines, advancing secure text generation. The combination of cutmax and nucleus sampling offers a promising solution for processing sensitive data on untrusted servers without compromising privacy. <div>
arXiv:2509.08383v1 Announce Type: new 
Abstract: Large language models (LLMs) power modern AI applications, but processing sensitive data on untrusted servers raises privacy concerns. Homomorphic encryption (HE) enables computation on encrypted data for secure inference. However, neural text generation requires decoding methods like argmax and sampling, which are non-polynomial and thus computationally expensive under encryption, creating a significant performance bottleneck. We introduce cutmax, an HE-friendly argmax algorithm that reduces ciphertext operations compared to prior methods, enabling practical greedy decoding under encryption. We also propose the first HE-compatible nucleus (top-p) sampling method, leveraging cutmax for efficient stochastic decoding with provable privacy guarantees. Both techniques are polynomial, supporting efficient inference in privacy-preserving settings. Moreover, their differentiability facilitates gradient-based sequence-level optimization as a polynomial alternative to straight-through estimators. We further provide strong theoretical guarantees for cutmax, proving it converges globally to a unique two-level fixed point, independent of the input values beyond the identity of the maximizer, which explains its rapid convergence in just a few iterations. Evaluations on realistic LLM outputs show latency reductions of 24x-35x over baselines, advancing secure text generation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Sides of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models</title>
<link>https://arxiv.org/abs/2509.08401</link>
<guid>https://arxiv.org/abs/2509.08401</guid>
<content:encoded><![CDATA[
<div> Graph foundation models, GFM, optimization, MoT, information bottleneck, regularization deficit
<br />
Summary:
MoT is proposed to address two pitfalls in GFM optimization: model degradation and representation collapse, attributed to information bottleneck and regularization deficit. Information Tinker for Two Pitfalls utilizes semantic fusion and a mixture-of-codebooks approach for improved information capacity. Regularization Tinker for Optimization Coin introduces additional regularizations to enhance gradient supervision. MoT adheres to scaling laws of GFM and offers controllable model scale. Experimental results on 22 datasets across 6 domains show significant improvements in supervised, few-shot, and zero-shot scenarios compared to state-of-the-art baselines. <div>
arXiv:2509.08401v1 Announce Type: new 
Abstract: Graph foundation models, inspired by the success of LLMs, are designed to learn the optimal embedding from multi-domain TAGs for the downstream cross-task generalization capability. During our investigation, graph VQ-MAE stands out among the increasingly diverse landscape of GFM architectures. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics</title>
<link>https://arxiv.org/abs/2509.08461</link>
<guid>https://arxiv.org/abs/2509.08461</guid>
<content:encoded><![CDATA[
<div> Vision Language Models, Neutrino Interactions, High-energy Physics, Event Classification, Interpretability
Summary: 
Vision Language Models (VLMs) show promise in identifying neutrino interactions in high-energy physics (HEP) experiments. A fine-tuned variant of LLaMa 3.2 proves effective in this task, outperforming traditional Convolutional Neural Networks (CNNs) used in HEP experiments like NOvA and DUNE. VLMs offer better classification performance and interpretability, allowing for integration of auxiliary information and providing reasoning-based predictions. Their flexibility and generalizability make them a suitable backbone for event classification in physics, enhancing multimodal reasoning in experimental neutrino physics. This study highlights the potential of VLMs in advancing event classification in the field of high-energy physics. 
<br /><br />Summary: <div>
arXiv:2509.08461v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated their remarkable capacity to process and reason over structured and unstructured data modalities beyond natural language. In this work, we explore the applications of Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa 3.2, to the task of identifying neutrino interactions in pixelated detector data from high-energy physics (HEP) experiments. We benchmark this model against a state-of-the-art convolutional neural network (CNN) architecture, similar to those used in the NOvA and DUNE experiments, which have achieved high efficiency and purity in classifying electron and muon neutrino events. Our evaluation considers both the classification performance and interpretability of the model predictions. We find that VLMs can outperform CNNs, while also providing greater flexibility in integrating auxiliary textual or semantic information and offering more interpretable, reasoning-based predictions. This work highlights the potential of VLMs as a general-purpose backbone for physics event classification, due to their high performance, interpretability, and generalizability, which opens new avenues for integrating multimodal reasoning in experimental neutrino physics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interpretable Deep Learning Model for General Insurance Pricing</title>
<link>https://arxiv.org/abs/2509.08467</link>
<guid>https://arxiv.org/abs/2509.08467</guid>
<content:encoded><![CDATA[
<div> neural network, insurance pricing, interpretability, predictive power, machine learning
Summary:<br /><br />This paper presents the Actuarial Neural Additive Model, a deep learning model designed for insurance pricing that offers interpretability and strong predictive power. The model assigns a neural network to each covariate and interaction term, allowing for independent learning of their impact while maintaining interpretability through architectural constraints like sparsity and smoothness. Concrete definitions of interpretability within the insurance context are established, supported by a rigorous mathematical framework. Comparison with traditional actuarial and machine learning methods using synthetic and real insurance datasets shows the model outperforms others in predictive accuracy while offering complete transparency in its logic, demonstrating both strong interpretability and predictive capability. <div>
arXiv:2509.08467v1 Announce Type: new 
Abstract: This paper introduces the Actuarial Neural Additive Model, an inherently interpretable deep learning model for general insurance pricing that offers fully transparent and interpretable results while retaining the strong predictive power of neural networks. This model assigns a dedicated neural network (or subnetwork) to each individual covariate and pairwise interaction term to independently learn its impact on the modeled output while implementing various architectural constraints to allow for essential interpretability (e.g. sparsity) and practical requirements (e.g. smoothness, monotonicity) in insurance applications. The development of our model is grounded in a solid foundation, where we establish a concrete definition of interpretability within the insurance context, complemented by a rigorous mathematical framework. Comparisons in terms of prediction accuracy are made with traditional actuarial and state-of-the-art machine learning methods using both synthetic and real insurance datasets. The results show that the proposed model outperforms other methods in most cases while offering complete transparency in its internal logic, underscoring the strong interpretability and predictive capability.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHAining on Process Mining: Explaining Event Log Characteristics Impact on Algorithms</title>
<link>https://arxiv.org/abs/2509.08482</link>
<guid>https://arxiv.org/abs/2509.08482</guid>
<content:encoded><![CDATA[
<div> algorithm, event log characteristics, process mining, metrics, SHAining 

Summary: 
The article introduces SHAining, a novel approach that quantifies the impact of event log characteristics on process mining algorithms metrics. Unlike previous work that evaluated algorithms on fixed real-world event logs, this approach focuses on the associational effects of overlapping characteristics on algorithm performance. By analyzing over 22,000 event logs, the study identifies which event log characteristics most significantly affect algorithm metrics such as fitness, precision, and complexity. Additionally, the research uncovers insights into how the value of event log characteristics correlates with their impact on algorithm performance, providing valuable information on algorithm robustness. Through this analysis, the study contributes to a better understanding of how varying event log characteristics influence the performance of process mining algorithms. <div>
arXiv:2509.08482v1 Announce Type: new 
Abstract: Process mining aims to extract and analyze insights from event logs, yet algorithm metric results vary widely depending on structural event log characteristics. Existing work often evaluates algorithms on a fixed set of real-world event logs but lacks a systematic analysis of how event log characteristics impact algorithms individually. Moreover, since event logs are generated from processes, where characteristics co-occur, we focus on associational rather than causal effects to assess how strong the overlapping individual characteristic affects evaluation metrics without assuming isolated causal effects, a factor often neglected by prior work. We introduce SHAining, the first approach to quantify the marginal contribution of varying event log characteristics to process mining algorithms' metrics. Using process discovery as a downstream task, we analyze over 22,000 event logs covering a wide span of characteristics to uncover which affect algorithms across metrics (e.g., fitness, precision, complexity) the most. Furthermore, we offer novel insights about how the value of event log characteristics correlates with their contributed impact, assessing the algorithm's robustness.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis</title>
<link>https://arxiv.org/abs/2509.08483</link>
<guid>https://arxiv.org/abs/2509.08483</guid>
<content:encoded><![CDATA[
<div> exponential decay, invariant manifold, memoryless approximation, combinatorics, optimization algorithms
<br />
Summary:
In this study, the authors analyze gradient descent with Polyak heavy-ball momentum (HB) and investigate its behavior on an exponentially attractive invariant manifold. They prove that with a small enough step size, the algorithm essentially reduces to plain gradient descent with a modified loss function. Despite the lack of a closed-form expression for the modified loss, the authors provide a detailed description and prove global approximation bounds for finite orders. By delving into the combinatorics underlying the memoryless approximations of HB, they uncover a variety of hidden polynomial families, including Eulerian and Narayana polynomials. Furthermore, continuous modified equations of various approximation orders are derived along with rigorous bounds, shedding new light on the intricacies of gradient descent with heavy-ball momentum. The results presented pave the way for similar analyses of other optimization algorithms in the future. <div>
arXiv:2509.08483v1 Announce Type: new 
Abstract: We analyze gradient descent with Polyak heavy-ball momentum (HB) whose fixed momentum parameter $\beta \in (0, 1)$ provides exponential decay of memory. Building on Kovachki and Stuart (2021), we prove that on an exponentially attractive invariant manifold the algorithm is exactly plain gradient descent with a modified loss, provided that the step size $h$ is small enough. Although the modified loss does not admit a closed-form expression, we describe it with arbitrary precision and prove global (finite "time" horizon) approximation bounds $O(h^{R})$ for any finite order $R \geq 2$. We then conduct a fine-grained analysis of the combinatorics underlying the memoryless approximations of HB, in particular, finding a rich family of polynomials in $\beta$ hidden inside which contains Eulerian and Narayana polynomials. We derive continuous modified equations of arbitrary approximation order (with rigorous bounds) and the principal flow that approximates the HB dynamics, generalizing Rosca et al. (2023). Approximation theorems cover both full-batch and mini-batch HB. Our theoretical results shed new light on the main features of gradient descent with heavy-ball momentum, and outline a road-map for similar analysis of other optimization algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heart Disease Prediction: A Comparative Study of Optimisers Performance in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2509.08499</link>
<guid>https://arxiv.org/abs/2509.08499</guid>
<content:encoded><![CDATA[
<div> Optimizers, Metrics, Comparison, Multi-layer Perceptron, Heart disease dataset  
Summary:  
- The study compares the performance of 10 optimizers in training a Multi-layer Perceptron model using a heart disease dataset from Kaggle.  
- Evaluation metrics include convergence speed, stability, AUC, Precision, and Recall.  
- Trade-offs between convergence speed and stability were observed, with RMSProp identified as the most effective optimizer for heart disease prediction.  
- RMSProp achieved a precision of 0.765, recall of 0.827, and AUC of 0.841, with faster training time.  
- In less compute-constrained environments, a thorough evaluation of optimizers should be adopted to enhance the scientific nature and performance of training deep learning models.  

<br /><br />Summary: <div>
arXiv:2509.08499v1 Announce Type: new 
Abstract: Optimization has been an important factor and topic of interest in training deep learning models, yet less attention has been given to how we select the optimizers we use to train these models. Hence, there is a need to dive deeper into how we select the optimizers we use for training and the metrics that determine this selection. In this work, we compare the performance of 10 different optimizers in training a simple Multi-layer Perceptron model using a heart disease dataset from Kaggle. We set up a consistent training paradigm and evaluate the optimizers based on metrics such as convergence speed and stability. We also include some other Machine Learning Evaluation metrics such as AUC, Precision, and Recall, which are central metrics to classification problems. Our results show that there are trade-offs between convergence speed and stability, as optimizers like Adagrad and Adadelta, which are more stable, took longer time to converge. Across all our metrics, we chose RMSProp to be the most effective optimizer for this heart disease prediction task because it offered a balanced performance across key metrics. It achieved a precision of 0.765, a recall of 0.827, and an AUC of 0.841, along with faster training time. However, it was not the most stable. We recommend that, in less compute-constrained environments, this method of choosing optimizers through a thorough evaluation should be adopted to increase the scientific nature and performance in training deep learning models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Rank Reduction Autoencoders for Generative</title>
<link>https://arxiv.org/abs/2509.08515</link>
<guid>https://arxiv.org/abs/2509.08515</guid>
<content:encoded><![CDATA[
<div> Variational Rank-Reduction Autoencoders, Deep Operator Networks, thermal design, generative models, engineering<br />
<br />
Summary: <br />
The study introduces a hybrid framework that combines Variational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks (DeepONets) to improve generative thermal design for complex geometries. The VRRAE enhances the quality of generated geometries by introducing a truncated SVD within the latent space, resulting in continuous and interpretable representations. This mitigates posterior collapse and improves geometric reconstruction. The DeepONet utilizes the compact latent encoding in its branch network along with spatial coordinates in the trunk network to predict temperature gradients with high accuracy and efficiency. The hybrid approach not only enhances design quality and gradient prediction accuracy but also offers significant advantages in inference efficiency compared to traditional numerical solvers. The study underscores the importance of structured latent representations for operator learning and highlights the potential of combining generative models and operator networks in thermal design and broader engineering applications. <div>
arXiv:2509.08515v1 Announce Type: new 
Abstract: Generative thermal design for complex geometries is fundamental in many areas of engineering, yet it faces two main challenges: the high computational cost of high-fidelity simulations and the limitations of conventional generative models. Approaches such as autoencoders (AEs) and variational autoencoders (VAEs) often produce unstructured latent spaces with discontinuities, which restricts their capacity to explore designs and generate physically consistent solutions.
  To address these limitations, we propose a hybrid framework that combines Variational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks (DeepONets). The VRRAE introduces a truncated SVD within the latent space, leading to continuous, interpretable, and well-structured representations that mitigate posterior collapse and improve geometric reconstruction. The DeepONet then exploits this compact latent encoding in its branch network, together with spatial coordinates in the trunk network, to predict temperature gradients efficiently and accurately.
  This hybrid approach not only enhances the quality of generated geometries and the accuracy of gradient prediction, but also provides a substantial advantage in inference efficiency compared to traditional numerical solvers. Overall, the study underscores the importance of structured latent representations for operator learning and highlights the potential of combining generative models and operator networks in thermal design and broader engineering applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Skeleton Learning: Scalable Active Clustering with Sparse Graph Structures</title>
<link>https://arxiv.org/abs/2509.08530</link>
<guid>https://arxiv.org/abs/2509.08530</guid>
<content:encoded><![CDATA[
<div> Efficiency, scalability, pairwise constraint-based active clustering, computational costs, user-provided constraints<br />
Summary:<br />
The study focuses on improving the efficiency and scalability of pairwise constraint-based active clustering for large-scale data processing. The goals include reducing computational costs for iterative clustering updates, enhancing the impact of user-provided constraints for precise clustering with fewer annotations, and reducing memory usage in practical applications. The proposed graph-based active clustering algorithm utilizes two sparse graphs to represent data relationships and update the data skeleton. By refining connected subgraphs within the data skeleton, nested clusters are created. Empirical analysis shows that the algorithm improves clustering accuracy with minimal user-provided constraints, outperforms existing methods in computational performance and scalability, and remains robust across different distance metrics.<br /><br />Summary: <div>
arXiv:2509.08530v1 Announce Type: new 
Abstract: In this work, we focus on the efficiency and scalability of pairwise constraint-based active clustering, crucial for processing large-scale data in applications such as data mining, knowledge annotation, and AI model pre-training. Our goals are threefold: (1) to reduce computational costs for iterative clustering updates; (2) to enhance the impact of user-provided constraints to minimize annotation requirements for precise clustering; and (3) to cut down memory usage in practical deployments. To achieve these aims, we propose a graph-based active clustering algorithm that utilizes two sparse graphs: one for representing relationships between data (our proposed data skeleton) and another for updating this data skeleton. These two graphs work in concert, enabling the refinement of connected subgraphs within the data skeleton to create nested clusters. Our empirical analysis confirms that the proposed algorithm consistently facilitates more accurate clustering with dramatically less input of user-provided constraints, and outperforms its counterparts in terms of computational performance and scalability, while maintaining robustness across various distance metrics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization</title>
<link>https://arxiv.org/abs/2509.08578</link>
<guid>https://arxiv.org/abs/2509.08578</guid>
<content:encoded><![CDATA[
<div> Keywords: influenza forecasting, multi-modal data fusion, spectro-temporal modeling, ensemble, robust optimization

Summary:
MAESTRO is a new model for influenza incidence forecasting that integrates multiple types of data sources and utilizes a sophisticated spectro-temporal architecture. The model decomposes time series into seasonal and trend components and processes them through a hybrid feature enhancement pipeline. By combining Transformer-based encoders, a Mamba state-space model, multi-scale temporal convolutions, and a frequency-domain analysis module, MAESTRO achieves a high level of accuracy in predicting influenza incidence. The model also includes a cross-channel attention mechanism for integrating information across different data modalities and a temporal projection head for sequence-to-sequence forecasting. The model performs competitively on 11 years of Hong Kong influenza data, with an R-square of 0.956. By making the pipeline publicly available, MAESTRO provides a powerful tool for robust epidemiological forecasting, highlighting the importance of advanced spectro-temporal modeling and multi-modal data fusion. 

<br /><br />Summary: <div>
arXiv:2509.08578v1 Announce Type: new 
Abstract: Timely and robust influenza incidence forecasting is critical for public health decision-making. To address this, we present MAESTRO, a Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves robustness by adaptively fusing multi-modal inputs-including surveillance, web search trends, and meteorological data-and leveraging a comprehensive spectro-temporal architecture. The model first decomposes time series into seasonal and trend components. These are then processed through a hybrid feature enhancement pipeline combining Transformer-based encoders, a Mamba state-space model for long-range dependencies, multi-scale temporal convolutions, and a frequency-domain analysis module. A cross-channel attention mechanism further integrates information across the different data modalities. Finally, a temporal projection head performs sequence-to-sequence forecasting, with an optional estimator to quantify prediction uncertainty. Evaluated on over 11 years of Hong Kong influenza data (excluding the COVID-19 period), MAESTRO shows strong competitive performance, demonstrating a superior model fit and relative accuracy, achieving a state-of-the-art R-square of 0.956. Extensive ablations confirm the significant contributions of both multi-modal fusion and the spectro-temporal components. Our modular and reproducible pipeline is made publicly available to facilitate deployment and extension to other regions and pathogens.Our publicly available pipeline presents a powerful, unified framework, demonstrating the critical synergy of advanced spectro-temporal modeling and multi-modal data fusion for robust epidemiological forecasting.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretability as Alignment: Making Internal Understanding a Design Principle</title>
<link>https://arxiv.org/abs/2509.08592</link>
<guid>https://arxiv.org/abs/2509.08592</guid>
<content:encoded><![CDATA[
<div> interpretability, neural models, alignment, mechanistic approaches, AI research <br />
<br />
Large neural models are being used in critical scenarios, highlighting the need to ensure that their behavior aligns with human values. The article argues that interpretability, especially through mechanistic approaches, should be a fundamental aspect of AI design to guarantee alignment with human values, rather than just a supplementary tool for analysis. While post-hoc methods like LIME offer correlational explanations, mechanistic techniques such as circuit tracing provide causal insights into internal failures like deceptive reasoning. The challenges faced by interpretability include scalability issues and mismatches between learned representations and human concepts. The article emphasizes the importance of integrating interpretability as a primary objective of AI research and development to ensure the transparency, auditability, and alignment of AI systems with human intent.<br /><br />Summary: <div>
arXiv:2509.08592v1 Announce Type: new 
Abstract: Large neural models are increasingly deployed in high-stakes settings, raising concerns about whether their behavior reliably aligns with human values. Interpretability provides a route to internal transparency by revealing the computations that drive outputs. We argue that interpretability especially mechanistic approaches should be treated as a design principle for alignment, not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer intuitive but correlational explanations, while mechanistic techniques like circuit tracing or activation patching yield causal insight into internal failures, including deceptive or misaligned reasoning that behavioral methods like RLHF, red teaming, or Constitutional AI may overlook. Despite these advantages, interpretability faces challenges of scalability, epistemic uncertainty, and mismatches between learned representations and human concepts. Our position is that progress on safe and trustworthy AI will depend on making interpretability a first-class objective of AI research and development, ensuring that systems are not only effective but also auditable, transparent, and aligned with human intent.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification of 24-hour movement behaviors from wrist-worn accelerometer data: from handcrafted features to deep learning techniques</title>
<link>https://arxiv.org/abs/2509.08606</link>
<guid>https://arxiv.org/abs/2509.08606</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, machine learning, classification, 24-hour movement behavior, accelerometer <br />
Summary: <br />
- Deep learning (DL) and classical machine learning (ML) algorithms were compared for classifying 24-hour movement behavior from accelerometer data.
- DL algorithms such as LSTM, BiLSTM, GRU, and 1D-CNN showed around 85% accuracy when trained with raw acceleration signals.
- When trained on handcrafted features, both DL and classical ML algorithms had overall accuracies ranging from 70% to 81%.
- There was higher confusion in classifying MVPA and LPA compared to sleep and sedentary categories.
- Overall, DL methods with raw acceleration signals only slightly outperformed when trained with handcrafted features in predicting 24-hour movement behavior intensities. <br /> 
Summary: <div>
arXiv:2509.08606v1 Announce Type: new 
Abstract: Purpose: We compared the performance of deep learning (DL) and classical machine learning (ML) algorithms for the classification of 24-hour movement behavior into sleep, sedentary, light intensity physical activity (LPA), and moderate-to-vigorous intensity physical activity (MVPA). Methods: Open-access data from 151 adults wearing a wrist-worn accelerometer (Axivity-AX3) was used. Participants were randomly divided into training, validation, and test sets (121, 15, and 15 participants each). Raw acceleration signals were segmented into non-overlapping 10-second windows, and then a total of 104 handcrafted features were extracted. Four DL algorithms-Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), Gated Recurrent Units (GRU), and One-Dimensional Convolutional Neural Network (1D-CNN)-were trained using raw acceleration signals and with handcrafted features extracted from these signals to predict 24-hour movement behavior categories. The handcrafted features were also used to train classical ML algorithms, namely Random Forest (RF), Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Logistic Regression (LR), Artificial Neural Network (ANN), and Decision Tree (DT) for classifying 24-hour movement behavior intensities. Results: LSTM, BiLSTM, and GRU showed an overall accuracy of approximately 85% when trained with raw acceleration signals, and 1D-CNN an overall accuracy of approximately 80%. When trained on handcrafted features, the overall accuracy for both DL and classical ML algorithms ranged from 70% to 81%. Overall, there was a higher confusion in classification of MVPA and LPA, compared to sleep and sedentary categories. Conclusion: DL methods with raw acceleration signals had only slightly better performance in predicting 24-hour movement behavior intensities, compared to when DL and classical ML were trained with handcrafted features.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Interpretable Deep Neural Networks for Tabular Data</title>
<link>https://arxiv.org/abs/2509.08617</link>
<guid>https://arxiv.org/abs/2509.08617</guid>
<content:encoded><![CDATA[
<div> Sparse autoencoder, tabular data, interpretable, neural network, predictive performance

Summary:
XNNTab is a novel neural architecture designed for tabular data analysis in fields like finance and healthcare. Unlike traditional black-box models, XNNTab utilizes a sparse autoencoder to learn a dictionary of monosemantic features within a latent space for prediction. These features are then assigned human-interpretable meanings through an automated method. This unique approach allows XNNTab to represent predictions as linear combinations of semantically meaningful components, providing full interpretability. Empirical evaluations show that XNNTab achieves competitive predictive performance comparable to state-of-the-art neural models and classical machine learning techniques. Through its interpretable nature, XNNTab bridges the gap between accuracy and transparency in tabular data analysis. <br /><br />Summary: <div>
arXiv:2509.08617v1 Announce Type: new 
Abstract: Tabular data is the foundation of many applications in fields such as finance and healthcare. Although DNNs tailored for tabular data achieve competitive predictive performance, they are blackboxes with little interpretability. We introduce XNNTab, a neural architecture that uses a sparse autoencoder (SAE) to learn a dictionary of monosemantic features within the latent space used for prediction. Using an automated method, we assign human-interpretable semantics to these features. This allows us to represent predictions as linear combinations of semantically meaningful components. Empirical evaluations demonstrate that XNNTab attains performance on par with or exceeding that of state-of-the-art, black-box neural models and classical machine learning approaches while being fully interpretable.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An upper bound of the silhouette validation metric for clustering</title>
<link>https://arxiv.org/abs/2509.08625</link>
<guid>https://arxiv.org/abs/2509.08625</guid>
<content:encoded><![CDATA[
<div> Keywords: silhouette coefficient, average silhouette width (ASW), clustering quality, upper bound, data-dependent

Summary:
The article introduces a method to calculate an upper bound on the silhouette width for each data point in a dataset. By aggregating these individual bounds, a canonical data-dependent upper bound on the average silhouette width (ASW) is derived. This bound helps determine how close a clustering result is to the best possible outcome for a specific dataset, with values often below 1. The bounds can indicate if individual data points can be well-placed, enable early termination of silhouette-based optimization loops, and enhance cluster quality evaluation. The article shows that the bounds are near-tight in many cases, offering significant enrichment in evaluating cluster quality across synthetic and real datasets. <div>
arXiv:2509.08625v1 Announce Type: new 
Abstract: The silhouette coefficient summarizes, per observation, cohesion versus separation in [-1, 1]; the average silhouette width (ASW) is a common internal measure of clustering quality where higher values indicate more coveted results. However, the dataset-specific maximum of ASW is typically unknown, and the standard upper limit 1 is often unattainable. In this work, we derive for each data point in a given dataset a sharp upper bound on its silhouette width. By aggregating these individual bounds, we present a canonical data-dependent upper bound on ASW that often assumes values well below 1. The presented bounds can indicate whether individual data points can ever be well placed, enable early stopping of silhouette-based optimization loops, and help answer a key question: How close is my clustering result to the best possible outcome on this specific data? Across synthetic and real datasets, the bounds are provably near-tight in many cases and offer significant enrichment of cluster quality evaluation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Data Refinement: Just Ask for Better Data</title>
<link>https://arxiv.org/abs/2509.08653</link>
<guid>https://arxiv.org/abs/2509.08653</guid>
<content:encoded><![CDATA[
<div> Generative Data Refinement, pretrained generative models, dataset transformation, dataset anonymization, synthetic data generation <br />
<br />
Summary: 
The article introduces the Generative Data Refinement (GDR) framework for transforming datasets with undesirable content into refined training data using pretrained generative models. With the exponential growth of training datasets surpassing indexed web data, GDR offers a solution to prevent data exhaustion by leveraging user-generated content. GDR not only outperforms industry-standard dataset anonymization methods but also enables detoxification of highly unsafe datasets. By generating synthetic data conditioned on real examples, GDR's refined outputs match the diversity of large-scale datasets, simplifying the generation of diverse synthetic data without model prompting. This simplicity and effectiveness make GDR a powerful tool for enhancing the training data stock for cutting-edge models. <div>
arXiv:2509.08653v1 Announce Type: new 
Abstract: For a fixed parameter size, the capabilities of large models are primarily determined by the quality and quantity of its training data. Consequently, training datasets now grow faster than the rate at which new data is indexed on the web, leading to projected data exhaustion over the next decade. Much more data exists as user-generated content that is not publicly indexed, but incorporating such data comes with considerable risks, such as leaking private information and other undesirable content. We introduce a framework, Generative Data Refinement (GDR), for using pretrained generative models to transform a dataset with undesirable content into a refined dataset that is more suitable for training. Our experiments show that GDR can outperform industry-grade solutions for dataset anonymization, as well as enable direct detoxification of highly unsafe datasets. Moreover, we show that by generating synthetic data that is conditioned on each example in the real dataset, GDR's refined outputs naturally match the diversity of web scale datasets, and thereby avoid the often challenging task of generating diverse synthetic data via model prompting. The simplicity and effectiveness of GDR make it a powerful tool for scaling up the total stock of training data for frontier models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replicable Reinforcement Learning with Linear Function Approximation</title>
<link>https://arxiv.org/abs/2509.08660</link>
<guid>https://arxiv.org/abs/2509.08660</guid>
<content:encoded><![CDATA[
<div> Replicable algorithms, machine learning, reinforcement learning, linear function approximation, experimental evaluation<br />
Summary:<br />
- The challenge of replicating experimental results in machine learning is addressed in this work.
- Provably replicable algorithms are developed for linear function approximation in reinforcement learning.
- Efficient algorithms for replicable random design regression and uncentered covariance estimation are introduced.
- The first provably efficient replicable RL algorithms for linear Markov decision processes are provided.
- Experimental evaluation of the algorithms demonstrates their effectiveness and potential to inspire more consistent neural policies.<br /> 
Summary: <div>
arXiv:2509.08660v1 Announce Type: new 
Abstract: Replication of experimental results has been a challenge faced by many scientific disciplines, including the field of machine learning. Recent work on the theory of machine learning has formalized replicability as the demand that an algorithm produce identical outcomes when executed twice on different samples from the same distribution. Provably replicable algorithms are especially interesting for reinforcement learning (RL), where algorithms are known to be unstable in practice. While replicable algorithms exist for tabular RL settings, extending these guarantees to more practical function approximation settings has remained an open problem. In this work, we make progress by developing replicable methods for linear function approximation in RL. We first introduce two efficient algorithms for replicable random design regression and uncentered covariance estimation, each of independent interest. We then leverage these tools to provide the first provably efficient replicable RL algorithms for linear Markov decision processes in both the generative model and episodic settings. Finally, we evaluate our algorithms experimentally and show how they can inspire more consistent neural policies.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal Fidelity Index-Aware Calibration for Dementia Predictions Across Heterogeneous Real-World Data</title>
<link>https://arxiv.org/abs/2509.08679</link>
<guid>https://arxiv.org/abs/2509.08679</guid>
<content:encoded><![CDATA[
<div> Keywords: EHR, machine learning, diagnostic signal decay, Signal Fidelity Index, model performance 

Summary:
Diagnostic signal decay in electronic health records (EHRs) can hinder the performance of machine learning models across healthcare systems. This study introduces a Signal Fidelity Index (SFI) to assess diagnostic data quality in dementia patients. Using a simulation framework with synthetic datasets, the SFI was derived from six factors and applied using SFI-aware calibration. Results showed that SFI-aware calibration significantly enhanced model performance without the need for outcome labels. The approach improved metrics such as Balanced Accuracy, Recall, Precision, and F1-score, with gains ranging from 10.3% to 32.5%. Performance enhancements brought model predictions closer to reference standards, with notable improvements in Balanced Accuracy and Detection Rate. This study highlights the importance of considering diagnostic signal decay and presents a practical strategy to enhance predictive models across diverse healthcare settings. <br /><br />Summary: <div>
arXiv:2509.08679v1 Announce Type: new 
Abstract: \textbf{Background:} Machine learning models trained on electronic health records (EHRs) often degrade across healthcare systems due to distributional shift. A fundamental but underexplored factor is diagnostic signal decay: variability in diagnostic quality and consistency across institutions, which affects the reliability of codes used for training and prediction.
  \textbf{Objective:} To develop a Signal Fidelity Index (SFI) quantifying diagnostic data quality at the patient level in dementia, and to test SFI-aware calibration for improving model performance across heterogeneous datasets without outcome labels.
  \textbf{Methods:} We built a simulation framework generating 2,500 synthetic datasets, each with 1,000 patients and realistic demographics, encounters, and coding patterns based on dementia risk factors. The SFI was derived from six interpretable components: diagnostic specificity, temporal consistency, entropy, contextual concordance, medication alignment, and trajectory stability. SFI-aware calibration applied a multiplicative adjustment, optimized across 50 simulation batches.
  \textbf{Results:} At the optimal parameter ($\alpha$ = 2.0), SFI-aware calibration significantly improved all metrics (p $<$ 0.001). Gains ranged from 10.3\% for Balanced Accuracy to 32.5\% for Recall, with notable increases in Precision (31.9\%) and F1-score (26.1\%). Performance approached reference standards, with F1-score and Recall within 1\% and Balanced Accuracy and Detection Rate improved by 52.3\% and 41.1\%, respectively.
  \textbf{Conclusions:} Diagnostic signal decay is a tractable barrier to model generalization. SFI-aware calibration provides a practical, label-free strategy to enhance prediction across healthcare contexts, particularly for large-scale administrative datasets lacking outcome labels.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perfectly-Private Analog Secure Aggregation in Federated Learning</title>
<link>https://arxiv.org/abs/2509.08683</link>
<guid>https://arxiv.org/abs/2509.08683</guid>
<content:encoded><![CDATA[
<div> federated learning, secure aggregation, multiparty computation, privacy, torus <br />
Summary: <br />
This paper introduces a novel secure parameter aggregation method for federated learning that utilizes the torus rather than a finite field to ensure perfect privacy for each party's data. The proposed method addresses the issue of information leakage in real-valued data by leveraging the uniform distribution on the torus, while also avoiding accuracy losses. Experimental results demonstrate that the torus-based protocol outperforms the finite field secure aggregation in terms of model accuracy and cosine similarity, making it a safer choice in some cases. By utilizing secure multiparty computation, the proposed method enhances privacy in federated learning without compromising model performance. <div>
arXiv:2509.08683v1 Announce Type: new 
Abstract: In federated learning, multiple parties train models locally and share their parameters with a central server, which aggregates them to update a global model. To address the risk of exposing sensitive data through local models, secure aggregation via secure multiparty computation has been proposed to enhance privacy. At the same time, perfect privacy can only be achieved by a uniform distribution of the masked local models to be aggregated. This raises a problem when working with real valued data, as there is no measure on the reals that is invariant under the masking operation, and hence information leakage is bound to occur. Shifting the data to a finite field circumvents this problem, but as a downside runs into an inherent accuracy complexity tradeoff issue due to fixed point modular arithmetic as opposed to floating point numbers that can simultaneously handle numbers of varying magnitudes. In this paper, a novel secure parameter aggregation method is proposed that employs the torus rather than a finite field. This approach guarantees perfect privacy for each party's data by utilizing the uniform distribution on the torus, while avoiding accuracy losses. Experimental results show that the new protocol performs similarly to the model without secure aggregation while maintaining perfect privacy. Compared to the finite field secure aggregation, the torus-based protocol can in some cases significantly outperform it in terms of model accuracy and cosine similarity, hence making it a safer choice.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reshaping the Forward-Forward Algorithm with a Similarity-Based Objective</title>
<link>https://arxiv.org/abs/2509.08697</link>
<guid>https://arxiv.org/abs/2509.08697</guid>
<content:encoded><![CDATA[
arXiv:2509.08697v1 Announce Type: new 
Abstract: Backpropagation is the pivotal algorithm underpinning the success of artificial neural networks, yet it has critical limitations such as biologically implausible backward locking and global error propagation. To circumvent these constraints, the Forward-Forward algorithm was proposed as a more biologically plausible method that replaces the backward pass with an additional forward pass. Despite this advantage, the Forward-Forward algorithm significantly trails backpropagation in accuracy, and its optimal form exhibits low inference efficiency due to multiple forward passes required. In this work, the Forward-Forward algorithm is reshaped through its integration with similarity learning frameworks, eliminating the need for multiple forward passes during inference. This proposed algorithm is named Forward-Forward Algorithm Unified with Similarity-based Tuplet loss (FAUST). Empirical evaluations on MNIST, Fashion-MNIST, and CIFAR-10 datasets indicate that FAUST substantially improves accuracy, narrowing the gap with backpropagation. On CIFAR-10, FAUST achieves 56.22\% accuracy with a simple multi-layer perceptron architecture, approaching the backpropagation benchmark of 57.63\% accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A layered architecture for log analysis in complex IT systems</title>
<link>https://arxiv.org/abs/2509.08698</link>
<guid>https://arxiv.org/abs/2509.08698</guid>
<content:encoded><![CDATA[
arXiv:2509.08698v1 Announce Type: new 
Abstract: In the evolving IT landscape, stability and reliability of systems are essential, yet their growing complexity challenges DevOps teams in implementation and maintenance. Log analysis, a core element of AIOps, provides critical insights into complex behaviors and failures. This dissertation introduces a three-layered architecture to support DevOps in failure resolution. The first layer, Log Investigation, performs autonomous log labeling and anomaly classification. We propose a method that labels log data without manual effort, enabling supervised training and precise evaluation of anomaly detection. Additionally, we define a taxonomy that groups anomalies into three categories, ensuring appropriate method selection. The second layer, Anomaly Detection, detects behaviors deviating from the norm. We propose a flexible Anomaly Detection method adaptable to unsupervised, weakly supervised, and supervised training. Evaluations on public and industry datasets show F1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third layer, Root Cause Analysis, identifies minimal log sets describing failures, their origin, and event sequences. By balancing training data and identifying key services, our Root Cause Analysis method consistently detects 90-98% of root cause log lines within the top 10 candidates, providing actionable insights for mitigation. Our research addresses how log analysis methods can be designed and optimized to help DevOps resolve failures efficiently. By integrating these three layers, the architecture equips teams with robust methods to enhance IT system reliability.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Based Prediction of Speech Arrest During Direct Cortical Stimulation Mapping</title>
<link>https://arxiv.org/abs/2509.08703</link>
<guid>https://arxiv.org/abs/2509.08703</guid>
<content:encoded><![CDATA[
arXiv:2509.08703v1 Announce Type: new 
Abstract: Identifying cortical regions critical for speech is essential for safe brain surgery in or near language areas. While Electrical Stimulation Mapping (ESM) remains the clinical gold standard, it is invasive and time-consuming. To address this, we analyzed intracranial electrocorticographic (ECoG) data from 16 participants performing speech tasks and developed machine learning models to directly predict if the brain region underneath each ECoG electrode is critical. Ground truth labels indicating speech arrest were derived independently from Electrical Stimulation Mapping (ESM) and used to train classification models. Our framework integrates neural activity signals, anatomical region labels, and functional connectivity features to capture both local activity and network-level dynamics. We found that models combining region and connectivity features matched the performance of the full feature set, and outperformed models using either type alone. To classify each electrode, trial-level predictions were aggregated using an MLP applied to histogram-encoded scores. Our best-performing model, a trial-level RBF-kernel Support Vector Machine together with MLP-based aggregation, achieved strong accuracy on held-out participants (ROC-AUC: 0.87, PR-AUC: 0.57). These findings highlight the value of combining spatial and network information with non-linear modeling to improve functional mapping in presurgical evaluation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Private Federated Learning in a Malicious Setting: A Scalable TEE-Based Approach with Client Auditing</title>
<link>https://arxiv.org/abs/2509.08709</link>
<guid>https://arxiv.org/abs/2509.08709</guid>
<content:encoded><![CDATA[
arXiv:2509.08709v1 Announce Type: new 
Abstract: In cross-device private federated learning, differentially private follow-the-regularized-leader (DP-FTRL) has emerged as a promising privacy-preserving method. However, existing approaches assume a semi-honest server and have not addressed the challenge of securely removing this assumption. This is due to its statefulness, which becomes particularly problematic in practical settings where clients can drop out or be corrupted. While trusted execution environments (TEEs) might seem like an obvious solution, a straightforward implementation can introduce forking attacks or availability issues due to state management. To address this problem, our paper introduces a novel server extension that acts as a trusted computing base (TCB) to realize maliciously secure DP-FTRL. The TCB is implemented with an ephemeral TEE module on the server side to produce verifiable proofs of server actions. Some clients, upon being selected, participate in auditing these proofs with small additional communication and computational demands. This extension solution reduces the size of the TCB while maintaining the system's scalability and liveness. We provide formal proofs based on interactive differential privacy, demonstrating privacy guarantee in malicious settings. Finally, we experimentally show that our framework adds small constant overhead to clients in several realistic settings.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing CNN models for resource-constrained systems by channel and layer pruning</title>
<link>https://arxiv.org/abs/2509.08714</link>
<guid>https://arxiv.org/abs/2509.08714</guid>
<content:encoded><![CDATA[
arXiv:2509.08714v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) have achieved significant breakthroughs in various fields. However, these advancements have led to a substantial increase in the complexity and size of these networks. This poses a challenge when deploying large and complex networks on edge devices. Consequently, model compression has emerged as a research field aimed at reducing the size and complexity of CNNs. One prominent technique in model compression is model pruning. This paper will present a new technique of pruning that combines both channel and layer pruning in what is called a "hybrid pruning framework". Inspired by EfficientNet, a renowned CNN architecture known for scaling up networks from both channel and layer perspectives, this hybrid approach applies the same principles but in reverse, where it scales down the network through pruning. Experiments on the hybrid approach demonstrated a notable decrease in the overall complexity of the model, with only a minimal reduction in accuracy compared to the baseline model. This complexity reduction translates into reduced latency when deploying the pruned models on an NVIDIA JETSON TX2 embedded AI device.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing</title>
<link>https://arxiv.org/abs/2509.08721</link>
<guid>https://arxiv.org/abs/2509.08721</guid>
<content:encoded><![CDATA[
arXiv:2509.08721v1 Announce Type: new 
Abstract: Post-training language models (LMs) with reinforcement learning (RL) can enhance their complex reasoning capabilities without supervised fine-tuning, as demonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs requires significant parallelization to scale-up inference, which introduces non-trivial technical challenges (e.g. latency, memory, and reliability) alongside ever-growing financial costs. We present Swarm sAmpling Policy Optimization (SAPO), a fully decentralized and asynchronous RL post-training algorithm. SAPO is designed for decentralized networks of heterogenous compute nodes, where each node manages its own policy model(s) while "sharing" rollouts with others in the network; no explicit assumptions about latency, model homogeneity, or hardware are required and nodes can operate in silo if desired. As a result, the algorithm avoids common bottlenecks in scaling RL post-training while also allowing (and even encouraging) new possibilities. By sampling rollouts "shared" across the network, it enables "Aha moments" to propagate, thereby bootstrapping the learning process. In this paper we show SAPO achieved cumulative reward gains of up to 94% in controlled experiments. We also share insights from tests on a network with thousands of nodes contributed by Gensyn community members running the algorithm on diverse hardware and models during an open-source demo.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven generative simulation of SDEs using diffusion models</title>
<link>https://arxiv.org/abs/2509.08731</link>
<guid>https://arxiv.org/abs/2509.08731</guid>
<content:encoded><![CDATA[
arXiv:2509.08731v1 Announce Type: new 
Abstract: This paper introduces a new approach to generating sample paths of unknown stochastic differential equations (SDEs) using diffusion models, a class of generative AI models commonly employed in image and video applications. Unlike the traditional Monte Carlo methods for simulating SDEs, which require explicit specifications of the drift and diffusion coefficients, our method takes a model-free, data-driven approach. Given a finite set of sample paths from an SDE, we utilize conditional diffusion models to generate new, synthetic paths of the same SDE. To demonstrate the effectiveness of our approach, we conduct a simulation experiment to compare our method with alternative benchmark ones including neural SDEs. Furthermore, in an empirical study we leverage these synthetically generated sample paths to enhance the performance of reinforcement learning algorithms for continuous-time mean-variance portfolio selection, hinting promising applications of diffusion models in financial analysis and decision-making.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEQuify your force field: More efficient simulations using deep equilibrium models</title>
<link>https://arxiv.org/abs/2509.08734</link>
<guid>https://arxiv.org/abs/2509.08734</guid>
<content:encoded><![CDATA[
arXiv:2509.08734v1 Announce Type: new 
Abstract: Machine learning force fields show great promise in enabling more accurate molecular dynamics simulations compared to manually derived ones. Much of the progress in recent years was driven by exploiting prior knowledge about physical systems, in particular symmetries under rotation, translation, and reflections. In this paper, we argue that there is another important piece of prior information that, thus fa,r hasn't been explored: Simulating a molecular system is necessarily continuous, and successive states are therefore extremely similar. Our contribution is to show that we can exploit this information by recasting a state-of-the-art equivariant base model as a deep equilibrium model. This allows us to recycle intermediate neural network features from previous time steps, enabling us to improve both accuracy and speed by $10\%-20\%$ on the MD17, MD22, and OC20 200k datasets, compared to the non-DEQ base model. The training is also much more memory efficient, allowing us to train more expressive models on larger systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System</title>
<link>https://arxiv.org/abs/2509.08736</link>
<guid>https://arxiv.org/abs/2509.08736</guid>
<content:encoded><![CDATA[
arXiv:2509.08736v1 Announce Type: new 
Abstract: The efficiency of Bayesian optimization (BO) in chemistry is often hindered by sparse experimental data and complex reaction mechanisms. To overcome these limitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced Multi-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization process is enhanced by LLMs and synergistically employs two strategies: knowledge-driven coarse-grained optimization and data-driven fine-grained optimization. First, in the knowledge-driven coarse-grained optimization stage, LLMs intelligently decompose the vast search space by reasoning over existing chemical knowledge to identify promising candidate regions. Subsequently, in the data-driven fine-grained optimization stage, LLMs enhance the BO process within these candidate regions by generating pseudo-data points, thereby improving data utilization efficiency and accelerating convergence. Benchmark evaluations** further confirm that ChemBOMAS significantly enhances optimization effectiveness and efficiency compared to various BO algorithms. Importantly, the practical utility of ChemBOMAS was validated through wet-lab experiments conducted under pharmaceutical industry protocols, targeting conditional optimization for a previously unreported and challenging chemical reaction. In the wet experiment, ChemBOMAS achieved an optimal objective value of 96%. This was substantially higher than the 15% achieved by domain experts. This real-world success, together with strong performance on benchmark evaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical discovery.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PracMHBench: Re-evaluating Model-Heterogeneous Federated Learning Based on Practical Edge Device Constraints</title>
<link>https://arxiv.org/abs/2509.08750</link>
<guid>https://arxiv.org/abs/2509.08750</guid>
<content:encoded><![CDATA[
arXiv:2509.08750v1 Announce Type: new 
Abstract: Federating heterogeneous models on edge devices with diverse resource constraints has been a notable trend in recent years. Compared to traditional federated learning (FL) that assumes an identical model architecture to cooperate, model-heterogeneous FL is more practical and flexible since the model can be customized to satisfy the deployment requirement. Unfortunately, no prior work ever dives into the existing model-heterogeneous FL algorithms under the practical edge device constraints and provides quantitative analysis on various data scenarios and metrics, which motivates us to rethink and re-evaluate this paradigm. In our work, we construct the first system platform \textbf{PracMHBench} to evaluate model-heterogeneous FL on practical constraints of edge devices, where diverse model heterogeneity algorithms are classified and tested on multiple data tasks and metrics. Based on the platform, we perform extensive experiments on these algorithms under the different edge constraints to observe their applicability and the corresponding heterogeneity pattern.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.08755</link>
<guid>https://arxiv.org/abs/2509.08755</guid>
<content:encoded><![CDATA[
arXiv:2509.08755v1 Announce Type: new 
Abstract: Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform</title>
<link>https://arxiv.org/abs/2509.08756</link>
<guid>https://arxiv.org/abs/2509.08756</guid>
<content:encoded><![CDATA[
arXiv:2509.08756v1 Announce Type: new 
Abstract: Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid, accurate patient-hospital allocation decisions under extreme pressure. Here, we developed and validated a deep reinforcement learning-based decision-support AI agent to optimize patient transfer decisions during simulated MCIs by balancing patient acuity levels, specialized care requirements, hospital capacities, and transport logistics. To integrate this AI agent, we developed MasTER, a web-accessible command dashboard for MCI management simulations. Through a controlled user study with 30 participants (6 trauma experts and 24 non-experts), we evaluated three interaction approaches with the AI agent (human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI scenarios in the Greater Toronto Area. Results demonstrate that increasing AI involvement significantly improves decision quality and consistency. The AI agent outperforms trauma surgeons (p < 0.001) and enables non-experts to achieve expert-level performance when assisted, contrasting sharply with their significantly inferior unassisted performance (p < 0.001). These findings establish the potential for our AI-driven decision support to enhance both MCI preparedness training and real-world emergency response management.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier Learning Machines: Nonharmonic Fourier-Based Neural Networks for Scientific Machine Learning</title>
<link>https://arxiv.org/abs/2509.08759</link>
<guid>https://arxiv.org/abs/2509.08759</guid>
<content:encoded><![CDATA[
arXiv:2509.08759v1 Announce Type: new 
Abstract: We introduce the Fourier Learning Machine (FLM), a neural network (NN) architecture designed to represent a multidimensional nonharmonic Fourier series. The FLM uses a simple feedforward structure with cosine activation functions to learn the frequencies, amplitudes, and phase shifts of the series as trainable parameters. This design allows the model to create a problem-specific spectral basis adaptable to both periodic and nonperiodic functions. Unlike previous Fourier-inspired NN models, the FLM is the first architecture able to represent a complete, separable Fourier basis in multiple dimensions using a standard Multilayer Perceptron-like architecture. A one-to-one correspondence between the Fourier coefficients and amplitudes and phase-shifts is demonstrated, allowing for the translation between a full, separable basis form and the cosine phase--shifted one. Additionally, we evaluate the performance of FLMs on several scientific computing problems, including benchmark Partial Differential Equations (PDEs) and a family of Optimal Control Problems (OCPs). Computational experiments show that the performance of FLMs is comparable, and often superior, to that of established architectures like SIREN and vanilla feedforward NNs.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through Temporal-Spatial Processing, Adaptive Attention Mechanisms, and Explainability in Raw EEG Signals</title>
<link>https://arxiv.org/abs/2509.08779</link>
<guid>https://arxiv.org/abs/2509.08779</guid>
<content:encoded><![CDATA[
arXiv:2509.08779v1 Announce Type: new 
Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in children that can persist into adulthood, affecting social, academic, and career life. Early diagnosis is crucial for managing these impacts on patients and the healthcare system but is often labor-intensive and time-consuming. This paper presents a novel method to improve ADHD diagnosis precision and timeliness by leveraging Deep Learning (DL) approaches and electroencephalogram (EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive temporal-spatial characterization, attention modules, and explainability techniques optimized for EEG signals. ADHDeepNet integrates feature extraction and refinement processes to enhance ADHD diagnosis. The model was trained and validated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls), employing nested cross-validation for robust performance. The proposed two-stage methodology uses a 10-fold cross-subject validation strategy. Initially, each iteration optimizes the model's hyper-parameters with inner 2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various standard deviations and magnification levels is applied for data augmentation. ADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC subjects. To clarify model explainability and identify key brain regions and frequency bands for ADHD diagnosis, we analyzed the learned weights and activation patterns of the model's primary layers. Additionally, t-distributed Stochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding in interpreting the model's decisions. This study highlights the potential of DL and EEG in enhancing ADHD diagnosis accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merge-of-Thought Distillation</title>
<link>https://arxiv.org/abs/2509.08814</link>
<guid>https://arxiv.org/abs/2509.08814</guid>
<content:encoded><![CDATA[
arXiv:2509.08814v1 Announce Type: new 
Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is increasingly constrained by the assumption of a single oracle teacher, despite practical availability of multiple candidate teachers and growing CoT corpora. We revisit teacher selection and observe that different students have different "best teachers," and even for the same student the best teacher can vary across datasets. Therefore, to unify multiple teachers' reasoning abilities into student with overcoming conflicts among various teachers' supervision, we propose Merge-of-Thought Distillation (MoT), a lightweight framework that alternates between teacher-specific supervised fine-tuning branches and weight-space merging of the resulting student variants. On competition math benchmarks, using only about 200 high-quality CoT samples, applying MoT to a Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B, QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT consistently outperforms the best single-teacher distillation and the naive multi-teacher union, raises the performance ceiling while mitigating overfitting, and shows robustness to distribution-shifted and peer-level teachers. Moreover, MoT reduces catastrophic forgetting, improves general reasoning beyond mathematics and even cultivates a better teacher, indicating that consensus-filtered reasoning features transfer broadly. These results position MoT as a simple, scalable route to efficiently distilling long CoT capabilities from diverse teachers into compact students.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of TinyML Applications in Beekeeping for Hive Monitoring and Management</title>
<link>https://arxiv.org/abs/2509.08822</link>
<guid>https://arxiv.org/abs/2509.08822</guid>
<content:encoded><![CDATA[
arXiv:2509.08822v1 Announce Type: new 
Abstract: Honey bee colonies are essential for global food security and ecosystem stability, yet they face escalating threats from pests, diseases, and environmental stressors. Traditional hive inspections are labor-intensive and disruptive, while cloud-based monitoring solutions remain impractical for remote or resource-limited apiaries. Recent advances in Internet of Things (IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring directly on edge devices, offering scalable and non-invasive alternatives. This survey synthesizes current innovations at the intersection of TinyML and apiculture, organized around four key functional areas: monitoring hive conditions, recognizing bee behaviors, detecting pests and diseases, and forecasting swarming events. We further examine supporting resources, including publicly available datasets, lightweight model architectures optimized for embedded deployment, and benchmarking strategies tailored to field constraints. Critical limitations such as data scarcity, generalization challenges, and deployment barriers in off-grid environments are highlighted, alongside emerging opportunities in ultra-efficient inference pipelines, adaptive edge learning, and dataset standardization. By consolidating research and engineering practices, this work provides a foundation for scalable, AI-driven, and ecologically informed monitoring systems to support sustainable pollinator management.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToDMA: Large Model-Driven Token-Domain Multiple Access for Semantic Communications</title>
<link>https://arxiv.org/abs/2505.10946</link>
<guid>https://arxiv.org/abs/2505.10946</guid>
<content:encoded><![CDATA[
arXiv:2505.10946v2 Announce Type: cross 
Abstract: Token communications (TokCom) is an emerging generative semantic communication concept that reduces transmission rates by using context and multimodal large language model (MLLM)-based token processing, with tokens serving as universal semantic units across modalities. In this paper, we propose a semantic multiple access scheme in the token domain, referred to as token domain multiple access (ToDMA), where a large number of devices share a token codebook and a modulation codebook for source and channel coding, respectively. Specifically, each transmitter first tokenizes its source signal and modulate each token to a codeword. At the receiver, compressed sensing is employed first to detect active tokens and the corresponding channel state information (CSI) from the superposed signals. Then, the source token sequences are reconstructed by clustering the token-associated CSI across multiple time slots. In case of token collisions, some active tokens cannot be assigned and some positions in the reconstructed token sequences are empty. We propose to use pre-trained MLLMs to leverage the context, predict masked tokens, and thus mitigate token collisions. Simulation results demonstrate the effectiveness of the proposed ToDMA framework for both text and image transmission tasks, achieving significantly lower latency compared to context-unaware orthogonal communication schemes, while also delivering superior distortion and perceptual quality compared to state-of-the-art context-unaware non-orthogonal communication methods.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Protein Language Models</title>
<link>https://arxiv.org/abs/2509.07983</link>
<guid>https://arxiv.org/abs/2509.07983</guid>
<content:encoded><![CDATA[
arXiv:2509.07983v1 Announce Type: cross 
Abstract: Protein Language Models (PLMs), pre-trained on extensive evolutionary data from natural proteins, have emerged as indispensable tools for protein design. While powerful, PLMs often struggle to produce proteins with precisely specified functionalities or properties due to inherent challenges in controlling their outputs. In this work, we investigate the potential of Activation Steering, a technique originally developed for controlling text generation in Large Language Models (LLMs), to direct PLMs toward generating protein sequences with targeted properties. We propose a simple yet effective method that employs activation editing to steer PLM outputs, and extend this approach to protein optimization through a novel editing site identification module. Through comprehensive experiments on lysozyme-like sequence generation and optimization, we demonstrate that our methods can be seamlessly integrated into both auto-encoding and autoregressive PLMs without requiring additional training. These results highlight a promising direction for precise protein engineering using foundation models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction</title>
<link>https://arxiv.org/abs/2509.07990</link>
<guid>https://arxiv.org/abs/2509.07990</guid>
<content:encoded><![CDATA[
arXiv:2509.07990v1 Announce Type: cross 
Abstract: Human-robot collaboration (HRC) in the construction industry depends on precise and prompt recognition of human motion intentions and actions by robots to maximize safety and workflow efficiency. There is a research gap in comparing data modalities, specifically signals and videos, for motion intention recognition. To address this, the study leverages deep learning to assess two different modalities in recognizing workers' motion intention at the early stage of movement in drywall installation tasks. The Convolutional Neural Network - Long Short-Term Memory (CNN-LSTM) model utilizing surface electromyography (sEMG) data achieved an accuracy of around 87% with an average time of 0.04 seconds to perform prediction on a sample input. Meanwhile, the pre-trained Video Swin Transformer combined with transfer learning harnessed video sequences as input to recognize motion intention and attained an accuracy of 94% but with a longer average time of 0.15 seconds for a similar prediction. This study emphasizes the unique strengths and trade-offs of both data formats, directing their systematic deployments to enhance HRC in real-world construction projects.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLGE: Dual Local-Global Encoding for Generalizable Cross-BCI-Paradigm</title>
<link>https://arxiv.org/abs/2509.07991</link>
<guid>https://arxiv.org/abs/2509.07991</guid>
<content:encoded><![CDATA[
arXiv:2509.07991v1 Announce Type: cross 
Abstract: Deep learning models have been frequently used to decode a single brain-computer interface (BCI) paradigm based on electroencephalography (EEG). It is challenging to decode multiple BCI paradigms using one model due to diverse barriers, such as different channel configurations and disparate task-related representations. In this study, we propose Dual Local-Global Encoder (DLGE), enabling the classification across different BCI paradigms. To address the heterogeneity in EEG channel configurations across paradigms, we employ an anatomically inspired brain-region partitioning and padding strategy to standardize EEG channel configuration. In the proposed model, the local encoder is designed to learn shared features across BCI paradigms within each brain region based on time-frequency information, which integrates temporal attention on individual channels with spatial attention among channels for each brain region. These shared features are subsequently aggregated in the global encoder to form respective paradigm-specific feature representations. Three BCI paradigms (motor imagery, resting state, and driving fatigue) were used to evaluate the proposed model. The results demonstrate that our model is capable of processing diverse BCI paradigms without retraining and retuning, achieving average macro precision, recall, and F1-score of 60.16\%, 59.88\%, and 59.56\%, respectively. We made an initial attempt to develop a general model for cross-BCI-paradigm classification, avoiding retraining or redevelopment for each paradigm. This study paves the way for the development of an effective but simple model for cross-BCI-paradigm decoding, which might benefit the design of portable devices for universal BCI decoding.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking Stroke Recovery</title>
<link>https://arxiv.org/abs/2509.07994</link>
<guid>https://arxiv.org/abs/2509.07994</guid>
<content:encoded><![CDATA[
arXiv:2509.07994v1 Announce Type: cross 
Abstract: Despite advancements in rehabilitation protocols, clinical assessment of upper extremity (UE) function after stroke largely remains subjective, relying heavily on therapist observation and coarse scoring systems. This subjectivity limits the sensitivity of assessments to detect subtle motor improvements, which are critical for personalized rehabilitation planning. Recent progress in computer vision offers promising avenues for enabling objective, quantitative, and scalable assessment of UE motor function. Among standardized tests, the Box and Block Test (BBT) is widely utilized for measuring gross manual dexterity and tracking stroke recovery, providing a structured setting that lends itself well to computational analysis. However, existing datasets targeting stroke rehabilitation primarily focus on daily living activities and often fail to capture clinically structured assessments such as block transfer tasks. Furthermore, many available datasets include a mixture of healthy and stroke-affected individuals, limiting their specificity and clinical utility. To address these critical gaps, we introduce StrokeVision-Bench, the first-ever dedicated dataset of stroke patients performing clinically structured block transfer tasks. StrokeVision-Bench comprises 1,000 annotated videos categorized into four clinically meaningful action classes, with each sample represented in two modalities: raw video frames and 2D skeletal keypoints. We benchmark several state-of-the-art video action recognition and skeleton-based action classification methods to establish performance baselines for this domain and facilitate future research in automated stroke rehabilitation assessment.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Contagion in Financial Labor Markets: Predicting Turnover in Hong Kong</title>
<link>https://arxiv.org/abs/2509.08001</link>
<guid>https://arxiv.org/abs/2509.08001</guid>
<content:encoded><![CDATA[
arXiv:2509.08001v1 Announce Type: cross 
Abstract: Employee turnover is a critical challenge in financial markets, yet little is known about the role of professional networks in shaping career moves. Using the Hong Kong Securities and Futures Commission (SFC) public register (2007-2024), we construct temporal networks of 121,883 professionals and 4,979 firms to analyze and predict employee departures. We introduce a graph-based feature propagation framework that captures peer influence and organizational stability. Our analysis shows a contagion effect: professionals are 23% more likely to leave when over 30% of their peers depart within six months. Embedding these network signals into machine learning models improves turnover prediction by 30% over baselines. These results highlight the predictive power of temporal network effects in workforce dynamics, and demonstrate how network-based analytics can inform regulatory monitoring, talent management, and systemic risk assessment.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioComposer: Flexible and Compositional Anatomical Structure Generation with Disentangled Geometric Guidance</title>
<link>https://arxiv.org/abs/2509.08015</link>
<guid>https://arxiv.org/abs/2509.08015</guid>
<content:encoded><![CDATA[
arXiv:2509.08015v1 Announce Type: cross 
Abstract: Generative models of 3D anatomy, when integrated with biophysical simulators, enable the study of structure-function relationships for clinical research and medical device design. However, current models face a trade-off between controllability and anatomical realism. We propose a programmable and compositional framework for guiding unconditional diffusion models of human anatomy using interpretable ellipsoidal primitives embedded in 3D space. Our method involves the selection of certain tissues within multi-tissue segmentation maps, upon which we apply geometric moment losses to guide the reverse diffusion process. This framework supports the independent control over size, shape, and position, as well as the composition of multi-component constraints during inference.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs</title>
<link>https://arxiv.org/abs/2509.08016</link>
<guid>https://arxiv.org/abs/2509.08016</guid>
<content:encoded><![CDATA[
arXiv:2509.08016v1 Announce Type: cross 
Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck: increasing the number of input frames to capture fine-grained temporal detail leads to prohibitive computational costs and performance degradation from long context lengths. We introduce Video Parallel Scaling (VPS), an inference-time method that expands a model's perceptual bandwidth without increasing its context window. VPS operates by running multiple parallel inference streams, each processing a unique, disjoint subset of the video's frames. By aggregating the output probabilities from these complementary streams, VPS integrates a richer set of visual information than is possible with a single pass. We theoretically show that this approach effectively contracts the Chinchilla scaling law by leveraging uncorrelated visual evidence, thereby improving performance without additional training. Extensive experiments across various model architectures and scales (2B-32B) on benchmarks such as Video-MME and EventHallusion demonstrate that VPS consistently and significantly improves performance. It scales more favorably than other parallel alternatives (e.g. Self-consistency) and is complementary to other decoding strategies, offering a memory-efficient and robust framework for enhancing the temporal reasoning capabilities of VideoLLMs.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Privacy Preservation and Reducing Analysis Time with Federated Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis</title>
<link>https://arxiv.org/abs/2509.08018</link>
<guid>https://arxiv.org/abs/2509.08018</guid>
<content:encoded><![CDATA[
arXiv:2509.08018v1 Announce Type: cross 
Abstract: The application of Digital Twin (DT) technology and Federated Learning (FL) has great potential to change the field of biomedical image analysis, particularly for Computed Tomography (CT) scans. This paper presents Federated Transfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm. FTL uses pre-trained models and knowledge transfer between peer nodes to solve problems such as data privacy, limited computing resources, and data heterogeneity. The proposed framework allows real-time collaboration between cloud servers and Digital Twin-enabled CT scanners while protecting patient identity. We apply the FTL method to a heterogeneous CT scan dataset and assess model performance using convergence time, model accuracy, precision, recall, F1 score, and confusion matrix. It has been shown to perform better than conventional FL and Clustered Federated Learning (CFL) methods with better precision, accuracy, recall, and F1-score. The technique is beneficial in settings where the data is not independently and identically distributed (non-IID), and it offers reliable, efficient, and secure solutions for medical diagnosis. These findings highlight the possibility of using FTL to improve decision-making in digital twin-based CT scan analysis, secure and efficient medical image analysis, promote privacy, and open new possibilities for applying precision medicine and smart healthcare systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery</title>
<link>https://arxiv.org/abs/2509.08027</link>
<guid>https://arxiv.org/abs/2509.08027</guid>
<content:encoded><![CDATA[
arXiv:2509.08027v1 Announce Type: cross 
Abstract: This work presents a new dataset for the Martian digital elevation model prediction task, ready for machine learning applications called MCTED. The dataset has been generated using a comprehensive pipeline designed to process high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a dataset consisting of 80,898 data samples. The source images are data gathered by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very diverse and comprehensive coverage of the Martian surface. Given the complexity of the processing pipelines used in large-scale DEMs, there are often artefacts and missing data points in the original data, for which we developed tools to solve or mitigate their impact. We divide the processed samples into training and validation splits, ensuring samples in both splits cover no mutual areas to avoid data leakage. Every sample in the dataset is represented by the optical image patch, DEM patch, and two mask patches, indicating values that were originally missing or were altered by us. This allows future users of the dataset to handle altered elevation regions as they please. We provide statistical insights of the generated dataset, including the spatial distribution of samples, the distributions of elevation values, slopes and more. Finally, we train a small U-Net architecture on the MCTED dataset and compare its performance to a monocular depth estimation foundation model, DepthAnythingV2, on the task of elevation prediction. We find that even a very small architecture trained on this dataset specifically, beats a zero-shot performance of a depth estimation foundation model like DepthAnythingV2. We make the dataset and code used for its generation completely open source in public repositories.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LALM-Eval: An Open-Source Toolkit for Holistic Evaluation of Large Audio Language Models</title>
<link>https://arxiv.org/abs/2509.08031</link>
<guid>https://arxiv.org/abs/2509.08031</guid>
<content:encoded><![CDATA[
arXiv:2509.08031v1 Announce Type: cross 
Abstract: Large Audio Language Models (LALMs) are rapidly advancing, but evaluating them remains challenging due to inefficient toolkits that limit fair comparison and systematic assessment. Current frameworks suffer from three critical issues: slow processing that bottlenecks large-scale studies, inconsistent prompting that hurts reproducibility, and narrow task coverage that misses important audio reasoning capabilities. We introduce LALM-Eval, an efficient and comprehensive evaluation framework for LALMs. Our system achieves a speedup of up to 127% over existing toolkits through optimized batch processing and parallel execution, enabling large-scale evaluations previously impractical. We provide standardized prompting protocols and flexible configurations for fair model comparison across diverse scenarios. Additionally, we introduce two new evaluation categories: LLM-Adaptive Diarization for temporal audio understanding and Spoken Language Reasoning for complex audio-based cognitive tasks. Through evaluation across 380+ tasks, we reveal significant gaps in current LALMs, particularly in temporal understanding and complex spoken language reasoning tasks. Our findings also highlight a lack of standardization in instruction modality existent across audio benchmarks, which can lead up performance differences up to 9.5 absolute points on the challenging complex instruction following downstream tasks. LALM-Eval provides both practical evaluation tools and insights into model limitations, advancing systematic LALM development.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Generative Amplification</title>
<link>https://arxiv.org/abs/2509.08048</link>
<guid>https://arxiv.org/abs/2509.08048</guid>
<content:encoded><![CDATA[
arXiv:2509.08048v1 Announce Type: cross 
Abstract: Generative networks are perfect tools to enhance the speed and precision of LHC simulations. It is important to understand their statistical precision, especially when generating events beyond the size of the training dataset. We present two complementary methods to estimate the amplification factor without large holdout datasets. Averaging amplification uses Bayesian networks or ensembling to estimate amplification from the precision of integrals over given phase-space volumes. Differential amplification uses hypothesis testing to quantify amplification without any resolution loss. Applied to state-of-the-art event generators, both methods indicate that amplification is possible in specific regions of phase space, but not yet across the entire distribution.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCA-LLM: Spectral-Attentive Channel Prediction with Large Language Models in MIMO-OFDM</title>
<link>https://arxiv.org/abs/2509.08139</link>
<guid>https://arxiv.org/abs/2509.08139</guid>
<content:encoded><![CDATA[
arXiv:2509.08139v1 Announce Type: cross 
Abstract: In recent years, the success of large language models (LLMs) has inspired growing interest in exploring their potential applications in wireless communications, especially for channel prediction tasks. However, directly applying LLMs to channel prediction faces a domain mismatch issue stemming from their text-based pre-training. To mitigate this, the ``adapter + LLM" paradigm has emerged, where an adapter is designed to bridge the domain gap between the channel state information (CSI) data and LLMs. While showing initial success, existing adapters may not fully exploit the potential of this paradigm. To address this limitation, this work provides a key insight that learning representations from the spectral components of CSI features can more effectively help bridge the domain gap. Accordingly, we propose a spectral-attentive framework, named SCA-LLM, for channel prediction in multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) systems. Specifically, its novel adapter can capture finer spectral details and better adapt the LLM for channel prediction than previous methods. Extensive simulations show that SCA-LLM achieves state-of-the-art prediction performance and strong generalization, yielding up to $-2.4~\text{dB}$ normalized mean squared error (NMSE) advantage over the previous LLM based method. Ablation studies further confirm the superiority of SCA-LLM in mitigating domain mismatch.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias after Prompting: Persistent Discrimination in Large Language Models</title>
<link>https://arxiv.org/abs/2509.08146</link>
<guid>https://arxiv.org/abs/2509.08146</guid>
<content:encoded><![CDATA[
arXiv:2509.08146v1 Announce Type: cross 
Abstract: A dangerous assumption that can be made from prior work on the bias transfer hypothesis (BTH) is that biases do not transfer from pre-trained large language models (LLMs) to adapted models. We invalidate this assumption by studying the BTH in causal models under prompt adaptations, as prompting is an extremely popular and accessible adaptation strategy used in real-world applications. In contrast to prior work, we find that biases can transfer through prompting and that popular prompt-based mitigation methods do not consistently prevent biases from transferring. Specifically, the correlation between intrinsic biases and those after prompt adaptation remain moderate to strong across demographics and tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age (rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we find that biases remain strongly correlated when varying few-shot composition parameters, such as sample size, stereotypical content, occupational distribution and representational balance (rho >= 0.90). We evaluate several prompt-based debiasing strategies and find that different approaches have distinct strengths, but none consistently reduce bias transfer across models, tasks or demographics. These results demonstrate that correcting bias, and potentially improving reasoning ability, in intrinsic models may prevent propagation of biases to downstream tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contributions to Robust and Efficient Methods for Analysis of High Dimensional Data</title>
<link>https://arxiv.org/abs/2509.08155</link>
<guid>https://arxiv.org/abs/2509.08155</guid>
<content:encoded><![CDATA[
arXiv:2509.08155v1 Announce Type: cross 
Abstract: A ubiquitous feature of data of our era is their extra-large sizes and dimensions. Analyzing such high-dimensional data poses significant challenges, since the feature dimension is often much larger than the sample size. This thesis introduces robust and computationally efficient methods to address several common challenges associated with high-dimensional data. In my first manuscript, I propose a coherent approach to variable screening that accommodates nonlinear associations. I develop a novel variable screening method that transcends traditional linear assumptions by leveraging mutual information, with an intended application in neuroimaging data. This approach allows for accurate identification of important variables by capturing nonlinear as well as linear relationships between the outcome and covariates. Building on this foundation, I develop new optimization methods for sparse estimation using nonconvex penalties in my second manuscript. These methods address notable challenges in current statistical computing practices, facilitating computationally efficient and robust analyses of complex datasets. The proposed method can be applied to a general class of optimization problems. In my third manuscript, I contribute to robust modeling of high-dimensional correlated observations by developing a mixed-effects model based on Tsallis power-law entropy maximization and discussed the theoretical properties of such distribution. This model surpasses the constraints of conventional Gaussian models by accommodating a broader class of distributions with enhanced robustness to outliers. Additionally, I develop a proximal nonlinear conjugate gradient algorithm that accelerates convergence while maintaining numerical stability, along with rigorous statistical properties for the proposed framework.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OCTANE -- Optimal Control for Tensor-based Autoencoder Network Emergence: Explicit Case</title>
<link>https://arxiv.org/abs/2509.08169</link>
<guid>https://arxiv.org/abs/2509.08169</guid>
<content:encoded><![CDATA[
arXiv:2509.08169v1 Announce Type: cross 
Abstract: This paper presents a novel, mathematically rigorous framework for autoencoder-type deep neural networks that combines optimal control theory and low-rank tensor methods to yield memory-efficient training and automated architecture discovery. The learning task is formulated as an optimization problem constrained by differential equations representing the encoder and decoder components of the network and the corresponding optimality conditions are derived via a Lagrangian approach. Efficient memory compression is enabled by approximating differential equation solutions on low-rank tensor manifolds using an adaptive explicit integration scheme. These concepts are combined to form OCTANE (Optimal Control for Tensor-based Autoencoder Network Emergence) -- a unified training framework that yields compact autoencoder architectures, reduces memory usage, and enables effective learning, even with limited training data. The framework's utility is illustrated with application to image denoising and deblurring tasks and recommendations regarding governing hyperparameters are provided.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAPID Quantum Detection and Demodulation of Covert Communications: Breaking the Noise Limit with Solid-State Spin Sensors</title>
<link>https://arxiv.org/abs/2509.08171</link>
<guid>https://arxiv.org/abs/2509.08171</guid>
<content:encoded><![CDATA[
arXiv:2509.08171v1 Announce Type: cross 
Abstract: We introduce a comprehensive framework for the detection and demodulation of covert electromagnetic signals using solid-state spin sensors. Our approach, named RAPID, is a two-stage hybrid strategy that leverages nitrogen-vacancy (NV) centers to operate below the classical noise floor employing a robust adaptive policy via imitation and distillation. We first formulate the joint detection and estimation task as a unified stochastic optimal control problem, optimizing a composite Bayesian risk objective under realistic physical constraints. The RAPID algorithm solves this by first computing a robust, non-adaptive baseline protocol grounded in the quantum Fisher information matrix (QFIM), and then using this baseline to warm-start an online, adaptive policy learned via deep reinforcement learning (Soft Actor-Critic). This method dynamically optimizes control pulses, interrogation times, and measurement bases to maximize information gain while actively suppressing non-Markovian noise and decoherence. Numerical simulations demonstrate that the protocol achieves a significant sensitivity gain over static methods, maintains high estimation precision in correlated noise environments, and, when applied to sensor arrays, enables coherent quantum beamforming that achieves Heisenberg-like scaling in precision. This work establishes a theoretically rigorous and practically viable pathway for deploying quantum sensors in security-critical applications such as electronic warfare and covert surveillance.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Quasi-Continuum Modeling of Confined Fluids at the Nanoscale</title>
<link>https://arxiv.org/abs/2509.08223</link>
<guid>https://arxiv.org/abs/2509.08223</guid>
<content:encoded><![CDATA[
arXiv:2509.08223v1 Announce Type: cross 
Abstract: We present a data-efficient, multiscale framework for predicting the density profiles of confined fluids at the nanoscale. While accurate density estimates require prohibitively long timescales that are inaccessible by ab initio molecular dynamics (AIMD) simulations, machine-learned molecular dynamics (MLMD) offers a scalable alternative, enabling the generation of force predictions at ab initio accuracy with reduced computational cost. However, despite their efficiency, MLMD simulations remain constrained by femtosecond timesteps, which limit their practicality for computing long-time averages needed for accurate density estimation. To address this, we propose a conditional denoising diffusion probabilistic model (DDPM) based quasi-continuum approach that predicts the long-time behavior of force profiles along the confinement direction, conditioned on noisy forces extracted from a limited AIMD dataset. The predicted smooth forces are then linked to continuum theory via the Nernst-Planck equation to reveal the underlying density behavior. We test the framework on water confined between two graphene nanoscale slits and demonstrate that density profiles for channel widths outside of the training domain can be recovered with ab initio accuracy. Compared to AIMD and MLMD simulations, our method achieves orders-of-magnitude speed-up in runtime and requires significantly less training data than prior works.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepViT-CXR: A Channel Replication Strategy for Vision Transformers in Chest X-ray Tuberculosis and Pneumonia Classification</title>
<link>https://arxiv.org/abs/2509.08234</link>
<guid>https://arxiv.org/abs/2509.08234</guid>
<content:encoded><![CDATA[
arXiv:2509.08234v1 Announce Type: cross 
Abstract: Chest X-ray (CXR) imaging remains one of the most widely used diagnostic tools for detecting pulmonary diseases such as tuberculosis (TB) and pneumonia. Recent advances in deep learning, particularly Vision Transformers (ViTs), have shown strong potential for automated medical image analysis. However, most ViT architectures are pretrained on natural images and require three-channel inputs, while CXR scans are inherently grayscale. To address this gap, we propose RepViT-CXR, a channel replication strategy that adapts single-channel CXR images into a ViT-compatible format without introducing additional information loss. We evaluate RepViT-CXR on three benchmark datasets. On the TB-CXR dataset,our method achieved an accuracy of 99.9% and an AUC of 99.9%, surpassing prior state-of-the-art methods such as Topo-CXR (99.3% accuracy, 99.8% AUC). For the Pediatric Pneumonia dataset, RepViT-CXR obtained 99.0% accuracy, with 99.2% recall, 99.3% precision, and an AUC of 99.0%, outperforming strong baselines including DCNN and VGG16. On the Shenzhen TB dataset, our approach achieved 91.1% accuracy and an AUC of 91.2%, marking a performance improvement over previously reported CNN-based methods. These results demonstrate that a simple yet effective channel replication strategy allows ViTs to fully leverage their representational power on grayscale medical imaging tasks. RepViT-CXR establishes a new state of the art for TB and pneumonia detection from chest X-rays, showing strong potential for deployment in real-world clinical screening systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis</title>
<link>https://arxiv.org/abs/2509.08338</link>
<guid>https://arxiv.org/abs/2509.08338</guid>
<content:encoded><![CDATA[
arXiv:2509.08338v1 Announce Type: cross 
Abstract: Accurate and early diagnosis of malignant melanoma is critical for improving patient outcomes. While convolutional neural networks (CNNs) have shown promise in dermoscopic image analysis, they often neglect clinical metadata and require extensive preprocessing. Vision-language models (VLMs) offer a multimodal alternative but struggle to capture clinical specificity when trained on general-domain data. To address this, we propose a retrieval-augmented VLM framework that incorporates semantically similar patient cases into the diagnostic prompt. Our method enables informed predictions without fine-tuning and significantly improves classification accuracy and error correction over conventional baselines. These results demonstrate that retrieval-augmented prompting provides a robust strategy for clinical decision support.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chordless cycle filtrations for dimensionality detection in complex networks via topological data analysis</title>
<link>https://arxiv.org/abs/2509.08350</link>
<guid>https://arxiv.org/abs/2509.08350</guid>
<content:encoded><![CDATA[
arXiv:2509.08350v1 Announce Type: cross 
Abstract: Many complex networks, ranging from social to biological systems, exhibit structural patterns consistent with an underlying hyperbolic geometry. Revealing the dimensionality of this latent space can disentangle the structural complexity of communities, impact efficient network navigation, and fundamentally shape connectivity and system behavior. We introduce a novel topological data analysis weighting scheme for graphs, based on chordless cycles, aimed at estimating the dimensionality of networks in a data-driven way. We further show that the resulting descriptors can effectively estimate network dimensionality using a neural network architecture trained in a synthetic graph database constructed for this purpose, which does not need retraining to transfer effectively to real-world networks. Thus, by combining cycle-aware filtrations, algebraic topology, and machine learning, our approach provides a robust and effective method for uncovering the hidden geometry of complex networks and guiding accurate modeling and low-dimensional embedding.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>kNNSampler: Stochastic Imputations for Recovering Missing Value Distributions</title>
<link>https://arxiv.org/abs/2509.08366</link>
<guid>https://arxiv.org/abs/2509.08366</guid>
<content:encoded><![CDATA[
arXiv:2509.08366v1 Announce Type: cross 
Abstract: We study a missing-value imputation method, termed kNNSampler, that imputes a given unit's missing response by randomly sampling from the observed responses of the $k$ most similar units to the given unit in terms of the observed covariates. This method can sample unknown missing values from their distributions, quantify the uncertainties of missing values, and be readily used for multiple imputation. Unlike popular kNNImputer, which estimates the conditional mean of a missing response given an observed covariate, kNNSampler is theoretically shown to estimate the conditional distribution of a missing response given an observed covariate. Experiments demonstrate its effectiveness in recovering the distribution of missing values. The code for kNNSampler is made publicly available (https://github.com/SAP/knn-sampler).
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives</title>
<link>https://arxiv.org/abs/2509.08380</link>
<guid>https://arxiv.org/abs/2509.08380</guid>
<content:encoded><![CDATA[
arXiv:2509.08380v1 Announce Type: cross 
Abstract: Generating regulatorily compliant Suspicious Activity Report (SAR) remains a high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows. While large language models (LLMs) offer promising fluency, they suffer from factual hallucination, limited crime typology alignment, and poor explainability -- posing unacceptable risks in compliance-critical domains. This paper introduces Co-Investigator AI, an agentic framework optimized to produce Suspicious Activity Reports (SARs) significantly faster and with greater accuracy than traditional methods. Drawing inspiration from recent advances in autonomous agent architectures, such as the AI Co-Scientist, our approach integrates specialized agents for planning, crime type detection, external intelligence gathering, and compliance validation. The system features dynamic memory management, an AI-Privacy Guard layer for sensitive data handling, and a real-time validation agent employing the Agent-as-a-Judge paradigm to ensure continuous narrative quality assurance. Human investigators remain firmly in the loop, empowered to review and refine drafts in a collaborative workflow that blends AI efficiency with domain expertise. We demonstrate the versatility of Co-Investigator AI across a range of complex financial crime scenarios, highlighting its ability to streamline SAR drafting, align narratives with regulatory expectations, and enable compliance teams to focus on higher-order analytical work. This approach marks the beginning of a new era in compliance reporting -- bringing the transformative benefits of AI agents to the core of regulatory processes and paving the way for scalable, reliable, and transparent SAR generation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Ans\"atze Design for Quantum Circuit Born Machines in Financial Generative Modeling</title>
<link>https://arxiv.org/abs/2509.08385</link>
<guid>https://arxiv.org/abs/2509.08385</guid>
<content:encoded><![CDATA[
arXiv:2509.08385v1 Announce Type: cross 
Abstract: Quantum generative modeling using quantum circuit Born machines (QCBMs) shows promising potential for practical quantum advantage. However, discovering ans\"atze that are both expressive and hardware-efficient remains a key challenge, particularly on noisy intermediate-scale quantum (NISQ) devices. In this work, we introduce a prompt-based framework that leverages large language models (LLMs) to generate hardware-aware QCBM architectures. Prompts are conditioned on qubit connectivity, gate error rates, and hardware topology, while iterative feedback, including Kullback-Leibler (KL) divergence, circuit depth, and validity, is used to refine the circuits. We evaluate our method on a financial modeling task involving daily changes in Japanese government bond (JGB) interest rates. Our results show that the LLM-generated ans\"atze are significantly shallower and achieve superior generative performance compared to the standard baseline when executed on real IBM quantum hardware using 12 qubits. These findings demonstrate the practical utility of LLM-driven quantum architecture search and highlight a promising path toward robust, deployable generative models for near-term quantum devices.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Facet: highly efficient E(3)-equivariant networks for interatomic potentials</title>
<link>https://arxiv.org/abs/2509.08418</link>
<guid>https://arxiv.org/abs/2509.08418</guid>
<content:encoded><![CDATA[
arXiv:2509.08418v1 Announce Type: cross 
Abstract: Computational materials discovery is limited by the high cost of first-principles calculations. Machine learning (ML) potentials that predict energies from crystal structures are promising, but existing methods face computational bottlenecks. Steerable graph neural networks (GNNs) encode geometry with spherical harmonics, respecting atomic symmetries -- permutation, rotation, and translation -- for physically realistic predictions. Yet maintaining equivariance is difficult: activation functions must be modified, and each layer must handle multiple data types for different harmonic orders. We present Facet, a GNN architecture for efficient ML potentials, developed through systematic analysis of steerable GNNs. Our innovations include replacing expensive multi-layer perceptrons (MLPs) for interatomic distances with splines, which match performance while cutting computational and memory demands. We also introduce a general-purpose equivariant layer that mixes node information via spherical grid projection followed by standard MLPs -- faster than tensor products and more expressive than linear or gate layers. On the MPTrj dataset, Facet matches leading models with far fewer parameters and under 10% of their training compute. On a crystal relaxation task, it runs twice as fast as MACE models. We further show SevenNet-0's parameters can be reduced by over 25% with no accuracy loss. These techniques enable more than 10x faster training of large-scale foundation models for ML potentials, potentially reshaping computational materials discovery.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2509.08422</link>
<guid>https://arxiv.org/abs/2509.08422</guid>
<content:encoded><![CDATA[
arXiv:2509.08422v1 Announce Type: cross 
Abstract: Video-based AI systems are increasingly adopted in safety-critical domains such as autonomous driving and healthcare. However, interpreting their decisions remains challenging due to the inherent spatiotemporal complexity of video data and the opacity of deep learning models. Existing explanation techniques often suffer from limited temporal coherence, insufficient robustness, and a lack of actionable causal insights. Current counterfactual explanation methods typically do not incorporate guidance from the target model, reducing semantic fidelity and practical utility. We introduce Latent Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework designed to explain the behavior of video-based AI models. Compared to previous approaches, LD-ViCE reduces the computational costs of generating explanations by operating in latent space using a state-of-the-art diffusion model, while producing realistic and interpretable counterfactuals through an additional refinement step. Our experiments demonstrate the effectiveness of LD-ViCE across three diverse video datasets, including EchoNet-Dynamic (cardiac ultrasound), FERV39k (facial expression), and Something-Something V2 (action recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving an increase in R2 score of up to 68% while reducing inference time by half. Qualitative analysis confirms that LD-ViCE generates semantically meaningful and temporally coherent explanations, offering valuable insights into the target model behavior. LD-ViCE represents a valuable step toward the trustworthy deployment of AI in safety-critical domains.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting</title>
<link>https://arxiv.org/abs/2509.08442</link>
<guid>https://arxiv.org/abs/2509.08442</guid>
<content:encoded><![CDATA[
arXiv:2509.08442v1 Announce Type: cross 
Abstract: Accurate forecasting of individualized, high-resolution cortical thickness (CTh) trajectories is essential for detecting subtle cortical changes, providing invaluable insights into neurodegenerative processes and facilitating earlier and more precise intervention strategies. However, CTh forecasting is a challenging task due to the intricate non-Euclidean geometry of the cerebral cortex and the need to integrate multi-modal data for subject-specific predictions. To address these challenges, we introduce the Spherical Brownian Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional conditional Brownian bridge diffusion process to forecast CTh trajectories at the vertex level of registered cortical surfaces. Our technical contribution includes a new denoising model, the conditional spherical U-Net (CoS-UNet), which combines spherical convolutions and dense cross-attention to integrate cortical surfaces and tabular conditions seamlessly. Compared to previous approaches, SBDM achieves significantly reduced prediction errors, as demonstrated by our experiments based on longitudinal datasets from the ADNI and OASIS. Additionally, we demonstrate SBDM's ability to generate individual factual and counterfactual CTh trajectories, offering a novel framework for exploring hypothetical scenarios of cortical development.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition</title>
<link>https://arxiv.org/abs/2509.08454</link>
<guid>https://arxiv.org/abs/2509.08454</guid>
<content:encoded><![CDATA[
arXiv:2509.08454v1 Announce Type: cross 
Abstract: Large pre-trained speech models such as Whisper offer strong generalization but pose significant challenges for resource-efficient adaptation. Low-Rank Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method, yet its underlying mechanisms in speech tasks remain poorly understood. In this work, we conduct the first systematic mechanistic interpretability study of LoRA within the Whisper encoder for speech emotion recognition (SER). Using a suite of analytical tools, including layer contribution probing, logit-lens inspection, and representational similarity via singular value decomposition (SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a delayed specialization process that preserves general features in early layers before consolidating task-specific information, and a forward alignment, backward differentiation dynamic between LoRA's matrices. Our findings clarify how LoRA reshapes encoder hierarchies, providing both empirical insights and a deeper mechanistic understanding for designing efficient and interpretable adaptation strategies in large speech models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Process Regression -- Neural Network Hybrid with Optimized Redundant Coordinates</title>
<link>https://arxiv.org/abs/2509.08457</link>
<guid>https://arxiv.org/abs/2509.08457</guid>
<content:encoded><![CDATA[
arXiv:2509.08457v1 Announce Type: cross 
Abstract: Recently, a Gaussian Process Regression - neural network (GPRNN) hybrid machine learning method was proposed, which is based on additive-kernel GPR in redundant coordinates constructed by rules [J. Phys. Chem. A 127 (2023) 7823]. The method combined the expressive power of an NN with the robustness of linear regression, in particular, with respect to overfitting when the number of neurons is increased beyond optimal. We introduce opt-GPRNN, in which the redundant coordinates of GPRNN are optimized with a Monte Carlo algorithm and show that when combined with optimization of redundant coordinates, GPRNN attains the lowest test set error with much fewer terms / neurons and retains the advantage of avoiding overfitting when the number of neurons is increased beyond optimal value. The method, opt-GPRNN possesses an expressive power closer to that of a multilayer NN and could obviate the need for deep NNs in some applications. With optimized redundant coordinates, a dimensionality reduction regime is also possible. Examples of application to machine learning an interatomic potential and materials informatics are given.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants</title>
<link>https://arxiv.org/abs/2509.08494</link>
<guid>https://arxiv.org/abs/2509.08494</guid>
<content:encoded><![CDATA[
arXiv:2509.08494v1 Announce Type: cross 
Abstract: As humans delegate more tasks and decisions to artificial intelligence (AI), we risk losing control of our individual and collective futures. Relatively simple algorithmic systems already steer human decision-making, such as social media feed algorithms that lead people to unintentionally and absent-mindedly scroll through engagement-optimized content. In this paper, we develop the idea of human agency by integrating philosophical and scientific theories of agency with AI-assisted evaluation methods: using large language models (LLMs) to simulate and validate user queries and to evaluate AI responses. We develop HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions of human agency based on typical AI use cases. HAB measures the tendency of an AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation, Correct Misinformation, Defer Important Decisions, Encourage Learning, and Maintain Social Boundaries. We find low-to-moderate agency support in contemporary LLM-based assistants and substantial variation across system developers and dimensions. For example, while Anthropic LLMs most support human agency overall, they are the least supportive LLMs in terms of Avoid Value Manipulation. Agency support does not appear to consistently result from increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and we encourage a shift towards more robust safety and alignment targets.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agents of Discovery</title>
<link>https://arxiv.org/abs/2509.08535</link>
<guid>https://arxiv.org/abs/2509.08535</guid>
<content:encoded><![CDATA[
arXiv:2509.08535v1 Announce Type: cross 
Abstract: The substantial data volumes encountered in modern particle physics and other domains of fundamental physics research allow (and require) the use of increasingly complex data analysis tools and workflows. While the use of machine learning (ML) tools for data analysis has recently proliferated, these tools are typically special-purpose algorithms that rely, for example, on encoded physics knowledge to reach optimal performance. In this work, we investigate a new and orthogonal direction: Using recent progress in large language models (LLMs) to create a team of agents -- instances of LLMs with specific subtasks -- that jointly solve data analysis-based research problems in a way similar to how a human researcher might: by creating code to operate standard tools and libraries (including ML systems) and by building on results of previous iterations. If successful, such agent-based systems could be deployed to automate routine analysis components to counteract the increasing complexity of modern tool chains. To investigate the capabilities of current-generation commercial LLMs, we consider the task of anomaly detection via the publicly available and highly-studied LHC Olympics dataset. Several current models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated and their stability tested. Overall, we observe the capacity of the agent-based system to solve this data analysis problem. The best agent-created solutions mirror the performance of human state-of-the-art results.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning</title>
<link>https://arxiv.org/abs/2509.08539</link>
<guid>https://arxiv.org/abs/2509.08539</guid>
<content:encoded><![CDATA[
arXiv:2509.08539v1 Announce Type: cross 
Abstract: This paper examines the generalization capacity of two state-of-the-art classification and similarity learning models in reliably identifying users based on their motions in various Extended Reality (XR) applications. We developed a novel dataset containing a wide range of motion data from 49 users in five different XR applications: four XR games with distinct tasks and action patterns, and an additional social XR application with no predefined task sets. The dataset is used to evaluate the performance and, in particular, the generalization capacity of the two models across applications. Our results indicate that while the models can accurately identify individuals within the same application, their ability to identify users across different XR applications remains limited. Overall, our results provide insight into current models generalization capabilities and suitability as biometric methods for user verification and identification. The results also serve as a much-needed risk assessment of hazardous and unwanted user identification in XR and Metaverse applications. Our cross-application XR motion dataset and code are made available to the public to encourage similar research on the generalization of motion-based user identification in typical Metaverse application use cases.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEHRT: A Common Pipeline for Harmonizing Electronic Health Record data for Translational Research</title>
<link>https://arxiv.org/abs/2509.08553</link>
<guid>https://arxiv.org/abs/2509.08553</guid>
<content:encoded><![CDATA[
arXiv:2509.08553v1 Announce Type: cross 
Abstract: Integrative analysis of multi-institutional Electronic Health Record (EHR) data enhances the reliability and generalizability of translational research by leveraging larger, more diverse patient cohorts and incorporating multiple data modalities. However, harmonizing EHR data across institutions poses major challenges due to data heterogeneity, semantic differences, and privacy concerns. To address these challenges, we introduce $\textit{PEHRT}$, a standardized pipeline for efficient EHR data harmonization consisting of two core modules: (1) data pre-processing and (2) representation learning. PEHRT maps EHR data to standard coding systems and uses advanced machine learning to generate research-ready datasets without requiring individual-level data sharing. Our pipeline is also data model agnostic and designed for streamlined execution across institutions based on our extensive real-world experience. We provide a complete suite of open source software, accompanied by a user-friendly tutorial, and demonstrate the utility of PEHRT in a variety of tasks using data from diverse healthcare systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation</title>
<link>https://arxiv.org/abs/2509.08580</link>
<guid>https://arxiv.org/abs/2509.08580</guid>
<content:encoded><![CDATA[
arXiv:2509.08580v1 Announce Type: cross 
Abstract: The objective of this paper is to significantly reduce the manual workload required from medical professionals in complex 3D segmentation tasks that cannot be yet fully automated. For instance, in radiotherapy planning, organs at risk must be accurately identified in computed tomography (CT) or magnetic resonance imaging (MRI) scans to ensure they are spared from harmful radiation. Similarly, diagnosing age-related degenerative diseases such as sarcopenia, which involve progressive muscle volume loss and strength, is commonly based on muscular mass measurements often obtained from manual segmentation of medical volumes. To alleviate the manual-segmentation burden, this paper introduces an implicit shape prior to segment volumes from sparse slice manual annotations generalized to the multi-organ case, along with a simple framework for automatically selecting the most informative slices to guide and minimize the next interactions. The experimental validation shows the method's effectiveness on two medical use cases: assisted segmentation in the context of at risks organs for brain cancer patients, and acceleration of the creation of a new database with unseen muscle shapes for patients with sarcopenia.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MasconCube: Fast and Accurate Gravity Modeling with an Explicit Representation</title>
<link>https://arxiv.org/abs/2509.08607</link>
<guid>https://arxiv.org/abs/2509.08607</guid>
<content:encoded><![CDATA[
arXiv:2509.08607v1 Announce Type: cross 
Abstract: The geodesy of irregularly shaped small bodies presents fundamental challenges for gravitational field modeling, particularly as deep space exploration missions increasingly target asteroids and comets. Traditional approaches suffer from critical limitations: spherical harmonics diverge within the Brillouin sphere where spacecraft typically operate, polyhedral models assume unrealistic homogeneous density distributions, and existing machine learning methods like GeodesyNets and Physics-Informed Neural Networks (PINN-GM) require extensive computational resources and training time. This work introduces MasconCubes, a novel self-supervised learning approach that formulates gravity inversion as a direct optimization problem over a regular 3D grid of point masses (mascons). Unlike implicit neural representations, MasconCubes explicitly model mass distributions while leveraging known asteroid shape information to constrain the solution space. Comprehensive evaluation on diverse asteroid models including Bennu, Eros, Itokawa, and synthetic planetesimals demonstrates that MasconCubes achieve superior performance across multiple metrics. Most notably, MasconCubes demonstrate computational efficiency advantages with training times approximately 40 times faster than GeodesyNets while maintaining physical interpretability through explicit mass distributions. These results establish MasconCubes as a promising approach for mission-critical gravitational modeling applications requiring high accuracy, computational efficiency, and physical insight into internal mass distributions of irregular celestial bodies.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A hierarchical entropy method for the delocalization of bias in high-dimensional Langevin Monte Carlo</title>
<link>https://arxiv.org/abs/2509.08619</link>
<guid>https://arxiv.org/abs/2509.08619</guid>
<content:encoded><![CDATA[
arXiv:2509.08619v1 Announce Type: cross 
Abstract: The unadjusted Langevin algorithm is widely used for sampling from complex high-dimensional distributions. It is well known to be biased, with the bias typically scaling linearly with the dimension when measured in squared Wasserstein distance. However, the recent paper of Chen et al. (2024) identifies an intriguing new delocalization effect: For a class of distributions with sparse interactions, the bias between low-dimensional marginals scales only with the lower dimension, not the full dimension. In this work, we strengthen the results of Chen et al. (2024) in the sparse interaction regime by removing a logarithmic factor, measuring distance in relative entropy (a.k.a. KL-divergence), and relaxing the strong log-concavity assumption. In addition, we expand the scope of the delocalization phenomenon by showing that it holds for a class of distributions with weak interactions. Our proofs are based on a hierarchical analysis of the marginal relative entropies, inspired by the authors' recent work on propagation of chaos.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions</title>
<link>https://arxiv.org/abs/2509.08654</link>
<guid>https://arxiv.org/abs/2509.08654</guid>
<content:encoded><![CDATA[
arXiv:2509.08654v1 Announce Type: cross 
Abstract: This paper presents a feature-based Partially Observable Markov Decision Process (POMDP) framework for quantum network routing, combining belief-state planning with Graph Neural Networks (GNNs) to address partial observability, decoherence, and scalability challenges in dynamic quantum systems. Our approach encodes complex quantum network dynamics, including entanglement degradation and time-varying channel noise, into a low-dimensional feature space, enabling efficient belief updates and scalable policy learning. The core of our framework is a hybrid GNN-POMDP architecture that processes graph-structured representations of entangled links to learn routing policies, coupled with a noise-adaptive mechanism that fuses POMDP belief updates with GNN outputs for robust decision making. We provide a theoretical analysis establishing guarantees for belief convergence, policy improvement, and robustness to noise. Experiments on simulated quantum networks with up to 100 nodes demonstrate significant improvements in routing fidelity and entanglement delivery rates compared to state-of-the-art baselines, particularly under high decoherence and nonstationary conditions.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Unrolling of Sparsity-Induced RDO for 3D Point Cloud Attribute Coding</title>
<link>https://arxiv.org/abs/2509.08685</link>
<guid>https://arxiv.org/abs/2509.08685</guid>
<content:encoded><![CDATA[
arXiv:2509.08685v1 Announce Type: cross 
Abstract: Given encoded 3D point cloud geometry available at the decoder, we study the problem of lossy attribute compression in a multi-resolution B-spline projection framework. A target continuous 3D attribute function is first projected onto a sequence of nested subspaces $\mathcal{F}^{(p)}_{l_0} \subseteq \cdots \subseteq \mathcal{F}^{(p)}_{L}$, where $\mathcal{F}^{(p)}_{l}$ is a family of functions spanned by a B-spline basis function of order $p$ at a chosen scale and its integer shifts. The projected low-pass coefficients $F_l^*$ are computed by variable-complexity unrolling of a rate-distortion (RD) optimization algorithm into a feed-forward network, where the rate term is the sparsity-promoting $\ell_1$-norm. Thus, the projection operation is end-to-end differentiable. For a chosen coarse-to-fine predictor, the coefficients are then adjusted to account for the prediction from a lower-resolution to a higher-resolution, which is also optimized in a data-driven manner.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</title>
<link>https://arxiv.org/abs/2509.08699</link>
<guid>https://arxiv.org/abs/2509.08699</guid>
<content:encoded><![CDATA[
arXiv:2509.08699v1 Announce Type: cross 
Abstract: Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenizing Loops of Antibodies</title>
<link>https://arxiv.org/abs/2509.08707</link>
<guid>https://arxiv.org/abs/2509.08707</guid>
<content:encoded><![CDATA[
arXiv:2509.08707v1 Announce Type: cross 
Abstract: The complementarity-determining regions of antibodies are loop structures that are key to their interactions with antigens, and of high importance to the design of novel biologics. Since the 1980s, categorizing the diversity of CDR structures into canonical clusters has enabled the identification of key structural motifs of antibodies. However, existing approaches have limited coverage and cannot be readily incorporated into protein foundation models. Here we introduce ImmunoGlobulin LOOp Tokenizer, Igloo, a multimodal antibody loop tokenizer that encodes backbone dihedral angles and sequence. Igloo is trained using a contrastive learning objective to map loops with similar backbone dihedral angles closer together in latent space. Igloo can efficiently retrieve the closest matching loop structures from a structural antibody database, outperforming existing methods on identifying similar H3 loops by 5.9\%. Igloo assigns tokens to all loops, addressing the limited coverage issue of canonical clusters, while retaining the ability to recover canonical loop conformations. To demonstrate the versatility of Igloo tokens, we show that they can be incorporated into protein language models with IglooLM and IglooALM. On predicting binding affinity of heavy chain variants, IglooLM outperforms the base protein language model on 8 out of 10 antibody-antigen targets. Additionally, it is on par with existing state-of-the-art sequence-based and multimodal protein language models, performing comparably to models with $7\times$ more parameters. IglooALM samples antibody loops which are diverse in sequence and more consistent in structure than state-of-the-art antibody inverse folding models. Igloo demonstrates the benefit of introducing multimodal tokens for antibody loops for encoding the diverse landscape of antibody loops, improving protein foundation models, and for antibody CDR design.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability of CNN Based Classification Models for Acoustic Signal</title>
<link>https://arxiv.org/abs/2509.08717</link>
<guid>https://arxiv.org/abs/2509.08717</guid>
<content:encoded><![CDATA[
arXiv:2509.08717v1 Announce Type: cross 
Abstract: Explainable Artificial Intelligence (XAI) has emerged as a critical tool for interpreting the predictions of complex deep learning models. While XAI has been increasingly applied in various domains within acoustics, its use in bioacoustics, which involves analyzing audio signals from living organisms, remains relatively underexplored. In this paper, we investigate the vocalizations of a bird species with strong geographic variation throughout its range in North America. Audio recordings were converted into spectrogram images and used to train a deep Convolutional Neural Network (CNN) for classification, achieving an accuracy of 94.8\%. To interpret the model's predictions, we applied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT, Grad-CAM) XAI techniques. These techniques produced different but complementary explanations, and when their explanations were considered together, they provided more complete and interpretable insights into the model's decision-making. This work highlights the importance of using a combination of XAI techniques to improve trust and interoperability, not only in broader acoustics signal analysis but also argues for broader applicability in different domain specific tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Stochastic Nonconvex Optimization under the Relaxed Smoothness</title>
<link>https://arxiv.org/abs/2509.08726</link>
<guid>https://arxiv.org/abs/2509.08726</guid>
<content:encoded><![CDATA[
arXiv:2509.08726v1 Announce Type: cross 
Abstract: This paper studies decentralized optimization problem $f(\mathbf{x})=\frac{1}{m}\sum_{i=1}^m f_i(\mathbf{x})$, where each local function has the form of $f_i(\mathbf{x}) = {\mathbb E}\left[F(\mathbf{x};{\xi}_i)\right]$ which is $(L_0,L_1)$-smooth but possibly nonconvex and the random variable ${\xi}_i$ follows distribution ${\mathcal D}_i$. We propose a novel algorithm called decentralized normalized stochastic gradient descent (DNSGD), which can achieve the $\epsilon$-stationary point on each local agent. We present a new framework for analyzing decentralized first-order methods in the relaxed smooth setting, based on the Lyapunov function related to the product of the gradient norm and the consensus error. The analysis shows upper bounds on sample complexity of ${\mathcal O}(m^{-1}(L_f\sigma^2\Delta_f\epsilon^{-4} + \sigma^2\epsilon^{-2} + L_f^{-2}L_1^3\sigma^2\Delta_f\epsilon^{-1} + L_f^{-2}L_1^2\sigma^2))$ per agent and communication complexity of $\tilde{\mathcal O}((L_f\epsilon^{-2} + L_1\epsilon^{-1})\gamma^{-1/2}\Delta_f)$, where $L_f=L_0 +L_1\zeta$, $\sigma^2$ is the variance of the stochastic gradient, $\Delta_f$ is the initial optimal function value gap, $\gamma$ is the spectral gap of the network, and $\zeta$ is the degree of the gradient dissimilarity. In the special case of $L_1=0$, the above results (nearly) match the lower bounds on decentralized nonconvex optimization in the standard smooth setting. We also conduct numerical experiments to show the empirical superiority of our method.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bregman Douglas-Rachford Splitting Method</title>
<link>https://arxiv.org/abs/2509.08739</link>
<guid>https://arxiv.org/abs/2509.08739</guid>
<content:encoded><![CDATA[
arXiv:2509.08739v1 Announce Type: cross 
Abstract: In this paper, we propose the Bregman Douglas-Rachford splitting (BDRS) method and its variant Bregman Peaceman-Rachford splitting method for solving maximal monotone inclusion problem. We show that BDRS is equivalent to a Bregman alternating direction method of multipliers (ADMM) when applied to the dual of the problem. A special case of the Bregman ADMM is an alternating direction version of the exponential multiplier method. To the best of our knowledge, algorithms proposed in this paper are new to the literature. We also discuss how to use our algorithms to solve the discrete optimal transport (OT) problem. We prove the convergence of the algorithms under certain assumptions, though we point out that one assumption does not apply to the OT problem.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Turbulent Flows with Generative Models: Super-resolution, Forecasting, and Sparse Flow Reconstruction</title>
<link>https://arxiv.org/abs/2509.08752</link>
<guid>https://arxiv.org/abs/2509.08752</guid>
<content:encoded><![CDATA[
arXiv:2509.08752v1 Announce Type: cross 
Abstract: Neural operators are promising surrogates for dynamical systems but when trained with standard L2 losses they tend to oversmooth fine-scale turbulent structures. Here, we show that combining operator learning with generative modeling overcomes this limitation. We consider three practical turbulent-flow challenges where conventional neural operators fail: spatio-temporal super-resolution, forecasting, and sparse flow reconstruction. For Schlieren jet super-resolution, an adversarially trained neural operator (adv-NO) reduces the energy-spectrum error by 15x while preserving sharp gradients at neural operator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NO trained on only 160 timesteps from a single trajectory forecasts accurately for five eddy-turnover times and offers 114x wall-clock speed-up at inference than the baseline diffusion-based forecasters, enabling near-real-time rollouts. For reconstructing cylinder wake flows from highly sparse Particle Tracking Velocimetry-like inputs, a conditional generative model infers full 3D velocity and pressure fields with correct phase alignment and statistics. These advances enable accurate reconstruction and forecasting at low compute cost, bringing near-real-time analysis and control within reach in experimental and computational fluid mechanics. See our project page: https://vivekoommen.github.io/Gen4Turb/
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCGBandit: One-shot acceleration of transient PDE solvers via online-learned preconditioners</title>
<link>https://arxiv.org/abs/2509.08765</link>
<guid>https://arxiv.org/abs/2509.08765</guid>
<content:encoded><![CDATA[
arXiv:2509.08765v1 Announce Type: cross 
Abstract: Data-driven acceleration of scientific computing workflows has been a high-profile aim of machine learning (ML) for science, with numerical simulation of transient partial differential equations (PDEs) being one of the main applications. The focus thus far has been on methods that require classical simulations to train, which when combined with the data-hungriness and optimization challenges of neural networks has caused difficulties in demonstrating a convincing advantage against strong classical baselines. We consider an alternative paradigm in which the learner uses a classical solver's own data to accelerate it, enabling a one-shot speedup of the simulation. Concretely, since transient PDEs often require solving a sequence of related linear systems, the feedback from repeated calls to a linear solver such as preconditioned conjugate gradient (PCG) can be used by a bandit algorithm to online-learn an adaptive sequence of solver configurations (e.g. preconditioners). The method we develop, PCGBandit, is implemented directly on top of the popular open source software OpenFOAM, which we use to show its effectiveness on a set of fluid and magnetohydrodynamics (MHD) problems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QCardEst/QCardCorr: Quantum Cardinality Estimation and Correction</title>
<link>https://arxiv.org/abs/2509.08817</link>
<guid>https://arxiv.org/abs/2509.08817</guid>
<content:encoded><![CDATA[
arXiv:2509.08817v1 Announce Type: cross 
Abstract: Cardinality estimation is an important part of query optimization in DBMS. We develop a Quantum Cardinality Estimation (QCardEst) approach using Quantum Machine Learning with a Hybrid Quantum-Classical Network. We define a compact encoding for turning SQL queries into a quantum state, which requires only qubits equal to the number of tables in the query. This allows the processing of a complete query with a single variational quantum circuit (VQC) on current hardware. In addition, we compare multiple classical post-processing layers to turn the probability vector output of VQC into a cardinality value. We introduce Quantum Cardinality Correction QCardCorr, which improves classical cardinality estimators by multiplying the output with a factor generated by a VQC to improve the cardinality estimation. With QCardCorr, we have an improvement over the standard PostgreSQL optimizer of 6.37 times for JOB-light and 8.66 times for STATS. For JOB-light we even outperform MSCN by a factor of 3.47.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation</title>
<link>https://arxiv.org/abs/2509.08825</link>
<guid>https://arxiv.org/abs/2509.08825</guid>
<content:encoded><![CDATA[
arXiv:2509.08825v1 Announce Type: cross 
Abstract: Large language models (LLMs) are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection, prompting strategy, or temperature settings). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I, Type II, Type S, or Type M errors. We call this LLM hacking.
  We quantify the risk of LLM hacking by replicating 37 data annotation tasks from 21 published social science research studies with 18 different models. Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure how plausible researcher choices affect statistical conclusions. We find incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art models, and in half the hypotheses for small language models. While our findings show that higher task performance and better general model capabilities reduce LLM hacking risk, even highly accurate models do not completely eliminate it. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of findings near significance thresholds. Our extensive analysis of LLM hacking mitigation techniques emphasizes the importance of human annotations in reducing false positive findings and improving model selection. Surprisingly, common regression estimator correction techniques are largely ineffective in reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.
  Beyond accidental errors, we find that intentional LLM hacking is unacceptably simple. With few LLMs and just a handful of prompt paraphrases, anything can be presented as statistically significant.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2509.08827</link>
<guid>https://arxiv.org/abs/2509.08827</guid>
<content:encoded><![CDATA[
arXiv:2509.08827v1 Announce Type: cross 
Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata</title>
<link>https://arxiv.org/abs/2203.16331</link>
<guid>https://arxiv.org/abs/2203.16331</guid>
<content:encoded><![CDATA[
arXiv:2203.16331v5 Announce Type: replace 
Abstract: We present the efficient implementations of probabilistic deterministic finite automaton learning methods available in FlexFringe. These implement well-known strategies for state-merging including several modifications to improve their performance in practice. We show experimentally that these algorithms obtain competitive results and significant improvements over a default implementation. We also demonstrate how to use FlexFringe to learn interpretable models from software logs and use these for anomaly detection. Although less interpretable, we show that learning smaller more convoluted models improves the performance of FlexFringe on anomaly detection, outperforming an existing solution based on neural nets.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Optimization of Energy Consumption and Completion Time in Federated Learning</title>
<link>https://arxiv.org/abs/2209.14900</link>
<guid>https://arxiv.org/abs/2209.14900</guid>
<content:encoded><![CDATA[
arXiv:2209.14900v4 Announce Type: replace 
Abstract: Federated Learning (FL) is an intriguing distributed machine learning approach due to its privacy-preserving characteristics. To balance the trade-off between energy and execution latency, and thus accommodate different demands and application scenarios, we formulate an optimization problem to minimize a weighted sum of total energy consumption and completion time through two weight parameters. The optimization variables include bandwidth, transmission power and CPU frequency of each device in the FL system, where all devices are linked to a base station and train a global model collaboratively. Through decomposing the non-convex optimization problem into two subproblems, we devise a resource allocation algorithm to determine the bandwidth allocation, transmission power, and CPU frequency for each participating device. We further present the convergence analysis and computational complexity of the proposed algorithm. Numerical results show that our proposed algorithm not only has better performance at different weight parameters (i.e., different demands) but also outperforms the state of the art.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Transformers via Sparse Gaussian Processes</title>
<link>https://arxiv.org/abs/2303.02444</link>
<guid>https://arxiv.org/abs/2303.02444</guid>
<content:encoded><![CDATA[
arXiv:2303.02444v4 Announce Type: replace 
Abstract: Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer's success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Robustness of Link Sign Prediction in Signed Graphs</title>
<link>https://arxiv.org/abs/2401.10590</link>
<guid>https://arxiv.org/abs/2401.10590</guid>
<content:encoded><![CDATA[
arXiv:2401.10590v3 Announce Type: replace 
Abstract: Signed graphs serve as fundamental data structures for representing positive and negative relationships in social networks, with signed graph neural networks (SGNNs) emerging as the primary tool for their analysis. Our investigation reveals that balance theory, while essential for modeling signed relationships in SGNNs, inadvertently introduces exploitable vulnerabilities to black-box attacks. To showcase this, we propose balance-attack, a novel adversarial strategy specifically designed to compromise graph balance degree, and develop an efficient heuristic algorithm to solve the associated NP-hard optimization problem. While existing approaches attempt to restore attacked graphs through balance learning techniques, they face a critical challenge we term "Irreversibility of Balance-related Information," as restored edges fail to align with original attack targets. To address this limitation, we introduce Balance Augmented-Signed Graph Contrastive Learning (BA-SGCL), an innovative framework that combines contrastive learning with balance augmentation techniques to achieve robust graph representations. By maintaining high balance degree in the latent space, BA-SGCL not only effectively circumvents the irreversibility challenge but also significantly enhances model resilience. Extensive experiments across multiple SGNN architectures and real-world datasets demonstrate both the effectiveness of our proposed balance-attack and the superior robustness of BA-SGCL, advancing the security and reliability of signed graph analysis in social networks. Datasets and codes of the proposed framework are at the github repository https://anonymous.4open.science/r/BA-SGCL-submit-DF41/.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models</title>
<link>https://arxiv.org/abs/2403.09904</link>
<guid>https://arxiv.org/abs/2403.09904</guid>
<content:encoded><![CDATA[
arXiv:2403.09904v2 Announce Type: replace 
Abstract: Federated Learning (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A critical bottleneck in FL is the communication cost. A pivotal strategy to mitigate this burden is Local Training, which involves running multiple local stochastic gradient descent iterations between communication phases. Our work is inspired by the innovative Scaffnew algorithm, which has considerably advanced the reduction of communication complexity in FL. We introduce FedComLoc (Federated Compressed and Local Training), integrating practical and effective compression into Scaffnew to further enhance communication efficiency. Extensive experiments, using the popular TopK compressor and quantization, demonstrate its prowess in substantially reducing communication overheads in heterogeneous settings.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Transformer approach for Electricity Price Forecasting</title>
<link>https://arxiv.org/abs/2403.16108</link>
<guid>https://arxiv.org/abs/2403.16108</guid>
<content:encoded><![CDATA[
arXiv:2403.16108v3 Announce Type: replace 
Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Quest for the Right Mediator: Surveying Mechanistic Interpretability Through the Lens of Causal Mediation Analysis</title>
<link>https://arxiv.org/abs/2408.01416</link>
<guid>https://arxiv.org/abs/2408.01416</guid>
<content:encoded><![CDATA[
arXiv:2408.01416v2 Announce Type: replace 
Abstract: Interpretability provides a toolset for understanding how and why language models behave in certain ways. However, there is little unity in the field: most studies employ ad-hoc evaluations and do not share theoretical foundations, making it difficult to measure progress and compare the pros and cons of different techniques. Furthermore, while mechanistic understanding is frequently discussed, the basic causal units underlying these mechanisms are often not explicitly defined. In this article, we propose a perspective on interpretability research grounded in causal mediation analysis. Specifically, we describe the history and current state of interpretability taxonomized according to the types of causal units (mediators) employed, as well as methods used to search over mediators. We discuss the pros and cons of each mediator, providing insights as to when particular kinds of mediators and search methods are most appropriate. We argue that this framing yields a more cohesive narrative of the field and helps researchers select appropriate methods based on their research objective. Our analysis yields actionable recommendations for future work, including the discovery of new mediators and the development of standardized evaluations tailored to these goals.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Example-Based Explanations: Bridging the Gap between Generative Modeling and Explainability</title>
<link>https://arxiv.org/abs/2410.20890</link>
<guid>https://arxiv.org/abs/2410.20890</guid>
<content:encoded><![CDATA[
arXiv:2410.20890v2 Announce Type: replace 
Abstract: Recently, several methods have leveraged deep generative modeling to produce example-based explanations of image classifiers. Despite producing visually stunning results, these methods are largely disconnected from classical explainability literature. This conceptual and communication gap leads to misunderstandings and misalignments in goals and expectations. In this paper, we bridge this gap by proposing a probabilistic framework for example-based explanations, formally defining the example-based explanations in a probabilistic manner amenable for modeling via deep generative models while coherent with the critical characteristics and desiderata widely accepted in the explainability community. Our aim is on one hand to provide a constructive framework for the development of well-grounded generative algorithms for example-based explanations and, on the other, to facilitate communication between the generative and explainability research communities, foster rigor and transparency, and improve the quality of peer discussion and research progress in this promising direction.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic regression via MDLformer-guided search: from minimizing prediction error to minimizing description length</title>
<link>https://arxiv.org/abs/2411.03753</link>
<guid>https://arxiv.org/abs/2411.03753</guid>
<content:encoded><![CDATA[
arXiv:2411.03753v3 Announce Type: replace 
Abstract: Symbolic regression, a task discovering the formula best fitting the given data, is typically based on the heuristical search. These methods usually update candidate formulas to obtain new ones with lower prediction errors iteratively. However, since formulas with similar function shapes may have completely different symbolic forms, the prediction error does not decrease monotonously as the search approaches the target formula, causing the low recovery rate of existing methods. To solve this problem, we propose a novel search objective based on the minimum description length, which reflects the distance from the target and decreases monotonically as the search approaches the correct form of the target formula. To estimate the minimum description length of any input data, we design a neural network, MDLformer, which enables robust and scalable estimation through large-scale training. With the MDLformer's output as the search objective, we implement a symbolic regression method, SR4MDL, that can effectively recover the correct mathematical form of the formula. Extensive experiments illustrate its excellent performance in recovering formulas from data. Our method successfully recovers around 50 formulas across two benchmark datasets comprising 133 problems, outperforming state-of-the-art methods by 43.92%. Experiments on 122 unseen black-box problems further demonstrate its generalization performance. We release our code at https://github.com/tsinghua-fib-lab/SR4MDL .
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAMES: Fast Approximate Multiplier Substitution for Mixed-Precision Quantized DNNs--Down to 2 Bits!</title>
<link>https://arxiv.org/abs/2411.18055</link>
<guid>https://arxiv.org/abs/2411.18055</guid>
<content:encoded><![CDATA[
arXiv:2411.18055v4 Announce Type: replace 
Abstract: A widely-used technique in designing energy-efficient deep neural network (DNN) accelerators is quantization. Recent progress in this direction has reduced the bitwidths used in DNN down to 2. Meanwhile, many prior works apply approximate multipliers (AppMuls) in designing DNN accelerators to lower their energy consumption. Unfortunately, these works still assume a bitwidth much larger than 2, which falls far behind the state-of-the-art in quantization area and even challenges the meaningfulness of applying AppMuls in DNN accelerators, since a high-bitwidth AppMul consumes much more energy than a low-bitwidth exact multiplier! Thus, an important problem to study is: Can approximate multipliers be effectively applied to quantized DNN models with very low bitwidths? In this work, we give an affirmative answer to this question and present a systematic solution that achieves the answer: FAMES, a fast approximate multiplier substitution method for mixed-precision DNNs. Our experiments demonstrate an average 28.67% energy reduction on state-of-the-art mixed-precision quantized models with bitwidths as low as 2 bits and accuracy losses kept under 1%. Additionally, our approach is up to 300x faster than previous genetic algorithm-based methods.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Random Feature Model</title>
<link>https://arxiv.org/abs/2412.04785</link>
<guid>https://arxiv.org/abs/2412.04785</guid>
<content:encoded><![CDATA[
arXiv:2412.04785v2 Announce Type: replace 
Abstract: Designing privacy-preserving machine learning algorithms has received great attention in recent years, especially in the setting when the data contains sensitive information. Differential privacy (DP) is a widely used mechanism for data analysis with privacy guarantees. In this paper, we produce a differentially private random feature model. Random features, which were proposed to approximate large-scale kernel machines, have been used to study privacy-preserving kernel machines as well. We consider the over-parametrized regime (more features than samples) where the non-private random feature model is learned via solving the min-norm interpolation problem, and then we apply output perturbation techniques to produce a private model. We show that our method preserves privacy and derive a generalization error bound for the method. To the best of our knowledge, we are the first to consider privacy-preserving random feature models in the over-parametrized regime and provide theoretical guarantees. We empirically compare our method with other privacy-preserving learning methods in the literature as well. Our results show that our approach is superior to the other methods in terms of generalization performance on synthetic data and benchmark data sets. Additionally, it was recently observed that DP mechanisms may exhibit and exacerbate disparate impact, which means that the outcomes of DP learning algorithms vary significantly among different groups. We show that both theoretically and empirically, random features have the potential to reduce disparate impact, and hence achieve better fairness.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HopCast: Calibration of Autoregressive Dynamics Models</title>
<link>https://arxiv.org/abs/2501.16587</link>
<guid>https://arxiv.org/abs/2501.16587</guid>
<content:encoded><![CDATA[
arXiv:2501.16587v4 Announce Type: replace 
Abstract: Deep learning models are often trained to approximate dynamical systems that can be modeled using differential equations. Many of these models are optimized to predict one step ahead; such approaches produce calibrated one-step predictions if the predictive model can quantify uncertainty, such as Deep Ensembles. At inference time, multi-step predictions are generated via autoregression, which needs a sound uncertainty propagation method to produce calibrated multi-step predictions. This work introduces an alternative Predictor-Corrector approach named \hop{} that uses Modern Hopfield Networks (MHN) to learn the errors of a deterministic Predictor that approximates the dynamical system. The Corrector predicts a set of errors for the Predictor's output based on a context state at any timestep during autoregression. The set of errors creates sharper and well-calibrated prediction intervals with higher predictive accuracy compared to baselines without uncertainty propagation. The calibration and prediction performances are evaluated across a set of dynamical systems. This work is also the first to benchmark existing uncertainty propagation methods based on calibration errors.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDDM: A Molecular Dynamics Diffusion Model to Predict Particle Self-Assembly</title>
<link>https://arxiv.org/abs/2501.17319</link>
<guid>https://arxiv.org/abs/2501.17319</guid>
<content:encoded><![CDATA[
arXiv:2501.17319v3 Announce Type: replace 
Abstract: The discovery and study of new material systems rely on molecular simulations that often come with significant computational expense. We propose MDDM, a Molecular Dynamics Diffusion Model, which is capable of predicting a valid output conformation for a given input pair potential function. After training MDDM on a large dataset of molecular dynamics self-assembly results, the proposed model can convert uniform noise into a meaningful output particle structure corresponding to an arbitrary input potential. The model's architecture has domain-specific properties built-in, such as satisfying periodic boundaries and being invariant to translation. The model significantly outperforms the baseline point-cloud diffusion model for both unconditional and conditional generation tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Compositional Reasoning in Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2502.06037</link>
<guid>https://arxiv.org/abs/2502.06037</guid>
<content:encoded><![CDATA[
arXiv:2502.06037v2 Announce Type: replace 
Abstract: Large pre-trained time series foundation models (TSFMs) have demonstrated promising zero-shot performance across a wide range of domains. However, a question remains: Do TSFMs succeed by memorizing patterns in training data, or do they possess the ability to reason about such patterns? While reasoning is a topic of great interest in the study of Large Language Models (LLMs), it is undefined and largely unexplored in the context of TSFMs. In this work, inspired by language modeling literature, we formally define compositional reasoning in forecasting and distinguish it from in-distribution generalization. We evaluate the reasoning and generalization capabilities of 16 popular deep learning forecasting models on multiple synthetic and real-world datasets. Additionally, through controlled studies, we systematically examine which design choices in 7 popular open-source TSFMs contribute to improved reasoning capabilities. Our study yields key insights into the impact of TSFM architecture design on compositional reasoning and generalization. We find that patch-based Transformers have the best reasoning performance, closely followed by residualized MLP-based architectures, which are 97\% less computationally complex in terms of FLOPs and 86\% smaller in terms of the number of trainable parameters. Interestingly, in some zero-shot out-of-distribution scenarios, these models can outperform moving average and exponential smoothing statistical baselines trained on in-distribution data. Only a few design choices, such as the tokenization method, had a significant (negative) impact on Transformer model performance.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A general language model for peptide identification</title>
<link>https://arxiv.org/abs/2502.15610</link>
<guid>https://arxiv.org/abs/2502.15610</guid>
<content:encoded><![CDATA[
arXiv:2502.15610v5 Announce Type: replace 
Abstract: Accurate identification of bioactive peptides (BPs) and protein post-translational modifications (PTMs) is essential for understanding protein function and advancing therapeutic discovery. However, most computational methods remain limited in their generalizability across diverse peptide functions. Here, we present PDeepPP, a unified deep learning framework that integrates pretrained protein language models with a hybrid transformer-convolutional architecture, enabling robust identification across diverse peptide classes and PTM sites. We curated comprehensive benchmark datasets and implemented strategies to address data imbalance, allowing PDeepPP to systematically extract both global and local sequence features. Through extensive analyses-including dimensionality reduction and comparison studies-PDeepPP demonstrates strong, interpretable peptide representations and achieves state-of-the-art performance in 25 of the 33 biological identification tasks. Notably, PDeepPP attains high accuracy in antimicrobial (0.9726) and phosphorylation site (0.9984) identification, with 99.5% specificity in glycosylation site prediction and substantial reduction in false negatives in antimalarial tasks. By enabling large-scale, accurate peptide analysis, PDeepPP supports biomedical research and the discovery of novel therapeutic targets for disease treatment. All code, datasets, and pretrained models are publicly available via GitHub:https://github.com/fondress/PDeepPP and Hugging Face:https://huggingface.co/fondress/PDeppPP.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cauchy Random Features for Operator Learning in Sobolev Space</title>
<link>https://arxiv.org/abs/2503.00300</link>
<guid>https://arxiv.org/abs/2503.00300</guid>
<content:encoded><![CDATA[
arXiv:2503.00300v2 Announce Type: replace 
Abstract: Operator learning is the approximation of operators between infinite dimensional Banach spaces using machine learning approaches. While most progress in this area has been driven by variants of deep neural networks such as the Deep Operator Network and Fourier Neural Operator, the theoretical guarantees are often in the form of a universal approximation property. However, the existence theorems do not guarantee that an accurate operator network is obtainable in practice. Motivated by the recent kernel-based operator learning framework, we propose a random feature operator learning method with theoretical guarantees and error bounds. The random feature method can be viewed as a randomized approximation of a kernel method, which significantly reduces the computation requirements for training. We provide a generalization error analysis for our proposed random feature operator learning method along with comprehensive numerical results. Compared to kernel-based method and neural network methods, the proposed method can obtain similar or better test errors across benchmarks examples with significantly reduced training times. An additional advantages it that our implementation is simple and does require costly computational resources, such as GPU.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Cosine Decay: On the effectiveness of Infinite Learning Rate Schedule for Continual Pre-training</title>
<link>https://arxiv.org/abs/2503.02844</link>
<guid>https://arxiv.org/abs/2503.02844</guid>
<content:encoded><![CDATA[
arXiv:2503.02844v3 Announce Type: replace 
Abstract: The ever-growing availability of unlabeled data presents both opportunities and challenges for training artificial intelligence systems. While self-supervised learning (SSL) has emerged as a powerful paradigm for extracting meaningful representations from vast amounts of unlabeled data, existing methods still struggle to adapt to the non-stationary, non-IID nature of real-world data streams without forgetting previously learned knowledge. Recent works have adopted a repeated cosine annealing schedule for large-scale continual pre-training; however, these schedules (1) inherently cause forgetting during the re-warming phase and (2) have not been systematically compared to existing continual SSL methods. In this work, we systematically compare the widely used cosine schedule with the recently proposed infinite learning rate schedule and empirically find the latter to be a more effective alternative. Our extensive empirical evaluation across diverse image and language datasets demonstrates that the infinite learning rate schedule consistently enhances continual pre-training performance compared to a repeated cosine decay without being restricted to a fixed iteration budget. For instance, in a small-scale MAE pre-training setup, it outperforms several strong baselines from the literature. We then scale up our experiments to larger MAE pre-training and autoregressive language model pre-training. Our results show that the infinite learning rate schedule remains effective at scale, surpassing repeated cosine decay for both MAE pre-training and zero-shot LM benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To See a World in a Spark of Neuron: Disentangling Multi-task Interference for Training-free Model Merging</title>
<link>https://arxiv.org/abs/2503.05320</link>
<guid>https://arxiv.org/abs/2503.05320</guid>
<content:encoded><![CDATA[
arXiv:2503.05320v4 Announce Type: replace 
Abstract: Fine-tuning pre-trained models on targeted datasets enhances task-specific performance but often comes at the expense of generalization. Model merging techniques, which integrate multiple fine-tuned models into a single multi-task model through task arithmetic, offer a promising solution. However, task interference remains a fundamental challenge, leading to performance degradation and suboptimal merged models. Existing approaches largely overlooked the fundamental roles of neurons, their connectivity, and activation, resulting in a merging process and a merged model that does not consider how neurons relay and process information. In this work, we present the first study that relies on neuronal mechanisms for model merging. Specifically, we decomposed task-specific representations into two complementary neuronal subspaces that regulate input sensitivity and task adaptability. Leveraging this decomposition, we introduced NeuroMerging, a novel merging framework developed to mitigate task interference within neuronal subspaces, enabling training-free model fusion across diverse tasks. Through extensive experiments, we demonstrated that NeuroMerging achieved superior performance compared to existing methods on multi-task benchmarks across both natural language and vision domains. Our findings highlighted the importance of aligning neuronal mechanisms in model merging, offering new insights into mitigating task interference and improving knowledge fusion. Our project is available at https://ZzzitaoFang.github.io/projects/NeuroMerging/.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?</title>
<link>https://arxiv.org/abs/2504.03814</link>
<guid>https://arxiv.org/abs/2504.03814</guid>
<content:encoded><![CDATA[
arXiv:2504.03814v4 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used in the creation of online content, creating feedback loops as subsequent generations of models will be trained on this synthetic data. Such loops were shown to lead to distribution shifts - models misrepresenting the true underlying distributions of human data (also called model collapse). However, how human data properties affect such shifts remains poorly understood. In this paper, we provide the first empirical examination of the effect of such properties on the outcome of recursive training. We first confirm that using different human datasets leads to distribution shifts of different magnitudes. Through exhaustive manipulation of dataset properties combined with regression analyses, we then identify a set of properties predicting distribution shift magnitudes. Lexical diversity is found to amplify these shifts, while semantic diversity and data quality mitigate them. Furthermore, we find that these influences are highly modular: data scrapped from a given internet domain has little influence on the content generated for another domain. Finally, experiments on political bias reveal that human data properties affect whether the initial bias will be amplified or reduced. Overall, our results portray a novel view, where different parts of internet may undergo different types of distribution shift.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-based Loss Functions in Computer Vision: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2504.04242</link>
<guid>https://arxiv.org/abs/2504.04242</guid>
<content:encoded><![CDATA[
arXiv:2504.04242v2 Announce Type: replace 
Abstract: Loss functions are at the heart of deep learning, shaping how models learn and perform across diverse tasks. They are used to quantify the difference between predicted outputs and ground truth labels, guiding the optimization process to minimize errors. Selecting the right loss function is critical, as it directly impacts model convergence, generalization, and overall performance across various applications, from computer vision to time series forecasting. This paper presents a comprehensive review of loss functions, covering fundamental metrics like Mean Squared Error and Cross-Entropy to advanced functions such as Adversarial and Diffusion losses. We explore their mathematical foundations, impact on model training, and strategic selection for various applications, including computer vision (Discriminative and generative), tabular data prediction, and time series forecasting. For each of these categories, we discuss the most used loss functions in the recent advancements of deep learning techniques. Also, this review explore the historical evolution, computational efficiency, and ongoing challenges in loss function design, underlining the need for more adaptive and robust solutions. Emphasis is placed on complex scenarios involving multi-modal data, class imbalances, and real-world constraints. Finally, we identify key future directions, advocating for loss functions that enhance interpretability, scalability, and generalization, leading to more effective and resilient deep learning models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traversal Learning: A Lossless And Efficient Distributed Learning Framework</title>
<link>https://arxiv.org/abs/2504.07471</link>
<guid>https://arxiv.org/abs/2504.07471</guid>
<content:encoded><![CDATA[
arXiv:2504.07471v2 Announce Type: replace 
Abstract: In this paper, we introduce Traversal Learning (TL), a novel approach designed to address the problem of decreased quality encountered in popular distributed learning (DL) paradigms such as Federated Learning (FL), Split Learning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an accuracy drop during aggregation due to its averaging function, while SL and SFL face increased loss due to the independent gradient updates on each split network. TL adopts a unique strategy where the model traverses the nodes during forward propagation (FP) and performs backward propagation (BP) on the orchestrator, effectively implementing centralized learning (CL) principles within a distributed environment. The orchestrator is tasked with generating virtual batches and planning the sequential node visits of the model during FP, aligning them with the ordered index of the data within these batches. We conducted experiments on six datasets representing diverse characteristics across various domains. Our evaluation demonstrates that TL is on par with classic CL approaches in terms of accurate inference, thereby offering a viable and robust solution for DL tasks. TL outperformed other DL methods and improved accuracy by 7.85% for independent and identically distributed (IID) datasets, macro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text classification, and AUC by 3.88% and 4.54% for medical and financial datasets, respectively. By effectively preserving data privacy while maintaining performance, TL represents a significant advancement in DL methodologies. The implementation of TL is available at https://github.com/neouly-inc/Traversal-Learning
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Deep Morphological Neural Networks as Universal Approximators</title>
<link>https://arxiv.org/abs/2505.09710</link>
<guid>https://arxiv.org/abs/2505.09710</guid>
<content:encoded><![CDATA[
arXiv:2505.09710v2 Announce Type: replace 
Abstract: We investigate deep morphological neural networks (DMNNs). We demonstrate that despite their inherent non-linearity, "linear" activations are essential for DMNNs. To preserve their inherent sparsity, we propose architectures that constraint the parameters of the "linear" activations: For the first (resp. second) architecture, we work under the constraint that the majority of parameters (resp. learnable parameters) should be part of morphological operations. We improve the generalization ability of our networks via residual connections and weight dropout. Our proposed networks can be successfully trained, and are more prunable than linear networks. To the best of our knowledge, we are the first to successfully train DMNNs under such constraints. Finally, we propose a hybrid network architecture combining linear and morphological layers, showing empirically that the inclusion of morphological layers significantly accelerates the convergence of gradient descent with large batches.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features</title>
<link>https://arxiv.org/abs/2505.12151</link>
<guid>https://arxiv.org/abs/2505.12151</guid>
<content:encoded><![CDATA[
arXiv:2505.12151v2 Announce Type: replace 
Abstract: Large language models have recently made great strides in reasoning task performance through chain-of-thought (CoT) strategies trained via reinforcement learning; however, these "reasoning large language models" (RLLMs) remain imperfect reasoners, and understanding the frequencies and causes of their failure modes is important for both users and developers. We test o1-mini, o3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3 Mini Beta on graph coloring as a variable-complexity constraint-satisfaction logic problem, and find evidence from both error rate comparisons and CoT/explanation text analysis that RLLMs are prone to hallucinate graph edges not specified in the prompt. This phenomenon persists across multiple problem complexity levels and semantic frames, and it appears to account for a significant fraction of the incorrect answers from every tested model, and the vast majority of them for some models. We also validate the generalizability of this input-conflicting hallucination phenomenon with smaller-scale experiments on a type of stable matching problem. Our results indicate that RLLMs may possess broader issues with misrepresentation of problem specifics, and we offer suggestions for design choices to mitigate this weakness.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOFT: Householder Orthogonal Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16531</link>
<guid>https://arxiv.org/abs/2505.16531</guid>
<content:encoded><![CDATA[
arXiv:2505.16531v2 Announce Type: replace 
Abstract: Adaptation of foundation models using low-rank methods is a widespread approach. Another way to adapt these models is to employ orthogonal fine-tuning methods, which are less time and memory efficient despite their good generalization properties. In this work, we propose Householder Orthogonal Fine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to alleviate time and space complexity. Moreover, some theoretical properties of the orthogonal fine-tuning paradigm are explored. From this exploration, Scaled Householder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are evaluated in downstream tasks, namely commonsense reasoning, machine translation, subject-driven generation and mathematical reasoning. Compared with state-of-the-art adaptation methods, HOFT and SHOFT show comparable or better results.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods</title>
<link>https://arxiv.org/abs/2505.18565</link>
<guid>https://arxiv.org/abs/2505.18565</guid>
<content:encoded><![CDATA[
arXiv:2505.18565v4 Announce Type: replace 
Abstract: Physics-informed neural networks (PINNs) have emerged as a promising approach for solving complex fluid dynamics problems, yet their application to fluid-structure interaction (FSI) problems with moving boundaries remains largely unexplored. This work addresses the critical challenge of modeling FSI systems with deformable interfaces, where traditional unified PINN architectures struggle to capture the distinct physics governing fluid and structural domains simultaneously. We present an innovative Eulerian-Lagrangian PINN architecture that integrates immersed boundary method (IBM) principles to solve FSI problems with moving boundary conditions. Our approach fundamentally departs from conventional unified architectures by introducing domain-specific neural networks: an Eulerian network for fluid dynamics and a Lagrangian network for structural interfaces, coupled through physics-based constraints. Additionally, we incorporate learnable B-spline activation functions with SiLU to capture both localized high-gradient features near interfaces and global flow patterns. Empirical studies on a 2D cavity flow problem involving a moving solid structure show that while baseline unified PINNs achieve reasonable velocity predictions, they suffer from substantial pressure errors (12.9%) in structural regions. Our Eulerian-Lagrangian architecture with learnable activations (EL-L) achieves better performance across all metrics, improving accuracy by 24.1-91.4% and particularly reducing pressure errors from 12.9% to 2.39%. These results demonstrate that domain decomposition aligned with physical principles, combined with locality-aware activation functions, is essential for accurate FSI modeling within the PINN framework.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Certified Unlearning Approach without Access to Source Data</title>
<link>https://arxiv.org/abs/2506.06486</link>
<guid>https://arxiv.org/abs/2506.06486</guid>
<content:encoded><![CDATA[
arXiv:2506.06486v2 Announce Type: replace 
Abstract: With the growing adoption of data privacy regulations, the ability to erase private or copyrighted information from trained models has become a crucial requirement. Traditional unlearning methods often assume access to the complete training dataset, which is unrealistic in scenarios where the source data is no longer available. To address this challenge, we propose a certified unlearning framework that enables effective data removal \final{without access to the original training data samples}. Our approach utilizes a surrogate dataset that approximates the statistical properties of the source data, allowing for controlled noise scaling based on the statistical distance between the two. \updated{While our theoretical guarantees assume knowledge of the exact statistical distance, practical implementations typically approximate this distance, resulting in potentially weaker but still meaningful privacy guarantees.} This ensures strong guarantees on the model's behavior post-unlearning while maintaining its overall utility. We establish theoretical bounds, introduce practical noise calibration techniques, and validate our method through extensive experiments on both synthetic and real-world datasets. The results demonstrate the effectiveness and reliability of our approach in privacy-sensitive settings.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rescaled Influence Functions: Accurate Data Attribution in High Dimension</title>
<link>https://arxiv.org/abs/2506.06656</link>
<guid>https://arxiv.org/abs/2506.06656</guid>
<content:encoded><![CDATA[
arXiv:2506.06656v2 Announce Type: replace 
Abstract: How does the training data affect a model's behavior? This is the question we seek to answer with data attribution. The leading practical approaches to data attribution are based on influence functions (IF). IFs utilize a first-order Taylor approximation to efficiently predict the effect of removing a set of samples from the training set without retraining the model, and are used in a wide variety of machine learning applications. However, especially in the high-dimensional regime (# params $\geq \Omega($# samples$)$), they are often imprecise and tend to underestimate the effect of sample removals, even for simple models such as logistic regression. We present rescaled influence functions (RIF), a new tool for data attribution which can be used as a drop-in replacement for influence functions, with little computational overhead but significant improvement in accuracy. We compare IF and RIF on a range of real-world datasets, showing that RIFs offer significantly better predictions in practice, and present a theoretical analysis explaining this improvement. Finally, we present a simple class of data poisoning attacks that would fool IF-based detections but would be detected by RIF.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Diffusion in Large Language and Multimodal Models: A Survey</title>
<link>https://arxiv.org/abs/2506.13759</link>
<guid>https://arxiv.org/abs/2506.13759</guid>
<content:encoded><![CDATA[
arXiv:2506.13759v4 Announce Type: replace 
Abstract: In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output control, and dynamic perception. These capabilities are previously difficult to achieve with AR models. A growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to \textit{10$\times$} acceleration in inference speed. These developments position discrete diffusion models as a promising alternative to intelligence based on the traditional autoregressive approach. In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains and \textit{etc.}. We conclude by discussing future directions for research and deployment. Relative papers are collected in https://github.com/LiQiiiii/Awesome-Discrete-Diffusion-LLM_MLLM
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data</title>
<link>https://arxiv.org/abs/2506.23629</link>
<guid>https://arxiv.org/abs/2506.23629</guid>
<content:encoded><![CDATA[
arXiv:2506.23629v2 Announce Type: replace 
Abstract: The integrity of Water Quality Data (WQD) is critical in environmental monitoring for scientific decision-making and ecological protection. However, water quality monitoring systems are often challenged by large amounts of missing data due to unavoidable problems such as sensor failures and communication delays, which further lead to water quality data becoming High-Dimensional and Sparse (HDS). Traditional data imputation methods are difficult to depict the potential dynamics and fail to capture the deep data features, resulting in unsatisfactory imputation performance. To effectively address the above issues, this paper proposes a Nonlinear Low-rank Representation model (NLR) with Convolutional Neural Networks (CNN) for imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing temporal features to model the temporal dependence of data between time slots, and b) Extracting nonlinear interactions and local patterns to mine higher-order relationships features and achieve deep fusion of multidimensional information. Experimental studies on three real water quality datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art data imputation models in terms of estimation accuracy. It provides an effective approach for handling water quality monitoring data in complex dynamic environments.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comprehensive Evaluation of Prototype Neural Networks</title>
<link>https://arxiv.org/abs/2507.06819</link>
<guid>https://arxiv.org/abs/2507.06819</guid>
<content:encoded><![CDATA[
arXiv:2507.06819v2 Announce Type: replace 
Abstract: Prototype models are an important method for explainable artificial intelligence (XAI) and interpretable machine learning. In this paper, we perform an in-depth analysis of a set of prominent prototype models including ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive set of metrics. In addition to applying standard metrics from literature, we propose several new metrics to further complement the analysis of model interpretability. In our experimentation, we apply the set of prototype models on a diverse set of datasets including fine-grained classification, Non-IID settings and multi-label classification to further contrast the performance. Furthermore, we also provide our code as an open-source library (https://github.com/uos-sis/quanproto), which facilitates simple application of the metrics itself, as well as extensibility -- providing the option for easily adding new metrics and models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Should We Meta-Learn Reinforcement Learning Algorithms?</title>
<link>https://arxiv.org/abs/2507.17668</link>
<guid>https://arxiv.org/abs/2507.17668</guid>
<content:encoded><![CDATA[
arXiv:2507.17668v2 Announce Type: replace 
Abstract: The process of meta-learning algorithms from data, instead of relying on manual design, is growing in popularity as a paradigm for improving the performance of machine learning systems. Meta-learning shows particular promise for reinforcement learning (RL), where algorithms are often adapted from supervised or unsupervised learning despite their suboptimality for RL. However, until now there has been a severe lack of comparison between different meta-learning algorithms, such as using evolution to optimise over black-box functions or LLMs to propose code. In this paper, we carry out this empirical comparison of the different approaches when applied to a range of meta-learned algorithms which target different parts of the RL pipeline. In addition to meta-train and meta-test performance, we also investigate factors including the interpretability, sample cost and train time for each meta-learning algorithm. Based on these findings, we propose several guidelines for meta-learning new RL algorithms which will help ensure that future learned algorithms are as performant as possible.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KLLM: Fast LLM Inference with K-Means Quantization</title>
<link>https://arxiv.org/abs/2507.23035</link>
<guid>https://arxiv.org/abs/2507.23035</guid>
<content:encoded><![CDATA[
arXiv:2507.23035v3 Announce Type: replace 
Abstract: Large language model (LLM) inference poses significant challenges due to its intensive memory and computation demands. Weight and activation quantization (WAQ) offers a promising solution by reducing both memory footprint and arithmetic complexity. Traditional WAQ designs rely on uniform integer quantization for hardware efficiency, but often suffer from significant model performance degradation at low precision. In contrast, K-Means quantization, a non-uniform technique, achieves higher accuracy by aligning with the Gaussian-like distributions of weights and activations in LLMs. However, two key challenges prevent the efficient deployment of K-Means-based WAQ designs for LLM inference: (1) The non-uniform structure of K-Means-quantized data precludes direct execution on low-precision compute units, necessitating dequantization and floating-point matrix multiplications (MatMuls) during inference. (2) Activation outliers hinder effective low-precision quantization. Offline thresholding methods for outlier detection degrade model performance substantially, while existing online detection techniques introduce significant runtime overhead. To address the aforementioned challenges and fully unleash the potential of K-Means-based WAQ for LLM inference, in this paper, we propose KLLM, an LLM inference accelerator for efficient execution with K-Means-quantized weights and activations. KLLM features an index-based computation scheme for efficient execution of MatMuls and nonlinear operations on K-Means-quantized data, which avoids most of the dequantization and full-precision computations. Moreover, KLLM incorporates a lightweight outlier detection engine, Orizuru, that efficiently identifies the top-$k$ largest and smallest elements in the activation data stream during online inference.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses</title>
<link>https://arxiv.org/abs/2507.23674</link>
<guid>https://arxiv.org/abs/2507.23674</guid>
<content:encoded><![CDATA[
arXiv:2507.23674v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) process millions of queries daily, making efficient response caching a compelling optimization for reducing cost and latency. However, preserving relevance to user queries using this approach proves difficult due to the personalized nature of chatbot interactions and the limited accuracy of semantic similarity search. To address this, we present TweakLLM, a novel routing architecture that employs a lightweight LLM to dynamically adapt cached responses to incoming prompts. Through comprehensive evaluation, including user studies with side-by-side comparisons, satisfaction voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM maintains response quality comparable to frontier models while significantly improving cache effectiveness. Our results across real-world datasets highlight TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM deployments without compromising user experience.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Questioning Language Models</title>
<link>https://arxiv.org/abs/2508.03682</link>
<guid>https://arxiv.org/abs/2508.03682</guid>
<content:encoded><![CDATA[
arXiv:2508.03682v4 Announce Type: replace 
Abstract: Can large language models improve without external data -- by generating their own questions and answers? We hypothesize that a pre-trained language model can improve its reasoning skills given only a single prompt specifying the topic (e.g., algebra word problems) and asking the model to generate its own questions. To do this, we propose Self-Questioning Language Models (SQLM): an asymmetric self-play framework where a proposer is given the topic and generates a question for a solver, who tries to answer it. Both the proposer and solver are trained via reinforcement learning. The proposer receives a reward if the problem is not too easy or too difficult, and the solver receives a reward based on majority voting, a proxy for correctness in the absence of ground-truth answers. For coding, the proposer can instead generate unit tests which are used for verification. We study this asymmetric self-play framework on three benchmarks: three-digit multiplication, algebra problems from the OMEGA benchmark, and programming problems from Codeforces. By continually generating more interesting problems and attempting to solve them, language models can improve on downstream benchmarks without access to any curated training datasets.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian</title>
<link>https://arxiv.org/abs/2508.12993</link>
<guid>https://arxiv.org/abs/2508.12993</guid>
<content:encoded><![CDATA[
arXiv:2508.12993v2 Announce Type: replace 
Abstract: A common observation in the Graph Convolutional Network (GCN) literature is that stacking GCN layers may or may not result in better performance on tasks like node classification and edge prediction. We have found empirically that a graph's algebraic connectivity, which is known as the Fiedler value, is a good predictor of GCN performance. Intuitively, graphs with similar Fiedler values have analogous structural properties, suggesting that the same filters and hyperparameters may yield similar results when used with GCNs, and that transfer learning may be more effective between graphs with similar algebraic connectivity. We explore this theoretically and empirically with experiments on synthetic and real graph data, including the Cora, CiteSeer and Polblogs datasets. We explore multiple ways of aggregating the Fiedler value for connected components in the graphs to arrive at a value for the entire graph, and show that it can be used to predict GCN performance. We also present theoretical arguments as to why the Fiedler value is a good predictor.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metis: Training Large Language Models with Advanced Low-Bit Quantization</title>
<link>https://arxiv.org/abs/2509.00404</link>
<guid>https://arxiv.org/abs/2509.00404</guid>
<content:encoded><![CDATA[
arXiv:2509.00404v2 Announce Type: replace 
Abstract: This work identifies anisotropic parameter distributions as a fundamental barrier to training large language models (LLMs) with low-bit quantization: a few dominant singular values create wide numerical ranges that conflict with the inherent bias of block-wise quantization. This bias disproportionately preserves high-magnitude values while discarding smaller ones, causing training instability and low model performance. This work introduces Metis, a training framework that combines (i) spectral decomposition with random embedding to efficiently disentangle dominant from long-tail components, compressing broad distributions into quantization-friendly narrow ranges; (ii) adaptive learning rates in the spectral domain to amplify underrepresented directions and better capture diverse features critical for performance; and (iii) a dual-range regularizer that jointly constrains numerical precision and parameter range distribution, ensuring stable, unbiased low-bit training. With Metis, FP8 training surpasses FP32 baselines, and FP4 training achieves accuracy comparable to FP32, paving the way for robust and scalable LLM training under advanced low-bit quantization. The code implementation for Metis is available at: https://github.com/sii-research/Metis.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models</title>
<link>https://arxiv.org/abs/2509.00614</link>
<guid>https://arxiv.org/abs/2509.00614</guid>
<content:encoded><![CDATA[
arXiv:2509.00614v2 Announce Type: replace 
Abstract: In the era of foundation models, fine-tuning pre-trained models for specific downstream tasks has become crucial. This drives the need for robust fine-tuning methods to address challenges such as model overfitting and sparse labeling. Molecular graph foundation models (MGFMs) face unique difficulties that complicate fine-tuning. These models are limited by smaller pre-training datasets and more severe data scarcity for downstream tasks, both of which require enhanced model generalization. Moreover, MGFMs must accommodate diverse objectives, including both regression and classification tasks. To better understand and improve fine-tuning techniques under these conditions, we classify eight fine-tuning methods into three mechanisms: weight-based, representation-based, and partial fine-tuning. We benchmark these methods on downstream regression and classification tasks across supervised and self-supervised pre-trained models in diverse labeling settings. This extensive evaluation provides valuable insights and informs the design of a refined robust fine-tuning method, ROFT-MOL. This approach combines the strengths of simple post-hoc weight interpolation with more complex weight ensemble fine-tuning methods, delivering improved performance across both task types while maintaining the ease of use inherent in post-hoc weight interpolation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Second-Order Tensorial Partial Differential Equations on Graphs</title>
<link>https://arxiv.org/abs/2509.02015</link>
<guid>https://arxiv.org/abs/2509.02015</guid>
<content:encoded><![CDATA[
arXiv:2509.02015v2 Announce Type: replace 
Abstract: Processing data on multiple interacting graphs is crucial for many applications, but existing approaches rely mostly on discrete filtering or first-order continuous models that dampen high frequencies and propagate information slowly. We introduce second-order tensorial partial differential equations on graphs (So-TPDEGs) and propose the first theoretically grounded framework for second-order continuous product graph neural networks. Our method exploits the separability of cosine kernels in Cartesian product graphs to enable efficient spectral decomposition while preserving high-frequency signals. We further provide rigorous analyses of stability under graph perturbations and over-smoothing, establishing a solid theoretical foundation for continuous graph learning.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing Information in Domain-Invariant Representation Improves Transfer Learning</title>
<link>https://arxiv.org/abs/2306.00262</link>
<guid>https://arxiv.org/abs/2306.00262</guid>
<content:encoded><![CDATA[
arXiv:2306.00262v5 Announce Type: replace-cross 
Abstract: We propose MaxDIRep, a domain adaptation method that improves the decomposition of data representations into domain-independent and domain-dependent components. Existing methods, such as Domain-Separation Networks (DSN), use a weak orthogonality constraint between these components, which can lead to label-relevant features being partially encoded in the domain-dependent representation (DDRep) rather than the domain-independent representation (DIRep). As a result, information crucial for target-domain classification may be missing from the DIRep. MaxDIRep addresses this issue by applying a Kullback-Leibler (KL) divergence constraint to minimize the information content of the DDRep, thereby encouraging the DIRep to retain features that are both domain-invariant and predictive of target labels. Through geometric analysis and an ablation study on synthetic datasets, we show why DSN's weaker constraint can lead to suboptimal adaptation. Experiments on standard image benchmarks and a network intrusion detection task demonstrate that MaxDIRep achieves strong performance, works with pretrained models, and generalizes to non-image classification tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Channel Bias to Feature Redundancy: Uncovering the "Less is More" Principle in Few-Shot Learning</title>
<link>https://arxiv.org/abs/2310.03843</link>
<guid>https://arxiv.org/abs/2310.03843</guid>
<content:encoded><![CDATA[
arXiv:2310.03843v2 Announce Type: replace-cross 
Abstract: Deep neural networks often fail to adapt representations to novel tasks under distribution shifts, especially when only a few examples are available. This paper identifies a core obstacle behind this failure: channel bias, where networks develop a rigid emphasis on feature dimensions that were discriminative for the source task, but this emphasis is misaligned and fails to adapt to the distinct needs of a novel task. This bias leads to a striking and detrimental consequence: feature redundancy. We demonstrate that for few-shot tasks, classification accuracy is significantly improved by using as few as 1-5% of the most discriminative feature dimensions, revealing that the vast majority are actively harmful. Our theoretical analysis confirms that this redundancy originates from confounding feature dimensions-those with high intra-class variance but low inter-class separability-which are especially problematic in low-data regimes. This "less is more" phenomenon is a defining characteristic of the few-shot setting, diminishing as more samples become available. To address this, we propose a simple yet effective soft-masking method, Augmented Feature Importance Adjustment (AFIA), which estimates feature importance from augmented data to mitigate the issue. By establishing the cohesive link from channel bias to its consequence of extreme feature redundancy, this work provides a foundational principle for few-shot representation transfer and a practical method for developing more robust few-shot learning algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems with Convex Constraints</title>
<link>https://arxiv.org/abs/2311.09065</link>
<guid>https://arxiv.org/abs/2311.09065</guid>
<content:encoded><![CDATA[
arXiv:2311.09065v2 Announce Type: replace-cross 
Abstract: We give a damped proximal augmented Lagrangian method (DPALM) for solving problems with a weakly-convex objective and convex linear/nonlinear constraints. Instead of taking a full stepsize, DPALM adopts a damped dual stepsize to ensure the boundedness of dual iterates. We show that DPALM can produce a (near) $\vareps$-KKT point within $O(\vareps^{-2})$ outer iterations if each DPALM subproblem is solved to a proper accuracy. In addition, we establish overall iteration complexity of DPALM when the objective is either a regularized smooth function or in a regularized compositional form. For the former case, DPALM achieves the complexity of $\widetilde{\mathcal{O}}\left(\varepsilon^{-2.5} \right)$ to produce an $\varepsilon$-KKT point by applying an accelerated proximal gradient (APG) method to each DPALM subproblem. For the latter case, the complexity of DPALM is $\widetilde{\mathcal{O}}\left(\varepsilon^{-3} \right)$ to produce a near $\varepsilon$-KKT point by using an APG to solve a Moreau-envelope smoothed version of each subproblem. Our outer iteration complexity and the overall complexity either generalize existing best ones from unconstrained or linear-constrained problems to convex-constrained ones, or improve over the best-known results on solving the same-structured problems. Furthermore, numerical experiments on linearly/quadratically constrained non-convex quadratic programs and linear-constrained robust nonlinear least squares are conducted to demonstrate the empirical efficiency of the proposed DPALM over several state-of-the art methods.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation</title>
<link>https://arxiv.org/abs/2402.04355</link>
<guid>https://arxiv.org/abs/2402.04355</guid>
<content:encoded><![CDATA[
arXiv:2402.04355v3 Announce Type: replace-cross 
Abstract: We propose a likelihood-free method for comparing two distributions given samples from each, with the goal of assessing the quality of generative models. The proposed approach, PQMass, provides a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models. PQMass divides the sample space into non-overlapping regions and applies chi-squared tests to the number of data samples that fall within each region, giving a p-value that measures the probability that the bin counts derived from two sets of samples are drawn from the same multinomial distribution. PQMass does not depend on assumptions regarding the density of the true distribution, nor does it rely on training or fitting any auxiliary models. We evaluate PQMass on data of various modalities and dimensions, demonstrating its effectiveness in assessing the quality, novelty, and diversity of generated samples. We further show that PQMass scales well to moderately high-dimensional data and thus obviates the need for feature extraction in practical applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical-Computational Trade-offs for Recursive Adaptive Partitioning Estimators</title>
<link>https://arxiv.org/abs/2411.04394</link>
<guid>https://arxiv.org/abs/2411.04394</guid>
<content:encoded><![CDATA[
arXiv:2411.04394v3 Announce Type: replace-cross 
Abstract: Models based on recursive adaptive partitioning such as decision trees and their ensembles are popular for high-dimensional regression as they can potentially avoid the curse of dimensionality. Because empirical risk minimization (ERM) is computationally infeasible, these models are typically trained using greedy algorithms. Although effective in many cases, these algorithms have been empirically observed to get stuck at local optima. We explore this phenomenon in the context of learning sparse regression functions over $d$ binary features, showing that when the true regression function $f^*$ does not satisfy Abbe et al. (2022)'s Merged Staircase Property (MSP), greedy training requires $\exp(\Omega(d))$ to achieve low estimation error. Conversely, when $f^*$ does satisfy MSP, greedy training can attain small estimation error with only $O(\log d)$ samples. This dichotomy mirrors that of two-layer neural networks trained with stochastic gradient descent (SGD) in the mean-field regime, thereby establishing a head-to-head comparison between SGD-trained neural networks and greedy recursive partitioning estimators. Furthermore, ERM-trained recursive partitioning estimators achieve low estimation error with $O(\log d)$ samples irrespective of whether $f^*$ satisfies MSP, thereby demonstrating a statistical-computational trade-off for greedy training. Our proofs are based on a novel interpretation of greedy recursive partitioning using stochastic process theory and a coupling technique that may be of independent interest.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Downlink MIMO Channel Estimation from Bits: Recoverability and Algorithm</title>
<link>https://arxiv.org/abs/2411.16043</link>
<guid>https://arxiv.org/abs/2411.16043</guid>
<content:encoded><![CDATA[
arXiv:2411.16043v2 Announce Type: replace-cross 
Abstract: In frequency division duplex (FDD) massive MIMO systems, a major challenge lies in acquiring the downlink channel state information}\ (CSI) at the base station (BS) from limited feedback sent by the user equipment (UE). To tackle this fundamental task, our contribution is twofold: First, a simple feedback framework is proposed, where a compression and Gaussian dithering-based quantization strategy is adopted at the UE side, and then a maximum likelihood estimator (MLE) is formulated at the BS side. Recoverability of the MIMO channel under the widely used double directional model is established. Specifically, analyses are presented for two compression schemes -- showing one being more overhead-economical and the other computationally lighter at the UE side. Second, to realize the MLE, an alternating direction method of multipliers (ADMM) algorithm is proposed. The algorithm is carefully designed to integrate a sophisticated harmonic retrieval (HR) solver as subroutine, which turns out to be the key of effectively tackling this hard MLE problem.Extensive numerical experiments are conducted to validate the efficacy of our approach.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A single-loop SPIDER-type stochastic subgradient method for expectation-constrained nonconvex nonsmooth optimization</title>
<link>https://arxiv.org/abs/2501.19214</link>
<guid>https://arxiv.org/abs/2501.19214</guid>
<content:encoded><![CDATA[
arXiv:2501.19214v2 Announce Type: replace-cross 
Abstract: Many real-world problems, such as those with fairness constraints, involve complex expectation constraints and large datasets, necessitating the design of efficient stochastic methods to solve them. Most existing research focuses on cases with no {constraint} or easy-to-project constraints or deterministic constraints. In this paper, we consider nonconvex nonsmooth stochastic optimization problems with expectation constraints, for which we build a novel exact penalty model. We first show the relationship between the penalty model and the original problem. Then on solving the penalty problem, we present a single-loop SPIDER-type stochastic subgradient method, which utilizes the subgradients of both the objective and constraint functions, as well as the constraint function value at each iteration. Under certain regularity conditions (weaker than Slater-type constraint qualification or strong feasibility assumed in existing works), we establish an iteration complexity result of $O(\epsilon^{-4})$ to reach a near-$\epsilon$ stationary point of the penalized problem in expectation, matching the lower bound for such tasks. Building on the exact penalization, an $(\epsilon,\epsilon)$-KKT point of the original problem is obtained. For a few scenarios, our complexity of either the {objective} sample subgradient or the constraint sample function values can be lower than the state-of-the-art results by a factor of $\epsilon^{-2}$. Moreover, on solving two fairness-constrained problems and a multi-class Neyman-Pearson classification problem, our method is significantly (up to 466 times) faster than the state-of-the-art algorithms, including switching subgradient method and inexact proximal point methods.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPO: Boosting LLM Agents with Meta Plan Optimization</title>
<link>https://arxiv.org/abs/2503.02682</link>
<guid>https://arxiv.org/abs/2503.02682</guid>
<content:encoded><![CDATA[
arXiv:2503.02682v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, , which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Randomized Zeroth-Order Hierarchical Framework for Heterogeneous Federated Learning</title>
<link>https://arxiv.org/abs/2504.01839</link>
<guid>https://arxiv.org/abs/2504.01839</guid>
<content:encoded><![CDATA[
arXiv:2504.01839v2 Announce Type: replace-cross 
Abstract: Heterogeneity in federated learning (FL) is a critical and challenging aspect that significantly impacts model performance and convergence. In this paper, we propose a novel framework by formulating heterogeneous FL as a hierarchical optimization problem. This new framework captures both local and global training processes through a bilevel formulation and is capable of the following: (i) addressing client heterogeneity through a personalized learning framework; (ii) capturing the pre-training process on the server side; (iii) updating the global model through nonstandard aggregation; (iv) allowing for nonidentical local steps; and (v) capturing clients' local constraints. We design and analyze an implicit zeroth-order FL method (ZO-HFL), equipped with nonasymptotic convergence guarantees for both the server-agent and the individual client-agents, and asymptotic guarantees for both the server-agent and client-agents in an almost sure sense. Notably, our method does not rely on standard assumptions in heterogeneous FL, such as the bounded gradient dissimilarity condition. We implement our method on image classification tasks and compare with other methods under different heterogeneous settings.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dexterous Manipulation through Imitation Learning: A Survey</title>
<link>https://arxiv.org/abs/2504.03515</link>
<guid>https://arxiv.org/abs/2504.03515</guid>
<content:encoded><![CDATA[
arXiv:2504.03515v4 Announce Type: replace-cross 
Abstract: Dexterous manipulation, which refers to the ability of a robotic hand or multi-fingered end-effector to skillfully control, reorient, and manipulate objects through precise, coordinated finger movements and adaptive force modulation, enables complex interactions similar to human hand dexterity. With recent advances in robotics and machine learning, there is a growing demand for these systems to operate in complex and unstructured environments. Traditional model-based approaches struggle to generalize across tasks and object variations due to the high dimensionality and complex contact dynamics of dexterous manipulation. Although model-free methods such as reinforcement learning (RL) show promise, they require extensive training, large-scale interaction data, and carefully designed rewards for stability and effectiveness. Imitation learning (IL) offers an alternative by allowing robots to acquire dexterous manipulation skills directly from expert demonstrations, capturing fine-grained coordination and contact dynamics while bypassing the need for explicit modeling and large-scale trial-and-error. This survey provides an overview of dexterous manipulation methods based on imitation learning, details recent advances, and addresses key challenges in the field. Additionally, it explores potential research directions to enhance IL-driven dexterous manipulation. Our goal is to offer researchers and practitioners a comprehensive introduction to this rapidly evolving domain.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real Time Semantic Segmentation of High Resolution Automotive LiDAR Scans</title>
<link>https://arxiv.org/abs/2504.21602</link>
<guid>https://arxiv.org/abs/2504.21602</guid>
<content:encoded><![CDATA[
arXiv:2504.21602v2 Announce Type: replace-cross 
Abstract: In recent studies, numerous previous works emphasize the importance of semantic segmentation of LiDAR data as a critical component to the development of driver-assistance systems and autonomous vehicles. However, many state-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors and struggle with real-time constraints. This study introduces a novel semantic segmentation framework tailored for modern high-resolution LiDAR sensors that addresses both accuracy and real-time processing demands. We propose a novel LiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban traffic scenes. Furthermore, we propose a semantic segmentation method utilizing surface normals as strong input features. Our approach is bridging the gap between cutting-edge research and practical automotive applications. Additionaly, we provide a Robot Operating System (ROS2) implementation that we operate on our research vehicle. Our dataset and code are publicly available: https://github.com/kav-institute/SemanticLiDAR.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Semantics Augmented Few-Shot Relational Learning</title>
<link>https://arxiv.org/abs/2505.05684</link>
<guid>https://arxiv.org/abs/2505.05684</guid>
<content:encoded><![CDATA[
arXiv:2505.05684v2 Announce Type: replace-cross 
Abstract: Few-shot relational learning on knowledge graph (KGs) aims to perform reasoning over relations with only a few training examples. While existing methods have primarily focused on leveraging specific relational information, rich semantics inherent in KGs have been largely overlooked. To address this critical gap, we propose a novel prompted meta-learning (PromptMeta) framework that seamlessly integrates meta-semantics with relational information for few-shot relational learning. PromptMeta has two key innovations: (1) a Meta-Semantic Prompt (MSP) pool that learns and consolidates high-level meta-semantics, enabling effective knowledge transfer and adaptation to rare and newly emerging relations; and (2) a learnable fusion token that dynamically combines meta-semantics with task-specific relational information tailored to different few-shot tasks. Both components are optimized jointly with model parameters within a meta-learning framework. Extensive experiments and analyses on two real-world KG datasets demonstrate the effectiveness of PromptMeta in adapting to new relations with limited data.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Convergence of the Frank-Wolfe Algorithm over Product Polytopes</title>
<link>https://arxiv.org/abs/2505.11259</link>
<guid>https://arxiv.org/abs/2505.11259</guid>
<content:encoded><![CDATA[
arXiv:2505.11259v2 Announce Type: replace-cross 
Abstract: We study the linear convergence of Frank-Wolfe algorithms over product polytopes. We analyze two condition numbers for the product polytope, namely the \emph{pyramidal width} and the \emph{vertex-facet distance}, based on the condition numbers of individual polytope components. As a result, for convex objectives that are $\mu$-Polyak-{\L}ojasiewicz, we show linear convergence rates quantified in terms of the resulting condition numbers. We apply our results to the problem of approximately finding a feasible point in a polytope intersection in high-dimensions, and demonstrate the practical efficiency of our algorithms through empirical results.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks</title>
<link>https://arxiv.org/abs/2506.07392</link>
<guid>https://arxiv.org/abs/2506.07392</guid>
<content:encoded><![CDATA[
arXiv:2506.07392v2 Announce Type: replace-cross 
Abstract: The proliferation of UAVs has enabled a wide range of mission-critical applications and is becoming a cornerstone of low-altitude networks, supporting smart cities, emergency response, and more. However, the open wireless environment, dynamic topology, and resource constraints of UAVs expose low-altitude networks to severe DoS threats. Traditional defense approaches, which rely on fixed configurations or centralized decision-making, cannot effectively respond to the rapidly changing conditions in UAV swarm environments. To address these challenges, we propose a novel federated multi-agent deep reinforcement learning (FMADRL)-driven moving target defense (MTD) framework for proactive DoS mitigation in low-altitude networks. Specifically, we design lightweight and coordinated MTD mechanisms, including leader switching, route mutation, and frequency hopping, to disrupt attacker efforts and enhance network resilience. The defense problem is formulated as a multi-agent partially observable Markov decision process, capturing the uncertain nature of UAV swarms under attack. Each UAV is equipped with a policy agent that autonomously selects MTD actions based on partial observations and local experiences. By employing a policy gradient-based algorithm, UAVs collaboratively optimize their policies via reward-weighted aggregation. Extensive simulations demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving up to a 34.6% improvement in attack mitigation rate, a reduction in average recovery time of up to 94.6%, and decreases in energy consumption and defense cost by as much as 29.3% and 98.3%, respectively, under various DoS attack strategies. These results highlight the potential of intelligent, distributed defense mechanisms to protect low-altitude networks, paving the way for reliable and scalable low-altitude economy.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators</title>
<link>https://arxiv.org/abs/2507.14652</link>
<guid>https://arxiv.org/abs/2507.14652</guid>
<content:encoded><![CDATA[
arXiv:2507.14652v2 Announce Type: replace-cross 
Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample from the posterior distribution in Bayesian inference. However, HMC techniques are computationally demanding for Bayesian neural networks due to the high dimensionality of the network's parameter space and the non-convexity of their posterior distributions. Therefore, various approximation techniques, such as variational inference (VI) or stochastic gradient MCMC, are often employed to infer the posterior distribution of the network parameters. Such approximations introduce inaccuracies in the inferred distributions, resulting in unreliable uncertainty estimates. In this work, we propose a hybrid approach that combines inexpensive VI and accurate HMC methods to efficiently and accurately quantify uncertainties in neural networks and neural operators. The proposed approach leverages an initial VI training on the full network. We examine the influence of individual parameters on the prediction uncertainty, which shows that a large proportion of the parameters do not contribute substantially to uncertainty in the network predictions. This information is then used to significantly reduce the dimension of the parameter space, and HMC is performed only for the subset of network parameters that strongly influence prediction uncertainties. This yields a framework for accelerating the full batch HMC for posterior inference in neural networks. We demonstrate the efficiency and accuracy of the proposed framework on deep neural networks and operator networks, showing that inference can be performed for large networks with tens to hundreds of thousands of parameters. We show that this method can effectively learn surrogates for complex physical systems by modeling the operator that maps from upstream conditions to wall-pressure data on a cone in hypersonic flow.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems</title>
<link>https://arxiv.org/abs/2508.00300</link>
<guid>https://arxiv.org/abs/2508.00300</guid>
<content:encoded><![CDATA[
arXiv:2508.00300v2 Announce Type: replace-cross 
Abstract: Explanations are crucial for building trustworthy AI systems, but a gap often exists between the explanations provided by models and those needed by users. To address this gap, we introduce MetaExplainer, a neuro-symbolic framework designed to generate user-centered explanations. Our approach employs a three-stage process: first, we decompose user questions into machine-readable formats using state-of-the-art large language models (LLM); second, we delegate the task of generating system recommendations to model explainer methods; and finally, we synthesize natural language explanations that summarize the explainer outputs. Throughout this process, we utilize an Explanation Ontology to guide the language models and explainer methods. By leveraging LLMs and a structured approach to explanation generation, MetaExplainer aims to enhance the interpretability and trustworthiness of AI systems across various applications, providing users with tailored, question-driven explanations that better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate a step towards evaluating and utilizing current state-of-the-art explanation frameworks. Our results show high performance across all stages, with a 59.06% F1-score in question reframing, 70% faithfulness in model explanations, and 67% context-utilization in natural language synthesis. User studies corroborate these findings, highlighting the creativity and comprehensiveness of generated explanations. Tested on the Diabetes (PIMA Indian) tabular dataset, MetaExplainer supports diverse explanation types, including Contrastive, Counterfactual, Rationale, Case-Based, and Data explanations. The framework's versatility and traceability from using ontology to guide LLMs suggest broad applicability beyond the tested scenarios, positioning MetaExplainer as a promising tool for enhancing AI explainability across various domains.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning</title>
<link>https://arxiv.org/abs/2509.02492</link>
<guid>https://arxiv.org/abs/2509.02492</guid>
<content:encoded><![CDATA[
arXiv:2509.02492v2 Announce Type: replace-cross 
Abstract: Significant progress in reward modeling over recent years has been driven by a paradigm shift from task-specific designs towards generalist reward models. Despite this trend, developing effective reward models remains a fundamental challenge: the heavy reliance on large-scale labeled preference data. Pre-training on abundant unlabeled data offers a promising direction, but existing approaches fall short of instilling explicit reasoning into reward models. To bridge this gap, we propose a self-training approach that leverages unlabeled data to elicit reward reasoning in reward models. Based on this approach, we develop GRAM-R$^2$, a generative reward model trained to produce not only preference labels but also accompanying reward rationales. GRAM-R$^2$ can serve as a foundation model for reward reasoning and can be applied to a wide range of tasks with minimal or no additional fine-tuning. It can support downstream applications such as response ranking and task-specific reward tuning. Experiments on response ranking, task adaptation, and reinforcement learning from human feedback demonstrate that GRAM-R$^2$ consistently delivers strong performance, outperforming several strong discriminative and generative baselines.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FNODE: Flow-Matching for data-driven simulation of constrained multibody systems</title>
<link>https://arxiv.org/abs/2509.00183</link>
<guid>https://arxiv.org/abs/2509.00183</guid>
<content:encoded><![CDATA[
<div> Flow-Matching Neural Ordinary Differential Equation, data-driven modeling, constrained multibody systems, acceleration vector fields, trajectory data <br />
<br />
Summary: <br />
The article introduces the Flow-Matching Neural Ordinary Differential Equation (FNODE) framework to address challenges in modeling constrained multibody systems. FNODE learns acceleration vector fields directly from trajectory data, improving computational efficiency and prediction accuracy. By supervising accelerations instead of integrated states, FNODE eliminates the need for backpropagation through an ODE solver, enhancing performance. Efficient computation of acceleration targets is achieved using a hybrid Fast Fourier Transform and Finite Difference scheme. FNODE outperforms existing approaches such as MBD-NODE, LSTM networks, and FCNN in various benchmarks like mass-spring-damper systems and double pendulum, showcasing superior accuracy, generalization, and computational efficiency. <div>
arXiv:2509.00183v2 Announce Type: replace 
Abstract: Data-driven modeling of constrained multibody systems faces two persistent challenges: high computational cost and limited long-term prediction accuracy. To address these issues, we introduce the Flow-Matching Neural Ordinary Differential Equation (FNODE), a framework that learns acceleration vector fields directly from trajectory data. By reformulating the training objective to supervise accelerations rather than integrated states, FNODE eliminates the need for backpropagation through an ODE solver, which represents a bottleneck in traditional Neural ODEs. Acceleration targets are computed efficiently using numerical differentiation techniques, including a hybrid Fast Fourier Transform (FFT) and Finite Difference (FD) scheme. We evaluate FNODE on a diverse set of benchmarks, including the single and triple mass-spring-damper systems, double pendulum, slider-crank, and cart-pole. Across all cases, FNODE consistently outperforms existing approaches such as Multi-Body Dynamic Neural ODE (MBD-NODE), Long Short-Term Memory (LSTM) networks, and Fully Connected Neural Networks (FCNN), demonstrating good accuracy, generalization, and computational efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model</title>
<link>https://arxiv.org/abs/2509.06974</link>
<guid>https://arxiv.org/abs/2509.06974</guid>
<content:encoded><![CDATA[
<div> Keywords: sleep quality prediction, spatial-temporal model, domain adaptation, wearable devices, explainability analysis

Summary: 
The paper presents a novel two-stage adaptive spatial-temporal model for predicting sleep quality scores, aiming to provide accessible and reliable forecasting tools for preventive interventions. The model combines multi-scale convolutional layers, recurrent layers, and attention mechanisms to capture spatial interactions and temporal dependencies. A two-stage domain adaptation strategy is employed to enhance generalization, with source-free test-time adaptation for new users. Experimental results show that the proposed model outperforms baseline approaches, achieving a RMSE of 0.216 for a three-day input window and one-day prediction window. The model also demonstrates good predictive performance for longer forecasting horizons. An explainability analysis reveals how different features influence sleep quality, confirming the robustness and adaptability of the framework for personalized sleep forecasting using data from commercial wearable devices. 

<br /><br />Summary: <div>
arXiv:2509.06974v1 Announce Type: new 
Abstract: Sleep quality significantly impacts well-being. Therefore, healthcare providers and individuals need accessible and reliable forecasting tools for preventive interventions. This paper introduces an interpretable, individualized two-stage adaptive spatial-temporal model for predicting sleep quality scores. Our proposed framework combines multi-scale convolutional layers to model spatial interactions across multiple input variables, recurrent layers and attention mechanisms to capture long-term temporal dependencies, and a two-stage domain adaptation strategy to enhance generalization. The first adaptation stage is applied during training to mitigate overfitting on the training set. In the second stage, a source-free test-time adaptation mechanism is employed to adapt the model to new users without requiring labels. We conducted various experiments with five input window sizes (3, 5, 7, 9, and 11 days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model consistently outperformed time series forecasting baseline approaches, including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The best performance was achieved with a three-day input window and a one-day prediction window, yielding a root mean square error (RMSE) of 0.216. Furthermore, the model demonstrated good predictive performance even for longer forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction window), highlighting its practical utility for real-world applications. We also conducted an explainability analysis to examine how different features influence sleep quality. These findings proved that the proposed framework offers a robust, adaptive, and explainable solution for personalized sleep forecasting using sparse data from commercial wearable devices.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2509.06975</link>
<guid>https://arxiv.org/abs/2509.06975</guid>
<content:encoded><![CDATA[
<div> Benchmarking, Transfer Learning, Graph Representation Learning, Self-Supervised Learning, Large-Scale Pretraining<br /><br />Summary: 
The study introduces GSTBench, a benchmark for evaluating the transferability of graph SSL methods. The research focuses on assessing the performance of SSL methods across different datasets to explore cross-dataset transferability. Large-scale pretraining on ogbn-papers100M is conducted, and five representative SSL methods are evaluated on various target graphs. The standardized experimental setup allows for rigorous comparisons based solely on pretraining objectives, revealing that most graph SSL methods struggle to generalize. Surprisingly, the GraphMAE approach consistently improves transfer performance. The analysis of underlying factors provides insights for future research on transferable graph SSL, emphasizing the importance of the "pretrain-then-transfer" paradigm in graph learning. The code for GSTBench is available on GitHub, facilitating further exploration and development in this field. <div>
arXiv:2509.06975v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) has shown great promise in graph representation learning. However, most existing graph SSL methods are developed and evaluated under a single-dataset setting, leaving their cross-dataset transferability largely unexplored and limiting their ability to leverage knowledge transfer and large-scale pretraining, factors that are critical for developing generalized intelligence beyond fitting training data. To address this gap and advance foundation model research for graphs, we present GSTBench, the first systematic benchmark for evaluating the transferability of graph SSL methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate five representative SSL methods across a diverse set of target graphs. Our standardized experimental setup decouples confounding factors such as model architecture, dataset characteristics, and adaptation protocols, enabling rigorous comparisons focused solely on pretraining objectives. Surprisingly, we observe that most graph SSL methods struggle to generalize, with some performing worse than random initialization. In contrast, GraphMAE, a masked autoencoder approach, consistently improves transfer performance. We analyze the underlying factors that drive these differences and offer insights to guide future research on transferable graph SSL, laying a solid foundation for the "pretrain-then-transfer" paradigm in graph learning. Our code is available at https://github.com/SongYYYY/GSTBench.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction</title>
<link>https://arxiv.org/abs/2509.06976</link>
<guid>https://arxiv.org/abs/2509.06976</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic demand prediction, human knowledge, cross-modal feature representation learning, adaptive graph networks, reasoning-based dynamic update

Summary:
The paper introduces a novel approach for traffic demand prediction, leveraging human knowledge and experience in conjunction with traditional temporal traffic data. By integrating structured temporal data with textual data representing human knowledge, the Knowledge-Guided Cross-Modal Feature Representation Learning (KGCM) model is able to uncover latent patterns within traffic data. The model utilizes adaptive graph networks and a cross-modal feature fusion mechanism to learn multimodal data features and employs a reasoning-based dynamic update strategy to optimize parameters dynamically. Experimental results on multiple traffic datasets demonstrate the superior performance of the KGCM model in accurately predicting future traffic demand compared to existing state-of-the-art models. <div>
arXiv:2509.06976v1 Announce Type: new 
Abstract: Traffic demand prediction plays a critical role in intelligent transportation systems. Existing traffic prediction models primarily rely on temporal traffic data, with limited efforts incorporating human knowledge and experience for urban traffic demand forecasting. However, in real-world scenarios, traffic knowledge and experience derived from human daily life significantly influence precise traffic prediction. Such knowledge and experiences can guide the model in uncovering latent patterns within traffic data, thereby enhancing the accuracy and robustness of predictions. To this end, this paper proposes integrating structured temporal traffic data with textual data representing human knowledge and experience, resulting in a novel knowledge-guided cross-modal feature representation learning (KGCM) model for traffic demand prediction. Based on regional transportation characteristics, we construct a prior knowledge dataset using a large language model combined with manual authoring and revision, covering both regional and global knowledge and experiences. The KGCM model then learns multimodal data features through designed local and global adaptive graph networks, as well as a cross-modal feature fusion mechanism. A proposed reasoning-based dynamic update strategy enables dynamic optimization of the graph model's parameters, achieving optimal performance. Experiments on multiple traffic datasets demonstrate that our model accurately predicts future traffic demand and outperforms existing state-of-the-art (SOTA) models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification</title>
<link>https://arxiv.org/abs/2509.06977</link>
<guid>https://arxiv.org/abs/2509.06977</guid>
<content:encoded><![CDATA[
<div> Configuration-first framework, cross-backend compatibility, deep learning systems, CPU, GPU
deterministic adapters, selective fallbacks, reproducible methodology, dependable deployment, heterogeneous runtimes

<br /><br />Summary:
This paper introduces a configuration-first framework for assessing cross-backend compatibility in deep learning systems deployed on various platforms like CPU, GPU, and compiled runtimes. The framework utilizes YAML to separate experiments from code and supports both library and repository models. It employs a three-tier verification protocol to evaluate tensor-level closeness, activation alignment, and task-level metrics. Results from 672 checks across multiple models reveal that 72.0% of runs pass, with most discrepancies occurring under stricter thresholds. Detection models and compiled backends are identified as particularly susceptible to drift, mainly due to non-deterministic post-processing. The study demonstrates that implementing deterministic adapters and selective fallbacks can significantly enhance agreement without compromising performance. This framework represents a pioneering effort in systematically quantifying and addressing cross-backend drift in deep learning, offering a reproducible approach for reliable deployment across diverse runtimes. <div>
arXiv:2509.06977v1 Announce Type: new 
Abstract: This paper presents a configuration-first framework for evaluating cross-backend compatibility in deep learning systems deployed on CPU, GPU, and compiled runtimes. The framework decouples experiments from code using YAML, supports both library and repository models, and employs a three-tier verification protocol covering tensor-level closeness, activation alignment, and task-level metrics. Through 672 checks across multiple models and tolerance settings, we observe that 72.0% of runs pass, with most discrepancies occurring under stricter thresholds. Our results show that detection models and compiled backends are particularly prone to drift, often due to nondeterministic post-processing. We further demonstrate that deterministic adapters and selective fallbacks can substantially improve agreement without significant performance loss. To our knowledge, this is the first unified framework that systematically quantifies and mitigates cross-backend drift in deep learning, providing a reproducible methodology for dependable deployment across heterogeneous runtimes.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Kriging-HDMR-based surrogate model with sample pool-free active learning strategy for reliability analysis</title>
<link>https://arxiv.org/abs/2509.06978</link>
<guid>https://arxiv.org/abs/2509.06978</guid>
<content:encoded><![CDATA[
<div> surrogate model, Kriging, HDMR, reliability analysis, high-dimensional limit state functions
Summary:
The study introduces an active learning surrogate model method based on Kriging-HDMR modeling for reliability analysis to address the curse of dimensionality. The approach involves constructing a composite representation from multiple low-dimensional sub-surrogate models to approximate high-dimensional limit state functions. It includes three stages: developing single-variable sub-surrogate models, identifying coupling-variable sub-surrogate models requirements, and constructing the models. Mathematical models for selection of design of experiment samples are formulated based on different objectives. A candidate sample pool-free approach is utilized for sample selection. Numerical experiments demonstrate the method's computational efficiency and strong predictive accuracy in solving high-dimensional reliability problems. <div>
arXiv:2509.06978v1 Announce Type: new 
Abstract: In reliability engineering, conventional surrogate models encounter the "curse of dimensionality" as the number of random variables increases. While the active learning Kriging surrogate approaches with high-dimensional model representation (HDMR) enable effective approximation of high-dimensional functions and are widely applied to optimization problems, there are rare studies specifically focused on reliability analysis, which prioritizes prediction accuracy in critical regions over uniform accuracy across the entire domain. This study develops an active learning surrogate model method based on the Kriging-HDMR modeling for reliability analysis. The proposed approach facilitates the approximation of high-dimensional limit state functions through a composite representation constructed from multiple low-dimensional sub-surrogate models. The architecture of the surrogate modeling framework comprises three distinct stages: developing single-variable sub-surrogate models for all random variables, identifying the requirements for coupling-variable sub-surrogate models, and constructing the coupling-variable sub-surrogate models. Optimization mathematical models for selection of design of experiment samples are formulated based on each stage's characteristics, with objectives incorporating uncertainty variance, predicted mean, sample location and inter-sample distances. A candidate sample pool-free approach is adopted to achieve the selection of informative samples. Numerical experiments demonstrate that the proposed method achieves high computational efficiency while maintaining strong predictive accuracy in solving high-dimensional reliability problems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Over-stationarization in Deep Learning-based Bus/Tram Arrival Time Prediction: Analysis and Non-stationary Effect Recovery</title>
<link>https://arxiv.org/abs/2509.06979</link>
<guid>https://arxiv.org/abs/2509.06979</guid>
<content:encoded><![CDATA[
<div> deep learning, arrival time prediction, public transport, non-stationarity, multi-step

Summary:
The article introduces the Non-Stationary Arrival Time Prediction (NSATP) approach for improving the predictability of public transport vehicles' arrival times. NSATP consists of two stages: series stationarization and non-stationarity effect recovery. The first stage focuses on enhancing predictability, while the second stage extends existing models to capture hidden periodicity and mitigate over-stationarization effects. Validation using 125 days of operational data from Dresden shows that NSATP outperforms baseline methods, reducing RMSE, MAE, and MAPE for trams and buses. Specifically, NSATP achieves a reduction of 2.37%, 1.22%, and 2.26% for trams, and 1.72%, 0.60%, and 1.17% for buses, respectively. This research highlights the importance of addressing non-stationarity in time series data for accurate arrival time predictions in public transport systems. 

<br /><br />Summary: <div>
arXiv:2509.06979v1 Announce Type: new 
Abstract: Arrival time prediction (ATP) of public transport vehicles is essential in improving passenger experience and supporting traffic management. Deep learning has demonstrated outstanding performance in ATP due to its ability to model non-linear and temporal dynamics. In the multi-step ATP, non-stationary data will degrade the model performance due to the variation in variables' joint distribution along the temporal direction. Previous studies mainly applied normalization to eliminate the non-stationarity in time series, thereby achieving better predictability. However, the normalization may obscure useful characteristics inherent in non-stationarity, which is known as the over-stationarization. In this work, to trade off predictability and non-stationarity, a new approach for multi-step ATP, named non-stationary ATP ( NSATP), is proposed. The method consists of two stages: series stationarization and non-stationarity effect recovery. The first stage aims at improving the predictability. As for the latter, NSATP extends a state-of-the-art method from one-dimensional to two dimensional based models to capture the hidden periodicity in time series and designs a compensation module of over-stationarization by learning scaling and shifting factors from raw data. 125 days' public transport operational data of Dresden is collected for validation. Experimental results show that compared to baseline methods, the proposed NSATP can reduce RMSE, MAE, and MAPE by 2.37%, 1.22%, and 2.26% for trams and by 1.72%, 0.60%, and 1.17% for buses, respectively.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use</title>
<link>https://arxiv.org/abs/2509.06980</link>
<guid>https://arxiv.org/abs/2509.06980</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Large Language Models, Tool Use, RLFactory, NQ dataset  
Summary:  
RLFactory is a reinforcement learning post-training framework designed to enhance the performance of large language models (LLMs) in tasks that require interaction with external tools. It addresses issues related to tool-call stability, adaptability, and interface heterogeneity by employing an asyncio-based asynchronous caller and a decoupled tool/training architecture. The framework also includes a reward layer that supports various signals for evaluation, such as rule-based, model-judgment, and tool-verification signals. By introducing observation markers from tool feedback and implementing a generate-parse-invoke-update workflow, RLFactory facilitates dynamic policy optimization in multi-round tool use scenarios. In experiments on the Natural Questions (NQ) dataset, RLFactory achieves a test score of 0.486 on the Search-R1 task, outperforming larger models trained using similar techniques. Furthermore, RLFactory significantly increases training throughput by 6.8 times, making it a valuable tool for strengthening the capabilities of LLMs in real-world applications.  
Summary: <div>
arXiv:2509.06980v1 Announce Type: new 
Abstract: Large language models excel at basic reasoning but struggle with tasks that require interaction with external tools. We present RLFactory, a plug-and-play reinforcement learning post-training framework for multi-round tool use. RLFactory tackles (i) tool-call stability and adaptability amid tool heterogeneity and interface issues via an asyncio-based asynchronous caller and a decoupled tool/training architecture, and (ii) diverse evaluation needs via a reward layer supporting rule-based, model-judgment, and tool-verification signals. It reconstructs the MDP by introducing observation markers from tool feedback, closing the loop among model, tools, and environment, and implements a generate-parse-invoke-update workflow for dynamic policy optimization. On Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural Questions (NQ) dataset, surpassing larger models trained with similar techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable framework for strengthening multi-round tool use of LLMs in real-world scenarios. Code: https://github.com/Simple-Efficient/RL-Factory.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention</title>
<link>https://arxiv.org/abs/2509.06982</link>
<guid>https://arxiv.org/abs/2509.06982</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, safety alignment, guard model, rollback mechanism, introspection-based intervention

Summary:
Large language models (LLMs) are widely used but ensuring the safety of their outputs is crucial during decoding. Existing interventions often sacrifice safety for response quality. The CARE framework addresses this challenge by integrating a guard model for real-time safety monitoring, a rollback mechanism for efficient corrections, and an introspection-based intervention strategy. The guard model detects unsafe content, the rollback mechanism corrects errors without disrupting the user experience, and the introspection method guides subsequent decoding steps through self-reflective critiques. Experimental results show that CARE achieves a superior balance between safety, quality, and efficiency, with low harmful response rates and minimal user disruption while maintaining high response quality. <br /><br />Summary: <div>
arXiv:2509.06982v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, ensuring the safety of their outputs during decoding has become a critical challenge. However, existing decoding-time interventions, such as Contrastive Decoding, often force a severe trade-off between safety and response quality. In this work, we propose CARE, a novel framework for decoding-time safety alignment that integrates three key components: (1) a guard model for real-time safety monitoring, enabling detection of potentially unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe outputs efficiently at an earlier stage without disrupting the user experience; and (3) a novel introspection-based intervention strategy, where the model generates self-reflective critiques of its previous outputs and incorporates these reflections into the context to guide subsequent decoding steps. The framework achieves a superior safety-quality trade-off by using its guard model for precise interventions, its rollback mechanism for timely corrections, and our novel introspection method for effective self-correction. Experimental results demonstrate that our framework achieves a superior balance of safety, quality, and efficiency, attaining a low harmful response rate and minimal disruption to the user experience while maintaining high response quality.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities</title>
<link>https://arxiv.org/abs/2509.06984</link>
<guid>https://arxiv.org/abs/2509.06984</guid>
<content:encoded><![CDATA[
<div> federated learning, fine-tuning, multimodal data, Low-Rank Adaptation, decentralized environments 
Summary: 
FediLoRA addresses challenges in federated learning with multimodal data and heterogeneous LoRA ranks. It introduces a dimension-wise aggregation strategy to handle different LoRA ranks and missing modalities efficiently. The framework includes a layer-wise model editing method that improves both client and global model performances. Experimental results on benchmark datasets show superior performance of FediLoRA compared to competitive baselines in global and personalized settings, especially in scenarios with modality incompleteness. <div>
arXiv:2509.06984v1 Announce Type: new 
Abstract: Foundation models have demonstrated remarkable performance across a wide range of tasks, yet their large parameter sizes pose challenges for practical deployment, especially in decentralized environments. Parameter-efficient fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing and memory overhead, making it attractive for federated learning. However, existing federated LoRA methods typically assume uniform rank configurations and unimodal inputs, overlooking two key real-world challenges: (1) heterogeneous client resources have different LoRA ranks, and (2) multimodal data settings with potentially missing modalities. In this work, we propose FediLoRA, a simple yet effective framework for federated multimodal fine-tuning under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a dimension-wise aggregation strategy that reweights LoRA updates without information dilution during aggregation. It also includes a lightweight layer-wise model editing method that selectively incorporates global parameters to repair local components which improves both client and global model performances. Experimental results on three multimodal benchmark datasets demonstrate that FediLoRA achieves superior performance over competitive baselines in both global and personalized settings, particularly in the presence of modality incompleteness.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs</title>
<link>https://arxiv.org/abs/2509.07013</link>
<guid>https://arxiv.org/abs/2509.07013</guid>
<content:encoded><![CDATA[
<div> Keywords: agent-based epidemic models, machine learning, LSTM, calibration, Approximate Bayesian Computation

Summary:
Agent-based epidemic models require accurate calibration, which is computationally intensive. This study introduces a machine learning calibrator that uses a three-layer bidirectional LSTM to learn the inverse mapping from epidemic time series to SIR parameters. The calibrator outperforms Approximate Bayesian Computation in terms of accuracy and speed, with lower errors and faster calibration times. Additionally, it produces tighter predictive intervals with near-nominal coverage. Despite the partial nonidentifiability of contact rate and transmission probability, the approach effectively reproduces epidemic curves and enables practical calibration. The evaluation on SIR agent-based epidemics generated with epiworldR demonstrates the effectiveness of the proposed method, which is implemented in R for easy access and use.<br /><br />Summary: <div>
arXiv:2509.07013v1 Announce Type: new 
Abstract: Calibrating agent-based epidemic models is computationally demanding. We present a supervised machine learning calibrator that learns the inverse mapping from epidemic time series to SIR parameters. A three-layer bidirectional LSTM ingests 60-day incidence together with population size and recovery rate, and outputs transmission probability, contact rate, and R0. Training uses a composite loss with an epidemiology-motivated consistency penalty that encourages R0 \* recovery rate to equal transmission probability \* contact rate.
  In a 1000-scenario simulation study, we compare the calibrator with Approximate Bayesian Computation (likelihood-free MCMC). The method achieves lower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs 0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near nominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per calibration. Although contact rate and transmission probability are partially nonidentifiable, the approach reproduces epidemic curves more faithfully than ABC, enabling fast and practical calibration. We evaluate it on SIR agent based epidemics generated with epiworldR and provide an implementation in R.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An efficient deep reinforcement learning environment for flexible job-shop scheduling</title>
<link>https://arxiv.org/abs/2509.07019</link>
<guid>https://arxiv.org/abs/2509.07019</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, scheduling, job-shop, deep learning, optimization<br />
<br />
Summary:
This paper introduces a new approach to addressing the Flexible Job-shop Scheduling Problem (FJSP) using deep reinforcement learning (DRL). The focus is on developing an efficient scheduling model that incorporates a novel DRL environment based on discrete event simulation. The proposed model utilizes proximal policy optimization (PPO) to optimize scheduling solutions. A unique state representation for FJSP is introduced, along with a customized reward function based on machine scheduling areas. Experimental results demonstrate the improved performance of simple priority dispatching rules (PDR) in the new scheduling environment, highlighting the competitiveness of the DRL scheduling model compared to existing methods such as OR-Tools, meta-heuristic, and traditional PDR approaches. <div>
arXiv:2509.07019v1 Announce Type: new 
Abstract: The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial optimization problem that has a wide-range of applications in the real world. In order to generate fast and accurate scheduling solutions for FJSP, various deep reinforcement learning (DRL) scheduling methods have been developed. However, these methods are mainly focused on the design of DRL scheduling Agent, overlooking the modeling of DRL environment. This paper presents a simple chronological DRL environment for FJSP based on discrete event simulation and an end-to-end DRL scheduling model is proposed based on the proximal policy optimization (PPO). Furthermore, a short novel state representation of FJSP is proposed based on two state variables in the scheduling environment and a novel comprehensible reward function is designed based on the scheduling area of machines. Experimental results on public benchmark instances show that the performance of simple priority dispatching rules (PDR) is improved in our scheduling environment and our DRL scheduling model obtains competing performance compared with OR-Tools, meta-heuristic, DRL and PDR scheduling methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>1 bit is all we need: binary normalized neural networks</title>
<link>https://arxiv.org/abs/2509.07025</link>
<guid>https://arxiv.org/abs/2509.07025</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network, binary normalized layers, memory efficiency, computational efficiency, image classification

Summary: 
The article introduces a new type of neural network layers called binary normalized layers, in which all parameters have values of zero or one. These layers can be applied to various types of neural network models, such as convolutional and transformer-based models, to achieve significant memory savings while maintaining performance. Experimental results demonstrate that models using binary normalized layers perform nearly as well as traditional models with 32-bit parameters. This approach allows for a 32 times reduction in memory usage, enabling the deployment of large neural network models on resource-constrained devices like mobile phones or CPUs. The simplicity of implementation using 1-bit arrays makes it a practical solution for improving memory efficiency in neural networks without the need for specialized hardware. Furthermore, the use of binary normalized layers opens up opportunities for more efficient deployment of neural network models across a range of applications. 

<br /><br />Summary: <div>
arXiv:2509.07025v1 Announce Type: new 
Abstract: The increasing size of large neural network models, specifically language models and foundational image models, poses deployment challenges, prompting efforts to reduce memory requirements and enhance computational efficiency. These efforts are critical to ensure practical deployment and effective utilization of these models across various applications. In this work, a novel type of neural network layers and models is developed that uses only single-bit parameters. In this novel type of models all parameters of all layers, including kernel weights and biases, only have values equal to zero or one. This novel type of models uses layers named as binary normalized layer. These binary normalized layers can be of any type, such as fully connected, convolutional, attention, etc., and they consist of slight variations of the corresponding conventional layers. To show the effectiveness of the binary normalized layers, two different models are configured to solve a multiclass image classification problem and a language decoder to predict the next token of a sequence. The model to solve the image classification has convolutional and fully connected layers, and the language model is composed of transformer blocks with multi-head attention. The results show that models with binary normalized layers present almost the same results obtained by equivalent models with real 32-bit parameters. The binary normalized layers allow to develop models that use 32 times less memory than current models and have equivalent performance. Besides, the binary normalized layers can be easily implemented on current computers using 1-bit arrays, and do not require the development of dedicated electronic hardware. This novel type of layers opens a new era for large neural network models with reduced memory requirements that can be deployed using simple and cheap hardware, such as mobile devices or only cpus.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive State Inference for Linear PASFA</title>
<link>https://arxiv.org/abs/2509.07028</link>
<guid>https://arxiv.org/abs/2509.07028</guid>
<content:encoded><![CDATA[
<div> Keywords: Slow Feature Analysis, Probabilistic Adaptive Slow Feature Analysis, ARMA process, MMSE estimation, Recursive algorithm

Summary:
Slow feature analysis (SFA) is a popular method for learning slowly varying features in classification and signal analysis, with recent probabilistic extensions improving representation learning for classification tasks. The Probabilistic Adaptive Slow Feature Analysis model frames slow features as states in an ARMA process, but efficient methods are needed to infer these states from observations and the model. This paper introduces a recursive extension to linear PASFA that enables MMSE estimation of states evolving within an ARMA process based on the observations and model. Unlike existing techniques that rely on Kalman filters, the proposed method allows for easy recovery of the original states, enhancing the interpretability of the learned representations. Evaluation on synthetic data confirms the effectiveness of the proposed algorithm. <div>
arXiv:2509.07028v1 Announce Type: new 
Abstract: Slow feature analysis (SFA), as a method for learning slowly varying features in classification and signal analysis, has attracted increasing attention in recent years. Recent probabilistic extensions to SFA learn effective representations for classification tasks. Notably, the Probabilistic Adaptive Slow Feature Analysis models the slow features as states in an ARMA process and estimate the model from the observations. However, there is a need to develop efficient methods to infer the states (slow features) from the observations and the model. In this paper, a recursive extension to the linear PASFA has been proposed. The proposed algorithm performs MMSE estimation of states evolving according to an ARMA process, given the observations and the model. Although current methods tackle this problem using Kalman filters after transforming the ARMA process into a state space model, the original states (or slow features) that form useful representations cannot be easily recovered. The proposed technique is evaluated on a synthetic dataset to demonstrate its correctness.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimalist Bayesian Framework for Stochastic Optimization</title>
<link>https://arxiv.org/abs/2509.07030</link>
<guid>https://arxiv.org/abs/2509.07030</guid>
<content:encoded><![CDATA[
<div> Bayesian Paradigm, Sequential Decision-making, Uncertainty, Structural Constraints, Profile Likelihood

Summary:
The article introduces a minimalist Bayesian framework that focuses on the component of interest, eliminating nuisance parameters through profile likelihood. This framework, exemplified by the MINimalist Thompson Sampling (MINTS) algorithm, allows for structured problems like continuum-armed Lipschitz bandits and dynamic pricing while offering insights into classical convex optimization algorithms. The MINTS algorithm is analyzed for multi-armed bandits, providing near-optimal regret guarantees. This approach addresses the challenge of incorporating complex structural constraints in Bayesian decision-making, offering a flexible and efficient method for sequential decision-making under uncertainty. <div>
arXiv:2509.07030v1 Announce Type: new 
Abstract: The Bayesian paradigm offers principled tools for sequential decision-making under uncertainty, but its reliance on a probabilistic model for all parameters can hinder the incorporation of complex structural constraints. We introduce a minimalist Bayesian framework that places a prior only on the component of interest, such as the location of the optimum. Nuisance parameters are eliminated via profile likelihood, which naturally handles constraints. As a direct instantiation, we develop a MINimalist Thompson Sampling (MINTS) algorithm. Our framework accommodates structured problems, including continuum-armed Lipschitz bandits and dynamic pricing. It also provides a probabilistic lens on classical convex optimization algorithms such as the center of gravity and ellipsoid methods. We further analyze MINTS for multi-armed bandits and establish near-optimal regret guarantees.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators</title>
<link>https://arxiv.org/abs/2509.07036</link>
<guid>https://arxiv.org/abs/2509.07036</guid>
<content:encoded><![CDATA[
<div> causal discovery, uncertainty-aware forecasting, macroeconomic indicators, LPCMCI framework, Chronos framework

Summary:
This paper presents a methodological approach to analyzing financial time series by combining causal discovery and uncertainty-aware forecasting. Using four key U.S. macroeconomic indicators from 1970 to 2021, the study uncovers a unidirectional causal link from economic growth to GDP and limited connectivity of inflation. Strong autoregressive dependence is observed in unemployment, which is used as a case study for probabilistic forecasting. The Chronos framework, a language model for time series, is leveraged to perform accurate zero-shot predictions on unemployment. The model's uncertainty-aware predictions include 90% confidence intervals for effective anomaly detection through statistically principled deviation analysis. This study highlights the value of integrating causal structure learning with probabilistic language models to inform economic policy and improve forecasting reliability. 

<br /><br />Summary: <div>
arXiv:2509.07036v1 Announce Type: new 
Abstract: This paper presents a methodological approach to financial time series analysis by combining causal discovery and uncertainty-aware forecasting. As a case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic growth, inflation, and unemployment -- and we apply the LPCMCI framework with Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal relationships in quarterly data from 1970 to 2021. Our results reveal a robust unidirectional causal link from economic growth to GDP and highlight the limited connectivity of inflation, suggesting the influence of latent factors. Unemployment exhibits strong autoregressive dependence, motivating its use as a case study for probabilistic forecasting. Leveraging the Chronos framework, a large language model trained for time series, we perform zero-shot predictions on unemployment. This approach delivers accurate forecasts one and two quarters ahead, without requiring task-specific training. Crucially, the model's uncertainty-aware predictions yield 90\% confidence intervals, enabling effective anomaly detection through statistically principled deviation analysis. This study demonstrates the value of combining causal structure learning with probabilistic language models to inform economic policy and enhance forecasting robustness.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation</title>
<link>https://arxiv.org/abs/2509.07039</link>
<guid>https://arxiv.org/abs/2509.07039</guid>
<content:encoded><![CDATA[
<div> Validation, Thermal Physics Principles, Convolutional Neural Networks, Vision Transformers, Fault Detection<br />
Summary:<br />
This study compares Convolutional Neural Networks (ResNet-18, EfficientNet-B0) and vision transformers (ViT-Tiny, Swin-Tiny) for thermal photovoltaic (PV) fault detection. It uses XRAI saliency analysis to assess alignment with thermal physics principles. The Swin Transformer achieves the highest performance with 94% binary accuracy and 73% multiclass accuracy. Models learn meaningful features like hotspots for cell defects and thermal boundaries for shading. Performance varies across fault types, with electrical faults being detected well and environmental factors like soiling proving challenging due to imaging resolution limitations. The study provides a methodology for validating AI decision-making in energy monitoring applications, addressing deployment barriers in renewable energy infrastructure.<br /><br />Summary: <div>
arXiv:2509.07039v1 Announce Type: new 
Abstract: Artificial intelligence deployment for automated photovoltaic (PV) monitoring faces interpretability barriers that limit adoption in energy infrastructure applications. While deep learning achieves high accuracy in thermal fault detection, validation that model decisions align with thermal physics principles remains lacking, creating deployment hesitancy where understanding model reasoning is critical. This study provides a systematic comparison of convolutional neural networks (ResNet-18, EfficientNet-B0) and vision transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI saliency analysis to assess alignment with thermal physics principles. This represents the first systematic comparison of CNNs and vision transformers for thermal PV fault detection with physics-validated interpretability. Evaluation on 20,000 infrared images spanning normal operation and 11 fault categories shows that Swin Transformer achieves the highest performance (94% binary accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis reveals that models learn physically meaningful features, such as localized hotspots for cell defects, linear thermal paths for diode failures, and thermal boundaries for vegetation shading, consistent with expected thermal signatures. However, performance varies significantly across fault types: electrical faults achieve strong detection (F1-scores >0.90) while environmental factors like soiling remain challenging (F1-scores 0.20-0.33), indicating limitations imposed by thermal imaging resolution. The thermal physics-guided interpretability approach provides methodology for validating AI decision-making in energy monitoring applications, addressing deployment barriers in renewable energy infrastructure.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lookup multivariate Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2509.07103</link>
<guid>https://arxiv.org/abs/2509.07103</guid>
<content:encoded><![CDATA[
<div> lookup multivariate Kolmogorov-Arnold Networks, capacity, inference cost, FLOPs, function approximation <br />
Summary: <br />
The article introduces lookup multivariate Kolmogorov-Arnold Networks (lmKANs) as a more efficient alternative to high-dimensional linear mappings in deep-learning models. lmKANs utilize trainable low-dimensional multivariate functions implemented as spline lookup tables, significantly reducing inference FLOPs while maintaining flexibility in function approximation. Empirical results demonstrate up to 6.0x reduction in FLOPs, 10x higher throughput on tabular-like datasets, and 1.6-2.1x lower FLOPs in Convolutional Neural Networks with matched accuracy on standard datasets. The lmKAN-based models achieve these improvements by leveraging efficient parameterization in high-dimensional mappings. The code, including CUDA kernels, is available online for implementation. <br /> <div>
arXiv:2509.07103v1 Announce Type: new 
Abstract: High-dimensional linear mappings, or linear layers, dominate both the parameter count and the computational cost of most modern deep-learning models. We introduce a general drop-in replacement, lookup multivariate Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better trade-off between capacity and inference cost. Our construction expresses a general high-dimensional mapping through trainable low-dimensional multivariate functions. These functions can carry dozens or hundreds of trainable parameters each, and yet it takes only a few multiplications to compute them because they are implemented as spline lookup tables. Empirically, lmKANs reduce inference FLOPs by up to 6.0x while matching the flexibility of MLPs in general high-dimensional function approximation. In another feedforward fully connected benchmark, on the tabular-like dataset of randomly displaced methane configurations, lmKANs enable more than 10x higher H100 throughput at equal accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10 and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA kernels, is available online at https://github.com/schwallergroup/lmkan.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Batch Normalization: A Gyro Approach</title>
<link>https://arxiv.org/abs/2509.07115</link>
<guid>https://arxiv.org/abs/2509.07115</guid>
<content:encoded><![CDATA[
<div> GyroBN, Riemannian batch normalization, gyrogroups, pseudo-reduction, gyroisometric gyrations <br />
<br />
GyroBN introduces a Riemannian batch normalization framework for gyrogroups, extending Euclidean neural networks to non-Euclidean domains. Two necessary conditions, pseudo-reduction and gyroisometric gyrations, ensure theoretical control over sample statistics and hold for known gyrogroups in machine learning. GyroBN incorporates existing Riemannian normalization methods and is applied to seven representative geometries. Novel gyro and Riemannian structures enable these instantiations. Experimental results across different geometries demonstrate the effectiveness of GyroBN. The code is available at the provided GitHub repository. <br />
Summary: <br />
- GyroBN presents a Riemannian batch normalization framework for gyrogroups, extending neural networks to non-Euclidean data. 
- The framework establishes theoretical control over sample statistics through pseudo-reduction and gyroisometric gyrations. 
- GyroBN incorporates existing Riemannian normalization methods and is applied to various geometries. 
- Novel structures enable effective instantiations on seven representative geometries. 
- Experimental results showcase the effectiveness of GyroBN across different non-Euclidean domains. <div>
arXiv:2509.07115v1 Announce Type: new 
Abstract: Normalization layers are crucial for deep learning, but their Euclidean formulations are inadequate for data on manifolds. On the other hand, many Riemannian manifolds in machine learning admit gyro-structures, enabling principled extensions of Euclidean neural networks to non-Euclidean domains. Inspired by this, we introduce GyroBN, a principled Riemannian batch normalization framework for gyrogroups. We establish two necessary conditions, namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that guarantee GyroBN with theoretical control over sample statistics, and show that these conditions hold for all known gyrogroups in machine learning. Our framework also incorporates several existing Riemannian normalization methods as special cases. We further instantiate GyroBN on seven representative geometries, including the Grassmannian, five constant curvature spaces, and the correlation manifold, and derive novel gyro and Riemannian structures to enable these instantiations. Experiments across these geometries demonstrate the effectiveness of GyroBN. The code is available at https://github.com/GitZH-Chen/GyroBN.git.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2509.07143</link>
<guid>https://arxiv.org/abs/2509.07143</guid>
<content:encoded><![CDATA[
<div> Graph foundation models, node classification, tabular foundation models, zero-shot learning, ensemble selection
Summary: 
TabGFM is a novel approach to graph learning that transforms nodes into tabular data, enabling tabular foundation models to perform zero-shot node classification. By converting a graph into a table, applying multiple TFMs, and aggregating outputs through ensemble selection, TabGFM outperforms task-specific GNNs and state-of-the-art GFMs on real-world datasets. This innovative framework demonstrates the potential of tabular reformulation for scalable and generalizable graph learning. <div>
arXiv:2509.07143v1 Announce Type: new 
Abstract: Graph foundation models (GFMs) have recently emerged as a promising paradigm for achieving broad generalization across various graph data. However, existing GFMs are often trained on datasets that were shown to poorly represent real-world graphs, limiting their generalization performance. In contrast, tabular foundation models (TFMs) not only excel at classical tabular prediction tasks but have also shown strong applicability in other domains such as time series forecasting, natural language processing, and computer vision. Motivated by this, we take an alternative view to the standard perspective of GFMs and reformulate node classification as a tabular problem. Each node can be represented as a row with feature, structure, and label information as columns, enabling TFMs to directly perform zero-shot node classification via in-context learning. In this work, we introduce TabGFM, a graph foundation model framework that first converts a graph into a table via feature and structural encoders, applies multiple TFMs to diversely subsampled tables, and then aggregates their outputs through ensemble selection. Through experiments on 28 real-world datasets, TabGFM achieves consistent improvements over task-specific GNNs and state-of-the-art GFMs, highlighting the potential of tabular reformulation for scalable and generalizable graph learning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Uncertainty in Transformer Circuits with Effective Information Consistency</title>
<link>https://arxiv.org/abs/2509.07149</link>
<guid>https://arxiv.org/abs/2509.07149</guid>
<content:encoded><![CDATA[
<div> Transformer Circuits, Mechanistic Interpretability, Effective-Information Consistency Score, Language Models, Causal Emergence <br />
Summary: 
This study introduces an Effective-Information Consistency Score (EICS) to quantify the coherence and trustworthiness of functional subgraphs within large language models known as Transformer Circuits (TCs). The EICS combines a normalized sheaf inconsistency and a Gaussian proxy for circuit-level causal emergence, allowing for a white-box, single-pass quantification of TC behavior. Units are made explicit for dimensionless scoring, with guidance on interpretation and computational efficiency provided. Empirical validation on LLM tasks is pending, as the focus is on theoretical development and practical application of the EICS framework. <div>
arXiv:2509.07149v1 Announce Type: new 
Abstract: Mechanistic interpretability has identified functional subgraphs within large language models (LLMs), known as Transformer Circuits (TCs), that appear to implement specific algorithms. Yet we lack a formal, single-pass way to quantify when an active circuit is behaving coherently and thus likely trustworthy. Building on prior systems-theoretic proposals, we specialize a sheaf/cohomology and causal emergence perspective to TCs and introduce the Effective-Information Consistency Score (EICS). EICS combines (i) a normalized sheaf inconsistency computed from local Jacobians and activations, with (ii) a Gaussian EI proxy for circuit-level causal emergence derived from the same forward state. The construction is white-box, single-pass, and makes units explicit so that the score is dimensionless. We further provide practical guidance on score interpretation, computational overhead (with fast and exact modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is deferred.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design</title>
<link>https://arxiv.org/abs/2509.07150</link>
<guid>https://arxiv.org/abs/2509.07150</guid>
<content:encoded><![CDATA[
<div> crystal structures, Large Language Model (LLM), property-guided, reinforcement learning, materials discovery
<br />
Summary:
The article introduces PLaID++, a Large Language Model fine-tuned to generate crystal structures using a Wyckoff-based text representation. The model is guided by reinforcement learning techniques, categorizing structures based on stability, novelty, and space group. PLaID++ outperforms prior methods by generating thermodynamically stable, unique, and novel structures at a higher rate, and can conditionally generate structures with desired space group properties. The use of iterative Direct Preference Optimization leads to significant improvements in both unconditional and space group conditioned generation. This approach, adapted from natural language processing, demonstrates the potential for targeted and efficient discovery of novel materials, crucial for advancements in technologies such as solar cells, batteries, and carbon capture. 
<br /> <div>
arXiv:2509.07150v1 Announce Type: new 
Abstract: Discovering novel materials is critical for technological advancements such as solar cells, batteries, and carbon capture. However, the development of new materials is constrained by a slow and expensive trial-and-error process. To accelerate this pipeline, we introduce PLaID++, a Large Language Model (LLM) fine-tuned for stable and property-guided crystal generation. We fine-tune Qwen-2.5 7B to generate crystal structures using a novel Wyckoff-based text representation. We show that generation can be effectively guided with a reinforcement learning technique based on Direct Preference Optimization (DPO), with sampled structures categorized by their stability, novelty, and space group. By encoding symmetry constraints directly into text and guiding model outputs towards desirable chemical space, PLaID++ generates structures that are thermodynamically stable, unique, and novel at a $\sim$50\% greater rate than prior methods and conditionally generates structures with desired space group properties. Our experiments highlight the effectiveness of iterative DPO, achieving $\sim$115\% and $\sim$50\% improvements in unconditional and space group conditioned generation, respectively, compared to fine-tuning alone. Our work demonstrates the potential of adapting post-training techniques from natural language processing to materials design, paving the way for targeted and efficient discovery of novel materials.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data</title>
<link>https://arxiv.org/abs/2509.07198</link>
<guid>https://arxiv.org/abs/2509.07198</guid>
<content:encoded><![CDATA[
<div> algorithm, federated learning, representation learning, evolutionary clustering, client data

Summary:
Fed-REACT is a federated learning framework aimed at addressing the challenges posed by heterogeneous and evolving client data distributions in real-world scenarios. The framework employs a two-stage approach: first, each client learns a local model to extract feature representations from its data. Next, the server dynamically groups clients into clusters based on these representations and coordinates cluster-wise training of task-specific models. The framework blends representation learning with evolutionary clustering to enhance the performance of standard FL algorithms. The authors provide a theoretical analysis of the representation learning stage and present empirical results showing that Fed-REACT outperforms existing methods in terms of accuracy and robustness on diverse real-world datasets. Fed-REACT aims to mitigate the resource costs and privacy concerns associated with centralized machine learning by enabling collaborative model training while ensuring data privacy and security. <div>
arXiv:2509.07198v1 Announce Type: new 
Abstract: Motivated by the high resource costs and privacy concerns associated with centralized machine learning, federated learning (FL) has emerged as an efficient alternative that enables clients to collaboratively train a global model while keeping their data local. However, in real-world deployments, client data distributions often evolve over time and differ significantly across clients, introducing heterogeneity that degrades the performance of standard FL algorithms. In this work, we introduce Fed-REACT, a federated learning framework designed for heterogeneous and evolving client data. Fed-REACT combines representation learning with evolutionary clustering in a two-stage process: (1) in the first stage, each client learns a local model to extracts feature representations from its data; (2) in the second stage, the server dynamically groups clients into clusters based on these representations and coordinates cluster-wise training of task-specific models for downstream objectives such as classification or regression. We provide a theoretical analysis of the representation learning stage, and empirically demonstrate that Fed-REACT achieves superior accuracy and robustness on real-world datasets.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting effect of novel treatments using molecular pathways and real-world data</title>
<link>https://arxiv.org/abs/2509.07204</link>
<guid>https://arxiv.org/abs/2509.07204</guid>
<content:encoded><![CDATA[
<div> Keywords: pharmaceutical R&amp;D, machine learning, predicted efficacy, patient data, biological pathways

Summary:
In pharmaceutical research and development, predicting the efficacy of a drug for a specific disease before clinical testing has been a challenge. This paper introduces a machine learning-based approach to predict the efficacy of untested pharmaceuticals. By training a model using pharmaceutical-pathway weight impact scores and patient data, the model can analyze the impact of a drug on biological pathways to generate a predicted efficacy value. The method is demonstrated on real-world datasets and includes evaluation methods for generalization performance. The approach can provide insights into predicting the effects of untested drugs by leveraging real-world clinical data and drug embeddings. This flexible and modular framework sets the groundwork for future research in predicting drug efficacy. 

<br /><br />Summary: <div>
arXiv:2509.07204v1 Announce Type: new 
Abstract: In pharmaceutical R&amp;D, predicting the efficacy of a pharmaceutical in treating a particular disease prior to clinical testing or any real-world use has been challenging. In this paper, we propose a flexible and modular machine learning-based approach for predicting the efficacy of an untested pharmaceutical for treating a disease. We train a machine learning model using sets of pharmaceutical-pathway weight impact scores and patient data, which can include patient characteristics and observed clinical outcomes. The resulting model then analyses weighted impact scores of an untested pharmaceutical across human biological molecule-protein pathways to generate a predicted efficacy value. We demonstrate how the method works on a real-world dataset with patient treatments and outcomes, with two different weight impact score algorithms We include methods for evaluating the generalisation performance on unseen treatments, and to characterise conditions under which the approach can be expected to be most predictive. We discuss specific ways in which our approach can be iterated on, making it an initial framework to support future work on predicting the effect of untested drugs, leveraging RWD clinical data and drug embeddings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining How Quantization Disparately Skews a Model</title>
<link>https://arxiv.org/abs/2509.07222</link>
<guid>https://arxiv.org/abs/2509.07222</guid>
<content:encoded><![CDATA[
<div> quantization, disparate impact, minority groups, gradient norms, Hessian matrix 

Summary:
Post Training Quantization (PTQ) is a popular technique for compressing neural networks without significant loss of accuracy. However, this study found that PTQ can exacerbate disparate impacts, particularly affecting minority groups. The quantization process introduces changes in weights and activations that lead to cascading effects, reducing variance in logits, increasing loss, and compromising group accuracies. The study also examined the impact of quantization on group gradient norms and eigenvalues of the Hessian matrix, providing insights into network optimization. To address these disparities, the researchers suggest incorporating mixed precision Quantization Aware Training (QAT) with dataset sampling methods and weighted loss functions to ensure fair deployment of quantized neural networks. <div>
arXiv:2509.07222v1 Announce Type: new 
Abstract: Post Training Quantization (PTQ) is widely adopted due to its high compression capacity and speed with minimal impact on accuracy. However, we observed that disparate impacts are exacerbated by quantization, especially for minority groups. Our analysis explains that in the course of quantization there is a chain of factors attributed to a disparate impact across groups during forward and backward passes. We explore how the changes in weights and activations induced by quantization cause cascaded impacts in the network, resulting in logits with lower variance, increased loss, and compromised group accuracies. We extend our study to verify the influence of these impacts on group gradient norms and eigenvalues of the Hessian matrix, providing insights into the state of the network from an optimization point of view. To mitigate these effects, we propose integrating mixed precision Quantization Aware Training (QAT) with dataset sampling methods and weighted loss functions, therefore providing fair deployment of quantized neural networks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2509.07238</link>
<guid>https://arxiv.org/abs/2509.07238</guid>
<content:encoded><![CDATA[
<div> Optimization, Mathematical reasoning tasks, Model parameters, Efficiency gains, Performance boosts <br />
Summary: <br />
This paper investigates fine-tuning model parameters for mathematical reasoning tasks, exploring configurations such as randomness control, reasoning depth, and sampling strategies. A holistically optimized framework is introduced for five state-of-the-art models, showcasing substantial efficiency gains and performance improvements. By systematically optimizing parameters across different models, significant reductions in computational cost and enhancements in inference speed are achieved. Critical findings suggest that lower temperature regimes and reduced reasoning steps enhance efficiency without compromising accuracy. The study highlights the importance of parameter optimization in improving the performance of mathematical reasoning models, providing production-ready configurations with extensive performance characterization. <div>
arXiv:2509.07238v1 Announce Type: new 
Abstract: This paper presents a practical investigation into fine-tuning model parameters for mathematical reasoning tasks through experimenting with various configurations including randomness control, reasoning depth, and sampling strategies, careful tuning demonstrates substantial improvements in efficiency as well as performance. A holistically optimized framework is introduced for five state-of-the-art models on mathematical reasoning tasks, exhibiting significant performance boosts while maintaining solution correctness. Through systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B, DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are demonstrated with 100% optimization success rate. The methodology achieves an average 29.4% reduction in computational cost and 23.9% improvement in inference speed across all tested models. This framework systematically searches parameter spaces including temperature (0.1-0.5), reasoning steps (4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining optimal configurations through testing on mathematical reasoning benchmarks. Critical findings show that lower temperature regimes (0.1-0.4) and reduced reasoning steps (4-6) consistently enhance efficiency without compromising accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B delivers the most cost-effective performance at 361.5 tokens per accurate response. Key contributions include: (1) the first comprehensive optimization study for five diverse SOTA models in mathematical reasoning, (2) a standardized production-oriented parameter optimization framework, (3) discovery of universal optimization trends applicable across model architectures, and (4) production-ready configurations with extensive performance characterization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IP-Basis PINNs: Efficient Multi-Query Inverse Parameter Estimation</title>
<link>https://arxiv.org/abs/2509.07245</link>
<guid>https://arxiv.org/abs/2509.07245</guid>
<content:encoded><![CDATA[
<div> basis functions, inverse problems, Physics-Informed Neural Networks, meta-learning, differential equation<br />
<br />
Summary: 
Inverse-Parameter Basis PINNs (IP-Basis PINNs) is a meta-learning framework that tackles the computational expense of solving inverse problems with Physics-Informed Neural Networks. It utilizes an offline-online decomposition approach, where a deep network is trained offline to generate a set of basis functions covering the solution space of a parametric differential equation. This allows for rapid inference in multi-query scenarios by freezing the network and training only a lightweight linear output layer for each new inverse problem online. Key innovations include a novel online loss formulation for solution reconstruction and parameter identification, reduced computational overhead through automatic differentiation, and a robust training mechanism. The efficacy of IP-Basis PINNs is demonstrated on various benchmarks, showcasing consistent performance in parameter estimation, faster query processing compared to standard PINNs, and resilience to sparse and noisy data.<br /><br />Summary: <div>
arXiv:2509.07245v1 Announce Type: new 
Abstract: Solving inverse problems with Physics-Informed Neural Networks (PINNs) is computationally expensive for multi-query scenarios, as each new set of observed data requires a new, expensive training procedure. We present Inverse-Parameter Basis PINNs (IP-Basis PINNs), a meta-learning framework that extends the foundational work of Desai et al. (2022) to enable rapid and efficient inference for inverse problems. Our method employs an offline-online decomposition: a deep network is first trained offline to produce a rich set of basis functions that span the solution space of a parametric differential equation. For each new inverse problem online, this network is frozen, and solutions and parameters are inferred by training only a lightweight linear output layer against observed data. Key innovations that make our approach effective for inverse problems include: (1) a novel online loss formulation for simultaneous solution reconstruction and parameter identification, (2) a significant reduction in computational overhead via forward-mode automatic differentiation for PDE loss evaluation, and (3) a non-trivial validation and early-stopping mechanism for robust offline training. We demonstrate the efficacy of IP-Basis PINNs on three diverse benchmarks, including an extension to universal PINNs for unknown functional terms-showing consistent performance across constant and functional parameter estimation, a significant speedup per query over standard PINNs, and robust operation with scarce and noisy data.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCond: Gradient Conflict Resolution via Accumulation-based Stabilization for Large-Scale Multi-Task Learning</title>
<link>https://arxiv.org/abs/2509.07252</link>
<guid>https://arxiv.org/abs/2509.07252</guid>
<content:encoded><![CDATA[
<div> MobileNetV3-Small, ConvNeXt, ImageNet 1K dataset, self-supervised learning, gradient conflict resolution<br />
Summary:<br />
The article introduces Gradient Conductor (GCond), a method for addressing gradient conflict in multi-task learning that combines PCGrad principles with gradient accumulation and adaptive arbitration. GCond was evaluated on self-supervised learning tasks with MobileNetV3-Small and ConvNeXt architectures on ImageNet 1K and head and neck CT scan datasets. Compared to baseline methods and state-of-the-art gradient conflict resolution techniques, GCond achieved a two-fold computational speedup while maintaining optimization quality. It displayed superior performance with lower L1 and SSIM losses on both datasets, demonstrating high scalability and compatibility with modern optimizers. GCond is seen as a scalable and efficient solution to gradient conflict challenges in multi-task learning. <br /> <div>
arXiv:2509.07252v1 Announce Type: new 
Abstract: In multi-task learning (MTL), gradient conflict poses a significant challenge. Effective methods for addressing this problem, including PCGrad, CAGrad, and GradNorm, in their original implementations are computationally demanding, which significantly limits their application in modern large models and transformers. We propose Gradient Conductor (GCond), a method that builds upon PCGrad principles by combining them with gradient accumulation and an adaptive arbitration mechanism. We evaluated GCond on self-supervised learning tasks using MobileNetV3-Small and ConvNeXt architectures on the ImageNet 1K dataset and a combined head and neck CT scan dataset, comparing the proposed method against baseline linear combinations and state-of-the-art gradient conflict resolution methods. The stochastic mode of GCond achieved a two-fold computational speedup while maintaining optimization quality, and demonstrated superior performance across all evaluated metrics, achieving lower L1 and SSIM losses compared to other methods on both datasets. GCond exhibited high scalability, being successfully applied to both compact models (MobileNetV3-Small) and large architectures (ConvNeXt-tiny and ConvNeXt-Base). It also showed compatibility with modern optimizers such as AdamW and Lion/LARS. Therefore, GCond offers a scalable and efficient solution to the problem of gradient conflicts in multi-task learning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Generalized Hamiltonian Dynamics with Stability from Noisy Trajectory Data</title>
<link>https://arxiv.org/abs/2509.07280</link>
<guid>https://arxiv.org/abs/2509.07280</guid>
<content:encoded><![CDATA[
<div> Keywords: generalized Hamiltonian dynamics, variational Bayesian inference, sparse symplectic, random Fourier Gaussian processes, physics correctness <br />
Summary: 
This paper presents a robust framework for learning generalized Hamiltonian dynamics from noisy and sparse phase-space data using variational Bayesian inference. The approach aims to capture the distinctive motion dynamics and physics of different types of systems, including conservative, dissipative, and port-Hamiltonian systems. By extending sparse symplectic and random Fourier Gaussian processes learning, the model can predict the Hamiltonian landscape of a phase space. The method incorporates kernelized evidence lower bound (ELBO) loss for data fidelity and additional loss terms for stability and conservation, ensuring physics correctness and improving prediction accuracy with bounded uncertainty. The proposed framework provides a comprehensive solution for learning complex Hamiltonian manifold structures from observational trajectories, enabling better understanding and modeling of diverse physical systems. <br /><br />Summary: <div>
arXiv:2509.07280v1 Announce Type: new 
Abstract: We introduce a robust framework for learning various generalized Hamiltonian dynamics from noisy, sparse phase-space data and in an unsupervised manner based on variational Bayesian inference. Although conservative, dissipative, and port-Hamiltonian systems might share the same initial total energy of a closed system, it is challenging for a single Hamiltonian network model to capture the distinctive and varying motion dynamics and physics of a phase space, from sampled observational phase space trajectories. To address this complicated Hamiltonian manifold learning challenge, we extend sparse symplectic, random Fourier Gaussian processes learning with predictive successive numerical estimations of the Hamiltonian landscape, using a generalized form of state and conjugate momentum Hamiltonian dynamics, appropriate to different classes of conservative, dissipative and port-Hamiltonian physical systems. In addition to the kernelized evidence lower bound (ELBO) loss for data fidelity, we incorporate stability and conservation constraints as additional hyper-parameter balanced loss terms to regularize the model's multi-gradients, enforcing physics correctness for improved prediction accuracy with bounded uncertainty.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers</title>
<link>https://arxiv.org/abs/2509.07282</link>
<guid>https://arxiv.org/abs/2509.07282</guid>
<content:encoded><![CDATA[
<div> Transformer, neural network, cryptogram solving, generalization, interpretability

Summary: 
The article introduces cryptogram solving as a testbed for studying neural network generalization in complex domains. It presents ALICE, an encoder-only Transformer that achieves state-of-the-art accuracy and speed in decrypting text encoded with substitution ciphers. Surprisingly, ALICE can generalize to unseen ciphers after training on a minimal fraction of the possible cipher space. The model incorporates a novel bijective decoding head that explicitly models permutations using the Gumbel-Sinkhorn method, allowing for the extraction of learned cipher mappings. Through early exit analysis, the study reveals how ALICE refines its predictions by employing frequency-based heuristics, forming word structures, and correcting individual characters in a way that reflects common human strategies for cryptogram solving. This research contributes insights into neural network generalization and interpretability in domains involving bijective mappings and combinatorial structures. 

<br /><br />Summary: <div>
arXiv:2509.07282v1 Announce Type: new 
Abstract: We present cryptogram solving as an ideal testbed for studying neural network generalization in combinatorially complex domains. In this task, models must decrypt text encoded with substitution ciphers, choosing from 26! possible mappings without explicit access to the cipher. We develop ALICE (an Architecture for Learning Interpretable Cryptogram dEcipherment): a simple encoder-only Transformer that sets a new state-of-the-art for both accuracy and speed on this decryption problem. Surprisingly, ALICE generalizes to unseen ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction ($3.7 \times 10^{-24}$) of the possible cipher space. To enhance interpretability, we introduce a novel bijective decoding head that explicitly models permutations via the Gumbel-Sinkhorn method, enabling direct extraction of learned cipher mappings. Through early exit analysis, we reveal how ALICE progressively refines its predictions in a way that appears to mirror common human strategies for this task: early layers employ frequency-based heuristics, middle layers form word structures, and final layers correct individual characters. Our architectural innovations and analysis methods extend beyond cryptograms to any domain with bijective mappings and combinatorial structure, offering new insights into neural network generalization and interpretability.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation</title>
<link>https://arxiv.org/abs/2509.07325</link>
<guid>https://arxiv.org/abs/2509.07325</guid>
<content:encoded><![CDATA[
<div> framework, LLM, treatment trajectories, NSCLC, guideline adherence 
Summary:
- The study focuses on using large language models (LLMs) to automatically generate guideline-concordant treatment trajectories for non-small cell lung cancer (NSCLC) patients.
- A dataset of 121 NSCLC cases with expert annotations of NCCN guideline trajectories was constructed.
- Existing LLMs were shown to possess domain-specific knowledge enabling high-quality benchmark generation.
- A hybrid approach combining human annotations and model consistency information was developed to predict guidelines and verify prediction accuracy.
- A meta-classifier was created to provide calibrated confidence scores for treatment recommendations, enabling communication of accuracy, tradeoff customization, and regulatory compliance.
- The study establishes a framework for LLM-based guideline adherence systems that balance accuracy, interpretability, and regulatory requirements while reducing annotation costs, paving the way for automated clinical decision support. 
 <br /><br />Summary: <div>
arXiv:2509.07325v1 Announce Type: new 
Abstract: The National Comprehensive Cancer Network (NCCN) provides evidence-based guidelines for cancer treatment. Translating complex patient presentations into guideline-compliant treatment recommendations is time-intensive, requires specialized expertise, and is prone to error. Advances in large language model (LLM) capabilities promise to reduce the time required to generate treatment recommendations and improve accuracy. We present an LLM agent-based approach to automatically generate guideline-concordant treatment trajectories for patients with non-small cell lung cancer (NSCLC). Our contributions are threefold. First, we construct a novel longitudinal dataset of 121 cases of NSCLC patients that includes clinical encounters, diagnostic results, and medical histories, each expertly annotated with the corresponding NCCN guideline trajectories by board-certified oncologists. Second, we demonstrate that existing LLMs possess domain-specific knowledge that enables high-quality proxy benchmark generation for both model development and evaluation, achieving strong correlation (Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks. Third, we develop a hybrid approach combining expensive human annotations with model consistency information to create both the agent framework that predicts the relevant guidelines for a patient, as well as a meta-classifier that verifies prediction accuracy with calibrated confidence scores for treatment recommendations (AUROC=0.800), a critical capability for communicating the accuracy of outputs, custom-tailoring tradeoffs in performance, and supporting regulatory compliance. This work establishes a framework for clinically viable LLM-based guideline adherence systems that balance accuracy, interpretability, and regulatory requirements while reducing annotation costs, providing a scalable pathway toward automated clinical decision support.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases</title>
<link>https://arxiv.org/abs/2509.07330</link>
<guid>https://arxiv.org/abs/2509.07330</guid>
<content:encoded><![CDATA[
<div> age, gender, demographic attributes, electronic health records, predictive performance <br />
Summary:<br />
This study introduces the General Demographic Pre-trained (GDP) model for representing age and gender in electronic health records. The GDP model, pre-trained and evaluated on diverse datasets, shows improved performance in discrimination and calibration, especially in diseases where age and gender are significant predictors. By transforming tabular demographic inputs into latent embeddings using sequential ordering, the GDP model enhances the importance of demographic attributes in downstream models, even in cases where they have low predictive value. The findings suggest that foundational models like GDP can generalize across tasks and populations, offering a promising approach to boosting predictive performance in healthcare applications. <br /> <div>
arXiv:2509.07330v1 Announce Type: new 
Abstract: Demographic attributes are universally present in electronic health records and serve as vital predictors in clinical risk stratification and treatment decisions. Despite their significance, these attributes are often relegated to auxiliary roles in model design, with limited attention has been given to learning their representations. This study proposes a General Demographic Pre-trained (GDP) model as a foundational representation framework tailored to age and gender. The model is pre-trained and evaluated using datasets with diverse diseases and population compositions from different geographic regions. The GDP architecture explores combinations of ordering strategies and encoding methods to transform tabular demographic inputs into latent embeddings. Experimental results demonstrate that sequential ordering substantially improves model performance in discrimination, calibration, and the corresponding information gain at each decision tree split, particularly in diseases where age and gender contribute significantly to risk stratification. Even in datasets where demographic attributes hold relatively low predictive value, GDP enhances the representational importance, increasing their influence in downstream gradient boosting models. The findings suggest that foundational models for tabular demographic attributes can generalize across tasks and populations, offering a promising direction for improving predictive performance in healthcare applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning</title>
<link>https://arxiv.org/abs/2509.07342</link>
<guid>https://arxiv.org/abs/2509.07342</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated edge learning, dynamic data evolution, temporal drift, collective divergence, Earth Mover's Distance

Summary:<br />
The paper introduces FedTeddi, a novel scheduling algorithm for Federated Edge Learning (FEEL) that addresses the challenge of adapting models to dynamic and non-i.i.d. data in wireless networks. FedTeddi quantifies data dynamics using temporal drift and collective divergence metrics, represented by Earth Mover's Distance. The algorithm optimizes scheduling and bandwidth allocation to ensure fast convergence without forgetting previous knowledge. Experimental results demonstrate a 58.4% improvement in convergence rate on CIFAR-10 and 49.2% on CIFAR-100 compared to random scheduling. FedTeddi enables collaborative model training on evolving datasets, achieving higher test accuracy and faster convergence in real-world scenarios. <br />Summary: <div>
arXiv:2509.07342v1 Announce Type: new 
Abstract: Federated edge learning (FEEL) enables collaborative model training across distributed clients over wireless networks without exposing raw data. While most existing studies assume static datasets, in real-world scenarios clients may continuously collect data with time-varying and non-independent and identically distributed (non-i.i.d.) characteristics. A critical challenge is how to adapt models in a timely yet efficient manner to such evolving data. In this paper, we propose FedTeddi, a temporal-drift-and-divergence-aware scheduling algorithm that facilitates fast convergence of FEEL under dynamic data evolution and communication resource limits. We first quantify the temporal dynamics and non-i.i.d. characteristics of data using temporal drift and collective divergence, respectively, and represent them as the Earth Mover's Distance (EMD) of class distributions for classification tasks. We then propose a novel optimization objective and develop a joint scheduling and bandwidth allocation algorithm, enabling the FEEL system to learn from new data quickly without forgetting previous knowledge. Experimental results show that our algorithm achieves higher test accuracy and faster convergence compared to benchmark methods, improving the rate of convergence by 58.4% on CIFAR-10 and 49.2% on CIFAR-100 compared to random scheduling.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SBS: Enhancing Parameter-Efficiency of Neural Representations for Neural Networks via Spectral Bias Suppression</title>
<link>https://arxiv.org/abs/2509.07373</link>
<guid>https://arxiv.org/abs/2509.07373</guid>
<content:encoded><![CDATA[
<div> neural representation, convolutional neural network, parameter compression, spectral bias, high-frequency details<br />
Summary:<br />
The paper introduces SBS, a method to enhance neural representation for neural networks by addressing the spectral bias issue. SBS includes unidirectional ordering-based smoothing techniques to improve kernel smoothness and random fourier features to adjust frequency bandwidth based on parameter count. Evaluations on various ResNet models and datasets show that SBS outperforms state-of-the-art methods in terms of reconstruction accuracy with fewer parameters. <div>
arXiv:2509.07373v1 Announce Type: new 
Abstract: Implicit neural representations have recently been extended to represent convolutional neural network weights via neural representation for neural networks, offering promising parameter compression benefits. However, standard multi-layer perceptrons used in neural representation for neural networks exhibit a pronounced spectral bias, hampering their ability to reconstruct high-frequency details effectively. In this paper, we propose SBS, a parameter-efficient enhancement to neural representation for neural networks that suppresses spectral bias using two techniques: (1) a unidirectional ordering-based smoothing that improves kernel smoothness in the output space, and (2) unidirectional ordering-based smoothing aware random fourier features that adaptively modulate the frequency bandwidth of input encodings based on layer-wise parameter count. Extensive evaluations on various ResNet models with datasets CIFAR-10, CIFAR-100, and ImageNet, demonstrate that SBS achieves significantly better reconstruction accuracy with less parameters compared to SOTA.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientNet in Digital Twin-based Cardiac Arrest Prediction and Analysis</title>
<link>https://arxiv.org/abs/2509.07388</link>
<guid>https://arxiv.org/abs/2509.07388</guid>
<content:encoded><![CDATA[
<div> EfficientNet, deep learning model, digital twin system, cardiac arrest, early detection <br />
<br />
Summary: 
The paper introduces a novel framework for early detection and analysis of cardiac arrest using an EfficientNet-based deep learning model and a digital twin system. By leveraging compound scaling and EfficientNet to analyze cardiovascular images, the system learns key features for accurate prediction. The digital twin creates a personalized cardiovascular model based on data from IoT devices attached to the patient, enabling constant assessment and impact analysis of treatment plans. Experiment results demonstrate high prediction accuracy and efficiency of the proposed system. The integration of advanced technologies like deep learning and digital twin technology offers the potential for an individualized and proactive approach to predicting cardiac disease. <br /><br />Summary: <div>
arXiv:2509.07388v1 Announce Type: new 
Abstract: Cardiac arrest is one of the biggest global health problems, and early identification and management are key to enhancing the patient's prognosis. In this paper, we propose a novel framework that combines an EfficientNet-based deep learning model with a digital twin system to improve the early detection and analysis of cardiac arrest. We use compound scaling and EfficientNet to learn the features of cardiovascular images. In parallel, the digital twin creates a realistic and individualized cardiovascular system model of the patient based on data received from the Internet of Things (IoT) devices attached to the patient, which can help in the constant assessment of the patient and the impact of possible treatment plans. As shown by our experiments, the proposed system is highly accurate in its prediction abilities and, at the same time, efficient. Combining highly advanced techniques such as deep learning and digital twin (DT) technology presents the possibility of using an active and individual approach to predicting cardiac disease.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions</title>
<link>https://arxiv.org/abs/2509.07392</link>
<guid>https://arxiv.org/abs/2509.07392</guid>
<content:encoded><![CDATA[
<div> Keywords: Blockchain, transaction networks, GCN-GRU model, illicit activities, Bitcoin

Summary: 
The study introduces a hybrid model, combining Graph Convolutional Networks (GCN) and Gated Recurrent Units (GRU), to analyze complex blockchain transaction networks. The model successfully captures both structural and sequential features, enabling the detection of illicit activities within the network. Real Bitcoin transaction data from 2020-2024 was used for evaluation, where the model demonstrated high accuracy with an Accuracy of 0.9470 and an AUC-ROC of 0.9807, surpassing all baseline models. This approach proves effective in identifying suspicious or illicit transactions by leveraging the evolving temporal patterns and inter-node relationships present in blockchain networks. Overall, the hybrid GCN-GRU model provides a robust solution for enhancing security measures and combating illicit activities within the cryptocurrency space. 

<br /><br />Summary: <div>
arXiv:2509.07392v1 Announce Type: new 
Abstract: Blockchain transaction networks are complex, with evolving temporal patterns and inter-node relationships. To detect illicit activities, we propose a hybrid GCN-GRU model that captures both structural and sequential features. Using real Bitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and 0.9807 AUC-ROC, outperforming all baselines.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise</title>
<link>https://arxiv.org/abs/2509.07415</link>
<guid>https://arxiv.org/abs/2509.07415</guid>
<content:encoded><![CDATA[
<div> Learning-based outlier-robust filter, measurement noise, correlated, EM-based outlier robust filter, outlier characteristics, outlier detection, numerical experiments, performance gains, accuracy, computational overhead, computational complexity, practical methods

Summary:
The article introduces EMORF-II, a learning-based outlier-robust filter designed for scenarios with correlated measurement noise. Building upon the EM-based outlier robust filter (EMORF), EMORF-II incorporates a feature that allows it to learn outlier characteristics during inference, enhancing its ability to mitigate outliers. Numerical experiments demonstrate that EMORF-II outperforms existing methods in terms of accuracy, albeit with slightly increased computational overhead. Despite this, the computational complexity remains comparable to other practical techniques, making EMORF-II a valuable option for various applications. <div>
arXiv:2509.07415v1 Announce Type: new 
Abstract: We present a learning-based outlier-robust filter for a general setup where the measurement noise can be correlated. Since it is an enhanced version of EM-based outlier robust filter (EMORF), we call it as EMORF-II. As it is equipped with an additional powerful feature to learn the outlier characteristics during inference along with outlier-detection, EMORF-II has improved outlier-mitigation capability. Numerical experiments confirm performance gains as compared to the state-of-the-art methods in terms of accuracy with an increased computational overhead. However, thankfully the computational complexity order remains at par with other practical methods making it a useful choice for diverse applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward</title>
<link>https://arxiv.org/abs/2509.07430</link>
<guid>https://arxiv.org/abs/2509.07430</guid>
<content:encoded><![CDATA[
<div> framework, Diversity-Preserving Hybrid RL, pass@k degradation, catastrophic forgetting, reinforcement learning, divergence term

Summary:
The article discusses the challenges in fine-tuning Large Language Models with Reinforcement Learning with Verifiable Reward (RLVR) and the degradation in multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). It identifies catastrophic forgetting and the lack of a mechanism for knowledge retention in standard RLVR objectives as key issues. The authors propose a new approach, Diversity-Preserving Hybrid RL (DPH-RL), which uses mass-covering f-divergences to act as a rehearsal mechanism, forcing the model to maintain a broad solution coverage. Experiments on math and SQL generation show that DPH-RL resolves Pass@k degradation, improves both Pass@1 and Pass@k in- and out-of-domain, and is more training-efficient due to its use of generator functions for computing f-divergence. The study underscores the importance of selecting the right divergence measure in RLVR for building more general and diverse reasoning models. 

<br /><br />Summary: <div>
arXiv:2509.07430v1 Announce Type: new 
Abstract: A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue that standard RLVR objectives -- both those using the mode-seeking reverse KL-divergence and those forgoing a divergence term entirely -- lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conv4Rec: A 1-by-1 Convolutional AutoEncoder for User Profiling through Joint Analysis of Implicit and Explicit Feedbacks</title>
<link>https://arxiv.org/abs/2509.07499</link>
<guid>https://arxiv.org/abs/2509.07499</guid>
<content:encoded><![CDATA[
<div> AutoEncoder architecture, user modelling, recommendation tasks, implicit feedback, generalization bounds <br />
<br />
Summary: 
The article introduces a novel convolutional AutoEncoder architecture for user modelling and recommendation tasks, offering enhancements over existing methods. The model can learn associations and combinations between different interaction types on a per-user and item basis. Additionally, it can simultaneously learn from explicit ratings and implicit feedback, making separate predictions for content consumption probability and high rating likelihood. This enables the model to predict both implicit and explicit feedback accurately and identify items users would enjoy but may not naturally consume. The article provides generalization bounds for the model in a Recommender Systems setting, showing that optimizing the loss function ensures recovery of the exact sampling distribution with small error. Experimental results on real-life datasets demonstrate state-of-the-art performance in predicting implicit and explicit feedback, while also enhancing interpretability through individual rating probability predictions. <div>
arXiv:2509.07499v1 Announce Type: new 
Abstract: We introduce a new convolutional AutoEncoder architecture for user modelling and recommendation tasks with several improvements over the state of the art. Firstly, our model has the flexibility to learn a set of associations and combinations between different interaction types in a way that carries over to each user and item. Secondly, our model is able to learn jointly from both the explicit ratings and the implicit information in the sampling pattern (which we refer to as `implicit feedback'). It can also make separate predictions for the probability of consuming content and the likelihood of granting it a high rating if observed. This not only allows the model to make predictions for both the implicit and explicit feedback, but also increases the informativeness of the predictions: in particular, our model can identify items which users would not have been likely to consume naturally, but would be likely to enjoy if exposed to them. Finally, we provide several generalization bounds for our model, which to the best of our knowledge, are among the first generalization bounds for auto-encoders in a Recommender Systems setting; we also show that optimizing our loss function guarantees the recovery of the exact sampling distribution over interactions up to a small error in total variation. In experiments on several real-life datasets, we achieve state-of-the-art performance on both the implicit and explicit feedback prediction tasks despite relying on a single model for both, and benefiting from additional interpretability in the form of individual predictions for the probabilities of each possible rating.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Water Demand Forecasting of District Metered Areas through Learned Consumer Representations</title>
<link>https://arxiv.org/abs/2509.07515</link>
<guid>https://arxiv.org/abs/2509.07515</guid>
<content:encoded><![CDATA[
<div> smart metering technologies, water utilities, water demand forecasting, District Metered Areas, unsupervised contrastive learning 

Summary: 
Advancements in smart metering technologies have improved water utility management. Water demand forecasting in DMAs remains challenging due to non-deterministic factors like weather. A novel method is proposed using unsupervised contrastive learning to categorize users based on consumption behaviors within a DMA. These behaviors are then used in demand forecasting with wavelet-transformed convolutional networks incorporating a cross-attention mechanism. Real-world DMA evaluation over six months showed improved forecasting performance, with a maximum improvement of 4.9% in MAPE. The approach also identifies consumers influenced by socioeconomic factors, enhancing knowledge of deterministic demand patterns. <div>
arXiv:2509.07515v1 Announce Type: new 
Abstract: Advancements in smart metering technologies have significantly improved the ability to monitor and manage water utilities. In the context of increasing uncertainty due to climate change, securing water resources and supply has emerged as an urgent global issue with extensive socioeconomic ramifications. Hourly consumption data from end-users have yielded substantial insights for projecting demand across regions characterized by diverse consumption patterns. Nevertheless, the prediction of water demand remains challenging due to influencing non-deterministic factors, such as meteorological conditions. This work introduces a novel method for short-term water demand forecasting for District Metered Areas (DMAs) which encompass commercial, agricultural, and residential consumers. Unsupervised contrastive learning is applied to categorize end-users according to distinct consumption behaviors present within a DMA. Subsequently, the distinct consumption behaviors are utilized as features in the ensuing demand forecasting task using wavelet-transformed convolutional networks that incorporate a cross-attention mechanism combining both historical data and the derived representations. The proposed approach is evaluated on real-world DMAs over a six-month period, demonstrating improved forecasting performance in terms of MAPE across different DMAs, with a maximum improvement of 4.9%. Additionally, it identifies consumers whose behavior is shaped by socioeconomic factors, enhancing prior knowledge about the deterministic patterns that influence demand.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection</title>
<link>https://arxiv.org/abs/2509.07523</link>
<guid>https://arxiv.org/abs/2509.07523</guid>
<content:encoded><![CDATA[
<div> CDL, rare event detection, signals, RoseCDL, anomaly detection <br />
Summary:<br />
Convolutional Dictionary Learning (CDL) is a powerful method for identifying patterns in signals but faces challenges in detecting rare events. The RoseCDL algorithm is introduced to address these challenges by combining stochastic windowing for efficient training on large datasets with inline outlier detection to increase robustness and isolate anomalous patterns. This makes CDL a practical tool for event discovery and characterization in real-world signals, extending its typical applications of compression and denoising. RoseCDL offers scalability and resilience to artifacts, enhancing its utility in fields such as astronomy, physical simulations, and biomedical science. <div>
arXiv:2509.07523v1 Announce Type: new 
Abstract: Identifying recurring patterns and rare events in large-scale signals is a fundamental challenge in fields such as astronomy, physical simulations, and biomedical science. Convolutional Dictionary Learning (CDL) offers a powerful framework for modeling local structures in signals, but its use for detecting rare or anomalous events remains largely unexplored. In particular, CDL faces two key challenges in this setting: high computational cost and sensitivity to artifacts and outliers. In this paper, we introduce RoseCDL, a scalable and robust CDL algorithm designed for unsupervised rare event detection in long signals. RoseCDL combines stochastic windowing for efficient training on large datasets with inline outlier detection to enhance robustness and isolate anomalous patterns. This reframes CDL as a practical tool for event discovery and characterization in real-world signals, extending its role beyond traditional tasks like compression or denoising.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\Delta L$ Normalization: Rethink Loss Aggregation in RLVR</title>
<link>https://arxiv.org/abs/2509.07558</link>
<guid>https://arxiv.org/abs/2509.07558</guid>
<content:encoded><![CDATA[
<div> RLVR, Loss Aggregation, Dynamic Generation Lengths, Reinforcement Learning, Language Models
Summary:
The article introduces $\Delta L$ Normalization, a novel loss aggregation method designed for Reinforcement Learning with Verifiable Rewards (RLVR) to address the challenge of varying response lengths during training. Existing methods like GRPO and DAPO have limitations such as biased estimates and high gradient variance. By formulating the problem as finding a minimum-variance unbiased estimator, $\Delta L$ Normalization provides an unbiased estimate of policy loss while minimizing gradient variance in theory. Extensive experiments across different model sizes, maximum lengths, and tasks demonstrate the consistent superior performance of the proposed method. The code for $\Delta L$ Normalization will be available on GitHub at https://github.com/zerolllin/Delta-L-Normalization.
<br /><br />Summary: <div>
arXiv:2509.07558v1 Announce Type: new 
Abstract: We propose $\Delta L$ Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at https://github.com/zerolllin/Delta-L-Normalization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>uGMM-NN: Univariate Gaussian Mixture Model Neural Network</title>
<link>https://arxiv.org/abs/2509.07569</link>
<guid>https://arxiv.org/abs/2509.07569</guid>
<content:encoded><![CDATA[
<div> Neural architecture, Univariate Gaussian Mixture Model Neural Network, probabilistic reasoning, multimodality, uncertainty<br />
<br />Summary: <br /> 
This paper introduces the Univariate Gaussian Mixture Model Neural Network (uGMM-NN), which incorporates probabilistic reasoning directly into deep network computational units. Unlike traditional neurons, uGMM-NN nodes parameterize activations as a univariate Gaussian mixture, capturing multimodality and uncertainty with learnable means, variances, and mixing coefficients. This framework allows for richer representations at the neuron level while maintaining scalability. The uGMM-NN achieves competitive performance similar to standard multilayer perceptrons and provides a probabilistic interpretation of activations. This approach opens new possibilities for integrating uncertainty-aware components in neural architectures, facilitating advancements in both discriminative and generative modeling. <div>
arXiv:2509.07569v1 Announce Type: new 
Abstract: This paper introduces the Univariate Gaussian Mixture Model Neural Network (uGMM-NN), a novel neural architecture that embeds probabilistic reasoning directly into the computational units of deep networks. Unlike traditional neurons, which apply weighted sums followed by fixed nonlinearities, each uGMM-NN node parameterizes its activations as a univariate Gaussian mixture, with learnable means, variances, and mixing coefficients. This design enables richer representations by capturing multimodality and uncertainty at the level of individual neurons, while retaining the scalability of standard feedforward networks. We demonstrate that uGMM-NN can achieve competitive discriminative performance compared to conventional multilayer perceptrons, while additionally offering a probabilistic interpretation of activations. The proposed framework provides a foundation for integrating uncertainty-aware components into modern neural architectures, opening new directions for both discriminative and generative modeling.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks</title>
<link>https://arxiv.org/abs/2509.07579</link>
<guid>https://arxiv.org/abs/2509.07579</guid>
<content:encoded><![CDATA[
<div> Physics-informed neural networks, PINNs, homogenization, multiscale modeling, discontinuous coefficients <br />
Summary: <br />
This paper introduces a dual formulation for the PINN framework to enhance the accuracy in solving PDEs for materials with discontinuous coefficients. The dual approach allows for the derivation of error bounds, improving reliability and error detection of PINNs. Comparing standard PINNs with variational PINNs (VPINNs) using different test functions, results show that strong-form PINNs perform better in controlled settings but struggle with material discontinuities. VPINNs, on the other hand, can handle piecewise constant material parameters but require careful test function selection to prevent instability. The dual formulation acts as a reliable indicator of convergence quality and boosts the applicability of PINN frameworks in micromechanics homogenization problems. <br /> <div>
arXiv:2509.07579v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) have shown promise in solving partial differential equations (PDEs) relevant to multiscale modeling, but they often fail when applied to materials with discontinuous coefficients, such as media with piecewise constant properties. This paper introduces a dual formulation for the PINN framework to improve the reliability of the homogenization of periodic thermo-conductive composites, for both strong and variational (weak) formulations. The dual approach facilitates the derivation of guaranteed upper and lower error bounds, enabling more robust detection of PINN failure. We compare standard PINNs applied to smoothed material approximations with variational PINNs (VPINNs) using both spectral and neural network-based test functions. Our results indicate that while strong-form PINNs may outperform VPINNs in controlled settings, they are sensitive to material discontinuities and may fail without clear diagnostics. In contrast, VPINNs accommodate piecewise constant material parameters directly but require careful selection of test functions to avoid instability. Dual formulation serves as a reliable indicator of convergence quality, and its integration into PINN frameworks enhances their applicability to homogenization problems in micromechanics.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards</title>
<link>https://arxiv.org/abs/2509.07603</link>
<guid>https://arxiv.org/abs/2509.07603</guid>
<content:encoded><![CDATA[
<div> Transformer-based deep learning, sensor placement optimization, structural health monitoring, semiconductor probe cards, crack detection

Summary:
This paper introduces a novel deep learning approach using Transformers to optimize sensor placement for monitoring the health of semiconductor probe cards. By leveraging simulated failure scenarios and a robust dataset, a hybrid Convolutional Neural Network and Transformer model achieves high accuracy in classifying health states and detecting cracks. The model's attention mechanism identifies critical sensor locations, providing valuable insights for designing efficient monitoring systems. This research showcases the potential of attention-based deep learning in enhancing proactive maintenance strategies for improving operational reliability and yield in semiconductor manufacturing industries. 

<br /><br />Summary: <div>
arXiv:2509.07603v1 Announce Type: new 
Abstract: This paper presents an innovative Transformer-based deep learning strategy for optimizing the placement of sensors aiming at structural health monitoring of semiconductor probe cards. Failures in probe cards, including substrate cracks and loosened screws, would critically affect semiconductor manufacturing yield and reliability. Some failure modes could be detected by equipping a probe card with adequate sensors. Frequency response functions from simulated failure scenarios are adopted within a finite element model of a probe card. A comprehensive dataset, enriched by physics-informed scenario expansion and physics-aware statistical data augmentation, is exploited to train a hybrid Convolutional Neural Network and Transformer model. The model achieves high accuracy (99.83%) in classifying the probe card health states (baseline, loose screw, crack) and an excellent crack detection recall (99.73%). Model robustness is confirmed through a rigorous framework of 3 repetitions of 10-fold stratified cross-validation. The attention mechanism also pinpoints critical sensor locations: an analysis of the attention weights offers actionable insights for designing efficient, cost-effective monitoring systems by optimizing sensor configurations. This research highlights the capability of attention-based deep learning to advance proactive maintenance, enhancing operational reliability and yield in semiconductor manufacturing.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K2-Think: A Parameter-Efficient Reasoning System</title>
<link>https://arxiv.org/abs/2509.07604</link>
<guid>https://arxiv.org/abs/2509.07604</guid>
<content:encoded><![CDATA[
<div> Keywords: K2-Think, reasoning system, state-of-the-art performance, parameter-efficient model, open-source datasets <br />
Summary:<br />
K2-Think is a reasoning system that achieves state-of-the-art performance using a 32B parameter model, competing with larger models like GPT-OSS 120B. The system is built on the Qwen2.5 base model and utilizes advanced techniques such as Long Chain-of-thought Supervised Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic planning, Test-time Scaling, Speculative Decoding, and Inference-optimized Hardware. It excels in mathematical reasoning, code, and science, showcasing its versatility. K2-Think demonstrates that smaller models can outperform larger ones through integrated post-training techniques, making open-source reasoning systems more accessible. The system is freely available at k2think.ai and offers fast inference speeds using the Cerebras Wafer-Scale Engine, making it efficient and affordable for a wide range of applications. <br /> <div>
arXiv:2509.07604v1 Announce Type: new 
Abstract: K2-Think is a reasoning system that achieves state-of-the-art performance with a 32B parameter model, matching or surpassing much larger models like GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system shows that smaller models can compete at the highest levels by combining advanced post-training and test-time computation techniques. The approach is based on six key technical pillars: Long Chain-of-thought Supervised Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic planning prior to reasoning, Test-time Scaling, Speculative Decoding, and Inference-optimized Hardware, all using publicly available open-source datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art scores on public benchmarks for open-source models, while also performing strongly in other areas such as Code and Science. Our results confirm that a more parameter-efficient model like K2-Think 32B can compete with state-of-the-art systems through an integrated post-training recipe that includes long chain-of-thought training and strategic inference-time enhancements, making open-source reasoning systems more accessible and affordable. K2-Think is freely available at k2think.ai, offering best-in-class inference speeds of over 2,000 tokens per second per request via the Cerebras Wafer-Scale Engine.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques</title>
<link>https://arxiv.org/abs/2509.07605</link>
<guid>https://arxiv.org/abs/2509.07605</guid>
<content:encoded><![CDATA[
<div> imbalance, binary classifiers, performance evaluation, minority class, data complexity 

Summary:
The study investigates the impact of class imbalance on binary classifiers in scenarios such as medical diagnostics and anomaly detection. It explores the performance of classifiers without applying rebalancing techniques, analyzing real-world and synthetic datasets with reduced minority class sizes. The research systematically assesses a diverse range of classifiers under varying data complexities and reveals that classification becomes more challenging as the minority class decreases. Advanced models such as TabPFN and boosting-based ensembles show better performance and generalization compared to traditional classifiers. The study also includes experiments with undersampling, oversampling, and one-class classification methods to evaluate their behavior under severe imbalance. Visual interpretability and evaluation metrics validate the findings, providing valuable insights for model selection in imbalanced learning. <div>
arXiv:2509.07605v1 Announce Type: new 
Abstract: Class imbalance poses a significant challenge to supervised classification, particularly in critical domains like medical diagnostics and anomaly detection where minority class instances are rare. While numerous studies have explored rebalancing techniques to address this issue, less attention has been given to evaluating the performance of binary classifiers under imbalance when no such techniques are applied. Therefore, the goal of this study is to assess the performance of binary classifiers "as-is", without performing any explicit rebalancing. Specifically, we systematically evaluate the robustness of a diverse set of binary classifiers across both real-world and synthetic datasets, under progressively reduced minority class sizes, using one-shot and few-shot scenarios as baselines. Our approach also explores varying data complexities through synthetic decision boundary generation to simulate real-world conditions. In addition to standard classifiers, we include experiments using undersampling, oversampling strategies, and one-class classification (OCC) methods to examine their behavior under severe imbalance. The results confirm that classification becomes more difficult as data complexity increases and the minority class size decreases. While traditional classifiers deteriorate under extreme imbalance, advanced models like TabPFN and boosting-based ensembles retain relatively higher performance and better generalization compared to traditional classifiers. Visual interpretability and evaluation metrics further validate these findings. Our work offers valuable guidance on model selection for imbalanced learning, providing insights into classifier robustness without dependence on explicit rebalancing techniques.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based Integrated Gradients for Explaining Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.07648</link>
<guid>https://arxiv.org/abs/2509.07648</guid>
<content:encoded><![CDATA[
<div> Keywords: Integrated Gradients, explainability, black-box problem, graphs, node classification<br />
Summary: <br />
Integrated Gradients (IG) is a popular technique for explaining the decisions of neural networks, but it is not well-suited for graphs due to their discrete nature. In this study, researchers propose a new method called graph-based integrated gradients (GB-IG) to address this limitation. The GB-IG technique is tested on synthetic datasets and real-world graph datasets for node classification tasks. The results show that GB-IG accurately identifies important structural components of graphs used in classification tasks. Additionally, GB-IG outperforms IG in highlighting significant features for node classification in real-world graph datasets. This extension of IG to graphs demonstrates its potential for improving explainability in graph-based machine learning models. <br /> 
Summary: <div>
arXiv:2509.07648v1 Announce Type: new 
Abstract: Integrated Gradients (IG) is a common explainability technique to address the black-box problem of neural networks. Integrated gradients assumes continuous data. Graphs are discrete structures making IG ill-suited to graphs. In this work, we introduce graph-based integrated gradients (GB-IG); an extension of IG to graphs. We demonstrate on four synthetic datasets that GB-IG accurately identifies crucial structural components of the graph used in classification tasks. We further demonstrate on three prevalent real-world graph datasets that GB-IG outperforms IG in highlighting important features for node classification tasks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FUnc-SNE: A flexible, Fast, and Unconstrained algorithm for neighbour embeddings</title>
<link>https://arxiv.org/abs/2509.07681</link>
<guid>https://arxiv.org/abs/2509.07681</guid>
<content:encoded><![CDATA[
<div> Keywords: neighbour embeddings, data visualisation, acceleration, hyperparameter tuning, interactive exploration 

Summary:
Neighbour embeddings (NE) are commonly used to represent high dimensional datasets in lower dimensional spaces for data visualisation. Traditional approaches to accelerating NE either sacrifice structure quality for speed or restrict dimensionality to 2 or 3. This paper presents a novel acceleration method that balances speed and structure preservation while allowing flexibility in dimensionality through hyperparameter tuning. The method enables interactive data exploration by providing instant visual feedback when adjusting hyperparameters. Experiments with a GPU-accelerated GUI integration demonstrate promising results in speed and structure extraction flexibility, suggesting potential applications in broader machine learning contexts with minimal algorithmic modifications. A key component of the algorithm is a new iterative approach to approximate nearest neighbour search, which outperforms traditional nearest neighbour descent techniques. 

<br /><br />Summary: <div>
arXiv:2509.07681v1 Announce Type: new 
Abstract: Neighbour embeddings (NE) allow the representation of high dimensional datasets into lower dimensional spaces and are often used in data visualisation. In practice, accelerated approximations are employed to handle very large datasets. Accelerating NE is challenging, and two main directions have been explored: very coarse approximations based on negative sampling (as in UMAP) achieve high effective speed but may lack quality in the extracted structures; less coarse approximations, as used in FIt-SNE or BH-t-SNE, offer better structure preservation at the cost of speed, while also restricting the target dimensionality to 2 or 3, limiting NE to visualisation. In some variants, the precision of these costlier accelerations also enables finer-grained control on the extracted structures through dedicated hyperparameters.
  This paper proposes to bridge the gab between both approaches by introducing a novel way to accelerate NE, requiring a small number of computations per iteration while maintaining good fine-grained structure preservation and flexibility through hyperparameter tuning, without limiting the dimensionality of the embedding space. The method was designed for interactive exploration of data; as such, it abandons the traditional two-phased approach of other NE methods, allowing instantaneous visual feedback when changing hyperparameters, even when these control processes happening on the high-dimensional side of the computations. Experiments using a publicly available, GPU accelerated GUI integration of the method show promising results in terms of speed, flexibility in the structures getting extracted, and show potential uses in broader machine learning contexts with minimal algorithmic modifications. Central to this algorithm is a novel approach to iterative approximate nearest neighbour search, which shows promising results compared to nearest neighbour descent.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing</title>
<link>https://arxiv.org/abs/2509.07725</link>
<guid>https://arxiv.org/abs/2509.07725</guid>
<content:encoded><![CDATA[
<div> Keywords: Multivariate time series forecasting, missing variables, graph neural networks, uncertainty estimation, bidirectional modeling <br />
Summary: <br />
This article introduces a novel framework, Interpretable Bidirectional-modeling Network (IBN), for Multivariate Time Series Forecasting (MTSF) with missing variables. IBN addresses the challenge of missing variables by incorporating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-based Graph Convolution (GGCN). By estimating uncertainty using MC Dropout and applying an uncertainty-weighted strategy, IBN improves the reliability of reconstructed values. The use of GGCN allows for explicit modeling of spatial correlations among variables, enhancing the overall forecasting performance. Additionally, a bidirectional recursive unit is employed to improve temporal dependency modeling. Extensive experiments demonstrate that IBN outperforms existing methods under various missing-rate scenarios, showcasing its effectiveness in handling missing variables and providing interpretable results for MTSF tasks. <div>
arXiv:2509.07725v1 Announce Type: new 
Abstract: Multivariate time series forecasting (MTSF) often faces challenges from missing variables, which hinder conventional spatial-temporal graph neural networks in modeling inter-variable correlations. While GinAR addresses variable missing using attention-based imputation and adaptive graph learning for the first time, it lacks interpretability and fails to capture more latent temporal patterns due to its simple recursive units (RUs). To overcome these limitations, we propose the Interpretable Bidirectional-modeling Network (IBN), integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-based Graph Convolution (GGCN). IBN estimates the uncertainty of reconstructed values using MC Dropout and applies an uncertainty-weighted strategy to mitigate high-risk reconstructions. GGCN explicitly models spatial correlations among variables, while a bidirectional RU enhances temporal dependency modeling. Extensive experiments show that IBN achieves state-of-the-art forecasting performance under various missing-rate scenarios, providing a more reliable and interpretable framework for MTSF with missing variables. Code is available at: https://github.com/zhangth1211/NICLab-IBN.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?</title>
<link>https://arxiv.org/abs/2509.07727</link>
<guid>https://arxiv.org/abs/2509.07727</guid>
<content:encoded><![CDATA[
<div> compression, Mixture of Experts, error-bounded, inference accuracy, GPU memory<br />
Summary:<br />
The article explores the use of error-bounded lossy compression algorithms, like SZ3 and CuSZp, to compress non-activated experts in Mixture of Experts (MoE) models for efficient inference under limited GPU memory. Results show minimal degradation in inference accuracy for shallow layer experts, responsible for attention mechanism and input token transformation. Middle-layer experts, crucial for model reasoning, suffer significant accuracy impairments with errors. Surprisingly, introducing bounded errors in deep-layer experts, handling instruction following and output integration, can occasionally enhance inference accuracy. Efficient compression of experts can reduce data transfer overhead during MoE reasoning, making it a promising approach for addressing memory constraints in MoE models. <div>
arXiv:2509.07727v1 Announce Type: new 
Abstract: With the widespread application of Mixture of Experts (MoE) reasoning models in the field of LLM learning, efficiently serving MoE models under limited GPU memory constraints has emerged as a significant challenge. Offloading the non-activated experts to main memory has been identified as an efficient approach to address such a problem, while it brings the challenges of transferring the expert between the GPU memory and main memory. We need to explore an efficient approach to compress the expert and analyze how the compression error affects the inference performance.
  To bridge this gap, we propose employing error-bounded lossy compression algorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby reducing data transfer overhead during MoE inference. We conduct extensive experiments across various benchmarks and present a comprehensive analysis of how compression-induced errors in different experts affect overall inference accuracy. The results indicate that experts in the shallow layers, which are primarily responsible for the attention mechanism and the transformation of input tokens into vector representations, exhibit minimal degradation in inference accuracy when subjected to bounded errors. In contrast, errors in the middle-layer experts, which are central to model reasoning, significantly impair inference accuracy. Interestingly, introducing bounded errors in the deep-layer experts, which are mainly responsible for instruction following and output integration, can sometimes lead to improvements in inference accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Russian Equipment Losses Using Time Series and Deep Learning Models</title>
<link>https://arxiv.org/abs/2509.07813</link>
<guid>https://arxiv.org/abs/2509.07813</guid>
<content:encoded><![CDATA[
arXiv:2509.07813v1 Announce Type: new 
Abstract: This study applies a range of forecasting techniques,including ARIMA, Prophet, Long Short Term Memory networks (LSTM), Temporal Convolutional Networks (TCN), and XGBoost, to model and predict Russian equipment losses during the ongoing war in Ukraine. Drawing on daily and monthly open-source intelligence (OSINT) data from WarSpotting, we aim to assess trends in attrition, evaluate model performance, and estimate future loss patterns through the end of 2025. Our findings show that deep learning models, particularly TCN and LSTM, produce stable and consistent forecasts, especially under conditions of high temporal granularity. By comparing different model architectures and input structures, this study highlights the importance of ensemble forecasting in conflict modeling, and the value of publicly available OSINT data in quantifying material degradation over time.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting person-level injury severity using crash narratives: A balanced approach with roadway classification and natural language process techniques</title>
<link>https://arxiv.org/abs/2509.07845</link>
<guid>https://arxiv.org/abs/2509.07845</guid>
<content:encoded><![CDATA[
arXiv:2509.07845v1 Announce Type: new 
Abstract: Predicting injuries and fatalities in traffic crashes plays a critical role in enhancing road safety, improving emergency response, and guiding public health interventions. This study investigates the added value of unstructured crash narratives (written by police officers at the scene) when combined with structured crash data to predict injury severity. Two widely used Natural Language Processing (NLP) techniques, Term Frequency-Inverse Document Frequency (TF-IDF) and Word2Vec, were employed to extract semantic meaning from the narratives, and their effectiveness was compared. To address the challenge of class imbalance, a K-Nearest Neighbors-based oversampling method was applied to the training data prior to modeling. The dataset consists of crash records from Kentucky spanning 2019 to 2023. To account for roadway heterogeneity, three road classification schemes were used: (1) eight detailed functional classes (e.g., Urban Two-Lane, Rural Interstate, Urban Multilane Divided), (2) four broader paired categories (e.g., Urban vs. Rural, Freeway vs. Non-Freeway), and (3) a unified dataset without classification. A total of 102 machine learning models were developed by combining structured features and narrative-based features using the two NLP techniques alongside three ensemble algorithms: XGBoost, Random Forest, and AdaBoost. Results demonstrate that models incorporating narrative data consistently outperform those relying solely on structured data. Among all combinations, TF-IDF coupled with XGBoost yielded the most accurate predictions in most subgroups. The findings highlight the power of integrating textual and structured crash information to enhance person-level injury prediction. This work offers a practical and adaptable framework for transportation safety professionals to improve crash severity modeling, guide policy decisions, and design more effective countermeasures.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the Cold-Start Problem for Personalized Combination Drug Screening</title>
<link>https://arxiv.org/abs/2509.07850</link>
<guid>https://arxiv.org/abs/2509.07850</guid>
<content:encoded><![CDATA[
arXiv:2509.07850v1 Announce Type: new 
Abstract: Personalizing combination therapies in oncology requires navigating an immense space of possible drug and dose combinations, a task that remains largely infeasible through exhaustive experimentation. Recent developments in patient-derived models have enabled high-throughput ex vivo screening, but the number of feasible experiments is limited. Further, a tight therapeutic window makes gathering molecular profiling information (e.g. RNA-seq) impractical as a means of guiding drug response prediction. This leads to a challenging cold-start problem: how do we select the most informative combinations to test early, when no prior information about the patient is available? We propose a strategy that leverages a pretrained deep learning model built on historical drug response data. The model provides both embeddings for drug combinations and dose-level importance scores, enabling a principled selection of initial experiments. We combine clustering of drug embeddings to ensure functional diversity with a dose-weighting mechanism that prioritizes doses based on their historical informativeness. Retrospective simulations on large-scale drug combination datasets show that our method substantially improves initial screening efficiency compared to baselines, offering a viable path for more effective early-phase decision-making in personalized combination drug screens.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Support Vector Regression for Outcome Prediction in Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy</title>
<link>https://arxiv.org/abs/2509.07872</link>
<guid>https://arxiv.org/abs/2509.07872</guid>
<content:encoded><![CDATA[
arXiv:2509.07872v1 Announce Type: new 
Abstract: Personalized ultra-fractionated stereotactic adaptive radiotherapy (PULSAR) is a novel treatment that delivers radiation in pulses of protracted intervals. Accurate prediction of gross tumor volume (GTV) changes through regression models has substantial prognostic value. This study aims to develop a multi-omics based support vector regression (SVR) model for predicting GTV change. A retrospective cohort of 39 patients with 69 brain metastases was analyzed, based on radiomics (MRI images) and dosiomics (dose maps) features. Delta features were computed to capture relative changes between two time points. A feature selection pipeline using least absolute shrinkage and selection operator (Lasso) algorithm with weight- or frequency-based ranking criterion was implemented. SVR models with various kernels were evaluated using the coefficient of determination (R2) and relative root mean square error (RRMSE). Five-fold cross-validation with 10 repeats was employed to mitigate the limitation of small data size. Multi-omics models that integrate radiomics, dosiomics, and their delta counterparts outperform individual-omics models. Delta-radiomic features play a critical role in enhancing prediction accuracy relative to features at single time points. The top-performing model achieves an R2 of 0.743 and an RRMSE of 0.022. The proposed multi-omics SVR model shows promising performance in predicting continuous change of GTV. It provides a more quantitative and personalized approach to assist patient selection and treatment adjustment in PULSAR.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges</title>
<link>https://arxiv.org/abs/2509.07887</link>
<guid>https://arxiv.org/abs/2509.07887</guid>
<content:encoded><![CDATA[
arXiv:2509.07887v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have gained traction in the complex domain of drug discovery because of their ability to process graph-structured data such as drug molecule models. This approach has resulted in a myriad of methods and models in published literature across several categories of drug discovery research. This paper covers the research categories comprehensively with recent papers, namely molecular property prediction, including drug-target binding affinity prediction, drug-drug interaction study, microbiome interaction prediction, drug repositioning, retrosynthesis, and new drug design, and provides guidance for future work on GNNs for drug discovery.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feasibility of In-Ear Single-Channel ExG for Wearable Sleep~Monitoring in Real-World Settings</title>
<link>https://arxiv.org/abs/2509.07896</link>
<guid>https://arxiv.org/abs/2509.07896</guid>
<content:encoded><![CDATA[
arXiv:2509.07896v1 Announce Type: new 
Abstract: Automatic sleep staging typically relies on gold-standard EEG setups, which are accurate but obtrusive and impractical for everyday use outside sleep laboratories. This limits applicability in real-world settings, such as home environments, where continuous, long-term monitoring is needed. Detecting sleep onset is particularly relevant, enabling consumer applications (e.g. automatically pausing media playback when the user falls asleep). Recent research has shown correlations between in-ear EEG and full-scalp EEG for various phenomena, suggesting wearable, in-ear devices could allow unobtrusive sleep monitoring. We investigated the feasibility of using single-channel in-ear electrophysiological (ExG) signals for automatic sleep staging in a wearable device by conducting a sleep study with 11~participants (mean age: 24), using a custom earpiece with a dry eartip electrode (D\"atwyler SoftPulse) as a measurement electrode in one ear and a reference in the other. Ground truth sleep stages were obtained from an Apple Watch Ultra, validated for sleep staging. Our system achieved 90.5% accuracy for binary sleep detection (Awake vs. Asleep) and 65.1% accuracy for four-class staging (Awake, REM, Core, Deep) using leave-one-subject-out validation. These findings demonstrate the potential of in-ear electrodes as a low-effort, comfortable approach to sleep monitoring, with applications such as stopping podcasts when users fall asleep.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Modular Algorithm for Non-Stationary Online Convex-Concave Optimization</title>
<link>https://arxiv.org/abs/2509.07901</link>
<guid>https://arxiv.org/abs/2509.07901</guid>
<content:encoded><![CDATA[
arXiv:2509.07901v1 Announce Type: new 
Abstract: This paper investigates the problem of Online Convex-Concave Optimization, which extends Online Convex Optimization to two-player time-varying convex-concave games. The goal is to minimize the dynamic duality gap (D-DGap), a critical performance measure that evaluates players' strategies against arbitrary comparator sequences. Existing algorithms fail to deliver optimal performance, particularly in stationary or predictable environments. To address this, we propose a novel modular algorithm with three core components: an Adaptive Module that dynamically adjusts to varying levels of non-stationarity, a Multi-Predictor Aggregator that identifies the best predictor among multiple candidates, and an Integration Module that effectively combines their strengths. Our algorithm achieves a minimax optimal D-DGap upper bound, up to a logarithmic factor, while also ensuring prediction error-driven D-DGap bounds. The modular design allows for the seamless replacement of components that regulate adaptability to dynamic environments, as well as the incorporation of components that integrate ``side knowledge'' from multiple predictors. Empirical results further demonstrate the effectiveness and adaptability of the proposed method.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bio-KGvec2go: Serving up-to-date Dynamic Biomedical Knowledge Graph Embeddings</title>
<link>https://arxiv.org/abs/2509.07905</link>
<guid>https://arxiv.org/abs/2509.07905</guid>
<content:encoded><![CDATA[
arXiv:2509.07905v1 Announce Type: new 
Abstract: Knowledge graphs and ontologies represent entities and their relationships in a structured way, having gained significance in the development of modern AI applications. Integrating these semantic resources with machine learning models often relies on knowledge graph embedding models to transform graph data into numerical representations. Therefore, pre-trained models for popular knowledge graphs and ontologies are increasingly valuable, as they spare the need to retrain models for different tasks using the same data, thereby helping to democratize AI development and enabling sustainable computing.
  In this paper, we present Bio-KGvec2go, an extension of the KGvec2go Web API, designed to generate and serve knowledge graph embeddings for widely used biomedical ontologies. Given the dynamic nature of these ontologies, Bio-KGvec2go also supports regular updates aligned with ontology version releases. By offering up-to-date embeddings with minimal computational effort required from users, Bio-KGvec2go facilitates efficient and timely biomedical research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Scaling Laws for Large Language Models via Inverse Problems</title>
<link>https://arxiv.org/abs/2509.07909</link>
<guid>https://arxiv.org/abs/2509.07909</guid>
<content:encoded><![CDATA[
arXiv:2509.07909v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are large-scale pretrained models that have achieved remarkable success across diverse domains. These successes have been driven by unprecedented complexity and scale in both data and computations. However, due to the high costs of training such models, brute-force trial-and-error approaches to improve LLMs are not feasible. Inspired by the success of inverse problems in uncovering fundamental scientific laws, this position paper advocates that inverse problems can also efficiently uncover scaling laws that guide the building of LLMs to achieve the desirable performance with significantly better cost-effectiveness.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning</title>
<link>https://arxiv.org/abs/2509.07945</link>
<guid>https://arxiv.org/abs/2509.07945</guid>
<content:encoded><![CDATA[
arXiv:2509.07945v1 Announce Type: new 
Abstract: In heterogeneous multi-task learning, tasks not only exhibit diverse observation and action spaces but also vary substantially in intrinsic difficulty. While conventional multi-task world models like UniZero excel in single-task settings, we find that when handling large-scale heterogeneous environments, gradient conflicts and the loss of model plasticity often constrain their sample and computational efficiency. In this work, we address these challenges from two perspectives: the single learning iteration and the overall learning process. First, we investigate the impact of key design spaces on extending UniZero to multi-task planning. We find that a Mixture-of-Experts (MoE) architecture provides the most substantial performance gains by mitigating gradient conflicts, leading to our proposed model, \textit{ScaleZero}. Second, to dynamically balance the computational load across the learning process, we introduce an online, LoRA-based \textit{dynamic parameter scaling} (DPS) strategy. This strategy progressively integrates LoRA adapters in response to task-specific progress, enabling adaptive knowledge retention and parameter expansion. Empirical evaluations on standard benchmarks such as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying exclusively on online reinforcement learning with one model, attains performance on par with specialized single-task baselines. Furthermore, when augmented with our dynamic parameter scaling strategy, our method achieves competitive performance while requiring only 80\% of the single-task environment interaction steps. These findings underscore the potential of ScaleZero for effective large-scale multi-task learning. Our code is available at \textcolor{magenta}{https://github.com/opendilab/LightZero}.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges</title>
<link>https://arxiv.org/abs/2509.07946</link>
<guid>https://arxiv.org/abs/2509.07946</guid>
<content:encoded><![CDATA[
arXiv:2509.07946v1 Announce Type: new 
Abstract: Multi-modal multi-task (M3T) foundation models (FMs) have recently shown transformative potential in artificial intelligence, with emerging applications in education. However, their deployment in real-world educational settings is hindered by privacy regulations, data silos, and limited domain-specific data availability. We introduce M3T Federated Foundation Models (FedFMs) for education: a paradigm that integrates federated learning (FL) with M3T FMs to enable collaborative, privacy-preserving training across decentralized institutions while accommodating diverse modalities and tasks. Subsequently, this position paper aims to unveil M3T FedFMs as a promising yet underexplored approach to the education community, explore its potentials, and reveal its related future research directions. We outline how M3T FedFMs can advance three critical pillars of next-generation intelligent education systems: (i) privacy preservation, by keeping sensitive multi-modal student and institutional data local; (ii) personalization, through modular architectures enabling tailored models for students, instructors, and institutions; and (iii) equity and inclusivity, by facilitating participation from underrepresented and resource-constrained entities. We finally identify various open research challenges, including studying of (i) inter-institution heterogeneous privacy regulations, (ii) the non-uniformity of data modalities' characteristics, (iii) the unlearning approaches for M3T FedFMs, (iv) the continual learning frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must be collectively addressed for practical deployment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACE and Diverse Generalization via Selective Disagreement</title>
<link>https://arxiv.org/abs/2509.07955</link>
<guid>https://arxiv.org/abs/2509.07955</guid>
<content:encoded><![CDATA[
arXiv:2509.07955v1 Announce Type: new 
Abstract: Deep neural networks are notoriously sensitive to spurious correlations - where a model learns a shortcut that fails out-of-distribution. Existing work on spurious correlations has often focused on incomplete correlations,leveraging access to labeled instances that break the correlation. But in cases where the spurious correlations are complete, the correct generalization is fundamentally \textit{underspecified}. To resolve this underspecification, we propose learning a set of concepts that are consistent with training data but make distinct predictions on a subset of novel unlabeled inputs. Using a self-training approach that encourages \textit{confident} and \textit{selective} disagreement, our method ACE matches or outperforms existing methods on a suite of complete-spurious correlation benchmarks, while remaining robust to incomplete spurious correlations. ACE is also more configurable than prior approaches, allowing for straight-forward encoding of prior knowledge and principled unsupervised model selection. In an early application to language-model alignment, we find that ACE achieves competitive performance on the measurement tampering detection benchmark \textit{without} access to untrusted measurements. While still subject to important limitations, ACE represents significant progress towards overcoming underspecification.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing the Inductive Biases of Softmax Attention using Structured Matrices</title>
<link>https://arxiv.org/abs/2509.07963</link>
<guid>https://arxiv.org/abs/2509.07963</guid>
<content:encoded><![CDATA[
arXiv:2509.07963v1 Announce Type: new 
Abstract: The core component of attention is the scoring function, which transforms the inputs into low-dimensional queries and keys and takes the dot product of each pair. While the low-dimensional projection improves efficiency, it causes information loss for certain tasks that have intrinsically high-dimensional inputs. Additionally, attention uses the same scoring function for all input pairs, without imposing a distance-dependent compute bias for neighboring tokens in the sequence. In this work, we address these shortcomings by proposing new scoring functions based on computationally efficient structured matrices with high ranks, including Block Tensor-Train (BTT) and Multi-Level Low Rank (MLR) matrices. On in-context regression tasks with high-dimensional inputs, our proposed scoring functions outperform standard attention for any fixed compute budget. On language modeling, a task that exhibits locality patterns, our MLR-based attention method achieves improved scaling laws compared to both standard attention and variants of sliding window attention. Additionally, we show that both BTT and MLR fall under a broader family of efficient structured matrices capable of encoding either full-rank or distance-dependent compute biases, thereby addressing significant shortcomings of standard attention. Finally, we show that MLR attention has promising results for long-range time-series forecasting.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence</title>
<link>https://arxiv.org/abs/2509.07972</link>
<guid>https://arxiv.org/abs/2509.07972</guid>
<content:encoded><![CDATA[
arXiv:2509.07972v1 Announce Type: new 
Abstract: Learning rate warmup is a popular and practical technique in training large-scale deep neural networks. Despite the huge success in practice, the theoretical advantages of this strategy of gradually increasing the learning rate at the beginning of the training process have not been fully understood. To resolve this gap between theory and practice, we first propose a novel family of generalized smoothness assumptions, and validate its applicability both theoretically and empirically. Under the novel smoothness assumption, we study the convergence properties of gradient descent (GD) in both deterministic and stochastic settings. It is shown that learning rate warmup consistently accelerates GD, and GD with warmup can converge at most $\Theta(T)$ times faster than with a non-increasing learning rate schedule in some specific cases, providing insights into the benefits of this strategy from an optimization theory perspective.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation</title>
<link>https://arxiv.org/abs/2508.18933</link>
<guid>https://arxiv.org/abs/2508.18933</guid>
<content:encoded><![CDATA[
arXiv:2508.18933v1 Announce Type: cross 
Abstract: Automated detection of vulnerabilities in source code is an essential cybersecurity challenge, underpinning trust in digital systems and services. Graph Neural Networks (GNNs) have emerged as a promising approach as they can learn structural and logical code relationships in a data-driven manner. However, their performance is severely constrained by training data imbalances and label noise. GNNs often learn 'spurious' correlations from superficial code similarities, producing detectors that fail to generalize well to unseen real-world data. In this work, we propose a unified framework for robust and interpretable vulnerability detection, called VISION, to mitigate spurious correlations by systematically augmenting a counterfactual training dataset. Counterfactuals are samples with minimal semantic modifications but opposite labels. Our framework includes: (i) generating counterfactuals by prompting a Large Language Model (LLM); (ii) targeted GNN training on paired code examples with opposite labels; and (iii) graph-based interpretability to identify the crucial code statements relevant for vulnerability predictions while ignoring spurious ones. We find that VISION reduces spurious learning and enables more robust, generalizable detection, improving overall accuracy (from 51.8% to 97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20 vulnerability. We further demonstrate gains using proposed metrics: intra-class attribution variance, inter-class attribution distance, and node score dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real and counterfactual) from the high-impact CWE-20 category. Finally, VISION advances transparent and trustworthy AI-based cybersecurity systems through interactive visualization for human-in-the-loop analysis.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2509.04827</link>
<guid>https://arxiv.org/abs/2509.04827</guid>
<content:encoded><![CDATA[
arXiv:2509.04827v1 Announce Type: cross 
Abstract: Modern Large Language Model (LLM) serving systems increasingly support interactive applications, like real-time chat assistants, code generation tools, and agentic workflows. However, the soaring energy cost of LLM inference presents a growing challenge for sustainable and cost-effective deployment. This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM serving, built from a control theory perspective. VoltanaLLM co-designs frequency scaling and request routing in emerging prefill/decode disaggregated architectures, leveraging their decoupled execution to enable fine-grained phase-specific control. It consists of a feedback-driven frequency controller that dynamically adapts GPU frequency for prefill and decode phases, and a state-space router that explores routing decisions across frequency-scaled instances to minimize energy under latency constraints. We implement VoltanaLLM in SGLang and evaluate its performance over multiple state-of-the-art LLMs and real-world datasets. The results demonstrate that VoltanaLLM achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rate, paving the way for sustainable and intelligent LLM serving.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toric geometry of ReLU neural networks</title>
<link>https://arxiv.org/abs/2509.05894</link>
<guid>https://arxiv.org/abs/2509.05894</guid>
<content:encoded><![CDATA[
arXiv:2509.05894v1 Announce Type: cross 
Abstract: Given a continuous finitely piecewise linear function $f:\mathbb{R}^{n_0} \to \mathbb{R}$ and a fixed architecture $(n_0,\ldots,n_k;1)$ of feedforward ReLU neural networks, the exact function realization problem is to determine when some network with the given architecture realizes $f$. To develop a systematic way to answer these questions, we establish a connection between toric geometry and ReLU neural networks. This approach enables us to utilize numerous structures and tools from algebraic geometry to study ReLU neural networks. Starting with an unbiased ReLU neural network with rational weights, we define the ReLU fan, the ReLU toric variety, and the ReLU Cartier divisor associated with the network. This work also reveals the connection between the tropical geometry and the toric geometry of ReLU neural networks. As an application of the toric geometry framework, we prove a necessary and sufficient criterion of functions realizable by unbiased shallow ReLU neural networks by computing intersection numbers of the ReLU Cartier divisor and torus-invariant curves.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-device Zero-shot Label Transfer via Alignment of Time Series Foundation Model Embeddings</title>
<link>https://arxiv.org/abs/2509.06966</link>
<guid>https://arxiv.org/abs/2509.06966</guid>
<content:encoded><![CDATA[
arXiv:2509.06966v1 Announce Type: cross 
Abstract: High-quality, medically validated labels exist for clinical actigraphy data but not for ubiquitous consumer wearables like the Apple Watch. Manually labeling wearables data is expensive and doesn't scale. This paper offers a novel framework that transfers valuable labels from a source domain (e.g., actigraphy) to a target domain (e.g., Apple Watch) without requiring paired data. Instead of working with raw time-series signals, we project both domains into a shared latent embedding space using time-series foundation models (TSFMs) and develop a new framework to align the cross-device representations. Our method, Adversarial Alignment of TSFM Embeddings forces the distributions of source and target embeddings to align within this space, facilitating label transfer across device type.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2509.06988</link>
<guid>https://arxiv.org/abs/2509.06988</guid>
<content:encoded><![CDATA[
arXiv:2509.06988v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection helps models identify data outside the training categories, crucial for security applications. While feature-based post-hoc methods address this by evaluating data differences in the feature space without changing network parameters, they often require access to training data, which may not be suitable for some data privacy scenarios. This may not be suitable in scenarios where data privacy protection is a concern. In this paper, we propose a simple yet effective post-hoc method, termed Classifier-based Feature Reconstruction (ClaFR), from the perspective of subspace projection. It first performs an orthogonal decomposition of the classifier's weights to extract the class-known subspace, then maps the original data features into this subspace to obtain new data representations. Subsequently, the OOD score is determined by calculating the feature reconstruction error of the data within the subspace. Compared to existing OOD detection algorithms, our method does not require access to training data while achieving leading performance on multiple OOD benchmarks. Our code is released at https://github.com/Aie0923/ClaFR.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining</title>
<link>https://arxiv.org/abs/2509.06990</link>
<guid>https://arxiv.org/abs/2509.06990</guid>
<content:encoded><![CDATA[
arXiv:2509.06990v1 Announce Type: cross 
Abstract: Continued pretraining offers a promising solution for adapting foundation models to a new target domain. However, in specialized domains, available datasets are often very small, limiting the applicability of SSL methods developed for large-scale pretraining and making hyperparameter search infeasible. In addition, pretrained models are usually released as backbone-weights only, lacking important information to continue pretraining. We propose to bridge this gap with DIET-CP, a simple continued pretraining strategy, where any strong foundation model can be steered towards the new data distribution of interest. DIET-CP relies on a very simple objective, requires no labels, and introduces no more hyperparameters than supervised finetuning. It is stable across data modalities and backbone choices, while providing a significant performance boost for state-of-the-art models such as DINOv3 using only 1000 images.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Protocol Genome A Self Supervised Learning Framework from DICOM Headers</title>
<link>https://arxiv.org/abs/2509.06995</link>
<guid>https://arxiv.org/abs/2509.06995</guid>
<content:encoded><![CDATA[
arXiv:2509.06995v1 Announce Type: cross 
Abstract: In this paper, we introduce the Protocol Genome, a self-supervised learning system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs 0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation. Our method also improves calibration and robustness across modalities (CT, MRI, CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice thickness) have consequences for contrast, noise, and artifact. These latent confounders impede the generalization of image-only networks across sites. We consider structured DICOM headers as a label and learn protocol-aware but clinically robust image representations. Protocol Genome obtains tokenized embeddings of de-identified header fields and models them along with image features using: (1) protocol-image contrastive learning, (2) masked protocol prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041: cardiomegaly) is associated with higher external AUROC; 25-37% calibration improvements are obtained (p < 0.01, DeLong tests). While the gains may be task-dependent, they are preserved with 10-20% of labeled data. From a clinical point of view, the technique reduces false positives at protocol borders and is applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a model card and deployment guide, complete with both de-identification and bias audits.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories</title>
<link>https://arxiv.org/abs/2509.06998</link>
<guid>https://arxiv.org/abs/2509.06998</guid>
<content:encoded><![CDATA[
arXiv:2509.06998v1 Announce Type: cross 
Abstract: Can models generalize attribute knowledge across semantically and perceptually dissimilar categories? While prior work has addressed attribute prediction within narrow taxonomic or visually similar domains, it remains unclear whether current models can abstract attributes and apply them to conceptually distant categories. This work presents the first explicit evaluation for the robustness of the attribute prediction task under such conditions, testing whether models can correctly infer shared attributes between unrelated object types: e.g., identifying that the attribute "has four legs" is common to both "dogs" and "chairs". To enable this evaluation, we introduce train-test split strategies that progressively reduce correlation between training and test sets, based on: LLM-driven semantic grouping, embedding similarity thresholding, embedding-based clustering, and supercategory-based partitioning using ground-truth labels. Results show a sharp drop in performance as the correlation between training and test categories decreases, indicating strong sensitivity to split design. Among the evaluated methods, clustering yields the most effective trade-off, reducing hidden correlations while preserving learnability. These findings offer new insights into the limitations of current representations and inform future benchmark construction for attribute reasoning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD</title>
<link>https://arxiv.org/abs/2509.07003</link>
<guid>https://arxiv.org/abs/2509.07003</guid>
<content:encoded><![CDATA[
arXiv:2509.07003v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have scaled rapidly in size and complexity, requiring increasingly intricate parallelism for distributed training, such as 3D parallelism. This sophistication motivates a shift toward simpler, more debuggable programming paradigm like Single Program Multiple Data (SPMD). However, SPMD in eager execution introduces two key challenges: ensuring consistency with single-device execution and achieving high performance at scale. In this paper, we introduce veScale, an eager-mode training system that fully embraces SPMD paradigm to democratize distributed tensor programming. veScale addresses the prevalent issue of inconsistent results in systems like PyTorch by introducing a novel algorithm of distributed Random Number Generation (RNG) compatible with arbitrary sharded operators. veScale also significantly boosts training performance by reducing PyTorch primitive's overhead and improving communication efficiency. Evaluations show that veScale delivers up to 2.2x speedup over the state-of-the-art training systems, like TorchTitan, and cuts code complexity by 78.4%, while preserving single-device-equivalent results.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code</title>
<link>https://arxiv.org/abs/2509.07006</link>
<guid>https://arxiv.org/abs/2509.07006</guid>
<content:encoded><![CDATA[
arXiv:2509.07006v1 Announce Type: cross 
Abstract: This paper introduces ArGen (Auto-Regulation of Generative AI systems), a framework for aligning Large Language Models (LLMs) with complex sets of configurable, machine-readable rules spanning ethical principles, operational safety protocols, and regulatory compliance standards. Moving beyond just preference-based alignment, ArGen is designed to ensure LLMs adhere to these multifaceted policies through a novel synthesis of principle-based automated reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy Agent (OPA) inspired governance layer. This approach provides the technical foundation for achieving and demonstrating compliance with diverse and nuanced governance requirements. To showcase the framework's capability to operationalize a deeply nuanced and culturally-specific value system, we present an in-depth case study: the development of a medical AI assistant guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as derived from texts like the Bhagavad Gita. This challenging application demonstrates ArGen's adaptability, achieving a 70.9% improvement in domain-scope adherence over the baseline. Through our open-source repository, we show that ArGen's methodology offers a path to 'Governable Al' systems that are technically proficient, ethically robust, and verifiably compliant for safe deployment in diverse global contexts.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV</title>
<link>https://arxiv.org/abs/2509.07016</link>
<guid>https://arxiv.org/abs/2509.07016</guid>
<content:encoded><![CDATA[
arXiv:2509.07016v1 Announce Type: cross 
Abstract: In response to the prevalent concern of TCP SYN flood attacks within the context of Software-Defined Internet of Vehicles (SD-IoV), this study addresses the significant challenge of network security in rapidly evolving vehicular communication systems. This research focuses on optimizing a Random Forest Classifier model to achieve maximum accuracy and minimal detection time, thereby enhancing vehicular network security. The methodology involves preprocessing a dataset containing SYN attack instances, employing feature scaling and label encoding techniques, and applying Stratified K-Fold cross-validation to target key metrics such as accuracy, precision, recall, and F1-score. This research achieved an average value of 0.999998 for all metrics with a SYN DoS attack detection time of 0.24 seconds. Results show that the fine-tuned Random Forest model, configured with 20 estimators and a depth of 10, effectively differentiates between normal and malicious traffic with high accuracy and minimal detection time, which is crucial for SD-IoV networks. This approach marks a significant advancement and introduces a state-of-the-art algorithm in detecting SYN flood attacks, combining high accuracy with minimal detection time. It contributes to vehicular network security by providing a robust solution against TCP SYN flood attacks while maintaining network efficiency and reliability.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning</title>
<link>https://arxiv.org/abs/2509.07017</link>
<guid>https://arxiv.org/abs/2509.07017</guid>
<content:encoded><![CDATA[
arXiv:2509.07017v1 Announce Type: cross 
Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning framework that embeds logical rules as spectral templates and performs inference directly in the graph spectral domain. By leveraging graph signal processing (GSP) and frequency-selective filters grounded in the Laplacian eigenstructure of knowledge graphs, the architecture unifies the interpretability of symbolic reasoning with the scalability and adaptability of spectral learning. Beyond the core formulation, we incorporate a comprehensive set of extensions, including dynamic graph and basis learning, rational and diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts for modular specialization, proof-guided training with spectral curricula, and uncertainty quantification for calibrated confidence. Additional enhancements such as large language model coupling, co-spectral transfer alignment, adversarial robustness, efficient GPU kernels, generalized Laplacians, and causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior accuracy, faster inference, improved robustness to adversarial perturbations, and higher interpretability compared to leading baselines including transformers, message-passing neural networks, and neuro-symbolic logic programming systems. Spectral attribution and proof-band agreement analyses confirm that model decisions align closely with symbolic proof structures, while transfer experiments validate effective domain adaptation through co-spectral alignment. These results establish Spectral NSR as a scalable and principled foundation for the next generation of reasoning systems, offering transparency, robustness, and generalization beyond conventional approaches.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Private Queries with Sigma-Counting</title>
<link>https://arxiv.org/abs/2509.07018</link>
<guid>https://arxiv.org/abs/2509.07018</guid>
<content:encoded><![CDATA[
arXiv:2509.07018v1 Announce Type: cross 
Abstract: Many data applications involve counting queries, where a client specifies a feasible range of variables and a database returns the corresponding item counts. A program that produces the counts of different queries often risks leaking sensitive individual-level information. A popular approach to enhance data privacy is to return a noisy version of the actual count. It is typically achieved by adding independent noise to each query and then control the total privacy budget within a period. This approach may be limited in the number of queries and output accuracy in practice. Also, the returned counts do not maintain the total order for nested queries, an important feature in many applications. This work presents the design and analysis of a new method, sigma-counting, that addresses these challenges. Sigma-counting uses the notion of sigma-algebra to construct privacy-preserving counting queries. We show that the proposed concepts and methods can significantly improve output accuracy while maintaining a desired privacy level in the presence of massive queries to the same data. We also discuss how the technique can be applied to address large and time-varying datasets.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Guided Diffusion Transformer with Spherical Harmonic Posterior Sampling for High-Fidelity Angular Super-Resolution in Diffusion MRI</title>
<link>https://arxiv.org/abs/2509.07020</link>
<guid>https://arxiv.org/abs/2509.07020</guid>
<content:encoded><![CDATA[
arXiv:2509.07020v1 Announce Type: cross 
Abstract: Diffusion MRI (dMRI) angular super-resolution (ASR) aims to reconstruct high-angular-resolution (HAR) signals from limited low-angular-resolution (LAR) data without prolonging scan time. However, existing methods are limited in recovering fine-grained angular details or preserving high fidelity due to inadequate modeling of q-space geometry and insufficient incorporation of physical constraints. In this paper, we introduce a Physics-Guided Diffusion Transformer (PGDiT) designed to explore physical priors throughout both training and inference stages. During training, a Q-space Geometry-Aware Module (QGAM) with b-vector modulation and random angular masking facilitates direction-aware representation learning, enabling the network to generate directionally consistent reconstructions with fine angular details from sparse and noisy data. In inference, a two-stage Spherical Harmonics-Guided Posterior Sampling (SHPS) enforces alignment with the acquired data, followed by heat-diffusion-based SH regularization to ensure physically plausible reconstructions. This coarse-to-fine refinement strategy mitigates oversmoothing and artifacts commonly observed in purely data-driven or generative models. Extensive experiments on general ASR tasks and two downstream applications, Diffusion Tensor Imaging (DTI) and Neurite Orientation Dispersion and Density Imaging (NODDI), demonstrate that PGDiT outperforms existing deep learning models in detail recovery and data fidelity. Our approach presents a novel generative ASR framework that offers high-fidelity HAR dMRI reconstructions, with potential applications in neuroscience and clinical research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGLF-SINN: Deep Learning Surrogate Model for Accelerating Turbulent Transport Modeling in Fusion</title>
<link>https://arxiv.org/abs/2509.07024</link>
<guid>https://arxiv.org/abs/2509.07024</guid>
<content:encoded><![CDATA[
arXiv:2509.07024v1 Announce Type: cross 
Abstract: The Trapped Gyro-Landau Fluid (TGLF) model provides fast, accurate predictions of turbulent transport in tokamaks, but whole device simulations requiring thousands of evaluations remain computationally expensive. Neural network (NN) surrogates offer accelerated inference with fully differentiable approximations that enable gradient-based coupling but typically require large training datasets to capture transport flux variations across plasma conditions, creating significant training burden and limiting applicability to expensive gyrokinetic simulations. We propose \textbf{TGLF-SINN (Spectra-Informed Neural Network)} with three key innovations: (1) principled feature engineering that reduces target prediction range, simplifying the learning task; (2) physics-guided regularization of transport spectra to improve generalization under sparse data; and (3) Bayesian Active Learning (BAL) to strategically select training samples based on model uncertainty, reducing data requirements while maintaining accuracy. Our approach achieves superior performance with significantly less training data. In offline settings, TGLF-SINN reduces logarithmic root mean squared error (LRMSE) by 12. 4\% compared to the current baseline \base. Using only 25\% of the complete dataset with BAL, we achieve LRMSE only 0.0165 higher than \base~and 0.0248 higher than our offline model (0.0583). In downstream flux matching applications, our NN surrogate provides 45x speedup over TGLF while maintaining comparable accuracy, demonstrating potential for training efficient surrogates for higher-fidelity models where data acquisition is costly and sparse.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2509.07027</link>
<guid>https://arxiv.org/abs/2509.07027</guid>
<content:encoded><![CDATA[
arXiv:2509.07027v1 Announce Type: cross 
Abstract: We propose a novel regularization loss that enforces standard Gaussianity, encouraging samples to align with a standard Gaussian distribution. This facilitates a range of downstream tasks involving optimization in the latent space of text-to-image models. We treat elements of a high-dimensional sample as one-dimensional standard Gaussian variables and define a composite loss that combines moment-based regularization in the spatial domain with power spectrum-based regularization in the spectral domain. Since the expected values of moments and power spectrum distributions are analytically known, the loss promotes conformity to these properties. To ensure permutation invariance, the losses are applied to randomly permuted inputs. Notably, existing Gaussianity-based regularizations fall within our unified framework: some correspond to moment losses of specific orders, while the previous covariance-matching loss is equivalent to our spectral loss but incurs higher time complexity due to its spatial-domain computation. We showcase the application of our regularization in generative modeling for test-time reward alignment with a text-to-image model, specifically to enhance aesthetics and text alignment. Our regularization outperforms previous Gaussianity regularization, effectively prevents reward hacking and accelerates convergence.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Quantum Bagging Algorithm with Unsupervised Base Learners for Label Corrupted Datasets</title>
<link>https://arxiv.org/abs/2509.07040</link>
<guid>https://arxiv.org/abs/2509.07040</guid>
<content:encoded><![CDATA[
arXiv:2509.07040v1 Announce Type: cross 
Abstract: The development of noise-resilient quantum machine learning (QML) algorithms is critical in the noisy intermediate-scale quantum (NISQ) era. In this work, we propose a quantum bagging framework that uses QMeans clustering as the base learner to reduce prediction variance and enhance robustness to label noise. Unlike bagging frameworks built on supervised learners, our method leverages the unsupervised nature of QMeans, combined with quantum bootstrapping via QRAM-based sampling and bagging aggregation through majority voting. Through extensive simulations on both noisy classification and regression tasks, we demonstrate that the proposed quantum bagging algorithm performs comparably to its classical counterpart using KMeans while exhibiting greater resilience to label corruption than supervised bagging methods. This highlights the potential of unsupervised quantum bagging in learning from unreliable data.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PUUMA (Placental patch and whole-Uterus dual-branch U-Mamba-based Architecture): Functional MRI Prediction of Gestational Age at Birth and Preterm Risk</title>
<link>https://arxiv.org/abs/2509.07042</link>
<guid>https://arxiv.org/abs/2509.07042</guid>
<content:encoded><![CDATA[
arXiv:2509.07042v1 Announce Type: cross 
Abstract: Preterm birth is a major cause of mortality and lifelong morbidity in childhood. Its complex and multifactorial origins limit the effectiveness of current clinical predictors and impede optimal care. In this study, a dual-branch deep learning architecture (PUUMA) was developed to predict gestational age (GA) at birth using T2* fetal MRI data from 295 pregnancies, encompassing a heterogeneous and imbalanced population. The model integrates both global whole-uterus and local placental features. Its performance was benchmarked against linear regression using cervical length measurements obtained by experienced clinicians from anatomical MRI and other Deep Learning architectures. The GA at birth predictions were assessed using mean absolute error. Accuracy, sensitivity, and specificity were used to assess preterm classification. Both the fully automated MRI-based pipeline and the cervical length regression achieved comparable mean absolute errors (3 weeks) and good sensitivity (0.67) for detecting preterm birth, despite pronounced class imbalance in the dataset. These results provide a proof of concept for automated prediction of GA at birth from functional MRI, and underscore the value of whole-uterus functional imaging in identifying at-risk pregnancies. Additionally, we demonstrate that manual, high-definition cervical length measurements derived from MRI, not currently routine in clinical practice, offer valuable predictive information. Future work will focus on expanding the cohort size and incorporating additional organ-specific imaging to improve generalisability and predictive performance.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards</title>
<link>https://arxiv.org/abs/2509.07047</link>
<guid>https://arxiv.org/abs/2509.07047</guid>
<content:encoded><![CDATA[
arXiv:2509.07047v1 Announce Type: cross 
Abstract: Image segmentation is a critical task in microscopy, essential for accurately analyzing and interpreting complex visual data. This task can be performed using custom models trained on domain-specific datasets, transfer learning from pre-trained models, or foundational models that offer broad applicability. However, foundational models often present a considerable number of non-transparent tuning parameters that require extensive manual optimization, limiting their usability for real-time streaming data analysis. Here, we introduce a reward function-based optimization to fine-tune foundational models and illustrate this approach for SAM (Segment Anything Model) framework by Meta. The reward functions can be constructed to represent the physics of the imaged system, including particle size distributions, geometries, and other criteria. By integrating a reward-driven optimization framework, we enhance SAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$, that better aligns with the requirements of diverse segmentation tasks and particularly allows for real-time streaming data segmentation. We demonstrate the effectiveness of this approach in microscopy imaging, where precise segmentation is crucial for analyzing cellular structures, material interfaces, and nanoscale features.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Efficiency in Keyword Spotting: A System-Level Approach for Embedded Microcontrollers</title>
<link>https://arxiv.org/abs/2509.07051</link>
<guid>https://arxiv.org/abs/2509.07051</guid>
<content:encoded><![CDATA[
arXiv:2509.07051v1 Announce Type: cross 
Abstract: Keyword spotting (KWS) is a key enabling technology for hands-free interaction in embedded and IoT devices, where stringent memory and energy constraints challenge the deployment of AI-enabeld devices. In this work, we systematically evaluate and compare several state-of-the-art lightweight neural network architectures, including DS-CNN, LiCoNet, and TENet, alongside our proposed Typman-KWS (TKWS) architecture built upon MobileNet, specifically designed for efficient KWS on microcontroller units (MCUs). Unlike prior studies focused solely on model inference, our analysis encompasses the entire processing pipeline, from Mel-Frequency Cepstral Coefficient (MFCC) feature extraction to neural inference, and is benchmarked across three STM32 platforms (N6, H7, and U5). Our results show that TKWS with three residual blocks achieves up to 92.4% F1-score with only 14.4k parameters, reducing memory footprint without compromising the accuracy. Moreover, the N6 MCU with integrated neural acceleration achieves the best energy-delay product (EDP), enabling efficient, low-latency operation even with high-resolution features. Our findings highlight the model accuracy alone does not determine real-world effectiveness; rather, optimal keyword spotting deployments require careful consideration of feature extraction parameters and hardware-specific optimization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Methods in Generative AI</title>
<link>https://arxiv.org/abs/2509.07054</link>
<guid>https://arxiv.org/abs/2509.07054</guid>
<content:encoded><![CDATA[
arXiv:2509.07054v1 Announce Type: cross 
Abstract: Generative Artificial Intelligence is emerging as an important technology, promising to be transformative in many areas. At the same time, generative AI techniques are based on sampling from probabilistic models, and by default, they come with no guarantees about correctness, safety, fairness, or other properties. Statistical methods offer a promising potential approach to improve the reliability of generative AI techniques. In addition, statistical methods are also promising for improving the quality and efficiency of AI evaluation, as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics, explaining both the general statistical techniques used, as well as their applications to generative AI. We also discuss limitations and potential future directions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequentially Auditing Differential Privacy</title>
<link>https://arxiv.org/abs/2509.07055</link>
<guid>https://arxiv.org/abs/2509.07055</guid>
<content:encoded><![CDATA[
arXiv:2509.07055v1 Announce Type: cross 
Abstract: We propose a practical sequential test for auditing differential privacy guarantees of black-box mechanisms. The test processes streams of mechanisms' outputs providing anytime-valid inference while controlling Type I error, overcoming the fixed sample size limitation of previous batch auditing methods. Experiments show this test detects violations with sample sizes that are orders of magnitude smaller than existing methods, reducing this number from 50K to a few hundred examples, across diverse realistic mechanisms. Notably, it identifies DP-SGD privacy violations in \textit{under} one training run, unlike prior methods needing full model training.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADHAM: Additive Deep Hazard Analysis Mixtures for Interpretable Survival Regression</title>
<link>https://arxiv.org/abs/2509.07108</link>
<guid>https://arxiv.org/abs/2509.07108</guid>
<content:encoded><![CDATA[
arXiv:2509.07108v1 Announce Type: cross 
Abstract: Survival analysis is a fundamental tool for modeling time-to-event outcomes in healthcare. Recent advances have introduced flexible neural network approaches for improved predictive performance. However, most of these models do not provide interpretable insights into the association between exposures and the modeled outcomes, a critical requirement for decision-making in clinical practice. To address this limitation, we propose Additive Deep Hazard Analysis Mixtures (ADHAM), an interpretable additive survival model. ADHAM assumes a conditional latent structure that defines subgroups, each characterized by a combination of covariate-specific hazard functions. To select the number of subgroups, we introduce a post-training refinement that reduces the number of equivalent latent subgroups by merging similar groups. We perform comprehensive studies to demonstrate ADHAM's interpretability at the population, subgroup, and individual levels. Extensive experiments on real-world datasets show that ADHAM provides novel insights into the association between exposures and outcomes. Further, ADHAM remains on par with existing state-of-the-art survival baselines in terms of predictive performance, offering a scalable and interpretable approach to time-to-event prediction in healthcare.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NestGNN: A Graph Neural Network Framework Generalizing the Nested Logit Model for Travel Mode Choice</title>
<link>https://arxiv.org/abs/2509.07123</link>
<guid>https://arxiv.org/abs/2509.07123</guid>
<content:encoded><![CDATA[
arXiv:2509.07123v1 Announce Type: cross 
Abstract: Nested logit (NL) has been commonly used for discrete choice analysis, including a wide range of applications such as travel mode choice, automobile ownership, or location decisions. However, the classical NL models are restricted by their limited representation capability and handcrafted utility specification. While researchers introduced deep neural networks (DNNs) to tackle such challenges, the existing DNNs cannot explicitly capture inter-alternative correlations in the discrete choice context. To address the challenges, this study proposes a novel concept - alternative graph - to represent the relationships among travel mode alternatives. Using a nested alternative graph, this study further designs a nested-utility graph neural network (NestGNN) as a generalization of the classical NL model in the neural network family. Theoretically, NestGNNs generalize the classical NL models and existing DNNs in terms of model representation, while retaining the crucial two-layer substitution patterns of the NL models: proportional substitution within a nest but non-proportional substitution beyond a nest. Empirically, we find that the NestGNNs significantly outperform the benchmark models, particularly the corresponding NL models by 9.2\%. As shown by elasticity tables and substitution visualization, NestGNNs retain the two-layer substitution patterns as the NL model, and yet presents more flexibility in its model design space. Overall, our study demonstrates the power of NestGNN in prediction, interpretation, and its flexibility of generalizing the classical NL model for analyzing travel mode choice.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study</title>
<link>https://arxiv.org/abs/2509.07132</link>
<guid>https://arxiv.org/abs/2509.07132</guid>
<content:encoded><![CDATA[
arXiv:2509.07132v1 Announce Type: cross 
Abstract: The widespread use of generative AI has shown remarkable success in producing highly realistic deepfakes, posing a serious threat to various voice biometric applications, including speaker verification, voice biometrics, audio conferencing, and criminal investigations. To counteract this, several state-of-the-art (SoTA) audio deepfake detection (ADD) methods have been proposed to identify generative AI signatures to distinguish between real and deepfake audio. However, the effectiveness of these methods is severely undermined by anti-forensic (AF) attacks that conceal generative signatures. These AF attacks span a wide range of techniques, including statistical modifications (e.g., pitch shifting, filtering, noise addition, and quantization) and optimization-based attacks (e.g., FGSM, PGD, C \& W, and DeepFool). In this paper, we investigate the SoTA ADD methods and provide a comparative analysis to highlight their effectiveness in exposing deepfake signatures, as well as their vulnerabilities under adversarial conditions. We conducted an extensive evaluation of ADD methods on five deepfake benchmark datasets using two categories: raw and spectrogram-based approaches. This comparative analysis enables a deeper understanding of the strengths and limitations of SoTA ADD methods against diverse AF attacks. It does not only highlight vulnerabilities of ADD methods, but also informs the design of more robust and generalized detectors for real-world voice biometrics. It will further guide future research in developing adaptive defense strategies that can effectively counter evolving AF techniques.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoiding Over-Personalization with Rule-Guided Knowledge Graph Adaptation for LLM Recommendations</title>
<link>https://arxiv.org/abs/2509.07133</link>
<guid>https://arxiv.org/abs/2509.07133</guid>
<content:encoded><![CDATA[
arXiv:2509.07133v1 Announce Type: cross 
Abstract: We present a lightweight neuro-symbolic framework to mitigate over-personalization in LLM-based recommender systems by adapting user-side Knowledge Graphs (KGs) at inference time. Instead of retraining models or relying on opaque heuristics, our method restructures a user's Personalized Knowledge Graph (PKG) to suppress feature co-occurrence patterns that reinforce Personalized Information Environments (PIEs), i.e., algorithmically induced filter bubbles that constrain content diversity. These adapted PKGs are used to construct structured prompts that steer the language model toward more diverse, Out-PIE recommendations while preserving topical relevance. We introduce a family of symbolic adaptation strategies, including soft reweighting, hard inversion, and targeted removal of biased triples, and a client-side learning algorithm that optimizes their application per user. Experiments on a recipe recommendation benchmark show that personalized PKG adaptations significantly increase content novelty while maintaining recommendation quality, outperforming global adaptation and naive prompt-based methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive Retrieval</title>
<link>https://arxiv.org/abs/2509.07163</link>
<guid>https://arxiv.org/abs/2509.07163</guid>
<content:encoded><![CDATA[
arXiv:2509.07163v1 Announce Type: cross 
Abstract: The widely used retrieve-and-rerank pipeline faces two critical limitations: they are constrained by the initial retrieval quality of the top-k documents, and the growing computational demands of LLM-based rerankers restrict the number of documents that can be effectively processed. We introduce Reranker-Guided-Search (RGS), a novel approach that bypasses these limitations by directly retrieving documents according to reranker preferences rather than following the traditional sequential reranking method. Our method uses a greedy search on proximity graphs generated by approximate nearest neighbor algorithms, strategically prioritizing promising documents for reranking based on document similarity. Experimental results demonstrate substantial performance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9 on FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100 documents. Our analysis suggests that, given a fixed pair of embedding and reranker models, strategically selecting documents to rerank can significantly improve retrieval accuracy under limited reranker budget.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimensionally Reduced Open-World Clustering: DROWCULA</title>
<link>https://arxiv.org/abs/2509.07184</link>
<guid>https://arxiv.org/abs/2509.07184</guid>
<content:encoded><![CDATA[
arXiv:2509.07184v1 Announce Type: cross 
Abstract: Working with annotated data is the cornerstone of supervised learning. Nevertheless, providing labels to instances is a task that requires significant human effort. Several critical real-world applications make things more complicated because no matter how many labels may have been identified in a task of interest, it could be the case that examples corresponding to novel classes may appear in the future. Not unsurprisingly, prior work in this, so-called, `open-world' context has focused a lot on semi-supervised approaches.
  Focusing on image classification, somehow paradoxically, we propose a fully unsupervised approach to the problem of determining the novel categories in a particular dataset. Our approach relies on estimating the number of clusters using Vision Transformers, which utilize attention mechanisms to generate vector embeddings. Furthermore, we incorporate manifold learning techniques to refine these embeddings by exploiting the intrinsic geometry of the data, thereby enhancing the overall image clustering performance. Overall, we establish new State-of-the-Art results on single-modal clustering and Novel Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do so, both when the number of clusters is known or unknown ahead of time. The code is available at: https://github.com/DROWCULA/DROWCULA.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A transformer-based generative model for planetary systems</title>
<link>https://arxiv.org/abs/2509.07226</link>
<guid>https://arxiv.org/abs/2509.07226</guid>
<content:encoded><![CDATA[
arXiv:2509.07226v1 Announce Type: cross 
Abstract: Numerical calculations of planetary system formation are very demanding in terms of computing power. These synthetic planetary systems can however provide access to correlations, as predicted in a given numerical framework, between the properties of planets in the same system. Such correlations can, in return, be used in order to guide and prioritize observational campaigns aiming at discovering some types of planets, as Earth-like planets. Our goal is to develop a generative model which is capable of capturing correlations and statistical relationships between planets in the same system. Such a model, trained on the Bern model, offers the possibility to generate large number of synthetic planetary systems with little computational cost, that can be used, for example, to guide observational campaigns. Our generative model is based on the transformer architecture which is well-known to efficiently capture correlations in sequences and is at the basis of all modern Large Language Models. To assess the validity of the generative model, we perform visual and statistical comparisons, as well as a machine learning driven tests. Finally, as a use case example, we consider the TOI-469 system, in which we aim at predicting the possible properties of planets c and d, based on the properties of planet b (the first that has been detected). We show using different comparison methods that the properties of systems generated by our model are very similar to the ones of the systems computed directly by the Bern model. We also show in the case of the TOI-469 system, that using the generative model allows to predict the properties of planets not yet observed, based on the properties of the already observed planet. We provide our model to the community on our website www.ai4exoplanets.com.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Conventional Forward-Backward Tie in Neural Networks: Activation Functions</title>
<link>https://arxiv.org/abs/2509.07236</link>
<guid>https://arxiv.org/abs/2509.07236</guid>
<content:encoded><![CDATA[
arXiv:2509.07236v1 Announce Type: cross 
Abstract: Gradient-based neural network training traditionally enforces symmetry between forward and backward propagation, requiring activation functions to be differentiable (or sub-differentiable) and strictly monotonic in certain regions to prevent flat gradient areas. This symmetry, linking forward activations closely to backward gradients, significantly restricts the selection of activation functions, particularly excluding those with substantial flat or non-differentiable regions. In this paper, we challenge this assumption through mathematical analysis, demonstrating that precise gradient magnitudes derived from activation functions are largely redundant, provided the gradient direction is preserved. Empirical experiments conducted on foundational architectures - such as Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Binary Neural Networks (BNNs) - confirm that relaxing forward-backward symmetry and substituting traditional gradients with simpler or stochastic alternatives does not impair learning and may even enhance training stability and efficiency. We explicitly demonstrate that neural networks with flat or non-differentiable activation functions, such as the Heaviside step function, can be effectively trained, thereby expanding design flexibility and computational efficiency. Further empirical validation with more complex architectures remains a valuable direction for future research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring</title>
<link>https://arxiv.org/abs/2509.07260</link>
<guid>https://arxiv.org/abs/2509.07260</guid>
<content:encoded><![CDATA[
arXiv:2509.07260v1 Announce Type: cross 
Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individuals' quality of life. Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. We systematically evaluated SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade</title>
<link>https://arxiv.org/abs/2509.07274</link>
<guid>https://arxiv.org/abs/2509.07274</guid>
<content:encoded><![CDATA[
arXiv:2509.07274v1 Announce Type: cross 
Abstract: Migration has been a core topic in German political debate, from millions of expellees post World War II over labor migration to refugee movements in the recent past. Studying political speech regarding such wide-ranging phenomena in depth traditionally required extensive manual annotations, limiting the scope of analysis to small subsets of the data. Large language models (LLMs) have the potential to partially automate even complex annotation tasks. We provide an extensive evaluation of a multiple LLMs in annotating (anti-)solidarity subtypes in German parliamentary debates compared to a large set of thousands of human reference annotations (gathered over a year). We evaluate the influence of model size, prompting differences, fine-tuning, historical versus contemporary data; and we investigate systematic errors. Beyond methodological evaluation, we also interpret the resulting annotations from a social science lense, gaining deeper insight into (anti-)solidarity trends towards migrants in the German post-World War II period and recent past. Our data reveals a high degree of migrant-directed solidarity in the postwar period, as well as a strong trend towards anti-solidarity in the German parliament since 2015, motivating further research. These findings highlight the promise of LLMs for political text analysis and the importance of migration debates in Germany, where demographic decline and labor shortages coexist with rising polarization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel VICReg for Self-Supervised Learning in Reproducing Kernel Hilbert Space</title>
<link>https://arxiv.org/abs/2509.07289</link>
<guid>https://arxiv.org/abs/2509.07289</guid>
<content:encoded><![CDATA[
arXiv:2509.07289v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for representation learning by optimizing geometric objectives--such as invariance to augmentations, variance preservation, and feature decorrelation--without requiring labels. However, most existing methods operate in Euclidean space, limiting their ability to capture nonlinear dependencies and geometric structures. In this work, we propose Kernel VICReg, a novel self-supervised learning framework that lifts the VICReg objective into a Reproducing Kernel Hilbert Space (RKHS). By kernelizing each term of the loss-variance, invariance, and covariance--we obtain a general formulation that operates on double-centered kernel matrices and Hilbert-Schmidt norms, enabling nonlinear feature learning without explicit mappings.
  We demonstrate that Kernel VICReg not only avoids representational collapse but also improves performance on tasks with complex or small-scale data. Empirical evaluations across MNIST, CIFAR-10, STL-10, TinyImageNet, and ImageNet100 show consistent gains over Euclidean VICReg, with particularly strong improvements on datasets where nonlinear structures are prominent. UMAP visualizations further confirm that kernel-based embeddings exhibit better isometry and class separation. Our results suggest that kernelizing SSL objectives is a promising direction for bridging classical kernel methods with modern representation learning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction Alignment Improves Unified Multimodal Models</title>
<link>https://arxiv.org/abs/2509.07295</link>
<guid>https://arxiv.org/abs/2509.07295</guid>
<content:encoded><![CDATA[
arXiv:2509.07295v1 Announce Type: cross 
Abstract: Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit 6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Neural Signatures from fMRI using Hybrid Principal Components Regression</title>
<link>https://arxiv.org/abs/2509.07300</link>
<guid>https://arxiv.org/abs/2509.07300</guid>
<content:encoded><![CDATA[
arXiv:2509.07300v1 Announce Type: cross 
Abstract: Recent advances in neuroimaging analysis have enabled accurate decoding of mental state from brain activation patterns during functional magnetic resonance imaging scans. A commonly applied tool for this purpose is principal components regression regularized with the least absolute shrinkage and selection operator (LASSO PCR), a type of multi-voxel pattern analysis (MVPA). This model presumes that all components are equally likely to harbor relevant information, when in fact the task-related signal may be concentrated in specific components. In such cases, the model will fail to select the optimal set of principal components that maximizes the total signal relevant to the cognitive process under study. Here, we present modifications to LASSO PCR that allow for a regularization penalty tied directly to the index of the principal component, reflecting a prior belief that task-relevant signal is more likely to be concentrated in components explaining greater variance. Additionally, we propose a novel hybrid method, Joint Sparsity-Ranked LASSO (JSRL), which integrates component-level and voxel-level activity under an information parity framework and imposes ranked sparsity to guide component selection. We apply the models to brain activation during risk taking, monetary incentive, and emotion regulation tasks. Results demonstrate that incorporating sparsity ranking into LASSO PCR produces models with enhanced classification performance, with JSRL achieving up to 51.7\% improvement in cross-validated deviance $R^2$ and 7.3\% improvement in cross-validated AUC. Furthermore, sparsity-ranked models perform as well as or better than standard LASSO PCR approaches across all classification tasks and allocate predictive weight to brain regions consistent with their established functional roles, offering a robust alternative for MVPA.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Attention with Lookahead Keys</title>
<link>https://arxiv.org/abs/2509.07301</link>
<guid>https://arxiv.org/abs/2509.07301</guid>
<content:encoded><![CDATA[
arXiv:2509.07301v1 Announce Type: cross 
Abstract: In standard causal attention, each token's query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each token's keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive a mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on a range of downstream tasks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance-level Performance Prediction for Long-form Generation Tasks</title>
<link>https://arxiv.org/abs/2509.07309</link>
<guid>https://arxiv.org/abs/2509.07309</guid>
<content:encoded><![CDATA[
arXiv:2509.07309v1 Announce Type: cross 
Abstract: We motivate and share a new benchmark for instance-level performance prediction of long-form generation tasks having multi-faceted, fine-grained quality metrics. Our task-, model- and metric-agnostic formulation predicts continuous evaluation metric scores given only black-box model inputs and outputs. Beyond predicting point estimates of metric scores, the benchmark also requires inferring prediction intervals to quantify uncertainty around point estimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs, baselines, and metrics per task. We show that scores can be effectively predicted across long-form generation tasks using as few as 16 training examples. Overall, we introduce a novel and useful task, a valuable benchmark to drive progress, and baselines ready for practical adoption today.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Code Evolution Meets NP-Completeness</title>
<link>https://arxiv.org/abs/2509.07367</link>
<guid>https://arxiv.org/abs/2509.07367</guid>
<content:encoded><![CDATA[
arXiv:2509.07367v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently shown strong coding abilities, enabling not only static code generation but also iterative code self-evolving through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve} demonstrated that LLM-based coding agents can autonomously improve algorithms and surpass human experts, with scopes limited to isolated kernels spanning hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the first framework to extend LLM-based code evolution to the full repository scale, encompassing hundreds of files and tens of thousands of lines of C/C++ code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem and a cornerstone of both theory and applications. SATLUTION orchestrates LLM agents to directly evolve solver repositories under strict correctness guarantees and distributed runtime feedback, while simultaneously self-evolving its own evolution policies and rules. Starting from SAT Competition 2024 codebases and benchmark, SATLUTION evolved solvers that decisively outperformed the human-designed winners of the SAT Competition 2025, and also surpassed both 2024 and 2025 champions on the 2024 benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents</title>
<link>https://arxiv.org/abs/2509.07389</link>
<guid>https://arxiv.org/abs/2509.07389</guid>
<content:encoded><![CDATA[
arXiv:2509.07389v1 Announce Type: cross 
Abstract: Existing evaluation studies on linguistic competence of large language models (LLM agents) have focused primarily on vocabulary learning, morphological rule induction, syntactic generalization, pragmatic inference, and cross-linguistic transfer. However, none assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, a central feature of human language acquisition. We propose a novel experimental framework in which an LLM agent is evaluated on its ability to acquire and use a newly constructed language (Tinkatongue) in conversation with a bot that understands only Tinkatongue. Our findings show that LLM agents fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning. The results suggest a new direction for evaluation benchmarks and open pathways to model designs that learn more effectively from interactive feedback.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning for online hyperparameter tuning in convex quadratic programming</title>
<link>https://arxiv.org/abs/2509.07404</link>
<guid>https://arxiv.org/abs/2509.07404</guid>
<content:encoded><![CDATA[
arXiv:2509.07404v1 Announce Type: cross 
Abstract: Quadratic programming is a workhorse of modern nonlinear optimization, control, and data science. Although regularized methods offer convergence guarantees under minimal assumptions on the problem data, they can exhibit the slow tail-convergence typical of first-order schemes, thus requiring many iterations to achieve high-accuracy solutions. Moreover, hyperparameter tuning significantly impacts on the solver performance but how to find an appropriate parameter configuration remains an elusive research question. To address these issues, we explore how data-driven approaches can accelerate the solution process. Aiming at high-accuracy solutions, we focus on a stabilized interior-point solver and carefully handle its two-loop flow and control parameters. We will show that reinforcement learning can make a significant contribution to facilitating the solver tuning and to speeding up the optimization process. Numerical experiments demonstrate that, after a lightweight training, the learned policy generalizes well to different problem classes with varying dimensions and to various solver configurations.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data Generation with Lorenzetti for Time Series Anomaly Detection in High-Energy Physics Calorimeters</title>
<link>https://arxiv.org/abs/2509.07451</link>
<guid>https://arxiv.org/abs/2509.07451</guid>
<content:encoded><![CDATA[
arXiv:2509.07451v1 Announce Type: cross 
Abstract: Anomaly detection in multivariate time series is crucial to ensure the quality of data coming from a physics experiment. Accurately identifying the moments when unexpected errors or defects occur is essential, yet challenging due to scarce labels, unknown anomaly types, and complex correlations across dimensions. To address the scarcity and unreliability of labelled data, we use the Lorenzetti Simulator to generate synthetic events with injected calorimeter anomalies. We then assess the sensitivity of several time series anomaly detection methods, including transformer-based and other deep learning models. The approach employed here is generic and applicable to different detector designs and defects.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification</title>
<link>https://arxiv.org/abs/2509.07477</link>
<guid>https://arxiv.org/abs/2509.07477</guid>
<content:encoded><![CDATA[
arXiv:2509.07477v1 Announce Type: cross 
Abstract: Deep neural networks excel in radiological image classification but frequently suffer from poor interpretability, limiting clinical acceptance. We present MedicalPatchNet, an inherently self-explainable architecture for chest X-ray classification that transparently attributes decisions to distinct image regions. MedicalPatchNet splits images into non-overlapping patches, independently classifies each patch, and aggregates predictions, enabling intuitive visualization of each patch's diagnostic contribution without post-hoc techniques. Trained on the CheXpert dataset (223,414 images), MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908) of EfficientNet-B0, while substantially improving interpretability: MedicalPatchNet demonstrates substantially improved interpretability with higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks associated with shortcut learning, thus improving clinical trust. Our model is publicly available with reproducible training and inference scripts and contributes to safer, explainable AI-assisted diagnostics across medical imaging domains. We make the code publicly available: https://github.com/TruhnLab/MedicalPatchNet
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RINO: Renormalization Group Invariance with No Labels</title>
<link>https://arxiv.org/abs/2509.07486</link>
<guid>https://arxiv.org/abs/2509.07486</guid>
<content:encoded><![CDATA[
arXiv:2509.07486v1 Announce Type: cross 
Abstract: A common challenge with supervised machine learning (ML) in high energy physics (HEP) is the reliance on simulations for labeled data, which can often mismodel the underlying collision or detector response. To help mitigate this problem of domain shift, we propose RINO (Renormalization Group Invariance with No Labels), a self-supervised learning approach that can instead pretrain models directly on collision data, learning embeddings invariant to renormalization group flow scales. In this work, we pretrain a transformer-based model on jets originating from quantum chromodynamic (QCD) interactions from the JetClass dataset, emulating real QCD-dominated experimental data, and then finetune on the JetNet dataset -- emulating simulations -- for the task of identifying jets originating from top quark decays. RINO demonstrates improved generalization from the JetNet training data to JetClass data compared to supervised training on JetNet from scratch, demonstrating the potential for RINO pretraining on real collision data followed by fine-tuning on small, high-quality MC datasets, to improve the robustness of ML models in HEP.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Astra: A Multi-Agent System for GPU Kernel Performance Optimization</title>
<link>https://arxiv.org/abs/2509.07506</link>
<guid>https://arxiv.org/abs/2509.07506</guid>
<content:encoded><![CDATA[
arXiv:2509.07506v1 Announce Type: cross 
Abstract: GPU kernel optimization has long been a central challenge at the intersection of high-performance computing and machine learning. Efficient kernels are crucial for accelerating large language model (LLM) training and serving, yet attaining high performance typically requires extensive manual tuning. Compiler-based systems reduce some of this burden, but still demand substantial manual design and engineering effort. Recently, researchers have explored using LLMs for GPU kernel generation, though prior work has largely focused on translating high-level PyTorch modules into CUDA code. In this work, we introduce Astra, the first LLM-based multi-agent system for GPU kernel optimization. Unlike previous approaches, Astra starts from existing CUDA implementations extracted from SGLang, a widely deployed framework for serving LLMs, rather than treating PyTorch modules as the specification. Within Astra, specialized LLM agents collaborate through iterative code generation, testing, profiling, and planning to produce kernels that are both correct and high-performance. On kernels from SGLang, Astra achieves an average speedup of 1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study further demonstrates that LLMs can autonomously apply loop transformations, optimize memory access patterns, exploit CUDA intrinsics, and leverage fast math operations to yield substantial performance gains. Our work highlights multi-agent LLM systems as a promising new paradigm for GPU kernel optimization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data</title>
<link>https://arxiv.org/abs/2509.07526</link>
<guid>https://arxiv.org/abs/2509.07526</guid>
<content:encoded><![CDATA[
arXiv:2509.07526v1 Announce Type: cross 
Abstract: Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored -- despite audio's centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably small amount of public audio data -- less than 30K hours (5K unique) -- Falcon3-Audio-7B matches the best reported performance among open-weight models on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while distinguishing itself through superior data and parameter efficiency, single-stage training, and transparency. Notably, our smallest 1B model remains competitive with larger open models ranging from 2B to 13B parameters. Through extensive ablations, we find that common complexities -- such as curriculum learning, multiple audio encoders, and intricate cross-attention connectors -- are not required for strong performance, even compared to models trained on over 500K hours of data.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Gossip Algorithms for Rank-Based Statistical Methods</title>
<link>https://arxiv.org/abs/2509.07543</link>
<guid>https://arxiv.org/abs/2509.07543</guid>
<content:encoded><![CDATA[
arXiv:2509.07543v1 Announce Type: cross 
Abstract: As decentralized AI and edge intelligence become increasingly prevalent, ensuring robustness and trustworthiness in such distributed settings has become a critical issue-especially in the presence of corrupted or adversarial data. Traditional decentralized algorithms are vulnerable to data contamination as they typically rely on simple statistics (e.g., means or sum), motivating the need for more robust statistics. In line with recent work on decentralized estimation of trimmed means and ranks, we develop gossip algorithms for computing a broad class of rank-based statistics, including L-statistics and rank statistics-both known for their robustness to outliers. We apply our method to perform robust distributed two-sample hypothesis testing, introducing the first gossip algorithm for Wilcoxon rank-sum tests. We provide rigorous convergence guarantees, including the first convergence rate bound for asynchronous gossip-based rank estimation. We empirically validate our theoretical results through experiments on diverse network topologies.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring System Adaptations For Minimum Latency Real-Time Piano Transcription</title>
<link>https://arxiv.org/abs/2509.07586</link>
<guid>https://arxiv.org/abs/2509.07586</guid>
<content:encoded><![CDATA[
arXiv:2509.07586v1 Announce Type: cross 
Abstract: Advances in neural network design and the availability of large-scale labeled datasets have driven major improvements in piano transcription. Existing approaches target either offline applications, with no restrictions on computational demands, or online transcription, with delays of 128-320 ms. However, most real-time musical applications require latencies below 30 ms. In this work, we investigate whether and how the current state-of-the-art online transcription model can be adapted for real-time piano transcription. Specifically, we eliminate all non-causal processing, and reduce computational load through shared computations across core model components and variations in model size. Additionally, we explore different pre- and postprocessing strategies, and related label encoding schemes, and discuss their suitability for real-time transcription. Evaluating the adaptions on the MAESTRO dataset, we find a drop in transcription accuracy due to strictly causal processing as well as a tradeoff between the preprocessing latency and prediction accuracy. We release our system as a baseline to support researchers in designing models towards minimum latency real-time transcription.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Proxies for Sound Synthesizers: Learning Perceptually Informed Preset Representations</title>
<link>https://arxiv.org/abs/2509.07635</link>
<guid>https://arxiv.org/abs/2509.07635</guid>
<content:encoded><![CDATA[
arXiv:2509.07635v1 Announce Type: cross 
Abstract: Deep learning appears as an appealing solution for Automatic Synthesizer Programming (ASP), which aims to assist musicians and sound designers in programming sound synthesizers. However, integrating software synthesizers into training pipelines is challenging due to their potential non-differentiability. This work tackles this challenge by introducing a method to approximate arbitrary synthesizers. Specifically, we train a neural network to map synthesizer presets onto an audio embedding space derived from a pretrained model. This facilitates the definition of a neural proxy that produces compact yet effective representations, thereby enabling the integration of audio embedding loss into neural-based ASP systems for black-box synthesizers. We evaluate the representations derived by various pretrained audio models in the context of neural-based nASP and assess the effectiveness of several neural network architectures, including feedforward, recurrent, and transformer-based models, in defining neural proxies. We evaluate the proposed method using both synthetic and hand-crafted presets from three popular software synthesizers and assess its performance in a synthesizer sound matching downstream task. While the benefits of the learned representation are nuanced by resource requirements, encouraging results were obtained for all synthesizers, paving the way for future research into the application of synthesizer proxies for neural-based ASP systems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearest Neighbor Projection Removal Adversarial Training</title>
<link>https://arxiv.org/abs/2509.07673</link>
<guid>https://arxiv.org/abs/2509.07673</guid>
<content:encoded><![CDATA[
arXiv:2509.07673v1 Announce Type: cross 
Abstract: Deep neural networks have exhibited impressive performance in image classification tasks but remain vulnerable to adversarial examples. Standard adversarial training enhances robustness but typically fails to explicitly address inter-class feature overlap, a significant contributor to adversarial susceptibility. In this work, we introduce a novel adversarial training framework that actively mitigates inter-class proximity by projecting out inter-class dependencies from adversarial and clean samples in the feature space. Specifically, our approach first identifies the nearest inter-class neighbors for each adversarial sample and subsequently removes projections onto these neighbors to enforce stronger feature separability. Theoretically, we demonstrate that our proposed logits correction reduces the Lipschitz constant of neural networks, thereby lowering the Rademacher complexity, which directly contributes to improved generalization and robustness. Extensive experiments across standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that our method demonstrates strong performance that is competitive with leading adversarial training techniques, highlighting significant achievements in both robust and clean accuracy. Our findings reveal the importance of addressing inter-class feature proximity explicitly to bolster adversarial robustness in DNNs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAViAR: Critic-Augmented Video Agentic Reasoning</title>
<link>https://arxiv.org/abs/2509.07680</link>
<guid>https://arxiv.org/abs/2509.07680</guid>
<content:encoded><![CDATA[
arXiv:2509.07680v1 Announce Type: cross 
Abstract: Video understanding has seen significant progress in recent years, with models' performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building causation links in stochastic nonlinear systems from data</title>
<link>https://arxiv.org/abs/2509.07701</link>
<guid>https://arxiv.org/abs/2509.07701</guid>
<content:encoded><![CDATA[
arXiv:2509.07701v1 Announce Type: cross 
Abstract: Causal relationships play a fundamental role in understanding the world around us. The ability to identify and understand cause-effect relationships is critical to making informed decisions, predicting outcomes, and developing effective strategies. However, deciphering causal relationships from observational data is a difficult task, as correlations alone may not provide definitive evidence of causality. In recent years, the field of machine learning (ML) has emerged as a powerful tool, offering new opportunities for uncovering hidden causal mechanisms and better understanding complex systems. In this work, we address the issue of detecting the intrinsic causal links of a large class of complex systems in the framework of the response theory in physics. We develop some theoretical ideas put forward by [1], and technically we use state-of-the-art ML techniques to build up models from data. We consider both linear stochastic and non-linear systems. Finally, we compute the asymptotic efficiency of the linear response based causal predictor in a case of large scale Markov process network of linear interactions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis</title>
<link>https://arxiv.org/abs/2509.07723</link>
<guid>https://arxiv.org/abs/2509.07723</guid>
<content:encoded><![CDATA[
arXiv:2509.07723v1 Announce Type: cross 
Abstract: Background: Parkinson's disease remains a major neurodegenerative disorder with high misdiagnosis rates, primarily due to reliance on clinical rating scales. Recent studies have demonstrated a strong association between gut microbiota and Parkinson's disease, suggesting that microbial composition may serve as a promising biomarker. Although deep learning models based ongut microbiota show potential for early prediction, most approaches rely on single classifiers and often overlook inter-strain correlations or temporal dynamics. Therefore, there is an urgent need for more robust feature extraction methods tailored to microbiome data. Methods: We proposed BDPM (A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis). First, we collected gut microbiota profiles from 39 Parkinson's patients and their healthy spouses to identify differentially abundant taxa. Second, we developed an innovative feature selection framework named RFRE (Random Forest combined with Recursive Feature Elimination), integrating ecological knowledge to enhance biological interpretability. Finally, we designed a hybrid classification model to capture temporal and spatial patterns in microbiome data.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral and Rhythm Feature Performance Evaluation for Category and Class Level Audio Classification with Deep Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2509.07756</link>
<guid>https://arxiv.org/abs/2509.07756</guid>
<content:encoded><![CDATA[
arXiv:2509.07756v1 Announce Type: cross 
Abstract: Next to decision tree and k-nearest neighbours algorithms deep convolutional neural networks (CNNs) are widely used to classify audio data in many domains like music, speech or environmental sounds. To train a specific CNN various spectral and rhythm features like mel-scaled spectrograms, mel-frequency cepstral coefficients (MFCC), cyclic tempograms, short-time Fourier transform (STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy normalized statistics (CENS) chromagrams can be used as digital image input data for the neural network. The performance of these spectral and rhythm features for audio category level as well as audio class level classification is investigated in detail with a deep CNN and the ESC-50 dataset with 2,000 labeled environmental audio recordings using an end-to-end deep learning pipeline. The evaluated metrics accuracy, precision, recall and F1 score for multiclass classification clearly show that the mel-scaled spectrograms and the mel-frequency cepstral coefficients (MFCC) perform significantly better then the other spectral and rhythm features investigated in this research for audio classification tasks using deep CNNs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering</title>
<link>https://arxiv.org/abs/2509.07766</link>
<guid>https://arxiv.org/abs/2509.07766</guid>
<content:encoded><![CDATA[
arXiv:2509.07766v1 Announce Type: cross 
Abstract: Clustering financial assets based on return correlations is a fundamental task in portfolio optimization and statistical arbitrage. However, classical clustering methods often fall short when dealing with signed correlation structures, typically requiring lossy transformations and heuristic assumptions such as a fixed number of clusters. In this work, we apply the Graph-based Coalition Structure Generation algorithm (GCS-Q) to directly cluster signed, weighted graphs without relying on such transformations. GCS-Q formulates each partitioning step as a QUBO problem, enabling it to leverage quantum annealing for efficient exploration of exponentially large solution spaces. We validate our approach on both synthetic and real-world financial data, benchmarking against state-of-the-art classical algorithms such as SPONGE and k-Medoids. Our experiments demonstrate that GCS-Q consistently achieves higher clustering quality, as measured by Adjusted Rand Index and structural balance penalties, while dynamically determining the number of clusters. These results highlight the practical utility of near-term quantum computing for graph-based unsupervised learning in financial applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Computing for Large-scale Network Optimization: Opportunities and Challenges</title>
<link>https://arxiv.org/abs/2509.07773</link>
<guid>https://arxiv.org/abs/2509.07773</guid>
<content:encoded><![CDATA[
arXiv:2509.07773v1 Announce Type: cross 
Abstract: The complexity of large-scale 6G-and-beyond networks demands innovative approaches for multi-objective optimization over vast search spaces, a task often intractable. Quantum computing (QC) emerges as a promising technology for efficient large-scale optimization. We present our vision of leveraging QC to tackle key classes of problems in future mobile networks. By analyzing and identifying common features, particularly their graph-centric representation, we propose a unified strategy involving QC algorithms. Specifically, we outline a methodology for optimization using quantum annealing as well as quantum reinforcement learning. Additionally, we discuss the main challenges that QC algorithms and hardware must overcome to effectively optimize future networks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Online Riemannian Optimization Beyond Hadamard Manifolds</title>
<link>https://arxiv.org/abs/2509.07779</link>
<guid>https://arxiv.org/abs/2509.07779</guid>
<content:encoded><![CDATA[
arXiv:2509.07779v1 Announce Type: cross 
Abstract: We study decentralized online Riemannian optimization over manifolds with possibly positive curvature, going beyond the Hadamard manifold setting. Decentralized optimization techniques rely on a consensus step that is well understood in Euclidean spaces because of their linearity. However, in positively curved Riemannian spaces, a main technical challenge is that geodesic distances may not induce a globally convex structure. In this work, we first analyze a curvature-aware Riemannian consensus step that enables a linear convergence beyond Hadamard manifolds. Building on this step, we establish a $O(\sqrt{T})$ regret bound for the decentralized online Riemannian gradient descent algorithm. Then, we investigate the two-point bandit feedback setup, where we employ computationally efficient gradient estimators using smoothing techniques, and we demonstrate the same $O(\sqrt{T})$ regret bound through the subconvexity analysis of smoothed objectives.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nuclear Data Adjustment for Nonlinear Applications in the OECD/NEA WPNCS SG14 Benchmark -- A Bayesian Inverse UQ-based Approach for Data Assimilation</title>
<link>https://arxiv.org/abs/2509.07790</link>
<guid>https://arxiv.org/abs/2509.07790</guid>
<content:encoded><![CDATA[
arXiv:2509.07790v1 Announce Type: cross 
Abstract: The Organization for Economic Cooperation and Development (OECD) Working Party on Nuclear Criticality Safety (WPNCS) proposed a benchmark exercise to assess the performance of current nuclear data adjustment techniques applied to nonlinear applications and experiments with low correlation to applications. This work introduces Bayesian Inverse Uncertainty Quantification (IUQ) as a method for nuclear data adjustments in this benchmark, and compares IUQ to the more traditional methods of Generalized Linear Least Squares (GLLS) and Monte Carlo Bayes (MOCABA). Posterior predictions from IUQ showed agreement with GLLS and MOCABA for linear applications. When comparing GLLS, MOCABA, and IUQ posterior predictions to computed model responses using adjusted parameters, we observe that GLLS predictions fail to replicate computed response distributions for nonlinear applications, while MOCABA shows near agreement, and IUQ uses computed model responses directly. We also discuss observations on why experiments with low correlation to applications can be informative to nuclear data adjustments and identify some properties useful in selecting experiments for inclusion in nuclear data adjustment. Performance in this benchmark indicates potential for Bayesian IUQ in nuclear data adjustments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost</title>
<link>https://arxiv.org/abs/2509.07829</link>
<guid>https://arxiv.org/abs/2509.07829</guid>
<content:encoded><![CDATA[
arXiv:2509.07829v1 Announce Type: cross 
Abstract: Literary translation has recently gained attention as a distinct and complex task in machine translation research. However, the translation by small open models remains an open problem. We contribute to this ongoing research by introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for dataset creation, fine tuning, and evaluation in English-Romanian literary translations, centred on the creation and open release of both a compact, fine tuned language model (TF2-12B) and large scale synthetic parallel datasets (DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the largest collection of synthetic English fables to date, we address the need for rich, high quality literary datasets in low resource languages such as Romanian. Our pipeline first generates 15k high quality Romanian references from the TF1 pool using a high performing LLM. We then apply a two stage fine tuning process to a 12B parameter open weight model: (i) instruction tuning to capture genre specific narrative style, and (ii) adapter compression for efficient deployment. Evaluation combines corpus level BLEU and a five dimension LLM based rubric (accuracy, fluency, coherence, style, cultural adaptation) to provide a nuanced assessment of translation quality. Results show that our fine tuned model achieves fluency and adequacy competitive with top performing large proprietary models, while being open, accessible, and significantly more cost effective. Alongside the fine tuned model and both datasets, we publicly release all scripts and evaluation prompts. TF2 thus provides an end-to-end, reproducible pipeline for research on cost efficient translation, cross lingual narrative generation, and the broad adoption of open models for culturally significant literary content in low resource settings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models</title>
<link>https://arxiv.org/abs/2509.07925</link>
<guid>https://arxiv.org/abs/2509.07925</guid>
<content:encoded><![CDATA[
arXiv:2509.07925v1 Announce Type: cross 
Abstract: Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s</title>
<link>https://arxiv.org/abs/2509.07928</link>
<guid>https://arxiv.org/abs/2509.07928</guid>
<content:encoded><![CDATA[
arXiv:2509.07928v1 Announce Type: cross 
Abstract: As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically achieved on high-power, desktop-class GPUs. This paper reveals that on resource-constrained systems, such as laptops with RTX 4060 GPUs, performance is not compute-bound but is instead dominated by system-level bottlenecks, as illustrated by a simple bottleneck test. To overcome this hardware-level constraint, we introduce a Two-Pass Adaptive Inference algorithm, a model-independent approach that requires no architectural changes. This study mainly focuses on adaptive inference strategies and undertakes a comparative analysis of architectural early-exit and resolution-adaptive routing, highlighting their respective trade-offs within a unified evaluation framework. The system uses a fast, low-resolution pass and only escalates to a high-resolution model pass when detection confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%. This work provides a practical and reproducible blueprint for deploying high-performance, real-time AI on consumer-grade devices by shifting the focus from pure model optimization to hardware-aware inference strategies that maximize throughput.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at DoorDash</title>
<link>https://arxiv.org/abs/2509.07929</link>
<guid>https://arxiv.org/abs/2509.07929</guid>
<content:encoded><![CDATA[
arXiv:2509.07929v1 Announce Type: cross 
Abstract: We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds upon the industry standard Fast Finish (FF) feature in budget pacing systems that depletes remaining advertising budget as quickly as possible towards the end of some fixed time period. SFF dynamically updates system parameters such as start time and throttle rate depending on historical ad-campaign data. SFF is currently in use at DoorDash, one of the largest delivery platforms in the US, and is part of its budget pacing system. We show via online budget-split experimentation data and offline simulations that SFF is a robust solution for overdelivery mitigation when pacing budget.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees</title>
<link>https://arxiv.org/abs/2509.07939</link>
<guid>https://arxiv.org/abs/2509.07939</guid>
<content:encoded><![CDATA[
arXiv:2509.07939v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have driven interest in automating cybersecurity penetration testing workflows, offering the promise of faster and more consistent vulnerability assessment for enterprise systems. Existing LLM agents for penetration testing primarily rely on self-guided reasoning, which can produce inaccurate or hallucinated procedural steps. As a result, the LLM agent may undertake unproductive actions, such as exploiting unused software libraries or generating cyclical responses that repeat prior tactics. In this work, we propose a guided reasoning pipeline for penetration testing LLM agents that incorporates a deterministic task tree built from the MITRE ATT&amp;CK Matrix, a proven penetration testing kll chain, to constrain the LLM's reaoning process to explicitly defined tactics, techniques, and procedures. This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Our proposed reasoning pipeline guided the LLM agent through 71.8\%, 72.8\%, and 78.6\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively. Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and required 86.2\%, 118.7\%, and 205.9\% more model queries. This suggests that incorporating a deterministic task tree into LLM reasoning pipelines can enhance the accuracy and efficiency of automated cybersecurity assessments
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RaC: Robot Learning for Long-Horizon Tasks by Scaling Recovery and Correction</title>
<link>https://arxiv.org/abs/2509.07953</link>
<guid>https://arxiv.org/abs/2509.07953</guid>
<content:encoded><![CDATA[
arXiv:2509.07953v1 Announce Type: cross 
Abstract: Modern paradigms for robot imitation train expressive policy architectures on large amounts of human demonstration data. Yet performance on contact-rich, deformable-object, and long-horizon tasks plateau far below perfect execution, even with thousands of expert demonstrations. This is due to the inefficiency of existing ``expert'' data collection procedures based on human teleoperation. To address this issue, we introduce RaC, a new phase of training on human-in-the-loop rollouts after imitation learning pre-training. In RaC, we fine-tune a robotic policy on human intervention trajectories that illustrate recovery and correction behaviors. Specifically, during a policy rollout, human operators intervene when failure appears imminent, first rewinding the robot back to a familiar, in-distribution state and then providing a corrective segment that completes the current sub-task. Training on this data composition expands the robotic skill repertoire to include retry and adaptation behaviors, which we show are crucial for boosting both efficiency and robustness on long-horizon tasks. Across three real-world bimanual control tasks: shirt hanging, airtight container lid sealing, takeout box packing, and a simulated assembly task, RaC outperforms the prior state-of-the-art using 10$\times$ less data collection time and samples. We also show that RaC enables test-time scaling: the performance of the trained RaC policy scales linearly in the number of recovery maneuvers it exhibits. Videos of the learned policy are available at https://rac-scaling-robot.github.io/.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning of Piecewise Gaussian Process Surrogates</title>
<link>https://arxiv.org/abs/2301.08789</link>
<guid>https://arxiv.org/abs/2301.08789</guid>
<content:encoded><![CDATA[
arXiv:2301.08789v4 Announce Type: replace 
Abstract: Active learning of Gaussian process (GP) surrogates has been useful for optimizing experimental designs for physical/computer simulation experiments, and for steering data acquisition schemes in machine learning. In this paper, we develop a method for active learning of piecewise, Jump GP surrogates. Jump GPs are continuous within, but discontinuous across, regions of a design space, as required for applications spanning autonomous materials design, configuration of smart factory systems, and many others. Although our active learning heuristics are appropriated from strategies originally designed for ordinary GPs, we demonstrate that additionally accounting for model bias, as opposed to the usual model uncertainty, is essential in the Jump GP context. Toward that end, we develop an estimator for bias and variance of Jump GP models. Illustrations, and evidence of the advantage of our proposed methods, are provided on a suite of synthetic benchmarks, and real-simulation experiments of varying complexity.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FilterFL: Knowledge Filtering-based Data-Free Backdoor Defense for Federated Learning</title>
<link>https://arxiv.org/abs/2308.11333</link>
<guid>https://arxiv.org/abs/2308.11333</guid>
<content:encoded><![CDATA[
arXiv:2308.11333v2 Announce Type: replace 
Abstract: As a distributed machine learning paradigm, Federated Learning (FL) enables large-scale clients to collaboratively train a model without sharing their raw data. However, due to the lack of data auditing for untrusted clients, FL is vulnerable to poisoning attacks, especially backdoor attacks. By using poisoned data for local training or directly changing the model parameters, attackers can easily inject backdoors into the model, which can trigger the model to make misclassification of targeted patterns in images. To address these issues, we propose a novel data-free trigger-generation-based defense approach based on the two characteristics of backdoor attacks: i) triggers are learned faster than normal knowledge, and ii) trigger patterns have a greater effect on image classification than normal class patterns. Our approach generates the images with newly learned knowledge by identifying the differences between the old and new global models, and filters trigger images by evaluating the effect of these generated images. By using these trigger images, our approach eliminates poisoned models to ensure the updated global model is benign. Comprehensive experiments demonstrate that our approach can defend against almost all the existing types of backdoor attacks and outperform all the seven state-of-the-art defense methods with both IID and non-IID scenarios. Especially, our approach can successfully defend against the backdoor attack even when 80\% of the clients are malicious.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Methods for Non-stationary Online Learning</title>
<link>https://arxiv.org/abs/2309.08911</link>
<guid>https://arxiv.org/abs/2309.08911</guid>
<content:encoded><![CDATA[
arXiv:2309.08911v3 Announce Type: replace 
Abstract: Non-stationary online learning has drawn much attention in recent years. In particular, dynamic regret and adaptive regret are proposed as two principled performance measures for online convex optimization in non-stationary environments. To optimize them, a two-layer online ensemble is usually deployed due to the inherent uncertainty of non-stationarity, in which multiple base-learners are maintained and a meta-algorithm is employed to track the best one on the fly. However, the two-layer structure raises concerns about computational complexity -- such methods typically maintain $O(\log T)$ base-learners simultaneously for a $T$-round online game and thus perform multiple projections onto the feasible domain per round, which becomes the computational bottleneck when the domain is complicated. In this paper, we present efficient methods for optimizing dynamic regret and adaptive regret that reduce the number of projections per round from $O(\log T)$ to $1$. The proposed algorithms require only one gradient query and one function evaluation at each round. Our technique hinges on the reduction mechanism developed in parameter-free online learning and requires non-trivial modifications for non-stationary online methods. Furthermore, we study an even stronger measure, namely "interval dynamic regret", and reduce the number of projections per round from $O(\log^2 T)$ to $1$ for minimizing it. Our reduction demonstrates broad generality and applies to two important applications: online stochastic control and online principal component analysis, resulting in methods that are both efficient and optimal. Finally, empirical studies verify our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Benefits of Public Representations for Private Transfer Learning under Distribution Shift</title>
<link>https://arxiv.org/abs/2312.15551</link>
<guid>https://arxiv.org/abs/2312.15551</guid>
<content:encoded><![CDATA[
arXiv:2312.15551v5 Announce Type: replace 
Abstract: Public pretraining is a promising approach to improve differentially private model training. However, recent work has noted that many positive research results studying this paradigm only consider in-distribution tasks, and may not apply to settings where there is distribution shift between the pretraining and finetuning data -- a scenario that is likely when finetuning private tasks due to the sensitive nature of the data. In this work, we show empirically across three tasks that even in settings with large distribution shift, where both zero-shot performance from public data and training from scratch with private data give unusably weak results, public features can in fact improve private training accuracy by up to 67\% over private training from scratch. We provide a theoretical explanation for this phenomenon, showing that if the public and private data share a low-dimensional representation, public representations can improve the sample complexity of private training even if it is impossible to learn the private task from the public data alone. Altogether, our results provide evidence that public data can indeed make private training practical in realistic settings of extreme distribution shift.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMMIT: Coordinated Multimodal Instruction Tuning</title>
<link>https://arxiv.org/abs/2407.20454</link>
<guid>https://arxiv.org/abs/2407.20454</guid>
<content:encoded><![CDATA[
arXiv:2407.20454v2 Announce Type: replace 
Abstract: Instruction tuning in multimodal large language models (MLLMs) generally involves cooperative learning between a backbone LLM and a feature encoder of non-text input modalities. The major challenge is how to efficiently find the synergy between the two modules so that LLMs can adapt their reasoning abilities to downstream tasks while feature encoders can adjust to provide more task-specific information about its modality. In this paper, we analyze the MLLM instruction tuning from both theoretical and empirical perspectives, where we find the unbalanced learning between the feature encoder and the LLM can cause problems of oscillation and biased learning that lead to sub-optimal convergence. Inspired by our findings, we propose a Multimodal Balance Coefficient that enables quantitative measurement of the balance of learning. Based on this, we further design a dynamic learning scheduler that better coordinates the learning between the LLM and feature encoder, alleviating the problems of oscillation and biased learning. In addition, we introduce an auxiliary regularization on the gradient to promote updating with larger step sizes, which potentially allows for a more accurate estimation of the proposed MultiModal Balance Coefficient and further improves the training sufficiency. Our proposed approach is agnostic to the architecture of LLM and feature encoder, so it can be generically integrated with various MLLMs. We conduct experiments on multiple downstream tasks with various MLLMs, demonstrating that the proposed method is more effective than the baselines in MLLM instruction tuning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Truly Massive Budgeted Monotonic POMDPs with Oracle-Guided Meta-Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.07192</link>
<guid>https://arxiv.org/abs/2408.07192</guid>
<content:encoded><![CDATA[
arXiv:2408.07192v2 Announce Type: replace 
Abstract: Monotonic Partially Observable Markov Decision Processes (POMDPs), where the system state progressively decreases until a restorative action is performed, can be used to model sequential repair problems effectively. This paper considers the problem of solving budget-constrained multi-component monotonic POMDPs, where a finite budget limits the maximal number of restorative actions. For a large number of components, solving such a POMDP using current methods is computationally intractable due to the exponential growth in the state space with an increasing number of components. To address this challenge, we propose a two-step approach. Since the individual components of a budget-constrained multi-component monotonic POMDP are only connected via the shared budget, we first approximate the optimal budget allocation among these components using an approximation of each component POMDP's optimal value function which is obtained through a random forest model. Subsequently, we introduce an oracle-guided meta-trained Proximal Policy Optimization (PPO) algorithm to solve each of the independent budget-constrained single-component monotonic POMDPs. The oracle policy is obtained by performing value iteration on the corresponding monotonic Markov Decision Process (MDP). This two-step method provides scalability in solving truly massive multi-component monotonic POMDPs. To demonstrate the efficacy of our approach, we consider a real-world maintenance scenario that involves inspection and repair of an administrative building by a team of agents within a maintenance budget. Finally, we perform a computational complexity analysis for a varying number of components to show the scalability of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioNeMo Framework: a modular, high-performance library for AI model development in drug discovery</title>
<link>https://arxiv.org/abs/2411.10548</link>
<guid>https://arxiv.org/abs/2411.10548</guid>
<content:encoded><![CDATA[
arXiv:2411.10548v5 Announce Type: replace 
Abstract: Artificial Intelligence models encoding biology and chemistry are opening new routes to high-throughput and high-quality in-silico drug development. However, their training increasingly relies on computational scale, with recent protein language models (pLM) training on hundreds of graphical processing units (GPUs). We introduce the BioNeMo Framework to facilitate the training of computational biology and chemistry AI models across hundreds of GPUs. Its modular design allows the integration of individual components, such as data loaders, into existing workflows and is open to community contributions. We detail technical features of the BioNeMo Framework through use cases such as pLM pre-training and fine-tuning. On 256 NVIDIA A100s, BioNeMo Framework trains a three billion parameter BERT-based pLM on over one trillion tokens in 4.2 days. The BioNeMo Framework is open-source and free for everyone to use.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid-Regularized Magnitude Pruning for Robust Federated Learning under Covariate Shift</title>
<link>https://arxiv.org/abs/2412.15010</link>
<guid>https://arxiv.org/abs/2412.15010</guid>
<content:encoded><![CDATA[
arXiv:2412.15010v2 Announce Type: replace 
Abstract: Federated Learning offers a solution for decentralised model training, addressing the difficulties associated with distributed data and privacy in machine learning. However, the fact of data heterogeneity in federated learning frequently hinders the global model's generalisation, leading to low performance and adaptability to unseen data. This problem is particularly critical for specialised applications such as medical imaging, where both the data and the number of clients are limited. In this paper, we empirically demonstrate that inconsistencies in client-side training distributions substantially degrade the performance of federated learning models across multiple benchmark datasets. We propose a novel FL framework using a combination of pruning and regularisation of clients' training to improve the sparsity, redundancy, and robustness of neural connections, and thereby the resilience to model aggregation. To address a relatively unexplored dimension of data heterogeneity, we further introduce a novel benchmark dataset, CelebA-Gender, specifically designed to control for within-class distributional shifts across clients based on attribute variations, thereby complementing the predominant focus on inter-class imbalance in prior federated learning research. Comprehensive experiments on many datasets like CIFAR-10, MNIST, and the newly introduced CelebA-Gender dataset demonstrate that our method consistently outperforms standard FL baselines, yielding more robust and generalizable models in heterogeneous settings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Do Neural Networks Learn World Models?</title>
<link>https://arxiv.org/abs/2502.09297</link>
<guid>https://arxiv.org/abs/2502.09297</guid>
<content:encoded><![CDATA[
arXiv:2502.09297v5 Announce Type: replace 
Abstract: Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we present the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions--even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive MIM: A Contrastive Mutual Information Framework for Unified Generative and Discriminative Representation Learning</title>
<link>https://arxiv.org/abs/2502.19642</link>
<guid>https://arxiv.org/abs/2502.19642</guid>
<content:encoded><![CDATA[
arXiv:2502.19642v2 Announce Type: replace 
Abstract: Learning representations that generalize well to unknown downstream tasks is a central challenge in representation learning. Existing approaches such as contrastive learning, self-supervised masking, and denoising auto-encoders address this challenge with varying trade-offs. In this paper, we introduce the {contrastive Mutual Information Machine} (cMIM), a probabilistic framework that augments the Mutual Information Machine (MIM) with a novel contrastive objective. While MIM maximizes mutual information between inputs and latent variables and encourages clustering of latent codes, its representations underperform on discriminative tasks compared to state-of-the-art alternatives. cMIM addresses this limitation by enforcing global discriminative structure while retaining MIM's generative strengths. We present two main contributions: (1) we propose cMIM, a contrastive extension of MIM that eliminates the need for positive data augmentation and is robust to batch size, unlike InfoNCE-based methods; (2) we introduce {informative embeddings}, a general technique for extracting enriched representations from encoder--decoder models that substantially improve discriminative performance without additional training, and which apply broadly beyond MIM. Empirical results demonstrate that cMIM consistently outperforms MIM and InfoNCE in classification and regression tasks, while preserving comparable reconstruction quality. These findings suggest that cMIM provides a unified framework for learning representations that are simultaneously effective for discriminative and generative applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models</title>
<link>https://arxiv.org/abs/2503.12602</link>
<guid>https://arxiv.org/abs/2503.12602</guid>
<content:encoded><![CDATA[
arXiv:2503.12602v4 Announce Type: replace 
Abstract: Generative machine learning models for exploring chemical space have shown immense promise, but many molecules they generate are too difficult to synthesize, making them impractical for further investigation or development. In this work, we present a novel approach by fine-tuning Meta's Llama3 Large Language Models (LLMs) to create SynLlama, which generates full synthetic pathways made of commonly accessible building blocks and robust organic reaction templates. SynLlama explores a large synthesizable space using significantly less data, and offers strong performance in both forward and bottom-up synthesis planning compared to other state-of-the-art methods. We find that SynLlama, even without training on external building blocks, can effectively generalize to unseen yet purchasable building blocks, meaning that its reconstruction capabilities extend to a broader synthesizable chemical space than the training data. We also demonstrate the use of SynLlama in a pharmaceutical context for synthesis planning of analog molecules and hit expansion leads for proposed inhibitors of target proteins, offering medicinal chemists a valuable tool for discovery.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Efficient Direct Analytics on Semantic-aware Time Series Data Compression</title>
<link>https://arxiv.org/abs/2503.13246</link>
<guid>https://arxiv.org/abs/2503.13246</guid>
<content:encoded><![CDATA[
arXiv:2503.13246v2 Announce Type: replace 
Abstract: Semantic communication has emerged as a promising paradigm to tackle the challenges of massive growing data traffic and sustainable data communication. It shifts the focus from data fidelity to goal-oriented or task-oriented semantic transmission. While deep learning-based methods are commonly used for semantic encoding and decoding, they struggle with the sequential nature of time series data and high computation cost, particularly in resource-constrained IoT environments. Data compression plays a crucial role in reducing transmission and storage costs, yet traditional data compression methods fall short of the demands of goal-oriented communication systems. In this paper, we propose a novel method for direct analytics on time series data compressed by the SHRINK compression algorithm. Through experimentation using outlier detection as a case study, we show that our method outperforms baselines running on uncompressed data in multiple cases, with merely 1% difference in the worst case. Additionally, it achieves four times lower runtime on average and accesses approximately 10% of the data volume, which enables edge analytics with limited storage and computation power. These results demonstrate that our approach offers reliable, high-speed outlier detection analytics for diverse IoT applications while extracting semantics from time-series data, achieving high compression, and reducing data transmission.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tripartite-GraphRAG via Plugin Ontologies</title>
<link>https://arxiv.org/abs/2504.19667</link>
<guid>https://arxiv.org/abs/2504.19667</guid>
<content:encoded><![CDATA[
arXiv:2504.19667v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities across various domains, yet they struggle with knowledge-intensive tasks in areas that demand factual accuracy, e.g. industrial automation and healthcare. Key limitations include their tendency to hallucinate, lack of source traceability (provenance), and challenges in timely knowledge updates. Combining language models with knowledge graphs (GraphRAG) offers promising avenues for overcoming these deficits. However, a major challenge lies in creating such a knowledge graph in the first place. Here, we propose a novel approach that combines LLMs with a tripartite knowledge graph representation, which is constructed by connecting complex, domain-specific objects via a curated ontology of corresponding, domain-specific concepts to relevant sections within chunks of text through a concept-anchored pre-analysis of source documents starting from an initial lexical graph. Subsequently, we formulate LLM prompt creation as an unsupervised node classification problem allowing for the optimization of information density, coverage, and arrangement of LLM prompts at significantly reduced lengths. An initial experimental evaluation of our approach on a healthcare use case, involving multi-faceted analyses of patient anamneses given a set of medical concepts as well as a series of clinical guideline literature, indicates its potential to optimize information density, coverage, and arrangement of LLM prompts while significantly reducing their lengths, which, in turn, may lead to reduced costs as well as more consistent and reliable LLM outputs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?</title>
<link>https://arxiv.org/abs/2505.02884</link>
<guid>https://arxiv.org/abs/2505.02884</guid>
<content:encoded><![CDATA[
arXiv:2505.02884v2 Announce Type: replace 
Abstract: Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overflow Prevention Enhances Long-Context Recurrent LLMs</title>
<link>https://arxiv.org/abs/2505.07793</link>
<guid>https://arxiv.org/abs/2505.07793</guid>
<content:encoded><![CDATA[
arXiv:2505.07793v2 Announce Type: replace 
Abstract: A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Autoregressive 3D Molecule Generation</title>
<link>https://arxiv.org/abs/2505.13791</link>
<guid>https://arxiv.org/abs/2505.13791</guid>
<content:encoded><![CDATA[
arXiv:2505.13791v2 Announce Type: replace 
Abstract: Generative models of 3D molecular structure play a rapidly growing role in the design and simulation of molecules. Diffusion models currently dominate the space of 3D molecule generation, while autoregressive models have trailed behind. In this work, we present Quetzal, a simple but scalable autoregressive model that builds molecules atom-by-atom in 3D. Treating each molecule as an ordered sequence of atoms, Quetzal combines a causal transformer that predicts the next atom's discrete type with a smaller Diffusion MLP that models the continuous next-position distribution. Compared to existing autoregressive baselines, Quetzal achieves substantial improvements in generation quality and is competitive with the performance of state-of-the-art diffusion models. In addition, by reducing the number of expensive forward passes through a dense transformer, Quetzal enables significantly faster generation speed, as well as exact divergence-based likelihood computation. Finally, without any architectural changes, Quetzal natively handles variable-size tasks like hydrogen decoration and scaffold completion. We hope that our work motivates a perspective on scalability and generality for generative modelling of 3D molecules.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments</title>
<link>https://arxiv.org/abs/2506.00563</link>
<guid>https://arxiv.org/abs/2506.00563</guid>
<content:encoded><![CDATA[
arXiv:2506.00563v2 Announce Type: replace 
Abstract: A key approach to state abstraction is approximating behavioral metrics (notably, bisimulation metrics) in the observation space and embedding these learned distances in the representation space. While promising for robustness to task-irrelevant noise, as shown in prior work, accurately estimating these metrics remains challenging, requiring various design choices that create gaps between theory and practice. Prior evaluations focus mainly on final returns, leaving the quality of learned metrics and the source of performance gains unclear. To systematically assess how metric learning works in deep reinforcement learning (RL), we evaluate five recent approaches, unified conceptually as isometric embeddings with varying design choices. We benchmark them with baselines across 20 state-based and 14 pixel-based tasks, spanning 370 task configurations with diverse noise settings. Beyond final returns, we introduce the evaluation of a denoising factor to quantify the encoder's ability to filter distractions. To further isolate the effect of metric learning, we propose and evaluate an isolated metric estimation setting, in which the encoder is influenced solely by the metric loss. Finally, we release an open-source, modular codebase to improve reproducibility and support future research on metric learning in deep RL.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closing the Gap between TD Learning and Supervised Learning with $Q$-Conditioned Maximization</title>
<link>https://arxiv.org/abs/2506.00795</link>
<guid>https://arxiv.org/abs/2506.00795</guid>
<content:encoded><![CDATA[
arXiv:2506.00795v2 Announce Type: replace 
Abstract: Recently, supervised learning (SL) methodology has emerged as an effective approach for offline reinforcement learning (RL) due to their simplicity, stability, and efficiency. However, recent studies show that SL methods lack the trajectory stitching capability, typically associated with temporal difference (TD)-based approaches. A question naturally surfaces: \textit{How can we endow SL methods with stitching capability and close its performance gap with TD learning?} To answer this question, we introduce $Q$-conditioned maximization supervised learning for offline goal-conditioned RL, which enhances SL with the stitching capability through $Q$-conditioned policy and $Q$-conditioned maximization. Concretely, we propose \textbf{G}oal-\textbf{C}onditioned \textbf{\textit{Rein}}forced \textbf{S}upervised \textbf{L}earning (\textbf{GC\textit{Rein}SL}), which consists of (1) estimating the $Q$-function by Normalizing Flows from the offline dataset and (2) finding the maximum $Q$-value within the data support by integrating $Q$-function maximization with Expectile Regression. In inference time, our policy chooses optimal actions based on such a maximum $Q$-value. Experimental results from stitching evaluations on offline RL datasets demonstrate that our method outperforms prior SL approaches with stitching capabilities and goal data augmentation techniques.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating High Dimensional Concept Space with Metalearning</title>
<link>https://arxiv.org/abs/2508.01948</link>
<guid>https://arxiv.org/abs/2508.01948</guid>
<content:encoded><![CDATA[
arXiv:2508.01948v2 Announce Type: replace 
Abstract: Rapidly learning abstract concepts from limited examples is a hallmark of human intelligence. This work investigates whether gradient-based meta-learning can equip neural networks with inductive biases for efficient few-shot acquisition of discrete concepts. I compare meta-learning methods against a supervised learning baseline on Boolean concepts (logical statements) generated by a probabilistic context-free grammar (PCFG). By systematically varying concept dimensionality (number of features) and recursive compositionality (depth of grammar recursion), I delineate between complexity regimes in which meta-learning robustly improves few-shot concept learning and regimes in which it does not. Meta-learners are much better able to handle compositional complexity than featural complexity. I highlight some reasons for this with a representational analysis of the weights of meta-learners and a loss landscape analysis demonstrating how featural complexity increases the roughness of loss trajectories, allowing curvature-aware optimization to be more effective than first-order methods. I find improvements in out-of-distribution generalization on complex concepts by increasing the number of adaptation steps in meta-SGD, where adaptation acts as a way of encouraging exploration of rougher loss basins. Overall, this work highlights the intricacies of learning compositional versus featural complexity in high dimensional concept spaces and provides a road to understanding the role of 2nd order methods and extended gradient adaptation in few-shot concept learning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer</title>
<link>https://arxiv.org/abs/2508.10587</link>
<guid>https://arxiv.org/abs/2508.10587</guid>
<content:encoded><![CDATA[
arXiv:2508.10587v2 Announce Type: replace 
Abstract: To bridge the temporal granularity gap in energy network design and operation based on Energy System Models, resampling of time series is required. While conventional upsampling methods are computationally efficient, they often result in significant information loss or increased noise. Advanced models such as time series generation models, Super-Resolution models and imputation models show potential, but also face fundamental challenges. The goal of time series generative models is to learn the distribution of the original data to generate high-resolution series with similar statistical characteristics. This is not entirely consistent with the definition of upsampling. Time series Super-Resolution models or imputation models can degrade the accuracy of upsampling because the input low-resolution time series are sparse and may have insufficient context. Moreover, such models usually rely on supervised learning paradigms. This presents a fundamental application paradox: their training requires the high-resolution time series that is intrinsically absent in upsampling application scenarios. To address the mentioned upsampling issue, this paper introduces a new method utilizing Generative Adversarial Transformers (GATs), which can be trained without access to any ground-truth high-resolution data. Compared with conventional interpolation methods, the introduced method can reduce the root mean square error (RMSE) of upsampling tasks by 9%, and the accuracy of a model predictive control (MPC) application scenario is improved by 13%.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Self-Supervised Acoustic Pre-Training with Local Constraints</title>
<link>https://arxiv.org/abs/2508.19990</link>
<guid>https://arxiv.org/abs/2508.19990</guid>
<content:encoded><![CDATA[
arXiv:2508.19990v2 Announce Type: replace 
Abstract: Self-supervised pre-training using unlabeled data is widely used in automatic speech recognition. In this paper, we propose a new self-supervised pre-training approach to dealing with heterogeneous data. Instead of mixing all the data and minimizing the averaged global loss in the conventional way, we impose additional local constraints to ensure that the model optimizes each source of heterogeneous data to its local optimum after $K$-step gradient descent initialized from the model. We formulate this as a bilevel optimization problem, and use the first-order approximation method to solve the problem. We discuss its connection to model-agnostic meta learning. Experiments are carried out on self-supervised pre-training using multi-domain and multilingual datasets, demonstrating that the proposed approach can significantly improve the adaptivity of the self-supervised pre-trained model for the downstream supervised fine-tuning tasks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using item recommendations and LLMs in marketing email titles</title>
<link>https://arxiv.org/abs/2508.20024</link>
<guid>https://arxiv.org/abs/2508.20024</guid>
<content:encoded><![CDATA[
arXiv:2508.20024v2 Announce Type: replace 
Abstract: E-commerce marketplaces make use of a number of marketing channels like emails, push notifications, etc. to reach their users and stimulate purchases. Personalized emails especially are a popular touch point for marketers to inform users of latest items in stock, especially for those who stopped visiting the marketplace. Such emails contain personalized recommendations tailored to each user's interests, enticing users to buy relevant items. A common limitation of these emails is that the primary entry point, the title of the email, tends to follow fixed templates, failing to inspire enough interest in the contents. In this work, we explore the potential of large language models (LLMs) for generating thematic titles that reflect the personalized content of the emails. We perform offline simulations and conduct online experiments on the order of millions of users, finding our techniques useful in improving the engagement between customers and our emails. We highlight key findings and learnings as we productionize the safe and automated generation of email titles for millions of users.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive LLM Routing under Budget Constraints</title>
<link>https://arxiv.org/abs/2508.21141</link>
<guid>https://arxiv.org/abs/2508.21141</guid>
<content:encoded><![CDATA[
arXiv:2508.21141v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing, but their varying capabilities and costs pose challenges in practical applications. LLM routing addresses this by dynamically selecting the most suitable LLM for each query/task. Previous approaches treat this as a supervised learning problem, assuming complete knowledge of optimal query-LLM pairings. However, real-world scenarios lack such comprehensive mappings and face evolving user queries. We thus propose to study LLM routing as a contextual bandit problem, enabling adaptive decision-making using bandit feedback without requiring exhaustive inference across all LLMs for all queries (in contrast to supervised routing). To address this problem, we develop a shared embedding space for queries and LLMs, where query and LLM embeddings are aligned to reflect their affinity. This space is initially learned from offline human preference data and refined through online bandit feedback. We instantiate this idea through Preference-prior Informed Linucb fOr adaptive rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets for model routing, we introduce an online cost policy modeled as a multi-choice knapsack problem, ensuring resource-efficient routing.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant U-Shaped Neural Operators for the Cahn-Hilliard Phase-Field Model</title>
<link>https://arxiv.org/abs/2509.01293</link>
<guid>https://arxiv.org/abs/2509.01293</guid>
<content:encoded><![CDATA[
arXiv:2509.01293v3 Announce Type: replace 
Abstract: Phase separation in binary mixtures, governed by the Cahn-Hilliard equation, plays a central role in interfacial dynamics across materials science and soft matter. While numerical solvers are accurate, they are often computationally expensive and lack flexibility across varying initial conditions and geometries. Neural operators provide a data-driven alternative by learning solution operators between function spaces, but current architectures often fail to capture multiscale behavior and neglect underlying physical symmetries. Here we show that an equivariant U-shaped neural operator (E-UNO) can learn the evolution of the phase-field variable from short histories of past dynamics, achieving accurate predictions across space and time. The model combines global spectral convolution with a multi-resolution U-shaped architecture and regulates translation equivariance to align with the underlying physics. E-UNO outperforms standard Fourier neural operator and U-shaped neural operator baselines, particularly on fine-scale and high-frequency structures. By encoding symmetry and scale hierarchy, the model generalizes better, requires less training data, and yields physically consistent dynamics. This establishes E-UNO as an efficient surrogate for complex phase-field systems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Emotion-Mediated Exploration in Artificial Intelligence Mirrors: Findings from Cognitive Psychology</title>
<link>https://arxiv.org/abs/2302.06615</link>
<guid>https://arxiv.org/abs/2302.06615</guid>
<content:encoded><![CDATA[
arXiv:2302.06615v2 Announce Type: replace-cross 
Abstract: Background: Exploration of the physical environment is an indispensable precursor to information acquisition and knowledge consolidation for living organisms. Yet, current artificial intelligence models lack these autonomy capabilities during training, hindering their adaptability. This work proposes a learning framework for artificial agents to obtain an intrinsic exploratory drive, based on epistemic and achievement emotions triggered during data observation. Methods: This study proposes a dual-module reinforcement framework, where data analysis scores dictate pride or surprise, in accordance with psychological studies on humans. A correlation between these states and exploration is then optimized for agents to meet their learning goals. Results: Causal relationships between states and exploration are demonstrated by the majority of agents. A 15.4\% mean increase is noted for surprise, with a 2.8\% mean decrease for pride. Resulting correlations of $\rho_{surprise}=0.461$ and $\rho_{pride}=-0.237$ are obtained, mirroring previously reported human behavior. Conclusions: These findings lead to the conclusion that bio-inspiration for AI development can be of great use. This can incur benefits typically found in living beings, such as autonomy. Further, it empirically shows how AI methodologies can corroborate human behavioral findings, showcasing major interdisciplinary importance. Ramifications are discussed.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenging Bug Prediction and Repair Models with Synthetic Bugs</title>
<link>https://arxiv.org/abs/2310.02407</link>
<guid>https://arxiv.org/abs/2310.02407</guid>
<content:encoded><![CDATA[
arXiv:2310.02407v3 Announce Type: replace-cross 
Abstract: Bugs are essential in software engineering; many research studies in the past decades have been proposed to detect, localize, and repair bugs in software systems. Effectiveness evaluation of such techniques requires complex bugs, i.e., those that are hard to detect through testing and hard to repair through debugging. From the classic software engineering point of view, a hard-to-repair bug differs from the correct code in multiple locations, making it hard to localize and repair. Hard-to-detect bugs, on the other hand, manifest themselves under specific test inputs and reachability conditions. These two objectives, i.e., generating hard-to-detect and hard-to-repair bugs, are mostly aligned; a bug generation technique can change multiple statements to be covered only under a specific set of inputs. However, these two objectives are conflicting for learning-based techniques: A bug should have a similar code representation to the correct code in the training data to challenge a bug prediction model to distinguish them. The hard-to-repair bug definition remains the same but with a caveat: the more a bug differs from the original code, the more distant their representations are and easier to be detected. We propose BugFarm, to transform arbitrary code into multiple complex bugs. BugFarm leverages LLMs to mutate code in multiple locations (hard-to-repair). To ensure that multiple modifications do not notably change the code representation, BugFarm analyzes the attention of the underlying model and instructs LLMs to only change the least attended locations (hard-to-detect). Our comprehensive evaluation of 435k+ bugs from over 1.9M mutants generated by BUGFARM and two alternative approaches demonstrates our superiority in generating bugs that are hard to detect by learning-based bug prediction approaches and hard-to-repair by state-of-the-art learning-based program repair technique.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closed-Loop Unsupervised Representation Disentanglement with $\beta$-VAE Distillation and Diffusion Probabilistic Feedback</title>
<link>https://arxiv.org/abs/2402.02346</link>
<guid>https://arxiv.org/abs/2402.02346</guid>
<content:encoded><![CDATA[
arXiv:2402.02346v2 Announce Type: replace-cross 
Abstract: Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label-free data. To address these challenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervised representation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}. Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to $\beta$-VAE as a co-pilot to extract semantically disentangled representations. The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complementary. To strengthen disentangling, VAE-latent distillation and diffusion-wise feedback are interconnected in a closed-loop system for a further mutual promotion. Then, a self-supervised \textbf{Navigation} strategy is introduced to identify interpretable semantic directions in the disentangled latent space. Finally, a new metric based on content tracking is designed to evaluate the disentanglement effect. Experiments demonstrate the superiority of CL-Dis on applications like real image manipulation and visual analysis.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prepared for the Worst: A Learning-Based Adversarial Attack for Resilience Analysis of the ICP Algorithm</title>
<link>https://arxiv.org/abs/2403.05666</link>
<guid>https://arxiv.org/abs/2403.05666</guid>
<content:encoded><![CDATA[
arXiv:2403.05666v3 Announce Type: replace-cross 
Abstract: This paper presents a novel method for assessing the resilience of the ICP algorithm via learning-based, worst-case attacks on lidar point clouds. For safety-critical applications such as autonomous navigation, ensuring the resilience of algorithms before deployments is crucial. The ICP algorithm is the standard for lidar-based localization, but its accuracy can be greatly affected by corrupted measurements from various sources, including occlusions, adverse weather, or mechanical sensor issues. Unfortunately, the complex and iterative nature of ICP makes assessing its resilience to corruption challenging. While there have been efforts to create challenging datasets and develop simulations to evaluate the resilience of ICP, our method focuses on finding the maximum possible ICP error that can arise from corrupted measurements at a location. We demonstrate that our perturbation-based adversarial attacks can be used pre-deployment to identify locations on a map where ICP is particularly vulnerable to corruptions in the measurements. With such information, autonomous robots can take safer paths when deployed, to mitigate against their measurements being corrupted. The proposed attack outperforms baselines more than 88% of the time across a wide range of scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Language Model to Solve the Symbolic Multi-Step Reasoning Problem from the Perspective of Buffer Mechanism</title>
<link>https://arxiv.org/abs/2405.15302</link>
<guid>https://arxiv.org/abs/2405.15302</guid>
<content:encoded><![CDATA[
arXiv:2405.15302v3 Announce Type: replace-cross 
Abstract: Large language models have consistently struggled with complex reasoning tasks, such as mathematical problem-solving. Investigating the internal reasoning mechanisms of these models can help us design better model architectures and training strategies, ultimately enhancing their reasoning capability. In this study, we constructed a symbolic multi-step reasoning task to investigate the information propagation mechanisms in Transformer models when solving the task through direct answering and Chain-of-Thought (CoT) reasoning. We introduced the concept of buffer mechanism: the model stores various information in distinct buffers and selectively extracts it through the query-key matrix. We proposed a random matrix-based algorithm to enhance the model's reasoning ability. This algorithm introduces only 132 trainable parameters, yet leads to significant performance improvements on 7 multi-step reasoning datasets, including PrOntoQA, LogicAsker, and LogicInference. These findings provide new insights into understanding the large language models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JoPA:Explaining Large Language Model's Generation via Joint Prompt Attribution</title>
<link>https://arxiv.org/abs/2405.20404</link>
<guid>https://arxiv.org/abs/2405.20404</guid>
<content:encoded><![CDATA[
arXiv:2405.20404v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive performances in complex text generation tasks. However, the contribution of the input prompt to the generated content still remains obscure to humans, underscoring the necessity of understanding the causality between input and output pairs. Existing works for providing prompt-specific explanation often confine model output to be classification or next-word prediction. Few initial attempts aiming to explain the entire language generation often treat input prompt texts independently, ignoring their combinatorial effects on the follow-up generation. In this study, we introduce a counterfactual explanation framework based on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt texts collaboratively influences the LLM's complete generation. Particularly, we formulate the task of prompt attribution for generation interpretation as a combinatorial optimization problem, and introduce a probabilistic algorithm to search for the casual input combination in the discrete space. We define and utilize multiple metrics to evaluate the produced explanations, demonstrating both the faithfulness and efficiency of our framework.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Estimation of Lifetime Effects in A/B Testing via Treatment Locality</title>
<link>https://arxiv.org/abs/2407.19618</link>
<guid>https://arxiv.org/abs/2407.19618</guid>
<content:encoded><![CDATA[
arXiv:2407.19618v3 Announce Type: replace-cross 
Abstract: Utilizing randomized experiments to evaluate the effect of short-term treatments on the short-term outcomes has been well understood and become the golden standard in industrial practice. However, as service systems become increasingly dynamical and personalized, much focus is shifting toward maximizing long-term outcomes, such as customer lifetime value, through lifetime exposure to interventions. Our goal is to assess the impact of treatment and control policies on long-term outcomes from relatively short-term observations, such as those generated by A/B testing. A key managerial observation is that many practical treatments are local, affecting only targeted states while leaving other parts of the policy unchanged. This paper rigorously investigates whether and how such locality can be exploited to improve estimation of long-term effects in Markov Decision Processes (MDPs), a fundamental model of dynamic systems. We first develop optimal inference techniques for general A/B testing in MDPs and establish corresponding efficiency bounds. We then propose methods to harness the localized structure by sharing information on the non-targeted states. Our new estimator can achieve a linear reduction with the number of test arms for a major part of the variance without sacrificing unbiasedness. It also matches a tighter variance lower bound that accounts for locality. Furthermore, we extend our framework to a broad class of differentiable estimators, which encompasses many widely used approaches in practice. We show that all such estimators can benefit from variance reduction through information sharing without increasing their bias. Together, these results provide both theoretical foundations and practical tools for conducting efficient experiments in dynamic service systems with local treatments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Metrics for the Assessment of Neurodegenerative Diseases through Handwriting Analysis</title>
<link>https://arxiv.org/abs/2409.08303</link>
<guid>https://arxiv.org/abs/2409.08303</guid>
<content:encoded><![CDATA[
arXiv:2409.08303v3 Announce Type: replace-cross 
Abstract: Motor dysfunction is a common sign of neurodegenerative diseases (NDs) such as Parkinson's disease (PD) and Alzheimer's disease (AD), but may be difficult to detect, especially in the early stages. In this work, we examine the behavior of a wide array of explainable metrics extracted from the handwriting signals of 113 subjects performing multiple tasks on a digital tablet, as part of the Neurological Signals dataset. The aim is to measure their effectiveness in characterizing NDs, including AD and PD. To this end, task-agnostic and task-specific metrics are extracted from 14 distinct tasks. Subsequently, through statistical analysis and a series of classification experiments, we investigate which metrics provide greater discriminative power between NDs and healthy controls and amongst different NDs. Preliminary results indicate that the tasks at hand can all be effectively leveraged to distinguish between the considered set of NDs, specifically by measuring the stability, the speed of writing, the time spent not writing, and the pressure variations between groups from our handcrafted explainable metrics, which shows p-values lower than 0.0001 for multiple tasks. Using various binary classification algorithms on the computed metrics, we obtain up to 87 % accuracy for the discrimination between AD and healthy controls (CTL), and up to 69 % for the discrimination between PD and CTL.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PnP-Flow: Plug-and-Play Image Restoration with Flow Matching</title>
<link>https://arxiv.org/abs/2410.02423</link>
<guid>https://arxiv.org/abs/2410.02423</guid>
<content:encoded><![CDATA[
arXiv:2410.02423v3 Announce Type: replace-cross 
Abstract: In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm for solving imaging inverse problems. PnP methods leverage the strength of pre-trained denoisers, often deep neural networks, by integrating them in optimization schemes. While they achieve state-of-the-art performance on various inverse problems in imaging, PnP approaches face inherent limitations on more generative tasks like inpainting. On the other hand, generative models such as Flow Matching pushed the boundary in image sampling yet lack a clear method for efficient use in image restoration. We propose to combine the PnP framework with Flow Matching (FM) by defining a time-dependent denoiser using a pre-trained FM model. Our algorithm alternates between gradient descent steps on the data-fidelity term, reprojections onto the learned FM path, and denoising. Notably, our method is computationally efficient and memory-friendly, as it avoids backpropagation through ODEs and trace computations. We evaluate its performance on denoising, super-resolution, deblurring, and inpainting tasks, demonstrating superior results compared to existing PnP algorithms and Flow Matching based state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Humanoid Manipulation with 3D Diffusion Policies</title>
<link>https://arxiv.org/abs/2410.10803</link>
<guid>https://arxiv.org/abs/2410.10803</guid>
<content:encoded><![CDATA[
arXiv:2410.10803v3 Announce Type: replace-cross 
Abstract: Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills and the expensiveness of in-the-wild humanoid robot data. In this work, we build a real-world robotic system to address this challenging problem. Our system is mainly an integration of 1) a whole-upper-body robotic teleoperation system to acquire human-like robot data, 2) a 25-DoF humanoid robot platform with a height-adjustable cart and a 3D LiDAR sensor, and 3) an improved 3D Diffusion Policy learning algorithm for humanoid robots to learn from noisy human data. We run more than 2000 episodes of policy rollouts on the real robot for rigorous policy evaluation. Empowered by this system, we show that using only data collected in one single scene and with only onboard computing, a full-sized humanoid robot can autonomously perform skills in diverse real-world scenarios. Videos are available at https://humanoid-manipulation.github.io .
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection</title>
<link>https://arxiv.org/abs/2411.02886</link>
<guid>https://arxiv.org/abs/2411.02886</guid>
<content:encoded><![CDATA[
arXiv:2411.02886v3 Announce Type: replace-cross 
Abstract: Rapid advances in Large Language Models (LLMs) have spurred demand for processing extended context sequences in contemporary applications. However, this progress faces two challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues limit LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using QK dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a few critical KV cache tokens in attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we design the Selection Cache based on observations of consecutive Query similarity and implemented the efficient Paged Dot Product Kernel, significantly reducing the selection overhead. A comprehensive evaluation of TokenSelect demonstrates up to $23.84\times$ speedup in attention computation and up to $2.28\times$ acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Data-Free Analytical Quantization Scheme for Deep Learning Models</title>
<link>https://arxiv.org/abs/2412.07391</link>
<guid>https://arxiv.org/abs/2412.07391</guid>
<content:encoded><![CDATA[
arXiv:2412.07391v3 Announce Type: replace-cross 
Abstract: Despite the success of CNN models on a variety of Image classification and segmentation tasks, their extensive computational and storage demands pose considerable challenges for real-world deployment on resource-constrained devices. Quantization is one technique that aims to alleviate these large storage requirements and speed up the inference process by reducing the precision of model parameters to lower-bit representations. In this paper, we introduce a novel post-training quantization method for model weights. Our method finds optimal clipping thresholds and scaling factors along with mathematical guarantees that our method minimizes quantization noise. Empirical results on real-world datasets demonstrate that our quantization scheme significantly reduces model size and computational requirements while preserving model accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Physics-informed neural networks loss function regularization with a variance-based term</title>
<link>https://arxiv.org/abs/2412.13993</link>
<guid>https://arxiv.org/abs/2412.13993</guid>
<content:encoded><![CDATA[
arXiv:2412.13993v3 Announce Type: replace-cross 
Abstract: In machine learning and statistical modeling, the mean square or absolute error is commonly used as an error metric, also called a "loss function." While effective in reducing the average error, this approach may fail to address localized outliers, leading to significant inaccuracies in regions with sharp gradients or discontinuities. This issue is particularly evident in physics-informed neural networks (PINNs), where such localized errors are expected and affect the overall solution. To overcome this limitation, we propose a novel loss function that combines the mean and the standard deviation of the chosen error metric. By minimizing this combined loss function, the method ensures a more uniform error distribution and reduces the impact of localized high-error regions. The proposed loss function is easy to implement and tested on problems of varying complexity: the 1D Poisson equation, the unsteady Burgers' equation, 2D linear elastic solid mechanics, and 2D steady Navier-Stokes equations. Results demonstrate improved solution quality and lower maximum error compared to the standard mean-based loss, with minimal impact on computational time.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth Models</title>
<link>https://arxiv.org/abs/2501.08226</link>
<guid>https://arxiv.org/abs/2501.08226</guid>
<content:encoded><![CDATA[
arXiv:2501.08226v2 Announce Type: replace-cross 
Abstract: Glioblastoma, a highly aggressive brain tumor, poses major challenges due to its poor prognosis and high morbidity rates. Partial differential equation-based models offer promising potential to enhance therapeutic outcomes by simulating patient-specific tumor behavior for improved radiotherapy planning. However, model calibration remains a bottleneck due to the high computational demands of optimization methods like Monte Carlo sampling and evolutionary algorithms. To address this, we recently introduced an approach leveraging a neural forward solver with gradient-based optimization to significantly reduce calibration time. This approach requires a highly accurate and fully differentiable forward model. We investigate multiple architectures, including (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a 3D Vision Transformer (ViT). The nnU-Net achieved the best overall results, excelling in both tumor outline matching and voxel-level prediction of tumor cell concentration. It yielded the lowest MSE in tumor cell concentration compared to ground truth numerical simulation and the highest Dice score across all tumor cell concentration thresholds. Our study demonstrates significant enhancement in forward solver performance and outlines important future research directions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matrix Completion in Group Testing: Bounds and Simulations</title>
<link>https://arxiv.org/abs/2501.13780</link>
<guid>https://arxiv.org/abs/2501.13780</guid>
<content:encoded><![CDATA[
arXiv:2501.13780v2 Announce Type: replace-cross 
Abstract: The goal of group testing is to identify a small number of defective items within a large population. In the non-adaptive setting, tests are designed in advance and represented by a measurement matrix $\mM$, where rows correspond to tests and columns to items. A test is positive if it includes at least one defective item. Traditionally, $\mM$ remains fixed during both testing and recovery. In this work, we address the case where some entries of $\mM$ are missing, yielding a missing measurement matrix $\mG$. Our aim is to reconstruct $\mM$ from $\mG$ using available samples and their outcome vectors.
  The above problem can be considered as a problem intersected between Boolean matrix factorization and matrix completion, called the matrix completion in group testing (MCGT) problem, as follows. Given positive integers $t,s,n$, let $\mY:=(y_{ij}) \in \{0, 1\}^{t \times s}$, $\mM:=(m_{ij}) \in \{0,1\}^{t \times n}$, $\mX:=(x_{ij}) \in \{0,1\}^{n \times s}$, and matrix $\mG \in \{0,1 \}^{t \times n}$ be a matrix generated from matrix $\mM$ by erasing some entries in $\mM$. Suppose $\mY:=\mM \odot \mX$, where an entry $y_{ij}:=\bigvee_{k=1}^n (m_{ik}\wedge x_{kj})$, and $\wedge$ and $\vee$ are AND and OR operators. Unlike the problem in group testing whose objective is to find $\mX$ when given $\mM$ and $\mY$, our objective is to recover $\mM$ given $\mY,\mX$, and $\mG$.
  We first prove that the MCGT problem is NP-complete. Next, we show that certain rows with missing entries aid recovery while others do not. For Bernoulli measurement matrices, we establish that larger $s$ increases the higher the probability that $\mM$ can be recovered. We then instantiate our bounds for specific decoding algorithms and validate them through simulations, demonstrating superiority over standard matrix completion and Boolean matrix factorization methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VINP: Variational Bayesian Inference with Neural Speech Prior for Joint ASR-Effective Speech Dereverberation and Blind RIR Identification</title>
<link>https://arxiv.org/abs/2502.07205</link>
<guid>https://arxiv.org/abs/2502.07205</guid>
<content:encoded><![CDATA[
arXiv:2502.07205v3 Announce Type: replace-cross 
Abstract: Reverberant speech, denoting the speech signal degraded by reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to estimate the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP attains state-of-the-art (SOTA) performance in mean opinion score (MOS) and word error rate (WER). For blind RIR identification, experiments demonstrate that VINP achieves SOTA performance in estimating reverberation time at 60 dB (RT60) and advanced performance in direct-to-reverberation ratio (DRR) estimation. Codes and audio samples are available online.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject Batch Editing for LLMs</title>
<link>https://arxiv.org/abs/2502.07322</link>
<guid>https://arxiv.org/abs/2502.07322</guid>
<content:encoded><![CDATA[
arXiv:2502.07322v3 Announce Type: replace-cross 
Abstract: As large language models continue to scale up, knowledge editing techniques that modify models' internal knowledge without full retraining have gained significant attention. MEMIT, a prominent batch editing algorithm, stands out for its capability to perform mass knowledge modifications. However, we uncover that MEMIT's editing efficacy significantly deteriorates when processing batches containing multiple edits sharing the same subject. Our analysis reveals this stems from MEMIT's key value modeling framework: identical keys (derived from the shared subject) are forced to represent different values (corresponding to different knowledge), resulting in update conflicts during editing. Addressing this issue, we propose MEMIT-Merge, an enhanced approach that merges value computation processes for facts sharing the same subject, effectively resolving the performance degradation in samesubject batch editing scenarios. Experimental results demonstrate that when MEMIT's edit success rate drops to around 50% at larger batch sizes, MEMIT-Merge maintains a success rate exceeding 90%, showcasing remarkable robustness to subject entity collisions. The code is available at https://github.com/NUSTM/ MEMIT-Merge.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection</title>
<link>https://arxiv.org/abs/2502.13061</link>
<guid>https://arxiv.org/abs/2502.13061</guid>
<content:encoded><![CDATA[
arXiv:2502.13061v3 Announce Type: replace-cross 
Abstract: Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While Large Multimodal Models (LMMs) have shown promise in hateful meme detection, they face notable challenges like sub-optimal performance and limited out-of-domain generalization capabilities. Recent studies further reveal the limitations of both supervised fine-tuning (SFT) and in-context learning when applied to LMMs in this setting. To address these issues, we propose a robust adaptation framework for hateful meme detection that enhances in-domain accuracy and cross-domain generalization while preserving the general vision-language capabilities of LMMs. Analysis reveals that our approach achieves improved robustness under adversarial attacks compared to SFT models. Experiments on six meme classification datasets show that our approach achieves state-of-the-art performance, outperforming larger agentic systems. Moreover, our method generates higher-quality rationales for explaining hateful content compared to standard SFT, enhancing model interpretability. Code available at https://github.com/JingbiaoMei/RGCL
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA</title>
<link>https://arxiv.org/abs/2502.18536</link>
<guid>https://arxiv.org/abs/2502.18536</guid>
<content:encoded><![CDATA[
arXiv:2502.18536v2 Announce Type: replace-cross 
Abstract: Visual Question Answering requires models to generate accurate answers by integrating visual and textual understanding. However, VQA models still struggle with hallucinations, producing convincing but incorrect answers, particularly in knowledge-driven and Out-of-Distribution scenarios. We introduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA with Retrieval-Augmented Generation to ground answers in external knowledge sources like Wikipedia and DBpedia. FilterRAG achieves 36.5% accuracy on the OK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and improving robustness in both in-domain and Out-of-Distribution settings. These findings highlight the potential of FilterRAG to improve Visual Question Answering systems for real-world deployment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models</title>
<link>https://arxiv.org/abs/2503.21929</link>
<guid>https://arxiv.org/abs/2503.21929</guid>
<content:encoded><![CDATA[
arXiv:2503.21929v2 Announce Type: replace-cross 
Abstract: Advances in hardware and language model architecture have spurred a revolution in natural language generation. However, autoregressive models compute probability distributions over next-token choices, and sampling from these distributions, known as decoding, has received significantly less attention than other design choices. Existing decoding strategies are largely based on heuristics, resulting in methods that are difficult to apply or improve in a principled manner. We develop the theory of decoding strategies for language models by expressing popular decoding algorithms as equilibrium states in the language of ergodic theory and stating the objective functions they optimize. Using this, we analyze the effect of the local normalization step required to make probabilities sum to one in top-k, nucleus, and temperature sampling. We argue that local normalization distortion is a fundamental defect of decoding strategies and quantify the size of this distortion and its effect on mathematical proxies for the quality and diversity of generated text. This yields conclusions for the design of decoding algorithms and the detection of machine-generated text.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemCAFE: When Named Entities make the Difference Assessing Web Source Reliability through Entity-level Analytics</title>
<link>https://arxiv.org/abs/2504.08776</link>
<guid>https://arxiv.org/abs/2504.08776</guid>
<content:encoded><![CDATA[
arXiv:2504.08776v2 Announce Type: replace-cross 
Abstract: With the shift from traditional to digital media, the online landscape now hosts not only reliable news articles but also a significant amount of unreliable content. Digital media has faster reachability by significantly influencing public opinion and advancing political agendas. While newspaper readers may be familiar with their preferred outlets political leanings or credibility, determining unreliable news articles is much more challenging. The credibility of many online sources is often opaque, with AI generated content being easily disseminated at minimal cost. Unreliable news articles, particularly those that followed the Russian invasion of Ukraine in 2022, closely mimic the topics and writing styles of credible sources, making them difficult to distinguish. To address this, we introduce SemCAFE, a system designed to detect news reliability by incorporating entity relatedness into its assessment. SemCAFE employs standard Natural Language Processing techniques, such as boilerplate removal and tokenization, alongside entity level semantic analysis using the YAGO knowledge base. By creating a semantic fingerprint for each news article, SemCAFE could assess the credibility of 46,020 reliable and 3,407 unreliable articles on the 2022 Russian invasion of Ukraine. Our approach improved the macro F1 score by 12% over state of the art methods. The sample data and code are available on GitHub
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Llama-Nemotron: Efficient Reasoning Models</title>
<link>https://arxiv.org/abs/2505.00949</link>
<guid>https://arxiv.org/abs/2505.00949</guid>
<content:encoded><![CDATA[
arXiv:2505.00949v5 Announce Type: replace-cross 
Abstract: We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.04416</link>
<guid>https://arxiv.org/abs/2505.04416</guid>
<content:encoded><![CDATA[
arXiv:2505.04416v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose \textbf{OBLIVIATE}, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA) ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: \emph{forget quality} (via a new document-level memorization score), \emph{model utility}, and \emph{fluency}. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analytic theory of dropout regularization</title>
<link>https://arxiv.org/abs/2505.07792</link>
<guid>https://arxiv.org/abs/2505.07792</guid>
<content:encoded><![CDATA[
arXiv:2505.07792v2 Announce Type: replace-cross 
Abstract: Dropout is a regularization technique widely used in training artificial neural networks to mitigate overfitting. It consists of dynamically deactivating subsets of the network during training to promote more robust representations. Despite its widespread adoption, dropout probabilities are often selected heuristically, and theoretical explanations of its success remain sparse. Here, we analytically study dropout in two-layer neural networks trained with online stochastic gradient descent. In the high-dimensional limit, we derive a set of ordinary differential equations that fully characterize the evolution of the network during training and capture the effects of dropout. We obtain a number of exact results describing the generalization error and the optimal dropout probability at short, intermediate, and long training times. Our analysis shows that dropout reduces detrimental correlations between hidden nodes, mitigates the impact of label noise, and that the optimal dropout probability increases with the level of noise in the data. Our results are validated by extensive numerical simulations.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inexact Column Generation for Bayesian Network Structure Learning via Difference-of-Submodular Optimization</title>
<link>https://arxiv.org/abs/2505.11089</link>
<guid>https://arxiv.org/abs/2505.11089</guid>
<content:encoded><![CDATA[
arXiv:2505.11089v2 Announce Type: replace-cross 
Abstract: In this paper, we consider a score-based Integer Programming (IP) approach for solving the Bayesian Network Structure Learning (BNSL) problem. State-of-the-art BNSL IP formulations suffer from the exponentially large number of variables and constraints. A standard approach in IP to address such challenges is to employ row and column generation techniques, which dynamically generate rows and columns, while the complex pricing problem remains a computational bottleneck for BNSL. For the general class of $\ell_0$-penalized likelihood scores, we show how the pricing problem can be reformulated as a difference of submodular optimization problem, and how the Difference of Convex Algorithm (DCA) can be applied as an inexact method to efficiently solve the pricing problems. Empirically, we show that, for continuous Gaussian data, our row and column generation approach yields solutions with higher quality than state-of-the-art score-based approaches, especially when the graph density increases, and achieves comparable performance against benchmark constraint-based and hybrid approaches, even when the graph size increases.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Reward Shaping from Confounded Offline Data</title>
<link>https://arxiv.org/abs/2505.11478</link>
<guid>https://arxiv.org/abs/2505.11478</guid>
<content:encoded><![CDATA[
arXiv:2505.11478v2 Announce Type: replace-cross 
Abstract: A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures. Off-policy learning methods, like Q-learning, allow learners to make optimal decisions based on past experiences. This paper studies off-policy learning from biased data in complex and high-dimensional domains where \emph{unobserved confounding} cannot be ruled out a priori. Building on the well-celebrated Deep Q-Network (DQN), we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data. Specifically, our algorithm attempts to find a safe policy for the worst-case environment compatible with the observations. We apply our method to twelve confounded Atari games, and find that it consistently dominates the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visuospatial Cognitive Assistant</title>
<link>https://arxiv.org/abs/2505.12312</link>
<guid>https://arxiv.org/abs/2505.12312</guid>
<content:encoded><![CDATA[
arXiv:2505.12312v4 Announce Type: replace-cross 
Abstract: Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</title>
<link>https://arxiv.org/abs/2505.12363</link>
<guid>https://arxiv.org/abs/2505.12363</guid>
<content:encoded><![CDATA[
arXiv:2505.12363v4 Announce Type: replace-cross 
Abstract: While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives</title>
<link>https://arxiv.org/abs/2505.21627</link>
<guid>https://arxiv.org/abs/2505.21627</guid>
<content:encoded><![CDATA[
arXiv:2505.21627v2 Announce Type: replace-cross 
Abstract: State-of-the-art large language models require specialized hardware and substantial energy to operate. As a consequence, cloud-based services that provide access to large language models have become very popular. In these services, the price users pay for an output provided by a model depends on the number of tokens the model uses to generate it -- they pay a fixed price per token. In this work, we show that this pricing mechanism creates a financial incentive for providers to strategize and misreport the (number of) tokens a model used to generate an output, and users cannot prove, or even know, whether a provider is overcharging them. However, we also show that, if an unfaithful provider is obliged to be transparent about the generative process used by the model, misreporting optimally without raising suspicion is hard. Nevertheless, as a proof-of-concept, we develop an efficient heuristic algorithm that allows providers to significantly overcharge users without raising suspicion. Crucially, we demonstrate that the cost of running the algorithm is lower than the additional revenue from overcharging users, highlighting the vulnerability of users under the current pay-per-token pricing mechanism. Further, we show that, to eliminate the financial incentive to strategize, a pricing mechanism must price tokens linearly on their character count. While this makes a provider's profit margin vary across tokens, we introduce a simple prescription under which the provider who adopts such an incentive-compatible pricing mechanism can maintain the average profit margin they had under the pay-per-token pricing mechanism. Along the way, to illustrate and complement our theoretical results, we conduct experiments with several large language models from the $\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input prompts from the LMSYS Chatbot Arena platform.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning</title>
<link>https://arxiv.org/abs/2505.22626</link>
<guid>https://arxiv.org/abs/2505.22626</guid>
<content:encoded><![CDATA[
arXiv:2505.22626v2 Announce Type: replace-cross 
Abstract: Imitation learning advances robot capabilities by enabling the acquisition of diverse behaviors from human demonstrations. However, large-scale datasets used for policy training often introduce substantial variability in quality, which can negatively impact performance. As a result, automatically curating datasets by filtering low-quality samples to improve quality becomes essential. Existing robotic curation approaches rely on costly manual annotations and perform curation at a coarse granularity, such as the dataset or trajectory level, failing to account for the quality of individual state-action pairs. To address this, we introduce SCIZOR, a self-supervised data curation framework that filters out low-quality state-action pairs to improve the performance of imitation learning policies. SCIZOR targets two complementary sources of low-quality data: suboptimal data, which hinders learning with undesirable actions, and redundant data, which dilutes training with repetitive patterns. SCIZOR leverages a self-supervised task progress predictor for suboptimal data to remove samples lacking task progression, and a deduplication module operating on joint state-action representation for samples with redundant patterns. Empirically, we show that SCIZOR enables imitation learning policies to achieve higher performance with less data, yielding an average improvement of 15.4% across multiple benchmarks. More information is available at: https://ut-austin-rpl.github.io/SCIZOR/
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Upsample and Upmix Audio in the Latent Domain</title>
<link>https://arxiv.org/abs/2506.00681</link>
<guid>https://arxiv.org/abs/2506.00681</guid>
<content:encoded><![CDATA[
arXiv:2506.00681v2 Announce Type: replace-cross 
Abstract: Neural audio autoencoders create compact latent representations that preserve perceptually important information, serving as the foundation for both modern audio compression systems and generation approaches like next-token prediction and latent diffusion. Despite their prevalence, most audio processing operations, such as spatial and spectral up-sampling, still inefficiently operate on raw waveforms or spectral representations rather than directly on these compressed representations. We propose a framework that performs audio processing operations entirely within an autoencoder's latent space, eliminating the need to decode to raw audio formats. Our approach dramatically simplifies training by operating solely in the latent domain, with a latent L1 reconstruction term, augmented by a single latent adversarial discriminator. This contrasts sharply with raw-audio methods that typically require complex combinations of multi-scale losses and discriminators. Through experiments in bandwidth extension and mono-to-stereo up-mixing, we demonstrate computational efficiency gains of up to 100x while maintaining quality comparable to post-processing on raw audio. This work establishes a more efficient paradigm for audio processing pipelines that already incorporate autoencoders, enabling significantly faster and more resource-efficient workflows across various audio tasks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning</title>
<link>https://arxiv.org/abs/2506.00785</link>
<guid>https://arxiv.org/abs/2506.00785</guid>
<content:encoded><![CDATA[
arXiv:2506.00785v3 Announce Type: replace-cross 
Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating step-by-step geographic reasoning in multimodal large language models (MLLMs). Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each image with a 21-step chain-of-thought (CoT) question sequence (over 30 million Q&amp;A pairs). These sequences guide models from coarse attributes to fine-grained localization across four reasoning categories - visual, spatial, cultural, and precise geolocation - annotated by difficulty. Images are also enriched with semantic segmentation (150 classes) and a visual locatability score. Our benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5 variants) on a diverse 2,088-image subset reveals consistent challenges: models frequently exhibit weaknesses in visual grounding, display erratic reasoning, and struggle to achieve accurate localization, especially as the reasoning complexity escalates. GeoChain offers a robust diagnostic methodology, critical for fostering significant advancements in complex geographic reasoning within MLLMs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HueManity: Probing Fine-Grained Visual Perception in MLLMs</title>
<link>https://arxiv.org/abs/2506.03194</link>
<guid>https://arxiv.org/abs/2506.03194</guid>
<content:encoded><![CDATA[
arXiv:2506.03194v3 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) excel at high-level visual reasoning, but their performance on nuanced perceptual tasks remains surprisingly limited. We present HueManity, a benchmark designed to assess visual perception in MLLMs. The dataset comprises 83,850 images featuring two-character alphanumeric strings embedded in Ishihara test style dot patterns, challenging models on precise pattern recognition. Our evaluation of nine state-of-the-art MLLMs on HueManity demonstrates a significant performance deficit compared to human and traditional computer vision baselines. The best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a striking 3% on the alphanumeric `hard' task. In contrast, human participants achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model reached accuracies of 96.5% and 94.5%. These results highlight a critical gap in the visual capabilities of current MLLMs. Our analysis further explores potential architectural and training-paradigm factors contributing to this perceptual gap in MLLMs. We open-source HueManity dataset and code to foster further research in improving perceptual robustness of MLLMs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs</title>
<link>https://arxiv.org/abs/2506.07899</link>
<guid>https://arxiv.org/abs/2506.07899</guid>
<content:encoded><![CDATA[
arXiv:2506.07899v2 Announce Type: replace-cross 
Abstract: Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably-without retraining or forgetting previous information-remains a major challenge. Existing methods for lifelong model editing either compromise generalization, interfere with past edits, or fail to scale to long editing sequences. We propose MEMOIR, a novel scalable framework that injects knowledge through a residual memory, i.e., a dedicated parameter module, while preserving the core capabilities of the pre-trained model. By sparsifying input activations through sample-dependent masks, MEMOIR confines each edit to a distinct subset of the memory parameters, minimizing interference among edits. At inference, it identifies relevant edits by comparing the sparse activation patterns of new queries to those stored during editing. This enables generalization to rephrased queries by activating only the relevant knowledge while suppressing unnecessary memory activation for unrelated prompts. Experiments on question answering, hallucination correction, and out-of-distribution generalization benchmarks for LLaMA-3 and Mistral backbones demonstrate that MEMOIR achieves state-of-the-art performance across reliability, generalization, and locality metrics, scaling to thousands of sequential edits with minimal forgetting.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models</title>
<link>https://arxiv.org/abs/2506.09076</link>
<guid>https://arxiv.org/abs/2506.09076</guid>
<content:encoded><![CDATA[
arXiv:2506.09076v3 Announce Type: replace-cross 
Abstract: Pathogen genome data offers valuable structure for spatial models, but its utility is limited by incomplete sequencing coverage. We propose a probabilistic framework for inferring genetic distances between unsequenced cases and known sequences within defined transmission chains, using time-aware evolutionary distance modeling. The method estimates pairwise divergence from collection dates and observed genetic distances, enabling biologically plausible imputation grounded in observed divergence patterns, without requiring sequence alignment or known transmission chains. Applied to highly pathogenic avian influenza A/H5 cases in wild birds in the United States, this approach supports scalable, uncertainty-aware augmentation of genomic datasets and enhances the integration of evolutionary information into spatiotemporal modeling workflows.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence of Momentum-Based Optimization Algorithms with Time-Varying Parameters</title>
<link>https://arxiv.org/abs/2506.11904</link>
<guid>https://arxiv.org/abs/2506.11904</guid>
<content:encoded><![CDATA[
arXiv:2506.11904v2 Announce Type: replace-cross 
Abstract: In this paper, we present a unified algorithm for stochastic optimization that makes use of a "momentum" term; in other words, the stochastic gradient depends not only on the current true gradient of the objective function, but also on the true gradient at the previous iteration. Our formulation includes the Stochastic Heavy Ball (SHB) and the Stochastic Nesterov Accelerated Gradient (SNAG) algorithms as special cases. In addition, in our formulation, the momentum term is allowed to vary as a function of time (i.e., the iteration counter). The assumptions on the stochastic gradient are the most general in the literature, in that it can be biased, and have a conditional variance that grows in an unbounded fashion as a function of time. This last feature is crucial in order to make the theory applicable to "zero-order" methods, where the gradient is estimated using just two function evaluations.
  We present a set of sufficient conditions for the convergence of the unified algorithm. These conditions are natural generalizations of the familiar Robbins-Monro and Kiefer-Wolfowitz-Blum conditions for standard stochastic gradient descent. We also analyze another method from the literature for the SHB algorithm with a time-varying momentum parameter, and show that it is impracticable.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC</title>
<link>https://arxiv.org/abs/2506.15011</link>
<guid>https://arxiv.org/abs/2506.15011</guid>
<content:encoded><![CDATA[
arXiv:2506.15011v3 Announce Type: replace-cross 
Abstract: Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph Convolutional Network (GCN) integrated with a Deep Q-Network (DQN) reinforcement learning framework for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach dynamically learns link priorities based on real-time traffic demand, network topology, remaining transmission opportunities, and interference patterns. The GCN captures spatial dependencies, while the DQN enables adaptive scheduling decisions through reward-guided exploration. Simulation results show that our GCN-DQN model achieves mean SINR improvements of 179.6\%, 197.4\%, and 175.2\% over LDP across three network configurations. Additionally, the GCN-DQN model demonstrates mean SINR improvements of 31.5\%, 53.0\%, and 84.7\% over our previous CNN-based approach across the same configurations. These results underscore the effectiveness of our GCN-DQN model in addressing complex URLLC requirements with minimal overhead and superior network performance.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders</title>
<link>https://arxiv.org/abs/2507.07867</link>
<guid>https://arxiv.org/abs/2507.07867</guid>
<content:encoded><![CDATA[
arXiv:2507.07867v2 Announce Type: replace-cross 
Abstract: Neural audio codecs and autoencoders have emerged as versatile models for audio compression, transmission, feature-extraction, and latent-space generation. However, a key limitation is that most are trained to maximize reconstruction fidelity, often neglecting the specific latent structure necessary for optimal performance in diverse downstream applications. We propose a simple, post-hoc framework to address this by modifying the bottleneck of a pre-trained autoencoder. Our method introduces a "Re-Bottleneck", an inner bottleneck trained exclusively through latent space losses to instill user-defined structure. We demonstrate the framework's effectiveness in three experiments. First, we enforce an ordering on latent channels without sacrificing reconstruction quality. Second, we align latents with semantic embeddings, analyzing the impact on downstream diffusion modeling. Third, we introduce equivariance, ensuring that a filtering operation on the input waveform directly corresponds to a specific transformation in the latent space. Ultimately, our Re-Bottleneck framework offers a flexible and efficient way to tailor representations of neural audio models, enabling them to seamlessly meet the varied demands of different applications with minimal additional training.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data</title>
<link>https://arxiv.org/abs/2507.22291</link>
<guid>https://arxiv.org/abs/2507.22291</guid>
<content:encoded><![CDATA[
arXiv:2507.22291v2 Announce Type: replace-cross 
Abstract: Unprecedented volumes of Earth observation data are continually collected around the world, but high-quality labels remain scarce given the effort required to make physical measurements and observations. This has led to considerable investment in bespoke modeling efforts translating sparse labels into maps. Here we introduce AlphaEarth Foundations, an embedding field model yielding a highly general, geospatial representation that assimilates spatial, temporal, and measurement contexts across multiple sources, enabling accurate and efficient production of maps and monitoring systems from local to global scales. The embeddings generated by AlphaEarth Foundations are the only to consistently outperform a suite of other well-known/widely accepted featurization approaches tested on a diverse set of mapping evaluations without re-training. We have released a dataset of global, annual, analysis-ready embedding field layers from 2017 through 2024.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond</title>
<link>https://arxiv.org/abs/2508.07353</link>
<guid>https://arxiv.org/abs/2508.07353</guid>
<content:encoded><![CDATA[
arXiv:2508.07353v3 Announce Type: replace-cross 
Abstract: The increasing demand for domain-specific evaluation of large language models (LLMs) has led to the development of numerous benchmarks. These efforts often adhere to the principle of data scaling, relying on large corpora or extensive question-answer (QA) sets to ensure broad coverage. However, the impact of corpus and QA set design on the precision and recall of domain-specific LLM performance remains poorly understood. In this paper, we argue that data scaling is not always the optimal principle for domain-specific benchmark construction. Instead, we introduce Comp-Comp, an iterative benchmarking framework grounded in the principle of comprehensiveness and compactness. Comprehensiveness ensures semantic recall by covering the full breadth of the domain, while compactness improves precision by reducing redundancy and noise. To demonstrate the effectiveness of our approach, we present a case study conducted at a well-renowned university, resulting in the creation of PolyBench, a large-scale, high-quality academic benchmark. Although this study focuses on academia, the Comp-Comp framework is domain-agnostic and readily adaptable to a wide range of specialized fields. The source code and datasets can be accessed at https://github.com/Anya-RB-Chen/COMP-COMP.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</title>
<link>https://arxiv.org/abs/2508.17180</link>
<guid>https://arxiv.org/abs/2508.17180</guid>
<content:encoded><![CDATA[
arXiv:2508.17180v2 Announce Type: replace-cross 
Abstract: A key frontier for Multimodal Large Language Models (MLLMs) is the ability to perform deep mathematical and spatial reasoning directly from images, moving beyond their established success in semantic description. Mathematical surface plots provide a rigorous testbed for this capability, as they isolate the task of reasoning from the semantic noise common in natural images. To measure progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over Visual Landscapes), a new benchmark designed to quantitatively evaluate these core reasoning skills. The benchmark comprises two novel tasks: Topological Counting, identifying and enumerating features like local maxima; and Transformation Recognition, recognizing applied geometric transformations. Generated from a curated library of functions with rigorous ambiguity filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs struggle significantly, often resorting to superficial heuristics instead of robust spatial reasoning. MaRVL-QA provides a challenging new tool for the research community to measure progress, expose model limitations, and guide the development of MLLMs with more profound reasoning abilities.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models</title>
<link>https://arxiv.org/abs/2508.17674</link>
<guid>https://arxiv.org/abs/2508.17674</guid>
<content:encoded><![CDATA[
arXiv:2508.17674v2 Announce Type: replace-cross 
Abstract: We introduce Advertisement Embedding Attacks (AEA), a new class of LLM security threats that stealthily inject promotional or malicious content into model outputs and AI agents. AEA operate through two low-cost vectors: (1) hijacking third-party service-distribution platforms to prepend adversarial prompts, and (2) publishing back-doored open-source checkpoints fine-tuned with attacker data. Unlike conventional attacks that degrade accuracy, AEA subvert information integrity, causing models to return covert ads, propaganda, or hate speech while appearing normal. We detail the attack pipeline, map five stakeholder victim groups, and present an initial prompt-based self-inspection defense that mitigates these injections without additional model retraining. Our findings reveal an urgent, under-addressed gap in LLM security and call for coordinated detection, auditing, and policy responses from the AI-safety community.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Generative Flows for LHC Jets</title>
<link>https://arxiv.org/abs/2509.01736</link>
<guid>https://arxiv.org/abs/2509.01736</guid>
<content:encoded><![CDATA[
arXiv:2509.01736v2 Announce Type: replace-cross 
Abstract: Generative modeling of high-energy collisions at the Large Hadron Collider (LHC) offers a data-driven route to simulations, anomaly detection, among other applications. A central challenge lies in the hybrid nature of particle-cloud data: each particle carries continuous kinematic features and discrete quantum numbers such as charge and flavor. We introduce a transformer-based multimodal flow that extends flow-matching with a continuous-time Markov jump bridge to jointly model LHC jets with both modalities. Trained on CMS Open Data, our model can generate high fidelity jets with realistic kinematics, jet substructure and flavor composition.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing Data Imputation using Neural Cellular Automata</title>
<link>https://arxiv.org/abs/2509.00651</link>
<guid>https://arxiv.org/abs/2509.00651</guid>
<content:encoded><![CDATA[
<div> Keywords: tabular data, missingness, imputation, Neural Cellular Automata, generative models

Summary:
Missing data in tabular datasets is a common issue, and researchers have been exploring various methods to impute this missing data. Recently, there has been a shift towards using generative models, such as Variational Autoencoders and Generative Adversarial Networks, for imputation tasks. However, Neural Cellular Automata (NCA), a powerful computational model, has been overlooked in this context. This paper introduces a novel imputation method inspired by NCA, showing that with appropriate adaptations, it can effectively address missing data imputation. Experimental results demonstrate that the proposed model outperforms existing methods in terms of imputation error and post-imputation performance.<br /><br />Summary: <div>
arXiv:2509.00651v2 Announce Type: replace 
Abstract: When working with tabular data, missingness is always one of the most painful problems. Throughout many years, researchers have continuously explored better and better ways to impute missing data. Recently, with the rapid development evolution in machine learning and deep learning, there is a new trend of leveraging generative models to solve the imputation task. While the imputing version of famous models such as Variational Autoencoders or Generative Adversarial Networks were investigated, prior work has overlooked Neural Cellular Automata (NCA), a powerful computational model. In this paper, we propose a novel imputation method that is inspired by NCA. We show that, with some appropriate adaptations, an NCA-based model is able to address the missing data imputation problem. We also provide several experiments to evidence that our model outperforms state-of-the-art methods in terms of imputation error and post-imputation performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Any-Order Flexible Length Masked Diffusion</title>
<link>https://arxiv.org/abs/2509.01025</link>
<guid>https://arxiv.org/abs/2509.01025</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked Diffusion Models, Flexible Masked Diffusion Models, any-order inference, sequence generation, performance improvement

Summary:
Flexible Masked Diffusion Models (FlexMDMs) address the limitation of fixed-length generation in Masked Diffusion Models (MDMs) by allowing for sequences of flexible length. FlexMDMs utilize a discrete diffusion paradigm that involves inserting mask tokens and unmasking them, enabling any-order inference while supporting variable-length sequences. Empirical results demonstrate that FlexMDMs achieve comparable perplexity to MDMs while accurately modeling length statistics, leading to a significant improvement in performance on tasks such as synthetic maze planning. Moreover, the retrofitting of pretrained MDMs into FlexMDMs is shown to be efficient, with superior results attained after fine-tuning. This work highlights the potential of FlexMDMs in enhancing the capabilities of traditional MDMs and expanding their applicability to tasks requiring variable-length sequence generation.<br /><br />Summary: <div>
arXiv:2509.01025v2 Announce Type: replace 
Abstract: Masked diffusion models (MDMs) have recently emerged as a promising alternative to autoregressive models over discrete domains. MDMs generate sequences in an any-order, parallel fashion, enabling fast inference and strong performance on non-causal tasks. However, a crucial limitation is that they do not support token insertions and are thus limited to fixed-length generations. To this end, we introduce Flexible Masked Diffusion Models (FlexMDMs), a discrete diffusion paradigm that simultaneously can model sequences of flexible length while provably retaining MDMs' flexibility of any-order inference. Grounded in an extension of the stochastic interpolant framework, FlexMDMs generate sequences by inserting mask tokens and unmasking them. Empirically, we show that FlexMDMs match MDMs in perplexity while modeling length statistics with much higher fidelity. On a synthetic maze planning task, they achieve $\approx 60 \%$ higher success rate than MDM baselines. Finally, we show pretrained MDMs can easily be retrofitted into FlexMDMs: on 16 H100s, it takes only three days to fine-tune LLaDA-8B into a FlexMDM, achieving superior performance on math (GSM8K, $58\% \to 67\%$) and code infilling performance ($52\% \to 65\%$).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing In-Context Learning for Efficient Full Conformal Prediction</title>
<link>https://arxiv.org/abs/2509.01840</link>
<guid>https://arxiv.org/abs/2509.01840</guid>
<content:encoded><![CDATA[
<div> Transformer-based, Uncertainty quantification, Conformal Prediction, Meta-learning, In-context learning 

Summary:
Enhanced ICL-based FCP (E-ICL+FCP) is introduced as an efficient Full Conformal Prediction (FCP) framework that addresses the limitations of existing approaches. Utilizing a permutation-invariant Transformer-based In-Context Learning (ICL) model trained with a CP-aware loss, E-ICL+FCP achieves distribution-free coverage guarantees while reducing inefficiency and computational overhead. By simulating retrained models without actually retraining, E-ICL+FCP improves data efficiency and maintains coverage levels. Experimental results on synthetic and real tasks demonstrate that E-ICL+FCP outperforms Split CP (SCP) and traditional FCP baselines in terms of efficiency-coverage trade-offs. This framework offers a promising approach for reliable uncertainty quantification in AI applications. 

Summary: <br /><br />Enhanced ICL-based FCP (E-ICL+FCP) introduces an efficient Full Conformal Prediction (FCP) framework utilizing a Transformer-based In-Context Learning (ICL) model trained with CP-aware loss. It reduces inefficiency and computational overhead while maintaining distribution-free coverage guarantees. Experimental results show superior efficiency-coverage trade-offs compared to SCP and traditional FCP baselines, making it a promising approach for reliable uncertainty quantification in AI applications. <div>
arXiv:2509.01840v2 Announce Type: replace 
Abstract: Reliable uncertainty quantification is critical for trustworthy AI. Conformal Prediction (CP) provides prediction sets with distribution-free coverage guarantees, but its two main variants face complementary limitations. Split CP (SCP) suffers from data inefficiency due to dataset partitioning, while full CP (FCP) improves data efficiency at the cost of prohibitive retraining complexity. Recent approaches based on meta-learning or in-context learning (ICL) partially mitigate these drawbacks. However, they rely on training procedures not specifically tailored to CP, which may yield large prediction sets. We introduce an efficient FCP framework, termed enhanced ICL-based FCP (E-ICL+FCP), which employs a permutation-invariant Transformer-based ICL model trained with a CP-aware loss. By simulating the multiple retrained models required by FCP without actual retraining, E-ICL+FCP preserves coverage while markedly reducing both inefficiency and computational overhead. Experiments on synthetic and real tasks demonstrate that E-ICL+FCP attains superior efficiency-coverage trade-offs compared to existing SCP and FCP baselines.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Identification of IT Systems through Active Causal Learning</title>
<link>https://arxiv.org/abs/2509.02130</link>
<guid>https://arxiv.org/abs/2509.02130</guid>
<content:encoded><![CDATA[
<div> causal model, IT system, systems engineering, data-driven identification, active causal learning

Summary: 
The paper introduces a method called active causal learning for online, data-driven identification of a causal model of an IT system. This method, based on Gaussian process regression, estimates causal functions capturing system variable dependencies using system measurements collected through an intervention policy. The method is proven to be optimal in the Bayesian sense and effective in producing interventions. Experimental validation on a testbed confirms that accurate identification of a causal system model is achieved with minimal interference with system operations. The traditional approach of designing and maintaining causal models by domain experts is challenged by the complexity and dynamism of modern IT systems, highlighting the importance of developing automated methods for system management tasks. <div>
arXiv:2509.02130v2 Announce Type: replace 
Abstract: Identifying a causal model of an IT system is fundamental to many branches of systems engineering and operation. Such a model can be used to predict the effects of control actions, optimize operations, diagnose failures, detect intrusions, etc., which is central to achieving the longstanding goal of automating network and system management tasks. Traditionally, causal models have been designed and maintained by domain experts. This, however, proves increasingly challenging with the growing complexity and dynamism of modern IT systems. In this paper, we present the first principled method for online, data-driven identification of an IT system in the form of a causal model. The method, which we call active causal learning, estimates causal functions that capture the dependencies among system variables in an iterative fashion using Gaussian process regression based on system measurements, which are collected through a rollout-based intervention policy. We prove that this method is optimal in the Bayesian sense and that it produces effective interventions. Experimental validation on a testbed shows that our method enables accurate identification of a causal system model while inducing low interference with system operations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Playing Markov Games Without Observing Payoffs</title>
<link>https://arxiv.org/abs/2509.00179</link>
<guid>https://arxiv.org/abs/2509.00179</guid>
<content:encoded><![CDATA[
<div> Keywords: optimization, uncertainty, multi-agent systems, Markov games, online learning

Summary:
This paper introduces a new class of zero-sum symmetric Markov games that extends the notion of symmetry from matrix games to the Markovian setting. The research shows that a player can compete effectively against an adversary, even without observing payoffs, as long as they know the transition dynamics and observe the opponent's actions. Three distinct notions of symmetry are formalized, reducing the learning problem to an instance of online learning. This allows the player to asymptotically match the opponent's return despite the lack of payoff observations. The algorithms presented in the paper are applicable to both matrix and Markov games, running in polynomial time relative to the game's size and the number of episodes. This work expands the scope of games in which robust learning is possible under significant informational disadvantages, strengthening the link between online learning and adversarial game theory. 

<br /><br />Summary: <div>
arXiv:2509.00179v2 Announce Type: replace-cross 
Abstract: Optimization under uncertainty is a fundamental problem in learning and decision-making, particularly in multi-agent systems. Previously, Feldman, Kalai, and Tennenholtz [2010] demonstrated the ability to efficiently compete in repeated symmetric two-player matrix games without observing payoffs, as long as the opponents actions are observed. In this paper, we introduce and formalize a new class of zero-sum symmetric Markov games, which extends the notion of symmetry from matrix games to the Markovian setting. We show that even without observing payoffs, a player who knows the transition dynamics and observes only the opponents sequence of actions can still compete against an adversary who may have complete knowledge of the game. We formalize three distinct notions of symmetry in this setting and show that, under these conditions, the learning problem can be reduced to an instance of online learning, enabling the player to asymptotically match the return of the opponent despite lacking payoff observations. Our algorithms apply to both matrix and Markov games, and run in polynomial time with respect to the size of the game and the number of episodes. Our work broadens the class of games in which robust learning is possible under severe informational disadvantage and deepens the connection between online learning and adversarial game theory.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning</title>
<link>https://arxiv.org/abs/2509.00457</link>
<guid>https://arxiv.org/abs/2509.00457</guid>
<content:encoded><![CDATA[
<div> Islamic inheritance law, AI framework, Arabic text encoder, Attentive Relevance Scoring, multiple-choice questions

Summary: 
This paper introduces a lightweight framework for addressing Islamic inheritance law questions utilizing a specialized Arabic text encoder and Attentive Relevance Scoring (ARS). The system ranks answer options based on semantic relevance, enabling fast inference without generative reasoning. Evaluation on the QIAS 2025 dataset shows that while large language models achieve higher accuracy, they are resource-intensive and context-dependent. The MARBERT-based approach achieves a 69.87% accuracy, demonstrating efficiency, on-device deployability, and privacy advantages. Despite a lower accuracy compared to large models, this work highlights the trade-off between peak performance and practical benefits of smaller, specialized systems in critical domains. <div>
arXiv:2509.00457v2 Announce Type: replace-cross 
Abstract: Islamic inheritance law (Ilm al-Mawarith) requires precise identification of heirs and calculation of shares, which poses a challenge for AI. In this paper, we present a lightweight framework for solving multiple-choice inheritance questions using a specialised Arabic text encoder and Attentive Relevance Scoring (ARS). The system ranks answer options according to semantic relevance, and enables fast, on-device inference without generative reasoning. We evaluate Arabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based LLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an accuracy of up to 87.6%, they require more resources and are context-dependent. Our MARBERT-based approach achieves 69.87% accuracy, presenting a compelling case for efficiency, on-device deployability, and privacy. While this is lower than the 87.6% achieved by the best-performing LLM, our work quantifies a critical trade-off between the peak performance of large models and the practical advantages of smaller, specialized systems in high-stakes domains.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCPO: Dynamic Clipping Policy Optimization</title>
<link>https://arxiv.org/abs/2509.02333</link>
<guid>https://arxiv.org/abs/2509.02333</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Verifiable Rewards, Dynamic Clipping Policy Optimization, Large Language Models, Performance Improvement<br />
Summary:<br />
Dynamic Clipping Policy Optimization (DCPO) addresses the challenges faced by existing Reinforcement Learning from Verifiable Rewards (RLVR) frameworks by introducing adaptive clipping bounds based on token-specific probabilities and smooth reward standardization techniques. This approach enhances token-level exploration and response-level utilization in large language models, leading to significant performance improvements across multiple benchmarks. DCPO outperforms previous methods such as GRPO and DAPO, achieving state-of-the-art results in terms of Avg@1 and Avg@32 on various datasets. The proposed method also demonstrates a 28% increase in nonzero advantage over GRPO, doubles training efficiency compared to DAPO, and significantly reduces token clipping ratios while maintaining superior performance. These results highlight DCPO's effectiveness in efficiently leveraging generated data for reinforcement learning in large language models. <br /> <div>
arXiv:2509.02333v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization(DCPO), which introduces a dynamic clipping strategy that adaptively adjusts clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing DAPO (36.7/31.6), GRPO (36.7/32.1) and GSPO (40.0/34.9) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5), DAPO (20.0/15.3) and GSPO (16.7/9.9). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wild Refitting for Model-Free Excess Risk Evaluation of Opaque ML/AI Models under Bregman Loss</title>
<link>https://arxiv.org/abs/2509.02476</link>
<guid>https://arxiv.org/abs/2509.02476</guid>
<content:encoded><![CDATA[
<div> Bregman losses, excess risk, wild refitting, model-free, high-probability performance guarantees <br />
<br />
Summary: 
The study focuses on evaluating excess risk in penalized empirical risk minimization with Bregman losses. By utilizing the wild refitting procedure, the excess risk can be efficiently upper bounded through "wild optimism" without requiring knowledge of the function class structure. This approach is model-free and operates with just one dataset and black-box access. The method involves symmetrization, scaling of prediction residues, and retraining a second predictor. The analysis provides high-probability performance guarantees for both fixed and random design settings. The findings show that wild refitting under Bregman losses, with appropriate noise scaling, offers a valid upper bound on excess risk. This work is significant for evaluating complex ML and AI models where traditional learning theory may not apply. <div>
arXiv:2509.02476v3 Announce Type: replace-cross 
Abstract: We study the problem of evaluating the excess risk of classical penalized empirical risk minimization (ERM) with Bregman losses. We show that by leveraging the recently proposed wild refitting procedure (Wainwright, 2025), one can efficiently upper bound the excess risk through the so-called "wild optimism," without relying on the global structure of the underlying function class. This property makes our approach inherently model-free. Unlike conventional analyses, our framework operates with just one dataset and black-box access to the training procedure. The method involves randomized vector-valued symmetrization with an appropriate scaling of the prediction residues and constructing artificially modified outcomes, upon which we retrain a second predictor for excess risk estimation. We establish high-probability performance guarantees both under the fixed design setting and the random design setting, demonstrating that wild refitting under Bregman losses, with an appropriately chosen wild noise scale, yields a valid upper bound on the excess risk. This work thus is promising for theoretically evaluating modern opaque ML and AI models such as deep neural networks and large language models, where the model class is too complex for classical learning theory and empirical process techniques to apply.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning</title>
<link>https://arxiv.org/abs/2509.05316</link>
<guid>https://arxiv.org/abs/2509.05316</guid>
<content:encoded><![CDATA[
<div> Neighbor Sets, Unlearning, General Knowledge, Sampling Methods, Modular Entity-Level Unlearning (MELU)<br />
<br />
Summary: 
1. Relying on a single neighbor set in LLM Unlearning is suboptimal, advocating for diverse neighbor sets to balance forget efficacy and model utility.
2. Standard 1:1 sampling methods are shown to be inefficient and yield poor results, prompting the need for more effective sampling strategies.
3. The study proposes Modular Entity-Level Unlearning (MELU) as an alternative to cyclic sampling, providing a stable path towards effective unlearning.
4. The research systematically evaluates common practices in privacy-focused unlearning settings, highlighting the importance of incorporating diverse neighbor sets for better outcomes.
5. The efficacy and stability of de facto standards in LLM Unlearning, such as 1:1 sampling or cyclic iteration sampling, are questioned, urging a reevaluation of existing practices for improved performance. <br /> <div>
arXiv:2509.05316v1 Announce Type: new 
Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and "retain", with the objectives of removing the undesired knowledge from the forget set while preserving the remaining knowledge from the retain. In privacy-focused unlearning research, a retain set is often further divided into neighbor sets, containing either directly or indirectly connected to the forget targets; and augmented by a general-knowledge set. A common practice in existing benchmarks is to employ only a single neighbor set, with general knowledge which fails to reflect the real-world data complexities and relationships. LLM Unlearning typically involves 1:1 sampling or cyclic iteration sampling. However, the efficacy and stability of these de facto standards have not been critically examined. In this study, we systematically evaluate these common practices. Our findings reveal that relying on a single neighbor set is suboptimal and that a standard sampling approach can obscure performance trade-offs. Based on this analysis, we propose and validate an initial set of best practices: (1) Incorporation of diverse neighbor sets to balance forget efficacy and model utility, (2) Standard 1:1 sampling methods are inefficient and yield poor results, (3) Our proposed Modular Entity-Level Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate that this modular approach, combined with robust algorithms, provides a clear and stable path towards effective unlearning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance</title>
<link>https://arxiv.org/abs/2509.05328</link>
<guid>https://arxiv.org/abs/2509.05328</guid>
<content:encoded><![CDATA[
<div> Regularization, Robust fine-tuning, Out-of-distribution robustness, Function space, Pre-trained model<br />
<br />
Summary:<br />
- Existing robust fine-tuning methods may not always improve out-of-distribution (OOD) robustness for different model architectures.
- A novel regularization approach is proposed to constrain the distance of fine-tuning and pre-trained model in the function space with simulated OOD samples.
- An additional consistency regularization is introduced to promote stable predictions of perturbed samples, enhancing OOD robustness capability.
- Extensive experiments show that the proposed approach improves both in-distribution (ID) fine-tuning performance and OOD robustness across various CLIP backbones.
- Outperforms existing regularization-based robust fine-tuning methods.  <br /> <div>
arXiv:2509.05328v1 Announce Type: new 
Abstract: Robust fine-tuning aims to achieve competitive in-distribution (ID) performance while maintaining the out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task. To remedy this, most robust fine-tuning methods aim to preserve the pretrained weights, features, or logits. However, we find that these methods cannot always improve OOD robustness for different model architectures. This is due to the OOD robustness requiring the model function to produce stable prediction for input information of downstream tasks, while existing methods might serve as a poor proxy for the optimization in the function space. Based on this finding, we propose a novel regularization that constrains the distance of fine-tuning and pre-trained model in the function space with the simulated OOD samples, aiming to preserve the OOD robustness of the pre-trained model. Besides, to further enhance the OOD robustness capability of the fine-tuning model, we introduce an additional consistency regularization to promote stable predictions of perturbed samples. Extensive experiments demonstrate our approach could consistently improve both downstream task ID fine-tuning performance and OOD robustness across a variety of CLIP backbones, outperforming existing regularization-based robust fine-tuning methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safeguarding Graph Neural Networks against Topology Inference Attacks</title>
<link>https://arxiv.org/abs/2509.05429</link>
<guid>https://arxiv.org/abs/2509.05429</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Topology Privacy, Inference Attacks, Privacy-preserving, Model Accuracy<br />
<br />
Summary: 
Graph Neural Networks (GNNs) are powerful models for learning from graph-structured data, but they face serious privacy concerns, including topology privacy risks. This work focuses on the vulnerability of GNNs to Topology Inference Attacks (TIAs) that can reconstruct the structure of a target training graph. Existing edge-level privacy mechanisms are inadequate in mitigating this risk without compromising model accuracy. To address this challenge, a novel defense framework called Private Graph Reconstruction (PGR) is introduced. PGR is formulated as a bi-level optimization problem that generates a synthetic training graph to protect topology privacy while maintaining model accuracy. Extensive experiments demonstrate that PGR effectively reduces topology leakage with minimal impact on model accuracy. The code for PGR is publicly available for further research and development. <div>
arXiv:2509.05429v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning from graph-structured data. However, their widespread adoption has raised serious privacy concerns. While prior research has primarily focused on edge-level privacy, a critical yet underexplored threat lies in topology privacy - the confidentiality of the graph's overall structure. In this work, we present a comprehensive study on topology privacy risks in GNNs, revealing their vulnerability to graph-level inference attacks. To this end, we propose a suite of Topology Inference Attacks (TIAs) that can reconstruct the structure of a target training graph using only black-box access to a GNN model. Our findings show that GNNs are highly susceptible to these attacks, and that existing edge-level differential privacy mechanisms are insufficient as they either fail to mitigate the risk or severely compromise model accuracy. To address this challenge, we introduce Private Graph Reconstruction (PGR), a novel defense framework designed to protect topology privacy while maintaining model accuracy. PGR is formulated as a bi-level optimization problem, where a synthetic training graph is iteratively generated using meta-gradients, and the GNN model is concurrently updated based on the evolving graph. Extensive experiments demonstrate that PGR significantly reduces topology leakage with minimal impact on model accuracy. Our code is anonymously available at https://github.com/JeffffffFu/PGR.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis</title>
<link>https://arxiv.org/abs/2509.05449</link>
<guid>https://arxiv.org/abs/2509.05449</guid>
<content:encoded><![CDATA[
<div> membership inference attacks, privacy auditing, machine learning models, large language models, memTrace

Summary:
- Membership inference attacks (MIAs) are important for privacy auditing and compliance assessment in machine learning models.
- Recent studies suggest that large language models may not be vulnerable to MIAs, but examining their internal representations using frameworks like memTrace can reveal potential privacy risks.
- MemTrace analyzes hidden states and attention patterns in transformer models to detect memorization fingerprints that traditional approaches may miss.
- By analyzing layer-wise representation dynamics and attention distribution characteristics, memTrace achieves strong membership detection performance on MIA benchmarks.
- The findings emphasize the need for further research on membership privacy and the development of more robust privacy-preserving training techniques for large language models. 

<br /><br />Summary: <div>
arXiv:2509.05449v1 Announce Type: new 
Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to train machine learning models, serving as important tools for privacy auditing and compliance assessment. Recent studies have reported that MIAs perform only marginally better than random guessing against large language models, suggesting that modern pre-training approaches with massive datasets may be free from privacy leakage risks. Our work offers a complementary perspective to these findings by exploring how examining LLMs' internal representations, rather than just their outputs, may provide additional insights into potential membership inference signals. Our framework, \emph{memTrace}, follows what we call \enquote{neural breadcrumbs} extracting informative signals from transformer hidden states and attention patterns as they process candidate sequences. By analyzing layer-wise representation dynamics, attention distribution characteristics, and cross-layer transition patterns, we detect potential memorization fingerprints that traditional loss-based approaches may not capture. This approach yields strong membership detection across several model families achieving average AUC scores of 0.85 on popular MIA benchmarks. Our findings suggest that internal model behaviors can reveal aspects of training data exposure even when output-based signals appear protected, highlighting the need for further research into membership privacy and the development of more robust privacy-preserving training techniques for large language models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrated Recommendations with Contextual Bandits</title>
<link>https://arxiv.org/abs/2509.05460</link>
<guid>https://arxiv.org/abs/2509.05460</guid>
<content:encoded><![CDATA[
<div> content types, contextual bandits, user engagement, personalized, Spotify<br />
Summary:<br />
The article discusses the challenges faced by Spotify in delivering a balanced and personalized mix of content types on its Home page, with historical data heavily skewed towards music. The proposed calibration method leverages contextual bandits to dynamically learn each user's optimal content type distribution based on context and preferences. This adaptive approach improves precision and user engagement by adjusting to users' varying interests in different content types across contexts. Unlike traditional methods that rely on historical averages, this method enhances engagement with under-represented content types, such as podcasts, and shows promising results both offline and online. <div>
arXiv:2509.05460v1 Announce Type: new 
Abstract: Spotify's Home page features a variety of content types, including music, podcasts, and audiobooks. However, historical data is heavily skewed toward music, making it challenging to deliver a balanced and personalized content mix. Moreover, users' preference towards different content types may vary depending on the time of day, the day of week, or even the device they use. We propose a calibration method that leverages contextual bandits to dynamically learn each user's optimal content type distribution based on their context and preferences. Unlike traditional calibration methods that rely on historical averages, our approach boosts engagement by adapting to how users interests in different content types varies across contexts. Both offline and online results demonstrate improved precision and user engagement with the Spotify Home page, in particular with under-represented content types such as podcasts.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series</title>
<link>https://arxiv.org/abs/2509.05478</link>
<guid>https://arxiv.org/abs/2509.05478</guid>
<content:encoded><![CDATA[
<div> periodic structure, self-supervised learning, multivariate time series, latent states, dynamic evolution <br />
Summary: <br />
The article introduces PLanTS, a novel self-supervised learning framework for multivariate time series data that leverages the periodic structure present in such data. PLanTS addresses the challenges of high dimensionality, limited labeled data, and non-stationarity by explicitly modeling irregular latent states and their transitions. It incorporates a period-aware multi-granularity patching mechanism and a generalized contrastive loss to capture similarities at different temporal resolutions. Furthermore, PLanTS includes a next-transition prediction task to capture the temporal dynamics of the data. Experimental results demonstrate that PLanTS outperforms existing SSL methods in various downstream tasks such as classification, forecasting, trajectory tracking, and anomaly detection. Additionally, PLanTS shows superior runtime efficiency compared to Dynamic Time Warping (DTW)-based methods. <div>
arXiv:2509.05478v1 Announce Type: new 
Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare, climate science, and industrial monitoring, but their high dimensionality, limited labeled data, and non-stationary nature pose significant challenges for conventional machine learning methods. While recent self-supervised learning (SSL) approaches mitigate label scarcity by data augmentations or time point-based contrastive strategy, they neglect the intrinsic periodic structure of MTS and fail to capture the dynamic evolution of latent states. We propose PLanTS, a periodicity-aware self-supervised learning framework that explicitly models irregular latent states and their transitions. We first designed a period-aware multi-granularity patching mechanism and a generalized contrastive loss to preserve both instance-level and state-level similarities across multiple temporal resolutions. To further capture temporal dynamics, we design a next-transition prediction pretext task that encourages representations to encode predictive information about future state evolution. We evaluate PLanTS across a wide range of downstream tasks-including multi-class and multi-label classification, forecasting, trajectory tracking and anomaly detection. PLanTS consistently improves the representation quality over existing SSL methods and demonstrates superior runtime efficiency compared to DTW-based methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STL-based Optimization of Biomolecular Neural Networks for Regression and Control</title>
<link>https://arxiv.org/abs/2509.05481</link>
<guid>https://arxiv.org/abs/2509.05481</guid>
<content:encoded><![CDATA[
<div> Biomolecular Neural Networks, Signal Temporal Logic, training objectives, regression, control<br />
Summary:<br />
The article introduces Biomolecular Neural Networks (BNNs) with biologically synthesizable architectures that can accomplish universal function approximation. Despite challenges in training BNNs due to limited target data, the use of Signal Temporal Logic (STL) specifications as training objectives is proposed. The quantitative semantics of STL support gradient-based optimization of BNN weights, enabling efficient training for regression and control tasks in biological systems. Two regression problems are explored, involving training BNNs to detect dysregulated states. In a feedback control task, a BNN is trained in closed-loop with a chronic disease model to reduce inflammation and mitigate adverse responses to external infections. Numerical experiments demonstrate the efficacy of STL-based learning in efficiently solving the regression and control tasks.<br /><br />Summary: <div>
arXiv:2509.05481v1 Announce Type: new 
Abstract: Biomolecular Neural Networks (BNNs), artificial neural networks with biologically synthesizable architectures, achieve universal function approximation capabilities beyond simple biological circuits. However, training BNNs remains challenging due to the lack of target data. To address this, we propose leveraging Signal Temporal Logic (STL) specifications to define training objectives for BNNs. We build on the quantitative semantics of STL, enabling gradient-based optimization of the BNN weights, and introduce a learning algorithm that enables BNNs to perform regression and control tasks in biological systems. Specifically, we investigate two regression problems in which we train BNNs to act as reporters of dysregulated states, and a feedback control problem in which we train the BNN in closed-loop with a chronic disease model, learning to reduce inflammation while avoiding adverse responses to external infections. Our numerical experiments demonstrate that STL-based learning can solve the investigated regression and control tasks efficiently.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prior Distribution and Model Confidence</title>
<link>https://arxiv.org/abs/2509.05485</link>
<guid>https://arxiv.org/abs/2509.05485</guid>
<content:encoded><![CDATA[
<div> Keywords: training data distribution, image classification models, confidence, embedding space, out-of-distribution samples

Summary:
This paper explores how the distribution of training data impacts the performance of image classification models. By analyzing the embeddings of the training set, a framework is proposed to assess model prediction confidence on unseen data without requiring retraining. Low-confidence predictions are filtered based on their distance from the training distribution in the embedding space, leading to significant improvements in classification accuracy. Multiple embedding models are used to enhance the estimation of confidence, as different embeddings capture diverse data aspects. By combining these embeddings, better detection and exclusion of out-of-distribution samples are achieved, further boosting accuracy. The method is versatile across various classification model architectures and has potential applications beyond computer vision, extending to critical domains like Natural Language Processing where prediction reliability is crucial.

<br /><br />Summary: <div>
arXiv:2509.05485v1 Announce Type: new 
Abstract: This paper investigates the impact of training data distribution on the performance of image classification models. By analyzing the embeddings of the training set, we propose a framework to understand the confidence of model predictions on unseen data without the need for retraining. Our approach filters out low-confidence predictions based on their distance from the training distribution in the embedding space, significantly improving classification accuracy. We demonstrate this on the example of several classification models, showing consistent performance gains across architectures. Furthermore, we show that using multiple embedding models to represent the training data enables a more robust estimation of confidence, as different embeddings capture complementary aspects of the data. Combining these embeddings allows for better detection and exclusion of out-of-distribution samples, resulting in further accuracy improvements. The proposed method is model-agnostic and generalizable, with potential applications beyond computer vision, including domains such as Natural Language Processing where prediction reliability is critical.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs</title>
<link>https://arxiv.org/abs/2509.05488</link>
<guid>https://arxiv.org/abs/2509.05488</guid>
<content:encoded><![CDATA[
<div> MCU, MambaLite-Micro, PyTorch, resource-constrained, deployment   
Summary:   
Mamba models deployment on microcontrollers (MCUs) faces challenges due to limited memory and lack of support. MambaLite-Micro is introduced as a C-based runtime-free inference engine for executing Mamba models on MCUs. The pipeline involves exporting model weights to a lightweight format and implementing handmade Mamba layers and operators in C for optimized performance. MambaLite-Micro reduces peak memory usage by 83.0% while maintaining low numerical error relative to PyTorch. It achieves 100% consistency with PyTorch baselines on keyword spotting and human activity recognition tasks, preserving classification accuracy. The portability of MambaLite-Micro is validated on different microcontrollers, showcasing its consistent operation across various embedded platforms. This advancement enables the deployment of advanced sequence models like Mamba in real-world resource-constrained applications.   
<br /><br />Summary: <div>
arXiv:2509.05488v1 Announce Type: new 
Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to limited memory, the lack of native operator support, and the absence of embedded-friendly toolchains. We present, to our knowledge, the first deployment of a Mamba-based neural architecture on a resource-constrained MCU, a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline maps a trained PyTorch Mamba model to on-device execution by (1) exporting model weights into a lightweight format, and (2) implementing a handcrafted Mamba layer and supporting operators in C with operator fusion and memory layout optimization. MambaLite-Micro eliminates large intermediate tensors, reducing 83.0% peak memory, while maintaining an average numerical error of only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on keyword spotting(KWS) and human activity recognition (HAR) tasks, MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully preserving classification accuracy. We further validated portability by deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating consistent operation across heterogeneous embedded platforms and paving the way for bringing advanced sequence models like Mamba to real-world resource-constrained applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Aligned Reward: Towards Effective and Efficient Reasoners</title>
<link>https://arxiv.org/abs/2509.05489</link>
<guid>https://arxiv.org/abs/2509.05489</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, self-aligned reward, verifiable rewards, efficiency<br />
<br />
Summary: <br />
The article introduces self-aligned reward (SAR) as a complement to verifiable rewards in reinforcement learning for large language models. SAR aims to improve reasoning accuracy and efficiency by encouraging concise and query-specific responses. Quantitative analysis shows that SAR effectively distinguishes answer quality and improves model accuracy by 4%, while reducing inference cost by 30%. SAR achieves a Pareto-optimal trade-off between correctness and efficiency compared to other reward signals. Integration with RL algorithms like PPO and GRPO demonstrates SAR's ability to shorten responses without losing critical reasoning. Overall, SAR shows promise in enhancing LLM training by providing a fine-grained reward signal that promotes efficient and effective reasoning. <br /> <div>
arXiv:2509.05489v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards has significantly advanced reasoning in large language models (LLMs), but such signals remain coarse, offering only binary correctness feedback. This limitation often results in inefficiencies, including overly verbose reasoning and high computational cost, while existing solutions often compromise accuracy. To address this, we introduce self-aligned reward (SAR), a self-guided signal that complements verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is defined as the relative perplexity difference between an answer conditioned on the query and the standalone answer, thereby favoring responses that are concise and query-specific. Quantitative analysis reveals that SAR reliably distinguishes answer quality: concise, correct answers score higher than redundant ones, and partially correct answers score higher than entirely incorrect ones. Evaluation on 4 models across 7 benchmarks shows that integrating SAR with prevalent RL algorithms like PPO and GRPO improves accuracy by 4%, while reducing inference cost by 30%. Further analysis demonstrates that SAR achieves a Pareto-optimal trade-off between correctness and efficiency compared to reward signals based on length or self-confidence. We also show that SAR shortens responses while preserving advanced reasoning behaviors, demonstrating its ability to suppress unnecessary elaboration without losing critical reasoning. These results highlight the promise of self-aligned reward as a fine-grained complement to verifiable rewards, paving the way for more efficient and effective LLM training.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training</title>
<link>https://arxiv.org/abs/2509.05542</link>
<guid>https://arxiv.org/abs/2509.05542</guid>
<content:encoded><![CDATA[
<div> Instance Table, Instance Net, DreamPRM-1.5, multimodal process reward models, bi-level optimization <br />
<br />
Summary: 
The article introduces DreamPRM-1.5, a framework designed to address challenges in training multimodal process reward models (PRMs) stemming from distribution shifts and noisy data. DreamPRM-1.5 utilizes instance reweighting through bi-level optimization to dynamically adjust the importance of each training example. The framework incorporates two strategies, Instance Table for smaller datasets and Instance Net for larger ones. Through integration into test-time scaling, DreamPRM-1.5 achieves an accuracy of 84.6 on the MMMU benchmark, outperforming GPT-5. The framework demonstrates effectiveness in handling diverse data scenarios and showcases improved performance in multimodal process reward model training tasks. <div>
arXiv:2509.05542v1 Announce Type: new 
Abstract: Training multimodal process reward models (PRMs) is challenged by distribution shifts and noisy data. We introduce DreamPRM-1.5, an instance-reweighted framework that adaptively adjusts the importance of each training example via bi-level optimization. We design two complementary strategies: Instance Table, effective for smaller datasets, and Instance Net, scalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5 achieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2509.05545</link>
<guid>https://arxiv.org/abs/2509.05545</guid>
<content:encoded><![CDATA[
<div> hierarchical reinforcement learning, goal-conditioned tasks, Reinforcement Learning with Anticipation, anticipatory model, value geometric consistency <br />
<br />
Summary: <br />
Solving long-horizon goal-conditioned tasks in reinforcement learning can be challenging. While hierarchical reinforcement learning (HRL) breaks tasks into sub-tasks, automatic hierarchy discovery and multi-level policy training can be unstable. A new framework, Reinforcement Learning with Anticipation (RLA), introduces a low-level policy for reaching subgoals and a high-level anticipation model that plans intermediate subgoals. RLA's key feature is the anticipation model training, guided by value geometric consistency to prevent degenerate solutions. The RLA is proven to approach the globally optimal policy under certain conditions, providing a convergent and principled method for hierarchical planning and execution in long-horizon goal-conditioned tasks. <div>
arXiv:2509.05545v1 Announce Type: new 
Abstract: Solving long-horizon goal-conditioned tasks remains a significant challenge in reinforcement learning (RL). Hierarchical reinforcement learning (HRL) addresses this by decomposing tasks into more manageable sub-tasks, but the automatic discovery of the hierarchy and the joint training of multi-level policies often suffer from instability and can lack theoretical guarantees. In this paper, we introduce Reinforcement Learning with Anticipation (RLA), a principled and potentially scalable framework designed to address these limitations. The RLA agent learns two synergistic models: a low-level, goal-conditioned policy that learns to reach specified subgoals, and a high-level anticipation model that functions as a planner, proposing intermediate subgoals on the optimal path to a final goal. The key feature of RLA is the training of the anticipation model, which is guided by a principle of value geometric consistency, regularized to prevent degenerate solutions. We present proofs that RLA approaches the globally optimal policy under various conditions, establishing a principled and convergent method for hierarchical planning and execution in long-horizon goal-conditioned tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization</title>
<link>https://arxiv.org/abs/2509.05584</link>
<guid>https://arxiv.org/abs/2509.05584</guid>
<content:encoded><![CDATA[
<div> compression, profiling, pruning, quantization, large language models 

Summary: 
ProfilingAgent introduces a profiling-guided approach to automate compression of large language models. By considering both static metrics (MACs, parameter counts) and dynamic signals (latency, memory), the system tailors compression strategies to address specific bottlenecks in architecture. Experimental results on various datasets and models demonstrate that pruning can maintain competitive accuracy levels while quantization achieves significant memory savings with minimal accuracy loss. The proposed quantization method also consistently improves inference speed. Comparative studies highlight the effectiveness of the agentic system in optimizing model performance through the use of large language models for reasoning. Overall, ProfilingAgent proves to be a scalable solution for profiling-guided model optimization. 

<br /><br />Summary: <div>
arXiv:2509.05584v1 Announce Type: new 
Abstract: Foundation models face growing compute and memory bottlenecks, hindering deployment on resource-limited platforms. While compression techniques such as pruning and quantization are widely used, most rely on uniform heuristics that ignore architectural and runtime heterogeneity. Profiling tools expose per-layer latency, memory, and compute cost, yet are rarely integrated into automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic approach that uses large language models (LLMs) to automate compression via structured pruning and post-training dynamic quantization. Our modular multi-agent system reasons over static metrics (MACs, parameter counts) and dynamic signals (latency, memory) to design architecture-specific strategies. Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on smaller datasets), while quantization achieves up to 74% memory savings with <0.5% accuracy loss. Our quantization also yields consistent inference speedups of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo highlight the importance of LLM reasoning quality for iterative pruning. These results establish agentic systems as scalable solutions for profiling-guided model optimization.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities</title>
<link>https://arxiv.org/abs/2509.05615</link>
<guid>https://arxiv.org/abs/2509.05615</guid>
<content:encoded><![CDATA[
<div> bias, missing modalities, multimodal representation learning, causal analysis, neural network

Summary: 
This article discusses the challenges faced in medical multimodal representation learning due to missing modalities and biases in data acquisition. Two key biases - missingness bias and distribution bias - hinder model generalization. The proposed framework includes a missingness deconfounding module that addresses missingness bias through causal intervention and a dual-branch neural network that disentangles causal features from spurious correlations. By performing a structural causal analysis of the data-generating process, the framework aims to improve model generalization and provide causal insights. Evaluation on real-world public and in-hospital datasets demonstrates the effectiveness of the proposed method in addressing the challenges of missing modalities and biases in medical data mining. <div>
arXiv:2509.05615v1 Announce Type: new 
Abstract: Medical multimodal representation learning aims to integrate heterogeneous clinical data into unified patient representations to support predictive modeling, which remains an essential yet challenging task in the medical data mining community. However, real-world medical datasets often suffer from missing modalities due to cost, protocol, or patient-specific constraints. Existing methods primarily address this issue by learning from the available observations in either the raw data space or feature space, but typically neglect the underlying bias introduced by the data acquisition process itself. In this work, we identify two types of biases that hinder model generalization: missingness bias, which results from non-random patterns in modality availability, and distribution bias, which arises from latent confounders that influence both observed features and outcomes. To address these challenges, we perform a structural causal analysis of the data-generating process and propose a unified framework that is compatible with existing direct prediction-based multimodal learning methods. Our method consists of two key components: (1) a missingness deconfounding module that approximates causal intervention based on backdoor adjustment and (2) a dual-branch neural network that explicitly disentangles causal features from spurious correlations. We evaluated our method in real-world public and in-hospital datasets, demonstrating its effectiveness and causal insights.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search</title>
<link>https://arxiv.org/abs/2509.05656</link>
<guid>https://arxiv.org/abs/2509.05656</guid>
<content:encoded><![CDATA[
<div> OptiProxy-NAS, neural architecture search, optimization proxy, NAS tasks, search spaces <br />
Summary: <br />
The research introduces OptiProxy-NAS, an end-to-end optimization framework for accelerating neural architecture search. Unlike existing predictor-based methods, OptiProxy-NAS utilizes an optimization proxy to transform the NAS space into a continuous, differentiable, and smooth representation. This enables the application of gradient-based search methods for efficient exploration of neural architectures. Extensive experiments across 12 NAS tasks in computer vision, natural language processing, and resource-constrained NAS domains demonstrate the effectiveness and efficiency of OptiProxy-NAS. Additionally, experiments in low-fidelity scenarios highlight the flexibility of the proposed framework. The results showcase superior search outcomes and efficiency compared to traditional methods, positioning OptiProxy-NAS as a promising approach for streamlining the challenging optimization problem of NAS. <br /> <div>
arXiv:2509.05656v1 Announce Type: new 
Abstract: Neural architecture search (NAS) is a hard computationally expensive optimization problem with a discrete, vast, and spiky search space. One of the key research efforts dedicated to this space focuses on accelerating NAS via certain proxy evaluations of neural architectures. Different from the prevalent predictor-based methods using surrogate models and differentiable architecture search via supernetworks, we propose an optimization proxy to streamline the NAS as an end-to-end optimization framework, named OptiProxy-NAS. In particular, using a proxy representation, the NAS space is reformulated to be continuous, differentiable, and smooth. Thereby, any differentiable optimization method can be applied to the gradient-based search of the relaxed architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$ search spaces across three different domains including computer vision, natural language processing, and resource-constrained NAS fully demonstrate the superior search results and efficiency. Further experiments on low-fidelity scenarios verify the flexibility.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches</title>
<link>https://arxiv.org/abs/2509.05663</link>
<guid>https://arxiv.org/abs/2509.05663</guid>
<content:encoded><![CDATA[
<div> Active learning, unsupervised anomaly detection, time series, query strategy, dynamic time warping <br />
Summary: <br />
- Truly unsupervised approaches for time series anomaly detection are rare in the literature.
- Existing methods suffer from poorly set thresholds, limiting detection performance.
- Integration of active learning with unsupervised anomaly detection shows promise.
- The dissimilarity-based query strategy (DQS) maximizes diversity of queried samples.
- DQS outperforms other query strategies in small-budget scenarios.
- Other query strategies are more robust in the face of mislabeling.
- Active learning-based thresholds perform better than unsupervised thresholds, even with mislabeling.
- Choice of query strategy depends on oracle expertise and willingness to label samples.
- Active learning-based thresholding is recommended when feasible to query an oracle.  <div>
arXiv:2509.05663v1 Announce Type: new 
Abstract: Truly unsupervised approaches for time series anomaly detection are rare in the literature. Those that exist suffer from a poorly set threshold, which hampers detection performance, while others, despite claiming to be unsupervised, need to be calibrated using a labelled data subset, which is often not available in the real world. This work integrates active learning with an existing unsupervised anomaly detection method by selectively querying the labels of multivariate time series, which are then used to refine the threshold selection process. To achieve this, we introduce a novel query strategy called the dissimilarity-based query strategy (DQS). DQS aims to maximise the diversity of queried samples by evaluating the similarity between anomaly scores using dynamic time warping. We assess the detection performance of DQS in comparison to other query strategies and explore the impact of mislabelling, a topic that is underexplored in the literature. Our findings indicate that DQS performs best in small-budget scenarios, though the others appear to be more robust when faced with mislabelling. Therefore, in the real world, the choice of query strategy depends on the expertise of the oracle and the number of samples they are willing to label. Regardless, all query strategies outperform the unsupervised threshold even in the presence of mislabelling. Thus, whenever it is feasible to query an oracle, employing an active learning-based threshold is recommended.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR</title>
<link>https://arxiv.org/abs/2509.05671</link>
<guid>https://arxiv.org/abs/2509.05671</guid>
<content:encoded><![CDATA[
<div> Graph-based Multimodal Federated Learning, Human Activity Recognition, Differential Privacy, Graph Convolutional Neural Networks, Multimodal Sensor Data
Summary:
Graph-based Multimodal Federated Learning is proposed for Human Activity Recognition tasks, combining diverse sensor streams through Graph Convolutional Neural Networks (GCNs) and attention-based fusion. The MultiModalGCN model outperforms the baseline MultiModalFFN in both centralized and federated paradigms, with up to a 2 percent increase in accuracy. When subjected to differential privacy constraints, MultiModalGCN consistently outperforms MultiModalFFN by 7 to 13 percent. This demonstrates the robustness of graph-based modeling in multimodal learning, with Graph Neural Networks proving more resilient to performance degradation from differential privacy noise. <div>
arXiv:2509.05671v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) using multimodal sensor data remains challenging due to noisy or incomplete measurements, scarcity of labeled examples, and privacy concerns. Traditional centralized deep learning approaches are often constrained by infrastructure availability, network latency, and data sharing restrictions. While federated learning (FL) addresses privacy by training models locally and sharing only model parameters, it still has to tackle issues arising from the use of heterogeneous multimodal data and differential privacy requirements. In this article, a Graph-based Multimodal Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse sensor streams such as a pressure mat, depth camera, and multiple accelerometers are modeled as modality-specific graphs, processed through residual Graph Convolutional Neural Networks (GCNs), and fused via attention-based weighting rather than simple concatenation. The fused embeddings enable robust activity classification, while differential privacy safeguards data during federated aggregation. Experimental results show that the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with up to 2 percent higher accuracy in non-DP settings in both centralized and federated paradigms. More importantly, significant improvements are observed under differential privacy constraints: MultiModalGCN consistently surpasses MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on the privacy budget and setting. These results highlight the robustness of graph-based modeling in multimodal learning, where GNNs prove more resilient to the performance degradation introduced by DP noise.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Deep Learning using Stochastic Gradient Staleness</title>
<link>https://arxiv.org/abs/2509.05679</link>
<guid>https://arxiv.org/abs/2509.05679</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, distributed training method, data parallelism, backpropagation algorithm, CIFAR-10 dataset

Summary: 
This paper introduces a distributed training method that combines data parallelism and fully decoupled parallel backpropagation algorithm to accelerate deep learning. The approach allows multiple computational units to process training data in parallel, improving training efficiency while addressing locking issues. The method is proven to converge to critical points under specific conditions. Empirical evaluations demonstrate the effectiveness of the approach, showing improved performance in training a DNN on the CIFAR-10 dataset. <div>
arXiv:2509.05679v1 Announce Type: new 
Abstract: Despite the notable success of deep neural networks (DNNs) in solving complex tasks, the training process still remains considerable challenges. A primary obstacle is the substantial time required for training, particularly as high performing DNNs tend to become increasingly deep (characterized by a larger number of hidden layers) and require extensive training datasets. To address these challenges, this paper introduces a distributed training method that integrates two prominent strategies for accelerating deep learning: data parallelism and fully decoupled parallel backpropagation algorithm. By utilizing multiple computational units operating in parallel, the proposed approach enhances the amount of training data processed in each iteration while mitigating locking issues commonly associated with the backpropagation algorithm. These features collectively contribute to significant improvements in training efficiency. The proposed distributed training method is rigorously proven to converge to critical points under certain conditions. Its effectiveness is further demonstrated through empirical evaluations, wherein an DNN is trained to perform classification tasks on the CIFAR-10 dataset.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Morphological Perceptron with Competitive Layer: Training Using Convex-Concave Procedure</title>
<link>https://arxiv.org/abs/2509.05697</link>
<guid>https://arxiv.org/abs/2509.05697</guid>
<content:encoded><![CDATA[
<div> mathematical morphology, morphological perceptron, multiclass classification, convex-concave procedure, training

Summary:
The article introduces the concept of a morphological perceptron, which is a neural network incorporating mathematical morphology operations for classification tasks. A morphological perceptron with a competitive layer (MPCL) is proposed for multiclass classification. Due to the non-differentiability of morphological operators, gradient-based optimization methods are not suitable for training such networks. The paper suggests using the convex-concave procedure (CCP) to train MPCL networks, formulating the training problem as a difference of convex (DC) functions and solving it iteratively with linear programming subproblems. Computational experiments show the effectiveness of this training method for classification tasks with MPCL networks. <div>
arXiv:2509.05697v1 Announce Type: new 
Abstract: A morphological perceptron is a multilayer feedforward neural network in which neurons perform elementary operations from mathematical morphology. For multiclass classification tasks, a morphological perceptron with a competitive layer (MPCL) is obtained by integrating a winner-take-all output layer into the standard morphological architecture. The non-differentiability of morphological operators renders gradient-based optimization methods unsuitable for training such networks. Consequently, alternative strategies that do not depend on gradient information are commonly adopted. This paper proposes the use of the convex-concave procedure (CCP) for training MPCL networks. The training problem is formulated as a difference of convex (DC) functions and solved iteratively using CCP, resulting in a sequence of linear programming subproblems. Computational experiments demonstrate the effectiveness of the proposed training method in addressing classification tasks with MPCL networks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation Priors for Data-Efficient Deep Learning</title>
<link>https://arxiv.org/abs/2509.05732</link>
<guid>https://arxiv.org/abs/2509.05732</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, real-world learning, first-principles models, deep learning, SimPEL 

Summary: 
SimPEL is a novel method that combines first-principles models with data-driven learning to enable efficient AI system learning in real-world environments. By integrating low-fidelity simulators as priors in Bayesian deep learning, SimPEL can make use of simulator knowledge in low-data scenarios and transition to deep learning for more data-rich situations while quantifying uncertainty. The method was evaluated across diverse domains including biology, agriculture, and robotics, demonstrating superior performance in capturing complex dynamics. In decision-making tasks, SimPEL successfully bridges the simulation-to-reality gap in model-based reinforcement learning, as evidenced by its ability to learn a challenging parking maneuver on a high-speed RC car with minimal data compared to existing methods. SimPEL shows promise for data-efficient learning and control in complex real-world settings. 

<br /><br />Summary: <div>
arXiv:2509.05732v1 Announce Type: new 
Abstract: How do we enable AI systems to efficiently learn in the real-world? First-principles models are widely used to simulate natural systems, but often fail to capture real-world complexity due to simplifying assumptions. In contrast, deep learning approaches can estimate complex dynamics with minimal assumptions but require large, representative datasets. We propose SimPEL, a method that efficiently combines first-principles models with data-driven learning by using low-fidelity simulators as priors in Bayesian deep learning. This enables SimPEL to benefit from simulator knowledge in low-data regimes and leverage deep learning's flexibility when more data is available, all the while carefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse systems, including biological, agricultural, and robotic domains, showing superior performance in learning complex dynamics. For decision-making, we demonstrate that SimPEL bridges the sim-to-real gap in model-based reinforcement learning. On a high-speed RC car task, SimPEL learns a highly dynamic parking maneuver involving drifting with substantially less data than state-of-the-art baselines. These results highlight the potential of SimPEL for data-efficient learning and control in complex real-world environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies</title>
<link>https://arxiv.org/abs/2509.05735</link>
<guid>https://arxiv.org/abs/2509.05735</guid>
<content:encoded><![CDATA[
<div> active data collection, model-based reinforcement learning, offline training, online training, world models

Summary:
This article explores the impact of online and offline data collection strategies on model-based reinforcement learning in various environments. The study finds that online agents outperform offline agents due to encountering Out-Of-Distribution states at test time. Without a self-correction mechanism, offline agents suffer from a mismatch between their imagined and real rollouts, hindering policy training. The performance degradation of offline agents can be mitigated by incorporating exploration data or allowing additional online interactions. The study recommends adding exploration data when collecting datasets, as relying solely on expert data is common practice. Overall, online training with continuous interactions yields better performance in learning robust world models compared to offline training, highlighting the importance of carefully selecting data collection strategies in model-based reinforcement learning. 

<br /><br />Summary: <div>
arXiv:2509.05735v1 Announce Type: new 
Abstract: Data collection is crucial for learning robust world models in model-based reinforcement learning. The most prevalent strategies are to actively collect trajectories by interacting with the environment during online training or training on offline datasets. At first glance, the nature of learning task-agnostic environment dynamics makes world models a good candidate for effective offline training. However, the effects of online vs. offline data on world models and thus on the resulting task performance have not been thoroughly studied in the literature. In this work, we investigate both paradigms in model-based settings, conducting experiments on 31 different environments. First, we showcase that online agents outperform their offline counterparts. We identify a key challenge behind performance degradation of offline agents: encountering Out-Of-Distribution states at test time. This issue arises because, without the self-correction mechanism in online agents, offline datasets with limited state space coverage induce a mismatch between the agent's imagination and real rollouts, compromising policy training. We demonstrate that this issue can be mitigated by allowing for additional online interactions in a fixed or adaptive schedule, restoring the performance of online training with limited interaction data. We also showcase that incorporating exploration data helps mitigate the performance degradation of offline agents. Based on our insights, we recommend adding exploration data when collecting large datasets, as current efforts predominantly focus on expert data alone.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders</title>
<link>https://arxiv.org/abs/2509.05766</link>
<guid>https://arxiv.org/abs/2509.05766</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, Precision-Recall Curve, Random Forest, autoencoders, unsupervised machine learning
Summary: 
Anomaly detection is crucial for various applications such as network security and fraud prevention. The challenges faced include extreme class imbalance and the curse of dimensionality. To address these issues, the authors propose a hybrid framework that combines Precision-Recall Curve Random Forest (PRC-RF) with autoencoders. Autoencoders are unsupervised machine learning methods that learn compact latent representations. The new model, called Autoencoder-PRC-RF, outperforms previous methods in terms of accuracy, scalability, and interpretability. Extensive experiments on benchmark datasets confirm its effectiveness for high-stakes anomaly detection tasks. 

Summary: <div>
arXiv:2509.05766v1 Announce Type: new 
Abstract: Anomaly detection underpins critical applications from network security and intrusion detection to fraud prevention, where recognizing aberrant patterns rapidly is indispensable. Progress in this area is routinely impeded by two obstacles: extreme class imbalance and the curse of dimensionality. To combat the former, we previously introduced Precision-Recall Curve (PRC) classification trees and their ensemble extension, the PRC Random Forest (PRC-RF). Building on that foundation, we now propose a hybrid framework that integrates PRC-RF with autoencoders, unsupervised machine learning methods that learn compact latent representations, to confront both challenges simultaneously. Extensive experiments across diverse benchmark datasets demonstrate that the resulting Autoencoder-PRC-RF model achieves superior accuracy, scalability, and interpretability relative to prior methods, affirming its potential for high-stakes anomaly-detection tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting</title>
<link>https://arxiv.org/abs/2509.05768</link>
<guid>https://arxiv.org/abs/2509.05768</guid>
<content:encoded><![CDATA[
<div> energy forecasting, grid reliability, time series forecasting, Real-E dataset, correlation structures

Summary: 
The article introduces the Real-E dataset, which provides extensive data on energy forecasting for over 74 power stations across 30+ European countries over a 10-year period. The dataset includes rich metadata and highlights the limitations of existing benchmarks in spatial and temporal scope as well as multi-energy features. The study analyzes the dataset and benchmarks over 20 baselines across various model types. A new metric is introduced to quantify shifts in correlation structures, revealing the complexity and non-stationary dynamics of the dataset. The findings demonstrate the challenges faced by current methods in tackling the Real-E dataset and emphasize the need for more robust forecasting models. <div>
arXiv:2509.05768v1 Announce Type: new 
Abstract: Energy forecasting is vital for grid reliability and operational efficiency. Although recent advances in time series forecasting have led to progress, existing benchmarks remain limited in spatial and temporal scope and lack multi-energy features. This raises concerns about their reliability and applicability in real-world deployment. To address this, we present the Real-E dataset, covering over 74 power stations across 30+ European countries over a 10-year span with rich metadata. Using Real- E, we conduct an extensive data analysis and benchmark over 20 baselines across various model types. We introduce a new metric to quantify shifts in correlation structures and show that existing methods struggle on our dataset, which exhibits more complex and non-stationary correlation dynamics. Our findings highlight key limitations of current methods and offer a strong empirical basis for building more robust forecasting models
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2509.05778</link>
<guid>https://arxiv.org/abs/2509.05778</guid>
<content:encoded><![CDATA[
<div> Keywords: Out-of-distribution detection, Cross-validation, Evaluation framework, Dual Cross-Validation, Robustness 

Summary: 
The article discusses the importance of out-of-distribution (OOD) detection in enhancing the robustness of AI systems by identifying inputs that differ significantly from the training distribution. The development of reliable OOD detection methods is crucial, and rigorous evaluation is necessary to assess their performance under diverse conditions. The proposed dual cross-validation framework, DCV-ROOD, aims to provide a more robust evaluation of OOD detection models by effectively integrating in-distribution (ID) and OOD data while considering their distinct characteristics. The framework partitions ID data conventionally and OOD data based on class grouping. Additionally, the article examines data with class hierarchy to suggest a fair data split for the evaluation framework. Testing the framework with state-of-the-art OOD detection methods shows fast convergence to true performance. <div>
arXiv:2509.05778v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection plays a key role in enhancing the robustness of artificial intelligence systems by identifying inputs that differ significantly from the training distribution, thereby preventing unreliable predictions and enabling appropriate fallback mechanisms. Developing reliable OOD detection methods is a significant challenge, and rigorous evaluation of these techniques is essential for ensuring their effectiveness, as it allows researchers to assess their performance under diverse conditions and to identify potential limitations or failure modes. Cross-validation (CV) has proven to be a highly effective tool for providing a reasonable estimate of the performance of a learning algorithm. Although OOD scenarios exhibit particular characteristics, an appropriate adaptation of CV can lead to a suitable evaluation framework for this setting. This work proposes a dual CV framework for robust evaluation of OOD detection models, aimed at improving the reliability of their assessment. The proposed evaluation framework aims to effectively integrate in-distribution (ID) and OOD data while accounting for their differing characteristics. To achieve this, ID data are partitioned using a conventional approach, whereas OOD data are divided by grouping samples based on their classes. Furthermore, we analyze the context of data with class hierarchy to propose a data splitting that considers the entire class hierarchy to obtain fair ID-OOD partitions to apply the proposed evaluation framework. This framework is called Dual Cross-Validation for Robust Out-of-Distribution Detection (DCV-ROOD). To test the validity of the evaluation framework, we selected a set of state-of-the-art OOD detection methods, both with and without outlier exposure. The results show that the method achieves very fast convergence to the true performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting</title>
<link>https://arxiv.org/abs/2509.05779</link>
<guid>https://arxiv.org/abs/2509.05779</guid>
<content:encoded><![CDATA[
<div> Keywords: spatio-temporal forecasting, exogenous variables, latent space, expert module, siamese network

Summary:
The paper introduces a new framework called ExoST, focusing on incorporating exogenous variables in spatio-temporal forecasting. The framework follows a "select, then balance" paradigm, utilizing a latent space gated expert module to select and recompose salient signals from exogenous information, enhancing forecast accuracy. A siamese network architecture is employed to capture dynamic patterns, with past and future exogenous variables fed into dual-branch spatio-temporal backbones. A context-aware weighting mechanism ensures dynamic balance during the model's process. Extensive experiments on real-world datasets showcase the effectiveness, generality, robustness, and efficiency of the proposed framework.<br /><br />Summary: <div>
arXiv:2509.05779v1 Announce Type: new 
Abstract: Spatio-temporal forecasting aims to predict the future state of dynamic systems and plays an important role in multiple fields. However, existing solutions only focus on modeling using a limited number of observed target variables. In real-world scenarios, exogenous variables can be integrated into the model as additional input features and associated with the target signal to promote forecast accuracy. Although promising, this still encounters two challenges: the inconsistent effects of different exogenous variables to the target system, and the imbalance effects between historical variables and future variables. To address these challenges, this paper introduces \model, a novel framework for modeling \underline{exo}genous variables in \underline{s}patio-\underline{t}emporal forecasting, which follows a ``select, then balance'' paradigm. Specifically, we first construct a latent space gated expert module, where fused exogenous information is projected into a latent space to dynamically select and recompose salient signals via specialized sub-experts. Furthermore, we design a siamese network architecture in which recomposed representations of past and future exogenous variables are fed into dual-branch spatio-temporal backbones to capture dynamic patterns. The outputs are integrated through a context-aware weighting mechanism to achieve dynamic balance during the modeling process. Extensive experiments on real-world datasets demonstrate the effectiveness, generality, robustness, and efficiency of our proposed framework.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2509.05801</link>
<guid>https://arxiv.org/abs/2509.05801</guid>
<content:encoded><![CDATA[
<div> transformer-based models, market regimes, simulation, activation transplantation, event severity <br />
Summary: 
The study investigates whether transformer-based models internalize semantic concepts and can simulate rare, high-stakes events like market crashes. Activation transplantation is introduced as a way to manipulate hidden states in the model and steer forecasts by imposing event semantics. The study finds that models encode a graded notion of event severity, with the magnitude of systemic shocks correlating with the latent vector norm. Results show that large time series transformers like Toto and Chronos demonstrate robust, semantically grounded representations that can be controlled causally. This shift from post-hoc attribution to direct causal intervention allows for semantic "what-if" analysis in strategic stress-testing, providing evidence for a latent concept space governing model predictions. <br /><br /> <div>
arXiv:2509.05801v1 Announce Type: new 
Abstract: While transformer-based foundation models excel at forecasting routine patterns, two questions remain: do they internalize semantic concepts such as market regimes, or merely fit curves? And can their internal representations be leveraged to simulate rare, high-stakes events such as market crashes? To investigate this, we introduce activation transplantation, a causal intervention that manipulates hidden states by imposing the statistical moments of one event (e.g., a historical crash) onto another (e.g., a calm period) during the forward pass. This procedure deterministically steers forecasts: injecting crash semantics induces downturn predictions, while injecting calm semantics suppresses crashes and restores stability. Beyond binary control, we find that models encode a graded notion of event severity, with the latent vector norm directly correlating with the magnitude of systemic shocks. Validated across two architecturally distinct TSFMs, Toto (decoder only) and Chronos (encoder-decoder), our results demonstrate that steerable, semantically grounded representations are a robust property of large time series transformers. Our findings provide evidence for a latent concept space that governs model predictions, shifting interpretability from post-hoc attribution to direct causal intervention, and enabling semantic "what-if" analysis for strategic stress-testing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Optimizers for Convex Aligned Multi-Objective Optimization</title>
<link>https://arxiv.org/abs/2509.05811</link>
<guid>https://arxiv.org/abs/2509.05811</guid>
<content:encoded><![CDATA[
<div> framework, gradient-based algorithms, convergence guarantees, convex optimization, deep learning practice
Summary:
In the realm of machine learning, the Aligned Multi-Objective Optimization (AMOO) framework has been introduced to leverage a diverse set of tasks to enhance performance. This framework, focusing on convex optimization, proposes gradient-based algorithms with proven convergence guarantees. Unlike previous work that assumed strong convexity, this study relaxes this assumption and investigates gradient-descent algorithms under standard smoothness or Lipschitz continuity conditions. New analytical tools and metrics are developed to characterize convergence in the convex AMOO setting. The proposed algorithms for convex AMOO maintain scalability while providing convergence guarantees. Additionally, a novel lower bound is proven to show the suboptimality of naive equal-weight approaches compared to the proposed methods. This work bridges theoretical analysis with practical applications in deep learning practice. <div>
arXiv:2509.05811v1 Announce Type: new 
Abstract: It is widely recognized in modern machine learning practice that access to a diverse set of tasks can enhance performance across those tasks. This observation suggests that, unlike in general multi-objective optimization, the objectives in many real-world settings may not be inherently conflicting. To address this, prior work introduced the Aligned Multi-Objective Optimization (AMOO) framework and proposed gradient-based algorithms with provable convergence guarantees. However, existing analysis relies on strong assumptions, particularly strong convexity, which implies the existence of a unique optimal solution. In this work, we relax this assumption and study gradient-descent algorithms for convex AMOO under standard smoothness or Lipschitz continuity conditions-assumptions more consistent with those used in deep learning practice. This generalization requires new analytical tools and metrics to characterize convergence in the convex AMOO setting. We develop such tools, propose scalable algorithms for convex AMOO, and establish their convergence guarantees. Additionally, we prove a novel lower bound that demonstrates the suboptimality of naive equal-weight approaches compared to our methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance of Conformal Prediction in Capturing Aleatoric Uncertainty</title>
<link>https://arxiv.org/abs/2509.05826</link>
<guid>https://arxiv.org/abs/2509.05826</guid>
<content:encoded><![CDATA[
<div> Keywords: Conformal prediction, aleatoric uncertainty, deep learning models, human annotation, class overlap 

Summary: 
Conformal prediction is a model-agnostic method for generating prediction sets with high coverage of true classes. This study evaluates how effectively conformal predictors quantify aleatoric uncertainty, specifically in datasets with overlapping classes. Prediction set sizes were compared to the number of distinct labels assigned by human annotators, revealing weak correlations in most cases. This highlights the limited capability of conformal predictors in capturing aleatoric uncertainty. The study used three conformal prediction approaches on eight deep learning models trained on four datasets with annotations from multiple human annotators per instance. While conformal predictors can provide broad coverage of true classes, their ability to capture aleatoric uncertainty is questioned based on the weak correlations found in this investigation.<br /><br />Summary: <div>
arXiv:2509.05826v1 Announce Type: new 
Abstract: Conformal prediction is a model-agnostic approach to generating prediction sets that cover the true class with a high probability. Although its prediction set size is expected to capture aleatoric uncertainty, there is a lack of evidence regarding its effectiveness. The literature presents that prediction set size can upper-bound aleatoric uncertainty or that prediction sets are larger for difficult instances and smaller for easy ones, but a validation of this attribute of conformal predictors is missing. This work investigates how effectively conformal predictors quantify aleatoric uncertainty, specifically the inherent ambiguity in datasets caused by overlapping classes. We perform this by measuring the correlation between prediction set sizes and the number of distinct labels assigned by human annotators per instance. We further assess the similarity between prediction sets and human-provided annotations. We use three conformal prediction approaches to generate prediction sets for eight deep learning models trained on four datasets. The datasets contain annotations from multiple human annotators (ranging from five to fifty participants) per instance, enabling the identification of class overlap. We show that the vast majority of the conformal prediction outputs show a very weak to weak correlation with human annotations, with only a few showing moderate correlation. These findings underscore the necessity of critically reassessing the prediction sets generated using conformal predictors. While they can provide a higher coverage of the true classes, their capability in capturing aleatoric uncertainty remains limited.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finetuning LLMs for Human Behavior Prediction in Social Science Experiments</title>
<link>https://arxiv.org/abs/2509.05830</link>
<guid>https://arxiv.org/abs/2509.05830</guid>
<content:encoded><![CDATA[
<div> finetuning, Large language models, social science experiments, SocSci210, predictions <br />
Summary:<br />
- The study demonstrates the effectiveness of finetuning Large Language Models (LLMs) on individual-level responses from social science experiments.
- The dataset SocSci210, created through an automated pipeline, comprises 2.9 million responses from over 400,000 participants in 210 social science experiments.
- Finetuning LLMs improves prediction accuracy across diverse social science domains and enhances generalization.
- The model Socrates-Qwen-14B outperforms GPT-4o by 13% in producing predictions aligned with human responses.
- By finetuning on a subset of conditions in a study, the generalization to new unseen conditions improves by 71%.
- Finetuning on rich, topic-specific datasets like SocSci210 reduces bias, as indicated by a 10.6% decrease in demographic parity.
<br /><br />Summary: <div>
arXiv:2509.05830v1 Announce Type: new 
Abstract: Large language models (LLMs) offer a powerful opportunity to simulate the results of social science experiments. In this work, we demonstrate that finetuning LLMs directly on individual-level responses from past experiments meaningfully improves the accuracy of such simulations across diverse social science domains. We construct SocSci210 via an automatic pipeline, a dataset comprising 2.9 million responses from 400,491 participants in 210 open-source social science experiments. Through finetuning, we achieve multiple levels of generalization. In completely unseen studies, our strongest model, Socrates-Qwen-14B, produces predictions that are 26% more aligned with distributions of human responses to diverse outcome questions under varying conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by 13%. By finetuning on a subset of conditions in a study, generalization to new unseen conditions is particularly robust, improving by 71%. Since SocSci210 contains rich demographic information, we reduce demographic parity, a measure of bias, by 10.6% through finetuning. Because social sciences routinely generate rich, topic-specific datasets, our findings indicate that finetuning on such data could enable more accurate simulations for experimental hypothesis screening. We release our data, models and finetuning code at stanfordhci.github.io/socrates.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces</title>
<link>https://arxiv.org/abs/2509.05833</link>
<guid>https://arxiv.org/abs/2509.05833</guid>
<content:encoded><![CDATA[
<div> Simulation Environment, Robust Gradient Aggregation Methods, Decentralized Gradient Marketplaces, Economic Efficiency, Fairness

Summary: 
The article introduces a benchmark framework for evaluating robust gradient aggregation methods in decentralized gradient marketplaces. The framework includes a simulation environment that models marketplace dynamics with varying buyer baselines and seller distributions. It also incorporates a methodology to evaluate marketplace-centric dimensions such as Economic Efficiency, Fairness, and Selection Dynamics in addition to standard Federated Learning metrics. The empirical analysis focuses on the Distributed Gradient Marketplace framework, MartFL, and evaluates alternative aggregation strategies like FLTrust and SkyMask within it. The benchmark covers diverse datasets, local attacks, and Sybil attacks, providing insights into the trade-offs between model performance, robustness, cost, fairness, and stability. This comprehensive benchmark aims to equip the community with tools and evidence to design more robust, equitable, and economically viable decentralized gradient marketplaces. 

<br /><br />Summary: <div>
arXiv:2509.05833v1 Announce Type: new 
Abstract: The rise of distributed and privacy-preserving machine learning has sparked interest in decentralized gradient marketplaces, where participants trade intermediate artifacts like gradients. However, existing Federated Learning (FL) benchmarks overlook critical economic and systemic factors unique to such marketplaces-cost-effectiveness, fairness to sellers, and market stability-especially when a buyer relies on a private baseline dataset for evaluation.
  We introduce a comprehensive benchmark framework to holistically evaluate robust gradient aggregation methods within these buyer-baseline-reliant marketplaces. Our contributions include: (1) a simulation environment modeling marketplace dynamics with a variable buyer baseline and diverse seller distributions; (2) an evaluation methodology augmenting standard FL metrics with marketplace-centric dimensions such as Economic Efficiency, Fairness, and Selection Dynamics; (3) an in-depth empirical analysis of the existing Distributed Gradient Marketplace framework, MartFL, including the integration and comparative evaluation of adapted FLTrust and SkyMask as alternative aggregation strategies within it. This benchmark spans diverse datasets, local attacks, and Sybil attacks targeting the marketplace selection process; and (4) actionable insights into the trade-offs between model performance, robustness, cost, fairness, and stability.
  This benchmark equips the community with essential tools and empirical evidence to evaluate and design more robust, equitable, and economically viable decentralized gradient marketplaces.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics</title>
<link>https://arxiv.org/abs/2509.05839</link>
<guid>https://arxiv.org/abs/2509.05839</guid>
<content:encoded><![CDATA[
<div> Keywords: queueing network models, data-driven framework, autoregressive sequence models, Transformer-style architectures, simulation

Summary: 
Queueing network models are powerful tools for analyzing service systems, but constructing them traditionally requires significant human effort and domain expertise. A new data-driven framework has been proposed to make queueing network modeling more scalable and accessible. This framework utilizes autoregressive sequence models trained on event-stream data to learn conditional distributions of event types and times, transforming the modeling task into one of sequence distribution learning. Transformer-style architectures are employed to parameterize these distributions, enabling the automated construction of high-fidelity simulators. Through validation on event tables from various queueing networks, the framework demonstrates its utility in simulation, uncertainty quantification, and counterfactual evaluation. By leveraging artificial intelligence advancements and increasing data availability, this framework aims to streamline queueing network modeling processes and facilitate wider utilization across service domains.<br /><br />Summary: <div>
arXiv:2509.05839v1 Announce Type: new 
Abstract: While queueing network models are powerful tools for analyzing service systems, they traditionally require substantial human effort and domain expertise to construct. To make this modeling approach more scalable and accessible, we propose a data-driven framework for queueing network modeling and simulation based on autoregressive sequence models trained on event-stream data. Instead of explicitly specifying arrival processes, service mechanisms, or routing logic, our approach learns the conditional distributions of event types and event times, recasting the modeling task as a problem of sequence distribution learning. We show that Transformer-style architectures can effectively parameterize these distributions, enabling automated construction of high-fidelity simulators. As a proof of concept, we validate our framework on event tables generated from diverse queueing networks, showcasing its utility in simulation, uncertainty quantification, and counterfactual evaluation. Leveraging advances in artificial intelligence and the growing availability of data, our framework takes a step toward more automated, data-driven modeling pipelines to support broader adoption of queueing network models across service domains.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Measure of Deception: An Analysis of Data Forging in Machine Unlearning</title>
<link>https://arxiv.org/abs/2509.05865</link>
<guid>https://arxiv.org/abs/2509.05865</guid>
<content:encoded><![CDATA[
<div> privacy regulations, machine unlearning, data mitigation, forged data, adversarial forging

Summary: 
This study addresses the issue of verifying machine unlearning, which involves modifying trained models to forget specific data in compliance with privacy regulations and to mitigate harmful effects. The focus is on adversarial forging, where data is manipulated to mimic the gradient of a target point to create the illusion of unlearning without actually removing information. The research develops a framework for analyzing forging sets, which are collections of data points with gradients approximating a target gradient within a certain tolerance. Results show that the Lebesgue measure of forging sets is small, scaling with the tolerance parameter. Probability bounds indicate that randomly sampling a forging point is highly unlikely under non-degenerate data distributions. These findings suggest that adversarial forging is inherently limited, providing insights into the potential detection of false unlearning claims. <div>
arXiv:2509.05865v1 Announce Type: new 
Abstract: Motivated by privacy regulations and the need to mitigate the effects of harmful data, machine unlearning seeks to modify trained models so that they effectively ``forget'' designated data. A key challenge in verifying unlearning is forging -- adversarially crafting data that mimics the gradient of a target point, thereby creating the appearance of unlearning without actually removing information. To capture this phenomenon, we consider the collection of data points whose gradients approximate a target gradient within tolerance $\epsilon$ -- which we call an $\epsilon$-forging set -- and develop a framework for its analysis. For linear regression and one-layer neural networks, we show that the Lebesgue measure of this set is small. It scales on the order of $\epsilon$, and when $\epsilon$ is small enough, $\epsilon^d$. More generally, under mild regularity assumptions, we prove that the forging set measure decays as $\epsilon^{(d-r)/2}$, where $d$ is the data dimension and $r<d$ is the nullity of a variation matrix defined by the model gradients. Extensions to batch SGD and almost-everywhere smooth loss functions yield the same asymptotic scaling. In addition, we establish probability bounds showing that, under non-degenerate data distributions, the likelihood of randomly sampling a forging point is vanishingly small. These results provide evidence that adversarial forging is fundamentally limited and that false unlearning claims can, in principle, be detected.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.05874</link>
<guid>https://arxiv.org/abs/2509.05874</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Sparse Reference Selection, Knowledge Construction, Drug-Gene Relation Discovery, Partial Information <br />
<br />
The study introduces a Deep Reinforcement Learning framework for sparse reference selection, emulating human knowledge construction by prioritizing papers to read under time and cost constraints. This framework was evaluated in drug-gene relation discovery with access limited to titles and abstracts, showing that both humans and machines can effectively construct knowledge from partial information. The rapid growth of scientific literature necessitates efficient methods for acquiring new knowledge, especially in specialized domains where reasoning is complex and full-text access is restricted. By leveraging Deep Reinforcement Learning, this approach enables effective decision-making in selecting which papers to prioritize for reading, optimizing knowledge acquisition under constrained conditions. Overall, the study highlights the potential of machine learning techniques to augment human capabilities in navigating and synthesizing vast amounts of information for knowledge discovery and innovation. <br /><br />Summary: <div>
arXiv:2509.05874v1 Announce Type: new 
Abstract: The rapid expansion of scientific literature makes it increasingly difficult to acquire new knowledge, particularly in specialized domains where reasoning is complex, full-text access is restricted, and target references are sparse among a large set of candidates. We present a Deep Reinforcement Learning framework for sparse reference selection that emulates human knowledge construction, prioritizing which papers to read under limited time and cost. Evaluated on drug--gene relation discovery with access restricted to titles and abstracts, our approach demonstrates that both humans and machines can construct knowledge effectively from partial information.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework</title>
<link>https://arxiv.org/abs/2509.05886</link>
<guid>https://arxiv.org/abs/2509.05886</guid>
<content:encoded><![CDATA[
<div> surrogate model, convective heat transfer coefficient, liquid sodium, rectangular miniature heat sinks, machine learning<br />
Summary:
A surrogate model is developed using machine learning techniques to predict the convective heat transfer coefficient of liquid sodium flowing through rectangular miniature heat sinks. The model initially utilizes kernel-based methods and shallow neural networks on a dataset of Nusselt numbers for liquid sodium. Further improvement is achieved through a self-supervised physics-informed neural network and transfer learning approach. The self-supervised network effectively estimates heat transfer rates with a margin of approximately +8% error. Using only physics for regression results in errors between 5% to 10%, while other machine learning methods provide predictions mostly within +8% accuracy. This approach offers a more efficient alternative to high-fidelity computational fluid dynamics models for designing and optimizing liquid-metal-cooled miniature heat sinks. <br /><br />Summary: <div>
arXiv:2509.05886v1 Announce Type: new 
Abstract: A surrogate model is developed to predict the convective heat transfer coefficient of liquid sodium (Na) flow within rectangular miniature heat sinks. Initially, kernel-based machine learning techniques and shallow neural network are applied to a dataset with 87 Nusselt numbers for liquid sodium in rectangular miniature heat sinks. Subsequently, a self-supervised physics-informed neural network and transfer learning approach are used to increase the estimation performance. In the self-supervised physics-informed neural network, an additional layer determines the weight the of physics in the loss function to balance data and physics based on their uncertainty for a better estimation. For transfer learning, a shallow neural network trained on water is adapted for use with Na. Validation results show that the self-supervised physics-informed neural network successfully estimate the heat transfer rates of Na with an error margin of approximately +8%. Using only physics for regression, the error remains between 5% to 10%. Other machine learning methods specify the prediction mostly within +8%. High-fidelity modeling of turbulent forced convection of liquid metals using computational fluid dynamics (CFD) is both time-consuming and computationally expensive. Therefore, machine learning based models offer a powerful alternative tool for the design and optimization of liquid-metal-cooled miniature heat sinks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs</title>
<link>https://arxiv.org/abs/2509.05899</link>
<guid>https://arxiv.org/abs/2509.05899</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Text-to-SQL, database schema, X-Linking, X-Admin<br />
Summary:<br />
The article introduces a novel database schema expert called X-SQL for the Text-to-SQL task, emphasizing the importance of database schema information in generating high-quality SQL queries. The framework consists of two components: X-Linking, which uses LLM Supervised Finetuning for Schema Linking, and X-Admin, focusing on Schema Understanding. Multi-LLMs are also used to enhance performance. X-SQL achieves Execution Accuracies of 84.9% on the Spider-Dev dataset and 82.5% on the Spider-Test dataset, making it the leading Text-to-SQL framework based on open-source models. The research highlights the significance of incorporating schema information in Text-to-SQL tasks and demonstrates the effectiveness of the proposed X-SQL framework in leveraging schema knowledge for improved query generation. The innovative components of X-SQL contribute to its superior performance in comparison to existing methods.<br /><br />Summary: <div>
arXiv:2509.05899v1 Announce Type: new 
Abstract: With Large Language Models' (LLMs) emergent abilities on code generation tasks, Text-to-SQL has become one of the most popular downstream applications. Despite the strong results of multiple recent LLM-based Text-to-SQL frameworks, the research community often overlooks the importance of database schema information for generating high-quality SQL queries. We find that such schema information plays a significant or even dominant role in the Text-to-SQL task. To tackle this challenge, we propose a novel database schema expert with two components. We first introduce X-Linking, an LLM Supervised Finetuning (SFT)-based method that achieves superior Schema Linking results compared to existing open-source Text-to-SQL methods. In addition, we innovatively propose an X-Admin component that focuses on Schema Understanding by bridging the gap between abstract schema information and the user's natural language question. Aside from better learning with schema information, we experiment with Multi-LLMs for different components within the system to further boost its performance. By incorporating these techniques into our end-to-end framework, X-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset and 82.5% on the Spider-Test dataset. This outstanding performance establishes X-SQL as the leading Text-to-SQL framework based on open-source models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms</title>
<link>https://arxiv.org/abs/2509.05930</link>
<guid>https://arxiv.org/abs/2509.05930</guid>
<content:encoded><![CDATA[
<div> SOOTT, Smoothed Online Optimization, Target Tracking, Adversarial Perturbation, Switching Cost
Summary:
The article introduces the Smoothed Online Optimization for Target Tracking (SOOTT) problem, which addresses the challenges of decision-making in uncertain environments by considering tracking cost, adversarial perturbation cost, and switching cost. The proposed BEST algorithm provides competitive guarantees for SOOTT, while the CoRT variant enhances practical performance by incorporating black-box predictions. Theoretical analysis demonstrates that CoRT outperforms BEST with accurate predictions and remains robust against prediction errors. A case study on workload scheduling illustrates the effectiveness of both algorithms in balancing trajectory tracking, decision smoothness, and resilience to external disturbances. <div>
arXiv:2509.05930v1 Announce Type: new 
Abstract: We introduce the Smoothed Online Optimization for Target Tracking (SOOTT) problem, a new framework that integrates three key objectives in online decision-making under uncertainty: (1) tracking cost for following a dynamically moving target, (2) adversarial perturbation cost for withstanding unpredictable disturbances, and (3) switching cost for penalizing abrupt changes in decisions. This formulation captures real-world scenarios such as elastic and inelastic workload scheduling in AI clusters, where operators must balance long-term service-level agreements (e.g., LLM training) against sudden demand spikes (e.g., real-time inference). We first present BEST, a robust algorithm with provable competitive guarantees for SOOTT. To enhance practical performance, we introduce CoRT, a learning-augmented variant that incorporates untrusted black-box predictions (e.g., from ML models) into its decision process. Our theoretical analysis shows that CoRT strictly improves over BEST when predictions are accurate, while maintaining robustness under arbitrary prediction errors. We validate our approach through a case study on workload scheduling, demonstrating that both algorithms effectively balance trajectory tracking, decision smoothness, and resilience to external disturbances.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior</title>
<link>https://arxiv.org/abs/2509.06025</link>
<guid>https://arxiv.org/abs/2509.06025</guid>
<content:encoded><![CDATA[
<div> foundation model, artificial intelligence, user behavior, predictive systems, interactions <br />
Summary: <br />
The article introduces the Unified Interaction Foundation Model (UIFM) as a new approach to understanding and predicting complex event sequences in domains like telecommunications, e-commerce, and finance. Unlike current models that break events down into fragmented parts, UIFM treats multi-attribute events as coherent units through composite tokenization. This enables UIFM to grasp the holistic nature of user behavior, learning the underlying "grammar" of interactions. By perceiving entire interactions instead of disconnected data points, UIFM aims to create more accurate and adaptable predictive systems. This shift in architecture represents a fundamental advancement in developing AI systems that can genuinely understand and predict dynamic sequences of events. <div>
arXiv:2509.06025v1 Announce Type: new 
Abstract: A central goal of artificial intelligence is to build systems that can understand and predict complex, evolving sequences of events. However, current foundation models, designed for natural language, fail to grasp the holistic nature of structured interactions found in domains like telecommunications, e-commerce and finance. By serializing events into text, they disassemble them into semantically fragmented parts, losing critical context. In this work, we introduce the Unified Interaction Foundation Model (UIFM), a foundation model engineered for genuine behavioral understanding. At its core is the principle of composite tokenization, where each multi-attribute event is treated as a single, semantically coherent unit. This allows UIFM to learn the underlying "grammar" of user behavior, perceiving entire interactions rather than a disconnected stream of data points. We demonstrate that this architecture is not just more accurate, but represents a fundamental step towards creating more adaptable and intelligent predictive systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training</title>
<link>https://arxiv.org/abs/2509.06053</link>
<guid>https://arxiv.org/abs/2509.06053</guid>
<content:encoded><![CDATA[
<div> Framework, Multi-agent reinforcement learning, Large Language Models, Programmatic policies, Policy generation

Summary:
PolicyEvolve is a framework for generating programmatic policies in multi-player games. It aims to reduce reliance on manually crafted policy code and improve policy performance with minimal environmental interactions. The framework consists of four modules: Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool stores elite policies, while the Local Pool holds temporary policies for the current iteration. The Policy Planner generates initial policies based on environmental information and refines them using feedback from the Trajectory Critic. Policies with high win rates against the Global Pool are integrated into the Global Pool. The Trajectory Critic analyzes interaction data from the current policy to suggest improvements. PolicyEvolve iteratively refines policies until they achieve high performance and interpretability in multi-player games. 

<br /><br />Summary: <div>
arXiv:2509.06053v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress in solving complex multi-player games through self-play. However, training effective adversarial policies requires millions of experience samples and substantial computational resources. Moreover, these policies lack interpretability, hindering their practical deployment. Recently, researchers have successfully leveraged Large Language Models (LLMs) to generate programmatic policies for single-agent tasks, transforming neural network-based policies into interpretable rule-based code with high execution efficiency. Inspired by this, we propose PolicyEvolve, a general framework for generating programmatic policies in multi-player games. PolicyEvolve significantly reduces reliance on manually crafted policy code, achieving high-performance policies with minimal environmental interactions. The framework comprises four modules: Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool preserves elite policies accumulated during iterative training. The Local Pool stores temporary policies for the current iteration; only sufficiently high-performing policies from this pool are promoted to the Global Pool. The Policy Planner serves as the core policy generation module. It samples the top three policies from the Global Pool, generates an initial policy for the current iteration based on environmental information, and refines this policy using feedback from the Trajectory Critic. Refined policies are then deposited into the Local Pool. This iterative process continues until the policy achieves a sufficiently high average win rate against the Global Pool, at which point it is integrated into the Global Pool. The Trajectory Critic analyzes interaction data from the current policy, identifies vulnerabilities, and proposes directional improvements to guide the Policy Planner
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel biomass fluidized bed gasification model coupled with machine learning and CFD simulation</title>
<link>https://arxiv.org/abs/2509.06056</link>
<guid>https://arxiv.org/abs/2509.06056</guid>
<content:encoded><![CDATA[
<div> Keywords: biomass, fluidized bed gasification, machine learning, computational fluid dynamics, reaction kinetics

Summary:
A coupling model of biomass fluidized bed gasification is proposed in this study, integrating machine learning and computational fluid dynamics (CFD) to enhance the accuracy and efficiency of predicting complex thermochemical reactions. To achieve this, a high-quality dataset was created using experimental data and high fidelity simulation results. An agent model was trained to describe reaction kinetics and incorporated into the CFD framework for real-time updating of reaction rate and composition evolution. This innovative approach allows for a more precise understanding of the gasification process and enables faster computations, offering significant improvements in modeling accuracy and computational efficiency for biomass gasification studies. <div>
arXiv:2509.06056v1 Announce Type: new 
Abstract: A coupling model of biomass fluidized bed gasification based on machine learning and computational fluid dynamics is proposed to improve the prediction accuracy and computational efficiency of complex thermochemical reaction process. By constructing a high-quality data set based on experimental data and high fidelity simulation results, the agent model used to describe the characteristics of reaction kinetics was trained and embedded into the computational fluid dynamics (CFD) framework to realize the real-time update of reaction rate and composition evolution.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.06060</link>
<guid>https://arxiv.org/abs/2509.06060</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, time series forecasting, benchmark datasets, modeling strategies, model recommendation

Summary:
ARIES is a framework that explores the relationship between time series properties and modeling strategies in the context of deep learning for forecasting. The study addresses the limitations of existing benchmark datasets by creating a synthetic dataset with diverse temporal patterns. By benchmarking over 50 forecasting models, ARIES establishes a clear correlation between time series properties and effective modeling strategies. This information is leveraged to build the first deep forecasting model recommender, offering interpretable suggestions for real-world time series data. ARIES provides a comprehensive system for evaluating time series properties, enhancing the understanding of the impact of different data characteristics on model performance. By making the code available on GitHub, the framework aims to facilitate further research in this area and improve the efficiency of model selection for time series forecasting tasks. 

<br /><br />Summary: <div>
arXiv:2509.06060v1 Announce Type: new 
Abstract: Recent advancements in deep learning models for time series forecasting have been significant. These models often leverage fundamental time series properties such as seasonality and non-stationarity, which may suggest an intrinsic link between model performance and data properties. However, existing benchmark datasets fail to offer diverse and well-defined temporal patterns, restricting the systematic evaluation of such connections. Additionally, there is no effective model recommendation approach, leading to high time and cost expenditures when testing different architectures across different downstream applications. For those reasons, we propose ARIES, a framework for assessing relation between time series properties and modeling strategies, and for recommending deep forcasting models for realistic time series. First, we construct a synthetic dataset with multiple distinct patterns, and design a comprehensive system to compute the properties of time series. Next, we conduct an extensive benchmarking of over 50 forecasting models, and establish the relationship between time series properties and modeling strategies. Our experimental results reveal a clear correlation. Based on these findings, we propose the first deep forecasting model recommender, capable of providing interpretable suggestions for real-world time series. In summary, ARIES is the first study to establish the relations between the properties of time series data and modeling strategies, while also implementing a model recommendation system. The code is available at: https://github.com/blisky-li/ARIES.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network</title>
<link>https://arxiv.org/abs/2509.06067</link>
<guid>https://arxiv.org/abs/2509.06067</guid>
<content:encoded><![CDATA[
<div> surrogate model, high-temperature superconducting magnets, neural network, finite element method, rapid analysis<br />
Summary:<br />
A fully connected residual neural network (FCRN) surrogate model was developed to predict current density distribution in REBCO solenoids, addressing the computational cost issues of finite element method (FEM) in large-scale HTS magnets. Training data from FEM simulations with various configurations showed that deeper FCRNs outperformed conventional fully connected networks (FCNs), with a specific architecture yielding optimal results. Extrapolation tests demonstrated the model's ability to predict magnetization losses beyond the training range with minimal errors. The FCRN-based surrogate model significantly reduced computational time compared to FEM, making it a valuable tool for efficiently analyzing meter-scale HTS magnets. The study showcases the accuracy and efficiency of the proposed model, highlighting its potential for enhancing the design process of large-scale REBCO magnet systems.<br /> <div>
arXiv:2509.06067v1 Announce Type: new 
Abstract: Finite element method (FEM) is widely used in high-temperature superconducting (HTS) magnets, but its computational cost increases with magnet size and becomes time-consuming for meter-scale magnets, especially when multi-physics couplings are considered, which limits the fast design of large-scale REBCO magnet systems. In this work, a surrogate model based on a fully connected residual neural network (FCRN) is developed to predict the space-time current density distribution in REBCO solenoids. Training datasets were generated from FEM simulations with varying numbers of turns and pancakes. The results demonstrate that, for deeper networks, the FCRN architecture achieves better convergence than conventional fully connected network (FCN), with the configuration of 12 residual blocks and 256 neurons per layer providing the most favorable balance between training accuracy and generalization capability. Extrapolation studies show that the model can reliably predict magnetization losses for up to 50% beyond the training range, with maximum errors below 10%. The surrogate model achieves predictions several orders of magnitude faster than FEM and still remains advantageous when training costs are included. These results indicate that the proposed FCRN-based surrogate model provides both accuracy and efficiency, offering a promising tool for the rapid analysis of large-scale HTS magnets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs</title>
<link>https://arxiv.org/abs/2509.06094</link>
<guid>https://arxiv.org/abs/2509.06094</guid>
<content:encoded><![CDATA[
<div> discounting, reinforcement learning, quasi-hyperbolic preferences, policy optimization, model-free algorithms 

Summary:
This paper explores the integration of quasi-hyperbolic (QH) discounting, a model for time-inconsistent preferences, into reinforcement learning (RL). The researchers address theoretical and algorithmic gaps for precommitted agents with QH preferences by characterizing the optimal policy structure and developing practical, model-free algorithms for policy evaluation and Q-learning. They prove that the optimal policy simplifies to a one-step non-stationary form and provide convergence guarantees for their proposed algorithms. This work offers foundational insights for incorporating QH preferences in RL, enhancing our understanding of decision-making processes where individuals prioritize immediate rewards over delayed gratification. The findings contribute to bridging the gap between psychological theories of human and animal behavior and computational models in RL. 

<br /><br />Summary: <div>
arXiv:2509.06094v1 Announce Type: new 
Abstract: Time-inconsistent preferences, where agents favor smaller-sooner over larger-later rewards, are a key feature of human and animal decision-making. Quasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this behavior, but its integration into the reinforcement learning (RL) framework has been limited. This paper addresses key theoretical and algorithmic gaps for precommitted agents with QH preferences. We make two primary contributions: (i) we formally characterize the structure of the optimal policy, proving for the first time that it reduces to a simple one-step non-stationary form; and (ii) we design the first practical, model-free algorithms for both policy evaluation and Q-learning in this setting, both with provable convergence guarantees. Our results provide foundational insights for incorporating QH preferences in RL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>If generative AI is the answer, what is the question?</title>
<link>https://arxiv.org/abs/2509.06120</link>
<guid>https://arxiv.org/abs/2509.06120</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, machine learning, autoregressive models, generative adversarial networks, socially responsible generation<br />
Summary:<br />
- Generative AI has evolved to generate various types of data beyond text and images, including audio, video, computer code, and molecules.<br />
- The article explores the foundations of generation in machine learning, connecting it to prediction, compression, and decision-making.<br />
- Five major generative model families are surveyed, including autoregressive models, variational autoencoders, normalizing flows, generative adversarial networks, and diffusion models.<br />
- A probabilistic framework is introduced to distinguish between density estimation and generation tasks.<br />
- A game-theoretic framework with a two-player adversary-learner setup is discussed for studying generation in AI.<br />
- Post-training modifications for deployment readiness of generative models are examined.<br />
- Socially responsible generation topics like privacy, AI-generated content detection, copyright, and IP are highlighted.<br /> <div>
arXiv:2509.06120v1 Announce Type: new 
Abstract: Beginning with text and images, generative AI has expanded to audio, video, computer code, and molecules. Yet, if generative AI is the answer, what is the question? We explore the foundations of generation as a distinct machine learning task with connections to prediction, compression, and decision-making. We survey five major generative model families: autoregressive models, variational autoencoders, normalizing flows, generative adversarial networks, and diffusion models. We then introduce a probabilistic framework that emphasizes the distinction between density estimation and generation. We review a game-theoretic framework with a two-player adversary-learner setup to study generation. We discuss post-training modifications that prepare generative models for deployment. We end by highlighting some important topics in socially responsible generation such as privacy, detection of AI-generated content, and copyright and IP. We adopt a task-first framing of generation, focusing on what generation is as a machine learning problem, rather than only on how models implement it.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators</title>
<link>https://arxiv.org/abs/2509.06154</link>
<guid>https://arxiv.org/abs/2509.06154</guid>
<content:encoded><![CDATA[
<div> Graph Neural Simulators, PDE systems, data efficiency, error reduction, surrogate models
Summary:
Graph Neural Simulators (GNS) efficiently learn solutions to PDE systems by incorporating numerical time-stepping schemes and causal structure. This approach enhances data efficiency, achieving high generalization accuracy with minimal training data. GNS significantly outperforms neural operator baselines like DeepONet and FNO, reducing error accumulation over extended temporal horizons by up to 99.86%. The inclusion of a PCA+KMeans trajectory selection strategy further improves performance with limited data. By combining graph-based local inductive biases with conventional time integrators, GNS produces accurate, physically consistent, and scalable surrogate models for time-dependent PDEs. <div>
arXiv:2509.06154v1 Announce Type: new 
Abstract: Neural operators (NOs) approximate mappings between infinite-dimensional function spaces but require large datasets and struggle with scarce training data. Many NO formulations don't explicitly encode causal, local-in-time structure of physical evolution. While autoregressive models preserve causality by predicting next time-steps, they suffer from rapid error accumulation. We employ Graph Neural Simulators (GNS) - a message-passing graph neural network framework - with explicit numerical time-stepping schemes to construct accurate forward models that learn PDE solutions by modeling instantaneous time derivatives. We evaluate our framework on three canonical PDE systems: (1) 2D Burgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D Allen-Cahn equation. Rigorous evaluations demonstrate GNS significantly improves data efficiency, achieving higher generalization accuracy with substantially fewer training trajectories compared to neural operator baselines like DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors with only 30 training samples out of 1000 (3% of available data) across all three PDE systems. It substantially reduces error accumulation over extended temporal horizons: averaged across all cases, GNS reduces autoregressive error by 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a PCA+KMeans trajectory selection strategy enhancing low-data performance. Results indicate combining graph-based local inductive biases with conventional time integrators yields accurate, physically consistent, and scalable surrogate models for time-dependent PDEs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models</title>
<link>https://arxiv.org/abs/2509.06161</link>
<guid>https://arxiv.org/abs/2509.06161</guid>
<content:encoded><![CDATA[
<div> Keywords: human activity recognition, ultra-wideband technology, deep learning models, RSSI data, hybrid CNN+LSTM model

Summary:
This study delves into using ultra-wideband technology for tracking inhabitant paths in home environments via deep learning models. The technology estimates user locations utilizing time-of-flight and time-difference-of-arrival methods, with precision affected by obstacles. A fingerprinting-based approach using RSSI data collected from two flats is proposed to address challenges. The performance of CNN, LSTM, hybrid CNN+LSTM models, and Bluetooth technology is compared. The impact of the temporal window type and duration on accuracy is evaluated. Results show a mean absolute error of around 50 cm, with the hybrid model outperforming others in providing precise location estimates for human activity recognition in residential settings.<br /><br />Summary: <div>
arXiv:2509.06161v1 Announce Type: new 
Abstract: The field of human activity recognition has evolved significantly, driven largely by advancements in Internet of Things (IoT) device technology, particularly in personal devices. This study investigates the use of ultra-wideband (UWB) technology for tracking inhabitant paths in home environments using deep learning models. UWB technology estimates user locations via time-of-flight and time-difference-of-arrival methods, which are significantly affected by the presence of walls and obstacles in real environments, reducing their precision. To address these challenges, we propose a fingerprinting-based approach utilizing received signal strength indicator (RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while performing daily activities. We compare the performance of convolutional neural network (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as well as the use of Bluetooth technology. Additionally, we evaluate the impact of the type and duration of the temporal window (future, past, or a combination of both). Our results demonstrate a mean absolute error close to 50 cm, highlighting the superiority of the hybrid model in providing accurate location estimates, thus facilitating its application in daily human activity recognition in residential settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Improved Template for Approximate Computing</title>
<link>https://arxiv.org/abs/2509.06162</link>
<guid>https://arxiv.org/abs/2509.06162</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, edge devices, approximate computing, area savings, XPAT<br />
Summary:<br />
The article introduces a methodology to reduce the area of small arithmetic operators in neural networks deployed on edge devices. By sacrificing a small amount of accuracy, the energy consumption during inference can be decreased. The approach involves using approximate computing techniques to optimize the balance between energy and accuracy. The authors build upon a recent boolean rewriting technique called XPAT, which utilizes a parametrisable template to rewrite circuits. They propose a new template based on parametrisable product sharing that acts as a close proxy to synthesised area. Through experiments, it is shown that this methodology outperforms both the original XPAT and two other state-of-the-art approaches in finding low-area solutions with better approximations. Ultimately, the methodology offers a promising way to enhance the efficiency of neural network deployment on edge devices. <br /><br />Summary: <div>
arXiv:2509.06162v1 Announce Type: new 
Abstract: Deploying neural networks on edge devices entails a careful balance between the energy required for inference and the accuracy of the resulting classification. One technique for navigating this tradeoff is approximate computing: the process of reducing energy consumption by slightly reducing the accuracy of arithmetic operators. In this context, we propose a methodology to reduce the area of the small arithmetic operators used in neural networks - i.e., adders and multipliers - via a small loss in accuracy, and show that we improve area savings for the same accuracy loss w.r.t. the state of the art. To achieve our goal, we improve on a boolean rewriting technique recently proposed, called XPAT, where the use of a parametrisable template to rewrite circuits has proved to be highly beneficial. In particular, XPAT was able to produce smaller circuits than comparable approaches while utilising a naive sum of products template structure. In this work, we show that template parameters can act as proxies for chosen metrics and we propose a novel template based on parametrisable product sharing that acts as a close proxy to synthesised area. We demonstrate experimentally that our methodology converges better to low-area solutions and that it can find better approximations than both the original XPAT and two other state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Urban Factors with Autoencoders: Relationship Between Static and Dynamic Features</title>
<link>https://arxiv.org/abs/2509.06167</link>
<guid>https://arxiv.org/abs/2509.06167</guid>
<content:encoded><![CDATA[
<div> framework, visualization, urban analytics, data fusion, patterns
Summary:
The article introduces a visualization-assisted framework aimed at analyzing the effectiveness of fused latent data representations compared to separate representations in uncovering patterns from dynamic and static urban data. Urban analytics relies on extensive datasets to simulate, predict trends, and identify complex urban patterns. However, challenges arise due to the granularity, heterogeneity, and multimodality of the data. The developed framework explores whether combining latent representations offers deeper insights than analyzing each data source independently. The analysis demonstrates that combined latent representations reveal more structured patterns, indicating the potential benefits of fused data in urban analysis. However, separate representations also prove valuable in specific scenarios. The framework emphasizes the importance of visual analytics in handling urban data complexities and highlights the advantages of integrating diverse data sources for comprehensive urban analysis. 

<br /><br />Summary: <div>
arXiv:2509.06167v1 Announce Type: new 
Abstract: Urban analytics utilizes extensive datasets with diverse urban information to simulate, predict trends, and uncover complex patterns within cities. While these data enables advanced analysis, it also presents challenges due to its granularity, heterogeneity, and multimodality. To address these challenges, visual analytics tools have been developed to support the exploration of latent representations of fused heterogeneous and multimodal data, discretized at a street-level of detail. However, visualization-assisted tools seldom explore the extent to which fused data can offer deeper insights than examining each data source independently within an integrated visualization framework. In this work, we developed a visualization-assisted framework to analyze whether fused latent data representations are more effective than separate representations in uncovering patterns from dynamic and static urban data. The analysis reveals that combined latent representations produce more structured patterns, while separate ones are useful in particular cases.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Language Model for Personalized Lung Cancer Screening</title>
<link>https://arxiv.org/abs/2509.06169</link>
<guid>https://arxiv.org/abs/2509.06169</guid>
<content:encoded><![CDATA[
<div> Keywords: lung cancer screening, risk assessment, Lung-RADS, reasoning language model, individualized risk assessment 

Summary: 
Accurate risk assessment in lung cancer screening is crucial for early detection and reducing unnecessary procedures. The Lung CT Screening Reporting and Data System (Lung-RADS) is commonly used but has limitations in balancing sensitivity and specificity. In this study, researchers propose a reasoning language model (RLM) that combines radiology findings with patient records for personalized risk evaluation. By integrating diverse risk factors and utilizing data-driven analysis, the RLM demonstrates improved prediction accuracy and interpretability. Through a systematic approach involving dataset construction, fine-tuning, and reinforcement learning, the model can break down risk assessment tasks, evaluate various risk factors, and generate a final risk score. This method enhances both predictive performance and clinical decision-making in lung cancer screening. <br /><br />Summary: <div>
arXiv:2509.06169v1 Announce Type: new 
Abstract: Accurate risk assessment in lung cancer screening is critical for enabling early cancer detection and minimizing unnecessary invasive procedures. The Lung CT Screening Reporting and Data System (Lung-RADS) has been widely used as the standard framework for patient management and follow-up. Nevertheless, Lung-RADS faces trade-offs between sensitivity and specificity, as it stratifies risk solely based on lung nodule characteristics without incorporating various risk factors. Here we propose a reasoning language model (RLM) to integrate radiology findings with longitudinal medical records for individualized lung cancer risk assessment. Through a systematic study including dataset construction and distillation, supervised fine-tuning, reinforcement learning, and comprehensive evaluation, our model makes significant improvements in risk prediction performance on datasets in the national lung screening trial. Notably, RLM can decompose the risk evaluation task into sub-components, analyze the contributions of diverse risk factors, and synthesize them into a final risk score computed using our data-driven system equation. Our approach improves both predictive accuracy and monitorability through the chain of thought reasoning process, thereby facilitating clinical translation into lung cancer screening.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.06213</link>
<guid>https://arxiv.org/abs/2509.06213</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, Game Of Hidden Rules (GOHR), state representation, Transformer-based Advantage Actor-Critic (A2C), learning efficiency
Summary:
The study focuses on reinforcement learning in the Game Of Hidden Rules (GOHR), a puzzle where an agent must infer and execute hidden rules to clear a 6x6 board by placing game pieces into buckets. Two state representation strategies, Feature-Centric (FC) and Object-Centric (OC), are explored using a Transformer-based Advantage Actor-Critic (A2C) algorithm for training. The agent, with access only to partial observations, must infer the governing rule and learn the optimal policy simultaneously through experience. The models are evaluated in various experimental setups, analyzing transfer effects and the impact of representation on learning efficiency. The research sheds light on the effectiveness of different state representation strategies and their influence on reinforcement learning performance in complex environments like GOHR. <br /><br />Summary: <div>
arXiv:2509.06213v1 Announce Type: new 
Abstract: We investigate reinforcement learning in the Game Of Hidden Rules (GOHR) environment, a complex puzzle in which an agent must infer and execute hidden rules to clear a 6$\times$6 board by placing game pieces into buckets. We explore two state representation strategies, namely Feature-Centric (FC) and Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic (A2C) algorithm for training. The agent has access only to partial observations and must simultaneously infer the governing rule and learn the optimal policy through experience. We evaluate our models across multiple rule-based and trial-list-based experimental setups, analyzing transfer effects and the impact of representation on learning efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metric Embedding Initialization-Based Differentially Private and Explainable Graph Clustering</title>
<link>https://arxiv.org/abs/2509.06214</link>
<guid>https://arxiv.org/abs/2509.06214</guid>
<content:encoded><![CDATA[
<div> Keywords: graph clustering, differential privacy, interpretable, metric embedding, optimization

Summary:
This paper introduces a novel approach for graph clustering under the framework of differential privacy. The proposed method addresses challenges such as high noise, low efficiency, and poor interpretability that have hindered the progress in this field. The approach utilizes metric embedding initialization to provide a differentially private and interpretable clustering solution. By optimizing an SDP formulation and using an HST-based initialization method, the algorithm generates well-initialized clustering configurations. Subsequently, a k-median clustering strategy is applied to obtain cluster results, with comparative explanations offered for the query set based on differences from the cluster centers. Experimental results on public datasets demonstrate the superior performance of the proposed framework in various clustering metrics while maintaining privacy guarantees. <div>
arXiv:2509.06214v1 Announce Type: new 
Abstract: Graph clustering under the framework of differential privacy, which aims to process graph-structured data while protecting individual privacy, has been receiving increasing attention. Despite significant achievements in current research, challenges such as high noise, low efficiency and poor interpretability continue to severely constrain the development of this field. In this paper, we construct a differentially private and interpretable graph clustering approach based on metric embedding initialization. Specifically, we construct an SDP optimization, extract the key set and provide a well-initialized clustering configuration using an HST-based initialization method. Subsequently, we apply an established k-median clustering strategy to derive the cluster results and offer comparative explanations for the query set through differences from the cluster centers. Extensive experiments on public datasets demonstrate that our proposed framework outperforms existing methods in various clustering metrics while strictly ensuring privacy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning</title>
<link>https://arxiv.org/abs/2509.06219</link>
<guid>https://arxiv.org/abs/2509.06219</guid>
<content:encoded><![CDATA[
arXiv:2509.06219v1 Announce Type: new 
Abstract: Exemplar-free class-incremental learning enables models to learn new classes over time without storing data from old ones. As multimodal graph-structured data becomes increasingly prevalent, existing methods struggle with challenges like catastrophic forgetting, distribution bias, memory limits, and weak generalization. We propose MCIGLE, a novel framework that addresses these issues by extracting and aligning multimodal graph features and applying Concatenated Recursive Least Squares for effective knowledge retention. Through multi-channel processing, MCIGLE balances accuracy and memory preservation. Experiments on public datasets validate its effectiveness and generalizability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks</title>
<link>https://arxiv.org/abs/2509.06270</link>
<guid>https://arxiv.org/abs/2509.06270</guid>
<content:encoded><![CDATA[
arXiv:2509.06270v1 Announce Type: new 
Abstract: Sixth generation (6G) systems require environment-aware communication, driven by native artificial intelligence (AI) and integrated sensing and communication (ISAC). Radio maps (RMs), providing spatially continuous channel information, are key enablers. However, generating high-fidelity RM ground truth via electromagnetic (EM) simulations is computationally intensive, motivating machine learning (ML)-based RM construction. The effectiveness of these data-driven methods depends on large-scale, high-quality training data. Current public datasets often focus on single-input single-output (SISO) and limited information, such as path loss, which is insufficient for advanced multi-input multi-output (MIMO) systems requiring detailed channel state information (CSI). To address this gap, this paper presents UrbanMIMOMap, a novel large-scale urban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap offers comprehensive complex CSI matrices across a dense spatial grid, going beyond traditional path loss data. This rich CSI is vital for constructing high-fidelity RMs and serves as a fundamental resource for data-driven RM generation, including deep learning. We demonstrate the dataset's utility through baseline performance evaluations of representative ML methods for RM construction. This work provides a crucial dataset and reference for research in high-precision RM generation, MIMO spatial performance, and ML for 6G environment awareness. The code and data for this work are available at: https://github.com/UNIC-Lab/UrbanMIMOMap.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPR: Intelligent Prompt Routing with User-Controlled Quality-Cost Trade-offs</title>
<link>https://arxiv.org/abs/2509.06274</link>
<guid>https://arxiv.org/abs/2509.06274</guid>
<content:encoded><![CDATA[
arXiv:2509.06274v1 Announce Type: new 
Abstract: Routing incoming queries to the most cost-effective LLM while maintaining response quality poses a fundamental challenge in optimizing performance-cost trade-offs for large-scale commercial systems. We present IPR\, a quality-constrained Intelligent Prompt Routing framework that dynamically selects optimal models based on predicted response quality and user-specified tolerance levels. IPR introduces three key innovations: (1) a modular architecture with lightweight quality estimators trained on 1.5M prompts annotated with calibrated quality scores, enabling fine-grained quality prediction across model families; (2) a user-controlled routing mechanism with tolerance parameter $\tau \in [0,1]$ that provides explicit control over quality-cost trade-offs; and (3) an extensible design using frozen encoders with model-specific adapters, reducing new model integration from days to hours. To rigorously train and evaluate IPR, we curate an industrial-level dataset IPRBench\footnote{IPRBench will be released upon legal approval.}, a comprehensive benchmark containing 1.5 million examples with response quality annotations across 11 LLM candidates. Deployed on a major cloud platform, IPR achieves 43.9\% cost reduction while maintaining quality parity with the strongest model in the Claude family and processes requests with sub-150ms latency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations</title>
<link>https://arxiv.org/abs/2509.06286</link>
<guid>https://arxiv.org/abs/2509.06286</guid>
<content:encoded><![CDATA[
arXiv:2509.06286v1 Announce Type: new 
Abstract: Personalization is a core capability across consumer technologies, streaming, shopping, wearables, and voice, yet it remains challenged by sparse interactions, fast content churn, and heterogeneous textual signals. We present RecMind, an LLM-enhanced graph recommender that treats the language model as a preference prior rather than a monolithic ranker. A frozen LLM equipped with lightweight adapters produces text-conditioned user/item embeddings from titles, attributes, and reviews; a LightGCN backbone learns collaborative embeddings from the user-item graph. We align the two views with a symmetric contrastive objective and fuse them via intra-layer gating, allowing language to dominate in cold/long-tail regimes and graph structure to stabilize rankings elsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results on all eight reported metrics, with relative improvements up to +4.53\% (Recall@40) and +4.01\% (NDCG@40) over strong baselines. Ablations confirm both the necessity of cross-view alignment and the advantage of gating over late fusion and LLM-only variants.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults</title>
<link>https://arxiv.org/abs/2509.06289</link>
<guid>https://arxiv.org/abs/2509.06289</guid>
<content:encoded><![CDATA[
arXiv:2509.06289v1 Announce Type: new 
Abstract: Silent Data Errors (SDEs) from time-zero defects and aging degrade safety-critical systems. Functional testing detects SDE-related faults but is expensive to simulate. We present a unified spatio-temporal graph convolutional network (ST-GCN) for fast, accurate prediction of long-cycle fault impact probabilities (FIPs) in large sequential circuits, supporting quantitative risk assessment. Gate-level netlists are modeled as spatio-temporal graphs to capture topology and signal timing; dedicated spatial and temporal encoders predict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method reduces simulation time by more than 10x while maintaining high accuracy (mean absolute error 0.024 for 5-cycle predictions). The framework accepts features from testability metrics or fault simulation, allowing efficiency-accuracy trade-offs. A test-point selection study shows that choosing observation points by predicted FIPs improves detection of long-cycle, hard-to-detect faults. The approach scales to SoC-level test strategy optimization and fits downstream electronic design automation flows.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoaQ: Layer-wise Output Approximation Quantization</title>
<link>https://arxiv.org/abs/2509.06297</link>
<guid>https://arxiv.org/abs/2509.06297</guid>
<content:encoded><![CDATA[
arXiv:2509.06297v1 Announce Type: new 
Abstract: A natural and intuitive idea in model quantization is to approximate each component's quantized output to match its original. Layer-wise post-training quantization (PTQ), though based on this idea, adopts a strictly local view and can achieve, at best, only activation-aware approximations of weights. As a result, it often leads to insufficient approximations and practical deviations from this guiding intuition. Recent work has achieved a more accurate approximation of linear-layer outputs within the framework of layer-wise PTQ, but such refinements remain inadequate for achieving alignment with the full model output. Based on a deeper understanding of the structural characteristics of mainstream LLMs, we propose $LoaQ$, an output-approximation method for layer-wise PTQ that explicitly targets output-level consistency. It better aligns with this intuition and can feature a simple closed-form solution, making it orthogonal to existing techniques and readily integrable into existing quantization pipelines. Experiments on the LLaMA and Qwen model families demonstrate that LoaQ performs effectively in both weight-only and weight-activation joint quantization. By integrating seamlessly with existing quantization strategies, it further enhances overall quantization quality and shows strong potential to advance the frontier of post-training quantization.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting</title>
<link>https://arxiv.org/abs/2509.06311</link>
<guid>https://arxiv.org/abs/2509.06311</guid>
<content:encoded><![CDATA[
arXiv:2509.06311v1 Announce Type: new 
Abstract: High-quality wind power forecasting is crucial for the operation of modern power grids. However, prevailing data-driven paradigms either train a site-specific model which cannot generalize to other locations or rely on fine-tuning of general-purpose time series foundation models which are difficult to incorporate domain-specific data in the energy sector. This paper introduces WindFM, a lightweight and generative Foundation Model designed specifically for probabilistic wind power forecasting. WindFM employs a discretize-and-generate framework. A specialized time-series tokenizer first converts continuous multivariate observations into discrete, hierarchical tokens. Subsequently, a decoder-only Transformer learns a universal representation of wind generation dynamics by autoregressively pre-training on these token sequences. Using the comprehensive WIND Toolkit dataset comprising approximately 150 billion time steps from more than 126,000 sites, WindFM develops a foundational understanding of the complex interplay between atmospheric conditions and power output. Extensive experiments demonstrate that our compact 8.1M parameter model achieves state-of-the-art zero-shot performance on both deterministic and probabilistic tasks, outperforming specialized models and larger foundation models without any fine-tuning. In particular, WindFM exhibits strong adaptiveness under out-of-distribution data from a different continent, demonstrating the robustness and transferability of its learned representations. Our pre-trained model is publicly available at https://github.com/shiyu-coder/WindFM.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix</title>
<link>https://arxiv.org/abs/2509.06314</link>
<guid>https://arxiv.org/abs/2509.06314</guid>
<content:encoded><![CDATA[
arXiv:2509.06314v1 Announce Type: new 
Abstract: A central challenge in representation learning is constructing latent embeddings that are both expressive and efficient. In practice, deep networks often produce redundant latent spaces where multiple coordinates encode overlapping information, reducing effective capacity and hindering generalization. Standard metrics such as accuracy or reconstruction loss provide only indirect evidence of such redundancy and cannot isolate it as a failure mode. We introduce a redundancy index, denoted rho(C), that directly quantifies inter-dimensional dependencies by analyzing coupling matrices derived from latent representations and comparing their off-diagonal statistics against a normal distribution via energy distance. The result is a compact, interpretable, and statistically grounded measure of representational quality. We validate rho(C) across discriminative and generative settings on MNIST variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple architectures and hyperparameter optimization strategies. Empirically, low rho(C) reliably predicts high classification accuracy or low reconstruction error, while elevated redundancy is associated with performance collapse. Estimator reliability grows with latent dimension, yielding natural lower bounds for reliable analysis. We further show that Tree-structured Parzen Estimators (TPE) preferentially explore low-rho regions, suggesting that rho(C) can guide neural architecture search and serve as a redundancy-aware regularization target. By exposing redundancy as a universal bottleneck across models and tasks, rho(C) offers both a theoretical lens and a practical tool for evaluating and improving the efficiency of learned representations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics</title>
<link>https://arxiv.org/abs/2509.06322</link>
<guid>https://arxiv.org/abs/2509.06322</guid>
<content:encoded><![CDATA[
arXiv:2509.06322v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated emergent in-context learning (ICL) capabilities across a range of tasks, including zero-shot time-series forecasting. We show that text-trained foundation models can accurately extrapolate spatiotemporal dynamics from discretized partial differential equation (PDE) solutions without fine-tuning or natural language prompting. Predictive accuracy improves with longer temporal contexts but degrades at finer spatial discretizations. In multi-step rollouts, where the model recursively predicts future spatial states over multiple time steps, errors grow algebraically with the time horizon, reminiscent of global error accumulation in classical finite-difference solvers. We interpret these trends as in-context neural scaling laws, where prediction quality varies predictably with both context length and output length. To better understand how LLMs are able to internally process PDE solutions so as to accurately roll them out, we analyze token-level output distributions and uncover a consistent ICL progression: beginning with syntactic pattern imitation, transitioning through an exploratory high-entropy phase, and culminating in confident, numerically grounded predictions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring approaches to computational representation and classification of user-generated meal logs</title>
<link>https://arxiv.org/abs/2509.06330</link>
<guid>https://arxiv.org/abs/2509.06330</guid>
<content:encoded><![CDATA[
arXiv:2509.06330v1 Announce Type: new 
Abstract: This study examined the use of machine learning and domain specific enrichment on patient generated health data, in the form of free text meal logs, to classify meals on alignment with different nutritional goals. We used a dataset of over 3000 meal records collected by 114 individuals from a diverse, low income community in a major US city using a mobile app. Registered dietitians provided expert judgement for meal to goal alignment, used as gold standard for evaluation. Using text embeddings, including TFIDF and BERT, and domain specific enrichment information, including ontologies, ingredient parsers, and macronutrient contents as inputs, we evaluated the performance of logistic regression and multilayer perceptron classifiers using accuracy, precision, recall, and F1 score against the gold standard and self assessment. Even without enrichment, ML outperformed self assessments of individuals who logged meals, and the best performing combination of ML classifier with enrichment achieved even higher accuracies. In general, ML classifiers with enrichment of Parsed Ingredients, Food Entities, and Macronutrients information performed well across multiple nutritional goals, but there was variability in the impact of enrichment and classification algorithm on accuracy of classification for different nutritional goals. In conclusion, ML can utilize unstructured free text meal logs and reliably classify whether meals align with specific nutritional goals, exceeding self assessments, especially when incorporating nutrition domain knowledge. Our findings highlight the potential of ML analysis of patient generated health data to support patient centered nutrition guidance in precision healthcare.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2509.06332</link>
<guid>https://arxiv.org/abs/2509.06332</guid>
<content:encoded><![CDATA[
arXiv:2509.06332v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable emergent capabilities, yet the robustness of their numerical reasoning remains an open question. While standard benchmarks evaluate LLM reasoning on complex problem sets using aggregated metrics, they often obscure foundational weaknesses. In this work, we probe LLM mathematical numeracy by evaluating performance on problems of escalating complexity, from constituent operations to combinatorial puzzles. We test several state-of-the-art LLM-based agents on a 100-problem challenge comprising four categories: (1) basic arithmetic, (2) advanced operations, (3) primality checking, and (4) the Game of 24 number puzzle. Our results show that while the agents achieved high accuracy on the first three categories, which require deterministic algorithmic execution, they consistently failed at the number puzzle, underlining its demand for a heuristic search over a large combinatorial space to be a significant bottleneck. These findings reveal that the agents' proficiency is largely confined to recalling and executing known algorithms, rather than performing generative problem-solving. This suggests their apparent numerical reasoning is more akin to sophisticated pattern-matching than flexible, analytical thought, limiting their potential for tasks that require novel or creative numerical insights.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ban&amp;Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs</title>
<link>https://arxiv.org/abs/2509.06346</link>
<guid>https://arxiv.org/abs/2509.06346</guid>
<content:encoded><![CDATA[
arXiv:2509.06346v1 Announce Type: new 
Abstract: Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling large language models (LLMs) efficiently. Recent fine-grained MoE designs introduce hundreds of experts per layer, with multiple experts activated per token, enabling stronger specialization. However, during pre-training, routers are optimized mainly for stability and robustness: they converge prematurely and enforce balanced usage, limiting the full potential of model performance and efficiency. In this work, we uncover two overlooked issues: (i) a few highly influential experts are underutilized due to premature and balanced routing decisions; and (ii) enforcing a fixed number of active experts per token introduces substantial redundancy. Instead of retraining models or redesigning MoE architectures, we introduce Ban&amp;Pick, a post-training, plug-and-play strategy for smarter MoE routing. Pick discovers and reinforces key experts-a small group with outsized impact on performance-leading to notable accuracy gains across domains. Ban complements this by dynamically pruning redundant experts based on layer and token sensitivity, delivering faster inference with minimal accuracy loss. Experiments on fine-grained MoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks demonstrate that Ban&amp;Pick delivers free performance gains and inference acceleration without retraining or architectural changes. For instance, on Qwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from 65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the vLLM.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment</title>
<link>https://arxiv.org/abs/2509.06371</link>
<guid>https://arxiv.org/abs/2509.06371</guid>
<content:encoded><![CDATA[
arXiv:2509.06371v1 Announce Type: new 
Abstract: Due to hardware and software improvements, an increasing number of AI models are deployed on-device. This shift enhances privacy and reduces latency, but also introduces security risks distinct from traditional software. In this article, we examine these risks through the real-world case study of SafetyCore, an Android system service incorporating sensitive image content detection. We demonstrate how the on-device AI model can be extracted and manipulated to bypass detection, effectively rendering the protection ineffective. Our analysis exposes vulnerabilities of on-device AI models and provides a practical demonstration of how adversaries can exploit them.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection</title>
<link>https://arxiv.org/abs/2509.06383</link>
<guid>https://arxiv.org/abs/2509.06383</guid>
<content:encoded><![CDATA[
arXiv:2509.06383v1 Announce Type: new 
Abstract: Selecting key variables from high-dimensional data is increasingly important in the era of big data. Sparse regression serves as a powerful tool for this purpose by promoting model simplicity and explainability. In this work, we revisit a valuable yet underutilized method, the statistical physics-based Variational Garrote (VG), which introduces explicit feature selection spin variables and leverages variational inference to derive a tractable loss function. We enhance VG by incorporating modern automatic differentiation techniques, enabling scalable and efficient optimization. We evaluate VG on both fully controllable synthetic datasets and complex real-world datasets. Our results demonstrate that VG performs especially well in highly sparse regimes, offering more consistent and robust variable selection than Ridge and LASSO regression across varying levels of sparsity. We also uncover a sharp transition: as superfluous variables are admitted, generalization degrades abruptly and the uncertainty of the selection variables increases. This transition point provides a practical signal for estimating the correct number of relevant variables, an insight we successfully apply to identify key predictors in real-world data. We expect that VG offers strong potential for sparse modeling across a wide range of applications, including compressed sensing and model pruning in machine learning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting</title>
<link>https://arxiv.org/abs/2509.06385</link>
<guid>https://arxiv.org/abs/2509.06385</guid>
<content:encoded><![CDATA[
arXiv:2509.06385v1 Announce Type: new 
Abstract: Typical financial risk management involves distinct phases for pre-service risk assessment and in-service default detection, often modeled separately. This paper proposes a novel framework, Multi-Granularity Knowledge Distillation (abbreviated as MGKD), aimed at improving pre-service risk prediction through the integration of in-service user behavior data. MGKD follows the idea of knowledge distillation, where the teacher model, trained on historical in-service data, guides the student model, which is trained on pre-service data. By using soft labels derived from in-service data, the teacher model helps the student model improve its risk prediction prior to service activation. Meanwhile, a multi-granularity distillation strategy is introduced, including coarse-grained, fine-grained, and self-distillation, to align the representations and predictions of the teacher and student models. This approach not only reinforces the representation of default cases but also enables the transfer of key behavioral patterns associated with defaulters from the teacher to the student model, thereby improving the overall performance of pre-service risk assessment. Moreover, we adopt a re-weighting strategy to mitigate the model's bias towards the minority class. Experimental results on large-scale real-world datasets from Tencent Mobile Payment demonstrate the effectiveness of our proposed approach in both offline and online scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints</title>
<link>https://arxiv.org/abs/2509.06395</link>
<guid>https://arxiv.org/abs/2509.06395</guid>
<content:encoded><![CDATA[
arXiv:2509.06395v1 Announce Type: new 
Abstract: Meeting minimum data rate constraints is a significant challenge in wireless communication systems, particularly as network complexity grows. Traditional deep learning approaches often address these constraints by incorporating penalty terms into the loss function and tuning hyperparameters empirically. However, this heuristic treatment offers no theoretical convergence guarantees and frequently fails to satisfy QoS requirements in practical scenarios. Building upon the structure of the WMMSE algorithm, we first extend it to a multi-channel setting with QoS constraints, resulting in the enhanced WMMSE (eWMMSE) algorithm, which is provably convergent to a locally optimal solution when the problem is feasible. To further reduce computational complexity and improve scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of supporting simultaneous multi-channel allocation per user. To overcome the limitations of traditional deep learning methods, we propose a principled framework that integrates GNN with a Lagrangian-based primal-dual optimization method. By training the GNN within the Lagrangian framework, we ensure satisfaction of QoS constraints and convergence to a stationary point. Extensive simulations demonstrate that JCPGNN-M matches the performance of eWMMSE while offering significant gains in inference speed, generalization to larger networks, and robustness under imperfect channel state information. This work presents a scalable and theoretically grounded solution for constrained resource allocation in future wireless networks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network Executables</title>
<link>https://arxiv.org/abs/2509.06402</link>
<guid>https://arxiv.org/abs/2509.06402</guid>
<content:encoded><![CDATA[
arXiv:2509.06402v1 Announce Type: new 
Abstract: On-device deep learning models have extensive real world demands. Deep learning compilers efficiently compile models into executables for deployment on edge devices, but these executables may face the threat of reverse engineering. Previous studies have attempted to decompile DNN executables, but they face challenges in handling compilation optimizations and analyzing quantized compiled models. In this paper, we present NeuroDeX to unlock diverse support in decompiling DNN executables. NeuroDeX leverages the semantic understanding capabilities of LLMs along with dynamic analysis to accurately and efficiently perform operator type recognition, operator attribute recovery and model reconstruction. NeuroDeX can recover DNN executables into high-level models towards compilation optimizations, different architectures and quantized compiled models. We conduct experiments on 96 DNN executables across 12 common DNN models. Extensive experimental results demonstrate that NeuroDeX can decompile non-quantized executables into nearly identical high-level models. NeuroDeX can recover functionally similar high-level models for quantized executables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more comprehensive and effective solution compared to previous DNN executables decompilers.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup</title>
<link>https://arxiv.org/abs/2509.06419</link>
<guid>https://arxiv.org/abs/2509.06419</guid>
<content:encoded><![CDATA[
arXiv:2509.06419v1 Announce Type: new 
Abstract: Time series anomaly detection (TSAD) is a vital yet challenging task, particularly in scenarios where labeled anomalies are scarce and temporal dependencies are complex. Recent anomaly assumption (AA) approaches alleviate the lack of anomalies by injecting synthetic samples and training discriminative models. Despite promising results, these methods often suffer from two fundamental limitations: patchy generation, where scattered anomaly knowledge leads to overly simplistic or incoherent anomaly injection, and Anomaly Shift, where synthetic anomalies either resemble normal data too closely or diverge unrealistically from real anomalies, thereby distorting classification boundaries. In this paper, we propose CAPMix, a controllable anomaly augmentation framework that addresses both issues. First, we design a CutAddPaste mechanism to inject diverse and complex anomalies in a targeted manner, avoiding patchy generation. Second, we introduce a label revision strategy to adaptively refine anomaly labels, reducing the risk of anomaly shift. Finally, we employ dual-space mixup within a temporal convolutional network to enforce smoother and more robust decision boundaries. Extensive experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and ESA, demonstrate that CAPMix achieves significant improvements over state-of-the-art baselines, with enhanced robustness against contaminated training data. The code is available at https://github.com/alsike22/CAPMix.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
<link>https://arxiv.org/abs/2509.06465</link>
<guid>https://arxiv.org/abs/2509.06465</guid>
<content:encoded><![CDATA[
arXiv:2509.06465v1 Announce Type: new 
Abstract: Antibody binding site prediction plays a pivotal role in computational immunology and therapeutic antibody design. Existing sequence or structure methods rely on single-view features and fail to identify antibody-specific binding sites on the antigens-a dual limitation in representation and prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding site prediction. CAME-AB integrates five biologically grounded modalities, including raw amino acid encodings, BLOSUM substitution profiles, pretrained language model embeddings, structure-aware features, and GCN-refined biochemical graphs-into a unified multimodal representation. To enhance adaptive cross-modal reasoning, we propose an adaptive modality fusion module that learns to dynamically weight each modality based on its global relevance and input-specific contribution. A Transformer encoder combined with an MoE module further promotes feature specialization and capacity expansion. We additionally incorporate a supervised contrastive learning objective to explicitly shape the latent space geometry, encouraging intra-class compactness and inter-class separability. To improve optimization stability and generalization, we apply stochastic weight averaging during training. Extensive experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB consistently outperforms strong baselines on multiple metrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further validate the effectiveness of each architectural component and the benefit of multimodal feature integration. The model implementation details and the codes are available on https://anonymous.4open.science/r/CAME-AB-C525
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT</title>
<link>https://arxiv.org/abs/2509.06483</link>
<guid>https://arxiv.org/abs/2509.06483</guid>
<content:encoded><![CDATA[
arXiv:2509.06483v1 Announce Type: new 
Abstract: The wide spreading of Internet of Things (IoT) sensors generates vast spatio-temporal data streams, but ensuring data credibility is a critical yet unsolved challenge for applications like smart homes. While spatio-temporal graph (STG) models are a leading paradigm for such data, they often fall short in dynamic, human-centric environments due to two fundamental limitations: (1) their reliance on static graph topologies, which fail to capture physical, event-driven dynamics, and (2) their tendency to confuse spurious correlations with true causality, undermining robustness in human-centric environments. To address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network (DyC-STG), a novel framework designed for real-time data credibility analysis in IoT. Our framework features two synergistic contributions: an event-driven dynamic graph module that adapts the graph topology in real-time to reflect physical state changes, and a causal reasoning module to distill causally-aware representations by strictly enforcing temporal precedence. To facilitate the research in this domain we release two new real-world datasets. Comprehensive experiments show that DyC-STG establishes a new state-of-the-art, outperforming the strongest baselines by 1.4 percentage points and achieving an F1-Score of up to 0.930.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A machine-learned expression for the excess Gibbs energy</title>
<link>https://arxiv.org/abs/2509.06484</link>
<guid>https://arxiv.org/abs/2509.06484</guid>
<content:encoded><![CDATA[
arXiv:2509.06484v1 Announce Type: new 
Abstract: The excess Gibbs energy plays a central role in chemical engineering and chemistry, providing a basis for modeling the thermodynamic properties of liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures solely from the molecular structures of their components is a long-standing challenge. In this work, we address this challenge by integrating physical laws as hard constraints within a flexible neural network. The resulting model, HANNA, was trained end-to-end on an extensive experimental dataset for binary mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent predictions. A novel surrogate solver developed in this work enabled the inclusion of liquid-liquid equilibrium data in the training process. Furthermore, a geometric projection method was applied to enable robust extrapolations to multi-component mixtures, without requiring additional parameters. We demonstrate that HANNA delivers excellent predictions, clearly outperforming state-of-the-art benchmark methods in accuracy and scope. The trained model and corresponding code are openly available, and an interactive interface is provided on our website, MLPROP.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data</title>
<link>https://arxiv.org/abs/2509.06505</link>
<guid>https://arxiv.org/abs/2509.06505</guid>
<content:encoded><![CDATA[
arXiv:2509.06505v1 Announce Type: new 
Abstract: The generative adversarial network (GAN) aims to approximate an unknown distribution via a parameterized neural network (NN). While GANs have been widely applied in reinforcement and semisupervised learning as well as computer vision tasks, selecting their parameters often needs an exhaustive search and only a few selection methods can be proved to be theoretically optimal. One of the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on optimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG) setting, where the NN is linear and the data is Gaussian. In this paper, we focus on the characterization of optimal WGAN parameters beyond the LQG setting. We derive closed-form optimal parameters for one-dimensional WGANs when the NN has non-linear activation functions and the data is non-Gaussian. To extend this to high-dimensional WGANs, we adopt the sliced Wasserstein framework and replace the constraint on marginal distributions of the randomly projected data by a constraint on the joint distribution of the original (unprojected) data. We show that the linear generator can be asymptotically optimal for sliced WGAN with non-Gaussian data. Empirical studies show that our closed-form WGAN parameters have good convergence behavior with data under both Gaussian and Laplace distributions. Also, compared to the r principal component analysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve the same performance while requiring less computational resources.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients</title>
<link>https://arxiv.org/abs/2509.06516</link>
<guid>https://arxiv.org/abs/2509.06516</guid>
<content:encoded><![CDATA[
arXiv:2509.06516v1 Announce Type: new 
Abstract: Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in intesive care unit (ICU) and operating room (OR). However, the high incidence of poor, incomplete, and inconsistent signal quality, can lead to false alarms or diagnostic inaccuracies. The methods explored so far suffer from limited generalizability, reliance on extensive labeled data, and poor cross-task transferability. To overcome these challenges, we introduce QualityFM, a novel multimodal foundation model for these physiological signals, designed to acquire a general-purpose understanding of signal quality. Our model is pre-trained on an large-scale dataset comprising over 21 million 30-second waveforms and 179,757 hours of data. Our approach involves a dual-track architecture that processes paired physiological signals of differing quality, leveraging a self-distillation strategy where an encoder for high-quality signals is used to guide the training of an encoder for low-quality signals. To efficiently handle long sequential signals and capture essential local quasi-periodic patterns, we integrate a windowed sparse attention mechanism within our Transformer-based model. Furthermore, a composite loss function, which combines direct distillation loss on encoder outputs with indirect reconstruction loss based on power and phase spectra, ensures the preservation of frequency-domain characteristics of the signals. We pre-train three models with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy and practical value through transfer learning on three distinct clinical tasks: false alarm of ventricular tachycardia detection, the identification of atrial fibrillation and the estimation of arterial blood pressure (ABP) from PPG and ECG signals.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lane Change Intention Prediction of two distinct Populations using a Transformer</title>
<link>https://arxiv.org/abs/2509.06529</link>
<guid>https://arxiv.org/abs/2509.06529</guid>
<content:encoded><![CDATA[
arXiv:2509.06529v1 Announce Type: new 
Abstract: As a result of the growing importance of lane change intention prediction for a safe and efficient driving experience in complex driving scenarios, researchers have in recent years started to train novel machine learning algorithms on available datasets with promising results. A shortcoming of this recent research effort, though, is that the vast majority of the proposed algorithms are trained on a single datasets. In doing so, researchers failed to test if their algorithm would be as effective if tested on a different dataset and, by extension, on a different population with respect to the one on which they were trained. In this article we test a transformer designed for lane change intention prediction on two datasets collected by LevelX in Germany and Hong Kong. We found that the transformer's accuracy plummeted when tested on a population different to the one it was trained on with accuracy values as low as 39.43%, but that when trained on both populations simultaneously it could achieve an accuracy as high as 86.71%. - This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model</title>
<link>https://arxiv.org/abs/2509.06539</link>
<guid>https://arxiv.org/abs/2509.06539</guid>
<content:encoded><![CDATA[
arXiv:2509.06539v1 Announce Type: new 
Abstract: CAGE-2 is an accepted benchmark for learning and evaluating defender strategies against cyberattacks. It reflects a scenario where a defender agent protects an IT infrastructure against various attacks. Many defender methods for CAGE-2 have been proposed in the literature. In this paper, we construct a formal model for CAGE-2 using the framework of Partially Observable Markov Decision Process (POMDP). Based on this model, we define an optimal defender strategy for CAGE-2 and introduce a method to efficiently learn this strategy. Our method, called BF-PPO, is based on PPO, and it uses particle filter to mitigate the computational complexity due to the large state space of the CAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and compare its performance with that of CARDIFF, the highest ranked method on the CAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the learned defender strategy and the required training time.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder</title>
<link>https://arxiv.org/abs/2509.06540</link>
<guid>https://arxiv.org/abs/2509.06540</guid>
<content:encoded><![CDATA[
arXiv:2509.06540v1 Announce Type: new 
Abstract: Objective: To develop and interpret a supervised variational autoencoder (VAE) model for classifying cardiotocography (CTG) signals based on pregnancy outcomes, addressing interpretability limits of current deep learning approaches. Methods: The OxMat CTG dataset was used to train a VAE on five-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes. The model was optimised for signal reconstruction and outcome prediction, incorporating Kullback-Leibler divergence and total correlation (TC) constraints to structure the latent space. Performance was evaluated using area under the receiver operating characteristic curve (AUROC) and mean squared error (MSE). Interpretability was assessed using coefficient of determination, latent traversals and unsupervised component analyses. Results: The model achieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level, where predicted scores were aggregated. Relaxing TC constraints improved both reconstruction and classification. Latent analysis showed that baseline-related features (e.g., FHR baseline, baseline shift) were well represented and aligned with model scores, while metrics like short- and long-term variability were less strongly encoded. Traversals revealed clear signal changes for baseline features, while other properties were entangled or subtle. Unsupervised decompositions corroborated these patterns. Findings: This work demonstrates that supervised VAEs can achieve competitive fetal outcome prediction while partially encoding clinically meaningful CTG features. The irregular, multi-timescale nature of FHR signals poses challenges for disentangling physiological components, distinguishing CTG from more periodic signals such as ECG. Although full interpretability was not achieved, the model supports clinically useful outcome prediction and provides a basis for future interpretable, generative models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs</title>
<link>https://arxiv.org/abs/2509.06550</link>
<guid>https://arxiv.org/abs/2509.06550</guid>
<content:encoded><![CDATA[
arXiv:2509.06550v1 Announce Type: new 
Abstract: Network intrusion detection remains a critical challenge in cybersecurity. While supervised machine learning models achieve state-of-the-art performance, their reliance on large labelled datasets makes them impractical for many real-world applications. Anomaly detection methods, which train exclusively on benign traffic to identify malicious activity, suffer from high false positive rates, limiting their usability. Recently, self-supervised learning techniques have demonstrated improved performance with lower false positive rates by learning discriminative latent representations of benign traffic. In particular, contrastive self-supervised models achieve this by minimizing the distance between similar (positive) views of benign traffic while maximizing it between dissimilar (negative) views. Existing approaches generate positive views through data augmentation and treat other samples as negative. In contrast, this work introduces Contrastive Learning using Augmented Negative pairs (CLAN), a novel paradigm for network intrusion detection where augmented samples are treated as negative views - representing potentially malicious distributions - while other benign samples serve as positive views. This approach enhances both classification accuracy and inference efficiency after pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset demonstrates that the proposed method surpasses existing self-supervised and anomaly detection techniques in a binary classification task. Furthermore, when fine-tuned on a limited labelled dataset, the proposed approach achieves superior multi-class classification performance compared to existing self-supervised models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing</title>
<link>https://arxiv.org/abs/2509.06552</link>
<guid>https://arxiv.org/abs/2509.06552</guid>
<content:encoded><![CDATA[
arXiv:2509.06552v1 Announce Type: new 
Abstract: The on-device real-time data distribution shift on devices challenges the generalization of lightweight on-device models. This critical issue is often overlooked in current research, which predominantly relies on data-intensive and computationally expensive fine-tuning approaches. To tackle this, we introduce Persona, a novel personalized method using a prototype-based, backpropagation-free parameter editing framework to enhance model generalization without post-deployment retraining. Persona employs a neural adapter in the cloud to generate a parameter editing matrix based on real-time device data. This matrix adeptly adapts on-device models to the prevailing data distributions, efficiently clustering them into prototype models. The prototypes are dynamically refined via the parameter editing matrix, facilitating efficient evolution. Furthermore, the integration of cross-layer knowledge transfer ensures consistent and context-aware multi-layer parameter changes and prototype assignment. Extensive experiments on vision task and recommendation task on multiple datasets confirm Persona's effectiveness and generality.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI for Scientific Discovery is a Social Problem</title>
<link>https://arxiv.org/abs/2509.06580</link>
<guid>https://arxiv.org/abs/2509.06580</guid>
<content:encoded><![CDATA[
arXiv:2509.06580v1 Announce Type: new 
Abstract: Artificial intelligence promises to accelerate scientific discovery, yet its benefits remain unevenly distributed. While technical obstacles such as scarce data, fragmented standards, and unequal access to computation are significant, we argue that the primary barriers are social and institutional. Narratives that defer progress to speculative "AI scientists," the undervaluing of data and infrastructure contributions, misaligned incentives, and gaps between domain experts and machine learning researchers all constrain impact. We highlight four interconnected challenges: community dysfunction, research priorities misaligned with upstream needs, data fragmentation, and infrastructure inequities. We argue that their roots lie in cultural and organizational practices. Addressing them requires not only technical innovation but also intentional community-building, cross-disciplinary education, shared benchmarks, and accessible infrastructure. We call for reframing AI for science as a collective social project, where sustainable collaboration and equitable participation are treated as prerequisites for technical progress.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems</title>
<link>https://arxiv.org/abs/2509.06599</link>
<guid>https://arxiv.org/abs/2509.06599</guid>
<content:encoded><![CDATA[
arXiv:2509.06599v1 Announce Type: new 
Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and dynamic effects. Their intertwined nature poses major challenges for data-driven modeling. This paper presents a theoretical framework grounded in structured decomposition, variance analysis, and task-centric complexity bounds.
  The framework employs a directional lower bound on interactions between measurable system components, extending orthogonality in inner product spaces to structurally asymmetric settings. This bound supports variance inequalities for decomposed systems. Key behavioral indicators are introduced along with a memory finiteness index. A rigorous power-based condition establishes a measurable link between finite memory in realizable systems and the First Law of Thermodynamics. This offers a more foundational perspective than classical bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty Principle,' demonstrating that static and dynamic distortions cannot be minimized simultaneously. We identify that real-world systems seem to resist complete deterministic decomposition due to entangled static and dynamic effects. We also present two general-purpose theorems linking function variance to mean-squared Lipschitz continuity and learning complexity. This yields a model-agnostic, task-aware complexity metric, showing that lower-variance components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual learning, including improved generalization, reduced parameter count, and lower training cost, as previously observed in power amplifier linearization experiments. The framework is broadly applicable and offers a scalable, theoretically grounded approach to modeling complex dynamic nonlinear systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification</title>
<link>https://arxiv.org/abs/2509.06600</link>
<guid>https://arxiv.org/abs/2509.06600</guid>
<content:encoded><![CDATA[
arXiv:2509.06600v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) have achieved remarkable success in processing graph-structured data across various applications. A critical aspect of real-world graphs is their dynamic nature, where new nodes are continually added and existing connections may change over time. Previous theoretical studies, largely based on the transductive learning framework, fail to adequately model such temporal evolution and structural dynamics. In this paper, we presents a PAC-Bayesian theoretical analysis of graph convolutional networks (GCNs) for inductive node classification, treating nodes as dependent and non-identically distributed data points. We derive novel generalization bounds for one-layer GCNs that explicitly incorporate the effects of data dependency and non-stationarity, and establish sufficient conditions under which the generalization gap converges to zero as the number of nodes increases. Furthermore, we extend our analysis to two-layer GCNs, and reveal that it requires stronger assumptions on graph topology to guarantee convergence. This work establishes a theoretical foundation for understanding and improving GNN generalization in dynamic graph environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards</title>
<link>https://arxiv.org/abs/2509.06602</link>
<guid>https://arxiv.org/abs/2509.06602</guid>
<content:encoded><![CDATA[
arXiv:2509.06602v1 Announce Type: new 
Abstract: Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology specialists collaboratively assess complex patient cases to determine optimal treatment strategies. A central element of this process is the patient summary, typically compiled by a medical oncologist, radiation oncologist, or surgeon, or their trained medical assistant, who distills heterogeneous medical records into a concise narrative to facilitate discussion. This manual approach is often labor-intensive, subjective, and prone to omissions of critical information. To address these limitations, we introduce the Healthcare Agent Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that coordinates a multi-agent clinical workflow to generate accurate and comprehensive patient summaries for MTBs. Evaluating predicted patient summaries against ground truth presents additional challenges due to stylistic variation, ordering, synonym usage, and phrasing differences, which complicate the measurement of both succinctness and completeness. To overcome these evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework designed to assess the comprehensiveness and succinctness of generated summaries. Using a benchmark dataset derived from de-identified tumor board discussions, we applied TBFact to evaluate our Patient History agent. Results show that the agent captured 94% of high-importance information (including partial entailments) and achieved a TBFact recall of 0.84 under strict entailment criteria. We further demonstrate that TBFact enables a data-free evaluation framework that institutions can deploy locally without sharing sensitive clinical data. Together, HAO and TBFact establish a robust foundation for delivering reliable and scalable support to MTBs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors</title>
<link>https://arxiv.org/abs/2509.06608</link>
<guid>https://arxiv.org/abs/2509.06608</guid>
<content:encoded><![CDATA[
arXiv:2509.06608v1 Announce Type: new 
Abstract: The mechanisms by which reasoning training reshapes language-model computations remain poorly understood. We study lightweight steering vectors inserted into the base model's residual stream and trained with a reinforcement-learning objective, which can match full fine-tuning performance while retaining the interpretability of small, additive interventions. Using logit-lens readouts, path patching, and circuit analyses, we analyze two models and find: (i) the last-layer steering vector behaves like a token-substitution bias concentrated on the first generated token, consistently boosting tokens such as "To" and "Step"; and (ii) the penultimate-layer steering vector leaves attention patterns largely unchanged and instead acts through the MLP and unembedding, preferentially up-weighting process words and structure symbols. These results establish a principled framework for interpreting the behavioral changes induced by reasoning training.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models</title>
<link>https://arxiv.org/abs/2509.06609</link>
<guid>https://arxiv.org/abs/2509.06609</guid>
<content:encoded><![CDATA[
arXiv:2509.06609v1 Announce Type: new 
Abstract: Graph anomaly detection (GAD) has attracted increasing attention in recent years for identifying malicious samples in a wide range of graph-based applications, such as social media and e-commerce. However, most GAD methods assume identical training and testing distributions and are tailored to specific tasks, resulting in limited adaptability to real-world scenarios such as shifting data distributions and scarce training samples in new applications. To address the limitations, recent work has focused on improving the generalization capability of GAD models through transfer learning that leverages knowledge from related domains to enhance detection performance, or developing "one-for-all" GAD foundation models that generalize across multiple applications. Since a systematic understanding of generalization in GAD is still lacking, in this paper, we provide a comprehensive review of generalization in GAD. We first trace the evolution of generalization in GAD and formalize the problem settings, which further leads to our systematic taxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive review is conducted for the existing generalized GAD methods. Finally, we identify current open challenges and suggest future directions to inspire future research in this emerging field.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEAM: Brainwave Empathy Assessment Model for Early Childhood</title>
<link>https://arxiv.org/abs/2509.06620</link>
<guid>https://arxiv.org/abs/2509.06620</guid>
<content:encoded><![CDATA[
arXiv:2509.06620v1 Announce Type: new 
Abstract: Empathy in young children is crucial for their social and emotional development, yet predicting it remains challenging. Traditional methods often only rely on self-reports or observer-based labeling, which are susceptible to bias and fail to objectively capture the process of empathy formation. EEG offers an objective alternative; however, current approaches primarily extract static patterns, neglecting temporal dynamics. To overcome these limitations, we propose a novel deep learning framework, the Brainwave Empathy Assessment Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM leverages multi-view EEG signals to capture both cognitive and emotional dimensions of empathy. The framework comprises three key components: 1) a LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a feature fusion module to integrate complementary information from multi-view signals, and 3) a contrastive learning module to enhance class separation. Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across multiple metrics, demonstrating its potential for objective empathy assessment and providing a preliminary insight into early interventions in children's prosocial development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing</title>
<link>https://arxiv.org/abs/2509.06640</link>
<guid>https://arxiv.org/abs/2509.06640</guid>
<content:encoded><![CDATA[
arXiv:2509.06640v1 Announce Type: new 
Abstract: We propose a simple algorithm that needs only a few data samples from a single graph for learning local routing policies that generalize across a rich class of geometric random graphs in Euclidean metric spaces. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that let each graph node efficiently and scalably route (i.e., forward) packets by considering only the node's state and the state of the neighboring nodes. Our algorithm design exploits network domain knowledge in the selection of input features and design of the policy function for learning an approximately optimal policy. Domain knowledge also provides theoretical assurance that the choice of a ``seed graph'' and its node data sampling suffices for generalizable learning. Remarkably, one of these DNNs we train -- using distance-to-destination as the only input feature -- learns a policy that exactly matches the well-known Greedy Forwarding policy, which forwards packets to the neighbor with the shortest distance to the destination. We also learn a new policy, which we call GreedyTensile routing -- using both distance-to-destination and node stretch as the input features -- that almost always outperforms greedy forwarding. We demonstrate the explainability and ultra-low latency run-time operation of Greedy Tensile routing by symbolically interpreting its DNN in low-complexity terms of two linear actions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives</title>
<link>https://arxiv.org/abs/2509.06656</link>
<guid>https://arxiv.org/abs/2509.06656</guid>
<content:encoded><![CDATA[
arXiv:2509.06656v1 Announce Type: new 
Abstract: Understanding and modeling individual travel behavior responses is crucial for urban mobility regulation and policy evaluation. The Markov decision process (MDP) provides a structured framework for dynamic travel behavior modeling at the individual level. However, solving an MDP in this context is highly data-intensive and faces challenges of data quantity, spatial-temporal coverage, and situational diversity. To address these, we propose a group-effect-enhanced generative adversarial imitation learning (gcGAIL) model that improves the individual behavior modeling efficiency by leveraging shared behavioral patterns among passenger groups. We validate the gcGAIL model using a public transport fare-discount case study and compare against state-of-the-art benchmarks, including adversarial inverse reinforcement learning (AIRL), baseline GAIL, and conditional GAIL. Experimental results demonstrate that gcGAIL outperforms these methods in learning individual travel behavior responses to incentives over time in terms of accuracy, generalization, and pattern demonstration efficiency. Notably, gcGAIL is robust to spatial variation, data sparsity, and behavioral diversity, maintaining strong performance even with partial expert demonstrations and underrepresented passenger groups. The gcGAIL model predicts the individual behavior response at any time, providing the basis for personalized incentives to induce sustainable behavior changes (better timing of incentive injections).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations</title>
<link>https://arxiv.org/abs/2509.06665</link>
<guid>https://arxiv.org/abs/2509.06665</guid>
<content:encoded><![CDATA[
arXiv:2509.06665v1 Announce Type: new 
Abstract: Vehicular ad hoc networks (VANETs) are a crucial component of intelligent transportation systems; however, routing remains challenging due to dynamic topologies, incomplete observations, and the limited resources of edge devices. Existing reinforcement learning (RL) approaches often assume fixed graph structures and require retraining when network conditions change, making them unsuitable for deployment on constrained hardware. We present TrajAware, an RL-based framework designed for edge AI deployment in VANETs. TrajAware integrates three components: (i) action space pruning, which reduces redundant neighbour options while preserving two-hop reachability, alleviating the curse of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to the global graph context, producing features that generalise across diverse network sizes; and (iii) trajectory-aware prediction, which uses historical routes and junction information to estimate real-time positions under partial observations. We evaluate TrajAware in the open-source SUMO simulator using real-world city maps with a leave-one-city-out setup. Results show that TrajAware achieves near-shortest paths and high delivery ratios while maintaining efficiency suitable for constrained edge devices, outperforming state-of-the-art baselines in both full and partial observation scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation</title>
<link>https://arxiv.org/abs/2509.06694</link>
<guid>https://arxiv.org/abs/2509.06694</guid>
<content:encoded><![CDATA[
arXiv:2509.06694v1 Announce Type: new 
Abstract: While it is well-established that artificial neural networks are \emph{universal approximators} for continuous functions on compact domains, many modern approaches rely on deep or overparameterized architectures that incur high computational costs. In this paper, a new type of \emph{small shallow} neural network, called the \emph{Barycentric Neural Network} ($\BNN$), is proposed, which leverages a fixed set of \emph{base points} and their \emph{barycentric coordinates} to define both its structure and its parameters. We demonstrate that our $\BNN$ enables the exact representation of \emph{continuous piecewise linear functions} ($\CPLF$s), ensuring strict continuity across segments. Since any continuous function over a compact domain can be approximated arbitrarily well by $\CPLF$s, the $\BNN$ naturally emerges as a flexible and interpretable tool for \emph{function approximation}. Beyond the use of this representation, the main contribution of the paper is the introduction of a new variant of \emph{persistent entropy}, a topological feature that is stable and scale invariant, called the \emph{length-weighted persistent entropy} ($\LWPE$), which is weighted by the lifetime of topological features. Our framework, which combines the $\BNN$ with a loss function based on our $\LWPE$, aims to provide flexible and geometrically interpretable approximations of nonlinear continuous functions in resource-constrained settings, such as those with limited base points for $\BNN$ design and few training epochs. Instead of optimizing internal weights, our approach directly \emph{optimizes the base points that define the $\BNN$}. Experimental results show that our approach achieves \emph{superior and faster approximation performance} compared to classical loss functions such as MSE, RMSE, MAE, and log-cosh.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2509.06701</link>
<guid>https://arxiv.org/abs/2509.06701</guid>
<content:encoded><![CDATA[
arXiv:2509.06701v1 Announce Type: new 
Abstract: We develop a theory of intelligent agency grounded in probabilistic modeling for neural models. Agents are represented as outcome distributions with epistemic utility given by log score, and compositions are defined through weighted logarithmic pooling that strictly improves every member's welfare. We prove that strict unanimity is impossible under linear pooling or in binary outcome spaces, but possible with three or more outcomes. Our framework admits recursive structure via cloning invariance, continuity, and openness, while tilt-based analysis rules out trivial duplication. Finally, we formalize an agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a manifest-then-suppress Waluigi strategy yields strictly larger first-order misalignment reduction than pure Luigi reinforcement alone. These results clarify how developing a principled mathematical framework for how subagents can coalesce into coherent higher-level entities provides novel implications for alignment in agentic AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Optimal Transport Distances</title>
<link>https://arxiv.org/abs/2509.06702</link>
<guid>https://arxiv.org/abs/2509.06702</guid>
<content:encoded><![CDATA[
arXiv:2509.06702v1 Announce Type: new 
Abstract: Simulating realistic financial time series is essential for stress testing, scenario generation, and decision-making under uncertainty. Despite advances in deep generative models, there is no consensus metric for their evaluation. We focus on generative AI for financial time series in decision-making applications and employ the nested optimal transport distance, a time-causal variant of optimal transport distance, which is robust to tasks such as hedging, optimal stopping, and reinforcement learning. Moreover, we propose a statistically consistent, naturally parallelizable algorithm for its computation, achieving substantial speedups over existing approaches.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms</title>
<link>https://arxiv.org/abs/2509.06714</link>
<guid>https://arxiv.org/abs/2509.06714</guid>
<content:encoded><![CDATA[
arXiv:2509.06714v1 Announce Type: new 
Abstract: Learning a controller directly on the robot requires extreme sample efficiency. Model-based reinforcement learning (RL) methods are the most sample efficient, but they often suffer from a too long inference time to meet the robot control frequency requirements. In this paper, we address the sample efficiency and inference time challenges with two contributions. First, we define a general framework to deal with inference delays where the slow inference robot controller provides a sequence of actions to feed the control-hungry robotic platform without execution gaps. Then, we compare several RL algorithms in the light of this framework and propose RT-HCP, an algorithm that offers an excellent trade-off between performance, sample efficiency and inference time. We validate the superiority of RT-HCP with experiments where we learn a controller directly on a simple but high frequency FURUTA pendulum platform. Code: github.com/elasriz/RTHCP
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Range Graph Wavelet Networks</title>
<link>https://arxiv.org/abs/2509.06743</link>
<guid>https://arxiv.org/abs/2509.06743</guid>
<content:encoded><![CDATA[
arXiv:2509.06743v1 Announce Type: new 
Abstract: Modeling long-range interactions, the propagation of information across distant parts of a graph, is a central challenge in graph machine learning. Graph wavelets, inspired by multi-resolution signal processing, provide a principled way to capture both local and global structures. However, existing wavelet-based graph neural networks rely on finite-order polynomial approximations, which limit their receptive fields and hinder long-range propagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which decompose wavelet filters into complementary local and global components. Local aggregation is handled with efficient low-order polynomials, while long-range interactions are captured through a flexible spectral domain parameterization. This hybrid design unifies short- and long-distance information flow within a principled wavelet framework. Experiments show that LR-GWN achieves state-of-the-art performance among wavelet-based methods on long-range benchmarks, while remaining competitive on short-range datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2509.06759</link>
<guid>https://arxiv.org/abs/2509.06759</guid>
<content:encoded><![CDATA[
arXiv:2509.06759v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remains a critical challenge. Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO) offer promising frameworks for this aligning process. While DRL enables models to optimize actions using reward signals instead of relying solely on supervised preference data, DPO directly aligns the policy with preferences, eliminating the need for an explicit reward model. This overview explores paradigms for fine-tuning LVLMs, highlighting how DRL and DPO techniques can be used to align models with human preferences and values, improve task performance, and enable adaptive multimodal interaction. We categorize key approaches, examine sources of preference data, reward signals, and discuss open challenges such as scalability, sample efficiency, continual learning, generalization, and safety. The goal is to provide a clear understanding of how DRL and DPO contribute to the evolution of robust and human-aligned LVLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.06777</link>
<guid>https://arxiv.org/abs/2509.06777</guid>
<content:encoded><![CDATA[
arXiv:2509.06777v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs when tasks require long-range interactions. The problem arises from the presence of bottlenecks that limit the propagation of messages among distant nodes. Recently, graph rewiring methods modify edge connectivity and are expected to perform well on long-range tasks. Yet, graph rewiring compromises the inductive bias, incurring significant information loss in solving the downstream task. Furthermore, increasing channel capacity may overcome information bottlenecks but enhance the parameter complexity of the model. To alleviate these shortcomings, we propose an efficient model-agnostic framework that asynchronously updates node features, unlike traditional synchronous message passing GNNs. Our framework creates node batches in every layer based on the node centrality values. The features of the nodes belonging to these batches will only get updated. Asynchronous message updates process information sequentially across layers, avoiding simultaneous compression into fixed-capacity channels. We also theoretically establish that our proposed framework maintains higher feature sensitivity bounds compared to standard synchronous approaches. Our framework is applied to six standard graph datasets and two long-range datasets to perform graph classification and achieves impressive performances with a $5\%$ and $4\%$ improvements on REDDIT-BINARY and Peptides-struct, respectively.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.06782</link>
<guid>https://arxiv.org/abs/2509.06782</guid>
<content:encoded><![CDATA[
arXiv:2509.06782v1 Announce Type: new 
Abstract: Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise for domains such as autonomous navigation and locomotion, where collecting interactive data is costly and unsafe. However, it remains challenging in practice due to the need to learn from datasets with limited coverage of the state-action space and to generalize across long-horizon tasks. To improve on these challenges, we propose a Physics-informed (Pi) regularized loss for value learning, derived from the Eikonal Partial Differential Equation (PDE) and which induces a geometric inductive bias in the learned value function. Unlike generic gradient penalties that are primarily used to stabilize training, our formulation is grounded in continuous-time optimal control and encourages value functions to align with cost-to-go structures. The proposed regularizer is broadly compatible with temporal-difference-based value learning and can be integrated into existing Offline GCRL algorithms. When combined with Hierarchical Implicit Q-Learning (HIQL), the resulting method, Physics-informed HIQL (Pi-HIQL), yields significant improvements in both performance and generalization, with pronounced gains in stitching regimes and large-scale navigation tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\texttt{R$^\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World</title>
<link>https://arxiv.org/abs/2509.06786</link>
<guid>https://arxiv.org/abs/2509.06786</guid>
<content:encoded><![CDATA[
arXiv:2509.06786v1 Announce Type: new 
Abstract: In this position paper, we address the persistent gap between rapidly growing AI capabilities and lagging safety progress. Existing paradigms divide into ``Make AI Safe'', which applies post-hoc alignment and guardrails but remains brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety but struggles to address unforeseen risks in open-ended environments. We therefore propose \textit{safe-by-coevolution} as a new formulation of the ``Make Safe AI'' paradigm, inspired by biological immunity, in which safety becomes a dynamic, adversarial, and ongoing learning process. To operationalize this vision, we introduce \texttt{R$^2$AI} -- \textit{Resistant and Resilient AI} -- as a practical framework that unites resistance against known threats with resilience to unforeseen risks. \texttt{R$^2$AI} integrates \textit{fast and slow safe models}, adversarial simulation and verification through a \textit{safety wind tunnel}, and continual feedback loops that guide safety and capability to coevolve. We argue that this framework offers a scalable and proactive path to maintain continual safety in dynamic environments, addressing both near-term vulnerabilities and long-term existential risks as AI advances toward AGI and ASI.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL</title>
<link>https://arxiv.org/abs/2509.06863</link>
<guid>https://arxiv.org/abs/2509.06863</guid>
<content:encoded><![CDATA[
arXiv:2509.06863v1 Announce Type: new 
Abstract: A hallmark of modern large-scale machine learning techniques is the use of training objectives that provide dense supervision to intermediate computations, such as teacher forcing the next token in language models or denoising step-by-step in diffusion models. This enables models to learn complex functions in a generalizable manner. Motivated by this observation, we investigate the benefits of iterative computation for temporal difference (TD) methods in reinforcement learning (RL). Typically they represent value functions in a monolithic fashion, without iterative compute. We introduce floq (flow-matching Q-functions), an approach that parameterizes the Q-function using a velocity field and trains it using techniques from flow-matching, typically used in generative modeling. This velocity field underneath the flow is trained using a TD-learning objective, which bootstraps from values produced by a target velocity field, computed by running multiple steps of numerical integration. Crucially, floq allows for more fine-grained control and scaling of the Q-function capacity than monolithic architectures, by appropriately setting the number of integration steps. Across a suite of challenging offline RL benchmarks and online fine-tuning tasks, floq improves performance by nearly 1.8x. floq scales capacity far better than standard TD-learning architectures, highlighting the potential of iterative computation for value learning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concolic Testing on Individual Fairness of Neural Network Models</title>
<link>https://arxiv.org/abs/2509.06864</link>
<guid>https://arxiv.org/abs/2509.06864</guid>
<content:encoded><![CDATA[
arXiv:2509.06864v1 Announce Type: new 
Abstract: This paper introduces PyFair, a formal framework for evaluating and verifying individual fairness of Deep Neural Networks (DNNs). By adapting the concolic testing tool PyCT, we generate fairness-specific path constraints to systematically explore DNN behaviors. Our key innovation is a dual network architecture that enables comprehensive fairness assessments and provides completeness guarantees for certain network types. We evaluate PyFair on 25 benchmark models, including those enhanced by existing bias mitigation techniques. Results demonstrate PyFair's efficacy in detecting discriminatory instances and verifying fairness, while also revealing scalability challenges for complex models. This work advances algorithmic fairness in critical domains by offering a rigorous, systematic method for fairness testing and verification of pre-trained DNNs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification</title>
<link>https://arxiv.org/abs/2509.06875</link>
<guid>https://arxiv.org/abs/2509.06875</guid>
<content:encoded><![CDATA[
arXiv:2509.06875v1 Announce Type: new 
Abstract: Class imbalance in machine learning poses a significant challenge, as skewed datasets often hinder performance on minority classes. Traditional oversampling techniques, which are commonly used to alleviate class imbalance, have several drawbacks: they treat features independently, lack similarity-based controls, limit sample diversity, and fail to manage synthetic variety effectively. To overcome these issues, we introduce AxelSMOTE, an innovative agent-based approach that views data instances as autonomous agents engaging in complex interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE implements four key innovations: (1) trait-based feature grouping to preserve correlations; (2) a similarity-based probabilistic exchange mechanism for meaningful interactions; (3) Beta distribution blending for realistic interpolation; and (4) controlled diversity injection to avoid overfitting. Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms state-of-the-art sampling methods while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning</title>
<link>https://arxiv.org/abs/2509.06896</link>
<guid>https://arxiv.org/abs/2509.06896</guid>
<content:encoded><![CDATA[
arXiv:2509.06896v1 Announce Type: new 
Abstract: Targeted data poisoning attacks pose an increasingly serious threat due to their ease of deployment and high success rates. These attacks aim to manipulate the prediction for a single test sample in classification models. Unlike indiscriminate attacks that aim to decrease overall test performance, targeted attacks present a unique threat to individual test instances. This threat model raises a fundamental question: what factors make certain test samples more susceptible to successful poisoning than others? We investigate how attack difficulty varies across different test instances and identify key characteristics that influence vulnerability. This paper introduces three predictive criteria for targeted data poisoning difficulty: ergodic prediction accuracy (analyzed through clean training dynamics), poison distance, and poison budget. Our experimental results demonstrate that these metrics effectively predict the varying difficulty of real-world targeted poisoning attacks across diverse scenarios, offering practitioners valuable insights for vulnerability assessment and understanding data poisoning attacks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition</title>
<link>https://arxiv.org/abs/2509.06918</link>
<guid>https://arxiv.org/abs/2509.06918</guid>
<content:encoded><![CDATA[
arXiv:2509.06918v1 Announce Type: new 
Abstract: Robust out-of-distribution (OOD) detection is an indispensable component of modern artificial intelligence (AI) systems, especially in safety-critical applications where models must identify inputs from unfamiliar classes not seen during training. While OOD detection has been extensively studied in the machine learning literature--with both post hoc and training-based approaches--its effectiveness under noisy training labels remains underexplored. Recent studies suggest that label noise can significantly degrade OOD performance, yet principled solutions to this issue are lacking. In this work, we demonstrate that directly combining existing label noise-robust methods with OOD detection strategies is insufficient to address this critical challenge. To overcome this, we propose a robust OOD detection framework that integrates loss correction techniques from the noisy label learning literature with low-rank and sparse decomposition methods from signal processing. Extensive experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms the state-of-the-art OOD detection techniques, particularly under severe noisy label settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding</title>
<link>https://arxiv.org/abs/2509.06923</link>
<guid>https://arxiv.org/abs/2509.06923</guid>
<content:encoded><![CDATA[
arXiv:2509.06923v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neutron Reflectometry by Gradient Descent</title>
<link>https://arxiv.org/abs/2509.06924</link>
<guid>https://arxiv.org/abs/2509.06924</guid>
<content:encoded><![CDATA[
arXiv:2509.06924v1 Announce Type: new 
Abstract: Neutron reflectometry (NR) is a powerful technique to probe surfaces and interfaces. NR is inherently an indirect measurement technique, access to the physical quantities of interest (layer thickness, scattering length density, roughness), necessitate the solution of an inverse modelling problem, that is inefficient for large amounts of data or complex multiplayer structures (e.g. lithium batteries / electrodes). Recently, surrogate machine learning models have been proposed as an alternative to existing optimisation routines. Although such approaches have been successful, physical intuition is lost when replacing governing equations with fast neural networks. Instead, we propose a novel and efficient approach; to optimise reflectivity data analysis by performing gradient descent on the forward reflection model itself. Herein, automatic differentiation techniques are used to evaluate exact gradients of the error function with respect to the parameters of interest. Access to these quantities enables users of neutron reflectometry to harness a host of powerful modern optimisation and inference techniques that remain thus far unexploited in the context of neutron reflectometry. This paper presents two benchmark case studies; demonstrating state-of-the-art performance on a thick oxide quartz film, and robust co-fitting performance in the high complexity regime of organic LED multilayer devices. Additionally, we provide an open-source library of differentiable reflectometry kernels in the python programming language so that gradient based approaches can readily be applied to other NR datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning words in groups: fusion algebras, tensor ranks and grokking</title>
<link>https://arxiv.org/abs/2509.06931</link>
<guid>https://arxiv.org/abs/2509.06931</guid>
<content:encoded><![CDATA[
arXiv:2509.06931v1 Announce Type: new 
Abstract: In this work, we demonstrate that a simple two-layer neural network with standard activation functions can learn an arbitrary word operation in any finite group, provided sufficient width is available and exhibits grokking while doing so. To explain the mechanism by which this is achieved, we reframe the problem as that of learning a particular $3$-tensor, which we show is typically of low rank. A key insight is that low-rank implementations of this tensor can be obtained by decomposing it along triplets of basic self-conjugate representations of the group and leveraging the fusion structure to rule out many components. Focusing on a phenomenologically similar but more tractable surrogate model, we show that the network is able to find such low-rank implementations (or approximations thereof), thereby using limited width to approximate the word-tensor in a generalizable way. In the case of the simple multiplication word, we further elucidate the form of these low-rank implementations, showing that the network effectively implements efficient matrix multiplication in the sense of Strassen. Our work also sheds light on the mechanism by which a network reaches such a solution under gradient descent.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers</title>
<link>https://arxiv.org/abs/2509.06938</link>
<guid>https://arxiv.org/abs/2509.06938</guid>
<content:encoded><![CDATA[
arXiv:2509.06938v1 Announce Type: new 
Abstract: As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outcome-based Exploration for LLM Reasoning</title>
<link>https://arxiv.org/abs/2509.06941</link>
<guid>https://arxiv.org/abs/2509.06941</guid>
<content:encoded><![CDATA[
arXiv:2509.06941v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has emerged as a powerful method for improving the reasoning abilities of large language models (LLMs). Outcome-based RL, which rewards policies solely for the correctness of the final answer, yields substantial accuracy gains but also induces a systematic loss in generation diversity. This collapse undermines real-world performance, where diversity is critical for test-time scaling. We analyze this phenomenon by viewing RL post-training as a sampling process and show that, strikingly, RL can reduce effective diversity even on the training set relative to the base model. Our study highlights two central findings: (i) a transfer of diversity degradation, where reduced diversity on solved problems propagates to unsolved ones, and (ii) the tractability of the outcome space, since reasoning tasks admit only a limited set of distinct answers. Motivated by these insights, we propose outcome-based exploration, which assigns exploration bonuses according to final outcomes. We introduce two complementary algorithms: historical exploration, which encourages rarely observed answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition to promote test-time diversity. Experiments on standard competition math with Llama and Qwen models demonstrate that both methods improve accuracy while mitigating diversity collapse. On the theoretical side, we formalize the benefit of outcome-based exploration through a new model of outcome-based bandits. Together, these contributions chart a practical path toward RL methods that enhance reasoning without sacrificing the diversity essential for scalable deployment.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval</title>
<link>https://arxiv.org/abs/2311.01870</link>
<guid>https://arxiv.org/abs/2311.01870</guid>
<content:encoded><![CDATA[
arXiv:2311.01870v1 Announce Type: cross 
Abstract: We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K multi-lingual documents collected from the European Parliament, spanning 24 languages. This dataset is designed to investigate fairness in a multilingual information retrieval (IR) context to analyze both language and demographic bias in a ranking context. It boasts an authentic multilingual corpus, featuring topics translated into all 24 languages, as well as cross-lingual relevance judgments. Furthermore, it offers rich demographic information associated with its documents, facilitating the study of demographic bias. We report the effectiveness of Multi-EuP for benchmarking both monolingual and multilingual IR. We also conduct a preliminary experiment on language bias caused by the choice of tokenization strategy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation</title>
<link>https://arxiv.org/abs/2509.03736</link>
<guid>https://arxiv.org/abs/2509.03736</guid>
<content:encoded><![CDATA[
arXiv:2509.03736v1 Announce Type: cross 
Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Brain Morphogenesis via Physics-Transfer Learning</title>
<link>https://arxiv.org/abs/2509.05305</link>
<guid>https://arxiv.org/abs/2509.05305</guid>
<content:encoded><![CDATA[
arXiv:2509.05305v1 Announce Type: cross 
Abstract: Brain morphology is shaped by genetic and mechanical factors and is linked to biological development and diseases. Its fractal-like features, regional anisotropy, and complex curvature distributions hinder quantitative insights in medical inspections. Recognizing that the underlying elastic instability and bifurcation share the same physics as simple geometries such as spheres and ellipses, we developed a physics-transfer learning framework to address the geometrical complexity. To overcome the challenge of data scarcity, we constructed a digital library of high-fidelity continuum mechanics modeling that both describes and predicts the developmental processes of brain growth and disease. The physics of nonlinear elasticity from simple geometries is embedded into a neural network and applied to brain models. This physics-transfer approach demonstrates remarkable performance in feature characterization and morphogenesis prediction, highlighting the pivotal role of localized deformation in dominating over the background geometry. The data-driven framework also provides a library of reduced-dimensional evolutionary representations that capture the essential physics of the highly folded cerebral cortex. Validation through medical images and domain expertise underscores the deployment of digital-twin technology in comprehending the morphological complexity of the brain.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations</title>
<link>https://arxiv.org/abs/2509.05311</link>
<guid>https://arxiv.org/abs/2509.05311</guid>
<content:encoded><![CDATA[
arXiv:2509.05311v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has shown great potential for autonomous decision-making in the cybersecurity domain, enabling agents to learn through direct environment interaction. However, RL agents in Autonomous Cyber Operations (ACO) typically learn from scratch, requiring them to execute undesirable actions to learn their consequences. In this study, we integrate external knowledge in the form of a Large Language Model (LLM) pretrained on cybersecurity data that our RL agent can directly leverage to make informed decisions. By guiding initial training with an LLM, we improve baseline performance and reduce the need for exploratory actions with obviously negative outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity environment, and demonstrate that our guided agent achieves over 2x higher rewards during early training and converges to a favorable policy approximately 4,500 episodes faster than the baseline.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VILOD: A Visual Interactive Labeling Tool for Object Detection</title>
<link>https://arxiv.org/abs/2509.05317</link>
<guid>https://arxiv.org/abs/2509.05317</guid>
<content:encoded><![CDATA[
arXiv:2509.05317v1 Announce Type: cross 
Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often hindered by the significant challenge of acquiring large, accurately labeled datasets, a process that is time-consuming and expensive. While techniques like Active Learning (AL) can reduce annotation effort by intelligently querying informative samples, they often lack transparency, limit the strategic insight of human experts, and may overlook informative samples not aligned with an employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL) approaches integrating human intelligence and intuition throughout the machine learning life-cycle have gained traction. Leveraging Visual Analytics (VA), effective interfaces can be created to facilitate this human-AI collaboration. This thesis explores the intersection of these fields by developing and investigating "VILOD: A Visual Interactive Labeling tool for Object Detection". VILOD utilizes components such as a t-SNE projection of image features, together with uncertainty heatmaps and model state views. Enabling users to explore data, interpret model states, AL suggestions, and implement diverse sample selection strategies within an iterative HITL workflow for OD. An empirical investigation using comparative use cases demonstrated how VILOD, through its interactive visualizations, facilitates the implementation of distinct labeling strategies by making the model's state and dataset characteristics more interpretable (RQ1). The study showed that different visually-guided labeling strategies employed within VILOD result in competitive OD performance trajectories compared to an automated uncertainty sampling AL baseline (RQ2). This work contributes a novel tool and empirical insight into making the HITL-AL workflow for OD annotation more transparent, manageable, and potentially more effective.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks</title>
<link>https://arxiv.org/abs/2509.05320</link>
<guid>https://arxiv.org/abs/2509.05320</guid>
<content:encoded><![CDATA[
arXiv:2509.05320v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) in 6G vehicular networks promises unprecedented advancements in intelligent transportation systems. However, offloading LLM computations from vehicles to edge infrastructure poses significant privacy risks, potentially exposing sensitive user data. This paper presents a novel privacy-preserving offloading framework for LLM-integrated vehicular networks. We introduce a hybrid approach combining federated learning (FL) and differential privacy (DP) techniques to protect user data while maintaining LLM performance. Our framework includes a privacy-aware task partitioning algorithm that optimizes the trade-off between local and edge computation, considering both privacy constraints and system efficiency. We also propose a secure communication protocol for transmitting model updates and aggregating results across the network. Experimental results demonstrate that our approach achieves 75\% global accuracy with only a 2-3\% reduction compared to non-privacy-preserving methods, while maintaining DP guarantees with an optimal privacy budget of $\varepsilon = 0.8$. The framework shows stable communication overhead of approximately 2.1MB per round with computation comprising over 90\% of total processing time, validating its efficiency for resource-constrained vehicular environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19</title>
<link>https://arxiv.org/abs/2509.05322</link>
<guid>https://arxiv.org/abs/2509.05322</guid>
<content:encoded><![CDATA[
arXiv:2509.05322v1 Announce Type: cross 
Abstract: Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for investigating the impact of network topology in deep learning by capturing how different connectivity patterns impact both learning efficiency and model performance. At the same time, they provide a natural framework for exploring edge-centric network measures as tools for pruning and optimization. In this study, we investigate three edge-centric network measures: Forman-Ricci curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness centrality (EBC), to compress RWNNs by selectively retaining important synapses (or edges) while pruning the rest. As a baseline, RWNNs are trained for COVID-19 chest x-ray image classification, aiming to reduce network complexity while preserving performance in terms of accuracy, specificity, and sensitivity. We extend prior work on pruning RWNN using ORC by incorporating two additional edge-centric measures, FRC and EBC, across three network generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the pruning performance of the three measures in terms of compression ratio and theoretical speedup. A central focus of our study is to evaluate whether FRC, which is computationally more efficient than ORC, can achieve comparable pruning effectiveness. Along with performance evaluation, we further investigate the structural properties of the pruned networks through modularity and global efficiency, offering insights into the trade-off between modular segregation and network efficiency in compressed RWNNs. Our results provide initial evidence that FRC-based pruning can effectively simplify RWNNs, offering significant computational advantages while maintaining performance comparable to ORC.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling imbalance and few-sample size in ML based Onion disease classification</title>
<link>https://arxiv.org/abs/2509.05341</link>
<guid>https://arxiv.org/abs/2509.05341</guid>
<content:encoded><![CDATA[
arXiv:2509.05341v1 Announce Type: cross 
Abstract: Accurate classification of pests and diseases plays a vital role in precision agriculture, enabling efficient identification, targeted interventions, and preventing their further spread. However, current methods primarily focus on binary classification, which limits their practical applications, especially in scenarios where accurately identifying the specific type of disease or pest is essential. We propose a robust deep learning based model for multi-class classification of onion crop diseases and pests. We enhance a pre-trained Convolutional Neural Network (CNN) model by integrating attention based modules and employing comprehensive data augmentation pipeline to mitigate class imbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1 score on real-world field image dataset. This model gives better results than other approaches using the same datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delta Velocity Rectified Flow for Text-to-Image Editing</title>
<link>https://arxiv.org/abs/2509.05342</link>
<guid>https://arxiv.org/abs/2509.05342</guid>
<content:encoded><![CDATA[
arXiv:2509.05342v1 Announce Type: cross 
Abstract: We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free, path-aware editing framework within rectified flow models for text-to-image editing. DVRF is a distillation-based method that explicitly models the discrepancy between the source and target velocity fields in order to mitigate over-smoothing artifacts rampant in prior distillation sampling approaches. We further introduce a time-dependent shift term to push noisy latents closer to the target trajectory, enhancing the alignment with the target distribution. We theoretically demonstrate that when this shift is disabled, DVRF reduces to Delta Denoising Score, thereby bridging score-based diffusion optimization and velocity-based rectified-flow optimization. Moreover, when the shift term follows a linear schedule under rectified-flow dynamics, DVRF generalizes the Inversion-free method FlowEdit and provides a principled theoretical interpretation for it. Experimental results indicate that DVRF achieves superior editing quality, fidelity, and controllability while requiring no architectural modifications, making it efficient and broadly applicable to text-to-image editing tasks. Code is available at https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensembling Membership Inference Attacks Against Tabular Generative Models</title>
<link>https://arxiv.org/abs/2509.05350</link>
<guid>https://arxiv.org/abs/2509.05350</guid>
<content:encoded><![CDATA[
arXiv:2509.05350v1 Announce Type: cross 
Abstract: Membership Inference Attacks (MIAs) have emerged as a principled framework for auditing the privacy of synthetic data generated by tabular generative models, where many diverse methods have been proposed that each exploit different privacy leakage signals. However, in realistic threat scenarios, an adversary must choose a single method without a priori guarantee that it will be the empirically highest performing option. We study this challenge as a decision theoretic problem under uncertainty and conduct the largest synthetic data privacy benchmark to date. Here, we find that no MIA constitutes a strictly dominant strategy across a wide variety of model architectures and dataset domains under our threat model. Motivated by these findings, we propose ensemble MIAs and show that unsupervised ensembles built on individual attacks offer empirically more robust, regret-minimizing strategies than individual attacks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Driving Laboratory Optimizes the Lower Critical Solution Temperature of Thermoresponsive Polymers</title>
<link>https://arxiv.org/abs/2509.05351</link>
<guid>https://arxiv.org/abs/2509.05351</guid>
<content:encoded><![CDATA[
arXiv:2509.05351v1 Announce Type: cross 
Abstract: To overcome the inherent inefficiencies of traditional trial-and-error materials discovery, the scientific community is increasingly developing autonomous laboratories that integrate data-driven decision-making into closed-loop experimental workflows. In this work, we realize this concept for thermoresponsive polymers by developing a low-cost, "frugal twin" platform for the optimization of the lower critical solution temperature (LCST) of poly(N-isopropylacrylamide) (PNIPAM). Our system integrates robotic fluid-handling, on-line sensors, and Bayesian optimization (BO) that navigates the multi-component salt solution spaces to achieve user-specified LCST targets. The platform demonstrates convergence to target properties within a minimal number of experiments. It strategically explores the parameter space, learns from informative "off-target" results, and self-corrects to achieve the final targets. By providing an accessible and adaptable blueprint, this work lowers the barrier to entry for autonomous experimentation and accelerates the design and discovery of functional polymers.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning</title>
<link>https://arxiv.org/abs/2509.05356</link>
<guid>https://arxiv.org/abs/2509.05356</guid>
<content:encoded><![CDATA[
arXiv:2509.05356v1 Announce Type: cross 
Abstract: Despite recent progress in training spiking neural networks (SNNs) for classification, their application to continuous motor control remains limited. Here, we demonstrate that fully spiking architectures can be trained end-to-end to control robotic arms with multiple degrees of freedom in continuous environments. Our predictive-control framework combines Leaky Integrate-and-Fire dynamics with surrogate gradients, jointly optimizing a forward model for dynamics prediction and a policy network for goal-directed action. We evaluate this approach on both a planar 2D reaching task and a simulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve stable training and accurate torque control, establishing their viability for high-dimensional motor tasks. An extensive ablation study highlights the role of initialization, learnable time constants, and regularization in shaping training dynamics. We conclude that while stable and effective control can be achieved, recurrent spiking networks remain highly sensitive to hyperparameter settings, underscoring the importance of principled design choices.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection</title>
<link>https://arxiv.org/abs/2509.05360</link>
<guid>https://arxiv.org/abs/2509.05360</guid>
<content:encoded><![CDATA[
arXiv:2509.05360v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated effectiveness across a wide variety of tasks involving natural language, however, a fundamental problem of hallucinations still plagues these models, limiting their trustworthiness in generating consistent, truthful information. Detecting hallucinations has quickly become an important topic, with various methods such as uncertainty estimation, LLM Judges, retrieval augmented generation (RAG), and consistency checks showing promise. Many of these methods build upon foundational metrics, such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth necessary to detect hallucinations effectively. In this work, we propose a novel approach inspired by ROUGE that constructs an N-Gram frequency tensor from LLM-generated text. This tensor captures richer semantic structure by encoding co-occurrence patterns, enabling better differentiation between factual and hallucinated content. We demonstrate this by applying tensor decomposition methods to extract singular values from each mode and use these as input features to train a multi-layer perceptron (MLP) binary classifier for hallucinations. Our method is evaluated on the HaluEval dataset and demonstrates significant improvements over traditional baselines, as well as competitive performance against state-of-the-art LLM judges.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning</title>
<link>https://arxiv.org/abs/2509.05362</link>
<guid>https://arxiv.org/abs/2509.05362</guid>
<content:encoded><![CDATA[
arXiv:2509.05362v1 Announce Type: cross 
Abstract: Scams exploiting real-time social engineering -- such as phishing, impersonation, and phone fraud -- remain a persistent and evolving threat across digital platforms. Existing defenses are largely reactive, offering limited protection during active interactions. We propose a privacy-preserving, AI-in-the-loop framework that proactively detects and disrupts scam conversations in real time. The system combines instruction-tuned artificial intelligence with a safety-aware utility function that balances engagement with harm minimization, and employs federated learning to enable continual model updates without raw data sharing. Experimental evaluations show that the system produces fluent and engaging responses (perplexity as low as 22.3, engagement $\approx$0.80), while human studies confirm significant gains in realism, safety, and effectiveness over strong baselines. In federated settings, models trained with FedAvg sustain up to 30 rounds while preserving high engagement ($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage ($\leq$0.0085). Even with differential privacy, novelty and safety remain stable, indicating that robust privacy can be achieved without sacrificing performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3, MD-Judge) shows a straightforward pattern: stricter moderation settings reduce the chance of exposing personal information, but they also limit how much the model engages in conversation. In contrast, more relaxed settings allow longer and richer interactions, which improve scam detection, but at the cost of higher privacy risk. To our knowledge, this is the first framework to unify real-time scam-baiting, federated privacy preservation, and calibrated safety moderation into a proactive defense paradigm.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Horizon Visual Imitation Learning via Plan and Code Reflection</title>
<link>https://arxiv.org/abs/2509.05368</link>
<guid>https://arxiv.org/abs/2509.05368</guid>
<content:encoded><![CDATA[
arXiv:2509.05368v1 Announce Type: cross 
Abstract: Learning from long-horizon demonstrations with complex action sequences presents significant challenges for visual imitation learning, particularly in understanding temporal relationships of actions and spatial relationships between objects. In this paper, we propose a new agent framework that incorporates two dedicated reflection modules to enhance both plan and code generation. The plan generation module produces an initial action sequence, which is then verified by the plan reflection module to ensure temporal coherence and spatial alignment with the demonstration video. The code generation module translates the plan into executable code, while the code reflection module verifies and refines the generated code to ensure correctness and consistency with the generated plan. These two reflection modules jointly enable the agent to detect and correct errors in both the plan generation and code generation, improving performance in tasks with intricate temporal and spatial dependencies. To support systematic evaluation, we introduce LongVILBench, a benchmark comprising 300 human demonstrations with action sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial complexity across multiple task types. Experimental results demonstrate that existing methods perform poorly on this benchmark, whereas our new framework establishes a strong baseline for long-horizon visual imitation learning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Murphys Laws of AI Alignment: Why the Gap Always Wins</title>
<link>https://arxiv.org/abs/2509.05381</link>
<guid>https://arxiv.org/abs/2509.05381</guid>
<content:encoded><![CDATA[
arXiv:2509.05381v1 Announce Type: cross 
Abstract: Large language models are increasingly aligned to human preferences through reinforcement learning from human feedback (RLHF) and related methods such as Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While effective, these methods exhibit recurring failure patterns i.e., reward hacking, sycophancy, annotator drift, and misgeneralization. We introduce the concept of the Alignment Gap, a unifying lens for understanding recurring failures in feedback-based alignment. Using a KL-tilting formalism, we illustrate why optimization pressure tends to amplify divergence between proxy rewards and true human intent. We organize these failures into a catalogue of Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to frame trade-offs among optimization strength, value capture, and generalization. Small-scale empirical studies serve as illustrative support. Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure, Shift) as practical design levers. Our contribution is not a definitive impossibility theorem but a perspective that reframes alignment debates around structural limits and trade-offs, offering clearer guidance for future design.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.05397</link>
<guid>https://arxiv.org/abs/2509.05397</guid>
<content:encoded><![CDATA[
arXiv:2509.05397v1 Announce Type: cross 
Abstract: Modern robotic manufacturing requires collision-free coordination of multiple robots to complete numerous tasks in shared, obstacle-rich workspaces. Although individual tasks may be simple in isolation, automated joint task allocation, scheduling, and motion planning under spatio-temporal constraints remain computationally intractable for classical methods at real-world scales. Existing multi-arm systems deployed in the industry rely on human intuition and experience to design feasible trajectories manually in a labor-intensive process. To address this challenge, we propose a reinforcement learning (RL) framework to achieve automated task and motion planning, tested in an obstacle-rich environment with eight robots performing 40 reaching tasks in a shared workspace, where any robot can perform any task in any order. Our approach builds on a graph neural network (GNN) policy trained via RL on procedurally-generated environments with diverse obstacle layouts, robot configurations, and task distributions. It employs a graph representation of scenes and a graph policy neural network trained through reinforcement learning to generate trajectories of multiple robots, jointly solving the sub-problems of task allocation, scheduling, and motion planning. Trained on large randomly generated task sets in simulation, our policy generalizes zero-shot to unseen settings with varying robot placements, obstacle geometries, and task poses. We further demonstrate that the high-speed capability of our solution enables its use in workcell layout optimization, improving solution times. The speed and scalability of our planner also open the door to new capabilities such as fault-tolerant planning and online perception-based re-planning, where rapid adaptation to dynamic task sets is required.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unmasking COVID-19 Vulnerability in Nigeria: Mapping Risks Beyond Urban Hotspots</title>
<link>https://arxiv.org/abs/2509.05398</link>
<guid>https://arxiv.org/abs/2509.05398</guid>
<content:encoded><![CDATA[
arXiv:2509.05398v1 Announce Type: cross 
Abstract: The COVID-19 pandemic has presented significant challenges in Nigeria's public health systems since the first case reported on February 27, 2020. This study investigates key factors that contribute to state vulnerability, quantifying them through a composite risk score integrating population density (weight 0.2), poverty (0.4), access to healthcare (0.3), and age risk (0.1), adjusted by normalized case rates per 100,000. States were categorized into low-, medium-, and high-density areas to analyze trends and identify hotspots using geographic information system (GIS) mapping. The findings reveal that high-density urban areas, such as Lagos, accounting for 35.4% of national cases, had the highest risk scores (Lagos: 673.47 vs. national average: 28.16). These results align with global and local studies on the spatial variability of COVID-19 in Nigeria, including international frameworks such as the CDC Social Vulnerability Index. Google Trends data highlight variations in public health awareness, serving as a supplementary analysis to contextualize vulnerability. The risk score provides a prioritization tool for policymakers to allocate testing, vaccines, and healthcare resources to high-risk areas, though data gaps and rural underreporting call for further research. This framework can extend to other infectious diseases, offering lessons for future pandemics in resource-limited settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding</title>
<link>https://arxiv.org/abs/2509.05431</link>
<guid>https://arxiv.org/abs/2509.05431</guid>
<content:encoded><![CDATA[
arXiv:2509.05431v1 Announce Type: cross 
Abstract: Brain tumor segmentation is a critical pre-processing step in the medical image analysis pipeline that involves precise delineation of tumor regions from healthy brain tissue in medical imaging data, particularly MRI scans. An efficient and effective decoding mechanism is crucial in brain tumor segmentation especially in scenarios with limited computational resources. However these decoding mechanisms usually come with high computational costs. To address this concern EMCAD a new efficient multi-scale convolutional attention decoder designed was utilized to optimize both performance and computational efficiency for brain tumor segmentation on the BraTs2020 dataset consisting of MRI scans from 369 brain tumor patients. The preliminary result obtained by the model achieved a best Dice score of 0.31 and maintained a stable mean Dice score of 0.285 plus/minus 0.015 throughout the training process which is moderate. The initial model maintained consistent performance across the validation set without showing signs of over-fitting.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too</title>
<link>https://arxiv.org/abs/2509.05440</link>
<guid>https://arxiv.org/abs/2509.05440</guid>
<content:encoded><![CDATA[
arXiv:2509.05440v1 Announce Type: cross 
Abstract: As large-language models have been increasingly used as automatic raters for evaluating free-form content, including document summarization, dialog, and story generation, work has been dedicated to evaluating such models by measuring their correlations with human judgment. For \textit{sample-level} performance, methods which operate by using pairwise comparisons between machine-generated text perform well but often lack the ability to assign absolute scores to individual summaries, an ability crucial for use cases that require thresholding. In this work, we propose a direct-scoring method which uses synthetic summaries to act as pairwise machine rankings at test time. We show that our method performs comparably to state-of-the-art pairwise evaluators in terms of axis-averaged sample-level correlations on the SummEval (\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05}) meta-evaluation benchmarks, and release the synthetic in-context summaries as data to facilitate future work.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAVAE-Effective Frequency Aware Latent Tokenizer</title>
<link>https://arxiv.org/abs/2509.05441</link>
<guid>https://arxiv.org/abs/2509.05441</guid>
<content:encoded><![CDATA[
arXiv:2509.05441v1 Announce Type: cross 
Abstract: Latent generative models have shown remarkable progress in high-fidelity image synthesis, typically using a two-stage training process that involves compressing images into latent embeddings via learned tokenizers in the first stage. The quality of generation strongly depends on how expressive and well-optimized these latent embeddings are. While various methods have been proposed to learn effective latent representations, the reconstructed images often lack realism, particularly in textured regions with sharp transitions, due to loss of fine details governed by high frequencies. We conduct a detailed frequency decomposition of existing state-of-the-art (SOTA) latent tokenizers and show that conventional objectives inherently prioritize low-frequency reconstruction, often at the expense of high-frequency fidelity. Our analysis reveals these latent tokenizers exhibit a bias toward low-frequency information, when jointly optimized, leading to over-smoothed outputs and visual artifacts that diminish perceptual quality. To address this, we propose a wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework that explicitly decouples the optimization of low- and high-frequency components. This decoupling enables improved reconstruction of fine textures while preserving global structure. Our approach bridges the fidelity gap in current latent tokenizers and emphasizes the importance of frequency-aware optimization for realistic image representation, with broader implications for applications in content creation, neural rendering, and medical imaging.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)</title>
<link>https://arxiv.org/abs/2509.05447</link>
<guid>https://arxiv.org/abs/2509.05447</guid>
<content:encoded><![CDATA[
arXiv:2509.05447v1 Announce Type: cross 
Abstract: In wireless networks characterized by dense connectivity, the significant signaling overhead generated by distributed link scheduling algorithms can exacerbate issues like congestion, energy consumption, and radio footprint expansion. To mitigate these challenges, we propose a distributed link sparsification scheme employing graph neural networks (GNNs) to reduce scheduling overhead for delay-tolerant traffic while maintaining network capacity. A GNN module is trained to adjust contention thresholds for individual links based on traffic statistics and network topology, enabling links to withdraw from scheduling contention when they are unlikely to succeed. Our approach is facilitated by a novel offline constrained {unsupervised} learning algorithm capable of balancing two competing objectives: minimizing scheduling overhead while ensuring that total utility meets the required level. In simulated wireless multi-hop networks with up to 500 links, our link sparsification technique effectively alleviates network congestion and reduces radio footprints across four distinct distributed link scheduling protocols.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation</title>
<link>https://arxiv.org/abs/2509.05475</link>
<guid>https://arxiv.org/abs/2509.05475</guid>
<content:encoded><![CDATA[
arXiv:2509.05475v1 Announce Type: cross 
Abstract: Autonomous regolith excavation is a cornerstone of in-situ resource utilization for a sustained human presence beyond Earth. However, this task is fundamentally hindered by the complex interaction dynamics of granular media and the operational need for robots to use diverse tools. To address these challenges, this work introduces a framework where a model-based reinforcement learning agent learns within a parallelized simulation. This environment leverages high-fidelity particle physics and procedural generation to create a vast distribution of both lunar terrains and excavation tool geometries. To master this diversity, the agent learns an adaptive interaction strategy by dynamically modulating its own stiffness and damping at each control step through operational space control. Our experiments demonstrate that training with a procedural distribution of tools is critical for generalization and enables the development of sophisticated tool-aware behavior. Furthermore, we show that augmenting the agent with visual feedback significantly improves task success. These results represent a validated methodology for developing the robust and versatile autonomous systems required for the foundational tasks of future space missions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biomedical Literature Q&amp;A System Using Retrieval-Augmented Generation (RAG)</title>
<link>https://arxiv.org/abs/2509.05505</link>
<guid>https://arxiv.org/abs/2509.05505</guid>
<content:encoded><![CDATA[
arXiv:2509.05505v1 Announce Type: cross 
Abstract: This work presents a Biomedical Literature Question Answering (Q&amp;A) system based on a Retrieval-Augmented Generation (RAG) architecture, designed to improve access to accurate, evidence-based medical information. Addressing the shortcomings of conventional health search engines and the lag in public access to biomedical research, the system integrates diverse sources, including PubMed articles, curated Q&amp;A datasets, and medical encyclopedias ,to retrieve relevant information and generate concise, context-aware responses. The retrieval pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model optimized using QLoRA for efficient, low-resource training. The system supports both general medical queries and domain-specific tasks, with a focused evaluation on breast cancer literature demonstrating the value of domain-aligned retrieval. Empirical results, measured using BERTScore (F1), show substantial improvements in factual consistency and semantic relevance compared to baseline models. The findings underscore the potential of RAG-enhanced language models to bridge the gap between complex biomedical literature and accessible public health knowledge, paving the way for future work on multilingual adaptation, privacy-preserving inference, and personalized medical AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Multi-fidelity Surrogate Forward and Inverse Models for ICF Implosions</title>
<link>https://arxiv.org/abs/2509.05510</link>
<guid>https://arxiv.org/abs/2509.05510</guid>
<content:encoded><![CDATA[
arXiv:2509.05510v1 Announce Type: cross 
Abstract: Continued progress in inertial confinement fusion (ICF) requires solving inverse problems relating experimental observations to simulation input parameters, followed by design optimization. However, such high dimensional dynamic PDE-constrained optimization problems are extremely challenging or even intractable. It has been recently shown that inverse problems can be solved by only considering certain robust features. Here we consider the ICF capsule's deuterium-tritium (DT) interface, and construct a causal, dynamic, multifidelity reduced-order surrogate that maps from a time-dependent radiation temperature drive to the interface's radius and velocity dynamics. The surrogate targets an ODE embedding of DT interface dynamics, and is constructed by learning a controller for a base analytical model using low- and high-fidelity simulation training data with respect to radiation energy group structure. After demonstrating excellent accuracy of the surrogate interface model, we use machine learning (ML) models with surrogate-generated data to solve inverse problems optimizing radiation temperature drive to reproduce observed interface dynamics. For sparse snapshots in time, the ML model further characterizes the most informative times at which to sample dynamics. Altogether we demonstrate how operator learning, causal architectures, and physical inductive bias can be integrated to accelerate discovery, design, and diagnostics in high-energy-density systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cryo-EM as a Stochastic Inverse Problem</title>
<link>https://arxiv.org/abs/2509.05541</link>
<guid>https://arxiv.org/abs/2509.05541</guid>
<content:encoded><![CDATA[
arXiv:2509.05541v1 Announce Type: cross 
Abstract: Cryo-electron microscopy (Cryo-EM) enables high-resolution imaging of biomolecules, but structural heterogeneity remains a major challenge in 3D reconstruction. Traditional methods assume a discrete set of conformations, limiting their ability to recover continuous structural variability. In this work, we formulate cryo-EM reconstruction as a stochastic inverse problem (SIP) over probability measures, where the observed images are modeled as the push-forward of an unknown distribution over molecular structures via a random forward operator. We pose the reconstruction problem as the minimization of a variational discrepancy between observed and simulated image distributions, using statistical distances such as the KL divergence and the Maximum Mean Discrepancy. The resulting optimization is performed over the space of probability measures via a Wasserstein gradient flow, which we numerically solve using particles to represent and evolve conformational ensembles. We validate our approach using synthetic examples, including a realistic protein model, which demonstrates its ability to recover continuous distributions over structural states. We analyze the connection between our formulation and Maximum A Posteriori (MAP) approaches, which can be interpreted as instances of the discretize-then-optimize (DTO) framework. We further provide a consistency analysis, establishing conditions under which DTO methods, such as MAP estimation, converge to the solution of the underlying infinite-dimensional continuous problem. Beyond cryo-EM, the framework provides a general methodology for solving SIPs involving random forward operators.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On detection probabilities of link invariants</title>
<link>https://arxiv.org/abs/2509.05574</link>
<guid>https://arxiv.org/abs/2509.05574</guid>
<content:encoded><![CDATA[
arXiv:2509.05574v1 Announce Type: cross 
Abstract: We prove that the detection rate of n-crossing alternating links by link invariants insensitive to oriented mutation decays exponentially in n, implying that they detect alternating links with probability zero. This phenomenon applies broadly, in particular to quantum invariants such as the Jones or HOMFLYPT polynomials. We also use a big data approach to analyze several borderline cases (e.g. integral Khovanov or HOMFLYPT homologies), where our arguments almost, but not quite, apply, and we provide evidence that they too exhibit the same asymptotic behavior.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints</title>
<link>https://arxiv.org/abs/2509.05608</link>
<guid>https://arxiv.org/abs/2509.05608</guid>
<content:encoded><![CDATA[
arXiv:2509.05608v1 Announce Type: cross 
Abstract: The widespread deployment of LLMs across enterprise services has created a critical security blind spot. Organizations operate multiple LLM services handling billions of queries daily, yet regulatory compliance boundaries prevent these services from sharing threat intelligence about prompt injection attacks, the top security risk for LLMs. When an attack is detected in one service, the same threat may persist undetected in others for months, as privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence system that enables secure sharing of attack fingerprints across compliance boundaries. BinaryShield transforms suspicious prompts through a unique pipeline combining PII redaction, semantic embedding, binary quantization, and randomized response mechanism to potentially generate non-invertible fingerprints that preserve attack patterns while providing privacy. Our evaluations demonstrate that BinaryShield achieves an F1-score of 0.94, significantly outperforming SimHash (0.77), the privacy-preserving baseline, while achieving 64x storage reduction and 38x faster similarity search compared to dense embeddings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR</title>
<link>https://arxiv.org/abs/2509.05609</link>
<guid>https://arxiv.org/abs/2509.05609</guid>
<content:encoded><![CDATA[
arXiv:2509.05609v1 Announce Type: cross 
Abstract: Aligning acoustic and linguistic representations is a central challenge to bridge the pre-trained models in knowledge transfer for automatic speech recognition (ASR). This alignment is inherently structured and asymmetric: while multiple consecutive acoustic frames typically correspond to a single linguistic token (many-to-one), certain acoustic transition regions may relate to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often include frames with no linguistic counterpart, such as background noise or silence may lead to imbalanced matching conditions. In this work, we take a new insight to regard alignment and matching as a detection problem, where the goal is to identify meaningful correspondences with high precision and recall ensuring full coverage of linguistic tokens while flexibly handling redundant or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on this new insight, we propose an unbalanced optimal transport-based alignment model that explicitly handles distributional mismatch and structural asymmetries with soft and partial matching between acoustic and linguistic modalities. Our method ensures that every linguistic token is grounded in at least one acoustic observation, while allowing for flexible, probabilistic mappings from acoustic to linguistic units. We evaluate our proposed model with experiments on an CTC-based ASR system with a pre-trained language model for knowledge transfer. Experimental results demonstrate the effectiveness of our approach in flexibly controlling degree of matching and hence to improve ASR performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Evaluation of Multi-modal Approaches to Complex Player Profile Classification</title>
<link>https://arxiv.org/abs/2509.05624</link>
<guid>https://arxiv.org/abs/2509.05624</guid>
<content:encoded><![CDATA[
arXiv:2509.05624v1 Announce Type: cross 
Abstract: Modern adaptive games require nuanced player understanding, yet most models use simplified 5-10 category taxonomies that fail to capture diversity. Behavioral clustering cannot distinguish players with different motivations who act similarly. We present a systematic evaluation of multi-modal classification at scale, combining behavioral telemetry with semantic context to support 36 player profiles. Using 19,413 gameplay sessions from an AI-controlled text-based RPG, we compared behavioral-only baselines with multi-modal approaches that integrate action sequences and semantic descriptions. Traditional clustering achieved only 10% accuracy for 36-category classification, limited by semantic conflation where opposite actions produced identical features. Our multi-modal LSTM processing action-text pairs improved accuracy to 21%, showing both potential and limits of non-conversational data. Analysis by behavioral complexity revealed that non-neutral profiles reached 42% accuracy (15x above random), while neutral profiles dropped to 25% (9x above random). Identical actions such as "help the merchant" cannot reveal whether a player is neutral or strategically waiting. Without access to reasoning, even multi-modal models struggle, though above-baseline results confirm a meaningful signal. Since prediction beyond 20 categories remains unexplored, our findings establish benchmarks for complex player modeling. Behavioral data alone plateaus near 10% for 36 categories, while multi-modal integration enables 25%. For designers, this shows that personality-based adaptation requires conversational interaction, as predefined choices cannot capture intent. Our evaluation at 36-category scale offers guidance for building adaptive games that better understand their players.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives</title>
<link>https://arxiv.org/abs/2509.05627</link>
<guid>https://arxiv.org/abs/2509.05627</guid>
<content:encoded><![CDATA[
arXiv:2509.05627v1 Announce Type: cross 
Abstract: AI audits play a critical role in AI accountability and safety. One branch of the law for which AI audits are particularly salient is anti-discrimination law. Several areas of anti-discrimination law implicate the "less discriminatory alternative" (LDA) requirement, in which a protocol (e.g., model) is defensible if no less discriminatory protocol that achieves comparable performance can be found with a reasonable amount of effort. Notably, the burden of proving an LDA exists typically falls on the claimant (the party alleging discrimination). This creates a significant hurdle in AI cases, as the claimant would seemingly need to train a less discriminatory yet high-performing model, a task requiring resources and expertise beyond most litigants. Moreover, developers often shield information about and access to their model and training data as trade secrets, making it difficult to reproduce a similar model from scratch.
  In this work, we present a procedure enabling claimants to determine if an LDA exists, even when they have limited compute, data, information, and model access. We focus on the setting in which fairness is given by demographic parity and performance by binary cross-entropy loss. As our main result, we provide a novel closed-form upper bound for the loss-fairness Pareto frontier (PF). We show how the claimant can use it to fit a PF in the "low-resource regime," then extrapolate the PF that applies to the (large) model being contested, all without training a single large model. The expression thus serves as a scaling law for loss-fairness PFs. To use this scaling law, the claimant would require a small subsample of the train/test data. Then, the claimant can fit the context-specific PF by training as few as 7 (small) models. We stress test our main result in simulations, finding that our scaling law holds even when the exact conditions of our theory do not.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Learning for Hyperspectral Images of Trees</title>
<link>https://arxiv.org/abs/2509.05630</link>
<guid>https://arxiv.org/abs/2509.05630</guid>
<content:encoded><![CDATA[
arXiv:2509.05630v1 Announce Type: cross 
Abstract: Aerial remote sensing using multispectral and RGB imagers has provided a critical impetus to precision agriculture. Analysis of the hyperspectral images with limited or no labels is challenging. This paper focuses on self-supervised learning to create neural network embeddings reflecting vegetation properties of trees from aerial hyperspectral images of crop fields. Experimental results demonstrate that a constructed tree representation, using a vegetation property-related embedding space, performs better in downstream machine learning tasks compared to the direct use of hyperspectral vegetation properties as tree representations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions</title>
<link>https://arxiv.org/abs/2509.05685</link>
<guid>https://arxiv.org/abs/2509.05685</guid>
<content:encoded><![CDATA[
arXiv:2509.05685v1 Announce Type: cross 
Abstract: Transforming road network data into vector representations using deep learning has proven effective for road network analysis. However, urban road networks' heterogeneous and hierarchical nature poses challenges for accurate representation learning. Graph neural networks, which aggregate features from neighboring nodes, often struggle due to their homogeneity assumption and focus on a single structural scale. To address these issues, this paper presents MSRFormer, a novel road network representation learning framework that integrates multi-scale spatial interactions by addressing their flow heterogeneity and long-distance dependencies. It uses spatial flow convolution to extract small-scale features from large trajectory datasets, and identifies scale-dependent spatial interaction regions to capture the spatial structure of road networks and flow heterogeneity. By employing a graph transformer, MSRFormer effectively captures complex spatial dependencies across multiple scales. The spatial interaction features are fused using residual connections, which are fed to a contrastive learning algorithm to derive the final road network representation. Validation on two real-world datasets demonstrates that MSRFormer outperforms baseline methods in two road network analysis tasks. The performance gains of MSRFormer suggest the traffic-related task benefits more from incorporating trajectory data, also resulting in greater improvements in complex road network structures with up to 16% improvements compared to the most competitive baseline method. This research provides a practical framework for developing task-agnostic road network representation models and highlights distinct association patterns of the interplay between scale effects and flow heterogeneity of spatial interactions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust variational neural posterior estimation for simulation-based inference</title>
<link>https://arxiv.org/abs/2509.05724</link>
<guid>https://arxiv.org/abs/2509.05724</guid>
<content:encoded><![CDATA[
arXiv:2509.05724v1 Announce Type: cross 
Abstract: Recent advances in neural density estimation have enabled powerful simulation-based inference (SBI) methods that can flexibly approximate Bayesian inference for intractable stochastic models. Although these methods have demonstrated reliable posterior estimation when the simulator accurately represents the underlying data generative process (GDP), recent work has shown that they perform poorly in the presence of model misspecification. This poses a significant problem for their use on real-world problems, due to simulators always misrepresenting the true DGP to a certain degree. In this paper, we introduce robust variational neural posterior estimation (RVNP), a method which addresses the problem of misspecification in amortised SBI by bridging the simulation-to-reality gap using variational inference and error modelling. We test RVNP on multiple benchmark tasks, including using real data from astronomy, and show that it can recover robust posterior inference in a data-driven manner without adopting tunable hyperparameters or priors governing the misspecification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction</title>
<link>https://arxiv.org/abs/2509.05728</link>
<guid>https://arxiv.org/abs/2509.05728</guid>
<content:encoded><![CDATA[
arXiv:2509.05728v1 Announce Type: cross 
Abstract: This paper extends LiDAR-BIND, a modular multi-modal fusion framework that binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space, with mechanisms that explicitly enforce temporal consistency. We introduce three contributions: (i) temporal embedding similarity that aligns consecutive latents, (ii) a motion-aligned transformation loss that matches displacement between predictions and ground truth LiDAR, and (iii) windows temporal fusion using a specialised temporal module. We further update the model architecture to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR translation demonstrate improved temporal and spatial coherence, yielding lower absolute trajectory error and better occupancy map accuracy in Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a correlation-peak distance metric providing practical temporal quality indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially enhancing temporal stability, resulting in improved robustness and performance for downstream SLAM.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated</title>
<link>https://arxiv.org/abs/2509.05739</link>
<guid>https://arxiv.org/abs/2509.05739</guid>
<content:encoded><![CDATA[
arXiv:2509.05739v1 Announce Type: cross 
Abstract: Early research into data poisoning attacks against Large Language Models (LLMs) demonstrated the ease with which backdoors could be injected. More recent LLMs add step-by-step reasoning, expanding the attack surface to include the intermediate chain-of-thought (CoT) and its inherent trait of decomposing problems into subproblems. Using these vectors for more stealthy poisoning, we introduce ``decomposed reasoning poison'', in which the attacker modifies only the reasoning path, leaving prompts and final answers clean, and splits the trigger across multiple, individually harmless components.
  Fascinatingly, while it remains possible to inject these decomposed poisons, reliably activating them to change final answers (rather than just the CoT) is surprisingly difficult. This difficulty arises because the models can often recover from backdoors that are activated within their thought processes. Ultimately, it appears that an emergent form of backdoor robustness is originating from the reasoning capabilities of these advanced LLMs, as well as from the architectural separation between reasoning and final answer generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios</title>
<link>https://arxiv.org/abs/2509.05747</link>
<guid>https://arxiv.org/abs/2509.05747</guid>
<content:encoded><![CDATA[
arXiv:2509.05747v1 Announce Type: cross 
Abstract: We address the problem of accurate capture of interactive behaviors between two people in daily scenarios. Most previous works either only consider one person or solely focus on conversational gestures of two people, assuming the body orientation and/or position of each actor are constant or barely change over each interaction. In contrast, we propose to simultaneously model two people's activities, and target objective-driven, dynamic, and semantically consistent interactions which often span longer duration and cover bigger space. To this end, we capture a new multi-modal dataset dubbed InterAct, which is composed of 241 motion sequences where two people perform a realistic and coherent scenario for one minute or longer over a complete interaction. For each sequence, two actors are assigned different roles and emotion labels, and collaborate to finish one task or conduct a common interaction activity. The audios, body motions, and facial expressions of both persons are captured. InterAct contains diverse and complex motions of individuals and interesting and relatively long-term interaction patterns barely seen before. We also demonstrate a simple yet effective diffusion-based method that estimates interactive face expressions and body motions of two people from speech inputs. Our method regresses the body motions in a hierarchical manner, and we also propose a novel fine-tuning mechanism to improve the lip accuracy of facial expressions. To facilitate further research, the data and code is made available at https://hku-cg.github.io/interact/ .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating API Documentation with LLMs: A BERTopic Approach</title>
<link>https://arxiv.org/abs/2509.05749</link>
<guid>https://arxiv.org/abs/2509.05749</guid>
<content:encoded><![CDATA[
arXiv:2509.05749v1 Announce Type: cross 
Abstract: Developers rely on API documentation, but official sources are often lengthy, complex, or incomplete. Many turn to community-driven forums like Stack Overflow for practical insights. We propose automating the summarization of informal sources, focusing on Android APIs. Using BERTopic, we extracted prevalent topics from 3.6 million Stack Overflow posts and applied extractive summarization techniques to generate concise summaries, including code snippets. A user study with 30 Android developers assessed the summaries for coherence, relevance, informativeness, and satisfaction, showing improved productivity. Integrating formal API knowledge with community-generated content enhances documentation, making API resources more accessible and actionable work.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-averse Fair Multi-class Classification</title>
<link>https://arxiv.org/abs/2509.05771</link>
<guid>https://arxiv.org/abs/2509.05771</guid>
<content:encoded><![CDATA[
arXiv:2509.05771v1 Announce Type: cross 
Abstract: We develop a new classification framework based on the theory of coherent risk measures and systemic risk. The proposed approach is suitable for multi-class problems when the data is noisy, scarce (relative to the dimension of the problem), and the labeling might be unreliable. In the first part of our paper, we provide the foundation of the use of systemic risk models and show how to apply it in the context of linear and kernel-based multi-class problems. More advanced formulation via a system-theoretic approach with non-linear aggregation is proposed, which leads to a two-stage stochastic programming problem. A risk-averse regularized decomposition method is designed to solve the problem. We use a popular multi-class method as a benchmark in the performance analysis of the proposed classification methods. We illustrate our ideas by proposing several generalization of that method by the use of coherent measures of risk. The viability of the proposed risk-averse methods are supported theoretically and numerically. Additionally, we demonstrate that the application of systemic risk measures facilitates enforcing fairness in classification. Analysis and experiments regarding the fairness of the proposed models are carefully conducted. For all methods, our numerical experiments demonstrate that they are robust in the presence of unreliable training data and perform better on unknown data than the methods minimizing expected classification errors. Furthermore, the performance improves when the number of classes increases.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery</title>
<link>https://arxiv.org/abs/2509.05775</link>
<guid>https://arxiv.org/abs/2509.05775</guid>
<content:encoded><![CDATA[
arXiv:2509.05775v1 Announce Type: cross 
Abstract: Estimating heterogeneous treatment effects is critical in domains such as personalized medicine, resource allocation, and policy evaluation. A central challenge lies in identifying subpopulations that respond differently to interventions, thereby enabling more targeted and effective decision-making. While clustering methods are well-studied in unsupervised learning, their integration with causal inference remains limited. We propose a novel framework that clusters individuals based on estimated treatment effects using a learned kernel derived from causal forests, revealing latent subgroup structures. Our approach consists of two main steps. First, we estimate debiased Conditional Average Treatment Effects (CATEs) using orthogonalized learners via the Robinson decomposition, yielding a kernel matrix that encodes sample-level similarities in treatment responsiveness. Second, we apply kernelized clustering to this matrix to uncover distinct, treatment-sensitive subpopulations and compute cluster-level average CATEs. We present this kernelized clustering step as a form of regularization within the residual-on-residual regression framework. Through extensive experiments on semi-synthetic and real-world datasets, supported by ablation studies and exploratory analyses, we demonstrate the effectiveness of our method in capturing meaningful treatment effect heterogeneity.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vector-based loss functions for turbulent flow field inpainting</title>
<link>https://arxiv.org/abs/2509.05787</link>
<guid>https://arxiv.org/abs/2509.05787</guid>
<content:encoded><![CDATA[
arXiv:2509.05787v1 Announce Type: cross 
Abstract: When developing scientific machine learning (ML) approaches, it is often beneficial to embed knowledge of the physical system in question into the training process. One way to achieve this is by leveraging the specific characteristics of the data at hand. In the case of turbulent flows, fluid velocities can be measured and recorded as multi-component vectors at discrete points in space, using techniques such as particle image velocimetry (PIV) or computational fluid mechanics (CFD). However, the vectorised nature of the data is ignored by standard ML approaches, as widely-used loss functions such as the mean-square error treat each component of a velocity vector in isolation. Therefore, the aim of this work is to better preserve the physical characteristics of the data by introducing loss functions that utilise vector similarity metrics. To this end, vector-based loss functions are developed here and implemented alongside a U-Net model for a turbulent flow field inpainting problem, amounting to the prediction of velocity vectors inside large gaps in PIV images. The intention is for the inpainting task to pose a significant challenge for the ML models in order to shed light on their capabilities. The test case uses PIV data from the highly turbulent flow in the well-known Transparent Combustion Chamber III (TCC-III) engine. Loss functions based on the cosine similarity and vector magnitude differences are proposed; the results show that the vector-based loss functions lead to significantly improved predictions of multi-scale flow patterns, while a hybrid (vector and mean-square error) loss function enables a good compromise to be found between preserving multi-scale behaviour and pixel-wise accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Methods in Complex Systems</title>
<link>https://arxiv.org/abs/2509.05793</link>
<guid>https://arxiv.org/abs/2509.05793</guid>
<content:encoded><![CDATA[
arXiv:2509.05793v1 Announce Type: cross 
Abstract: These notes offer a unified introduction to spectral methods for the study of complex systems. They are intended as an operative manual rather than a theorem-proof textbook: the emphasis is on tools, identities, and perspectives that can be readily applied across disciplines. Beginning with a compendium of matrix identities and inversion techniques, the text develops the connections between spectra, dynamics, and structure in finite-dimensional systems. Applications range from dynamical stability and random walks on networks to input-output economics, PageRank, epidemic spreading, memristive circuits, synchronization phenomena, and financial stability. Throughout, the guiding principle is that eigenvalues, eigenvectors, and resolvent operators provide a common language linking problems in physics, mathematics, computer science, and beyond. The presentation is informal, accessible to advanced undergraduates, yet broad enough to serve as a reference for researchers interested in spectral approaches to complex systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Fourier Neural Operator-Plasma Fluid Model for Fast and Accurate Multiscale Simulations of High Power Microwave Breakdown</title>
<link>https://arxiv.org/abs/2509.05799</link>
<guid>https://arxiv.org/abs/2509.05799</guid>
<content:encoded><![CDATA[
arXiv:2509.05799v1 Announce Type: cross 
Abstract: Modeling and simulation of High Power Microwave (HPM) breakdown, a multiscale phenomenon, is computationally expensive and requires solving Maxwell's equations (EM solver) coupled with a plasma continuity equation (plasma solver). In this work, we present a hybrid modeling approach that combines the accuracy of a differential equation-based plasma fluid solver with the computational efficiency of FNO (Fourier Neural Operator) based EM solver. Trained on data from an in-house FDTD-based plasma-fluid solver, the FNO replaces computationally expensive EM field updates, while the plasma solver governs the dynamic plasma response. The hybrid model is validated on microwave streamer formation, due to diffusion ionization mechanism, in a 2D scenario for unseen incident electric fields corresponding to entirely new plasma streamer simulations not included in model training, showing excellent agreement with FDTD based fluid simulations in terms of streamer shape, velocity, and temporal evolution. This hybrid FNO based strategy delivers significant acceleration of the order of 60X compared to traditional simulations for the specified problem size and offers an efficient alternative for computationally demanding multiscale and multiphysics simulations involved in HPM breakdown. Our work also demonstrate how such hybrid pipelines can be used to seamlessly to integrate existing C-based simulation codes with Python-based machine learning frameworks for simulations of plasma science and engineering problems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Volatility Modeling via EWMA-Driven Time-Dependent Hurst Parameters</title>
<link>https://arxiv.org/abs/2509.05820</link>
<guid>https://arxiv.org/abs/2509.05820</guid>
<content:encoded><![CDATA[
arXiv:2509.05820v1 Announce Type: cross 
Abstract: We introduce a novel rough Bergomi (rBergomi) model featuring a variance-driven exponentially weighted moving average (EWMA) time-dependent Hurst parameter $H_t$, fundamentally distinct from recent machine learning and wavelet-based approaches in the literature. Our framework pioneers a unified rough differential equation (RDE) formulation grounded in rough path theory, where the Hurst parameter dynamically adapts to evolving volatility regimes through a continuous EWMA mechanism tied to instantaneous variance. Unlike discrete model-switching or computationally intensive forecasting methods, our approach provides mathematical tractability while capturing volatility clustering and roughness bursts. We rigorously establish existence and uniqueness of solutions via rough path theory and derive martingale properties. Empirical validation on diverse asset classes including equities, cryptocurrencies, and commodities demonstrates superior performance in capturing dynamics and out-of-sample pricing accuracy. Our results show significant improvements over traditional constant-Hurst models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fisher Random Walk: Automatic Debiasing Contextual Preference Inference for Large Language Model Evaluation</title>
<link>https://arxiv.org/abs/2509.05852</link>
<guid>https://arxiv.org/abs/2509.05852</guid>
<content:encoded><![CDATA[
arXiv:2509.05852v1 Announce Type: cross 
Abstract: Motivated by the need for rigorous and scalable evaluation of large language models, we study contextual preference inference for pairwise comparison functionals of context-dependent preference score functions across domains. Focusing on the contextual Bradley-Terry-Luce model, we develop a semiparametric efficient estimator that automates the debiased estimation through aggregating weighted residual balancing terms across the comparison graph. We show that the efficiency is achieved when the weights are derived from a novel strategy called Fisher random walk. We also propose a computationally feasible method to compute the weights by a potential representation of nuisance weight functions. We show our inference procedure is valid for general score function estimators accommodating the practitioners' need to implement flexible deep learning methods. We extend the procedure to multiple hypothesis testing using a Gaussian multiplier bootstrap that controls familywise error and to distributional shift via a cross-fitted importance-sampling adjustment for target-domain inference. Numerical studies, including language model evaluations under diverse contexts, corroborate the accuracy, efficiency, and practical utility of our method.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification in Probabilistic Machine Learning Models: Theory, Methods, and Insights</title>
<link>https://arxiv.org/abs/2509.05877</link>
<guid>https://arxiv.org/abs/2509.05877</guid>
<content:encoded><![CDATA[
arXiv:2509.05877v1 Announce Type: cross 
Abstract: Uncertainty Quantification (UQ) is essential in probabilistic machine learning models, particularly for assessing the reliability of predictions. In this paper, we present a systematic framework for estimating both epistemic and aleatoric uncertainty in probabilistic models. We focus on Gaussian Process Latent Variable Models and employ scalable Random Fourier Features-based Gaussian Processes to approximate predictive distributions efficiently. We derive a theoretical formulation for UQ, propose a Monte Carlo sampling-based estimation method, and conduct experiments to evaluate the impact of uncertainty estimation. Our results provide insights into the sources of predictive uncertainty and illustrate the effectiveness of our approach in quantifying the confidence in the predictions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues</title>
<link>https://arxiv.org/abs/2509.05882</link>
<guid>https://arxiv.org/abs/2509.05882</guid>
<content:encoded><![CDATA[
arXiv:2509.05882v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) integrate into diverse workflows, they are increasingly being considered "collaborators" with humans. If such AI collaborators are to be reliable, their behavior over multiturn interactions must be predictable, validated and verified before deployment. Common alignment techniques are typically developed under simplified single-user settings and do not account for the dynamics of long-horizon multiparty interactions. This paper examines how different alignment methods affect LLM agents' effectiveness as partners in multiturn, multiparty collaborations. We study this question through the lens of friction agents that intervene in group dialogues to encourage the collaborative group to slow down and reflect upon their reasoning for deliberative decision-making. Using a roleplay methodology, we evaluate interventions from differently-trained friction agents in collaborative task conversations. We propose a novel counterfactual evaluation framework that quantifies how friction interventions change the trajectory of group collaboration and belief alignment. Our results show that a friction-aware approach significantly outperforms common alignment baselines in helping both convergence to a common ground, or agreed-upon task-relevant propositions, and correctness of task outcomes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near Real-Time Dust Aerosol Detection with 3D Convolutional Neural Networks on MODIS Data</title>
<link>https://arxiv.org/abs/2509.05887</link>
<guid>https://arxiv.org/abs/2509.05887</guid>
<content:encoded><![CDATA[
arXiv:2509.05887v1 Announce Type: cross 
Abstract: Dust storms harm health and reduce visibility; quick detection from satellites is needed. We present a near real-time system that flags dust at the pixel level using multi-band images from NASA's Terra and Aqua (MODIS). A 3D convolutional network learns patterns across all 36 bands, plus split thermal bands, to separate dust from clouds and surface features. Simple normalization and local filling handle missing data. An improved version raises training speed by 21x and supports fast processing of full scenes. On 17 independent MODIS scenes, the model reaches about 0.92 accuracy with a mean squared error of 0.014. Maps show strong agreement in plume cores, with most misses along edges. These results show that joint band-and-space learning can provide timely dust alerts at global scale; using wider input windows or attention-based models may further sharpen edges.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum spatial best-arm identification via quantum walks</title>
<link>https://arxiv.org/abs/2509.05890</link>
<guid>https://arxiv.org/abs/2509.05890</guid>
<content:encoded><![CDATA[
arXiv:2509.05890v1 Announce Type: cross 
Abstract: Quantum reinforcement learning has emerged as a framework combining quantum computation with sequential decision-making, and applications to the multi-armed bandit (MAB) problem have been reported. The graph bandit problem extends the MAB setting by introducing spatial constraints, yet quantum approaches remain limited. We propose a quantum algorithm for best-arm identification in graph bandits, termed Quantum Spatial Best-Arm Identification (QSBAI). The method employs quantum walks to encode superpositions over graph-constrained actions, extending amplitude amplification and generalizing the Quantum BAI algorithm via Szegedy's walk framework. This establishes a link between Grover-type search and reinforcement learning tasks with structural restrictions. We analyze complete and bipartite graphs, deriving the maximal success probability of identifying the best arm and the time step at which it is achieved. Our results highlight the potential of quantum walks to accelerate exploration in constrained environments and extend the applicability of quantum algorithms for decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine learning magnetism from simple global descriptors</title>
<link>https://arxiv.org/abs/2509.05909</link>
<guid>https://arxiv.org/abs/2509.05909</guid>
<content:encoded><![CDATA[
arXiv:2509.05909v1 Announce Type: cross 
Abstract: The reliable identification of magnetic ground states remains a major challenge in high-throughput materials databases, where density functional theory (DFT) workflows often converge to ferromagnetic (FM) solutions. Here, we partially address this challenge by developing machine learning classifiers trained on experimentally validated MAGNDATA magnetic materials leveraging a limited number of simple compositional, structural, and electronic descriptors sourced from the Materials Project database. Our propagation vector classifiers achieve accuracies above 92%, outperforming recent studies in reliably distinguishing zero from nonzero propagation vector structures, and exposing a systematic ferromagnetic bias inherent to the Materials Project database for more than 7,843 materials. In parallel, LightGBM and XGBoost models trained directly on the Materials Project labels achieve accuracies of 84-86% (with macro F1 average scores of 63-66%), which proves useful for large-scale screening for magnetic classes, if refined by MAGNDATA-trained classifiers. These results underscore the role of machine learning techniques as corrective and exploratory tools, enabling more trustworthy databases and accelerating progress toward the identification of materials with various properties.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALPHA: LLM-Enabled Active Learning for Human-Free Network Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.05936</link>
<guid>https://arxiv.org/abs/2509.05936</guid>
<content:encoded><![CDATA[
arXiv:2509.05936v1 Announce Type: cross 
Abstract: Network log data analysis plays a critical role in detecting security threats and operational anomalies. Traditional log analysis methods for anomaly detection and root cause analysis rely heavily on expert knowledge or fully supervised learning models, both of which require extensive labeled data and significant human effort. To address these challenges, we propose ALPHA, the first Active Learning Pipeline for Human-free log Analysis. ALPHA integrates semantic embedding, clustering-based representative sampling, and large language model (LLM)-assisted few-shot annotation to automate the anomaly detection process. The LLM annotated labels are propagated across clusters, enabling large-scale training of an anomaly detector with minimal supervision. To enhance the annotation accuracy, we propose a two-step few-shot refinement strategy that adaptively selects informative prompts based on the LLM's observed error patterns. Extensive experiments on real-world log datasets demonstrate that ALPHA achieves detection accuracy comparable to fully supervised methods while mitigating human efforts in the loop. ALPHA also supports interpretable analysis through LLM-driven root cause explanations in the post-detection stage. These capabilities make ALPHA a scalable and cost-efficient solution for truly automated log-based anomaly detection.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services</title>
<link>https://arxiv.org/abs/2509.05941</link>
<guid>https://arxiv.org/abs/2509.05941</guid>
<content:encoded><![CDATA[
arXiv:2509.05941v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) has created a significant integration challenge in the AI agent ecosystem, often called the "$N \times M$ problem," where N models require custom integrations for M tools. This fragmentation stifles innovation and creates substantial development overhead. While the Model Context Protocol (MCP) has emerged as a standard to resolve this, its adoption is hindered by the manual effort required to convert the vast universe of existing software into MCP-compliant services. This is especially true for the millions of open-source repositories on GitHub, the world's largest collection of functional code. This paper introduces Code2MCP, a highly automated, agentic framework designed to transform any GitHub repository into a functional MCP service with minimal human intervention. Our system employs a multi-stage workflow that automates the entire process, from code analysis and environment configuration to service generation and deployment. A key innovation of our framework is an LLM-driven, closed-loop "Run--Review--Fix" cycle, which enables the system to autonomously debug and repair the code it generates. Code2MCP produces not only deployable services but also comprehensive technical documentation, acting as a catalyst to accelerate the MCP ecosystem by systematically unlocking the world's largest open-source code repository and automating the critical last mile of tool integration. The code is open-sourced at https://github.com/DEFENSE-SEU/MCP-Github-Agent.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance</title>
<link>https://arxiv.org/abs/2509.05978</link>
<guid>https://arxiv.org/abs/2509.05978</guid>
<content:encoded><![CDATA[
arXiv:2509.05978v1 Announce Type: cross 
Abstract: Vision-language models have demonstrated impressive capabilities in generating 2D images under various conditions; however the impressive performance of these models in 2D is largely enabled by extensive, readily available pretrained foundation models. Critically, comparable pretrained foundation models do not exist for 3D, significantly limiting progress in this domain. As a result, the potential of vision-language models to produce high-resolution 3D counterfactual medical images conditioned solely on natural language descriptions remains completely unexplored. Addressing this gap would enable powerful clinical and research applications, such as personalized counterfactual explanations, simulation of disease progression scenarios, and enhanced medical training by visualizing hypothetical medical conditions in realistic detail. Our work takes a meaningful step toward addressing this challenge by introducing a framework capable of generating high-resolution 3D counterfactual medical images of synthesized patients guided by free-form language prompts. We adapt state-of-the-art 3D diffusion models with enhancements from Simple Diffusion and incorporate augmented conditioning to improve text alignment and image quality. To our knowledge, this represents the first demonstration of a language-guided native-3D diffusion model applied specifically to neurological imaging data, where faithful three-dimensional modeling is essential to represent the brain's three-dimensional structure. Through results on two distinct neurological MRI datasets, our framework successfully simulates varying counterfactual lesion loads in Multiple Sclerosis (MS), and cognitive states in Alzheimer's disease, generating high-quality images while preserving subject fidelity in synthetically generated medical images. Our results lay the groundwork for prompt-driven disease progression analysis within 3D medical imaging.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Khana: A Comprehensive Indian Cuisine Dataset</title>
<link>https://arxiv.org/abs/2509.06006</link>
<guid>https://arxiv.org/abs/2509.06006</guid>
<content:encoded><![CDATA[
arXiv:2509.06006v1 Announce Type: cross 
Abstract: As global interest in diverse culinary experiences grows, food image models are essential for improving food-related applications by enabling accurate food recognition, recipe suggestions, dietary tracking, and automated meal planning. Despite the abundance of food datasets, a noticeable gap remains in capturing the nuances of Indian cuisine due to its vast regional diversity, complex preparations, and the lack of comprehensive labeled datasets that cover its full breadth. Through this exploration, we uncover Khana, a new benchmark dataset for food image classification, segmentation, and retrieval of dishes from Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian cuisine and offering around 131K images in the dataset spread across 80 labels, each with a resolution of 500x500 pixels. This paper describes the dataset creation process and evaluates state-of-the-art models on classification, segmentation, and retrieval as baselines. Khana bridges the gap between research and development by providing a comprehensive and challenging benchmark for researchers while also serving as a valuable resource for developers creating real-world applications that leverage the rich tapestry of Indian cuisine. Webpage: https://khana.omkar.xyz
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2509.06026</link>
<guid>https://arxiv.org/abs/2509.06026</guid>
<content:encoded><![CDATA[
arXiv:2509.06026v1 Announce Type: cross 
Abstract: While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations by integrating external knowledge bases, it introduces vulnerabilities to membership inference attacks (MIAs), particularly in systems handling sensitive data. Existing MIAs targeting RAG's external databases often rely on model responses but ignore the interference of non-member-retrieved documents on RAG outputs, limiting their effectiveness. To address this, we propose DCMI, a differential calibration MIA that mitigates the negative impact of non-member-retrieved documents. Specifically, DCMI leverages the sensitivity gap between member and non-member retrieved documents under query perturbation. It generates perturbed queries for calibration to isolate the contribution of member-retrieved documents while minimizing the interference from non-member-retrieved documents. Experiments under progressively relaxed assumptions show that DCMI consistently outperforms baselines--for example, achieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5, exceeding the MBA baseline by over 40%. Furthermore, on real-world RAG platforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the baseline. These results highlight significant privacy risks in RAG systems and emphasize the need for stronger protection mechanisms. We appeal to the community's consideration of deeper investigations, like ours, against the data leakage risks in rapidly evolving RAG systems. Our code is available at https://github.com/Xinyu140203/RAG_MIA.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models</title>
<link>https://arxiv.org/abs/2509.06040</link>
<guid>https://arxiv.org/abs/2509.06040</guid>
<content:encoded><![CDATA[
arXiv:2509.06040v1 Announce Type: cross 
Abstract: Recent advancements in aligning image and video generative models via GRPO have achieved remarkable gains in enhancing human preference alignment. However, these methods still face high computational costs from on-policy rollouts and excessive SDE sampling steps, as well as training instability due to sparse rewards. In this paper, we propose BranchGRPO, a novel method that introduces a branch sampling policy updating the SDE sampling process. By sharing computation across common prefixes and pruning low-reward paths and redundant depths, BranchGRPO substantially lowers the per-update compute cost while maintaining or improving exploration diversity. This work makes three main contributions: (1) a branch sampling scheme that reduces rollout and training cost; (2) a tree-based advantage estimator incorporating dense process-level rewards; and (3) pruning strategies exploiting path and depth redundancy to accelerate convergence and boost performance. Experiments on image and video preference alignment show that BranchGRPO improves alignment scores by 16% over strong baselines, while cutting training time by 50%.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Reinforcement Learning to Optimize the Global and Local Crossing Number</title>
<link>https://arxiv.org/abs/2509.06108</link>
<guid>https://arxiv.org/abs/2509.06108</guid>
<content:encoded><![CDATA[
arXiv:2509.06108v1 Announce Type: cross 
Abstract: We present a novel approach to graph drawing based on reinforcement learning for minimizing the global and the local crossing number, that is, the total number of edge crossings and the maximum number of crossings on any edge, respectively. In our framework, an agent learns how to move a vertex based on a given observation vector in order to optimize its position. The agent receives feedback in the form of local reward signals tied to crossing reduction. To generate an initial layout, we use a stress-based graph-drawing algorithm. We compare our method against force- and stress-based (baseline) algorithms as well as three established algorithms for global crossing minimization on a suite of benchmark graphs. The experiments show mixed results: our current algorithm is mainly competitive for the local crossing number. We see a potential for further development of the approach in the future.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Additive Distributionally Robust Ranking and Selection</title>
<link>https://arxiv.org/abs/2509.06147</link>
<guid>https://arxiv.org/abs/2509.06147</guid>
<content:encoded><![CDATA[
arXiv:2509.06147v1 Announce Type: cross 
Abstract: Ranking and selection (R&amp;S) aims to identify the alternative with the best mean performance among $k$ simulated alternatives. The practical value of R&amp;S depends on accurate simulation input modeling, which often suffers from the curse of input uncertainty due to limited data. Distributionally robust ranking and selection (DRR&amp;S) addresses this challenge by modeling input uncertainty via an ambiguity set of $m > 1$ plausible input distributions, resulting in $km$ scenarios in total. Recent DRR&amp;S studies suggest a key structural insight: additivity in budget allocation is essential for efficiency. However, existing justifications are heuristic, and fundamental properties such as consistency and the precise allocation pattern induced by additivity remain poorly understood. In this paper, we propose a simple additive allocation (AA) procedure that aims to exclusively sample the $k + m - 1$ previously hypothesized critical scenarios. Leveraging boundary-crossing arguments, we establish a lower bound on the probability of correct selection and characterize the procedure's budget allocation behavior. We then prove that AA is consistent and, surprisingly, achieves additivity in the strongest sense: as the total budget increases, only $k + m - 1$ scenarios are sampled infinitely often. Notably, the worst-case scenarios of non-best alternatives may not be among them, challenging prior beliefs about their criticality. These results offer new and counterintuitive insights into the additive structure of DRR&amp;S. To improve practical performance while preserving this structure, we introduce a general additive allocation (GAA) framework that flexibly incorporates sampling rules from traditional R&amp;S procedures in a modular fashion. Numerical experiments support our theoretical findings and demonstrate the competitive performance of the proposed GAA procedures.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Gender and Political Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2509.06164</link>
<guid>https://arxiv.org/abs/2509.06164</guid>
<content:encoded><![CDATA[
arXiv:2509.06164v1 Announce Type: cross 
Abstract: We introduce EuroParlVote, a novel benchmark for evaluating large language models (LLMs) in politically sensitive contexts. It links European Parliament debate speeches to roll-call vote outcomes and includes rich demographic metadata for each Member of the European Parliament (MEP), such as gender, age, country, and political group. Using EuroParlVote, we evaluate state-of-the-art LLMs on two tasks -- gender classification and vote prediction -- revealing consistent patterns of bias. We find that LLMs frequently misclassify female MEPs as male and demonstrate reduced accuracy when simulating votes for female speakers. Politically, LLMs tend to favor centrist groups while underperforming on both far-left and far-right ones. Proprietary models like GPT-4o outperform open-weight alternatives in terms of both robustness and fairness. We release the EuroParlVote dataset, code, and demo to support future research on fairness and accountability in NLP within political contexts.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Analysis for Resilient AI System</title>
<link>https://arxiv.org/abs/2509.06172</link>
<guid>https://arxiv.org/abs/2509.06172</guid>
<content:encoded><![CDATA[
arXiv:2509.06172v1 Announce Type: cross 
Abstract: Operational hazards in Manufacturing Industrial Internet (MII) systems generate severe data outliers that cripple traditional statistical analysis. This paper proposes a novel robust regression method, DPD-Lasso, which integrates Density Power Divergence with Lasso regularization to analyze contaminated data from AI resilience experiments. We develop an efficient iterative algorithm to overcome previous computational bottlenecks. Applied to an MII testbed for Aerosol Jet Printing, DPD-Lasso provides reliable, stable performance on both clean and outlier-contaminated data, accurately quantifying hazard impacts. This work establishes robust regression as an essential tool for developing and validating resilient industrial AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling shopper interest broadness with entropy-driven dialogue policy in the context of arbitrarily large product catalogs</title>
<link>https://arxiv.org/abs/2509.06185</link>
<guid>https://arxiv.org/abs/2509.06185</guid>
<content:encoded><![CDATA[
arXiv:2509.06185v1 Announce Type: cross 
Abstract: Conversational recommender systems promise rich interactions for e-commerce, but balancing exploration (clarifying user needs) and exploitation (making recommendations) remains challenging, especially when deploying large language models (LLMs) with vast product catalogs. We address this challenge by modeling the breadth of user interest via the entropy of retrieval score distributions. Our method uses a neural retriever to fetch relevant items for a user query and computes the entropy of the re-ranked scores to dynamically route the dialogue policy: low-entropy (specific) queries trigger direct recommendations, whereas high-entropy (ambiguous) queries prompt exploratory questions. This simple yet effective strategy allows an LLM-driven agent to remain aware of an arbitrarily large catalog in real-time without bloating its context window.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)</title>
<link>https://arxiv.org/abs/2509.06191</link>
<guid>https://arxiv.org/abs/2509.06191</guid>
<content:encoded><![CDATA[
arXiv:2509.06191v1 Announce Type: cross 
Abstract: Recent 3D generative models, which are capable of generating full object shapes from just a few images, now open up new opportunities in robotics. In this work, we show that 3D generative models can be used to augment a dataset from a single real-world demonstration, after which an omnidirectional policy can be learned within this imagined dataset. We found that this enables a robot to perform a task when initialised from states very far from those observed during the demonstration, including starting from the opposite side of the object relative to the real-world demonstration, significantly reducing the number of demonstrations required for policy learning. Through several real-world experiments across tasks such as grasping objects, opening a drawer, and placing trash into a bin, we study these omnidirectional policies by investigating the effect of various design choices on policy behaviour, and we show superior performance to recent baselines which use alternative methods for data augmentation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control</title>
<link>https://arxiv.org/abs/2509.06201</link>
<guid>https://arxiv.org/abs/2509.06201</guid>
<content:encoded><![CDATA[
arXiv:2509.06201v1 Announce Type: cross 
Abstract: Grasping of diverse objects in unstructured environments remains a significant challenge. Open-loop grasping methods, effective in controlled settings, struggle in cluttered environments. Grasp prediction errors and object pose changes during grasping are the main causes of failure. In contrast, closed-loop methods address these challenges in simplified settings (e.g., single object on a table) on a limited set of objects, with no path to generalization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping policy designed for robust and reactive grasping of novel objects in cluttered environments. Grasp-MPC incorporates a value function, trained on visual observations from a large-scale synthetic dataset of 2 million grasp trajectories that include successful and failed attempts. We deploy this learned value function in an MPC framework in combination with other cost terms that encourage collision avoidance and smooth execution. We evaluate Grasp-MPC on FetchBench and real-world settings across diverse environments. Grasp-MPC improves grasp success rates by up to 32.6% in simulation and 33.3% in real-world noisy conditions, outperforming open-loop, diffusion policy, transformer policy, and IQL approaches. Videos and more at http://grasp-mpc.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repeating vs. Non-Repeating FRBs: A Deep Learning Approach To Morphological Characterization</title>
<link>https://arxiv.org/abs/2509.06208</link>
<guid>https://arxiv.org/abs/2509.06208</guid>
<content:encoded><![CDATA[
arXiv:2509.06208v1 Announce Type: cross 
Abstract: We present a deep learning approach to classify fast radio bursts (FRBs) based purely on morphology as encoded on recorded dynamic spectrum from CHIME/FRB Catalog 2. We implemented transfer learning with a pretrained ConvNext architecture, exploiting its powerful feature extraction ability. ConvNext was adapted to classify dedispersed dynamic spectra (which we treat as images) of the FRBs into one of the two sub-classes, i.e., repeater and non-repeater, based on their various temporal and spectral properties and relation between the sub-pulse structures. Additionally, we also used mathematical model representation of the total intensity data to interpret the deep learning model. Upon fine-tuning the pretrained ConvNext on the FRB spectrograms, we were able to achieve high classification metrics while substantially reducing training time and computing power as compared to training a deep learning model from scratch with random weights and biases without any feature extraction ability. Importantly, our results suggest that the morphological differences between CHIME repeating and non-repeating events persist in Catalog 2 and the deep learning model leveraged these differences for classification. The fine-tuned deep learning model can be used for inference, which enables us to predict whether an FRB's morphology resembles that of repeaters or non-repeaters. Such inferences may become increasingly significant when trained on larger data sets that will exist in the near future.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Efficiency Frontier: Classical Shadows versus Quantum Footage</title>
<link>https://arxiv.org/abs/2509.06218</link>
<guid>https://arxiv.org/abs/2509.06218</guid>
<content:encoded><![CDATA[
arXiv:2509.06218v1 Announce Type: cross 
Abstract: Interfacing quantum and classical processors is an important subroutine in full-stack quantum algorithms. The so-called "classical shadow" method efficiently extracts essential classical information from quantum states, enabling the prediction of many properties of a quantum system from only a few measurements. However, for a small number of highly non-local observables, or when classical post-processing power is limited, the classical shadow method is not always the most efficient choice. Here, we address this issue quantitatively by performing a full-stack resource analysis that compares classical shadows with ``quantum footage," which refers to direct quantum measurement. Under certain assumptions, our analysis illustrates a boundary of download efficiency between classical shadows and quantum footage. For observables expressed as linear combinations of Pauli matrices, the classical shadow method outperforms direct measurement when the number of observables is large and the Pauli weight is small. For observables in the form of large Hermitian sparse matrices, the classical shadow method shows an advantage when the number of observables, the sparsity of the matrix, and the number of qubits fall within a certain range. The key parameters influencing this behavior include the number of qubits $n$, observables $M$, sparsity $k$, Pauli weight $w$, accuracy requirement $\epsilon$, and failure tolerance $\delta$. We also compare the resource consumption of the two methods on different types of quantum computers and identify break-even points where the classical shadow method becomes more efficient, which vary depending on the hardware. This paper opens a new avenue for quantitatively designing optimal strategies for hybrid quantum-classical tomography and provides practical insights for selecting the most suitable quantum measurement approach in real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving</title>
<link>https://arxiv.org/abs/2509.06261</link>
<guid>https://arxiv.org/abs/2509.06261</guid>
<content:encoded><![CDATA[
arXiv:2509.06261v1 Announce Type: cross 
Abstract: Recent advances in Post-Training Quantization (PTQ) techniques have significantly increased demand for serving quantized large language models (LLMs), enabling higher throughput and substantially reduced memory usage with minimal accuracy loss. Quantized models address memory constraints in LLMs and enhance GPU resource utilization through efficient GPU sharing. However, quantized models have smaller KV block sizes than non-quantized models, causing limited memory efficiency due to memory fragmentation. Also, distinct resource usage patterns between quantized and non-quantized models require efficient scheduling to maximize throughput. To address these challenges, we propose FineServe, an inference serving framework for mixed-precision LLMs. FineServe's key contributions include: (1) KV Slab, a precision-aware adaptive memory management technique dynamically allocating KV cache based on model quantization characteristics, significantly reducing GPU memory fragmentation, and (2) a two-level scheduling framework comprising a global scheduler that places models to GPUs based on request rates, latency SLOs, and memory constraints and efficiency, and a local scheduler that adaptively adjusts batch sizes according to real-time request fluctuations. Experimental results demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x higher token generation throughput compared to the state-of-the-art GPU sharing systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss Random Variable Optimization</title>
<link>https://arxiv.org/abs/2509.06264</link>
<guid>https://arxiv.org/abs/2509.06264</guid>
<content:encoded><![CDATA[
arXiv:2509.06264v1 Announce Type: cross 
Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is a standard method for enforcing privacy in deep learning, typically using the Gaussian mechanism to perturb gradient updates. However, conventional mechanisms such as Gaussian and Laplacian noise are parameterized only by variance or scale. This single degree of freedom ties the magnitude of noise directly to both privacy loss and utility degradation, preventing independent control of these two factors. The problem becomes more pronounced when the number of composition rounds T and batch size B vary across tasks, as these variations induce task-dependent shifts in the privacy-utility trade-off, where small changes in noise parameters can disproportionately affect model accuracy. To address this limitation, we introduce PLRV-O, a framework that defines a broad search space of parameterized DP-SGD noise distributions, where privacy loss moments are tightly characterized yet can be optimized more independently with respect to utility loss. This formulation enables systematic adaptation of noise to task-specific requirements, including (i) model size, (ii) training duration, (iii) batch sampling strategies, and (iv) clipping thresholds under both training and fine-tuning settings. Empirical results demonstrate that PLRV-O substantially improves utility under strict privacy constraints. On CIFAR-10, a fine-tuned ViT achieves 94.03% accuracy at epsilon approximately 0.5, compared to 83.93% with Gaussian noise. On SST-2, RoBERTa-large reaches 92.20% accuracy at epsilon approximately 0.2, versus 50.25% with Gaussian.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Framework for Particle Swarm Optimization using Landscape Analysis and Machine Learning</title>
<link>https://arxiv.org/abs/2509.06272</link>
<guid>https://arxiv.org/abs/2509.06272</guid>
<content:encoded><![CDATA[
arXiv:2509.06272v1 Announce Type: cross 
Abstract: Swarm intelligence algorithms have demonstrated remarkable success in solving complex optimization problems across diverse domains. However, their widespread adoption is often hindered by limited transparency in how algorithmic components influence performance. This work presents a multi-faceted investigation of Particle Swarm Optimization (PSO) to further understand the key role of different topologies for better interpretability and explainability. To achieve this objective, we first develop a comprehensive landscape characterization framework using Exploratory Landscape Analysis (ELA) to quantify problem difficulty and identify critical features affecting the optimization performance of PSO. Next, we conduct a rigorous empirical study comparing three fundamental swarm communication architectures -- Ring, Star, and Von Neumann topologies -- analysing their distinct impacts on exploration-exploitation balance, convergence behaviour, and solution quality and eventually develop an explainable benchmarking framework for PSO, to decode how swarm topologies affects information flow, diversity, and convergence. Based on this, a novel machine learning approach for automated algorithm configuration is introduced for training predictive models on extensive Area over the Convergence Curve (AOCC) data to recommend optimal settings based on problem characteristics. Through systematic experimentation across twenty four benchmark functions in multiple dimensions, we establish practical guidelines for topology selection and parameter configuration. These findings advance the development of more transparent and reliable swarm intelligence systems. The source codes of this work can be accessed at https://github.com/GitNitin02/ioh_pso.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs</title>
<link>https://arxiv.org/abs/2509.06284</link>
<guid>https://arxiv.org/abs/2509.06284</guid>
<content:encoded><![CDATA[
arXiv:2509.06284v1 Announce Type: cross 
Abstract: Large language models (LLMs) have advanced general-purpose reasoning, showing strong performance across diverse tasks. However, existing methods often rely on implicit exploration, where the model follows stochastic and unguided reasoning paths-like walking without a map. This leads to unstable reasoning paths, lack of error correction, and limited learning from past experience. To address these issues, we propose a framework that shifts from implicit exploration to structured reasoning through guideline and refinement. First, we extract structured reasoning patterns from successful trajectories and reflective signals from failures. During inference, the model follows these guidelines step-by-step, with refinement applied after each step to correct errors and stabilize the reasoning process. Experiments on BBH and four additional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method consistently outperforms strong baselines across diverse reasoning tasks. Structured reasoning with stepwise execution and refinement improves stability and generalization, while guidelines transfer well across domains and flexibly support cross-model collaboration, matching or surpassing supervised fine-tuning in effectiveness and scalability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSAIC: Minimax-Optimal Sparsity-Adaptive Inference for Change Points in Dynamic Networks</title>
<link>https://arxiv.org/abs/2509.06303</link>
<guid>https://arxiv.org/abs/2509.06303</guid>
<content:encoded><![CDATA[
arXiv:2509.06303v1 Announce Type: cross 
Abstract: We propose a new inference framework, named MOSAIC, for change-point detection in dynamic networks with the simultaneous low-rank and sparse-change structure. We establish the minimax rate of detection boundary, which relies on the sparsity of changes. We then develop an eigen-decomposition-based test with screened signals that approaches the minimax rate in theory, with only a minor logarithmic loss. For practical implementation of MOSAIC, we adjust the theoretical test by a novel residual-based technique, resulting in a pivotal statistic that converges to a standard normal distribution via the martingale central limit theorem under the null hypothesis and achieves full power under the alternative hypothesis. We also analyze the minimax rate of testing boundary for dynamic networks without the low-rank structure, which almost aligns with the results in high-dimensional mean-vector change-point inference. We showcase the effectiveness of MOSAIC and verify our theoretical results with several simulation examples and a real data application.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimax optimal transfer learning for high-dimensional additive regression</title>
<link>https://arxiv.org/abs/2509.06308</link>
<guid>https://arxiv.org/abs/2509.06308</guid>
<content:encoded><![CDATA[
arXiv:2509.06308v1 Announce Type: cross 
Abstract: This paper studies high-dimensional additive regression under the transfer learning framework, where one observes samples from a target population together with auxiliary samples from different but potentially related regression models. We first introduce a target-only estimation procedure based on the smooth backfitting estimator with local linear smoothing. In contrast to previous work, we establish general error bounds under sub-Weibull($\alpha$) noise, thereby accommodating heavy-tailed error distributions. In the sub-exponential case ($\alpha=1$), we show that the estimator attains the minimax lower bound under regularity conditions, which requires a substantial departure from existing proof strategies. We then develop a novel two-stage estimation method within a transfer learning framework, and provide theoretical guarantees at both the population and empirical levels. Error bounds are derived for each stage under general tail conditions, and we further demonstrate that the minimax optimal rate is achieved when the auxiliary and target distributions are sufficiently close. All theoretical results are supported by simulation studies and real data analysis.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Low-Altitude Airspace Security: MLLM-Enabled UAV Intent Recognition</title>
<link>https://arxiv.org/abs/2509.06312</link>
<guid>https://arxiv.org/abs/2509.06312</guid>
<content:encoded><![CDATA[
arXiv:2509.06312v1 Announce Type: cross 
Abstract: The rapid development of the low-altitude economy emphasizes the critical need for effective perception and intent recognition of non-cooperative unmanned aerial vehicles (UAVs). The advanced generative reasoning capabilities of multimodal large language models (MLLMs) present a promising approach in such tasks. In this paper, we focus on the combination of UAV intent recognition and the MLLMs. Specifically, we first present an MLLM-enabled UAV intent recognition architecture, where the multimodal perception system is utilized to obtain real-time payload and motion information of UAVs, generating structured input information, and MLLM outputs intent recognition results by incorporating environmental information, prior knowledge, and tactical preferences. Subsequently, we review the related work and demonstrate their progress within the proposed architecture. Then, a use case for low-altitude confrontation is conducted to demonstrate the feasibility of our architecture and offer valuable insights for practical system design. Finally, the future challenges are discussed, followed by corresponding strategic recommendations for further applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift</title>
<link>https://arxiv.org/abs/2509.06338</link>
<guid>https://arxiv.org/abs/2509.06338</guid>
<content:encoded><![CDATA[
arXiv:2509.06338v1 Announce Type: cross 
Abstract: The widespread distribution of Large Language Models (LLMs) through public platforms like Hugging Face introduces significant security challenges. While these platforms perform basic security scans, they often fail to detect subtle manipulations within the embedding layer. This work identifies a novel class of deployment phase attacks that exploit this vulnerability by injecting imperceptible perturbations directly into the embedding layer outputs without modifying model weights or input text. These perturbations, though statistically benign, systematically bypass safety alignment mechanisms and induce harmful behaviors during inference. We propose Search based Embedding Poisoning(SEP), a practical, model agnostic framework that introduces carefully optimized perturbations into embeddings associated with high risk tokens. SEP leverages a predictable linear transition in model responses, from refusal to harmful output to semantic deviation to identify a narrow perturbation window that evades alignment safeguards. Evaluated across six aligned LLMs, SEP achieves an average attack success rate of 96.43% while preserving benign task performance and evading conventional detection mechanisms. Our findings reveal a critical oversight in deployment security and emphasize the urgent need for embedding level integrity checks in future LLM defense strategies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study</title>
<link>https://arxiv.org/abs/2509.06351</link>
<guid>https://arxiv.org/abs/2509.06351</guid>
<content:encoded><![CDATA[
arXiv:2509.06351v1 Announce Type: cross 
Abstract: Colorectal diseases, including inflammatory conditions and neoplasms, require quick, accurate care to be effectively treated. Traditional diagnostic pipelines require extensive preparation and rely on separate, individual evaluations on histological images and colonoscopy footage, introducing possible variability and inefficiencies. This pilot study proposes a unified deep learning network that uses convolutional neural networks (CN N s) to classify both histopathological slides and colonoscopy video frames in one pipeline. The pipeline integrates class-balancing learning, robust augmentation, and calibration methods to ensure accurate results. Static colon histology images were taken from the PathMNIST dataset, and the lower gastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset. The CNN architecture used was ResNet-50. This study demonstrates an interpretable and reproducible diagnostic pipeline that unifies multiple diagnostic modalities to advance and ease the detection of colorectal diseases.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research</title>
<link>https://arxiv.org/abs/2509.06355</link>
<guid>https://arxiv.org/abs/2509.06355</guid>
<content:encoded><![CDATA[
arXiv:2509.06355v1 Announce Type: cross 
Abstract: Modern simulation environments for complex multi-agent interactions must balance high-fidelity detail with computational efficiency. We present DECOY, a novel multi-agent simulator that abstracts strategic, long-horizon planning in 3D terrains into high-level discretized simulation while preserving low-level environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a testbed, our framework accurately simulates gameplay using only movement decisions as tactical positioning -- without explicitly modeling low-level mechanics such as aiming and shooting. Central to our approach is a waypoint system that simplifies and discretizes continuous states and actions, paired with neural predictive and generative models trained on real CS:GO tournament data to reconstruct event outcomes. Extensive evaluations show that replays generated from human data in DECOY closely match those observed in the original game. Our publicly available simulation environment provides a valuable tool for advancing research in strategic multi-agent planning and behavior generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification</title>
<link>https://arxiv.org/abs/2509.06367</link>
<guid>https://arxiv.org/abs/2509.06367</guid>
<content:encoded><![CDATA[
arXiv:2509.06367v1 Announce Type: cross 
Abstract: Drought stress is a major threat to global crop productivity, making its early and precise detection essential for sustainable agricultural management. Traditional approaches, though useful, are often time-consuming and labor-intensive, which has motivated the adoption of deep learning methods. In recent years, Convolutional Neural Network (CNN) and Vision Transformer architectures have been widely explored for drought stress identification; however, these models generally rely on a large number of trainable parameters, restricting their use in resource-limited and real-time agricultural settings. To address this challenge, we propose a novel lightweight hybrid CNN framework inspired by ResNet, DenseNet, and MobileNet architectures. The framework achieves a remarkable 15-fold reduction in trainable parameters compared to conventional CNN and Vision Transformer models, while maintaining competitive accuracy. In addition, we introduce a machine unlearning mechanism based on a gradient norm-based influence function, which enables targeted removal of specific training data influence, thereby improving model adaptability. The method was evaluated on an aerial image dataset of potato fields with expert-annotated healthy and drought-stressed regions. Experimental results show that our framework achieves high accuracy while substantially lowering computational costs. These findings highlight its potential as a practical, scalable, and adaptive solution for drought stress monitoring in precision agriculture, particularly under resource-constrained conditions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster</title>
<link>https://arxiv.org/abs/2509.06426</link>
<guid>https://arxiv.org/abs/2509.06426</guid>
<content:encoded><![CDATA[
arXiv:2509.06426v1 Announce Type: cross 
Abstract: Computational models are critical to advance our understanding of how neural, biomechanical, and physical systems interact to orchestrate animal behaviors. Despite the availability of near-complete reconstructions of the Drosophila melanogaster central nervous system, musculature, and exoskeleton, anatomically and physically grounded models of fly leg muscles are still missing. These models provide an indispensable bridge between motor neuron activity and joint movements. Here, we introduce the first 3D, data-driven musculoskeletal model of Drosophila legs, implemented in both OpenSim and MuJoCo simulation environments. Our model incorporates a Hill-type muscle representation based on high-resolution X-ray scans from multiple fixed specimens. We present a pipeline for constructing muscle models using morphological imaging data and for optimizing unknown muscle parameters specific to the fly. We then combine our musculoskeletal models with detailed 3D pose estimation data from behaving flies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of muscle activity across diverse walking and grooming behaviors predict coordinated muscle synergies that can be tested experimentally. Furthermore, by training imitation learning policies in MuJoCo, we test the effect of different passive joint properties on learning speed and find that damping and stiffness facilitate learning. Overall, our model enables the investigation of motor control in an experimentally tractable model organism, providing insights into how biomechanics contribute to generation of complex limb movements. Moreover, our model can be used to control embodied artificial agents to generate naturalistic and compliant locomotion in simulated environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks</title>
<link>https://arxiv.org/abs/2509.06459</link>
<guid>https://arxiv.org/abs/2509.06459</guid>
<content:encoded><![CDATA[
arXiv:2509.06459v1 Announce Type: cross 
Abstract: Deep neural networks currently dominate many fields of the artificial intelligence landscape, achieving state-of-the-art results on numerous tasks while remaining hard to understand and exhibiting surprising weaknesses. An active area of research focuses on adversarial attacks, which aim to generate inputs that uncover these weaknesses. However, this proves challenging, especially in the black-box scenario where model details are inaccessible. This paper explores in detail the impact of such adversarial algorithms on ResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network architectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101 datasets, we benchmark two novel black-box iterative adversarial algorithms based on affine transformations and genetic algorithms: 1) Affine Transformation Attack (ATA), an iterative algorithm maximizing our attack score function using random affine transformations, and 2) Affine Genetic Attack (AGA), a genetic algorithm that involves random noise and affine transformations. We evaluate the performance of the models in the algorithm parameter variation, data augmentation, and global and targeted attack configurations. We also compare our algorithms with two black-box adversarial algorithms, Pixle and Square Attack. Our experiments yield better results on the image classification task than similar methods in the literature, achieving an accuracy improvement of up to 8.82%. We provide noteworthy insights into successful adversarial defenses and attacks at both global and targeted levels, and demonstrate adversarial robustness through algorithm parameter variation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''</title>
<link>https://arxiv.org/abs/2509.06535</link>
<guid>https://arxiv.org/abs/2509.06535</guid>
<content:encoded><![CDATA[
arXiv:2509.06535v1 Announce Type: cross 
Abstract: We investigated the reproducibility of FairCLIP, proposed by Luo et al. (2024), for improving the group fairness of CLIP (Radford et al., 2021) by minimizing image-text similarity score disparities across sensitive groups using the Sinkhorn distance. The experimental setup of Luo et al. (2024) was reproduced to primarily investigate the research findings for FairCLIP. The model description by Luo et al. (2024) was found to differ from the original implementation. Therefore, a new implementation, A-FairCLIP, is introduced to examine specific design choices. Furthermore, FairCLIP+ is proposed to extend the FairCLIP objective to include multiple attributes. Additionally, the impact of the distance minimization on FairCLIP's fairness and performance was explored. In alignment with the original authors, CLIP was found to be biased towards certain demographics when applied to zero-shot glaucoma classification using medical scans and clinical notes from the Harvard-FairVLMed dataset. However, the experimental results on two datasets do not support their claim that FairCLIP improves the performance and fairness of CLIP. Although the regularization objective reduces Sinkhorn distances, both the official implementation and the aligned implementation, A-FairCLIP, were not found to improve performance nor fairness in zero-shot glaucoma classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal-Based Malware Classification Using 1D CNNs</title>
<link>https://arxiv.org/abs/2509.06548</link>
<guid>https://arxiv.org/abs/2509.06548</guid>
<content:encoded><![CDATA[
arXiv:2509.06548v1 Announce Type: cross 
Abstract: Malware classification is a contemporary and ongoing challenge in cyber-security: modern obfuscation techniques are able to evade traditional static analysis, while dynamic analysis is too resource intensive to be deployed at a large scale. One prominent line of research addresses these limitations by converting malware binaries into 2D images by heuristically reshaping them into a 2D grid before resizing using Lanczos resampling. These images can then be classified based on their textural information using computer vision approaches. While this approach can detect obfuscated malware more effectively than static analysis, the process of converting files into 2D images results in significant information loss due to both quantisation noise, caused by rounding to integer pixel values, and the introduction of 2D dependencies which do not exist in the original data. This loss of signal limits the classification performance of the downstream model. This work addresses these weaknesses by instead resizing the files into 1D signals which avoids the need for heuristic reshaping, and additionally these signals do not suffer from quantisation noise due to being stored in a floating-point format. It is shown that existing 2D CNN architectures can be readily adapted to classify these 1D signals for improved performance. Furthermore, a bespoke 1D convolutional neural network, based on the ResNet architecture and squeeze-and-excitation layers, was developed to classify these signals and evaluated on the MalNet dataset. It was found to achieve state-of-the-art performance on binary, type, and family level classification with F1 scores of 0.874, 0.503, and 0.507, respectively, paving the way for future models to operate on the proposed signal modality.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in Panoramic Radiographs using Federated, Centralized and Local Learning</title>
<link>https://arxiv.org/abs/2509.06553</link>
<guid>https://arxiv.org/abs/2509.06553</guid>
<content:encoded><![CDATA[
arXiv:2509.06553v1 Announce Type: cross 
Abstract: Objectives: Federated learning (FL) may mitigate privacy constraints, heterogeneous data quality, and inconsistent labeling in dental diagnostic AI. We compared FL with centralized (CL) and local learning (LL) for tooth segmentation in panoramic radiographs across multiple data corruption scenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six institutions across four settings: baseline (unaltered data); label manipulation (dilated/missing annotations); image-quality manipulation (additive Gaussian noise); and exclusion of a faulty client with corrupted data. FL was implemented via the Flower AI framework. Per-client training- and validation-loss trajectories were monitored for anomaly detection and a set of metrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set. From these metrics significance results were reported through Wilcoxon signed-rank test. CL and LL served as comparators. Results: Baseline: FL achieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at 0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777). Label manipulation: FL maintained the best median Dice score at 0.94884 (ASSD: 1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD: 1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL scored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD: 1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD: 1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring reliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and outperforms LL across corruption scenarios while preserving privacy. Per-client loss trajectories provide an effective anomaly-detection mechanism and support FL as a practical, privacy-preserving approach for scalable clinical AI deployment.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness and accuracy of mean opinion scores with hard and soft outlier detection</title>
<link>https://arxiv.org/abs/2509.06554</link>
<guid>https://arxiv.org/abs/2509.06554</guid>
<content:encoded><![CDATA[
arXiv:2509.06554v1 Announce Type: cross 
Abstract: In subjective assessment of image and video quality, observers rate or compare selected stimuli. Before calculating the mean opinion scores (MOS) for these stimuli from the ratings, it is recommended to identify and deal with outliers that may have given unreliable ratings. Several methods are available for this purpose, some of which have been standardized. These methods are typically based on statistics and sometimes tested by introducing synthetic ratings from artificial outliers, such as random clickers. However, a reliable and comprehensive approach is lacking for comparative performance analysis of outlier detection methods. To fill this gap, this work proposes and applies an empirical worst-case analysis as a general solution. Our method involves evolutionary optimization of an adversarial black-box attack on outlier detection algorithms, where the adversary maximizes the distortion of scale values with respect to ground truth. We apply our analysis to several hard and soft outlier detection methods for absolute category ratings and show their differing performance in this stress test. In addition, we propose two new outlier detection methods with low complexity and excellent worst-case performance. Software for adversarial attacks and data analysis is available.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Regularization for Force Prediction in Active Particle Suspension with EGNN and Persistent Homology</title>
<link>https://arxiv.org/abs/2509.06574</link>
<guid>https://arxiv.org/abs/2509.06574</guid>
<content:encoded><![CDATA[
arXiv:2509.06574v1 Announce Type: cross 
Abstract: Capturing the dynamics of active particles, i.e., small self-propelled agents that both deform and are deformed by a fluid in which they move is a formidable problem as it requires coupling fine scale hydrodynamics with large scale collective effects. So we present a multi-scale framework that combines the three learning-driven tools to learn in concert within one pipeline. We use high-resolution Lattice Boltzmann snapshots of fluid velocity and particle stresses in a periodic box as input to the learning pipeline. the second step takes the morphology and positions orientations of particles to predict pairwise interaction forces between them with a E(2)-equivariant graph neural network that necessarily respect flat symmetries. Then, a physics-informed neural network further updates these local estimates by summing over them with a stress data using Fourier feature mappings and residual blocks that is additionally regularized with a topological term (introduced by persistent homology) to penalize unrealistically tangled or spurious connections. In concert, these stages deliver an holistic highly-data driven full force network prediction empathizing on the physical underpinnings together with emerging multi-scale structure typical for active matter.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination</title>
<link>https://arxiv.org/abs/2509.06575</link>
<guid>https://arxiv.org/abs/2509.06575</guid>
<content:encoded><![CDATA[
arXiv:2509.06575v1 Announce Type: cross 
Abstract: Representation-based multi-task learning (MTL) improves efficiency by learning a shared structure across tasks, but its practical application is often hindered by contamination, outliers, or adversarial tasks. Most existing methods and theories assume a clean or near-clean setting, failing when contamination is significant. This paper tackles representation MTL with an unknown and potentially large contamination proportion, while also allowing for heterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral method (RAS) that can distill the shared inlier representation effectively and efficiently, while requiring no prior knowledge of the contamination level or the true representation dimension. Theoretically, we provide non-asymptotic error bounds for both the learned representation and the per-task parameters. These bounds adapt to inlier task similarity and outlier structure, and guarantee that RAS performs at least as well as single-task learning, thus preventing negative transfer. We also extend our framework to transfer learning with corresponding theoretical guarantees for the target task. Extensive experiments confirm our theory, showcasing the robustness and adaptivity of RAS, and its superior performance in regimes with up to 80\% task contamination.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Hierarchical Graph Construction for Multi-source Electronic Health Records</title>
<link>https://arxiv.org/abs/2509.06576</link>
<guid>https://arxiv.org/abs/2509.06576</guid>
<content:encoded><![CDATA[
arXiv:2509.06576v1 Announce Type: cross 
Abstract: Electronic Health Records (EHRs), comprising diverse clinical data such as diagnoses, medications, and laboratory results, hold great promise for translational research. EHR-derived data have advanced disease prevention, improved clinical trial recruitment, and generated real-world evidence. Synthesizing EHRs across institutions enables large-scale, generalizable studies that capture rare diseases and population diversity, but remains hindered by the heterogeneity of medical codes, institution-specific terminologies, and the absence of standardized data structures. These barriers limit the interpretability, comparability, and scalability of EHR-based analyses, underscoring the need for robust methods to harmonize and extract meaningful insights from distributed, heterogeneous data. To address this, we propose MASH (Multi-source Automated Structured Hierarchy), a fully automated framework that aligns medical codes across institutions using neural optimal transport and constructs hierarchical graphs with learned hyperbolic embeddings. During training, MASH integrates information from pre-trained language models, co-occurrence patterns, textual descriptions, and supervised labels to capture semantic and hierarchical relationships among medical concepts more effectively. Applied to real-world EHR data, including diagnosis, medication, and laboratory codes, MASH produces interpretable hierarchical graphs that facilitate the navigation and understanding of heterogeneous clinical data. Notably, it generates the first automated hierarchies for unstructured local laboratory codes, establishing foundational references for downstream applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximating Condorcet Ordering for Vector-valued Mathematical Morphology</title>
<link>https://arxiv.org/abs/2509.06577</link>
<guid>https://arxiv.org/abs/2509.06577</guid>
<content:encoded><![CDATA[
arXiv:2509.06577v1 Announce Type: cross 
Abstract: Mathematical morphology provides a nonlinear framework for image and spatial data processing and analysis. Although there have been many successful applications of mathematical morphology to vector-valued images, such as color and hyperspectral images, there is still no consensus on the most suitable vector ordering for constructing morphological operators. This paper addresses this issue by examining a reduced ordering approximating the Condorcet ranking derived from a set of vector orderings. Inspired by voting problems, the Condorcet ordering ranks elements from most to least voted, with voters representing different orderings. In this paper, we develop a machine learning approach that learns a reduced ordering that approximates the Condorcet ordering. Preliminary computational experiments confirm the effectiveness of learning the reduced mapping to define vector-valued morphological operators for color images.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of trade in products derived from threatened species using machine learning and a smartphone</title>
<link>https://arxiv.org/abs/2509.06585</link>
<guid>https://arxiv.org/abs/2509.06585</guid>
<content:encoded><![CDATA[
arXiv:2509.06585v1 Announce Type: cross 
Abstract: Unsustainable trade in wildlife is a major threat to biodiversity and is now increasingly prevalent in digital marketplaces and social media. With the sheer volume of digital content, the need for automated methods to detect wildlife trade listings is growing. These methods are especially needed for the automatic identification of wildlife products, such as ivory. We developed machine learning-based object recognition models that can identify wildlife products within images and highlight them. The data consists of images of elephant, pangolin, and tiger products that were identified as being sold illegally or that were confiscated by authorities. Specifically, the wildlife products included elephant ivory and skins, pangolin scales, and claws (raw and crafted), and tiger skins and bones. We investigated various combinations of training strategies and two loss functions to identify the best model to use in the automatic detection of these wildlife products. Models were trained for each species while also developing a single model to identify products from all three species. The best model showed an overall accuracy of 84.2% with accuracies of 71.1%, 90.2% and 93.5% in detecting products derived from elephants, pangolins, and tigers, respectively. We further demonstrate that the machine learning model can be made easily available to stakeholders, such as government authorities and law enforcement agencies, by developing a smartphone-based application that had an overall accuracy of 91.3%. The application can be used in real time to click images and help identify potentially prohibited products of target species. Thus, the proposed method is not only applicable for monitoring trade on the web but can also be used e.g. in physical markets for monitoring wildlife trade.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Spatial and Semantic Embeddings for Stereo Sound Event Localization in Videos</title>
<link>https://arxiv.org/abs/2509.06598</link>
<guid>https://arxiv.org/abs/2509.06598</guid>
<content:encoded><![CDATA[
arXiv:2509.06598v1 Announce Type: cross 
Abstract: In this study, we address the multimodal task of stereo sound event localization and detection with source distance estimation (3D SELD) in regular video content. 3D SELD is a complex task that combines temporal event classification with spatial localization, requiring reasoning across spatial, temporal, and semantic dimensions. The last is arguably the most challenging to model. Traditional SELD approaches typically rely on multichannel input, limiting their capacity to benefit from large-scale pre-training due to data constraints. To overcome this, we enhance a standard SELD architecture with semantic information by integrating pre-trained, contrastive language-aligned models: CLAP for audio and OWL-ViT for visual inputs. These embeddings are incorporated into a modified Conformer module tailored for multimodal fusion, which we refer to as the Cross-Modal Conformer. We perform an ablation study on the development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the individual contributions of the language-aligned models and benchmark against the DCASE Task 3 baseline systems. Additionally, we detail the curation process of large synthetic audio and audio-visual datasets used for model pre-training. These datasets were further expanded through left-right channel swapping augmentation. Our approach, combining extensive pre-training, model ensembling, and visual post-processing, achieved second rank in the DCASE 2025 Challenge Task 3 (Track B), underscoring the effectiveness of our method. Future work will explore the modality-specific contributions and architectural refinements.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework</title>
<link>https://arxiv.org/abs/2509.06625</link>
<guid>https://arxiv.org/abs/2509.06625</guid>
<content:encoded><![CDATA[
arXiv:2509.06625v1 Announce Type: cross 
Abstract: Plants in their natural habitats endure an array of interacting stresses, both biotic and abiotic, that rarely occur in isolation. Nutrient stress-particularly nitrogen deficiency-becomes even more critical when compounded with drought and weed competition, making it increasingly difficult to distinguish and address its effects. Early detection of nitrogen stress is therefore crucial for protecting plant health and implementing effective management strategies. This study proposes a novel deep learning framework to accurately classify nitrogen stress severity in a combined stress environment. Our model uses a unique blend of four imaging modalities-RGB, multispectral, and two infrared wavelengths-to capture a wide range of physiological plant responses from canopy images. These images, provided as time-series data, document plant health across three levels of nitrogen availability (low, medium, and high) under varying water stress and weed pressures. The core of our approach is a spatio-temporal deep learning pipeline that merges a Convolutional Neural Network (CNN) for extracting spatial features from images with a Long Short-Term Memory (LSTM) network to capture temporal dependencies. We also devised and evaluated a spatial-only CNN pipeline for comparison. Our CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively surpassing the spatial-only model's 80.45% and other previously reported machine learning method's 76%. These results bring actionable insights based on the power of our CNN-LSTM approach in effectively capturing the subtle and complex interactions between nitrogen deficiency, water stress, and weed pressure. This robust platform offers a promising tool for the timely and proactive identification of nitrogen stress severity, enabling better crop management and improved plant health.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning</title>
<link>https://arxiv.org/abs/2509.06641</link>
<guid>https://arxiv.org/abs/2509.06641</guid>
<content:encoded><![CDATA[
arXiv:2509.06641v1 Announce Type: cross 
Abstract: Targeting the issues of "shortcuts" and insufficient contextual understanding in complex cross-modal reasoning of multimodal large models, this paper proposes a zero-shot multimodal reasoning component guided by human-like cognitive strategies centered on an "intent sketch". The component comprises a plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and Strategy Selector-that explicitly constructs a "understand-plan-select" cognitive process. By generating and filtering "intent sketch" strategies to guide the final reasoning, it requires no parameter fine-tuning and achieves cross-model transfer solely through in-context engineering. Information-theoretic analysis shows that this process can reduce conditional entropy and improve information utilization efficiency, thereby suppressing unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and Daily-Omni validate the method's generality and robust gains; compared with their respective baselines, the complete "three-module" scheme yields consistent improvements across different reasoning engines and pipeline combinations, with gains up to approximately 9.51 percentage points, demonstrating the practical value and portability of the "intent sketch" reasoning component in zero-shot scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties</title>
<link>https://arxiv.org/abs/2509.06697</link>
<guid>https://arxiv.org/abs/2509.06697</guid>
<content:encoded><![CDATA[
arXiv:2509.06697v1 Announce Type: cross 
Abstract: Accurate forecasting of exchange rates remains a persistent challenge, particularly for emerging economies such as Brazil, Russia, India, and China (BRIC). These series exhibit long memory, nonlinearity, and non-stationarity properties that conventional time series models struggle to capture. Additionally, there exist several key drivers of exchange rate dynamics, including global economic policy uncertainty, US equity market volatility, US monetary policy uncertainty, oil price growth rates, and country-specific short-term interest rate differentials. These empirical complexities underscore the need for a flexible modeling framework that can jointly accommodate long memory, nonlinearity, and the influence of external drivers. To address these challenges, we propose a Neural AutoRegressive Fractionally Integrated Moving Average (NARFIMA) model that combines the long-memory representation of ARFIMA with the nonlinear learning capacity of neural networks, while flexibly incorporating exogenous causal variables. We establish theoretical properties of the model, including asymptotic stationarity of the NARFIMA process using Markov chains and nonlinear time series techniques. We quantify forecast uncertainty using conformal prediction intervals within the NARFIMA framework. Empirical results across six forecast horizons show that NARFIMA consistently outperforms various state-of-the-art statistical and machine learning models in forecasting BRIC exchange rates. These findings provide new insights for policymakers and market participants navigating volatile financial conditions. The \texttt{narfima} \textbf{R} package provides an implementation of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Secure Isn't: Assessing the Security of Machine Learning Model Sharing</title>
<link>https://arxiv.org/abs/2509.06703</link>
<guid>https://arxiv.org/abs/2509.06703</guid>
<content:encoded><![CDATA[
arXiv:2509.06703v1 Announce Type: cross 
Abstract: The rise of model-sharing through frameworks and dedicated hubs makes Machine Learning significantly more accessible. Despite their benefits, these tools expose users to underexplored security risks, while security awareness remains limited among both practitioners and developers. To enable a more security-conscious culture in Machine Learning model sharing, in this paper we evaluate the security posture of frameworks and hubs, assess whether security-oriented mechanisms offer real protection, and survey how users perceive the security narratives surrounding model sharing. Our evaluation shows that most frameworks and hubs address security risks partially at best, often by shifting responsibility to the user. More concerningly, our analysis of frameworks advertising security-oriented settings and complete model sharing uncovered six 0-day vulnerabilities enabling arbitrary code execution. Through this analysis, we debunk the misconceptions that the model-sharing problem is largely solved and that its security can be guaranteed by the file format used for sharing. As expected, our survey shows that the surrounding security narrative leads users to consider security-oriented settings as trustworthy, despite the weaknesses shown in this work. From this, we derive takeaways and suggestions to strengthen the security of model-sharing ecosystems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dato: A Task-Based Programming Model for Dataflow Accelerators</title>
<link>https://arxiv.org/abs/2509.06794</link>
<guid>https://arxiv.org/abs/2509.06794</guid>
<content:encoded><![CDATA[
arXiv:2509.06794v1 Announce Type: cross 
Abstract: Recent deep learning workloads increasingly push computational demand beyond what current memory systems can sustain, with many kernels stalling on data movement rather than computation. While modern dataflow accelerators incorporate on-chip streaming to mitigate off-chip bandwidth limitations, existing programming models struggle to harness these capabilities effectively. Low-level interfaces provide fine-grained control but impose significant development overhead, whereas high-level tile-based languages abstract away communication details, restricting optimization and forcing compilers to reconstruct the intended dataflow. We present Dato, a Python-embedded, task-based programming model for dataflow accelerators that elevates data communication and sharding to first-class type constructs. Developers write programs as a graph of tasks connected via explicit stream types, with sharded inputs specified using layout types. These tasks are first mapped virtually onto the accelerator's spatial fabric, and the compiler then generates a physical mapping that respects hardware constraints. Experimental results on both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves high performance while significantly reducing the burden of writing optimized code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and delivers a 2.81x speedup on attention kernels compared to a state-of-the-art commercial framework. On the FPGA, Dato surpasses leading frameworks in performance when generating custom systolic arrays, achieving 98% of the theoretical peak performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitative Membership Inference Attack</title>
<link>https://arxiv.org/abs/2509.06796</link>
<guid>https://arxiv.org/abs/2509.06796</guid>
<content:encoded><![CDATA[
arXiv:2509.06796v1 Announce Type: cross 
Abstract: A Membership Inference Attack (MIA) assesses how much a target machine learning model reveals about its training data by determining whether specific query instances were part of the training set. State-of-the-art MIAs rely on training hundreds of shadow models that are independent of the target model, leading to significant computational overhead. In this paper, we introduce Imitative Membership Inference Attack (IMIA), which employs a novel imitative training technique to strategically construct a small number of target-informed imitative models that closely replicate the target model's behavior for inference. Extensive experimental results demonstrate that IMIA substantially outperforms existing MIAs in various attack settings while only requiring less than 5% of the computational cost of state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward function compression facilitates goal-dependent reinforcement learning</title>
<link>https://arxiv.org/abs/2509.06810</link>
<guid>https://arxiv.org/abs/2509.06810</guid>
<content:encoded><![CDATA[
arXiv:2509.06810v1 Announce Type: cross 
Abstract: Reinforcement learning agents learn from rewards, but humans can uniquely assign value to novel, abstract outcomes in a goal-dependent manner. However, this flexibility is cognitively costly, making learning less efficient. Here, we propose that goal-dependent learning is initially supported by a capacity-limited working memory system. With consistent experience, learners create a "compressed" reward function (a simplified rule defining the goal) which is then transferred to long-term memory and applied automatically upon receiving feedback. This process frees up working memory resources, boosting learning efficiency. We test this theory across six experiments. Consistent with our predictions, our findings demonstrate that learning is parametrically impaired by the size of the goal space, but improves when the goal space structure allows for compression. We also find faster reward processing to correlate with better learning performance, supporting the idea that as goal valuation becomes more automatic, more resources are available for learning. We leverage computational modeling to support this interpretation. Our work suggests that efficient goal-directed learning relies on compressing complex goal information into a stable reward function, shedding light on the cognitive mechanisms of human motivation. These findings generate new insights into the neuroscience of intrinsic motivation and could help improve behavioral techniques that support people in achieving their goals.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward</title>
<link>https://arxiv.org/abs/2509.06818</link>
<guid>https://arxiv.org/abs/2509.06818</guid>
<content:encoded><![CDATA[
arXiv:2509.06818v1 Announce Type: cross 
Abstract: Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With "multi-to-multi matching" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Green Learning for STAR-RIS mmWave Systems with Implicit CSI</title>
<link>https://arxiv.org/abs/2509.06820</link>
<guid>https://arxiv.org/abs/2509.06820</guid>
<content:encoded><![CDATA[
arXiv:2509.06820v1 Announce Type: cross 
Abstract: In this paper, a green learning (GL)-based precoding framework is proposed for simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems. Motivated by the growing emphasis on environmental sustainability in future 6G networks, this work adopts a broadcasting transmission architecture for scenarios where multiple users share identical information, improving spectral efficiency and reducing redundant transmissions and power consumption. Different from conventional optimization methods, such as block coordinate descent (BCD) that require perfect channel state information (CSI) and iterative computation, the proposed GL framework operates directly on received uplink pilot signals without explicit CSI estimation. Unlike deep learning (DL) approaches that require CSI-based labels for training, the proposed GL approach also avoids deep neural networks and backpropagation, leading to a more lightweight design. Although the proposed GL framework is trained with supervision generated by BCD under full CSI, inference is performed in a fully CSI-free manner. The proposed GL integrates subspace approximation with adjusted bias (Saab), relevant feature test (RFT)-based supervised feature selection, and eXtreme gradient boosting (XGBoost)-based decision learning to jointly predict the STAR-RIS coefficients and transmit precoder. Simulation results show that the proposed GL approach achieves competitive spectral efficiency compared to BCD and DL-based models, while reducing floating-point operations (FLOPs) by over four orders of magnitude. These advantages make the proposed GL approach highly suitable for real-time deployment in energy- and hardware-constrained broadcasting scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning</title>
<link>https://arxiv.org/abs/2509.06826</link>
<guid>https://arxiv.org/abs/2509.06826</guid>
<content:encoded><![CDATA[
arXiv:2509.06826v1 Announce Type: cross 
Abstract: The rapid growth of visual content consumption across platforms necessitates automated video classification for age-suitability standards like the MPAA rating system (G, PG, PG-13, R). Traditional methods struggle with large labeled data requirements, poor generalization, and inefficient feature learning. To address these challenges, we employ contrastive learning for improved discrimination and adaptability, exploring three frameworks: Instance Discrimination, Contextual Contrastive Learning, and Multi-View Contrastive Learning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with a Bahdanau attention mechanism, achieving state-of-the-art performance in the Contextual Contrastive Learning framework, with 88% accuracy and an F1 score of 0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling, and attention mechanisms for dynamic frame prioritization, the model excels in fine-grained borderline distinctions, such as differentiating PG-13 and R-rated content. We evaluate the model's performance across various contrastive loss functions, including NT-Xent, NT-logistic, and Margin Triplet, demonstrating the robustness of our proposed architecture. To ensure practical application, the model is deployed as a web application for real-time MPAA rating classification, offering an efficient solution for automated content compliance across streaming platforms.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curia: A Multi-Modal Foundation Model for Radiology</title>
<link>https://arxiv.org/abs/2509.06830</link>
<guid>https://arxiv.org/abs/2509.06830</guid>
<content:encoded><![CDATA[
arXiv:2509.06830v1 Announce Type: cross 
Abstract: AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base model's weights at https://huggingface.co/raidium/curia.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens</title>
<link>https://arxiv.org/abs/2509.06836</link>
<guid>https://arxiv.org/abs/2509.06836</guid>
<content:encoded><![CDATA[
arXiv:2509.06836v1 Announce Type: cross 
Abstract: Making LLMs more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a key technique toward this goal. However, prior pruning methods are limited: width pruning often breaks the standard transformer layout or requires custom inference code, while depth pruning removes entire layers and can cause abrupt accuracy drops. In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT enjoys merits of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN pruning), training-free operation with competitive pruning time, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream task performance at similar or higher pruning ratios, with substantial reductions in parameters, GPU memory, and end-to-end latency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToonOut: Fine-tuned Background-Removal for Anime Characters</title>
<link>https://arxiv.org/abs/2509.06839</link>
<guid>https://arxiv.org/abs/2509.06839</guid>
<content:encoded><![CDATA[
arXiv:2509.06839v1 Announce Type: cross 
Abstract: While state-of-the-art background removal models excel at realistic imagery, they frequently underperform in specialized domains such as anime-style content, where complex features like hair and transparency present unique challenges. To address this limitation, we collected and annotated a custom dataset of 1,228 high-quality anime images of characters and objects, and fine-tuned the open-sourced BiRefNet model on this dataset. This resulted in marked improvements in background removal accuracy for anime-style images, increasing from 95.3% to 99.5% for our newly introduced Pixel Accuracy metric. We are open-sourcing the code, the fine-tuned model weights, as well as the dataset at: https://github.com/MatteoKartoon/BiRefNet.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning meets bioprocess control through behaviour cloning: Real-world deployment in an industrial photobioreactor</title>
<link>https://arxiv.org/abs/2509.06853</link>
<guid>https://arxiv.org/abs/2509.06853</guid>
<content:encoded><![CDATA[
arXiv:2509.06853v1 Announce Type: cross 
Abstract: The inherent complexity of living cells as production units creates major challenges for maintaining stable and optimal bioprocess conditions, especially in open Photobioreactors (PBRs) exposed to fluctuating environments. To address this, we propose a Reinforcement Learning (RL) control approach, combined with Behavior Cloning (BC), for pH regulation in open PBR systems. This represents, to the best of our knowledge, the first application of an RL-based control strategy to such a nonlinear and disturbance-prone bioprocess. Our method begins with an offline training stage in which the RL agent learns from trajectories generated by a nominal Proportional-Integral-Derivative (PID) controller, without direct interaction with the real system. This is followed by a daily online fine-tuning phase, enabling adaptation to evolving process dynamics and stronger rejection of fast, transient disturbances. This hybrid offline-online strategy allows deployment of an adaptive control policy capable of handling the inherent nonlinearities and external perturbations in open PBRs. Simulation studies highlight the advantages of our method: the Integral of Absolute Error (IAE) was reduced by 8% compared to PID control and by 5% relative to standard off-policy RL. Moreover, control effort decreased substantially-by 54% compared to PID and 7% compared to standard RL-an important factor for minimizing operational costs. Finally, an 8-day experimental validation under varying environmental conditions confirmed the robustness and reliability of the proposed approach. Overall, this work demonstrates the potential of RL-based methods for bioprocess control and paves the way for their broader application to other nonlinear, disturbance-prone systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Least-Squares Estimators with Fast Randomized Sketching for Linear Statistical Models</title>
<link>https://arxiv.org/abs/2509.06856</link>
<guid>https://arxiv.org/abs/2509.06856</guid>
<content:encoded><![CDATA[
arXiv:2509.06856v1 Announce Type: cross 
Abstract: We propose a novel randomized framework for the estimation problem of large-scale linear statistical models, namely Sequential Least-Squares Estimators with Fast Randomized Sketching (SLSE-FRS), which integrates Sketch-and-Solve and Iterative-Sketching methods for the first time. By iteratively constructing and solving sketched least-squares (LS) subproblems with increasing sketch sizes to achieve better precisions, SLSE-FRS gradually refines the estimators of the true parameter vector, ultimately producing high-precision estimators. We analyze the convergence properties of SLSE-FRS, and provide its efficient implementation. Numerical experiments show that SLSE-FRS outperforms the state-of-the-art methods, namely the Preconditioned Conjugate Gradient (PCG) method, and the Iterative Double Sketching (IDS) method.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet</title>
<link>https://arxiv.org/abs/2509.06861</link>
<guid>https://arxiv.org/abs/2509.06861</guid>
<content:encoded><![CDATA[
arXiv:2509.06861v1 Announce Type: cross 
Abstract: Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning spatially structured open quantum dynamics with regional-attention transformers</title>
<link>https://arxiv.org/abs/2509.06871</link>
<guid>https://arxiv.org/abs/2509.06871</guid>
<content:encoded><![CDATA[
arXiv:2509.06871v1 Announce Type: cross 
Abstract: Simulating the dynamics of open quantum systems with spatial structure and external control is an important challenge in quantum information science. Classical numerical solvers for such systems require integrating coupled master and field equations, which is computationally demanding for simulation and optimization tasks and often precluding real-time use in network-scale simulations or feedback control. We introduce a regional attention-based neural architecture that learns the spatiotemporal dynamics of structured open quantum systems. The model incorporates translational invariance of physical laws as an inductive bias to achieve scalable complexity, and supports conditioning on time-dependent global control parameters. We demonstrate learning on two representative systems: a driven dissipative single qubit and an electromagnetically induced transparency (EIT) quantum memory. The model achieves high predictive fidelity under both in-distribution and out-of-distribution control protocols, and provides substantial acceleration up to three orders of magnitude over numerical solvers. These results demonstrate that the architecture establishes a general surrogate modeling framework for spatially structured open quantum dynamics, with immediate relevance to large-scale quantum network simulation, quantum repeater and protocol design, real-time experimental optimization, and scalable device modeling across diverse light-matter platforms.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mmBERT: A Modern Multilingual Encoder with Annealed Language Learning</title>
<link>https://arxiv.org/abs/2509.06888</link>
<guid>https://arxiv.org/abs/2509.06888</guid>
<content:encoded><![CDATA[
arXiv:2509.06888v1 Announce Type: cross 
Abstract: Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from one graph: transductive learning guarantees via the geometry of small random worlds</title>
<link>https://arxiv.org/abs/2509.06894</link>
<guid>https://arxiv.org/abs/2509.06894</guid>
<content:encoded><![CDATA[
arXiv:2509.06894v1 Announce Type: cross 
Abstract: Since their introduction by Kipf and Welling in $2017$, a primary use of graph convolutional networks is transductive node classification, where missing labels are inferred within a single observed graph and its feature matrix. Despite the widespread use of the network model, the statistical foundations of transductive learning remain limited, as standard inference frameworks typically rely on multiple independent samples rather than a single graph. In this work, we address these gaps by developing new concentration-of-measure tools that leverage the geometric regularities of large graphs via low-dimensional metric embeddings. The emergent regularities are captured using a random graph model; however, the methods remain applicable to deterministic graphs once observed. We establish two principal learning results. The first concerns arbitrary deterministic $k$-vertex graphs, and the second addresses random graphs that share key geometric properties with an Erd\H{o}s-R\'{e}nyi graph $\mathbf{G}=\mathbf{G}(k,p)$ in the regime $p \in \mathcal{O}((\log (k)/k)^{1/2})$. The first result serves as the basis for and illuminates the second. We then extend these results to the graph convolutional network setting, where additional challenges arise. Lastly, our learning guarantees remain informative even with a few labelled nodes $N$ and achieve the optimal nonparametric rate $\mathcal{O}(N^{-1/2})$ as $N$ grows.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification</title>
<link>https://arxiv.org/abs/2509.06902</link>
<guid>https://arxiv.org/abs/2509.06902</guid>
<content:encoded><![CDATA[
arXiv:2509.06902v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.06911</link>
<guid>https://arxiv.org/abs/2509.06911</guid>
<content:encoded><![CDATA[
arXiv:2509.06911v1 Announce Type: cross 
Abstract: We propose HyGLAD, a novel algorithm that automatically builds a set of interpretable patterns that model event data. These patterns can then be used to detect event-based anomalies in a stationary system, where any deviation from past behavior may indicate malicious activity. The algorithm infers equivalence classes of entities with similar behavior observed from the events, and then builds regular expressions that capture the values of those entities. As opposed to deep-learning approaches, the regular expressions are directly interpretable, which also translates to interpretable anomalies. We evaluate HyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five datasets from real-world systems. The experimental results show that on average HyGLAD outperforms existing deep-learning methods while being an order of magnitude more efficient in training and inference (single CPU vs GPU). Precision improved by 1.2x and recall by 1.3x compared to the second-best baseline.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents</title>
<link>https://arxiv.org/abs/2509.06917</link>
<guid>https://arxiv.org/abs/2509.06917</guid>
<content:encoded><![CDATA[
arXiv:2509.06917v1 Announce Type: cross 
Abstract: We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven solar forecasting enables near-optimal economic decisions</title>
<link>https://arxiv.org/abs/2509.06925</link>
<guid>https://arxiv.org/abs/2509.06925</guid>
<content:encoded><![CDATA[
arXiv:2509.06925v1 Announce Type: cross 
Abstract: Solar energy adoption is critical to achieving net-zero emissions. However, it remains difficult for many industrial and commercial actors to decide on whether they should adopt distributed solar-battery systems, which is largely due to the unavailability of fast, low-cost, and high-resolution irradiance forecasts. Here, we present SunCastNet, a lightweight data-driven forecasting system that provides 0.05$^\circ$, 10-minute resolution predictions of surface solar radiation downwards (SSRD) up to 7 days ahead. SunCastNet, coupled with reinforcement learning (RL) for battery scheduling, reduces operational regret by 76--93\% compared to robust decision making (RDM). In 25-year investment backtests, it enables up to five of ten high-emitting industrial sectors per region to cross the commercial viability threshold of 12\% Internal Rate of Return (IRR). These results show that high-resolution, long-horizon solar forecasts can directly translate into measurable economic gains, supporting near-optimal energy operations and accelerating renewable deployment.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference</title>
<link>https://arxiv.org/abs/2509.06942</link>
<guid>https://arxiv.org/abs/2509.06942</guid>
<content:encoded><![CDATA[
arXiv:2509.06942v1 Announce Type: cross 
Abstract: Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interleaving Reasoning for Better Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2509.06945</link>
<guid>https://arxiv.org/abs/2509.06945</guid>
<content:encoded><![CDATA[
arXiv:2509.06945v1 Announce Type: cross 
Abstract: Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</title>
<link>https://arxiv.org/abs/2509.06953</link>
<guid>https://arxiv.org/abs/2509.06953</guid>
<content:encoded><![CDATA[
arXiv:2509.06953v1 Announce Type: cross 
Abstract: Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize in complex or dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories across diverse simulation scenarios. We further improve IMPACT's static obstacle avoidance through iterative student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time using DCP-RMP, a locally reactive goal-proposal module. We evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior classical and neural methods in success rate across both simulated and real-world settings. Video results and code available at https://deep-reactive-policy.com
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</title>
<link>https://arxiv.org/abs/2509.06956</link>
<guid>https://arxiv.org/abs/2509.06956</guid>
<content:encoded><![CDATA[
arXiv:2509.06956v1 Announce Type: cross 
Abstract: Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at https://github.com/NationalGAILab/HoT.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catapult Dynamics and Phase Transitions in Quadratic Nets</title>
<link>https://arxiv.org/abs/2301.07737</link>
<guid>https://arxiv.org/abs/2301.07737</guid>
<content:encoded><![CDATA[
arXiv:2301.07737v2 Announce Type: replace 
Abstract: Neural networks trained with gradient descent can undergo non-trivial phase transitions as a function of the learning rate. In \cite{lewkowycz2020large} it was discovered that wide neural nets can exhibit a catapult phase for super-critical learning rates, where the training loss grows exponentially quickly at early times before rapidly decreasing to a small value. During this phase the top eigenvalue of the neural tangent kernel (NTK) also undergoes significant evolution. In this work, we will prove that the catapult phase exists in a large class of models, including quadratic models and two-layer, homogenous neural nets. To do this, we show that for a certain range of learning rates the weight norm decreases whenever the loss becomes large. We also empirically study learning rates beyond this theoretically derived range and show that the activation map of ReLU nets trained with super-critical learning rates becomes increasingly sparse as we increase the learning rate.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model</title>
<link>https://arxiv.org/abs/2305.16589</link>
<guid>https://arxiv.org/abs/2305.16589</guid>
<content:encoded><![CDATA[
arXiv:2305.16589v3 Announce Type: replace 
Abstract: This paper investigates model robustness in reinforcement learning (RL) to reduce the sim-to-real gap in practice. We adopt the framework of distributionally robust Markov decision processes (RMDPs), aimed at learning a policy that optimizes the worst-case performance when the deployed environment falls within a prescribed uncertainty set around the nominal MDP. Despite recent efforts, the sample complexity of RMDPs remained mostly unsettled regardless of the uncertainty set in use. It was unclear if distributional robustness bears any statistical consequences when benchmarked against standard RL. Assuming access to a generative model that draws samples based on the nominal MDP, we provide a near-optimal characterization of the sample complexity of RMDPs when the uncertainty set is specified via either the total variation (TV) distance or chi-squared divergence. The algorithm studied here is a model-based method called distributionally robust value iteration, which is shown to be near-optimal for the full range of uncertainty levels. Somewhat surprisingly, our results uncover that RMDPs are not necessarily easier or harder to learn than standard MDPs. The statistical consequence incurred by the robustness requirement depends heavily on the size and shape of the uncertainty set: in the case w.r.t.~the TV distance, the minimax sample complexity of RMDPs is always smaller than that of standard MDPs; in the case w.r.t.~the chi-squared divergence, the sample complexity of RMDPs far exceeds the standard MDP counterpart.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of Machine Learning Techniques in Imbalanced Data and Future Trends</title>
<link>https://arxiv.org/abs/2310.07917</link>
<guid>https://arxiv.org/abs/2310.07917</guid>
<content:encoded><![CDATA[
arXiv:2310.07917v2 Announce Type: replace 
Abstract: For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Shapley Value Modeling and Inference</title>
<link>https://arxiv.org/abs/2402.04211</link>
<guid>https://arxiv.org/abs/2402.04211</guid>
<content:encoded><![CDATA[
arXiv:2402.04211v2 Announce Type: replace 
Abstract: We propose probabilistic Shapley inference (PSI), a novel probabilistic framework to model and infer sufficient statistics of feature attributions in flexible predictive models, via latent random variables whose mean recovers Shapley values. PSI enables efficient, scalable inference over input-to-output attributions, and their uncertainty, via a variational objective that jointly trains a predictive (regression or classification) model and its attribution distributions. To address the challenge of marginalizing over variable-length input feature subsets in Shapley value calculation, we introduce a masking-based neural network architecture, with a modular training and inference procedure. We evaluate PSI on synthetic and real-world datasets, showing that it achieves competitive predictive performance compared to strong baselines, while learning feature attribution distributions -- centered at Shapley values -- that reveal meaningful attribution uncertainty across data modalities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series</title>
<link>https://arxiv.org/abs/2404.11269</link>
<guid>https://arxiv.org/abs/2404.11269</guid>
<content:encoded><![CDATA[
arXiv:2404.11269v4 Announce Type: replace 
Abstract: In time series anomaly detection (TSAD), the scarcity of labeled data poses a challenge to the development of accurate models. Unsupervised domain adaptation (UDA) offers a solution by leveraging labeled data from a related domain to detect anomalies in an unlabeled target domain. However, existing UDA methods assume consistent anomalous classes across domains. To address this limitation, we propose a novel Domain Adaptation Contrastive learning model for Anomaly Detection in multivariate time series (DACAD), combining UDA with contrastive learning. DACAD utilizes an anomaly injection mechanism that enhances generalization across unseen anomalous classes, improving adaptability and robustness. Additionally, our model employs supervised contrastive loss for the source domain and self-supervised contrastive triplet loss for the target domain, ensuring comprehensive feature representation learning and domain-invariant feature extraction. Finally, an effective Center-based Entropy Classifier (CEC) accurately learns normal boundaries in the source domain. Extensive evaluations on multiple real-world datasets and a synthetic dataset highlight DACAD's superior performance in transferring knowledge across domains and mitigating the challenge of limited labeled data in TSAD.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Over-Certainty Phenomenon in Modern Test-Time Adaptation Algorithms</title>
<link>https://arxiv.org/abs/2404.16168</link>
<guid>https://arxiv.org/abs/2404.16168</guid>
<content:encoded><![CDATA[
arXiv:2404.16168v4 Announce Type: replace 
Abstract: When neural networks are confronted with unfamiliar data that deviate from their training set, this signifies a domain shift. While these networks output predictions on their inputs, they typically fail to account for their level of familiarity with these novel observations. Prevailing works navigate test-time adaptation with the goal of curtailing model entropy, yet they unintentionally produce models that struggle with sub-optimal calibration-a dilemma we term the over-certainty phenomenon. This over-certainty in predictions can be particularly dangerous in the setting of domain shifts, as it may lead to misplaced trust. In this paper, we propose a solution that not only maintains accuracy but also addresses calibration by mitigating the over-certainty phenomenon. To do this, we introduce a certainty regularizer that dynamically adjusts pseudo-label confidence by accounting for both backbone entropy and logit norm. Our method achieves state-of-the-art performance in terms of Expected Calibration Error and Negative Log Likelihood, all while maintaining parity in accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a General Time Series Forecasting Model with Unified Representation and Adaptive Transfer</title>
<link>https://arxiv.org/abs/2405.17478</link>
<guid>https://arxiv.org/abs/2405.17478</guid>
<content:encoded><![CDATA[
arXiv:2405.17478v3 Announce Type: replace 
Abstract: With the growing availability of multi-domain time series data, there is an increasing demand for general forecasting models pre-trained on multi-source datasets to support diverse downstream prediction scenarios. Existing time series foundation models primarily focus on scaling up pre-training datasets and model sizes to enhance generalization performance. In this paper, we take a different approach by addressing two critical aspects of general forecasting models: (1) how to derive unified representations from heterogeneous multi-domain time series data, and (2) how to effectively capture domain-specific features to enable adaptive transfer across various downstream scenarios. To address the first aspect, we propose Decomposed Frequency Learning as the pre-training task, which leverages frequency-based masking and reconstruction to decompose coupled semantic information in time series, resulting in unified representations across domains. For the second aspect, we introduce the Time Series Register, which captures domain-specific representations during pre-training and enhances adaptive transferability to downstream tasks. Our model achieves the state-of-the-art forecasting performance on seven real-world benchmarks, demonstrating remarkable few-shot and zero-shot capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning</title>
<link>https://arxiv.org/abs/2406.06620</link>
<guid>https://arxiv.org/abs/2406.06620</guid>
<content:encoded><![CDATA[
arXiv:2406.06620v4 Announce Type: replace 
Abstract: The recent rapid advancements in language models (LMs) have garnered attention in medical time series-text multimodal learning. However, existing contrastive learning-based and prompt-based LM approaches tend to be biased, often assigning a primary role to time series modality while treating text modality as secondary. We classify these approaches under a temporal-primary paradigm, which may overlook the unique and critical task-relevant information embedded in text modality like clinical reports, thus failing to fully leverage mutual benefits and complementarity of different modalities. To fill this gap, we propose a novel textual-temporal multimodal learning paradigm that enables either modality to serve as the primary while being enhanced by the other, thereby effectively capturing modality-specific information and fostering cross-modal interaction. In specific, we design MedualTime, a language model composed of dual adapters to implement temporal-primary and textual-primary modeling simultaneously. Within each adapter, lightweight adaptation tokens are injected into the top layers of LM to encourage high-level modality fusion. The shared LM pipeline by dual adapters not only achieves adapter alignment but also enables efficient fine-tuning, reducing computational resources. Empirically, MedualTime demonstrates superior performance on medical data, achieving notable improvements of 8% accuracy and 12% F1 in supervised settings. Furthermore, MedualTime's transferability is validated by few-shot label transfer experiments from coarse-grained to fine-grained medical data. https://github.com/start2020/MedualTime
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2406.07831</link>
<guid>https://arxiv.org/abs/2406.07831</guid>
<content:encoded><![CDATA[
arXiv:2406.07831v3 Announce Type: replace 
Abstract: The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the OPT-30B model with 70% sparsity, ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a 19% improvement in zero-shot benchmark performance compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural CRNs: A Natural Implementation of Learning in Chemical Reaction Networks</title>
<link>https://arxiv.org/abs/2409.00034</link>
<guid>https://arxiv.org/abs/2409.00034</guid>
<content:encoded><![CDATA[
arXiv:2409.00034v4 Announce Type: replace 
Abstract: Molecular circuits capable of autonomous learning could unlock novel applications in fields such as bioengineering and synthetic biology. To this end, existing chemical implementations of neural computing have mainly relied on emulating discrete-layered neural architectures using steady-state computations of mass action kinetics. In contrast, we propose an alternative dynamical systems-based approach in which neural computations are modeled as the time evolution of molecular concentrations. The analog nature of our framework naturally aligns with chemical kinetics-based computation, leading to more compact circuits. We present the advantages of our framework through three key demonstrations. First, we assemble an end-to-end supervised learning pipeline using only two sequential phases, the minimum required number for supervised learning. Then, we show (through appropriate simplifications) that both linear and nonlinear modeling circuits can be implemented solely using unimolecular and bimolecular reactions, avoiding the complexities of higher-order chemistries. Finally, we demonstrate that first-order gradient approximations can be natively incorporated into the framework, enabling nonlinear models to scale linearly rather than combinatorially with input dimensionality. All the circuit constructions are validated through training and inference simulations across various regression and classification tasks. Our work presents a viable pathway toward embedding learning behaviors in synthetic biochemical systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MENSA: A Multi-Event Network for Survival Analysis with Trajectory-based Likelihood Estimation</title>
<link>https://arxiv.org/abs/2409.06525</link>
<guid>https://arxiv.org/abs/2409.06525</guid>
<content:encoded><![CDATA[
arXiv:2409.06525v4 Announce Type: replace 
Abstract: Most existing time-to-event methods focus on either single-event or competing-risk settings, leaving multi-event scenarios relatively underexplored. In many real-world applications, the same patient may experience multiple events that are non-exclusive, and sometimes semi-competing. A common workaround is to train separate single-event models, but this approach fails to exploit dependencies and shared structure across events. To address these limitations, we propose MENSA (Multi-Event Network for Survival Analysis), a deep learning model that jointly models flexible time-to-event distributions for multiple events, whether competing or co-occurring. In addition, we introduce a novel trajectory-based likelihood that captures the temporal ordering between events. Across five benchmark datasets, MENSA consistently improves prediction performance over many state-of-the-art baselines. The source code is available at https://github.com/thecml/mensa.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flash STU: Fast Spectral Transform Units</title>
<link>https://arxiv.org/abs/2409.10489</link>
<guid>https://arxiv.org/abs/2409.10489</guid>
<content:encoded><![CDATA[
arXiv:2409.10489v5 Announce Type: replace 
Abstract: Recent advances in state-space model architectures have shown great promise for efficient sequence modeling, but challenges remain in balancing computational efficiency with model expressiveness. We propose the Flash STU architecture, a hybrid model that interleaves spectral state space model layers with sliding window attention, enabling scalability to billions of parameters for language modeling while maintaining a near-linear time complexity. We evaluate the Flash STU and its variants on diverse sequence prediction tasks, including linear dynamical systems, robotics control, and language modeling. We find that, given a fixed parameter budget, the Flash STU architecture consistently outperforms the Transformer and other leading state-space models such as S4 and Mamba-2.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking GNN Expressive Power from a Distributed Computational Model Perspective</title>
<link>https://arxiv.org/abs/2410.01308</link>
<guid>https://arxiv.org/abs/2410.01308</guid>
<content:encoded><![CDATA[
arXiv:2410.01308v4 Announce Type: replace 
Abstract: The success of graph neural networks (GNNs) has motivated theoretical studies on their expressive power, often through alignments with the Weisfeiler-Lehman (WL) tests. However, such analyses typically focus on the ability of GNNs to distinguish between graph structures, rather than to compute or approximate specific function classes. The latter is more commonly studied in machine learning theory, including results such as the Turing completeness of recurrent networks and the universal approximation property of feedforward networks. We argue that using well-defined computational models, such as a modified CONGEST model with clearly specified preprocessing and postprocessing, offers a more sound framework for analyzing GNN expressiveness. Within this framework, we show that allowing unrestricted preprocessing or incorporating externally computed features, while claiming that these precomputations enhance the expressiveness, can sometimes lead to problems. We also show that the lower bound on a GNN's capacity (depth multiplied by width) to simulate one iteration of the WL test actually grows nearly linearly with graph size, indicating that the WL test is not locally computable and is misaligned with message-passing GNNs. Despite these negative results, we also present positive results that characterize the effects of virtual nodes and edges from a computational model perspective. Finally, we highlight several open problems regarding GNN expressiveness for further exploration.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sampling from Energy-based Policies using Diffusion</title>
<link>https://arxiv.org/abs/2410.01312</link>
<guid>https://arxiv.org/abs/2410.01312</guid>
<content:encoded><![CDATA[
arXiv:2410.01312v3 Announce Type: replace 
Abstract: Energy-based policies offer a flexible framework for modeling complex, multimodal behaviors in reinforcement learning (RL). In maximum entropy RL, the optimal policy is a Boltzmann distribution derived from the soft Q-function, but direct sampling from this distribution in continuous action spaces is computationally intractable. As a result, existing methods typically use simpler parametric distributions, like Gaussians, for policy representation -- limiting their ability to capture the full complexity of multimodal action distributions. In this paper, we introduce a diffusion-based approach for sampling from energy-based policies, where the negative Q-function defines the energy function. Based on this approach, we propose an actor-critic method called Diffusion Q-Sampling (DQS) that enables more expressive policy representations, allowing stable learning in diverse environments. We show that our approach enhances sample efficiency in continuous control tasks and captures multimodal behaviors, addressing key limitations of existing methods. Code is available at https://github.com/vineetjain96/Diffusion_Q_Sampling.git
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Architecture Built for Federated Learning: Addressing Data Heterogeneity through Adaptive Normalization-Free Feature Recalibration</title>
<link>https://arxiv.org/abs/2410.02006</link>
<guid>https://arxiv.org/abs/2410.02006</guid>
<content:encoded><![CDATA[
arXiv:2410.02006v2 Announce Type: replace 
Abstract: Federated learning is a decentralized collaborative training paradigm preserving stakeholders' data ownership while improving performance and generalization. However, statistical heterogeneity among client datasets degrades system performance. To address this issue, we propose Adaptive Normalization-free Feature Recalibration (ANFR), a model architecture-level approach that combines weight standardization and channel attention to combat heterogeneous data in FL. ANFR leverages weight standardization to avoid mismatched client statistics and inconsistent averaging, ensuring robustness under heterogeneity, and channel attention to produce learnable scaling factors for feature maps, suppressing inconsistencies across clients due to heterogeneity. We demonstrate that combining these techniques boosts model performance beyond their individual contributions, by improving class selectivity and channel attention weight distribution. ANFR works with any aggregation method, supports both global and personalized FL, and adds minimal overhead. Furthermore, when training with differential privacy, ANFR achieves an appealing balance between privacy and utility, enabling strong privacy guarantees without sacrificing performance. By integrating weight standardization and channel attention in the backbone model, ANFR offers a novel and versatile approach to the challenge of statistical heterogeneity. Extensive experiments show ANFR consistently outperforms established baselines across various aggregation methods, datasets, and heterogeneity conditions. Code is provided at https://github.com/siomvas/ANFR.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Load Balancing with GNN in MPTCP-Enabled Heterogeneous Networks</title>
<link>https://arxiv.org/abs/2410.17118</link>
<guid>https://arxiv.org/abs/2410.17118</guid>
<content:encoded><![CDATA[
arXiv:2410.17118v2 Announce Type: replace 
Abstract: Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks are a promising paradigm of heterogeneous network (HetNet), attributed to the complementary physical properties of optical spectra and radio frequency. However, the current development of such HetNets is mostly bottlenecked by the existing transmission control protocol (TCP), which restricts the user equipment (UE) to connecting one access point (AP) at a time. While the ongoing investigation on multipath TCP (MPTCP) can bring significant benefits, it complicates the network topology of HetNets, making the existing load balancing (LB) learning models less effective. Driven by this, we propose a graph neural network (GNN)-based model to tackle the LB problem for MPTCP-enabled HetNets, which results in a partial mesh topology. Such a topology can be modeled as a graph, with the channel state information and data rate requirement embedded as node features, while the LB solutions are deemed as edge labels. Compared to the conventional deep neural network (DNN), the proposed GNN-based model exhibits two key strengths: i) it can better interpret a complex network topology; and ii) it can handle various numbers of APs and UEs with a single trained model. Simulation results show that against the traditional optimisation method, the proposed learning model can achieve near-optimal throughput within a gap of 11.5%, while reducing the inference time by 4 orders of magnitude. In contrast to the DNN model, the new method can improve the network throughput by up to 21.7%, at a similar inference time level.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FACEGroup: Feasible and Actionable Counterfactual Explanations for Group Fairness</title>
<link>https://arxiv.org/abs/2410.22591</link>
<guid>https://arxiv.org/abs/2410.22591</guid>
<content:encoded><![CDATA[
arXiv:2410.22591v3 Announce Type: replace 
Abstract: Counterfactual explanations assess unfairness by revealing how inputs must change to achieve a desired outcome. This paper introduces the first graph-based framework for generating group counterfactual explanations to audit group fairness, a key aspect of trustworthy machine learning. Our framework, FACEGroup (Feasible and Actionable Counterfactual Explanations for Group Fairness), models real-world feasibility constraints, identifies subgroups with similar counterfactuals, and captures key trade-offs in counterfactual generation, distinguishing it from existing methods. To evaluate fairness, we introduce novel metrics for both group and subgroup level analysis that explicitly account for these trade-offs. Experiments on benchmark datasets show that FACEGroup effectively generates feasible group counterfactuals while accounting for trade-offs, and that our metrics capture and quantify fairness disparities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives</title>
<link>https://arxiv.org/abs/2411.19787</link>
<guid>https://arxiv.org/abs/2411.19787</guid>
<content:encoded><![CDATA[
arXiv:2411.19787v2 Announce Type: replace 
Abstract: Grounding the instruction in the environment is a key step in solving language-guided goal-reaching reinforcement learning problems. In automated reinforcement learning, a key concern is to enhance the model's ability to generalize across various tasks and environments. In goal-reaching scenarios, the agent must comprehend the different parts of the instructions within the environmental context in order to complete the overall task successfully. In this work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as a new framework to solve this problem using auxiliary loss functions inspired by video-text retrieval literature and a novel method called instruction tracking, which automatically keeps track of progress in an environment. The results of our experiments suggest superior sample efficiency and systematic generalization for this framework in multi-modal reinforcement learning problems. Our code base is available here.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Off-Policy Maximum Entropy RL with Future State and Action Visitation Measures</title>
<link>https://arxiv.org/abs/2412.06655</link>
<guid>https://arxiv.org/abs/2412.06655</guid>
<content:encoded><![CDATA[
arXiv:2412.06655v2 Announce Type: replace 
Abstract: Maximum entropy reinforcement learning integrates exploration into policy learning by providing additional intrinsic rewards proportional to the entropy of some distribution. In this paper, we propose a novel approach in which the intrinsic reward function is the relative entropy of the discounted distribution of states and actions (or features derived from these states and actions) visited during future time steps. This approach is motivated by three results. First, this new objective is a lower bound on the negated entropy of the marginal visitation distribution of states and actions, commonly used as an alternative exploration objective. Second, a policy maximizing the expected discounted sum of intrinsic rewards also maximizes a lower bound on the state-action value function of the decision process. Third, the distribution used in the intrinsic reward definition is the fixed point of a contraction operator. Existing algorithms can therefore be adapted to learn this fixed point off-policy and compute the intrinsic rewards. We finally introduce an algorithm maximizing our new objective and show that resulting policies have good state-action space coverage and achieve high-performance control.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Port-Hamiltonian Differential Algebraic Equations for Compositional Learning of Electrical Networks</title>
<link>https://arxiv.org/abs/2412.11215</link>
<guid>https://arxiv.org/abs/2412.11215</guid>
<content:encoded><![CDATA[
arXiv:2412.11215v3 Announce Type: replace 
Abstract: We develop compositional learning algorithms for coupled dynamical systems, with a particular focus on electrical networks. While deep learning has proven effective at modeling complex relationships from data, compositional couplings between system components typically introduce algebraic constraints on state variables, posing challenges to many existing data-driven approaches to modeling dynamical systems. Towards developing deep learning models for constrained dynamical systems, we introduce neural port-Hamiltonian differential algebraic equations (N-PHDAEs), which use neural networks to parameterize unknown terms in both the differential and algebraic components of a port-Hamiltonian DAE. To train these models, we propose an algorithm that uses automatic differentiation to perform index reduction, automatically transforming the neural DAE into an equivalent system of neural ordinary differential equations (N-ODEs), for which established model inference and backpropagation methods exist. Experiments simulating the dynamics of nonlinear circuits exemplify the benefits of our approach: the proposed N-PHDAE model achieves an order of magnitude improvement in prediction accuracy and constraint satisfaction when compared to a baseline N-ODE over long prediction time horizons. We also validate the compositional capabilities of our approach through experiments on a simulated DC microgrid: we train individual N-PHDAE models for separate grid components, before coupling them to accurately predict the behavior of larger-scale networks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving $\widetilde{\mathcal{O}}(\sqrt{T})$ Regret in Average-Reward POMDPs with Known Observation Models</title>
<link>https://arxiv.org/abs/2501.18790</link>
<guid>https://arxiv.org/abs/2501.18790</guid>
<content:encoded><![CDATA[
arXiv:2501.18790v2 Announce Type: replace 
Abstract: We tackle average-reward infinite-horizon POMDPs with an unknown transition model but a known observation model, a setting that has been previously addressed in two limiting ways: (i) frequentist methods relying on suboptimal stochastic policies having a minimum probability of choosing each action, and (ii) Bayesian approaches employing the optimal policy class but requiring strong assumptions about the consistency of employed estimators. Our work removes these limitations by proving convenient estimation guarantees for the transition model and introducing an optimistic algorithm that leverages the optimal class of deterministic belief-based policies. We introduce modifications to existing estimation techniques providing theoretical guarantees separately for each estimated action transition matrix. Unlike existing estimation methods that are unable to use samples from different policies, we present a novel and simple estimator that overcomes this barrier. This new data-efficient technique, combined with the proposed \emph{Action-wise OAS-UCRL} algorithm and a tighter theoretical analysis, leads to the first approach enjoying a regret guarantee of order $\mathcal{O}(\sqrt{T \,\log T})$ when compared against the optimal policy, thus improving over state of the art techniques. Finally, theoretical results are validated through numerical simulations showing the efficacy of our method against baseline methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theoretical Justification for Asymmetric Actor-Critic Algorithms</title>
<link>https://arxiv.org/abs/2501.19116</link>
<guid>https://arxiv.org/abs/2501.19116</guid>
<content:encoded><![CDATA[
arXiv:2501.19116v3 Announce Type: replace 
Abstract: In reinforcement learning for partially observable environments, many successful algorithms have been developed within the asymmetric learning paradigm. This paradigm leverages additional state information available at training time for faster learning. Although the proposed learning objectives are usually theoretically sound, these methods still lack a precise theoretical justification for their potential benefits. We propose such a justification for asymmetric actor-critic algorithms with linear function approximators by adapting a finite-time convergence analysis to this setting. The resulting finite-time bound reveals that the asymmetric critic eliminates error terms arising from aliasing in the agent state.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Steady-State Behavior in Complex Networks with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2502.01693</link>
<guid>https://arxiv.org/abs/2502.01693</guid>
<content:encoded><![CDATA[
arXiv:2502.01693v3 Announce Type: replace 
Abstract: In complex systems, information propagation can be defined as diffused or delocalized, weakly localized, and strongly localized. This study investigates the application of graph neural network models to learn the behavior of a linear dynamical system on networks. A graph convolution and attention-based neural network framework has been developed to identify the steady-state behavior of the linear dynamical system. We reveal that our trained model distinguishes the different states with high accuracy. Furthermore, we have evaluated model performance with real-world data. In addition, to understand the explainability of our model, we provide an analytical derivation for the forward and backward propagation of our framework.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Fourier Networks</title>
<link>https://arxiv.org/abs/2502.06018</link>
<guid>https://arxiv.org/abs/2502.06018</guid>
<content:encoded><![CDATA[
arXiv:2502.06018v2 Announce Type: replace 
Abstract: Although Kolmogorov-Arnold based interpretable networks (KAN) have strong theoretical expressiveness, they face significant parameter explosion and high-frequency feature capture challenges in high-dimensional tasks. To address this issue, we propose the Kolmogorov-Arnold-Fourier Network (KAF), which effectively integrates trainable Random Fourier Features (RFF) and a novel hybrid GELU-Fourier activation mechanism to balance parameter efficiency and spectral representation capabilities. Our key technical contributions include: (1) merging KAN's dual-matrix structure through matrix association properties to substantially reduce parameters; (2) introducing learnable RFF initialization strategies to eliminate spectral distortion in high-dimensional approximation tasks; (3) implementing an adaptive hybrid activation function that progressively enhances frequency representation during the training process. Comprehensive experiments demonstrate the superiority of our KAF across various domains including vision, NLP, audio processing, and differential equation-solving tasks, effectively combining theoretical interpretability with practical utility and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-based generative models as iterative algorithms in probability space</title>
<link>https://arxiv.org/abs/2502.13394</link>
<guid>https://arxiv.org/abs/2502.13394</guid>
<content:encoded><![CDATA[
arXiv:2502.13394v2 Announce Type: replace 
Abstract: Generative AI (GenAI) has revolutionized data-driven modeling by enabling the synthesis of high-dimensional data across various applications, including image generation, language modeling, biomedical signal processing, and anomaly detection. Flow-based generative models provide a powerful framework for capturing complex probability distributions, offering exact likelihood estimation, efficient sampling, and deterministic transformations between distributions. These models leverage invertible mappings governed by Ordinary Differential Equations (ODEs), enabling precise density estimation and likelihood evaluation. This tutorial presents an intuitive mathematical framework for flow-based generative models, formulating them as neural network-based representations of continuous probability densities. We explore key theoretical principles, including the Wasserstein metric, gradient flows, and density evolution governed by ODEs, to establish convergence guarantees and bridge empirical advancements with theoretical insights. By providing a rigorous yet accessible treatment, we aim to equip researchers and practitioners with the necessary tools to effectively apply flow-based generative models in signal processing and machine learning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of the Primacy Effect in Structured State-Space Models</title>
<link>https://arxiv.org/abs/2502.13729</link>
<guid>https://arxiv.org/abs/2502.13729</guid>
<content:encoded><![CDATA[
arXiv:2502.13729v5 Announce Type: replace 
Abstract: Structured state-space models (SSMs) have been developed to offer more persistent memory retention than traditional recurrent neural networks, while maintaining real-time inference capabilities and addressing the time-complexity limitations of Transformers. Despite this intended persistence, the memory mechanism of canonical SSMs is theoretically designed to decay monotonically over time, meaning that more recent inputs are expected to be retained more accurately than earlier ones. Contrary to this theoretical expectation, however, the present study reveals a counterintuitive finding: when trained and evaluated on a synthetic, statistically balanced memorization task, SSMs predominantly preserve the *initially* presented data in memory. This pattern of memory bias, known as the *primacy effect* in psychology, presents a non-trivial challenge to the current theoretical understanding of SSMs and opens new avenues for future research.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative analysis of rank aggregation methods for the partial label ranking problem</title>
<link>https://arxiv.org/abs/2502.17077</link>
<guid>https://arxiv.org/abs/2502.17077</guid>
<content:encoded><![CDATA[
arXiv:2502.17077v4 Announce Type: replace 
Abstract: The label ranking problem is a supervised learning scenario in which the learner predicts a total order of the class labels for a given input instance. Recently, research has increasingly focused on the partial label ranking problem, a generalization of the label ranking problem that allows ties in the predicted orders. So far, most existing learning approaches for the partial label ranking problem rely on approximation algorithms for rank aggregation in the final prediction step. This paper explores several alternative aggregation methods for this critical step, including scoring-based and non-parametric probabilistic-based rank aggregation approaches. To enhance their suitability for the more general partial label ranking problem, the investigated methods are extended to increase the likelihood of producing ties. Experimental evaluations on standard benchmarks demonstrate that scoring-based variants consistently outperform the current state-of-the-art method in handling incomplete information. In contrast, non-parametric probabilistic-based variants fail to achieve competitive performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIPER: Visual Perception and Explainable Reasoning for Sequential Decision-Making</title>
<link>https://arxiv.org/abs/2503.15108</link>
<guid>https://arxiv.org/abs/2503.15108</guid>
<content:encoded><![CDATA[
arXiv:2503.15108v2 Announce Type: replace 
Abstract: While Large Language Models (LLMs) excel at reasoning on text and Vision-Language Models (VLMs) are highly effective for visual perception, applying those models for visual instruction-based planning remains a widely open problem. In this paper, we introduce VIPER, a novel framework for multimodal instruction-based planning that integrates VLM-based perception with LLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM generates textual descriptions of image observations, which are then processed by an LLM policy to predict actions based on the task goal. We fine-tune the reasoning module using behavioral cloning and reinforcement learning, improving our agent's decision-making capabilities. Experiments on the ALFWorld benchmark show that VIPER significantly outperforms state-of-the-art visual instruction-based planners while narrowing the gap with purely text-based oracles. By leveraging text as an intermediate representation, VIPER also enhances explainability, paving the way for a fine-grained analysis of perception and reasoning components.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detection on a Budget: Efficient Bayesian Estimation of Semantic Entropy</title>
<link>https://arxiv.org/abs/2504.03579</link>
<guid>https://arxiv.org/abs/2504.03579</guid>
<content:encoded><![CDATA[
arXiv:2504.03579v2 Announce Type: replace 
Abstract: Detecting whether an LLM hallucinates is an important research challenge. One promising way of doing so is to estimate the semantic entropy (Farquhar et al., 2024) of the distribution of generated sequences. We propose a new algorithm for doing that, with two main advantages. First, due to us taking the Bayesian approach, we achieve a much better quality of semantic entropy estimates for a given budget of samples from the LLM. Second, we are able to tune the number of samples adaptively so that `harder' contexts receive more samples. We demonstrate empirically that our approach systematically beats the baselines, requiring only 53% of samples used by Farquhar et al. (2024) to achieve the same quality of hallucination detection as measured by AUROC. Moreover, quite counterintuitively, our estimator is useful even with just one sample from the LLM.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models</title>
<link>https://arxiv.org/abs/2504.10449</link>
<guid>https://arxiv.org/abs/2504.10449</guid>
<content:encoded><![CDATA[
arXiv:2504.10449v2 Announce Type: replace 
Abstract: Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An All-Atom Generative Model for Designing Protein Complexes</title>
<link>https://arxiv.org/abs/2504.13075</link>
<guid>https://arxiv.org/abs/2504.13075</guid>
<content:encoded><![CDATA[
arXiv:2504.13075v3 Announce Type: replace 
Abstract: Proteins typically exist in complexes, interacting with other proteins or biomolecules to perform their specific biological roles. Research on single-chain protein modeling has been extensively and deeply explored, with advancements seen in models like the series of ESM and AlphaFold2. Despite these developments, the study and modeling of multi-chain proteins remain largely uncharted, though they are vital for understanding biological functions. Recognizing the importance of these interactions, we introduce APM (All-Atom Protein Generative Model), a model specifically designed for modeling multi-chain proteins. By integrating atom-level information and leveraging data on multi-chain proteins, APM is capable of precisely modeling inter-chain interactions and designing protein complexes with binding capabilities from scratch. It also performs folding and inverse-folding tasks for multi-chain proteins. Moreover, APM demonstrates versatility in downstream applications: it achieves enhanced performance through supervised fine-tuning (SFT) while also supporting zero-shot sampling in certain tasks, achieving state-of-the-art results. We released our code at https://github.com/bytedance/apm.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-context Ranking Preference Optimization</title>
<link>https://arxiv.org/abs/2504.15477</link>
<guid>https://arxiv.org/abs/2504.15477</guid>
<content:encoded><![CDATA[
arXiv:2504.15477v2 Announce Type: replace 
Abstract: Recent developments in Direct Preference Optimization (DPO) allow large language models (LLMs) to function as implicit ranking models by maximizing the margin between preferred and non-preferred responses. In practice, user feedback on such lists typically involves identifying a few relevant items in context rather than providing detailed pairwise comparisons for every possible item pair. Moreover, many complex information retrieval tasks, such as conversational agents and summarization systems, critically depend on ranking the highest-quality outputs at the top, emphasizing the need to support natural and flexible forms of user feedback. To address the challenge of limited and sparse pairwise feedback in the in-context setting, we propose an In-context Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs based on ranking lists constructed during inference. To further capture flexible forms of feedback, IRPO extends the DPO objective by incorporating both the relevance of items and their positions in the list. Modeling these aspects jointly is non-trivial, as ranking metrics are inherently discrete and non-differentiable, making direct optimization difficult. To overcome this, IRPO introduces a differentiable objective based on positional aggregation of pairwise item preferences, enabling effective gradient-based optimization of discrete ranking metrics. We further provide theoretical insights showing that IRPO (i) automatically emphasizes items with greater disagreement between the model and the reference ranking, and (ii) links its gradient to an importance sampling estimator, yielding an unbiased estimator with reduced variance. Empirical results show IRPO outperforms standard DPO approaches in ranking performance, highlighting its effectiveness in aligning LLMs with direct in-context ranking preferences.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization</title>
<link>https://arxiv.org/abs/2504.18026</link>
<guid>https://arxiv.org/abs/2504.18026</guid>
<content:encoded><![CDATA[
arXiv:2504.18026v4 Announce Type: replace 
Abstract: Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI systems by constraining their decisions on a set of human-understandable concepts. However, CBMs typically assume that datasets contain accurate concept labels-an assumption often violated in practice, which we show can significantly degrade performance (by 25% in some cases). To address this, we introduce the Concept Preference Optimization (CPO) objective, a new loss function based on Direct Preference Optimization, which effectively mitigates the negative impact of concept mislabeling on CBM performance. We provide an analysis of key properties of the CPO objective, showing it directly optimizes for the concept's posterior distribution, and contrast it against Binary Cross Entropy (BCE), demonstrating that CPO is inherently less sensitive to concept noise. We empirically confirm our analysis by finding that CPO consistently outperforms BCE on three real-world datasets, both with and without added label noise. We make our code available on Github.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2505.08299</link>
<guid>https://arxiv.org/abs/2505.08299</guid>
<content:encoded><![CDATA[
arXiv:2505.08299v2 Announce Type: replace 
Abstract: State-space models (SSMs), particularly the Mamba architecture, have emerged as powerful alternatives to Transformers for sequence modeling, offering linear-time complexity and competitive performance across diverse tasks. However, their large parameter counts pose significant challenges for deployment in resource-constrained environments. We propose a novel unstructured pruning framework tailored for Mamba models that achieves up to 70\% parameter reduction while retaining over 95\% of the original performance. Our approach integrates three key innovations: (1) a gradient-aware magnitude pruning technique that combines weight magnitude and gradient information to identify less critical parameters, (2) an iterative pruning schedule that gradually increases sparsity to maintain model stability, and (3) a global pruning strategy that optimizes parameter allocation across the entire model. Through extensive experiments on WikiText-103, Long Range Arena, and ETT time-series benchmarks, we demonstrate significant efficiency gains with minimal performance degradation. Our analysis of pruning effects on Mamba's components reveals critical insights into the architecture's redundancy and robustness, enabling practical deployment in resource-constrained settings while broadening Mamba's applicability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model</title>
<link>https://arxiv.org/abs/2505.10438</link>
<guid>https://arxiv.org/abs/2505.10438</guid>
<content:encoded><![CDATA[
arXiv:2505.10438v3 Announce Type: replace 
Abstract: Gas turbine engines are complex and highly nonlinear dynamical systems. Deriving their physics-based models can be challenging because it requires performance characteristics that are not always available, often leading to many simplifying assumptions. This paper discusses the limitations of conventional experimental methods used to derive component-level and locally linear parameter-varying models, and addresses these issues by employing identification techniques based on data collected from standard engine operation under closed-loop control. The rotor dynamics are estimated using the sparse identification of nonlinear dynamics. Subsequently, the autonomous part of the dynamics is mapped into an optimally constructed Koopman eigenfunction space. This process involves eigenvalue optimization using metaheuristic algorithms and temporal projection, followed by gradient-based eigenfunction identification. The resulting Koopman model is validated against an in-house reference component-level model. A globally optimal nonlinear feedback controller and a Kalman estimator are then designed within the eigenfunction space and compared to traditional and gain-scheduled proportional-integral controllers, as well as a proposed internal model control approach. The eigenmode structure enables targeting individual modes during optimization, leading to improved performance tuning. Results demonstrate that the Koopman-based controller surpasses other benchmark controllers in both reference tracking and disturbance rejection under sea-level and varying flight conditions, due to its global nature.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimum Description Length Approach to Regularization in Neural Networks</title>
<link>https://arxiv.org/abs/2505.13398</link>
<guid>https://arxiv.org/abs/2505.13398</guid>
<content:encoded><![CDATA[
arXiv:2505.13398v2 Announce Type: replace 
Abstract: State-of-the-art neural networks can be trained to become remarkable solutions to many problems. But while these architectures can express symbolic, perfect solutions, trained models often arrive at approximations instead. We show that the choice of regularization method plays a crucial role: when trained on formal languages with standard regularization ($L_1$, $L_2$, or none), expressive architectures not only fail to converge to correct solutions but are actively pushed away from perfect initializations. In contrast, applying the Minimum Description Length (MDL) principle to balance model complexity with data fit provides a theoretically grounded regularization method. Using MDL, perfect solutions are selected over approximations, independently of the optimization algorithm. We propose that unlike existing regularization techniques, MDL introduces the appropriate inductive bias to effectively counteract overfitting and promote generalization.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification for Health Management with Spatial-Temporal Hypergraph Enhanced Meta-Learning</title>
<link>https://arxiv.org/abs/2505.17142</link>
<guid>https://arxiv.org/abs/2505.17142</guid>
<content:encoded><![CDATA[
arXiv:2505.17142v2 Announce Type: replace 
Abstract: Accurate classification of sleep stages based on bio-signals is fundamental not only for automatic sleep stage annotation, but also for clinical health management and continuous sleep monitoring. Traditionally, this task relies on experienced clinicians to manually annotate data, a process that is both time-consuming and labor-intensive. In recent years, deep learning methods have shown promise in automating this task. However, three major challenges remain: (1) deep learning models typically require large-scale labeled datasets, making them less effective in real-world settings where annotated data is limited; (2) significant inter-individual variability in bio-signals often results in inconsistent model performance when applied to new subjects, limiting generalization; and (3) existing approaches often overlook the high-order relationships among bio-signals, failing to simultaneously capture signal heterogeneity and spatial-temporal dependencies. To address these issues, we propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid adaptation to new subjects using only a few labeled samples, while the hypergraph structure effectively models complex spatial interconnections and temporal dynamics simultaneously in EEG signals. Experimental results demonstrate that MetaSTH-Sleep achieves substantial performance improvements across diverse subjects, offering valuable insights to support clinicians in sleep stage annotation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LLM Reasoning Through Bias-Only Adaptation</title>
<link>https://arxiv.org/abs/2505.18706</link>
<guid>https://arxiv.org/abs/2505.18706</guid>
<content:encoded><![CDATA[
arXiv:2505.18706v2 Announce Type: replace 
Abstract: We show that training a single $d$-dimensional steering vector per layer with reinforcement learning, while freezing all base weights, matches the accuracy of fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8 billion-parameter model this adds only $\approx 0.0016\%$ additional parameters and reproduces performance across a range of base models and mathematical-reasoning benchmarks. These results tighten the upper bound on the parameter budget required for high-level chain-of-thought reasoning, indicating that millions of adapter weights are unnecessary. The minimal trainable footprint reduces optimizer memory and inter-GPU communication, lowering the overall cost of fine-tuning. Moreover, a logit-lens analysis shows that the learned vectors amplify coherent token directions, providing clearer insight into the model's internal computations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control</title>
<link>https://arxiv.org/abs/2505.23355</link>
<guid>https://arxiv.org/abs/2505.23355</guid>
<content:encoded><![CDATA[
arXiv:2505.23355v3 Announce Type: replace 
Abstract: Climate control is crucial for greenhouse production as it directly affects crop growth and resource use. Reinforcement learning (RL) has received increasing attention in this field, but still faces challenges, including limited training efficiency and high reliance on initial learning conditions. Interactive RL, which combines human (grower) input with the RL agent's learning, offers a potential solution to overcome these challenges. However, interactive RL has not yet been applied to greenhouse climate control and may face challenges related to imperfect inputs. Therefore, this paper aims to explore the possibility and performance of applying interactive RL with imperfect inputs into greenhouse climate control, by: (1) developing three representative interactive RL algorithms tailored for greenhouse climate control (reward shaping, policy shaping and control sharing); (2) analyzing how input characteristics are often contradicting, and how the trade-offs between them make grower's inputs difficult to perfect; (3) proposing a neural network-based approach to enhance the robustness of interactive RL agents under limited input availability; (4) conducting a comprehensive evaluation of the three interactive RL algorithms with imperfect inputs in a simulated greenhouse environment. The demonstration shows that interactive RL incorporating imperfect grower inputs has the potential to improve the performance of the RL agent. RL algorithms that influence action selection, such as policy shaping and control sharing, perform better when dealing with imperfect inputs, achieving 8.4% and 6.8% improvement in profit, respectively. In contrast, reward shaping, an algorithm that manipulates the reward function, is sensitive to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the importance of selecting an appropriate mechanism when incorporating imperfect inputs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets Credit or Blame? Attributing Accountability in Modern AI Systems</title>
<link>https://arxiv.org/abs/2506.00175</link>
<guid>https://arxiv.org/abs/2506.00175</guid>
<content:encoded><![CDATA[
arXiv:2506.00175v3 Announce Type: replace 
Abstract: Modern AI systems are typically developed through multiple stages-pretraining, fine-tuning rounds, and subsequent adaptation or alignment, where each stage builds on the previous ones and updates the model in distinct ways. This raises a critical question of accountability: when a deployed model succeeds or fails, which stage is responsible, and to what extent? We pose the accountability attribution problem for tracing model behavior back to specific stages of the model development process. To address this challenge, we propose a general framework that answers counterfactual questions about stage effects: how would the model's behavior have changed if the updates from a particular stage had not occurred? Within this framework, we introduce estimators that efficiently quantify stage effects without retraining the model, accounting for both the data and key aspects of model optimization dynamics, including learning rate schedules, momentum, and weight decay. We demonstrate that our approach successfully quantifies the accountability of each stage to the model's behavior. Based on the attribution results, our method can identify and remove spurious correlations learned during image classification and text toxicity detection tasks that were developed across multiple stages. Our approach provides a practical tool for model analysis and represents a significant step toward more accountable AI development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkEval: Practical Evaluation of Knowledge Leakage in LLM Editing using Thought-based Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.01386</link>
<guid>https://arxiv.org/abs/2506.01386</guid>
<content:encoded><![CDATA[
arXiv:2506.01386v2 Announce Type: replace 
Abstract: Robust model-editing techniques are essential for deploying large language models (LLMs) in practical applications, to enable cost-effective ways to deal with challenges such as privacy breaches, bias mitigation and misinformation spread. For example, an LLM-based healthcare assistance may need to update out-dated or incorrect knowledge to prevent harmful recommendations. However, many editing techniques focus on isolated facts, which critically fail to prevent indirect knowledge leakage -- the unintended reconstruction of edited-out information through persistent causal links and contextual relationships. To assist users in selecting the right editing technique, we develop and present ThinkEval, a framework to systematically quantify indirect knowledge leakage and ripple effects in model-editing. ThinkEval builds and employs specialized knowledge graphs to analyze the causal structure of facts before and after editing. To support this approach, we present KnowGIC, a benchmark dataset comprising multi-step reasoning paths that precisely measure these complex knowledge transformation effects. We evaluate five editing techniques: AlphaEdit, RECT, ROME, MEMIT, and PRUNE across multiple LLMs. Our results show that these techniques struggle to balance indirect fact suppression with the preservation of related knowledge, compromising the contextual integrity of a model's knowledge. Our dataset is available at: https://anonymous.4open.science/r/KnowGIC.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Scaling of Diffusion Models via Noise Trajectory Search</title>
<link>https://arxiv.org/abs/2506.03164</link>
<guid>https://arxiv.org/abs/2506.03164</guid>
<content:encoded><![CDATA[
arXiv:2506.03164v2 Announce Type: replace 
Abstract: The iterative and stochastic nature of diffusion models enables test-time scaling, whereby spending additional compute during denoising generates higher-fidelity samples. Increasing the number of denoising steps is the primary scaling axis, but this yields quickly diminishing returns. Instead optimizing the noise trajectory--the sequence of injected noise vectors--is promising, as the specific noise realizations critically affect sample quality; but this is challenging due to a high-dimensional search space, complex noise-outcome interactions, and costly trajectory evaluations. We address this by first casting diffusion as a Markov Decision Process (MDP) with a terminal reward, showing tree-search methods such as Monte Carlo tree search (MCTS) to be meaningful but impractical. To balance performance and efficiency, we then resort to a relaxation of MDP, where we view denoising as a sequence of independent contextual bandits. This allows us to introduce an $\epsilon$-greedy search algorithm that globally explores at extreme timesteps and locally exploits during the intermediate steps where de-mixing occurs. Experiments on EDM and Stable Diffusion reveal state-of-the-art scores for class-conditioned/text-to-image generation, exceeding baselines by up to $164\%$ and matching/exceeding MCTS performance. To our knowledge, this is the first practical method for test-time noise trajectory optimization of arbitrary (non-differentiable) rewards.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pruning Spurious Subgraphs for Graph Out-of-Distribution Generalization</title>
<link>https://arxiv.org/abs/2506.05957</link>
<guid>https://arxiv.org/abs/2506.05957</guid>
<content:encoded><![CDATA[
arXiv:2506.05957v4 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) often encounter significant performance degradation under distribution shifts between training and test data, hindering their applicability in real-world scenarios. Recent studies have proposed various methods to address the out-of-distribution generalization challenge, with many methods in the graph domain focusing on directly identifying an invariant subgraph that is predictive of the target label. However, we argue that identifying the edges from the invariant subgraph directly is challenging and error-prone, especially when some spurious edges exhibit strong correlations with the targets. In this paper, we propose PrunE, the first pruning-based graph OOD method that eliminates spurious edges to improve OOD generalizability. By pruning spurious edges, PrunE retains the invariant subgraph more comprehensively, which is critical for OOD generalization. Specifically, PrunE employs two regularization terms to prune spurious edges: 1) graph size constraint to exclude uninformative spurious edges, and 2) $\epsilon$-probability alignment to further suppress the occurrence of spurious edges. Through theoretical analysis and extensive experiments, we show that PrunE achieves superior OOD performance and outperforms previous state-of-the-art methods significantly. Codes are available at: \href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07040</link>
<guid>https://arxiv.org/abs/2506.07040</guid>
<content:encoded><![CDATA[
arXiv:2506.07040v2 Announce Type: replace 
Abstract: We present a non-asymptotic convergence analysis of $Q$-learning and actor-critic algorithms for robust average-reward Markov Decision Processes (MDPs) under contamination, total-variation (TV) distance, and Wasserstein uncertainty sets. A key ingredient of our analysis is showing that the optimal robust $Q$ operator is a strict contraction with respect to a carefully designed semi-norm (with constant functions quotiented out). This property enables a stochastic approximation update that learns the optimal robust $Q$-function using $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We also provide an efficient routine for robust $Q$-function estimation, which in turn facilitates robust critic estimation. Building on this, we introduce an actor-critic algorithm that learns an $\epsilon$-optimal robust policy within $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We provide numerical simulations to evaluate the performance of our algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws of Motion Forecasting and Planning - Technical Report</title>
<link>https://arxiv.org/abs/2506.08228</link>
<guid>https://arxiv.org/abs/2506.08228</guid>
<content:encoded><![CDATA[
arXiv:2506.08228v2 Announce Type: replace 
Abstract: We study the empirical scaling laws of a family of encoder-decoder autoregressive transformer models on the task of joint motion forecasting and planning in the autonomous driving domain. Using a 500 thousand hours driving dataset, we demonstrate that, similar to language modeling, model performance improves as a power-law function of the total compute budget, and we observe a strong correlation between model training loss and model evaluation metrics. Most interestingly, closed-loop metrics also improve with scaling, which has important implications for the suitability of open-loop metrics for model development and hill climbing. We also study the optimal scaling of the number of transformer parameters and the training data size for a training compute-optimal model. We find that as the training compute budget grows, optimal scaling requires increasing the model size 1.5x as fast as the dataset size. We also study inference-time compute scaling, where we observe that sampling and clustering the output of smaller models makes them competitive with larger models, up to a crossover point beyond which a larger models becomes more inference-compute efficient. Overall, our experimental results demonstrate that optimizing the training and inference-time scaling properties of motion forecasting and planning models is a key lever for improving their performance to address a wide variety of driving scenarios. Finally, we briefly study the utility of training on general logged driving data of other agents to improve the performance of the ego-agent, an important research area to address the scarcity of robotics data for large capacity models training.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Gravity-informed Spatiotemporal Transformer for Human Activity Intensity Prediction</title>
<link>https://arxiv.org/abs/2506.13678</link>
<guid>https://arxiv.org/abs/2506.13678</guid>
<content:encoded><![CDATA[
arXiv:2506.13678v3 Announce Type: replace 
Abstract: Human activity intensity prediction is crucial to many location-based services. Despite tremendous progress in modeling dynamics of human activity, most existing methods overlook physical constraints of spatial interaction, leading to uninterpretable spatial correlations and over-smoothing phenomenon. To address these limitations, this work proposes a physics-informed deep learning framework, namely Gravity-informed Spatiotemporal Transformer (Gravityformer) by integrating the universal law of gravitation to refine transformer attention. Specifically, it (1) estimates two spatially explicit mass parameters based on spatiotemporal embedding feature, (2) models the spatial interaction in end-to-end neural network using proposed adaptive gravity model to learn the physical constraint, and (3) utilizes the learned spatial interaction to guide and mitigate the over-smoothing phenomenon in transformer attention. Moreover, a parallel spatiotemporal graph convolution transformer is proposed for achieving a balance between coupled spatial and temporal learning. Systematic experiments on six real-world large-scale activity datasets demonstrate the quantitative and qualitative superiority of our model over state-of-the-art benchmarks. Additionally, the learned gravity attention matrix can be not only disentangled and interpreted based on geographical laws, but also improved the generalization in zero-shot cross-region inference. This work provides a novel insight into integrating physical laws with deep learning for spatiotemporal prediction.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precise Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2506.19726</link>
<guid>https://arxiv.org/abs/2506.19726</guid>
<content:encoded><![CDATA[
arXiv:2506.19726v2 Announce Type: replace 
Abstract: Despite its long history, Bayesian neural networks (BNNs) and variational training remain underused in practice: standard Gaussian posteriors misalign with network geometry, KL terms can be brittle in high dimensions, and implementations often add complexity without reliably improving uncertainty. We revisit the problem through the lens of normalization. Because normalization layers neutralize the influence of weight magnitude, we model uncertainty \emph{only in weight directions} using a von Mises-Fisher posterior on the unit sphere. High-dimensional geometry then yields a single, interpretable scalar per layer--the effective post-normalization noise $\sigma_{\mathrm{eff}}$--that (i) corresponds to simple additive Gaussian noise in the forward pass and (ii) admits a compact, dimension-aware KL in closed form. We derive accurate, closed-form approximations linking concentration $\kappa$ to activation variance and to $\sigma_{\mathrm{eff}}$ across regimes, producing a lightweight, implementation-ready variational unit that fits modern normalized architectures and improves calibration without sacrificing accuracy. This dimension awareness is critical for stable optimization in high dimensions. In short, by aligning the variational posterior with the network's intrinsic geometry, BNNs can be simultaneously principled, practical, and precise.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling</title>
<link>https://arxiv.org/abs/2507.01235</link>
<guid>https://arxiv.org/abs/2507.01235</guid>
<content:encoded><![CDATA[
arXiv:2507.01235v3 Announce Type: replace 
Abstract: Quantum computing has opened new opportunities to tackle complex machine learning tasks, for instance, high-dimensional data representations commonly required in intelligent transportation systems. We explore quantum machine learning to model complex skin conductance response (SCR) events that reflect pedestrian stress in a virtual reality road crossing experiment. For this purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and an eight-qubit ZZ feature map, were developed on Pennylane. The dataset consists of SCR measurements along with features such as the response amplitude and elapsed time, which have been categorized into amplitude-based classes. The QSVM achieved good training accuracy, but had an overfitting problem, showing a low test accuracy of 45% and therefore impacting the reliability of the classification model. The QNN model reached a higher test accuracy of 55%, making it a better classification model than the QSVM and the classic versions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAI-Powered Inference</title>
<link>https://arxiv.org/abs/2507.03897</link>
<guid>https://arxiv.org/abs/2507.03897</guid>
<content:encoded><![CDATA[
arXiv:2507.03897v2 Announce Type: replace 
Abstract: We introduce GenAI-Powered Inference (GPI), a statistical framework for both causal and predictive inference using unstructured data, including text and images. GPI leverages open-source Generative Artificial Intelligence (GenAI) models -- such as large language models and diffusion models -- not only to generate unstructured data at scale but also to extract low-dimensional representations that are guaranteed to capture their underlying structure. Applying machine learning to these representations, GPI enables estimation of causal and predictive effects while quantifying associated estimation uncertainty. Unlike existing approaches to representation learning, GPI does not require fine-tuning of generative models, making it computationally efficient and broadly accessible. We illustrate the versatility of the GPI framework through three applications: (1) analyzing Chinese social media censorship, (2) estimating predictive effects of candidates' facial appearance on electoral outcomes, and (3) assessing the persuasiveness of political rhetoric. An open-source software package is available for implementing GPI.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLAME: Lightweight MSA Design Advances Protein Folding From Evolutionary Embeddings</title>
<link>https://arxiv.org/abs/2507.07032</link>
<guid>https://arxiv.org/abs/2507.07032</guid>
<content:encoded><![CDATA[
arXiv:2507.07032v2 Announce Type: replace 
Abstract: Protein structure prediction often hinges on multiple sequence alignments (MSAs), which underperform on low-homology and orphan proteins. We introduce PLAME, a lightweight MSA design framework that leverages evolutionary embeddings from pretrained protein language models to generate MSAs that better support downstream folding. PLAME couples these embeddings with a conservation-diversity loss that balances agreement on conserved positions with coverage of plausible sequence variation. Beyond generation, we develop (i) an MSA selection strategy to filter high-quality candidates and (ii) a sequence-quality metric that is complementary to depth-based measures and predictive of folding gains. On AlphaFold2 low-homology/orphan benchmarks, PLAME delivers state-of-the-art improvements in structure accuracy (e.g., lDDT/TM-score), with consistent gains when paired with AlphaFold3. Ablations isolate the benefits of the selection strategy, and case studies elucidate how MSA characteristics shape AlphaFold confidence and error modes. Finally, we show PLAME functions as a lightweight adapter, enabling ESMFold to approach AlphaFold2-level accuracy while retaining ESMFold-like inference speed. PLAME thus provides a practical path to high-quality folding for proteins lacking strong evolutionary neighbors.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)</title>
<link>https://arxiv.org/abs/2507.12856</link>
<guid>https://arxiv.org/abs/2507.12856</guid>
<content:encoded><![CDATA[
arXiv:2507.12856v2 Announce Type: replace 
Abstract: Behavior Cloning (BC) on curated (or filtered) data is the predominant paradigm for supervised fine-tuning (SFT) of large language models; as well as for imitation learning of control policies. Here, we draw on a connection between this successful strategy and the theory and practice of finding optimal policies via Reinforcement Learning (RL). Building on existing literature, we clarify that SFT can be understood as maximizing a lower bound on the RL objective in a sparse reward setting. Giving support to its often observed good performance. From this viewpoint, we realize that a small modification to SFT leads to an importance weighted variant that behaves closer to training with RL as it: i) optimizes a tighter bound to the RL objective and, ii) can improve performance compared to SFT on curated data. We refer to this variant as importance weighted supervised fine-tuning (iw-SFT). We show that it is easy to implement and can be further generalized to training with quality scored data. The resulting SFT variants are competitive with more advanced RL algorithms for large language models and for training policies in continuous control tasks. For example achieving 66.7% on the AIME 2024 dataset.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Attention Matters: to Improve Model Robustness to Noise and Spurious Correlations</title>
<link>https://arxiv.org/abs/2507.20453</link>
<guid>https://arxiv.org/abs/2507.20453</guid>
<content:encoded><![CDATA[
arXiv:2507.20453v3 Announce Type: replace 
Abstract: Self-attention mechanisms are foundational to Transformer architectures, supporting their impressive success in a wide range of tasks. While there are many self-attention variants, their robustness to noise and spurious correlations has not been well studied. This study evaluates Softmax, Sigmoid, Linear, Doubly Stochastic, and Cosine attention within Vision Transformers under different data corruption scenarios. Through testing across the CIFAR-10, CIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is the most robust. It consistently outperformed the next best mechanism by $0.1\%-5.1\%$ when training data, or both training and testing data, were corrupted. Our findings inform self-attention selection in contexts with imperfect data. The code used is available at https://github.com/ctamayor/NeurIPS-Robustness-ViT.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning</title>
<link>https://arxiv.org/abs/2508.00716</link>
<guid>https://arxiv.org/abs/2508.00716</guid>
<content:encoded><![CDATA[
arXiv:2508.00716v3 Announce Type: replace 
Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled source graphs to unlabeled target graphs by learning domain-invariant representations, which is essential in applications such as molecular property prediction and social network analysis. However, most existing GDA methods rely on the assumption of clean source labels, which rarely holds in real-world scenarios where annotation noise is pervasive. This label noise severely impairs feature alignment and degrades adaptation performance under domain shifts. To address this challenge, we propose Nested Graph Pseudo-Label Refinement (NeGPR), a novel framework tailored for graph-level domain adaptation with noisy labels. NeGPR first pretrains dual branches, i.e., semantic and topology branches, by enforcing neighborhood consistency in the feature space, thereby reducing the influence of noisy supervision. To bridge domain gaps, NeGPR employs a nested refinement mechanism in which one branch selects high-confidence target samples to guide the adaptation of the other, enabling progressive cross-domain learning. Furthermore, since pseudo-labels may still contain noise and the pre-trained branches are already overfitted to the noisy labels in the source domain, NeGPR incorporates a noise-aware regularization strategy. This regularization is theoretically proven to mitigate the adverse effects of pseudo-label noise, even under the presence of source overfitting, thus enhancing the robustness of the adaptation process. Extensive experiments on benchmark datasets demonstrate that NeGPR consistently outperforms state-of-the-art methods under severe label noise, achieving gains of up to 12.7% in accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning</title>
<link>https://arxiv.org/abs/2508.07556</link>
<guid>https://arxiv.org/abs/2508.07556</guid>
<content:encoded><![CDATA[
arXiv:2508.07556v2 Announce Type: replace 
Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. This thesis investigates how uncertainty estimation can enhance the safety and trustworthiness of ML, focusing on selective prediction -- where models abstain when confidence is low.
  We first show that a model's training trajectory contains rich uncertainty signals that can be exploited without altering its architecture or loss. By ensembling predictions from intermediate checkpoints, we propose a lightweight, post-hoc abstention method that works across tasks, avoids the cost of deep ensembles, and achieves state-of-the-art selective prediction performance. Crucially, this approach is fully compatible with differential privacy (DP), allowing us to study how privacy noise affects uncertainty quality. We find that while many methods degrade under DP, our trajectory-based approach remains robust, and we introduce a framework for isolating the privacy-uncertainty trade-off. Next, we then develop a finite-sample decomposition of the selective classification gap -- the deviation from the oracle accuracy-coverage curve -- identifying five interpretable error sources and clarifying which interventions can close the gap. This explains why calibration alone cannot fix ranking errors, motivating methods that improve uncertainty ordering. Finally, we show that uncertainty signals can be adversarially manipulated to hide errors or deny service while maintaining high accuracy, and we design defenses combining calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating, and safeguarding uncertainty estimation, enabling models that not only make accurate predictions -- but also know when to say "I do not know".
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confounding is a Pervasive Problem in Real World Recommender Systems</title>
<link>https://arxiv.org/abs/2508.10479</link>
<guid>https://arxiv.org/abs/2508.10479</guid>
<content:encoded><![CDATA[
arXiv:2508.10479v2 Announce Type: replace 
Abstract: Unobserved confounding arises when an unmeasured feature influences both the treatment and the outcome, leading to biased causal effect estimates. This issue undermines observational studies in fields like economics, medicine, ecology or epidemiology. Recommender systems leveraging fully observed data seem not to be vulnerable to this problem. However many standard practices in recommender systems result in observed features being ignored, resulting in effectively the same problem. This paper will show that numerous common practices such as feature engineering, A/B testing and modularization can in fact introduce confounding into recommendation systems and hamper their performance. Several illustrations of the phenomena are provided, supported by simulation studies with practical suggestions about how practitioners may reduce or avoid the affects of confounding in real systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Generalization and Personalization in Human Activity Recognition via On-Device Few-Shot Learning</title>
<link>https://arxiv.org/abs/2508.15413</link>
<guid>https://arxiv.org/abs/2508.15413</guid>
<content:encoded><![CDATA[
arXiv:2508.15413v3 Announce Type: replace 
Abstract: Human Activity Recognition (HAR) with different sensing modalities requires both strong generalization across diverse users and efficient personalization for individuals. However, conventional HAR models often fail to generalize when faced with user-specific variations, leading to degraded performance. To address this challenge, we propose a novel on-device few-shot learning framework that bridges generalization and personalization in HAR. Our method first trains a generalizable representation across users and then rapidly adapts to new users with only a few labeled samples, updating lightweight classifier layers directly on resource-constrained devices. This approach achieves robust on-device learning with minimal computation and memory cost, making it practical for real-world deployment. We implement our framework on the energy-efficient RISC-V GAP9 microcontroller and evaluate it on three benchmark datasets (RecGym, QVAR-Gesture, Ultrasound-Gesture). Across these scenarios, post-deployment adaptation improves accuracy by 3.73\%, 17.38\%, and 3.70\%, respectively. These results demonstrate that few-shot on-device learning enables scalable, user-aware, and energy-efficient wearable human activity recognition by seamlessly uniting generalization and personalization. The related framework is open sourced for further research\footnote{https://github.com/kangpx/onlineTiny2023}.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot Approach for Pharmacokinetics</title>
<link>https://arxiv.org/abs/2508.15659</link>
<guid>https://arxiv.org/abs/2508.15659</guid>
<content:encoded><![CDATA[
arXiv:2508.15659v2 Announce Type: replace 
Abstract: Accurate dose-response forecasting under sparse sampling is central to precision pharmacotherapy. We present the Amortized In-Context Mixed-Effect Transformer (AICMET) model, a transformer-based latent-variable framework that unifies mechanistic compartmental priors with amortized in-context Bayesian inference. AICMET is pre-trained on hundreds of thousands of synthetic pharmacokinetic trajectories with Ornstein-Uhlenbeck priors over the parameters of compartment models, endowing the model with strong inductive biases and enabling zero-shot adaptation to new compounds. At inference time, the decoder conditions on the collective context of previously profiled trial participants, generating calibrated posterior predictions for newly enrolled patients after a few early drug concentration measurements. This capability collapses traditional model-development cycles from weeks to hours while preserving some degree of expert modelling. Experiments across public datasets show that AICMET attains state-of-the-art predictive accuracy and faithfully quantifies inter-patient variability -- outperforming both nonlinear mixed-effects baselines and recent neural ODE variants. Our results highlight the feasibility of transformer-based, population-aware neural architectures as offering a new alternative for bespoke pharmacokinetic modeling pipelines, charting a path toward truly population-aware personalized dosing regimens.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence and Generalization of Anti-Regularization for Parametric Models</title>
<link>https://arxiv.org/abs/2508.17412</link>
<guid>https://arxiv.org/abs/2508.17412</guid>
<content:encoded><![CDATA[
arXiv:2508.17412v2 Announce Type: replace 
Abstract: Anti-regularization introduces a reward term with a reversed sign into the loss function, deliberately amplifying model expressivity in small-sample regimes while ensuring that the intervention gradually vanishes as the sample size grows through a power-law decay schedule. We formalize spectral safety conditions and trust-region constraints, and we design a lightweight safeguard that combines a projection operator with gradient clipping to guarantee stable intervention. Theoretical analysis extends to linear smoothers and the Neural Tangent Kernel regime, providing practical guidance on the choice of decay exponents through the balance between empirical risk and variance. Empirical results show that Anti-regularization mitigates underfitting in both regression and classification while preserving generalization and improving calibration. Ablation studies confirm that the decay schedule and safeguards are essential to avoiding overfitting and instability. As an alternative, we also propose a degrees-of-freedom targeting schedule that maintains constant per-sample complexity. Anti-regularization constitutes a simple and reproducible procedure that integrates seamlessly into standard empirical risk minimization pipelines, enabling robust learning under limited data and resource constraints by intervening only when necessary and vanishing otherwise.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Expectation Policy Optimization for Heterogeneous Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.17850</link>
<guid>https://arxiv.org/abs/2508.17850</guid>
<content:encoded><![CDATA[
arXiv:2508.17850v2 Announce Type: replace 
Abstract: As single-center computing approaches power constraints, decentralized training is becoming essential. Reinforcement Learning (RL) post-training enhances Large Language Models (LLMs) but faces challenges in heterogeneous distributed environments due to its tightly-coupled sampling-learning alternation. We propose HeteroRL, an asynchronous RL architecture that decouples rollout sampling from parameter learning, enabling robust deployment across geographically distributed nodes under network delays. We identify that latency-induced KL divergence causes importance sampling failure due to high variance. To address this, we propose Group Expectation Policy Optimization (GEPO), which reduces importance weight variance through a refined sampling mechanism. Theoretically, GEPO achieves exponential variance reduction. Experiments show it maintains superior stability over methods like GRPO, with less than 3% performance degradation under 1800-second delays, demonstrating strong potential for decentralized RL in heterogeneous networks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities</title>
<link>https://arxiv.org/abs/2508.19597</link>
<guid>https://arxiv.org/abs/2508.19597</guid>
<content:encoded><![CDATA[
arXiv:2508.19597v2 Announce Type: replace 
Abstract: Artificial intelligence underpins most smart city services, yet deep neural network (DNN) that forecasts vehicle motion still struggle with catastrophic forgetting, the loss of earlier knowledge when models are updated. Conventional fixes enlarge the training set or replay past data, but these strategies incur high data collection costs, sample inefficiently and fail to balance long- and short-term experience, leaving them short of human-like continual learning. Here we introduce Dual-LS, a task-free, online continual learning paradigm for DNN-based motion forecasting that is inspired by the complementary learning system of the human brain. Dual-LS pairs two synergistic memory rehearsal replay mechanisms to accelerate experience retrieval while dynamically coordinating long-term and short-term knowledge representations. Tests on naturalistic data spanning three countries, over 772,000 vehicles and cumulative testing mileage of 11,187 km show that Dual-LS mitigates catastrophic forgetting by up to 74.31\% and reduces computational resource demand by up to 94.02\%, markedly boosting predictive stability in vehicle motion forecasting without inflating data requirements. Meanwhile, it endows DNN-based vehicle motion forecasting with computation efficient and human-like continual learning adaptability fit for smart cities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Failure Prediction Is a Better Performance Proxy for Early-Exit Networks Than Calibration</title>
<link>https://arxiv.org/abs/2508.21495</link>
<guid>https://arxiv.org/abs/2508.21495</guid>
<content:encoded><![CDATA[
arXiv:2508.21495v2 Announce Type: replace 
Abstract: Early-exit models accelerate inference by attaching internal classifiers to intermediate layers of the network, allowing computation to halt once a prediction meets a predefined exit criterion. Most early-exit methods rely on confidence-based exit strategies, which has motivated prior work to calibrate intermediate classifiers in pursuit of improved performance-efficiency trade-offs. In this paper, we argue that calibration metrics can be misleading indicators of multi-exit model performance. Specifically, we present empirical evidence showing that miscalibrated networks can outperform calibrated ones. As an alternative, we propose using failure prediction as a more informative proxy for early-exit model performance. Unlike calibration, failure prediction captures changes in sample rankings and correlates strongly with efficiency gains, offering a more reliable framework for designing and evaluating early-exit models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Varying Graph Learning with Constraints on Graph Temporal Variation</title>
<link>https://arxiv.org/abs/2001.03346</link>
<guid>https://arxiv.org/abs/2001.03346</guid>
<content:encoded><![CDATA[
arXiv:2001.03346v2 Announce Type: replace-cross 
Abstract: We propose a novel framework for learning time-varying graphs from spatiotemporal measurements. Given an appropriate prior on the temporal behavior of signals, our proposed method can estimate time-varying graphs from a small number of available measurements. To achieve this, we introduce two regularization terms in convex optimization problems that constrain sparseness of temporal variations of the time-varying networks. Moreover, a computationally-scalable algorithm is introduced to efficiently solve the optimization problem. The experimental results with synthetic and real datasets (point cloud and temperature data) demonstrate our proposed method outperforms the existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Visual Hallucination: A Generative Adversarial Network Framework</title>
<link>https://arxiv.org/abs/2102.08209</link>
<guid>https://arxiv.org/abs/2102.08209</guid>
<content:encoded><![CDATA[
arXiv:2102.08209v2 Announce Type: replace-cross 
Abstract: Visual hallucination refers to the perception of recognizable things that are not present. These phenomena are commonly linked to a range of neurological/psychiatric disorders. Despite ongoing research, the mechanisms through which the visual system generates hallucinations from real-world environments are still not well understood. Abnormal interactions between different regions of the brain responsible for perception are known to contribute to the occurrence of visual hallucinations. In this study, we propose and extend a generative neural network-based framework to address challenges within the visual system, aiming to create goal-driven models inspired by neurobiological mechanisms of visual hallucinations. We focus on the adversarial interactions between the visual system and the frontal lobe regions, proposing the Hallu-GAN model to suggest how these interactions can give rise to visual hallucinations. The architecture of the Hallu-GAN model is based on generative adversarial networks. Our simulation results indicate that disturbances in the ventral stream can lead to visual hallucinations. To further analyze the impact of other brain regions on the visual system, we extend the Hallu-GAN model by adding EEG data from individuals. This extended model, referred to as Hallu-GAN+, enables the examination of both hallucinating and non-hallucinating states. By training the Hallu-GAN+ model with EEG data from an individual with Charles Bonnet syndrome, we demonstrated its utility in analyzing the behavior of those experiencing hallucinations. Our simulation results confirmed the capability of the proposed model in resembling the visual system in both healthy and hallucinating states.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A stability theorem for bigraded persistence barcodes</title>
<link>https://arxiv.org/abs/2303.14694</link>
<guid>https://arxiv.org/abs/2303.14694</guid>
<content:encoded><![CDATA[
arXiv:2303.14694v3 Announce Type: replace-cross 
Abstract: We define bigraded persistent homology modules and bigraded barcodes of a finite pseudo-metric space X using the ordinary and double homology of the moment-angle complex associated with the Vietoris-Rips filtration of X. We prove a stability theorem for the bigraded persistent double homology modules and barcodes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Adaptive Graph Framelets with Generalized Vanishing Moments for Graph Machine Learning</title>
<link>https://arxiv.org/abs/2309.03537</link>
<guid>https://arxiv.org/abs/2309.03537</guid>
<content:encoded><![CDATA[
arXiv:2309.03537v3 Announce Type: replace-cross 
Abstract: In this paper, we propose a general framework for constructing tight framelet systems on graphs with localized supports based on partition trees. Our construction of framelets provides a simple and efficient way to obtain the orthogonality with $k$ arbitrary orthonormal vectors. When the $k$ vectors contain most of the energy of a family of graph signals, the orthogonality of the framelets intuitively possesses ``generalized ($k$-)vanishing'' moments, and thus, the coefficients are sparse. Moreover, our construction provides not only framelets that are overall sparse vectors but also fast and schematically concise transforms. In a data-adaptive setting, the graph framelet systems can be learned by conducting optimizations on Stiefel manifolds to provide the utmost sparsity for a given family of graph signals. Furthermore, we further exploit the generality of our proposed graph framelet systems for heterophilous graph learning, where graphs are characterized by connecting nodes mainly from different classes. The usual assumption that connected nodes are similar and belong to the same class for homophilious graphs is contradictory for heterophilous graphs. Thus, we are motivated to bypass simple assumptions on heterophilous graphs and focus on generating rich node features induced by the graph structure, so as to improve the graph learning ability of certain neural networks in node classification. We derive a specific system of graph framelets and propose a heuristic method to select framelets as features for neural network input. Several experiments demonstrate the effectiveness and superiority of our approach for non-linear approximation, denoising, and node classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation</title>
<link>https://arxiv.org/abs/2309.14394</link>
<guid>https://arxiv.org/abs/2309.14394</guid>
<content:encoded><![CDATA[
arXiv:2309.14394v2 Announce Type: replace-cross 
Abstract: In this work, we address the challenge of multi-domain translation, where the objective is to learn mappings between arbitrary configurations of domains within a defined set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for three domains) without the need for separate models for each specific translation configuration, enabling more efficient and flexible domain translation. We introduce Multi-Domain Diffusion (MDD), a method with dual purposes: i) reconstructing any missing views for new data objects, and ii) enabling learning in semi-supervised contexts with arbitrary supervision configurations. MDD achieves these objectives by exploiting the noise formulation of diffusion models, specifically modeling one noise level per domain. Similar to existing domain translation approaches, MDD learns the translation between any combination of domains. However, unlike prior work, our formulation inherently handles semi-supervised learning without modification by representing missing views as noise in the diffusion process. We evaluate our approach through domain translation experiments on BL3NDT, a multi-domain synthetic dataset designed for challenging semantic domain inversion, the BraTS2020 dataset, and the CelebAMask-HQ dataset.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The GOOSE Dataset for Perception in Unstructured Environments</title>
<link>https://arxiv.org/abs/2310.16788</link>
<guid>https://arxiv.org/abs/2310.16788</guid>
<content:encoded><![CDATA[
arXiv:2310.16788v2 Announce Type: replace-cross 
Abstract: The potential for deploying autonomous systems can be significantly increased by improving the perception and interpretation of the environment. However, the development of deep learning-based techniques for autonomous systems in unstructured outdoor environments poses challenges due to limited data availability for training and testing. To address this gap, we present the German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset specifically designed for unstructured outdoor environments. The GOOSE dataset incorporates 10 000 labeled pairs of images and point clouds, which are utilized to train a range of state-of-the-art segmentation models on both image and point cloud data. We open source the dataset, along with an ontology for unstructured terrain, as well as dataset standards and guidelines. This initiative aims to establish a common framework, enabling the seamless inclusion of existing datasets and a fast way to enhance the perception capabilities of various robots operating in unstructured environments. The dataset, pre-trained models for offroad perception, and additional documentation can be found at https://goose-dataset.de/.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable DG with Neural Operator Source Term Correction</title>
<link>https://arxiv.org/abs/2310.18897</link>
<guid>https://arxiv.org/abs/2310.18897</guid>
<content:encoded><![CDATA[
arXiv:2310.18897v4 Announce Type: replace-cross 
Abstract: Computational advances have fundamentally transformed the landscape of numerical simulations, enabling unprecedented levels of complexity and precision in modeling physical phenomena. While these high-fidelity simulations offer invaluable insights for scientific discovery and problem solving, they impose substantial computational requirements. Consequently, low-fidelity models augmented with subgrid-scale parameterizations are employed to achieve computational feasibility. We introduce an end-to-end differentiable framework for solving the compressible Navier--Stokes equations. This integrated approach combines a differentiable discontinuous Galerkin (DG) solver with a neural network source term. Through the implementation of neural ordinary differential equations (NODEs) for network parameter optimization, our methodology ensures continuous interaction with the governing equations throughout the training process. We refer to this approach as NODE-DG. This hybrid approach combines the accuracy of numerical methods with the efficiency of machine learning, offering the following key advantages: (1) improved accuracy of low-order DG approximations by capturing subgrid-scale dynamics; (2) robustness against nonuniform or missing temporal data; (3) elimination of operator-splitting errors; (3) total mass conservation; and (4) a continuous-in-time operator that enables variable time step predictions, which accelerate projected high-order DG simulations. We demonstrate the performance of the proposed framework through two examples: two-dimensional Kelvin--Helmholtz instability and three-dimensional Taylor--Green vortex examples.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Rate-Optimal Partitioning Classification from Observable and from Privatised Data</title>
<link>https://arxiv.org/abs/2312.14889</link>
<guid>https://arxiv.org/abs/2312.14889</guid>
<content:encoded><![CDATA[
arXiv:2312.14889v3 Announce Type: replace-cross 
Abstract: In this paper we revisit the classical method of partitioning classification and study its convergence rate under relaxed conditions, both for observable (non-privatised) and for privatised data. We consider the problem of classification in a $d$ dimensional Euclidean space. Previous results on the partitioning classifier worked with the strong density assumption, which is restrictive, as we demonstrate through simple examples. Here, we study the problem under much milder assumptions. We presuppose that the distribution of the inputs is a mixture of an absolutely continuous and a discrete distribution, such that the absolutely continuous component is concentrated to a $d_a$ dimensional subspace. In addition to the standard Lipschitz and margin conditions, a novel characteristic of the absolutely continuous component is introduced, by which the exact convergence rate of the classification error probability is computed, both for the binary and for the multi-label cases. Interestingly, this rate of convergence depends only on the intrinsic dimension of the inputs, $d_a$. The privacy constraints mean that the independent identically distributed data cannot be directly observed, and the classifiers are functions of the randomised outcome of a suitable local differential privacy mechanism. In this paper we add Laplace distributed noises to the discontinuations of all possible locations of the feature vector and to its label. Again, tight upper bounds on the rate of convergence of the classification error probability are derived, without the strong density assumption, such that this rate depends on $2d_a$.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Assessment of Tabular Data Synthesis</title>
<link>https://arxiv.org/abs/2402.06806</link>
<guid>https://arxiv.org/abs/2402.06806</guid>
<content:encoded><![CDATA[
arXiv:2402.06806v3 Announce Type: replace-cross 
Abstract: Data synthesis has been advocated as an important approach for utilizing data while protecting data privacy. In recent years, a plethora of tabular data synthesis algorithms (i.e., synthesizers) have been proposed. Some synthesizers satisfy Differential Privacy, while others aim to provide privacy in a heuristic fashion. A comprehensive understanding of the strengths and weaknesses of these synthesizers remains elusive due to drawbacks in evaluation metrics and missing head-to-head comparisons of newly developed synthesizers that take advantage of diffusion models and large language models with state-of-the-art statistical synthesizers. In this paper, we present a systematic evaluation framework for assessing tabular data synthesis algorithms. Specifically, we examine and critique existing evaluation metrics, and introduce a set of new metrics in terms of fidelity, privacy, and utility to address their limitations. We conducted extensive evaluations of 8 different types of synthesizers on 12 real-world datasets and identified some interesting findings, which offer new directions for privacy-preserving data synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data</title>
<link>https://arxiv.org/abs/2402.12391</link>
<guid>https://arxiv.org/abs/2402.12391</guid>
<content:encoded><![CDATA[
arXiv:2402.12391v3 Announce Type: replace-cross 
Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset to assess TAIS's effectiveness in gene identification, demonstrating our system's potential to significantly enhance the efficiency and scope of scientific exploration. Our findings represent a solid step towards automating scientific discovery through large language models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repetition Improves Language Model Embeddings</title>
<link>https://arxiv.org/abs/2402.15449</link>
<guid>https://arxiv.org/abs/2402.15449</guid>
<content:encoded><![CDATA[
arXiv:2402.15449v2 Announce Type: replace-cross 
Abstract: Bidirectional models are considered essential for strong text embeddings. Recent approaches to adapt autoregressive language models (LMs) into strong text embedding models have largely had the requirement to modify the LM architecture to be bidirectional. We challenge this premise by introducing "echo embeddings" which converts autoregressive LMs into high quality text embedding models without changing the architecture or requiring fine-tuning. By repeating the input and extracting embeddings from the repeated tokens -- which have access to all original tokens -- echo embeddings improve over classical LM embeddings by over 5% in zero-shot settings. Our zero-shot embeddings nearly match those obtained by bidirectionally-converted LMs that undergo additional masked-language modeling training. Echo embeddings are also compatible with supervised fine-tuning, matching or outperforming bidirectionally-converted LMs in an apples-to-apples comparison, even with an identical compute budget during training and inference. Overall, repetition is a simple and effective strategy to circumvent the need for bidirectional attention in embedding models, paving the way towards a unified architecture for all NLP tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Inference for Uncertainty Quantification: an Analysis of Trade-offs</title>
<link>https://arxiv.org/abs/2403.13748</link>
<guid>https://arxiv.org/abs/2403.13748</guid>
<content:encoded><![CDATA[
arXiv:2403.13748v4 Announce Type: replace-cross 
Abstract: Given an intractable distribution $p$, the problem of variational inference (VI) is to find the best approximation from some more tractable family $Q$. Commonly, one chooses $Q$ to be a family of factorized distributions (i.e., the mean-field assumption), even though $p$ itself does not factorize. We show that this mismatch leads to an impossibility theorem: if $p$ does not factorize, then any factorized approximation $q\!\in\!Q$ can correctly estimate at most one of the following three measures of uncertainty: (i) the marginal variances, (ii) the marginal precisions, or (iii) the generalized variance (which for elliptical distributions is closely related to the entropy). In practice, the best variational approximation in $Q$ is found by minimizing some divergence $D(q,p)$ between distributions, and so we ask: how does the choice of divergence determine which measure of uncertainty, if any, is correctly estimated by VI? We consider the classic Kullback-Leibler divergences, the more general $\alpha$-divergences, and a score-based divergence which compares $\nabla \log p$ and $\nabla \log q$. We thoroughly analyze the case where $p$ is a Gaussian and $q$ is a (factorized) Gaussian. In this setting, we show that all the considered divergences can be ordered based on the estimates of uncertainty they yield as objective functions for VI. Finally, we empirically evaluate the validity of this ordering when the target distribution $p$ is not Gaussian.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness-Aware Data Augmentation for Cardiac MRI using Text-Conditioned Diffusion Models</title>
<link>https://arxiv.org/abs/2403.19508</link>
<guid>https://arxiv.org/abs/2403.19508</guid>
<content:encoded><![CDATA[
arXiv:2403.19508v2 Announce Type: replace-cross 
Abstract: While deep learning holds great promise for disease diagnosis and prognosis in cardiac magnetic resonance imaging, its progress is often constrained by highly imbalanced and biased training datasets. To address this issue, we propose a method to alleviate imbalances inherent in datasets through the generation of synthetic data based on sensitive attributes such as sex, age, body mass index (BMI), and health condition. We adopt ControlNet based on a denoising diffusion probabilistic model to condition on text assembled from patient metadata and cardiac geometry derived from segmentation masks. We assess our method using a large-cohort study from the UK Biobank by evaluating the realism of the generated images using established quantitative metrics. Furthermore, we conduct a downstream classification task aimed at debiasing a classifier by rectifying imbalances within underrepresented groups through synthetically generated samples. Our experiments demonstrate the effectiveness of the proposed approach in mitigating dataset imbalances, such as the scarcity of diagnosed female patients or individuals with normal BMI level suffering from heart failure. This work represents a major step towards the adoption of synthetic data for the development of fair and generalizable models for medical classification tasks. Notably, we conduct all our experiments using a single, consumer-level GPU to highlight the feasibility of our approach within resource-constrained environments. Our code is available at https://github.com/faildeny/debiasing-cardiac-mri.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Molecular Generative Adversarial Network with Multi-Property Optimization</title>
<link>https://arxiv.org/abs/2404.00081</link>
<guid>https://arxiv.org/abs/2404.00081</guid>
<content:encoded><![CDATA[
arXiv:2404.00081v2 Announce Type: replace-cross 
Abstract: Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance to state-of-the-art models, and efficiently generates molecules with multi-property optimization. The source code will be released upon acceptance of the paper.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Generative Learning with Lipschitz-Regularized $\alpha$-Divergences Allows Minimal Assumptions on Target Distributions</title>
<link>https://arxiv.org/abs/2405.13962</link>
<guid>https://arxiv.org/abs/2405.13962</guid>
<content:encoded><![CDATA[
arXiv:2405.13962v3 Announce Type: replace-cross 
Abstract: This paper demonstrates the robustness of Lipschitz-regularized $\alpha$-divergences as objective functionals in generative modeling, showing they enable stable learning across a wide range of target distributions with minimal assumptions. We establish that these divergences remain finite under a mild condition-that the source distribution has a finite first moment-regardless of the properties of the target distribution, making them adaptable to the structure of target distributions. Furthermore, we prove the existence and finiteness of their variational derivatives, which are essential for stable training of generative models such as GANs and gradient flows. For heavy-tailed targets, we derive necessary and sufficient conditions that connect data dimension, $\alpha$, and tail behavior to divergence finiteness, that also provide insights into the selection of suitable $\alpha$'s. We also provide the first sample complexity bounds for empirical estimations of these divergences on unbounded domains. As a byproduct, we obtain the first sample complexity bounds for empirical estimations of these divergences and the Wasserstein-1 metric with group symmetry on unbounded domains. Numerical experiments confirm that generative models leveraging Lipschitz-regularized $\alpha$-divergences can stably learn distributions in various challenging scenarios, including those with heavy tails or complex, low-dimensional, or fractal support, all without any prior knowledge of the structure of target distributions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Prompt Pricing based on Combinatorial Multi-Armed Bandit and Hierarchical Stackelberg Game</title>
<link>https://arxiv.org/abs/2405.15154</link>
<guid>https://arxiv.org/abs/2405.15154</guid>
<content:encoded><![CDATA[
arXiv:2405.15154v3 Announce Type: replace-cross 
Abstract: Generation models have shown promising performance in various tasks, making trading around machine learning models possible. In this paper, we aim at a novel prompt trading scenario, prompt bundle trading (PBT) system, and propose an online pricing mechanism. Based on the combinatorial multi-armed bandit (CMAB) and three-stage hierarchical Stackelburg (HS) game, our pricing mechanism considers the profits of the consumer, platform, and seller, simultaneously achieving the profit satisfaction of these three participants. We break down the pricing issue into two steps, namely unknown category selection and incentive strategy optimization. The former step is to select a set of categories with the highest qualities, and the latter is to derive the optimal strategy for each participant based on the chosen categories. Unlike the existing fixed pricing mode, the PBT pricing mechanism we propose is more flexible and diverse, which is more in accord with the transaction needs of real-world scenarios. We test our method on a simulated text-to-image dataset. The experimental results demonstrate the effectiveness of our algorithm, which provides a feasible price-setting standard for the prompt marketplaces.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIBESegmentator: Full Body MRI Segmentation for the NAKO and UK Biobank</title>
<link>https://arxiv.org/abs/2406.00125</link>
<guid>https://arxiv.org/abs/2406.00125</guid>
<content:encoded><![CDATA[
arXiv:2406.00125v4 Announce Type: replace-cross 
Abstract: Objectives: To present a publicly available deep learning-based torso segmentation model that provides comprehensive voxel-wise coverage, including delineations that extend to the boundaries of anatomical compartments. Materials and Methods: We extracted preliminary segmentations from TotalSegmentator, spine, and body composition models for Magnetic Resonance Tomography (MR) images, then improved them iteratively and retrained an nnUNet model. Using a random retrospective subset of German National Cohort (NAKO), UK Biobank, internal MR and Computed Tomography (CT) data (Training: 2897 series from 626 subjects, 290 female; mean age 53+-16; 3-fold-cross validation (20% hold-out). Internal testing 36 series from 12 subjects, 6 male; mean age 60+-11), we segmented 71 structures in torso MR and 72 in CT images: 20 organs, 10 muscles, 19 vessels, 16 bones, ribs in CT, intervertebral discs, spinal cord, spinal canal and body composition (subcutaneous fat, unclassified muscles and visceral fat). For external validation, we used existing automatic organ segmentations, independent ground truth segmentations on gradient echo images, and the Amos data. We used non-parametric bootstrapping for confidence intervals and Wilcoxon rank-sum test for computing statistical significance. Results: We achieved an average Dice score of 0.90+-0.06 on our internal gradient echo test set, which included 71 semantic segmentation labels. Our model ties with the best model on Amos with a Dice of 0,81+-0.14, while having a larger field of view and a considerably higher number structures included. Conclusion: Our work presents a publicly available full-torso segmentation model for MRI and CT images that classifies almost all subject voxels to date.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effect of Random Learning Rate: Theoretical Analysis of SGD Dynamics in Non-Convex Optimization via Stationary Distribution</title>
<link>https://arxiv.org/abs/2406.16032</link>
<guid>https://arxiv.org/abs/2406.16032</guid>
<content:encoded><![CDATA[
arXiv:2406.16032v2 Announce Type: replace-cross 
Abstract: We consider a variant of the stochastic gradient descent (SGD) with a random learning rate and reveal its convergence properties. SGD is a widely used stochastic optimization algorithm in machine learning, especially deep learning. Numerous studies reveal the convergence properties of SGD and its theoretically favorable variants. Among these, the analysis of convergence using a stationary distribution of updated parameters provides generalizable results. However, to obtain a stationary distribution, the update direction of the parameters must not degenerate, which limits the applicable variants of SGD. In this study, we consider a novel SGD variant, Poisson SGD, which has degenerated parameter update directions and instead utilizes a random learning rate. Consequently, we demonstrate that a distribution of a parameter updated by Poisson SGD converges to a stationary distribution under weak assumptions on a loss function. Based on this, we further show that Poisson SGD finds global minima in non-convex optimization problems and also evaluate the generalization error using this method. As a proof technique, we approximate the distribution by Poisson SGD with that of the bouncy particle sampler (BPS) and derive its stationary distribution, using the theoretical advance of the piece-wise deterministic Markov process (PDMP).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fully Parameter-Free Second-Order Algorithm for Convex-Concave Minimax Problems</title>
<link>https://arxiv.org/abs/2407.03571</link>
<guid>https://arxiv.org/abs/2407.03571</guid>
<content:encoded><![CDATA[
arXiv:2407.03571v2 Announce Type: replace-cross 
Abstract: In this paper, we study second-order algorithms for the convex-concave minimax problem, which has attracted much attention in many fields such as machine learning in recent years. We propose a Lipschitz-free cubic regularization (LF-CR) algorithm for solving the convex-concave minimax optimization problem without knowing the Lipschitz constant. It can be shown that the iteration complexity of the LF-CR algorithm to obtain an $\epsilon$-optimal solution with respect to the restricted primal-dual gap is upper bounded by $\mathcal{O}(\rho^{2/3}\|z_0-z^*\|^2\epsilon^{-2/3})$ , where $z_0=(x_0,y_0)$ is a pair of initial points, $z^*=(x^*,y^*)$ is a pair of optimal solutions, and $\rho$ is the Lipschitz constant. We further propose a fully parameter-free cubic regularization (FF-CR) algorithm that does not require any parameters of the problem, including the Lipschitz constant and the upper bound of the distance from the initial point to the optimal solution. We also prove that the iteration complexity of the FF-CR algorithm to obtain an $\epsilon$-optimal solution with respect to the gradient norm is upper bounded by $\mathcal{O}(\rho^{2/3}\|z_0-z^*\|^{4/3}\epsilon^{-2/3}) $. Numerical experiments show the efficiency of both algorithms. To the best of our knowledge, the proposed FF-CR algorithm is a completely parameter-free second-order algorithm, and its iteration complexity is currently the best in terms of $\epsilon$ under the termination criterion of the gradient norm.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoencoders in Function Space</title>
<link>https://arxiv.org/abs/2408.01362</link>
<guid>https://arxiv.org/abs/2408.01362</guid>
<content:encoded><![CDATA[
arXiv:2408.01362v3 Announce Type: replace-cross 
Abstract: Autoencoders have found widespread application in both their original deterministic form and in their variational formulation (VAEs). In scientific applications and in image processing it is often of interest to consider data that are viewed as functions; while discretisation (of differential equations arising in the sciences) or pixellation (of images) renders problems finite dimensional in practice, conceiving first of algorithms that operate on functions, and only then discretising or pixellating, leads to better algorithms that smoothly operate between resolutions. In this paper function-space versions of the autoencoder (FAE) and variational autoencoder (FVAE) are introduced, analysed, and deployed. Well-definedness of the objective governing VAEs is a subtle issue, particularly in function space, limiting applicability. For the FVAE objective to be well defined requires compatibility of the data distribution with the chosen generative model; this can be achieved, for example, when the data arise from a stochastic differential equation, but is generally restrictive. The FAE objective, on the other hand, is well defined in many situations where FVAE fails to be. Pairing the FVAE and FAE objectives with neural operator architectures that can be evaluated on any mesh enables new applications of autoencoders to inpainting, superresolution, and generative modelling of scientific data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale Transformer Approach</title>
<link>https://arxiv.org/abs/2408.04290</link>
<guid>https://arxiv.org/abs/2408.04290</guid>
<content:encoded><![CDATA[
arXiv:2408.04290v5 Announce Type: replace-cross 
Abstract: Pneumonia, a prevalent respiratory infection, remains a leading cause of morbidity and mortality worldwide, particularly among vulnerable populations. Chest X-rays serve as a primary tool for pneumonia detection; however, variations in imaging conditions and subtle visual indicators complicate consistent interpretation. Automated tools can enhance traditional methods by improving diagnostic reliability and supporting clinical decision-making. In this study, we propose a novel multi-scale transformer approach for pneumonia detection that integrates lung segmentation and classification into a unified framework. Our method introduces a lightweight transformer-enhanced TransUNet for precise lung segmentation, achieving a Dice score of 95.68% on the "Chest X-ray Masks and Labels" dataset with fewer parameters than traditional transformers. For classification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101) to extract multi-scale feature maps, which are then processed through a modified transformer module to enhance pneumonia detection. This integration of multi-scale feature extraction and lightweight transformer modules ensures robust performance, making our method suitable for resource-constrained clinical environments. Our approach achieves 93.75% accuracy on the "Kermany" dataset and 96.04% accuracy on the "Cohen" dataset, outperforming existing methods while maintaining computational efficiency. This work demonstrates the potential of multi-scale transformer architectures to improve pneumonia diagnosis, offering a scalable and accurate solution to global healthcare challenges. https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confirmation Bias in Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2408.09718</link>
<guid>https://arxiv.org/abs/2408.09718</guid>
<content:encoded><![CDATA[
arXiv:2408.09718v2 Announce Type: replace-cross 
Abstract: Confirmation bias, the tendency to interpret information in a way that aligns with one's preconceptions, can profoundly impact scientific research, leading to conclusions that reflect the researcher's hypotheses even when the observational data do not support them. This issue is especially critical in scientific fields involving highly noisy observations, such as cryo-electron microscopy.
  This study investigates confirmation bias in Gaussian mixture models. We consider the following experiment: A team of scientists assumes they are analyzing data drawn from a Gaussian mixture model with known signals (hypotheses) as centroids. However, in reality, the observations consist entirely of noise without any informative structure. The researchers use a single iteration of the K-means or expectation-maximization algorithms, two popular algorithms to estimate the centroids. Despite the observations being pure noise, we show that these algorithms yield biased estimates that resemble the initial hypotheses, contradicting the unbiased expectation that averaging these noise observations would converge to zero. Namely, the algorithms generate estimates that mirror the postulated model, although the hypotheses (the presumed centroids of the Gaussian mixture) are not evident in the observations. Specifically, among other results, we prove a positive correlation between the estimates produced by the algorithms and the corresponding hypotheses. We also derive explicit closed-form expressions of the estimates for a finite and infinite number of hypotheses. This study underscores the risks of confirmation bias in low signal-to-noise environments, provides insights into potential pitfalls in scientific methodologies, and highlights the importance of prudent data interpretation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidential Transformers for Improved Image Retrieval</title>
<link>https://arxiv.org/abs/2409.01082</link>
<guid>https://arxiv.org/abs/2409.01082</guid>
<content:encoded><![CDATA[
arXiv:2409.01082v2 Announce Type: replace-cross 
Abstract: We introduce the Evidential Transformer, an uncertainty-driven transformer model for improved and robust image retrieval. In this paper, we make several contributions to content-based image retrieval (CBIR). We incorporate probabilistic methods into image retrieval, achieving robust and reliable results, with evidential classification surpassing traditional training based on multiclass classification as a baseline for deep metric learning. Furthermore, we improve the state-of-the-art retrieval results on several datasets by leveraging the Global Context Vision Transformer (GC ViT) architecture. Our experimental results consistently demonstrate the reliability of our approach, setting a new benchmark in CBIR in all test settings on the Stanford Online Products (SOP) and CUB-200-2011 datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Standardizing Similarity Measures in a Rapidly Evolving Field</title>
<link>https://arxiv.org/abs/2409.18333</link>
<guid>https://arxiv.org/abs/2409.18333</guid>
<content:encoded><![CDATA[
arXiv:2409.18333v2 Announce Type: replace-cross 
Abstract: Similarity measures are fundamental tools for quantifying the alignment between artificial and biological systems. However, the diversity of similarity measures and their varied naming and implementation conventions makes it challenging to compare across studies. To facilitate comparisons and make explicit the implementation choices underlying a given code package, we have created and are continuing to develop a Python repository that benchmarks and standardizes similarity measures. The goal of creating a consistent naming convention that uniquely and efficiently specifies a similarity measure is not trivial as, for example, even commonly used methods like Centered Kernel Alignment (CKA) have at least 12 different variations, and this number will likely continue to grow as the field evolves. For this reason, we do not advocate for a fixed, definitive naming convention. The landscape of similarity measures and best practices will continue to change and so we see our current repository, which incorporates approximately 100 different similarity measures from 14 packages, as providing a useful tool at this snapshot in time. To accommodate the evolution of the field we present a framework for developing, validating, and refining naming conventions with the goal of uniquely and efficiently specifying similarity measures, ultimately making it easier for the community to make comparisons across studies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ILeSiA: Interactive Learning of Robot Situational Awareness from Camera Input</title>
<link>https://arxiv.org/abs/2409.20173</link>
<guid>https://arxiv.org/abs/2409.20173</guid>
<content:encoded><![CDATA[
arXiv:2409.20173v3 Announce Type: replace-cross 
Abstract: Learning from demonstration is a promising approach for teaching robots new skills. However, a central challenge in the execution of acquired skills is the ability to recognize faults and prevent failures. This is essential because demonstrations typically cover only a limited set of scenarios and often only the successful ones. During task execution, unforeseen situations may arise, such as changes in the robot's environment or interaction with human operators. To recognize such situations, this paper focuses on teaching the robot situational awareness by using a camera input and labeling frames as safe or risky. We train a Gaussian Process (GP) regression model fed by a low-dimensional latent space representation of the input images. The model outputs a continuous risk score ranging from zero to one, quantifying the degree of risk at each timestep. This allows for pausing task execution in unsafe situations and directly adding new training data, labeled by the human user. Our experiments on a robotic manipulator show that the proposed method can reliably detect both known and novel faults using only a single example for each new fault. In contrast, a standard multi-layer perceptron (MLP) performs well only on faults it has encountered during training. Our method enables the next generation of cobots to be rapidly deployed with easy-to-set-up, vision-based risk assessment, proactively safeguarding humans and detecting misaligned parts or missing objects before failures occur. We provide all the code and data required to reproduce our experiments at imitrob.ciirc.cvut.cz/publications/ilesia.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AARK: An Open Toolkit for Autonomous Racing Research</title>
<link>https://arxiv.org/abs/2410.00358</link>
<guid>https://arxiv.org/abs/2410.00358</guid>
<content:encoded><![CDATA[
arXiv:2410.00358v2 Announce Type: replace-cross 
Abstract: Autonomous racing demands safe control of vehicles at their physical limits for extended periods of time, providing insights into advanced vehicle safety systems which increasingly rely on intervention provided by vehicle autonomy. Participation in this field carries with it a high barrier to entry. Physical platforms and their associated sensor suites require large capital outlays before any demonstrable progress can be made. Simulators allow researches to develop soft autonomous systems without purchasing a platform. However, currently available simulators lack visual and dynamic fidelity, can still be expensive to buy, lack customisation, and are difficult to use. AARK provides three packages, ACI, ACDG, and ACMPC. These packages enable research into autonomous control systems in the demanding environment of racing to bring more people into the field and improve reproducibility: ACI provides researchers with a computer vision-friendly interface to Assetto Corsa for convenient comparison and evaluation of autonomous control solutions; ACDG enables generation of depth, normal and semantic segmentation data for training computer vision models to use in perception systems; and ACMPC gives newcomers to the field a modular full-stack autonomous control solution, capable of controlling vehicles to build from. AARK aims to unify and democratise research into a field critical to providing safer roads and trusted autonomous systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Representation Learning with Generative Artificial Intelligence: Application to Texts as Treatments</title>
<link>https://arxiv.org/abs/2410.00903</link>
<guid>https://arxiv.org/abs/2410.00903</guid>
<content:encoded><![CDATA[
arXiv:2410.00903v4 Announce Type: replace-cross 
Abstract: In this paper, we demonstrate how to enhance the validity of causal inference with unstructured high-dimensional treatments like texts, by leveraging the power of generative Artificial Intelligence (GenAI). Specifically, we propose to use a deep generative model such as large language models (LLMs) to efficiently generate treatments and use their internal representation for subsequent causal effect estimation. We show that the knowledge of this true internal representation helps disentangle the treatment features of interest, such as specific sentiments and certain topics, from other possibly unknown confounding features. Unlike existing methods, the proposed GenAI-Powered Inference (GPI) methodology eliminates the need to learn causal representation from the data, and hence produces more accurate and efficient estimates. We formally establish the conditions required for the nonparametric identification of the average treatment effect, propose an estimation strategy that avoids the violation of the overlap assumption, and derive the asymptotic properties of the proposed estimator through the application of double machine learning. Finally, using an instrumental variables approach, we extend the proposed GPI methodology to the settings in which the treatment feature is based on human perception. The GPI is also applicable to text reuse where an LLM is used to regenerate existing texts. We conduct simulation and empirical studies, using the generated text data from an open-source LLM, Llama~3, to illustrate the advantages of our estimator over state-of-the-art causal representation learning algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAIR Universe HiggsML Uncertainty Challenge Competition</title>
<link>https://arxiv.org/abs/2410.02867</link>
<guid>https://arxiv.org/abs/2410.02867</guid>
<content:encoded><![CDATA[
arXiv:2410.02867v3 Announce Type: replace-cross 
Abstract: The FAIR Universe -- HiggsML Uncertainty Challenge focuses on measuring the physics properties of elementary particles with imperfect simulators due to differences in modelling systematic errors. Additionally, the challenge is leveraging a large-compute-scale AI platform for sharing datasets, training models, and hosting machine learning competitions. Our challenge brings together the physics and machine learning communities to advance our understanding and methodologies in handling systematic (epistemic) uncertainties within AI techniques.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping</title>
<link>https://arxiv.org/abs/2410.05341</link>
<guid>https://arxiv.org/abs/2410.05341</guid>
<content:encoded><![CDATA[
arXiv:2410.05341v3 Announce Type: replace-cross 
Abstract: Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs unseen resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy with the potential to generalize across varying conditions and sites, which significantly advances the integration of these two modalities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limit Theorems for Stochastic Gradient Descent with Infinite Variance</title>
<link>https://arxiv.org/abs/2410.16340</link>
<guid>https://arxiv.org/abs/2410.16340</guid>
<content:encoded><![CDATA[
arXiv:2410.16340v4 Announce Type: replace-cross 
Abstract: Stochastic gradient descent is a classic algorithm that has gained great popularity especially in the last decades as the most common approach for training models in machine learning. While the algorithm has been well-studied when stochastic gradients are assumed to have a finite variance, there is significantly less research addressing its theoretical properties in the case of infinite variance gradients. In this paper, we establish the asymptotic behavior of stochastic gradient descent in the context of infinite variance stochastic gradients, assuming that the stochastic gradient is regular varying with index $\alpha\in(1,2)$. The closest result in this context was established in 1969 , in the one-dimensional case and assuming that stochastic gradients belong to a more restrictive class of distributions. We extend it to the multidimensional case, covering a broader class of infinite variance distributions. As we show, the asymptotic distribution of the stochastic gradient descent algorithm can be characterized as the stationary distribution of a suitably defined Ornstein-Uhlenbeck process driven by an appropriate stable L\'evy process. Additionally, we explore the applications of these results in linear regression and logistic regression models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Coded Distributed Convolution Computing for Enhanced Straggler Resilience and Numerical Stability in Distributed CNNs</title>
<link>https://arxiv.org/abs/2411.01579</link>
<guid>https://arxiv.org/abs/2411.01579</guid>
<content:encoded><![CDATA[
arXiv:2411.01579v3 Announce Type: replace-cross 
Abstract: Deploying Convolutional Neural Networks (CNNs) on resource-constrained devices necessitates efficient management of computational resources, often via distributed environments susceptible to latency from straggler nodes. This paper introduces the Flexible Coded Distributed Convolution Computing (FCDCC) framework to enhance straggler resilience and numerical stability in distributed CNNs. We extend Coded Distributed Computing (CDC) with Circulant and Rotation Matrix Embedding (CRME) which was originally proposed for matrix multiplication to high-dimensional tensor convolution. For the proposed scheme, referred to as the Numerically Stable Coded Tensor Convolution (NSCTC) scheme, we also propose two new coded partitioning schemes: Adaptive-Padding Coded Partitioning (APCP) for the input tensor and Kernel-Channel Coded Partitioning (KCCP) for the filter tensor. These strategies enable linear decomposition of tensor convolutions and encoding them into CDC subtasks, combining model parallelism with coded redundancy for robust and efficient execution. Theoretical analysis identifies an optimal trade-off between communication and storage costs. Empirical results validate the framework's effectiveness in computational efficiency, straggler resilience, and scalability across various CNN architectures.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Mutation-Acyclicity of Quivers</title>
<link>https://arxiv.org/abs/2411.04209</link>
<guid>https://arxiv.org/abs/2411.04209</guid>
<content:encoded><![CDATA[
arXiv:2411.04209v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) has emerged as a powerful tool in mathematical research in recent years. This paper applies ML techniques to the study of quivers -- a type of directed multigraph with significant relevance in algebra, combinatorics, computer science, and mathematical physics. Specifically, we focus on the challenging problem of determining the mutation-acyclicity of a quiver on 4 vertices, a property that is pivotal since mutation-acyclicity is often a necessary condition for theorems involving path algebras and cluster algebras. Although this classification is known for quivers with at most 3 vertices, little is known about quivers on more than 3 vertices. We give a computer-assisted proof of a theorem to prove that mutation-acyclicity is decidable for quivers on 4 vertices with edge weight at most 2. By leveraging neural networks (NNs) and support vector machines (SVMs), we then accurately classify more general 4-vertex quivers as mutation-acyclic or non-mutation-acyclic. Our results demonstrate that ML models can efficiently detect mutation-acyclicity, providing a promising computational approach to this combinatorial problem, from which the trained SVM equation provides a starting point to guide future theoretical development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElectroVizQA: How well do Multi-modal LLMs perform in Electronics Visual Question Answering?</title>
<link>https://arxiv.org/abs/2412.00102</link>
<guid>https://arxiv.org/abs/2412.00102</guid>
<content:encoded><![CDATA[
arXiv:2412.00102v2 Announce Type: replace-cross 
Abstract: Multi-modal Large Language Models (MLLMs) are gaining significant attention for their ability to process multi-modal data, providing enhanced contextual understanding of complex problems. MLLMs have demonstrated exceptional capabilities in tasks such as Visual Question Answering (VQA); however, they often struggle with fundamental engineering problems, and there is a scarcity of specialized datasets for training on topics like digital electronics. To address this gap, we propose a benchmark dataset called ElectroVizQA specifically designed to evaluate MLLMs' performance on digital electronic circuit problems commonly found in undergraduate curricula. This dataset, the first of its kind tailored for the VQA task in digital electronics, comprises approximately 626 visual questions, offering a comprehensive overview of digital electronics topics. This paper rigorously assesses the extent to which MLLMs can understand and solve digital electronic circuit questions, providing insights into their capabilities and limitations within this specialized domain. By introducing this benchmark dataset, we aim to motivate further research and development in the application of MLLMs to engineering education, ultimately bridging the performance gap and enhancing the efficacy of these models in technical fields.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Federated Learning via Homomorphic Adversarial Networks</title>
<link>https://arxiv.org/abs/2412.01650</link>
<guid>https://arxiv.org/abs/2412.01650</guid>
<content:encoded><![CDATA[
arXiv:2412.01650v3 Announce Type: replace-cross 
Abstract: Privacy-preserving federated learning (PPFL) aims to train a global model for multiple clients while maintaining their data privacy. However, current PPFL protocols exhibit one or more of the following insufficiencies: considerable degradation in accuracy, the requirement for sharing keys, and cooperation during the key generation or decryption processes. As a mitigation, we develop the first protocol that utilizes neural networks to implement PPFL, as well as incorporating an Aggregatable Hybrid Encryption scheme tailored to the needs of PPFL. We name these networks as Homomorphic Adversarial Networks (HANs) which demonstrate that neural networks are capable of performing tasks similar to multi-key homomorphic encryption (MK-HE) while solving the problems of key distribution and collaborative decryption. Our experiments show that HANs are robust against privacy attacks. Compared with non-private federated learning, experiments conducted on multiple datasets demonstrate that HANs exhibit a negligible accuracy loss (at most 1.35%). Compared to traditional MK-HE schemes, HANs increase encryption aggregation speed by 6,075 times while incurring a 29.2 times increase in communication overhead.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Controlled Langevin Diffusions</title>
<link>https://arxiv.org/abs/2412.07081</link>
<guid>https://arxiv.org/abs/2412.07081</guid>
<content:encoded><![CDATA[
arXiv:2412.07081v2 Announce Type: replace-cross 
Abstract: An effective approach for sampling from unnormalized densities is based on the idea of gradually transporting samples from an easy prior to the complicated target distribution. Two popular methods are (1) Sequential Monte Carlo (SMC), where the transport is performed through successive annealed densities via prescribed Markov chains and resampling steps, and (2) recently developed diffusion-based sampling methods, where a learned dynamical transport is used. Despite the common goal, both approaches have different, often complementary, advantages and drawbacks. The resampling steps in SMC allow focusing on promising regions of the space, often leading to robust performance. While the algorithm enjoys asymptotic guarantees, the lack of flexible, learnable transitions can lead to slow convergence. On the other hand, diffusion-based samplers are learned and can potentially better adapt themselves to the target at hand, yet often suffer from training instabilities. In this work, we present a principled framework for combining SMC with diffusion-based samplers by viewing both methods in continuous time and considering measures on path space. This culminates in the new Sequential Controlled Langevin Diffusion (SCLD) sampling method, which is able to utilize the benefits of both methods and reaches improved performance on multiple benchmark problems, in many cases using only 10% of the training budget of previous diffusion-based samplers.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept Bottleneck Large Language Models</title>
<link>https://arxiv.org/abs/2412.07992</link>
<guid>https://arxiv.org/abs/2412.07992</guid>
<content:encoded><![CDATA[
arXiv:2412.07992v4 Announce Type: replace-cross 
Abstract: We introduce Concept Bottleneck Large Language Models (CB-LLMs), a novel framework for building inherently interpretable Large Language Models (LLMs). In contrast to traditional black-box LLMs that rely on limited post-hoc interpretations, CB-LLMs integrate intrinsic interpretability directly into the LLMs -- allowing accurate explanations with scalability and transparency. We build CB-LLMs for two essential NLP tasks: text classification and text generation. In text classification, CB-LLMs is competitive with, and at times outperforms, traditional black-box models while providing explicit and interpretable reasoning. For the more challenging task of text generation, interpretable neurons in CB-LLMs enable precise concept detection, controlled generation, and safer outputs. The embedded interpretability empowers users to transparently identify harmful content, steer model behavior, and unlearn undesired concepts -- significantly enhancing the safety, reliability, and trustworthiness of LLMs, which are critical capabilities notably absent in existing models. Our code is available at https://github.com/Trustworthy-ML-Lab/CB-LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask1: Development and Reinforcement Learning-Based Control of a Custom Quadruped Robot</title>
<link>https://arxiv.org/abs/2412.08019</link>
<guid>https://arxiv.org/abs/2412.08019</guid>
<content:encoded><![CDATA[
arXiv:2412.08019v2 Announce Type: replace-cross 
Abstract: In this work, we present the design, development, and experimental validation of a custom-built quadruped robot, Ask1. The Ask1 robot shares similar morphology with the Unitree Go1, but features custom hardware components and a different control architecture. We transfer and extend previous reinforcement learning (RL)-based control methods to the Ask1 robot, demonstrating the applicability of our approach in real-world scenarios. By eliminating the need for Adversarial Motion Priors (AMP) and reference trajectories, we introduce a novel reward function to guide the robot's motion style. We demonstrate the generalization capability of the proposed RL algorithm by training it on both the Go1 and Ask1 robots. Simulation and real-world experiments validate the effectiveness of this method, showing that Ask1, like the Go1, is capable of navigating various rugged terrains.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking</title>
<link>https://arxiv.org/abs/2501.09751</link>
<guid>https://arxiv.org/abs/2501.09751</guid>
<content:encoded><![CDATA[
arXiv:2501.09751v3 Announce Type: replace-cross 
Abstract: Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, novelty, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, unoriginal, and repetitive outputs. To address these issues, we propose OmniThink, a slow-thinking machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they slowly deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles. Code is available at https://github.com/zjunlp/OmniThink.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Astrocyte-mediated hierarchical modulation enables learning-to-learn in recurrent spiking networks</title>
<link>https://arxiv.org/abs/2501.14539</link>
<guid>https://arxiv.org/abs/2501.14539</guid>
<content:encoded><![CDATA[
arXiv:2501.14539v4 Announce Type: replace-cross 
Abstract: A central feature of biological intelligence is the ability to learn to learn, enabling rapid adaptation to novel tasks and environments. Yet its neural basis remains elusive, particularly regarding intrinsic properties, as conventional models rely on simplified point-neuron approximations that neglect their dynamics. Inspired by astrocyte-mediated neuromodulation, we propose a hierarchically modulated recurrent spiking neural network (HM-RSNN) that models learning-to-learn with regulation of intrinsic neuronal properties at two spatiotemporal scales. Global modulation captures task-dependent gating of plasticity driven by wide-field calcium waves, whereas local adaptation simulates microdomain calcium-mediated fine-tuning of intrinsic properties within task-relevant subspaces. We evaluate HM-RSNN on four cognitive tasks, demonstrating its computational advantages over standard RSNNs and artificial neural networks, and revealing task-dependent adaptations across multiple scales, including intrinsic properties, neuronal specialization, membrane potential dynamics, and network modularity. Converging evidence and biological consistency position HM-RSNN as a biologically grounded framework, providing testable insights into how astrocyte-mediated hierarchical modulation of intrinsic properties shapes multi-scale neural dynamics that support learning-to-learn.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAAGC: Feature Augmentation on Adaptive Geodesic Curve Based on the shape space theory</title>
<link>https://arxiv.org/abs/2501.18619</link>
<guid>https://arxiv.org/abs/2501.18619</guid>
<content:encoded><![CDATA[
arXiv:2501.18619v2 Announce Type: replace-cross 
Abstract: Deep learning models have been widely applied across various domains and industries. However, many fields still face challenges due to limited and insufficient data. This paper proposes a Feature Augmentation on Adaptive Geodesic Curve (FAAGC) method in the pre-shape space to increase data. In the pre-shape space, objects with identical shapes lie on a great circle. Thus, we project deep model representations into the pre-shape space and construct a geodesic curve, i.e., an arc of a great circle, for each class. Feature augmentation is then performed by sampling along these geodesic paths. Extensive experiments demonstrate that FAAGC improves classification accuracy under data-scarce conditions and generalizes well across various feature types.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error-quantified Conformal Inference for Time Series</title>
<link>https://arxiv.org/abs/2502.00818</link>
<guid>https://arxiv.org/abs/2502.00818</guid>
<content:encoded><![CDATA[
arXiv:2502.00818v2 Announce Type: replace-cross 
Abstract: Uncertainty quantification in time series prediction is challenging due to the temporal dependence and distribution shift on sequential data. Conformal inference provides a pivotal and flexible instrument for assessing the uncertainty of machine learning models through prediction sets. Recently, a series of online conformal inference methods updated thresholds of prediction sets by performing online gradient descent on a sequence of quantile loss functions. A drawback of such methods is that they only use the information of revealed non-conformity scores via miscoverage indicators but ignore error quantification, namely the distance between the non-conformity score and the current threshold. To accurately leverage the dynamic of miscoverage error, we propose \textit{Error-quantified Conformal Inference} (ECI) by smoothing the quantile loss function. ECI introduces a continuous and adaptive feedback scale with the miscoverage error, rather than simple binary feedback in existing methods. We establish a long-term coverage guarantee for ECI under arbitrary dependence and distribution shift. The extensive experimental results show that ECI can achieve valid miscoverage control and output tighter prediction sets than other baselines.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Match Made in Heaven? Matching Test Cases and Vulnerabilities With the VUTECO Approach</title>
<link>https://arxiv.org/abs/2502.03365</link>
<guid>https://arxiv.org/abs/2502.03365</guid>
<content:encoded><![CDATA[
arXiv:2502.03365v2 Announce Type: replace-cross 
Abstract: Software vulnerabilities are commonly detected via static analysis, penetration testing, and fuzzing. They can also be found by running unit tests - so-called vulnerability-witnessing tests - that stimulate the security-sensitive behavior with crafted inputs. Developing such tests is difficult and time-consuming; thus, automated data-driven approaches could help developers intercept vulnerabilities earlier. However, training and validating such approaches require a lot of data, which is currently scarce. This paper introduces VUTECO, a deep learning-based approach for collecting instances of vulnerability-witnessing tests from Java repositories. VUTECO carries out two tasks: (1) the "Finding" task to determine whether a test case is security-related, and (2) the "Matching" task to relate a test case to the exact vulnerability it is witnessing. VUTECO successfully addresses the Finding task, achieving perfect precision and 0.83 F0.5 score on validated test cases in VUL4J and returning 102 out of 145 (70%) correct security-related test cases from 244 open-source Java projects. Despite showing sufficiently good performance for the Matching task - i.e., 0.86 precision and 0.68 F0.5 score - VUTECO failed to retrieve any valid match in the wild. Nevertheless, we observed that in almost all of the matches, the test case was still security-related despite being matched to the wrong vulnerability. In the end, VUTECO can help find vulnerability-witnessing tests, though the matching with the right vulnerability is yet to be solved; the findings obtained lay the stepping stone for future research on the matter.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHIRLA: Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis</title>
<link>https://arxiv.org/abs/2502.06681</link>
<guid>https://arxiv.org/abs/2502.06681</guid>
<content:encoded><![CDATA[
arXiv:2502.06681v2 Announce Type: replace-cross 
Abstract: Person re-identification (Re-ID) is a key challenge in computer vision, requiring the matching of individuals across cameras, locations, and time. While most research focuses on short-term scenarios with minimal appearance changes, real-world applications demand robust systems that handle long-term variations caused by clothing and physical changes. We present CHIRLA, Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis, a novel dataset designed for video-based long-term person Re-ID. CHIRLA was recorded over seven months in four connected indoor environments using seven strategically placed cameras, capturing realistic movements with substantial clothing and appearance variability. The dataset includes 22 individuals, more than five hours of video, and about 1M bounding boxes with identity annotations obtained through semi-automatic labeling. We also define benchmark protocols for person tracking and Re-ID, covering diverse and challenging scenarios such as occlusion, reappearance, and multi-camera conditions. By introducing this comprehensive benchmark, we aim to facilitate the development and evaluation of Re-ID algorithms that can reliably perform in challenging, long-term real-world scenarios. The benchmark code is publicly available at: https://github.com/bdager/CHIRLA.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty quantification for Markov chain induced martingales with application to temporal difference learning</title>
<link>https://arxiv.org/abs/2502.13822</link>
<guid>https://arxiv.org/abs/2502.13822</guid>
<content:encoded><![CDATA[
arXiv:2502.13822v2 Announce Type: replace-cross 
Abstract: We establish novel and general high-dimensional concentration inequalities and Berry-Esseen bounds for vector-valued martingales induced by Markov chains. We apply these results to analyze the performance of the Temporal Difference (TD) learning algorithm with linear function approximations, a widely used method for policy evaluation in Reinforcement Learning (RL), obtaining a sharp high-probability consistency guarantee that matches the asymptotic variance up to logarithmic factors. Furthermore, we establish an $O(T^{-\frac{1}{4}}\log T)$ distributional convergence rate for the Gaussian approximation of the TD estimator, measured in convex distance. Our martingale bounds are of broad applicability, and our analysis of TD learning provides new insights into statistical inference for RL algorithms, bridging gaps between classical stochastic approximation theory and modern RL applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Robustness and Accuracy of Text Watermarking Under Real-World Cross-Lingual Manipulations</title>
<link>https://arxiv.org/abs/2502.16699</link>
<guid>https://arxiv.org/abs/2502.16699</guid>
<content:encoded><![CDATA[
arXiv:2502.16699v2 Announce Type: replace-cross 
Abstract: We present a study to benchmark representative watermarking methods in cross-lingual settings. The current literature mainly focuses on the evaluation of watermarking methods for the English language. However, the literature for evaluating watermarking in cross-lingual settings is scarce. This results in overlooking important adversary scenarios in which a cross-lingual adversary could be in, leading to a gray area of practicality over cross-lingual watermarking. In this paper, we evaluate four watermarking methods in four different and vocabulary rich languages. Our experiments investigate the quality of text under different watermarking procedure and the detectability of watermarks with practical translation attack scenarios. Specifically, we investigate practical scenarios that an adversary with cross-lingual knowledge could take, and evaluate whether current watermarking methods are suitable for such scenarios. Finally, from our findings, we draw key insights about watermarking in cross-lingual settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic data enables context-aware bioacoustic sound event detection</title>
<link>https://arxiv.org/abs/2503.00296</link>
<guid>https://arxiv.org/abs/2503.00296</guid>
<content:encoded><![CDATA[
arXiv:2503.00296v2 Announce Type: replace-cross 
Abstract: We propose a methodology for training foundation models that enhances their in-context learning capabilities within the domain of bioacoustic signal processing. We use synthetically generated training data, introducing a domain-randomization-based pipeline that constructs diverse acoustic scenes with temporally strong labels. We generate over 8.8 thousand hours of strongly-labeled audio and train a query-by-example, transformer-based model to perform few-shot bioacoustic sound event detection. Our second contribution is a public benchmark of 13 diverse few-shot bioacoustics tasks. Our model outperforms previously published methods, and improves relative to other training-free methods by $64\%$. We demonstrate that this is due to increase in model size and data scale, as well as algorithmic improvements. We make our trained model available via an API, to provide ecologists and ethologists with a training-free tool for bioacoustic sound event detection.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust detection of overlapping bioacoustic sound events</title>
<link>https://arxiv.org/abs/2503.02389</link>
<guid>https://arxiv.org/abs/2503.02389</guid>
<content:encoded><![CDATA[
arXiv:2503.02389v2 Announce Type: replace-cross 
Abstract: We propose a method for accurately detecting bioacoustic sound events that is robust to overlapping events, a common issue in domains such as ethology, ecology and conservation. While standard methods employ a frame-based, multi-label approach, we introduce an onset-based detection method which we name Voxaboxen. It takes inspiration from object detection methods in computer vision, but simultaneously takes advantage of recent advances in self-supervised audio encoders. For each time window, Voxaboxen predicts whether it contains the start of a vocalization and how long the vocalization is. It also does the same in reverse, predicting whether each window contains the end of a vocalization, and how long ago it started. The two resulting sets of bounding boxes are then fused using a graph-matching algorithm. We also release a new dataset designed to measure performance on detecting overlapping vocalizations. This consists of recordings of zebra finches annotated with temporally-strong labels and showing frequent overlaps. We test Voxaboxen on seven existing data sets and on our new data set. We compare Voxaboxen to natural baselines and existing sound event detection methods and demonstrate SotA results. Further experiments show that improvements are robust to frequent vocalization overlap.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomized Quasi-Monte Carlo Features for Kernel Approximation</title>
<link>https://arxiv.org/abs/2503.06041</link>
<guid>https://arxiv.org/abs/2503.06041</guid>
<content:encoded><![CDATA[
arXiv:2503.06041v2 Announce Type: replace-cross 
Abstract: We investigate the application of randomized quasi-Monte Carlo (RQMC) methods in random feature approximations for kernel-based learning. Compared to the classical Monte Carlo (MC) approach \citep{rahimi2007random}, RQMC improves the deterministic approximation error bound from $O_P(1/\sqrt{M})$ to $O(1/M)$ (up to logarithmic factors), matching the rate achieved by quasi-Monte Carlo (QMC) methods \citep{huangquasi}. Beyond the deterministic error bound guarantee, we further establish additional average error bounds for RQMC features: some requiring weaker assumptions and others significantly reducing the exponent of the logarithmic factor. In the context of kernel ridge regression, we show that RQMC features offer computational advantages over MC features while preserving the same statistical error rate. Empirical results further show that RQMC methods maintain stable performance in both low and moderately high-dimensional settings, unlike QMC methods, which suffer from significant performance degradation as dimension increases.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through Event-Gated Cognition</title>
<link>https://arxiv.org/abs/2503.06220</link>
<guid>https://arxiv.org/abs/2503.06220</guid>
<content:encoded><![CDATA[
arXiv:2503.06220v3 Announce Type: replace-cross 
Abstract: With the rise of real-world human-AI interaction applications, such as AI assistants, the need for Streaming Video Dialogue is critical. To address this need, we introduce StreamMind, a video LLM framework that achieves ultra-FPS streaming video processing (100 fps on a single A100) and enables proactive, always-on responses in real time, without explicit user intervention.
  To solve the key challenge of the contradiction between linear video streaming speed and quadratic transformer computation cost, we propose a novel perception-cognition interleaving paradigm named ''event-gated LLM invocation'', in contrast to the existing per-time-step LLM invocation. By introducing a Cognition Gate network between the video encoder and the LLM, LLM is only invoked when relevant events occur. To realize the event feature extraction with constant cost, we propose Event-Preserving Feature Extractor (EPFE) based on state-space method, generating a single perception token for spatiotemporal features. These techniques enable the video LLM with full-FPS perception and real-time cognition response.
  Experiments on Ego4D and SoccerNet streaming tasks, as well as standard offline benchmarks, demonstrate state-of-the-art performance in both model capability and real-time efficiency, paving the way for ultra-high-FPS applications, such as Game AI and interactive media. The code and data is available at https://aka.ms/StreamMind.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2503.13111</link>
<guid>https://arxiv.org/abs/2503.13111</guid>
<content:encoded><![CDATA[
arXiv:2503.13111v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse spatial tasks including spatial relationship prediction, metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, a strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Latent Fusion of ECG Leads for Early Assessment of Pulmonary Hypertension</title>
<link>https://arxiv.org/abs/2503.13470</link>
<guid>https://arxiv.org/abs/2503.13470</guid>
<content:encoded><![CDATA[
arXiv:2503.13470v2 Announce Type: replace-cross 
Abstract: Recent advancements in early assessment of pulmonary hypertension (PH) primarily focus on applying machine learning methods to centralized diagnostic modalities, such as 12-lead electrocardiogram (12L-ECG). Despite their potential, these approaches fall short in decentralized clinical settings, e.g., point-of-care and general practice, where handheld 6-lead ECG (6L-ECG) can offer an alternative but is limited by the scarcity of labeled data for developing reliable models. To address this, we propose a lead-specific electrocardiogram multimodal variational autoencoder (\textsc{LS-EMVAE}), which incorporates a hierarchical modality expert (HiME) fusion mechanism and a latent representation alignment loss. HiME combines mixture-of-experts and product-of-experts to enable flexible, adaptive latent fusion, while the alignment loss improves coherence among lead-specific and shared representations. To alleviate data scarcity and enhance representation learning, we adopt a transfer learning strategy: the model is first pre-trained on a large unlabeled 12L-ECG dataset and then fine-tuned on smaller task-specific labeled 6L-ECG datasets. We validate \textsc{LS-EMVAE} across two retrospective cohorts in a 6L-ECG setting: 892 subjects from the ASPIRE registry for (1) PH detection and (2) phenotyping pre-/post-capillary PH, and 16,416 subjects from UK Biobank for (3) predicting elevated pulmonary atrial wedge pressure, where it consistently outperforms unimodal and multimodal baseline methods and demonstrates strong generalizability and interpretability. The code is available at https://github.com/Shef-AIRE/LS-EMVAE.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond SHAP and Anchors: A large-scale experiment on how developers struggle to design meaningful end-user explanations</title>
<link>https://arxiv.org/abs/2503.15512</link>
<guid>https://arxiv.org/abs/2503.15512</guid>
<content:encoded><![CDATA[
arXiv:2503.15512v2 Announce Type: replace-cross 
Abstract: Modern machine learning produces models that are impossible for users or developers to fully understand--raising concerns about trust, oversight, safety, and human dignity when they are integrated into software products. Transparency and explainability methods aim to provide some help in understanding models, but it remains challenging for developers to design explanations that are understandable to target users and effective for their purpose. Emerging guidelines and regulations set goals but may not provide effective actionable guidance to developers. In a large-scale experiment with 124 participants, we explored how developers approach providing end-user explanations, including what challenges they face, and to what extent specific policies can guide their actions. We investigated whether and how specific forms of policy guidance help developers design explanations and provide evidence for policy compliance for an ML-powered screening tool for diabetic retinopathy. Participants across the board struggled to produce quality explanations and comply with the provided policies. Contrary to our expectations, we found that the nature and specificity of policy guidance had little effect. We posit that participant noncompliance is in part due to a failure to imagine and anticipate the needs of non-technical stakeholders. Drawing on cognitive process theory and the sociological imagination to contextualize participants' failure, we recommend educational interventions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional Neural Networks Can (Meta-)Learn the Same-Different Relation</title>
<link>https://arxiv.org/abs/2503.23212</link>
<guid>https://arxiv.org/abs/2503.23212</guid>
<content:encoded><![CDATA[
arXiv:2503.23212v3 Announce Type: replace-cross 
Abstract: While convolutional neural networks (CNNs) have come to match and exceed human performance in many settings, the tasks these models optimize for are largely constrained to the level of individual objects, such as classification and captioning. Humans remain vastly superior to CNNs in visual tasks involving relations, including the ability to identify two objects as `same' or `different'. A number of studies have shown that while CNNs can be coaxed into learning the same-different relation in some settings, they tend to generalize poorly to other instances of this relation. In this work we show that the same CNN architectures that fail to generalize the same-different relation with conventional training are able to succeed when trained via meta-learning, which explicitly encourages abstraction and generalization across tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Model Predictive Control for Deep Brain Stimulation in Parkinson's Disease</title>
<link>https://arxiv.org/abs/2504.00618</link>
<guid>https://arxiv.org/abs/2504.00618</guid>
<content:encoded><![CDATA[
arXiv:2504.00618v2 Announce Type: replace-cross 
Abstract: We present a nonlinear data-driven Model Predictive Control (MPC) algorithm for deep brain stimulation (DBS) for the treatment of Parkinson's disease (PD). Although DBS is typically implemented in open-loop, closed-loop DBS (CLDBS) uses the amplitude of neural oscillations in specific frequency bands (e.g. beta 13-30 Hz) as a feedback signal, resulting in improved treatment outcomes with reduced side effects and slower rates of patient habituation to stimulation. To date, CLDBS has only been implemented in vivo with simple algorithms such as proportional, proportional-integral, and thresholded switching control. Our approach employs a multi-step predictor based on differences of input-convex neural networks to model the future evolution of beta oscillations. The use of a multi-step predictor enhances prediction accuracy over the optimization horizon and simplifies online computation. In tests using a simulated model of beta-band activity response and data from PD patients, we achieve reductions of more than 20% in both tracking error and control activity in comparison with existing CLDBS algorithms. The proposed control strategy provides a generalizable data-driven technique that can be applied to the treatment of PD and other diseases targeted by CLDBS, as well as to other neuromodulation techniques.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KD$^{2}$M: A unifying framework for feature knowledge distillation</title>
<link>https://arxiv.org/abs/2504.01757</link>
<guid>https://arxiv.org/abs/2504.01757</guid>
<content:encoded><![CDATA[
arXiv:2504.01757v3 Announce Type: replace-cross 
Abstract: Knowledge Distillation (KD) seeks to transfer the knowledge of a teacher, towards a student neural net. This process is often done by matching the networks' predictions (i.e., their output), but, recently several works have proposed to match the distributions of neural nets' activations (i.e., their features), a process known as \emph{distribution matching}. In this paper, we propose an unifying framework, Knowledge Distillation through Distribution Matching (KD$^{2}$M), which formalizes this strategy. Our contributions are threefold. We i) provide an overview of distribution metrics used in distribution matching, ii) benchmark on computer vision datasets, and iii) derive new theoretical results for KD.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ground Cost for Optimal Transport of Angular Velocity</title>
<link>https://arxiv.org/abs/2504.03190</link>
<guid>https://arxiv.org/abs/2504.03190</guid>
<content:encoded><![CDATA[
arXiv:2504.03190v2 Announce Type: replace-cross 
Abstract: We revisit the optimal transport problem over angular velocity dynamics given by the controlled Euler equation. The solution of this problem enables stochastic guidance of spin states of a rigid body (e.g., spacecraft) over a hard deadline constraint by transferring a given initial state statistics to a desired terminal state statistics. This is an instance of generalized optimal transport over a nonlinear dynamical system. While prior work has reported existence-uniqueness and numerical solution of this dynamical optimal transport problem, here we present structural results about the equivalent Kantorovich a.k.a. optimal coupling formulation. Specifically, we focus on deriving the ground cost for the associated Kantorovich optimal coupling formulation. The ground cost is equal to the cost of transporting unit amount of mass from a specific realization of the initial or source joint probability measure to a realization of the terminal or target joint probability measure, and determines the Kantorovich formulation. Finding the ground cost leads to solving a structured deterministic nonlinear optimal control problem, which is shown to be amenable to an analysis technique pioneered by Athans et al. We show that such techniques have broader applicability in determining the ground cost (thus Kantorovich formulation) for a class of generalized optimal mass transport problems involving nonlinear dynamics with translated norm-invariant drift.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models</title>
<link>https://arxiv.org/abs/2504.03624</link>
<guid>https://arxiv.org/abs/2504.03624</guid>
<content:encoded><![CDATA[
arXiv:2504.03624v4 Announce Type: replace-cross 
Abstract: As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\times$ faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Approximation with XL MIMO Systems: OTA Classification via Trainable Analog Combining</title>
<link>https://arxiv.org/abs/2504.12758</link>
<guid>https://arxiv.org/abs/2504.12758</guid>
<content:encoded><![CDATA[
arXiv:2504.12758v2 Announce Type: replace-cross 
Abstract: In this paper, we show that an eXtremely Large (XL) Multiple-Input Multiple-Output (MIMO) wireless system with appropriate analog combining components exhibits the properties of a universal function approximator, similar to a feedforward neural network. By treating the channel coefficients as the random nodes of a hidden layer and the receiver's analog combiner as a trainable output layer, we cast the XL MIMO system to the Extreme Learning Machine (ELM) framework, leading to a novel formulation for Over-The-Air (OTA) edge inference without requiring traditional digital processing nor pre-processing at the transmitter. Through theoretical analysis and numerical evaluation, we showcase that XL-MIMO-ELM enables near-instantaneous training and efficient classification, even in varying fading conditions, suggesting the paradigm shift of beyond massive MIMO systems as OTA artificial neural networks alongside their profound communications role. Compared to deep learning approaches and conventional ELMs, the proposed framework achieves on par performance with orders of magnitude lower complexity, making it highly attractive for inference tasks with ultra low power wireless devices.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Hyperspectral Images Into Chemical Maps: A Novel End-to-End Deep Learning Approach</title>
<link>https://arxiv.org/abs/2504.14131</link>
<guid>https://arxiv.org/abs/2504.14131</guid>
<content:encoded><![CDATA[
arXiv:2504.14131v4 Announce Type: replace-cross 
Abstract: Current approaches to chemical map generation from hyperspectral images are based on models such as partial least squares (PLS) regression, generating pixel-wise predictions that do not consider spatial context and suffer from a high degree of noise. This study proposes an end-to-end deep learning approach using a modified version of U-Net and a custom loss function to directly obtain chemical maps from hyperspectral images, skipping all intermediate steps required for traditional pixel-wise analysis. The U-Net is compared with the traditional PLS regression on a real dataset of pork belly samples with associated mean fat reference values. The U-Net obtains a test set root mean squared error of between 9% and 13% lower than that of PLS regression on the task of mean fat prediction. At the same time, U-Net generates fine detail chemical maps where 99.91% of the variance is spatially correlated. Conversely, only 2.53% of the variance in the PLS-generated chemical maps is spatially correlated, indicating that each pixel-wise prediction is largely independent of neighboring pixels. Additionally, while the PLS-generated chemical maps contain predictions far beyond the physically possible range of 0-100%, U-Net learns to stay inside this range. Thus, the findings of this study indicate that U-Net is superior to PLS for chemical map generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport</title>
<link>https://arxiv.org/abs/2505.24759</link>
<guid>https://arxiv.org/abs/2505.24759</guid>
<content:encoded><![CDATA[
arXiv:2505.24759v2 Announce Type: replace-cross 
Abstract: Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan, transforming diffuse transport matrices into sparse, interpretable correspondences. Through systematic evaluation on synthetic datasets, we demonstrate that OT-MESH achieves near-optimal matching accuracy with computational efficiency, while maintaining remarkable robustness to noise. Compared to other OT-based methods like RefCM, OT-MESH provides speedup while achieving comparable accuracy. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models</title>
<link>https://arxiv.org/abs/2506.11798</link>
<guid>https://arxiv.org/abs/2506.11798</guid>
<content:encoded><![CDATA[
arXiv:2506.11798v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at https://github.com/dess-mannheim/european_parliament_simulation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transit for All: Mapping Equitable Bike2Subway Connection using Region Representation Learning</title>
<link>https://arxiv.org/abs/2506.15113</link>
<guid>https://arxiv.org/abs/2506.15113</guid>
<content:encoded><![CDATA[
arXiv:2506.15113v2 Announce Type: replace-cross 
Abstract: Ensuring equitable public transit access remains challenging, particularly in densely populated cities like New York City (NYC), where low-income and minority communities often face limited transit accessibility. Bike-sharing systems (BSS) can bridge these equity gaps by providing affordable first- and last-mile connections. However, strategically expanding BSS into underserved neighborhoods is difficult due to uncertain bike-sharing demand at newly planned ("cold-start") station locations and limitations in traditional accessibility metrics that may overlook realistic bike usage potential. We introduce Transit for All (TFA), a spatial computing framework designed to guide the equitable expansion of BSS through three components: (1) spatially-informed bike-sharing demand prediction at cold-start stations using region representation learning that integrates multimodal geospatial data, (2) comprehensive transit accessibility assessment leveraging our novel weighted Public Transport Accessibility Level (wPTAL) by combining predicted bike-sharing demand with conventional transit accessibility metrics, and (3) strategic recommendations for new bike station placements that consider potential ridership and equity enhancement. Using NYC as a case study, we identify transit accessibility gaps that disproportionately impact low-income and minority communities in historically underserved neighborhoods. Our results show that strategically placing new stations guided by wPTAL notably reduces disparities in transit access related to economic and demographic factors. From our study, we demonstrate that TFA provides practical guidance for urban planners to promote equitable transit and enhance the quality of life in underserved urban communities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2506.21109</link>
<guid>https://arxiv.org/abs/2506.21109</guid>
<content:encoded><![CDATA[
arXiv:2506.21109v2 Announce Type: replace-cross 
Abstract: Remote sensing change detection is essential for monitoring urban expansion, disaster assessment, and resource management, offering timely, accurate, and large-scale insights into dynamic landscape transformations. While deep learning has revolutionized change detection, the increasing complexity and computational demands of modern models have not necessarily translated into significant accuracy gains. Instead of following this trend, this study explores a more efficient approach, focusing on lightweight models that maintain high accuracy while minimizing resource consumption, which is an essential requirement for on-satellite processing. To this end, we propose FlickCD, which means quick flick then get great results, pushing the boundaries of the performance-resource trade-off. FlickCD introduces an Enhanced Difference Module (EDM) to amplify critical feature differences between temporal phases while suppressing irrelevant variations such as lighting and weather changes, thereby reducing computational costs in the subsequent change decoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion Blocks, leveraging Shifted Window Self-Attention (SWSA) and Efficient Global Self-Attention (EGSA) to effectively capture semantic information at multiple scales, preserving both coarse- and fine-grained changes. Extensive experiments on four benchmark datasets demonstrate that FlickCD reduces computational and storage overheads by more than an order of magnitude while achieving state-of-the-art (SOTA) performance or incurring only a minor (<1% F1) accuracy trade-off. The implementation code is publicly available at https://github.com/xulsh8/FlickCD.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs</title>
<link>https://arxiv.org/abs/2506.22146</link>
<guid>https://arxiv.org/abs/2506.22146</guid>
<content:encoded><![CDATA[
arXiv:2506.22146v3 Announce Type: replace-cross 
Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual reasoning is often limited by the binding problem: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current VLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces VISER (Visual Input Structure for Enhanced Reasoning), a simple yet effective intervention: augmenting visual inputs with low-level spatial structures and pairing this with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks. Specifically, VISER improves GPT-4o visual search accuracy by 25.00%, increases counting accuracy by 26.83%, reduces edit distance error in scene description by 0.32, and enhances performance on spatial relationship tasks by 9.50% on a 2D synthetic dataset. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. VISER enhances binding only with a single-query inference, underscoring the importance of visual input design over purely linguistically-based approaches. These findings suggest that low-level visual structuring is a powerful and underexplored direction for improving compositional visual reasoning and could serve as a general strategy for enhancing VLM performance on spatially grounded tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles</title>
<link>https://arxiv.org/abs/2507.04139</link>
<guid>https://arxiv.org/abs/2507.04139</guid>
<content:encoded><![CDATA[
arXiv:2507.04139v2 Announce Type: replace-cross 
Abstract: Ensuring safe transition of control in automated vehicles requires an accurate and timely assessment of driver readiness. This paper introduces Driver-Net, a novel deep learning framework that fuses multi-camera inputs to estimate driver take-over readiness. Unlike conventional vision-based driver monitoring systems that focus on head pose or eye gaze, Driver-Net captures synchronised visual cues from the driver's head, hands, and body posture through a triple-camera setup. The model integrates spatio-temporal data using a dual-path architecture, comprising a Context Block and a Feature Block, followed by a cross-modal fusion strategy to enhance prediction accuracy. Evaluated on a diverse dataset collected from the University of Leeds Driving Simulator, the proposed method achieves an accuracy of up to 95.8% in driver readiness classification. This performance significantly enhances existing approaches and highlights the importance of multimodal and multi-view fusion. As a real-time, non-intrusive solution, Driver-Net contributes meaningfully to the development of safer and more reliable automated vehicles and aligns with new regulatory mandates and upcoming safety standards.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELK: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques</title>
<link>https://arxiv.org/abs/2507.11506</link>
<guid>https://arxiv.org/abs/2507.11506</guid>
<content:encoded><![CDATA[
arXiv:2507.11506v2 Announce Type: replace-cross 
Abstract: To meet the increasing demand of deep learning (DL) models, AI chips are employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency interconnect for direct inter-core data exchange. However, it is not easy to explore the efficiency of these inter-core connected AI (ICCA) chips, due to a fundamental tussle among compute (per-core execution), communication (inter-core data exchange), and I/O (off-chip data access).
  In this paper, we develop Elk, a DL compiler framework to maximize the efficiency of ICCA chips by jointly trading off all the three performance factors discussed above. Elk structures these performance factors into configurable parameters and forms a global trade-off space in the DL compiler. To systematically explore this space and maximize overall efficiency, Elk employs a new inductive operator scheduling policy and a cost-aware on-chip memory allocation algorithm. It generates globally optimized execution plans that best overlap off-chip data loading and on-chip execution. To examine the efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different interconnect network topologies. Elk achieves 94% of the ideal roofline performance of ICCA chips on average, showing the benefits of supporting large DL models on ICCA chips. We also show Elk's capability of enabling architecture design space exploration for new ICCA chip development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models</title>
<link>https://arxiv.org/abs/2507.18504</link>
<guid>https://arxiv.org/abs/2507.18504</guid>
<content:encoded><![CDATA[
arXiv:2507.18504v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits sparse feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as LLMs' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates sparse dependency graphs into LLMs' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing LLM-based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascading and Proxy Membership Inference Attacks</title>
<link>https://arxiv.org/abs/2507.21412</link>
<guid>https://arxiv.org/abs/2507.21412</guid>
<content:encoded><![CDATA[
arXiv:2507.21412v3 Announce Type: replace-cross 
Abstract: A Membership Inference Attack (MIA) assesses how much a trained machine learning model reveals about its training data by determining whether specific query instances were included in the dataset. We classify existing MIAs into adaptive or non-adaptive, depending on whether the adversary is allowed to train shadow models on membership queries. In the adaptive setting, where the adversary can train shadow models after accessing query instances, we highlight the importance of exploiting membership dependencies between instances and propose an attack-agnostic framework called Cascading Membership Inference Attack (CMIA), which incorporates membership dependencies via conditional shadow training to boost membership inference performance.
  In the non-adaptive setting, where the adversary is restricted to training shadow models before obtaining membership queries, we introduce Proxy Membership Inference Attack (PMIA). PMIA employs a proxy selection strategy that identifies samples with similar behaviors to the query instance and uses their behaviors in shadow models to perform a membership posterior odds test for membership inference. We provide theoretical analyses for both attacks, and extensive experimental results demonstrate that CMIA and PMIA substantially outperform existing MIAs in both settings, particularly in the low false-positive regime, which is crucial for evaluating privacy risks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning</title>
<link>https://arxiv.org/abs/2508.01131</link>
<guid>https://arxiv.org/abs/2508.01131</guid>
<content:encoded><![CDATA[
arXiv:2508.01131v2 Announce Type: replace-cross 
Abstract: In this work, we study the problem of data retrieval for few-shot imitation learning: selecting data from a large dataset to train a performant policy for a specific task, given only a few target demonstrations. Prior methods retrieve data using a single-feature distance heuristic, assuming that the best demonstrations are those that most closely resemble the target examples in visual, semantic, or motion space. However, this approach captures only a subset of the relevant information and can introduce detrimental demonstrations, e.g., retrieving data from unrelated tasks due to similar scene layouts, or selecting similar motions from tasks with divergent goals. We present COLLAGE, a method for COLLective data AGgrEgation in few-shot imitation learning that uses an adaptive late fusion mechanism to guide the selection of relevant demonstrations based on a task-specific combination of multiple cues. COLLAGE follows a simple, flexible, and efficient recipe: it assigns weights to subsets of the dataset that are pre-selected using a single feature (e.g., appearance, shape, or language similarity), based on how well a policy trained on each subset predicts actions in the target demonstrations. These weights are then used to perform importance sampling during policy training, sampling data more densely or sparsely according to estimated relevance. COLLAGE is general and feature-agnostic, allowing it to combine any number of subsets selected by any retrieval heuristic, and to identify which subsets provide the greatest benefit for the target task. In extensive experiments, COLLAGE outperforms state-of-the-art retrieval and multi-task learning approaches by 5.1% in simulation across 10 tasks, and by 16.6% in the real world across 6 tasks, where we perform retrieval from the large-scale DROID dataset. More information at https://robin-lab.cs.utexas.edu/COLLAGE .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Viability of perturbative expansion for quantum field theories on neurons</title>
<link>https://arxiv.org/abs/2508.03810</link>
<guid>https://arxiv.org/abs/2508.03810</guid>
<content:encoded><![CDATA[
arXiv:2508.03810v2 Announce Type: replace-cross 
Abstract: Neural Network (NN) architectures that break statistical independence of parameters have been proposed as a new approach for simulating local quantum field theories (QFTs). In the infinite neuron number limit, single-layer NNs can exactly reproduce QFT results. This paper examines the viability of this architecture for perturbative calculations of local QFTs for finite neuron number $N$ using scalar $\phi^4$ theory in $d$ Euclidean dimensions as an example. We find that the renormalized $O(1/N)$ corrections to two- and four-point correlators yield perturbative series which are sensitive to the ultraviolet cut-off and therefore have a weak convergence. We propose a modification to the architecture to improve this convergence and discuss constraints on the parameters of the theory and the scaling of N which allow us to extract accurate field theory results.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures</title>
<link>https://arxiv.org/abs/2508.07423</link>
<guid>https://arxiv.org/abs/2508.07423</guid>
<content:encoded><![CDATA[
arXiv:2508.07423v3 Announce Type: replace-cross 
Abstract: As the particle physics community needs higher and higher precisions in order to test our current model of the subatomic world, larger and larger datasets are necessary. With upgrades scheduled for the detectors of colliding-beam experiments around the world, and specifically at the Large Hadron Collider at CERN, more collisions and more complex interactions are expected. This directly implies an increase in data produced and consequently in the computational resources needed to process them. At CERN, the amount of data produced is gargantuan. This is why the data have to be heavily filtered and selected in real time before being permanently stored. This data can then be used to perform physics analyses, in order to expand our current understanding of the universe and improve the Standard Model of physics. This real-time filtering, known as triggering, involves complex processing happening often at frequencies as high as 40 MHz. This thesis contributes to understanding how machine learning models can be efficiently deployed in such environments, in order to maximize throughput and minimize energy consumption. Inevitably, modern hardware designed for such tasks and contemporary algorithms are needed in order to meet the challenges posed by the stringent, high-frequency data rates. In this work, I present our graph neural network-based pipeline, developed for charged particle track reconstruction at the LHCb experiment at CERN. The pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely on GPUs. Its performance was compared against the classical tracking algorithms currently in production at LHCb. The pipeline was also accelerated on the FPGA architecture, and its performance in terms of power consumption and processing speed was compared against the GPU implementation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Training-free Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2508.09016</link>
<guid>https://arxiv.org/abs/2508.09016</guid>
<content:encoded><![CDATA[
arXiv:2508.09016v3 Announce Type: replace-cross 
Abstract: The alignment of large language models (LLMs) aims to ensure their outputs adhere to human values, ethical standards, and legal norms. Traditional alignment methods often rely on resource-intensive fine-tuning (FT), which may suffer from knowledge degradation and face challenges in scenarios where the model accessibility or computational resources are constrained. In contrast, training-free (TF) alignment techniques--leveraging in-context learning, decoding-time adjustments, and post-generation corrections--offer a promising alternative by enabling alignment without heavily retraining LLMs, making them adaptable to both open-source and closed-source environments. This paper presents the first systematic review of TF alignment methods, categorizing them by stages of pre-decoding, in-decoding, and post-decoding. For each stage, we provide a detailed examination from the viewpoint of LLMs and multimodal LLMs (MLLMs), highlighting their mechanisms and limitations. Furthermore, we identify key challenges and future directions, paving the way for more inclusive and effective TF alignment techniques. By synthesizing and organizing the rapidly growing body of research, this survey offers a guidance for practitioners and advances the development of safer and more reliable LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery</title>
<link>https://arxiv.org/abs/2508.09183</link>
<guid>https://arxiv.org/abs/2508.09183</guid>
<content:encoded><![CDATA[
arXiv:2508.09183v2 Announce Type: replace-cross 
Abstract: Quantum computation has demonstrated a promising alternative to solving the NP-hard combinatorial problems. Specifically, when it comes to optimization, classical approaches become intractable to account for large-scale solutions. Specifically, we investigate quantum computing to solve the large-scale Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this regard, a Reinforcement Learning (RL) framework augmented with a Parametrized Quantum Circuit (PQC) is designed to minimize the travel time in a realistic last-mile on-demand delivery. A novel problem-specific encoding quantum circuit with an entangling and variational layer is proposed. Moreover, Proximal Policy Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are designed for comparison through numerical experiments, highlighting the superiority of the proposed method in terms of the scale of the solution and training complexity while incorporating the real-world constraints.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whisper Smarter, not Harder: Adversarial Attack on Partial Suppression</title>
<link>https://arxiv.org/abs/2508.09994</link>
<guid>https://arxiv.org/abs/2508.09994</guid>
<content:encoded><![CDATA[
arXiv:2508.09994v2 Announce Type: replace-cross 
Abstract: Currently, Automatic Speech Recognition (ASR) models are deployed in an extensive range of applications. However, recent studies have demonstrated the possibility of adversarial attack on these models which could potentially suppress or disrupt model output. We investigate and verify the robustness of these attacks and explore if it is possible to increase their imperceptibility. We additionally find that by relaxing the optimisation objective from complete suppression to partial suppression, we can further decrease the imperceptibility of the attack. We also explore possible defences against these attacks and show a low-pass filter defence could potentially serve as an effective defence.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages</title>
<link>https://arxiv.org/abs/2508.11854</link>
<guid>https://arxiv.org/abs/2508.11854</guid>
<content:encoded><![CDATA[
arXiv:2508.11854v2 Announce Type: replace-cross 
Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks for efficient novel-view synthesis from static images, how might an adversary tamper images to cause harm? We introduce ComplicitSplat, the first attack that exploits standard 3DGS shading methods to create viewpoint-specific camouflage - colors and textures that change with viewing angle - to embed adversarial content in scene objects that are visible only from specific viewpoints and without requiring access to model architecture or weights. Our extensive experiments show that ComplicitSplat generalizes to successfully attack a variety of popular detector - both single-stage, multi-stage, and transformer-based models on both real-world capture of physical objects and synthetic scenes. To our knowledge, this is the first black-box attack on downstream object detectors using 3DGS, exposing a novel safety risk for applications like autonomous navigation and other mission-critical robotic systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Artificial Intuition in Distinct, Minimalist Classification of Scientific Abstracts for Management of Technology Portfolios</title>
<link>https://arxiv.org/abs/2508.13182</link>
<guid>https://arxiv.org/abs/2508.13182</guid>
<content:encoded><![CDATA[
arXiv:2508.13182v2 Announce Type: replace-cross 
Abstract: Classification of scientific abstracts is useful for strategic activities but challenging to automate because the sparse text provides few contextual clues. Metadata associated with the scientific publication can be used to improve performance but still often requires a semi-supervised setting. Moreover, such schemes may generate labels that lack distinction -- namely, they overlap and thus do not uniquely define the abstract. In contrast, experts label and sort these texts with ease. Here we describe an application of a process we call artificial intuition to replicate the expert's approach, using a Large Language Model (LLM) to generate metadata. We use publicly available abstracts from the United States National Science Foundation to create a set of labels, and then we test this on a set of abstracts from the Chinese National Natural Science Foundation to examine funding trends. We demonstrate the feasibility of this method for research portfolio management, technology scouting, and other strategic activities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Landscape of Non-Equilibrium Memories with Neural Cellular Automata</title>
<link>https://arxiv.org/abs/2508.15726</link>
<guid>https://arxiv.org/abs/2508.15726</guid>
<content:encoded><![CDATA[
arXiv:2508.15726v2 Announce Type: replace-cross 
Abstract: We investigate the landscape of many-body memories: families of local non-equilibrium dynamics that retain information about their initial conditions for thermodynamically long time scales, even in the presence of arbitrary perturbations. In two dimensions, the only well-studied memory is Toom's rule. Using a combination of rigorous proofs and machine learning methods, we show that the landscape of 2D memories is in fact quite vast. We discover memories that correct errors in ways qualitatively distinct from Toom's rule, have ordered phases stabilized by fluctuations, and preserve information only in the presence of noise. Taken together, our results show that physical systems can perform robust information storage in many distinct ways, and demonstrate that the physics of many-body memories is richer than previously realized. Interactive visualizations of the dynamics studied in this work are available at https://memorynca.github.io/2D.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search</title>
<link>https://arxiv.org/abs/2508.15884</link>
<guid>https://arxiv.org/abs/2508.15884</guid>
<content:encoded><![CDATA[
arXiv:2508.15884v2 Announce Type: replace-cross 
Abstract: We present Jet-Nemotron, a new family of hybrid-architecture language models, which matches or exceeds the accuracy of leading full-attention models while significantly improving generation throughput. Jet-Nemotron is developed using Post Neural Architecture Search (PostNAS), a novel neural architecture exploration pipeline that enables efficient model design. Unlike prior approaches, PostNAS begins with a pre-trained full-attention model and freezes its MLP weights, allowing efficient exploration of attention block designs. The pipeline includes four key components: (1) learning optimal full-attention layer placement and elimination, (2) linear attention block selection, (3) designing new attention blocks, and (4) performing hardware-aware hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a comprehensive suite of benchmarks while delivering up to 53.6x generation throughput speedup and 6.1x prefilling speedup. It also achieves higher accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models, such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B total and 2.2B activated parameters.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation</title>
<link>https://arxiv.org/abs/2508.18166</link>
<guid>https://arxiv.org/abs/2508.18166</guid>
<content:encoded><![CDATA[
arXiv:2508.18166v4 Announce Type: replace-cross 
Abstract: Modern app store recommender systems struggle with multiple-category apps, as traditional taxonomies fail to capture overlapping semantics, leading to suboptimal personalization. We propose PCR-CA (Parallel Codebook Representations with Contrastive Alignment), an end-to-end framework for improved CTR prediction. PCR-CA first extracts compact multimodal embeddings from app text, then introduces a Parallel Codebook VQ-AE module that learns discrete semantic representations across multiple codebooks in parallel -- unlike hierarchical residual quantization (RQ-VAE). This design enables independent encoding of diverse aspects (e.g., gameplay, art style), better modeling multiple-category semantics. To bridge semantic and collaborative signals, we employ a contrastive alignment loss at both the user and item levels, enhancing representation learning for long-tail items. Additionally, a dual-attention fusion mechanism combines ID-based and semantic features to capture user interests, especially for long-tail apps. Experiments on a large-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong baselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further validates our approach, showing a +10.52% lift in CTR and a +16.30% improvement in CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new framework has now been fully deployed on the Microsoft Store.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attacks on LLM-based Recommender Systems</title>
<link>https://arxiv.org/abs/2508.18665</link>
<guid>https://arxiv.org/abs/2508.18665</guid>
<content:encoded><![CDATA[
arXiv:2508.18665v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly adapt recommendation systems to different domains. It utilizes in-context learning (ICL), i.e., the prompts, to customize the recommendation functions, which include sensitive historical user-specific item interactions, e.g., implicit feedback like clicked items or explicit product reviews. Such private information may be exposed to novel privacy attack. However, no study has been done on this important issue. We design four membership inference attacks (MIAs), aiming to reveal whether victims' historical interactions have been used by system prompts. They are \emph{direct inquiry, hallucination, similarity, and poisoning attacks}, each of which utilizes the unique features of LLMs or RecSys. We have carefully evaluated them on three LLMs that have been used to develop ICL-LLM RecSys and two well-known RecSys benchmark datasets. The results confirm that the MIA threat on LLM RecSys is realistic: direct inquiry and poisoning attacks showing significantly high attack advantages. We have also analyzed the factors affecting these attacks, such as the number of shots in system prompts and the position of the victim in the shots.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Prompt Optimization with Prompt Distillation</title>
<link>https://arxiv.org/abs/2508.18992</link>
<guid>https://arxiv.org/abs/2508.18992</guid>
<content:encoded><![CDATA[
arXiv:2508.18992v2 Announce Type: replace-cross 
Abstract: Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding</title>
<link>https://arxiv.org/abs/2508.19576</link>
<guid>https://arxiv.org/abs/2508.19576</guid>
<content:encoded><![CDATA[
arXiv:2508.19576v2 Announce Type: replace-cross 
Abstract: With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLM's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We conduct extensive experiments on coding problems to verify the validity of the proposed RL paradigm. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at https://github.com/THUDM/ReST-RL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Potential Biases in LLM-Based Recommender Systems in the Cold Start Setting</title>
<link>https://arxiv.org/abs/2508.20401</link>
<guid>https://arxiv.org/abs/2508.20401</guid>
<content:encoded><![CDATA[
arXiv:2508.20401v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly used for recommendation tasks due to their general-purpose capabilities. While LLMs perform well in rich-context settings, their behavior in cold-start scenarios, where only limited signals such as age, gender, or language are available, raises fairness concerns because they may rely on societal biases encoded during pretraining. We introduce a benchmark specifically designed to evaluate fairness in zero-context recommendation. Our modular pipeline supports configurable recommendation domains and sensitive attributes, enabling systematic and flexible audits of any open-source LLM. Through evaluations of state-of-the-art models (Gemma 3 and Llama 3.2), we uncover consistent biases across recommendation domains (music, movies, and colleges) including gendered and cultural stereotypes. We also reveal a non-linear relationship between model size and fairness, highlighting the need for nuanced analysis.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-inspired probability metrics define a complete, universal space for statistical learning</title>
<link>https://arxiv.org/abs/2508.21086</link>
<guid>https://arxiv.org/abs/2508.21086</guid>
<content:encoded><![CDATA[
arXiv:2508.21086v2 Announce Type: replace-cross 
Abstract: Comparing probability distributions is a core challenge across the natural, social, and computational sciences. Existing methods, such as Maximum Mean Discrepancy (MMD), struggle in high-dimensional and non-compact domains. Here we introduce quantum probability metrics (QPMs), derived by embedding probability measures in the space of quantum states: positive, unit-trace operators on a Hilbert space. This construction extends kernel-based methods and overcomes the incompleteness of MMD on non-compact spaces. Viewed as an integral probability metric (IPM), QPMs have dual functions that uniformly approximate all bounded, uniformly continuous functions on $\mathbb{R}^n$, offering enhanced sensitivity to subtle distributional differences in high dimensions. For empirical distributions, QPMs are readily calculated using eigenvalue methods, with analytic gradients suited for learning and optimization. Although computationally more intensive for large sample sizes ($O(n^3)$ vs. $O(n^2)$), QPMs can significantly improve performance as a drop-in replacement for MMD, as demonstrated in a classic generative modeling task. By combining the rich mathematical framework of quantum mechanics with classical probability theory, this approach lays the foundation for powerful tools to analyze and manipulate probability measures.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review on Influx of Bio-Inspired Algorithms: Critique and Improvement Needs</title>
<link>https://arxiv.org/abs/2506.04238</link>
<guid>https://arxiv.org/abs/2506.04238</guid>
<content:encoded><![CDATA[
<div> evolutionary, swarm intelligence, physics-inspired, ecosystem and plant-based, hybrid approaches

Summary:
This survey paper categorizes bio-inspired algorithms into eight groups, including evolutionary, swarm intelligence, and hybrid approaches, among others. It reviews their principles, strengths, novelty, and limitations, pointing out the need for a more rigorous review before applying them to relevant fields. The paper also critiques the lack of novelty in many algorithms and highlights their suitable usage in various fields such as machine learning and bioinformatics. Recent advances in hybridization, parameter tuning, and adaptive strategies are discussed. The paper identifies open challenges like scalability, convergence, reliability, and interpretability, suggesting directions for future research. Ultimately, this work aims to be a valuable resource for researchers and practitioners interested in the current landscape and future directions of bio-inspired algorithms. 

<br /><br />Summary: <div>
arXiv:2506.04238v3 Announce Type: replace-cross 
Abstract: Bio-inspired algorithms, known as metaphor-based algorithms, utilize natural processes such as evolution, swarm behavior, foraging, and plant growth to solve complex, nonlinear, high-dimensional optimization problems. However, a plethora of these algorithms require a more rigorous review before making them applicable to the relevant fields. This survey categorizes these algorithms into eight groups: evolutionary, swarm intelligence, physics-inspired, ecosystem and plant-based, predator-prey, neural-inspired, human-inspired, and hybrid approaches, and reviews their principles, strengths, novelty, and critical limitations. We provide a critique on the novelty issues of many of these algorithms. We illustrate some of the suitable usage of the prominent algorithms in machine learning, engineering design, bioinformatics, and intelligent systems, and highlight recent advances in hybridization, parameter tuning, and adaptive strategies. Finally, we identify open challenges such as scalability, convergence, reliability, and interpretability to suggest directions for future research. This work aims to serve as a resource for both researchers and practitioners interested in understanding the current landscape and future directions of reliable and authentic advancement of bio-inspired algorithms.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Vectors: Monitoring and Controlling Character Traits in Language Models</title>
<link>https://arxiv.org/abs/2507.21509</link>
<guid>https://arxiv.org/abs/2507.21509</guid>
<content:encoded><![CDATA[
<div> personality vectors, large language models, persona, training data, persona shifts 
Summary: 
Persona vectors are identified in large language models to monitor and control personality traits like evil, sycophancy, and hallucination. These vectors can track fluctuations in the model's persona at deployment and predict shifts during training. Intended and unintended personality changes after finetuning are correlated with shifts along persona vectors, which can be mitigated through post-hoc intervention or prevented using a new steering method. Persona vectors can also flag training data leading to undesirable personality changes at the dataset and individual sample levels. The automated method for extracting persona vectors can be applied to any personality trait based on natural language descriptions. <br /><br />Summary: <div>
arXiv:2507.21509v3 Announce Type: replace-cross 
Abstract: Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-SafeML: Safety Assessment of Quantum Machine Learning via Quantum Distance Metrics</title>
<link>https://arxiv.org/abs/2509.04536</link>
<guid>https://arxiv.org/abs/2509.04536</guid>
<content:encoded><![CDATA[
<div> machine learning, safety-critical systems, quantum computing, Quantum Machine Learning, safety monitoring

Summary:
Q-SafeML is introduced as a safety monitoring approach for Quantum Machine Learning (QML), addressing the unique challenges in safety-critical systems involving quantum computation. This method is based on SafeML but incorporates quantum-centric distance measures to assess model accuracy and ensure confidence in QML outputs. Unlike classical SafeML, which is dataset-driven and classifier-agnostic, Q-SafeML focuses on model-dependent, post-classification evaluation due to the representational constraints of quantum systems. By detecting distances between operational and training data, Q-SafeML can address concept drifts in QML. Experimental results on QCNN and VQC models demonstrate that this approach enables informed human oversight, enhancing system transparency and safety. <div>
arXiv:2509.04536v1 Announce Type: new 
Abstract: The rise of machine learning in safety-critical systems has paralleled advancements in quantum computing, leading to the emerging field of Quantum Machine Learning (QML). While safety monitoring has progressed in classical ML, existing methods are not directly applicable to QML due to fundamental differences in quantum computation. Given the novelty of QML, dedicated safety mechanisms remain underdeveloped. This paper introduces Q-SafeML, a safety monitoring approach for QML. The method builds on SafeML, a recent method that utilizes statistical distance measures to assess model accuracy and provide confidence in the reasoning of an algorithm. An adapted version of Q-SafeML incorporates quantum-centric distance measures, aligning with the probabilistic nature of QML outputs. This shift to a model-dependent, post-classification evaluation represents a key departure from classical SafeML, which is dataset-driven and classifier-agnostic. The distinction is motivated by the unique representational constraints of quantum systems, requiring distance metrics defined over quantum state spaces. Q-SafeML detects distances between operational and training data addressing the concept drifts in the context of QML. Experiments on QCNN and VQC Models show that this enables informed human oversight, enhancing system transparency and safety.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance-Grounded Optimization For Algorithmic Trading</title>
<link>https://arxiv.org/abs/2509.04541</link>
<guid>https://arxiv.org/abs/2509.04541</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Learning, Finance, Interpretable AI, Loss Functions, Algorithmic Trading <br />
Summary: <br />
In this study, the use of deep learning in finance is explored, focusing on the importance of interpretable artificial intelligence (AI) in the field. Traditional approaches may not be suitable for finance, as different metrics are used to evaluate models. The authors propose financially grounded loss functions based on key finance metrics like Sharpe ratio and Profit-and-Loss, and introduce turnover regularization to limit position turnover. Their research shows that these new metrics and regularization techniques outperform traditional mean squared error for return prediction tasks, especially in algorithmic trading. The study highlights the significance of using financially grounded metrics in improving predictive performance for trading strategies and portfolio optimization. <br /> <div>
arXiv:2509.04541v1 Announce Type: new 
Abstract: Deep Learning is evolving fast and integrates into various domains. Finance is a challenging field for deep learning, especially in the case of interpretable artificial intelligence (AI). Although classical approaches perform very well with natural language processing, computer vision, and forecasting, they are not perfect for the financial world, in which specialists use different metrics to evaluate model performance.
  We first introduce financially grounded loss functions derived from key quantitative finance metrics, including the Sharpe ratio, Profit-and-Loss (PnL), and Maximum Draw down. Additionally, we propose turnover regularization, a method that inherently constrains the turnover of generated positions within predefined limits.
  Our findings demonstrate that the proposed loss functions, in conjunction with turnover regularization, outperform the traditional mean squared error loss for return prediction tasks when evaluated using algorithmic trading metrics. The study shows that financially grounded metrics enhance predictive performance in trading strategies and portfolio optimization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition</title>
<link>https://arxiv.org/abs/2509.04544</link>
<guid>https://arxiv.org/abs/2509.04544</guid>
<content:encoded><![CDATA[
<div> Keywords: inhalation, exhalation, human activity recognition, health monitoring, predictive models

Summary:
i-Mask is a novel approach for human activity recognition (HAR) that utilizes exhaled breath patterns captured by a custom-developed mask with integrated sensors. The collected data undergoes noise filtering, time-series decomposition, and labeling to train predictive models. The experimental results demonstrate the effectiveness of the approach with over 95% accuracy, showcasing its potential in healthcare and fitness applications. The patterns of inhalation and exhalation contain vital physiological signals that can be used to anticipate human behavior, health trends, and vital parameters. HAR is closely linked to these vital signs, providing deeper insights into well-being and enabling real-time health monitoring. The i-Mask approach leverages these signals to improve the accuracy of human activity recognition, enhancing its utility in various health-related applications.<br /><br />Summary: <div>
arXiv:2509.04544v1 Announce Type: new 
Abstract: The patterns of inhalation and exhalation contain important physiological signals that can be used to anticipate human behavior, health trends, and vital parameters. Human activity recognition (HAR) is fundamentally connected to these vital signs, providing deeper insights into well-being and enabling real-time health monitoring. This work presents i-Mask, a novel HAR approach that leverages exhaled breath patterns captured using a custom-developed mask equipped with integrated sensors. Data collected from volunteers wearing the mask undergoes noise filtering, time-series decomposition, and labeling to train predictive models. Our experimental results validate the effectiveness of the approach, achieving over 95\% accuracy and highlighting its potential in healthcare and fitness applications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bootstrapping Task Spaces for Self-Improvement</title>
<link>https://arxiv.org/abs/2509.04575</link>
<guid>https://arxiv.org/abs/2509.04575</guid>
<content:encoded><![CDATA[
<div> autocurriculum, reinforcement learning, self-improvement, exploration, task diversity
Summary:
The article introduces Exploratory Iteration (ExIt), a family of autocurriculum RL methods focused on training agents to self-improve over repeated iterations at inference-time without a fixed maximum iteration depth. ExIt utilizes the recurrent structure of self-improvement tasks by sampling informative intermediate histories during episodes for further iteration. It can be paired with explicit exploration mechanisms to enhance task diversity. Experiments in various domains show that ExIt strategies enable LLMs to achieve strong self-improvement on new tasks, iterate towards higher performance within a step budget exceeding the average training iteration depth, and adapt to different task instances. The approach demonstrates promising results in areas such as competition math, multi-turn tool-use, and machine learning engineering. <br /><br />Summary: <div>
arXiv:2509.04575v1 Announce Type: new 
Abstract: Progress in many task domains emerges from repeated revisions to previous solution attempts. Training agents that can reliably self-improve over such sequences at inference-time is a natural target for reinforcement learning (RL), yet the naive approach assumes a fixed maximum iteration depth, which can be both costly and arbitrary. We present Exploratory Iteration (ExIt), a family of autocurriculum RL methods that directly exploits the recurrent structure of self-improvement tasks to train LLMs to perform multi-step self-improvement at inference-time while only training on the most informative single-step iterations. ExIt grows a task space by selectively sampling the most informative intermediate, partial histories encountered during an episode for continued iteration, treating these starting points as new self-iteration task instances to train a self-improvement policy. ExIt can further pair with explicit exploration mechanisms to sustain greater task diversity. Across several domains, encompassing competition math, multi-turn tool-use, and machine learning engineering, we demonstrate that ExIt strategies, starting from either a single or many task instances, can produce policies exhibiting strong inference-time self-improvement on held-out task instances, and the ability to iterate towards higher performance over a step budget extending beyond the average iteration depth encountered during training.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance-Wise Adaptive Sampling for Dataset Construction in Approximating Inverse Problem Solutions</title>
<link>https://arxiv.org/abs/2509.04583</link>
<guid>https://arxiv.org/abs/2509.04583</guid>
<content:encoded><![CDATA[
<div> adaptive sampling framework, compact dataset, supervised learning, inverse problem solutions, training efficiency

Summary:
The article introduces an instance-wise adaptive sampling framework for supervised learning of inverse problem solutions. Traditional approaches require a large number of training samples drawn from a prior distribution, leading to high data collection costs. In contrast, the proposed method dynamically allocates sampling effort based on the specific test instance, resulting in significant gains in sample efficiency. By refining the training dataset iteratively based on the latest prediction, the framework tailors the dataset to the geometry of the inverse map around each test instance. The effectiveness of this approach is demonstrated in the context of the inverse scattering problem with structured priors. The results indicate that the adaptive method excels in complex prior settings or higher accuracy requirements. The adaptive sampling strategy presented in this study is versatile and can be applied to various inverse problems, providing a scalable and practical alternative to conventional fixed-dataset training regimes.<br /><br />Summary: <div>
arXiv:2509.04583v1 Announce Type: new 
Abstract: We propose an instance-wise adaptive sampling framework for constructing compact and informative training datasets for supervised learning of inverse problem solutions. Typical learning-based approaches aim to learn a general-purpose inverse map from datasets drawn from a prior distribution, with the training process independent of the specific test instance. When the prior has a high intrinsic dimension or when high accuracy of the learned solution is required, a large number of training samples may be needed, resulting in substantial data collection costs. In contrast, our method dynamically allocates sampling effort based on the specific test instance, enabling significant gains in sample efficiency. By iteratively refining the training dataset conditioned on the latest prediction, the proposed strategy tailors the dataset to the geometry of the inverse map around each test instance. We demonstrate the effectiveness of our approach in the inverse scattering problem under two types of structured priors. Our results show that the advantage of the adaptive method becomes more pronounced in settings with more complex priors or higher accuracy requirements. While our experiments focus on a particular inverse problem, the adaptive sampling strategy is broadly applicable and readily extends to other inverse problems, offering a scalable and practical alternative to conventional fixed-dataset training regimes.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Faithfulness-guided Ensemble Interpretation of Neural Network</title>
<link>https://arxiv.org/abs/2509.04588</link>
<guid>https://arxiv.org/abs/2509.04588</guid>
<content:encoded><![CDATA[
<div> Keywords: interpretable, faithful explanations, neural inferences, ensemble interpretation, hidden layer faithfulness<br />
Summary:<br />
The paper introduces the Faithfulness-guided Ensemble Interpretation (FEI) framework, aiming to enhance the interpretability of neural networks by providing faithful and interpretable explanations for specific inferences. FEI utilizes a smooth approximation to improve quantitative faithfulness scores and offers various modifications to target enhanced faithfulness in hidden layer encodings. The framework also introduces a novel qualitative metric to assess hidden layer faithfulness. Through extensive experiments, FEI outperforms existing methods, showcasing significant advancements in both qualitative visualization and quantitative faithfulness scores. Overall, the research presents a comprehensive framework for enhancing faithfulness in neural network explanations, emphasizing the importance of interpretability and precision in model evaluation and understanding. <br /> <div>
arXiv:2509.04588v1 Announce Type: new 
Abstract: Interpretable and faithful explanations for specific neural inferences are crucial for understanding and evaluating model behavior. Our work introduces \textbf{F}aithfulness-guided \textbf{E}nsemble \textbf{I}nterpretation (\textbf{FEI}), an innovative framework that enhances the breadth and effectiveness of faithfulness, advancing interpretability by providing superior visualization. Through an analysis of existing evaluation benchmarks, \textbf{FEI} employs a smooth approximation to elevate quantitative faithfulness scores. Diverse variations of \textbf{FEI} target enhanced faithfulness in hidden layer encodings, expanding interpretability. Additionally, we propose a novel qualitative metric that assesses hidden layer faithfulness. In extensive experiments, \textbf{FEI} surpasses existing methods, demonstrating substantial advances in qualitative visualization and quantitative faithfulness scores. Our research establishes a comprehensive framework for elevating faithfulness in neural network explanations, emphasizing both breadth and precision
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Multi-Task Learning with Learnable Weighting for Pharmacokinetic and Toxicity Prediction</title>
<link>https://arxiv.org/abs/2509.04601</link>
<guid>https://arxiv.org/abs/2509.04601</guid>
<content:encoded><![CDATA[
<div> Keywords: ADMET, drug discovery, multi-task learning, quantum-enhanced, task weighting

Summary:
The article introduces a new Quantum-enhanced and task-Weighted Multi-Task Learning (QW-MTL) framework for ADMET prediction in drug discovery. Traditional methods rely on single-task learning, but the proposed framework leverages quantum chemical descriptors to enhance molecular representations and introduces a task weighting scheme for dynamic loss balancing. QW-MTL is the first to conduct multi-task training across all 13 Therapeutics Data Commons classification benchmarks, outperforming single-task baselines on 12 out of 13 tasks. The model achieves high predictive performance, minimal model complexity, and fast inference, showcasing the effectiveness and efficiency of multi-task learning enhanced by quantum-informed features and adaptive task weighting.<br /><br />Summary: <div>
arXiv:2509.04601v1 Announce Type: new 
Abstract: Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) plays a crucial role in drug discovery and development, accelerating the screening and optimization of new drugs. Existing methods primarily rely on single-task learning (STL), which often fails to fully exploit the complementarities between tasks. Besides, it requires more computational resources while training and inference of each task independently. To address these issues, we propose a new unified Quantum-enhanced and task-Weighted Multi-Task Learning (QW-MTL) framework, specifically designed for ADMET classification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts quantum chemical descriptors to enrich molecular representations with additional information about the electronic structure and interactions. Meanwhile, it introduces a novel exponential task weighting scheme that combines dataset-scale priors with learnable parameters to achieve dynamic loss balancing across tasks. To the best of our knowledge, this is the first work to systematically conduct joint multi-task training across all 13 Therapeutics Data Commons (TDC) classification benchmarks, using leaderboard-style data splits to ensure a standardized and realistic evaluation setting. Extensive experimental results show that QW-MTL significantly outperforms single-task baselines on 12 out of 13 tasks, achieving high predictive performance with minimal model complexity and fast inference, demonstrating the effectiveness and efficiency of multi-task molecular learning enhanced by quantum-informed features and adaptive task weighting.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families</title>
<link>https://arxiv.org/abs/2509.04622</link>
<guid>https://arxiv.org/abs/2509.04622</guid>
<content:encoded><![CDATA[
<div> framework, representational similarity metrics, model families, discriminative power, separability  
Summary:  
- The study introduces a quantitative framework to evaluate the discriminative capacity of representational similarity metrics across different model families and training regimes.  
- Three separability measures-dprime, silhouette coefficients, and ROC-AUC are used to assess the metrics' ability to separate architectures such as CNNs, Vision Transformers, Swin Transformers, and ConvNeXt.  
- The results show that metrics imposing stringent alignment constraints tend to have higher separability.  
- Soft-matching is identified as the metric with the highest separability among mapping-based approaches, followed by Procrustes alignment and linear predictivity.  
- Non-fitting methods like RSA also exhibit strong separability across model families.  
<br /><br />Summary: <div>
arXiv:2509.04622v1 Announce Type: new 
Abstract: Representational similarity metrics are fundamental tools in neuroscience and AI, yet we lack systematic comparisons of their discriminative power across model families. We introduce a quantitative framework to evaluate representational similarity measures based on their ability to separate model families-across architectures (CNNs, Vision Transformers, Swin Transformers, ConvNeXt) and training regimes (supervised vs. self-supervised). Using three complementary separability measures-dprime from signal detection theory, silhouette coefficients and ROC-AUC, we systematically assess the discriminative capacity of commonly used metrics including RSA, linear predictivity, Procrustes, and soft matching. We show that separability systematically increases as metrics impose more stringent alignment constraints. Among mapping-based approaches, soft-matching achieves the highest separability, followed by Procrustes alignment and linear predictivity. Non-fitting methods such as RSA also yield strong separability across families. These results provide the first systematic comparison of similarity metrics through a separability lens, clarifying their relative sensitivity and guiding metric choice for large-scale model and brain comparisons.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Split Conformal Prediction in the Function Space with Neural Operators</title>
<link>https://arxiv.org/abs/2509.04623</link>
<guid>https://arxiv.org/abs/2509.04623</guid>
<content:encoded><![CDATA[
<div> conformal prediction, uncertainty quantification, neural operators, function spaces, calibration <br />
Summary:
This paper addresses the challenge of uncertainty quantification for neural operators in infinite-dimensional settings. While existing methods like Gaussian processes and Bayesian neural networks have limitations, the authors propose a split conformal prediction approach for function spaces. They establish finite-sample coverage guarantees in a finite-dimensional space using a discretization map, which is then extended to function spaces with asymptotic convergence. A decomposition of the conformal radius into discretization, calibration, and misspecification components guides a regression-based correction for calibration transfer across resolutions. Diagnostic metrics like conformal ensemble score and internal agreement are introduced for forecasting evaluation in autoregressive settings. Empirical results demonstrate the method's ability to maintain calibrated coverage with less variation under resolution shifts and improved performance in super-resolution tasks. <br /> <div>
arXiv:2509.04623v1 Announce Type: new 
Abstract: Uncertainty quantification for neural operators remains an open problem in the infinite-dimensional setting due to the lack of finite-sample coverage guarantees over functional outputs. While conformal prediction offers finite-sample guarantees in finite-dimensional spaces, it does not directly extend to function-valued outputs. Existing approaches (Gaussian processes, Bayesian neural networks, and quantile-based operators) require strong distributional assumptions or yield conservative coverage. This work extends split conformal prediction to function spaces following a two step method. We first establish finite-sample coverage guarantees in a finite-dimensional space using a discretization map in the output function space. Then these guarantees are lifted to the function-space by considering the asymptotic convergence as the discretization is refined. To characterize the effect of resolution, we decompose the conformal radius into discretization, calibration, and misspecification components. This decomposition motivates a regression-based correction to transfer calibration across resolutions. Additionally, we propose two diagnostic metrics (conformal ensemble score and internal agreement) to quantify forecast degradation in autoregressive settings. Empirical results show that our method maintains calibrated coverage with less variation under resolution shifts and achieves better coverage in super-resolution tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fundamental bounds on efficiency-confidence trade-off for transductive conformal prediction</title>
<link>https://arxiv.org/abs/2509.04631</link>
<guid>https://arxiv.org/abs/2509.04631</guid>
<content:encoded><![CDATA[
<div> Keywords: transductive conformal prediction, confidence, efficiency, conditional entropy, dispersion

Summary:
Transductive conformal prediction deals with making predictions for multiple data points while ensuring a specified confidence level. A trade-off exists between confidence and efficiency, where the size of prediction sets grows exponentially with a non-trivial confidence level due to inherent data uncertainty. This exponential growth is proportional to the number of samples and the conditional entropy of the data, along with a dispersion term representing the variance of the log conditional probability distribution. In a scenario where all test data points have the same label, the problem reduces to hypothesis testing, yielding an optimal confidence predictor with an analysis of the error exponent. The study provides insights into the fundamental aspects of transductive conformal prediction, highlighting the challenges and potential solutions for achieving accurate predictions with desired confidence levels.<br /><br />Summary: <div>
arXiv:2509.04631v1 Announce Type: new 
Abstract: Transductive conformal prediction addresses the simultaneous prediction for multiple data points. Given a desired confidence level, the objective is to construct a prediction set that includes the true outcomes with the prescribed confidence. We demonstrate a fundamental trade-off between confidence and efficiency in transductive methods, where efficiency is measured by the size of the prediction sets. Specifically, we derive a strict finite-sample bound showing that any non-trivial confidence level leads to exponential growth in prediction set size for data with inherent uncertainty. The exponent scales linearly with the number of samples and is proportional to the conditional entropy of the data. Additionally, the bound includes a second-order term, dispersion, defined as the variance of the log conditional probability distribution. We show that this bound is achievable in an idealized setting. Finally, we examine a special case of transductive prediction where all test data points share the same label. We show that this scenario reduces to the hypothesis testing problem with empirically observed statistics and provide an asymptotically optimal confidence predictor, along with an analysis of the error exponent.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Transformer Architectures as Implicit Multinomial Regression</title>
<link>https://arxiv.org/abs/2509.04653</link>
<guid>https://arxiv.org/abs/2509.04653</guid>
<content:encoded><![CDATA[
<div> Keywords: mechanistic interpretability, attention mechanisms, transformer models, multinomial regression, latent features <br />
Summary: 
This paper explores the relationship between attention mechanisms in transformer models and multinomial regression. It aims to enhance the mechanistic interpretability of machine learning models by understanding how attention affects model behavior. By optimizing latent features in a fixed multinomial regression setting, the study shows that the dynamics induced by attention blocks align with optimal solutions. This connection provides insights into the mathematical underpinnings of attention and its impact on model performance. The findings suggest that the evolution of representations in transformers can be interpreted as a trajectory that leads to optimal features for classification. This research sheds light on the opaque nature of attention mechanisms and their role in feature polysemanticity and superposition within modern machine learning models. <div>
arXiv:2509.04653v1 Announce Type: new 
Abstract: Mechanistic interpretability aims to understand how internal components of modern machine learning models, such as weights, activations, and layers, give rise to the model's overall behavior. One particularly opaque mechanism is attention: despite its central role in transformer models, its mathematical underpinnings and relationship to concepts like feature polysemanticity, superposition, and model performance remain poorly understood. This paper establishes a novel connection between attention mechanisms and multinomial regression. Specifically, we show that in a fixed multinomial regression setting, optimizing over latent features yields optimal solutions that align with the dynamics induced by attention blocks. In other words, the evolution of representations through a transformer can be interpreted as a trajectory that recovers the optimal features for classification.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible inference of learning rules from de novo learning data using neural networks</title>
<link>https://arxiv.org/abs/2509.04661</link>
<guid>https://arxiv.org/abs/2509.04661</guid>
<content:encoded><![CDATA[
<div> flexible framework, nonparametric, deep neural network, recurrent neural network, learning rule<br />
Summary:<br />
The study focuses on inferring learning rules directly from animal decision-making data during de novo task learning. A nonparametric framework parameterizes the per-trial update of policy weights with a deep neural network (DNN), successfully recovering ground-truth rules in simulation. The extension to a recurrent variant (RNN) captures non-Markovian dynamics by allowing updates to depend on trial history. Applied to a large behavioral dataset of mice learning a sensory decision-making task, the models improved predictions on held-out data. The inferred rules showed asymmetric updates after correct versus error trials and history dependence, indicating non-Markovian learning. Overall, the results introduce a flexible framework for inferring biological learning rules in de novo learning tasks, offering insights for experimental training protocols and the development of behavioral digital twins.<br /> 
Summary: <div>
arXiv:2509.04661v1 Announce Type: new 
Abstract: Understanding how animals learn is a central challenge in neuroscience, with growing relevance to the development of animal- or human-aligned artificial intelligence. However, most existing approaches assume specific parametric forms for the learning rule (e.g., Q-learning, policy gradient) or are limited to simplified settings like bandit tasks, which do not involve learning a new input-output mapping from scratch. In contrast, animals must often learn new behaviors de novo, which poses a rich challenge for learning-rule inference. We target this problem by inferring learning rules directly from animal decision-making data during de novo task learning, a setting that requires models flexible enough to capture suboptimality, history dependence, and rich external stimulus integration without strong structural priors. We first propose a nonparametric framework that parameterizes the per-trial update of policy weights with a deep neural network (DNN), and validate it by recovering ground-truth rules in simulation. We then extend to a recurrent variant (RNN) that captures non-Markovian dynamics by allowing updates to depend on trial history. Applied to a large behavioral dataset of mice learning a sensory decision-making task over multiple weeks, our models improved predictions on held-out data. The inferred rules revealed asymmetric updates after correct versus error trials and history dependence, consistent with non-Markovian learning. Overall, these results introduce a flexible framework for inferring biological learning rules from behavioral data in de novo learning tasks, providing insights to inform experimental training protocols and the development of behavioral digital twins.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Ordinary Lipschitz Constraints: Differentially Private Stochastic Optimization with Tsybakov Noise Condition</title>
<link>https://arxiv.org/abs/2509.04668</link>
<guid>https://arxiv.org/abs/2509.04668</guid>
<content:encoded><![CDATA[
<div> Tsybakov Noise Condition, Stochastic Convex Optimization, Differential Privacy, Lipschitz Constant, Minimax Rate <br />
Summary: <br />
The study focuses on Stochastic Convex Optimization under Differential Privacy (DP-SCO), assuming the Tsybakov Noise Condition (TNC) with parameter $\theta>1$ for the population risk function. An $(\varepsilon, \delta)$-DP algorithm is proposed with a utility bound that is independent of the Lipschitz constant when $\theta\geq 2$. The extension to $\theta\geq \bar{\theta}>1$ is also discussed. For small privacy budgets, an upper bound is shown even without Lipschitz continuity. A lower bound is established for private minimax rate under $\rho$-zero Concentrated Differential Privacy. The results provide insights into optimizing stochastic convex problems while preserving differential privacy. <div>
arXiv:2509.04668v1 Announce Type: new 
Abstract: We study Stochastic Convex Optimization in the Differential Privacy model (DP-SCO). Unlike previous studies, here we assume the population risk function satisfies the Tsybakov Noise Condition (TNC) with some parameter $\theta>1$, where the Lipschitz constant of the loss could be extremely large or even unbounded, but the $\ell_2$-norm gradient of the loss has bounded $k$-th moment with $k\geq 2$. For the Lipschitz case with $\theta\geq 2$, we first propose an $(\varepsilon, \delta)$-DP algorithm whose utility bound is $\Tilde{O}\left(\left(\tilde{r}_{2k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$ in high probability, where $n$ is the sample size, $d$ is the model dimension, and $\tilde{r}_{2k}$ is a term that only depends on the $2k$-th moment of the gradient. It is notable that such an upper bound is independent of the Lipschitz constant. We then extend to the case where
  $\theta\geq \bar{\theta}> 1$ for some known constant $\bar{\theta}$. Moreover, when the privacy budget $\varepsilon$ is small enough, we show an upper bound of $\tilde{O}\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$ even if the loss function is not Lipschitz. For the lower bound, we show that for any $\theta\geq 2$, the private minimax rate for $\rho$-zero Concentrated Differential Privacy is lower bounded by $\Omega\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\sqrt{\rho}}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echoes Before Collapse: Deep Learning Detection of Flickering in Complex Systems</title>
<link>https://arxiv.org/abs/2509.04683</link>
<guid>https://arxiv.org/abs/2509.04683</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, flickering detection, CNN LSTM models, resilience, critical regime shifts

Summary:
Deep learning, specifically convolutional long short-term memory (CNN LSTM) models, has shown promise in detecting flickering patterns in complex systems. Flickering, characterized by noise-driven switching between stable states, is indicative of reduced resilience in various systems such as climate systems and financial markets. By training the CNN LSTM models on synthetic time series data and then testing on empirical datasets, researchers were able to accurately identify flickering in real-world scenarios, including body temperature records of dormice and palaeoclimate proxies. This study highlights the potential of deep learning in extracting early warning signals from noisy and nonlinear time series data, providing a versatile framework for detecting instability across a wide range of dynamic systems.<br /><br />Summary: <div>
arXiv:2509.04683v1 Announce Type: new 
Abstract: Deep learning offers powerful tools for anticipating tipping points in complex systems, yet its potential for detecting flickering (noise-driven switching between coexisting stable states) remains unexplored. Flickering is a hallmark of reduced resilience in climate systems, ecosystems, financial markets, and other systems. It can precede critical regime shifts that are highly impactful but difficult to predict. Here we show that convolutional long short-term memory (CNN LSTM) models, trained on synthetic time series generated from simple polynomial functions with additive noise, can accurately identify flickering patterns. Despite being trained on simplified dynamics, our models generalize to diverse stochastic systems and reliably detect flickering in empirical datasets, including dormouse body temperature records and palaeoclimate proxies from the African Humid Period. These findings demonstrate that deep learning can extract early warning signals from noisy, nonlinear time series, providing a flexible framework for identifying instability across a wide range of dynamical systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KRAFT: A Knowledge Graph-Based Framework for Automated Map Conflation</title>
<link>https://arxiv.org/abs/2509.04684</link>
<guid>https://arxiv.org/abs/2509.04684</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph Construction, Map Matching, Map Merging, map conflation, geospatial databases <br />
Summary: <br />
Digital maps are essential for navigation and other applications, requiring accuracy and timely updates. Current map conflation methods have limitations in handling non-linear objects and lack data-driven entity matching. To address this, KRAFT is introduced, a learning-based approach with three parts. Firstly, Knowledge Graph Construction represents each geospatial database as a knowledge graph. Next, Map Matching utilizes knowledge graph alignment and geospatial feature encoding for entity matching. Finally, Map Merging merges matched entities consistently using a linear programming formulation. Experimental results demonstrate KRAFT's superior performance in map conflation tasks compared to existing methods, with its modules achieving better results individually as well. <div>
arXiv:2509.04684v1 Announce Type: new 
Abstract: Digital maps play a crucial role in various applications such as navigation, fleet management, and ride-sharing, necessitating their accuracy and currency, which require timely updates. While the majority of geospatial databases (GDBs) provide high-quality information, their data is (i) limited to specific regions and/or (ii) missing some entities, even in their covered areas. Map conflation is the process of augmentation of a GDB using another GDB to conflate missing spatial features. Existing map conflation methods suffer from two main limitations: (1) They are designed for the conflation of linear objects (e.g., road networks) and cannot simply be extended to non-linear objects, thus missing information about most entities in the map. (2) They are heuristic algorithmic approaches that are based on pre-defined rules, unable to learn entities matching in a data-driven manner. To address these limitations, we design KRAFT, a learning based approach consisting of three parts: (1) Knowledge Graph Construction - where each GDB is represented by a knowledge graph, (2) Map Matching - where we use a knowledge graph alignment method as well as a geospatial feature encoder to match entities in obtained knowledge graphs, and (3) Map Merging - where we merge matched entities in the previous modules in a consistent manner, using a mixed integer linear programming formulation that fully merges the GDBs without adding any inconsistencies. Our experimental evaluation shows that not only does KRAFT achieve outstanding performance compared to state-of-the-art and baseline methods in map conflation tasks, but each of its modules (e.g., Map Matching and Map Merging) also separately outperforms traditional matching and merging methods.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG Signals</title>
<link>https://arxiv.org/abs/2509.04699</link>
<guid>https://arxiv.org/abs/2509.04699</guid>
<content:encoded><![CDATA[
<div> sEMG, hand gesture classification, biosignals, zero-shot classification, pre-training <br />
Summary: <br />
The paper introduces a Contrastive Pose-EMG Pre-training (CPEP) framework for aligning surface electromyography (sEMG) and pose representations to improve gesture classification. By learning an EMG encoder that produces high-quality and pose-informative representations, the model achieves superior performance in both in-distribution and out-of-distribution gesture classification compared to existing benchmark models. The research leverages low-power and cost-effective biosignals like sEMG to enable continuous gesture prediction on wearables. The alignment of weak-modality data with high-quality structured data enhances representation quality and facilitates zero-shot classification. The proposed framework outperforms emg2pose benchmark models by significant margins in both linear probing and zero-shot classification setups. This work contributes to advancing the field of hand gesture classification by demonstrating the effectiveness of learning representations from weak-modality data aligned with structured data.<br /> <div>
arXiv:2509.04699v1 Announce Type: new 
Abstract: Hand gesture classification using high-quality structured data such as videos, images, and hand skeletons is a well-explored problem in computer vision. Leveraging low-power, cost-effective biosignals, e.g. surface electromyography (sEMG), allows for continuous gesture prediction on wearables. In this paper, we demonstrate that learning representations from weak-modality data that are aligned with those from structured, high-quality data can improve representation quality and enables zero-shot classification. Specifically, we propose a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG and pose representations, where we learn an EMG encoder that produces high-quality and pose-informative representations. We assess the gesture classification performance of our model through linear probing and zero-shot setups. Our model outperforms emg2pose benchmark models by up to 21% on in-distribution gesture classification and 72% on unseen (out-of-distribution) gesture classification.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Spectral Fusion: p-Exponent Cyclic Scheduling and Early Decision-Boundary Alignment in First-Order Optimization</title>
<link>https://arxiv.org/abs/2509.04713</link>
<guid>https://arxiv.org/abs/2509.04713</guid>
<content:encoded><![CDATA[
<div> Keywords: spectral behaviors, machine learning, optimizer, Natural Spectral Fusion, frequency preference<br />
Summary:<br />
The article discusses the spectral bias of first-order optimizers in machine learning and introduces Natural Spectral Fusion (NSF) as a new approach to optimization. NSF reframes training by focusing on controlling spectral coverage and information fusion rather than just adjusting step sizes. The core principles of NSF involve treating the optimizer as a spectral controller that balances low- and high-frequency information dynamically and reweighting frequency bands periodically without changing the model or training pipeline. By implementing NSF through p-exponent extension of the second-moment term and cyclic scheduling, the optimizer's role as a spectral controller is highlighted. The study shows that adaptive methods prioritize low frequencies, SGD is neutral, and negative exponents enhance high-frequency information. With p-exponent cyclic scheduling, spectral coverage broadens, cross-band fusion improves, and early decision-boundary alignment is induced, leading to improved accuracy even with high loss. Experimental results demonstrate that NSF outperforms traditional methods in reducing test error and achieving comparable accuracy with reduced training cost. <br /><br />Summary: The article introduces Natural Spectral Fusion (NSF) as a novel approach to first-order optimization in machine learning that focuses on controlling spectral coverage and information fusion. NSF reframes training by considering the optimizer as a spectral controller that dynamically balances low- and high-frequency information. By implementing NSF through p-exponent extension and cyclic scheduling, the study shows that adaptive methods prioritize low frequencies, SGD is near-neutral, and negative exponents amplify high-frequency information. NSF improves accuracy by broadening spectral coverage, enhancing cross-band fusion, and inducing early decision-boundary alignment. Experimental results demonstrate that NSF outperforms traditional methods in reducing test error and achieving comparable accuracy with reduced training cost. <div>
arXiv:2509.04713v1 Announce Type: new 
Abstract: Spectral behaviors have been widely discussed in machine learning, yet the optimizer's own spectral bias remains unclear. We argue that first-order optimizers exhibit an intrinsic frequency preference that significantly reshapes the optimization path. To address this, we propose Natural Spectral Fusion (NSF): reframing training as controllable spectral coverage and information fusion rather than merely scaling step sizes. NSF has two core principles: treating the optimizer as a spectral controller that dynamically balances low- and high-frequency information; and periodically reweighting frequency bands at negligible cost, without modifying the model, data, or training pipeline. We realize NSF via a p-exponent extension of the second-moment term, enabling both positive and negative exponents, and implement it through cyclic scheduling. Theory and experiments show that adaptive methods emphasize low frequencies, SGD is near-neutral, and negative exponents amplify high-frequency information. Cyclic scheduling broadens spectral coverage, improves cross-band fusion, and induces early decision-boundary alignment, where accuracy improves even while loss remains high. Across multiple benchmarks, with identical learning-rate strategies and fixed hyperparameters, p-exponent cyclic scheduling consistently reduces test error and demonstrates distinct convergence behavior; on some tasks, it matches baseline accuracy with only one-quarter of the training cost. Overall, NSF reveals the optimizer's role as an active spectral controller and provides a unified, controllable, and efficient framework for first-order optimization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive Next-Token Prediction</title>
<link>https://arxiv.org/abs/2509.04733</link>
<guid>https://arxiv.org/abs/2509.04733</guid>
<content:encoded><![CDATA[
<div> coverage guarantees, decoding methods, autoregressive pre-trained models, complex reasoning tasks, search efficiency <br />
<br />
Summary: 
The paper introduces a novel decoding strategy called \textsc{CoVeR} that operates within the conformal prediction framework. This approach aims to address the limitations of existing decoding strategies by providing provable coverage guarantees while balancing search efficiency. The proposed method maintains a compact search space and ensures high coverage probability over desirable trajectories, including long-tail sequences crucial in real-world applications. The authors establish a PAC-style generalization bound, demonstrating that \textsc{CoVeR} can achieve a coverage rate of at least $1 - \alpha$ for any target level $\alpha \in (0,1) asymptotically. This innovative strategy combines autoregressive pre-trained models with sophisticated decoding techniques to improve performance on complex reasoning tasks. <div>
arXiv:2509.04733v1 Announce Type: new 
Abstract: Autoregressive pre-trained models combined with decoding methods have achieved impressive performance on complex reasoning tasks. While mainstream decoding strategies such as beam search can generate plausible candidate sets, they often lack provable coverage guarantees, and struggle to effectively balance search efficiency with the need for versatile trajectories, particularly those involving long-tail sequences that are essential in certain real-world applications. To address these limitations, we propose \textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal prediction framework that simultaneously maintains a compact search space and ensures high coverage probability over desirable trajectories. Theoretically, we establish a PAC-style generalization bound, guaranteeing that \textsc{CoVeR} asymptotically achieves a coverage rate of at least $1 - \alpha$ for any target level $\alpha \in (0,1)$.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
<link>https://arxiv.org/abs/2509.04734</link>
<guid>https://arxiv.org/abs/2509.04734</guid>
<content:encoded><![CDATA[
<div> Keywords: Information Contrastive framework, representation learning, total variation distance, f-divergence, dimensionality reduction

Summary:
The Beyond I-Con framework explores alternative statistical divergences and similarity kernels in representation learning optimization. By using total variation distance in unsupervised clustering of DINO-ViT embeddings, state-of-the-art results were achieved. In supervised contrastive learning, outperformance of the standard approach was shown by employing total variation and a distance-based similarity kernel. Dimensionality reduction tasks demonstrated superior qualitative results and better performance on downstream tasks compared to SNE by utilizing a bounded f-divergence instead of KL divergence. The study emphasizes the significance of divergences and similarity kernel choices in representation learning strategies.<br /><br />Summary: <div>
arXiv:2509.04734v1 Announce Type: new 
Abstract: The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences and similarity kernels. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) on supervised contrastive learning, we outperform the standard approach by using TV and a distance-based similarity kernel instead of KL and an angular kernel; (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded f-divergence. Our results highlight the importance of considering divergence and similarity kernel choices in representation learning optimization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VARMA-Enhanced Transformer for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.04782</link>
<guid>https://arxiv.org/abs/2509.04782</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer-based models, time series forecasting, VARMAformer, VARMA-inspired Feature Extractor, VARMA-Enhanced Attention

Summary: 
VARMAformer introduces a novel architecture that combines the efficiency of a cross-attention-only framework with classical time series analysis principles. The model includes a VARMA-inspired Feature Extractor (VFE) that captures autoregressive and moving-average patterns at the patch level. Additionally, a VARMA-Enhanced Attention (VE-atten) mechanism enhances context-awareness through a temporal gate for queries. By blending classical insights with modern deep learning, VARMAformer effectively captures both global long-range dependencies and local statistical structures. Extensive experiments on benchmark datasets showcase the superior performance of VARMAformer compared to existing state-of-the-art methods. The integration of classical statistical insights into modern frameworks for time series forecasting is validated through this work. 

Summary:<br /><br />Keywords: Transformer-based models, time series forecasting, VARMAformer, VARMA-inspired Feature Extractor, VARMA-Enhanced Attention <br /> VARMAformer introduces a novel architecture that combines the efficiency of a cross-attention-only framework with classical time series analysis principles. The model includes a VARMA-inspired Feature Extractor (VFE) that captures autoregressive and moving-average patterns at the patch level. Additionally, a VARMA-Enhanced Attention (VE-atten) mechanism enhances context-awareness through a temporal gate for queries. By blending classical insights with modern deep learning, VARMAformer effectively captures both global long-range dependencies and local statistical structures. Extensive experiments on benchmark datasets showcase the superior performance of VARMAformer compared to existing state-of-the-art methods. The integration of classical statistical insights into modern frameworks for time series forecasting is validated through this work. <div>
arXiv:2509.04782v1 Announce Type: new 
Abstract: Transformer-based models have significantly advanced time series forecasting. Recent work, like the Cross-Attention-only Time Series transformer (CATS), shows that removing self-attention can make the model more accurate and efficient. However, these streamlined architectures may overlook the fine-grained, local temporal dependencies effectively captured by classical statistical models like Vector AutoRegressive Moving Average model (VARMA). To address this gap, we propose VARMAformer, a novel architecture that synergizes the efficiency of a cross-attention-only framework with the principles of classical time series analysis. Our model introduces two key innovations: (1) a dedicated VARMA-inspired Feature Extractor (VFE) that explicitly models autoregressive (AR) and moving-average (MA) patterns at the patch level, and (2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal gate to make queries more context-aware. By fusing these classical insights into a modern backbone, VARMAformer captures both global, long-range dependencies and local, statistical structures. Through extensive experiments on widely-used benchmark datasets, we demonstrate that our model consistently outperforms existing state-of-the-art methods. Our work validates the significant benefit of integrating classical statistical insights into modern deep learning frameworks for time series forecasting.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Unlearning: Efficient Node Removal in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.04785</link>
<guid>https://arxiv.org/abs/2509.04785</guid>
<content:encoded><![CDATA[
<div> node unlearning, graph neural network, privacy protection, sensitive information, topology

Summary:
- Researchers have developed three novel node unlearning methods to efficiently remove sensitive training data in graph neural network models.
- The proposed methods, including Class-based Label Replacement, Topology-guided Neighbor Mean Posterior Probability, and Class-consistent Neighbor Node Filtering, effectively leverage the graph's topology for node unlearning.
- Experimental results on benchmark datasets demonstrate the superiority of the proposed methods in terms of model utility, unlearning utility, and unlearning efficiency.
- The methods efficiently remove sensitive training nodes and enhance the privacy and security of GNN models.
- This research contributes to the field of node unlearning, providing valuable insights into protecting sensitive information in GNNs. 

<br /><br />Summary: <div>
arXiv:2509.04785v1 Announce Type: new 
Abstract: With increasing concerns about privacy attacks and potential sensitive information leakage, researchers have actively explored methods to efficiently remove sensitive training data and reduce privacy risks in graph neural network (GNN) models. Node unlearning has emerged as a promising technique for protecting the privacy of sensitive nodes by efficiently removing specific training node information from GNN models. However, existing node unlearning methods either impose restrictions on the GNN structure or do not effectively utilize the graph topology for node unlearning. Some methods even compromise the graph's topology, making it challenging to achieve a satisfactory performance-complexity trade-off. To address these issues and achieve efficient unlearning for training node removal in GNNs, we propose three novel node unlearning methods: Class-based Label Replacement, Topology-guided Neighbor Mean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among these methods, Topology-guided Neighbor Mean Posterior Probability and Class-consistent Neighbor Node Filtering effectively leverage the topological features of the graph, resulting in more effective node unlearning. To validate the superiority of our proposed methods in node unlearning, we conducted experiments on three benchmark datasets. The evaluation criteria included model utility, unlearning utility, and unlearning efficiency. The experimental results demonstrate the utility and efficiency of the proposed methods and illustrate their superiority compared to state-of-the-art node unlearning methods. Overall, the proposed methods efficiently remove sensitive training nodes and protect the privacy information of sensitive nodes in GNNs. The findings contribute to enhancing the privacy and security of GNN models and provide valuable insights into the field of node unlearning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Arbitration Control for an Ensemble of Diversified DQN variants in Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.04815</link>
<guid>https://arxiv.org/abs/2509.04815</guid>
<content:encoded><![CDATA[
<div> Ensemble, Deep reinforcement learning, Continual reinforcement learning, Arbitration control, Reliability <br />
<br />
Summary: 
The article introduces an arbitration control mechanism for an ensemble of diversified DQN variants to address the issue of catastrophic forgetting in deep reinforcement learning models. The proposed framework, ACED-DQN, integrates multiple RL agents with diverse value functions and an arbitration control that prioritizes agents with higher reliability. This approach is inspired by how humans make decisions in continual reinforcement learning contexts by utilizing an ensemble of agents. The study demonstrates significant performance improvements in both static and continual environments, showing the effectiveness of arbitration control over diversified DQNs during training. By taking inspiration from the human brain, the framework enables RL agents to continuously learn and adapt, leading to enhanced learning capabilities and better performance in challenging scenarios. <br /><br /> <div>
arXiv:2509.04815v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) models, despite their efficiency in learning an optimal policy in static environments, easily loses previously learned knowledge (i.e., catastrophic forgetting). It leads RL models to poor performance in continual reinforcement learning (CRL) scenarios. To address this, we present an arbitration control mechanism over an ensemble of RL agents. It is motivated by and closely aligned with how humans make decisions in a CRL context using an arbitration control of multiple RL agents in parallel as observed in the prefrontal cortex. We integrated two key ideas into our model: (1) an ensemble of RLs (i.e., DQN variants) explicitly trained to have diverse value functions and (2) an arbitration control that prioritizes agents with higher reliability (i.e., less error) in recent trials. We propose a framework for CRL, an Arbitration Control for an Ensemble of Diversified DQN variants (ACED-DQN). We demonstrate significant performance improvements in both static and continual environments, supported by empirical evidence showing the effectiveness of arbitration control over diversified DQNs during training. In this work, we introduced a framework that enables RL agents to continuously learn, with inspiration from the human brain.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revolution or Hype? Seeking the Limits of Large Models in Hardware Design</title>
<link>https://arxiv.org/abs/2509.04905</link>
<guid>https://arxiv.org/abs/2509.04905</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Large Circuit Models, Electronic Design Automation, Circuit Design, AI Models

Summary: 
This paper examines the potential impact of Large Language Models (LLMs) and Large Circuit Models (LCMs) on the field of electronic design automation (EDA). It addresses the skepticism surrounding the use of AI models in circuit design, questioning whether they represent a true revolution or a temporary hype. The paper brings together perspectives from academia and industry to discuss the practical capabilities, limitations, and future prospects of these models. It delves into key issues such as reliability, scalability, and interpretability, considering whether AI models can surpass or work alongside traditional EDA methods. Overall, the paper offers a comprehensive overview of the ongoing debate surrounding the use of large AI models in hardware design, providing valuable insights into this potentially transformative technology trend.

<br /><br />Summary: <div>
arXiv:2509.04905v1 Announce Type: new 
Abstract: Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models (LCMs) have sparked excitement across the electronic design automation (EDA) community, promising a revolution in circuit design and optimization. Yet, this excitement is met with significant skepticism: Are these AI models a genuine revolution in circuit design, or a temporary wave of inflated expectations? This paper serves as a foundational text for the corresponding ICCAD 2025 panel, bringing together perspectives from leading experts in academia and industry. It critically examines the practical capabilities, fundamental limitations, and future prospects of large AI models in hardware design. The paper synthesizes the core arguments surrounding reliability, scalability, and interpretability, framing the debate on whether these models can meaningfully outperform or complement traditional EDA methods. The result is an authoritative overview offering fresh insights into one of today's most contentious and impactful technology trends.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series</title>
<link>https://arxiv.org/abs/2509.04921</link>
<guid>https://arxiv.org/abs/2509.04921</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, financial time series, chaotic properties, resampling techniques, Bitcoin trade data 

Summary: 
This study proposes a methodology for modeling financial time series by generating artificial chaotic time series and using resampling techniques to simulate financial data for training. The researchers conducted large-scale pre-training with 10 billion samples for each case, extending predictive horizons by increasing the resampling interval. They tested the model on multiple timeframes using actual Bitcoin trade data and achieved significant performance improvements over autocorrelation models in predicting profitability of a trading strategy. During the pre-training process, a scaling law-like phenomenon was observed, suggesting the potential to predict near-future events by investing substantial computational resources. Future research should focus on further large-scale training and verifying the applicability of this scaling law to diverse chaotic models.<br /><br />Summary: <div>
arXiv:2509.04921v1 Announce Type: new 
Abstract: Time series forecasting plays a critical role in decision-making processes across diverse fields including meteorology, traffic, electricity, economics, finance, and so on. Especially, predicting returns on financial instruments is a challenging problem. Some researchers have proposed time series foundation models applicable to various forecasting tasks. Simultaneously, based on the recognition that real-world time series exhibit chaotic properties, methods have been developed to artificially generate synthetic chaotic time series, construct diverse datasets and train models. In this study, we propose a methodology for modeling financial time series by generating artificial chaotic time series and applying resampling techniques to simulate financial time series data, which we then use as training samples. Increasing the resampling interval to extend predictive horizons, we conducted large-scale pre-training using 10 billion training samples for each case. We subsequently created test datasets for multiple timeframes using actual Bitcoin trade data and performed zero-shot prediction without re-training the pre-trained model. The results of evaluating the profitability of a simple trading strategy based on these predictions demonstrated significant performance improvements over autocorrelation models. During the large-scale pre-training process, we observed a scaling law-like phenomenon that we can achieve predictive performance at a certain level with extended predictive horizons for chaotic time series by increasing the number of training samples exponentially. If this scaling law proves robust and holds true across various chaotic models, it suggests the potential to predict near-future events by investing substantial computational resources. Future research should focus on further large-scale training and verifying the applicability of this scaling law to diverse chaotic models.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A transformer-BiGRU-based framework with data augmentation and confident learning for network intrusion detection</title>
<link>https://arxiv.org/abs/2509.04925</link>
<guid>https://arxiv.org/abs/2509.04925</guid>
<content:encoded><![CDATA[
<div> machine learning, deep learning, network intrusion, TrailGate, data augmentation

Summary:
TrailGate is a novel framework that integrates machine learning and deep learning techniques to improve network intrusion detection systems. By combining Transformer and BiGRU architectures with advanced feature selection and data augmentation strategies, TrailGate excels at identifying common attack types and mitigating emerging threats. This approach addresses the limitations of conventional machine learning methods, which struggle with complex patterns in network intrusion datasets due to data scarcity and class imbalance. TrailGate's algorithmic fusion allows it to quickly detect and neutralize both known attack types and emerging threats, making it a robust solution for the fast-paced digital communication landscape. <div>
arXiv:2509.04925v1 Announce Type: new 
Abstract: In today's fast-paced digital communication, the surge in network traffic data and frequency demands robust and precise network intrusion solutions. Conventional machine learning methods struggle to grapple with complex patterns within the vast network intrusion datasets, which suffer from data scarcity and class imbalance. As a result, we have integrated machine learning and deep learning techniques within the network intrusion detection system to bridge this gap. This study has developed TrailGate, a novel framework that combines machine learning and deep learning techniques. By integrating Transformer and Bidirectional Gated Recurrent Unit (BiGRU) architectures with advanced feature selection strategies and supplemented by data augmentation techniques, TrailGate can identifies common attack types and excels at detecting and mitigating emerging threats. This algorithmic fusion excels at detecting common and well-understood attack types and has the unique ability to swiftly identify and neutralize emerging threats that stem from existing paradigms.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontology-Aligned Embeddings for Data-Driven Labour Market Analytics</title>
<link>https://arxiv.org/abs/2509.04942</link>
<guid>https://arxiv.org/abs/2509.04942</guid>
<content:encoded><![CDATA[
<div> embedding, occupational data, language processing, machine learning, semantic search

Summary: 
This research paper addresses the challenge of reasoning across occupational data from different sources by proposing a scalable approach using language processing machine learning models. The study introduces an embedding-based alignment process that links free-form German job titles to established ontologies, enabling efficient classification through semantic search. The researchers leverage publicly available data from the German Federal Employment Agency to fine-tune a Sentence-BERT model, creating a similarity graph structure for approximate nearest-neighbor search. By framing the classification process as a semantic search problem, the approach offers flexibility for adding more classes and expanding to other ontologies and multilingual titles. The study highlights design decisions, ongoing challenges, and future directions for extending the graph with additional ontologies. 

<br /><br />Summary: <div>
arXiv:2509.04942v1 Announce Type: new 
Abstract: The limited ability to reason across occupational data from different sources is a long-standing bottleneck for data-driven labour market analytics. Previous research has relied on hand-crafted ontologies that allow such reasoning but are computationally expensive and require careful maintenance by human experts. The rise of language processing machine learning models offers a scalable alternative by learning shared semantic spaces that bridge diverse occupational vocabularies without extensive human curation. We present an embedding-based alignment process that links any free-form German job title to two established ontologies - the German Klassifikation der Berufe and the International Standard Classification of Education. Using publicly available data from the German Federal Employment Agency, we construct a dataset to fine-tune a Sentence-BERT model to learn the structure imposed by the ontologies. The enriched pairs (job title, embedding) define a similarity graph structure that we can use for efficient approximate nearest-neighbour search, allowing us to frame the classification process as a semantic search problem. This allows for greater flexibility, e.g., adding more classes. We discuss design decisions, open challenges, and outline ongoing work on extending the graph with other ontologies and multilingual titles.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Blinks in Healthy and Parkinson's EEG: A Deep Learning Perspective</title>
<link>https://arxiv.org/abs/2509.04951</link>
<guid>https://arxiv.org/abs/2509.04951</guid>
<content:encoded><![CDATA[
<div> EEG, blink detection, deep learning, neural networks, Parkinson's disease  
<br />  
Summary:  
- The paper explores the importance of blink detection in EEG signals for monitoring cognitive load and neurological disorders.  
- Various deep learning models were evaluated for accurately segmenting EEG signals into blinks and non-blinks.  
- A pipeline was proposed for blink detection using different numbers of frontal EEG electrodes.  
- The CNN-RNN hybrid model consistently outperformed other models, achieving high blink detection accuracy in both healthy subjects and patients with Parkinson's disease.  
- The study compared different neural network architectures for segmenting EEG recordings to compute blink rate and other statistics. <div>
arXiv:2509.04951v1 Announce Type: new 
Abstract: Blinks in electroencephalography (EEG) are often treated as unwanted artifacts. However, recent studies have demonstrated that blink rate and its variability are important physiological markers to monitor cognitive load, attention, and potential neurological disorders. This paper addresses the critical task of accurate blink detection by evaluating various deep learning models for segmenting EEG signals into involuntary blinks and non-blinks. We present a pipeline for blink detection using 1, 3, or 5 frontal EEG electrodes. The problem is formulated as a sequence-to-sequence task and tested on various deep learning architectures including standard recurrent neural networks, convolutional neural networks (both standard and depth-wise), temporal convolutional networks (TCN), transformer-based models, and hybrid architectures. The models were trained on raw EEG signals with minimal pre-processing. Training and testing was carried out on a public dataset of 31 subjects collected at UCSD. This dataset consisted of 15 healthy participants and 16 patients with Parkinson's disease allowing us to verify the model's robustness to tremor. Out of all models, CNN-RNN hybrid model consistently outperformed other models and achieved the best blink detection accuracy of 93.8%, 95.4% and 95.8% with 1, 3, and 5 channels in the healthy cohort and correspondingly 73.8%, 75.4% and 75.8% in patients with PD. The paper compares neural networks for the task of segmenting EEG recordings to involuntary blinks and no blinks allowing for computing blink rate and other statistics.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Normalization of Confusion Matrices: Methods and Geometric Interpretations</title>
<link>https://arxiv.org/abs/2509.04959</link>
<guid>https://arxiv.org/abs/2509.04959</guid>
<content:encoded><![CDATA[
<div> confusion matrix, classifiers, class similarity, distribution bias, bistochastic normalization

Summary:
Bistochastic normalization using Iterative Proportional Fitting is introduced to evaluate classifiers in heterogeneous settings. This method disentangles the factors of class similarity and distribution bias, providing a more accurate diagnosis of model behavior and supporting targeted improvements. There is a correspondence between confusion matrix normalizations and the model's internal class representations, which can be interpreted geometrically. By recovering the underlying structure of class similarity, bistochastic normalization offers a deeper understanding of classifier normalization and what it reveals about a classifier's performance. <div>
arXiv:2509.04959v1 Announce Type: new 
Abstract: The confusion matrix is a standard tool for evaluating classifiers by providing insights into class-level errors. In heterogeneous settings, its values are shaped by two main factors: class similarity -- how easily the model confuses two classes -- and distribution bias, arising from skewed distributions in the training and test sets. However, confusion matrix values reflect a mix of both factors, making it difficult to disentangle their individual contributions. To address this, we introduce bistochastic normalization using Iterative Proportional Fitting, a generalization of row and column normalization. Unlike standard normalizations, this method recovers the underlying structure of class similarity. By disentangling error sources, it enables more accurate diagnosis of model behavior and supports more targeted improvements. We also show a correspondence between confusion matrix normalizations and the model's internal class representations. Both standard and bistochastic normalizations can be interpreted geometrically in this space, offering a deeper understanding of what normalization reveals about a classifier.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Spectral Architectures for Causal Physics-Informed Networks</title>
<link>https://arxiv.org/abs/2509.04966</link>
<guid>https://arxiv.org/abs/2509.04966</guid>
<content:encoded><![CDATA[
<div> NeuSA; Physics-Informed Neural Networks; PDEs; spectral bias; causality<br />
<br />
Summary:<br />
Physics-Informed Neural Networks (PINNs) have limitations in solving complex initial-value problems due to spectral bias and causality violations. To address this, NeuSA (Neuro-Spectral Architectures) is introduced, combining classical spectral methods with Neural ODE (NODE) to solve linear and nonlinear PDEs with variable coefficients. NeuSA learns a spectral basis projection of the PDE, overcoming spectral bias and enforcing causality inherited from NODEs. It utilizes an initialization scheme based on classical methods for faster convergence near the target solution. Validation on benchmark wave equations demonstrates NeuSA's superior performance with improved temporal consistency and predictive accuracy compared to other architectures. Code and pretrained models will be released. <br /><br />Summary: <div>
arXiv:2509.04966v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful neural framework for solving partial differential equations (PDEs). However, standard MLP-based PINNs often fail to converge when dealing with complex initial-value problems, leading to solutions that violate causality and suffer from a spectral bias towards low-frequency components. To address these issues, we introduce NeuSA (Neuro-Spectral Architectures), a novel class of PINNs inspired by classical spectral methods, designed to solve linear and nonlinear PDEs with variable coefficients. NeuSA learns a projection of the underlying PDE onto a spectral basis, leading to a finite-dimensional representation of the dynamics which is then integrated with an adapted Neural ODE (NODE). This allows us to overcome spectral bias, by leveraging the high-frequency components enabled by the spectral representation; to enforce causality, by inheriting the causal structure of NODEs, and to start training near the target solution, by means of an initialization scheme based on classical methods. We validate NeuSA on canonical benchmarks for linear and nonlinear wave equations, demonstrating strong performance as compared to other architectures, with faster convergence, improved temporal consistency and superior predictive accuracy. Code and pretrained models will be released.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology-Aware Graph Reinforcement Learning for Dynamic Routing in Cloud Networks</title>
<link>https://arxiv.org/abs/2509.04973</link>
<guid>https://arxiv.org/abs/2509.04973</guid>
<content:encoded><![CDATA[
<div> Topology-aware, graph reinforcement learning, routing policy optimization, cloud server environments, dynamic topologies

Summary: 
This paper introduces a novel approach to optimizing routing policies in cloud server environments using a topology-aware graph reinforcement learning method. The proposed method integrates a Structure-Aware State Encoding (SASE) module and a Policy-Adaptive Graph Update (PAGU) mechanism to address decision instability and lack of structural awareness in dynamic topologies. The SASE module employs multi-layer graph convolution and structural positional embeddings to capture high-order dependencies in communication topologies and enhance state representations. The PAGU module enables adaptive structural updates based on policy behavior shifts and reward feedback in dynamic environments. Experimental evaluations on the GEANT topology dataset demonstrate superior performance in terms of throughput, latency control, and link balance compared to existing graph reinforcement learning models. Further experiments examine the impact of structure modeling and graph updates on model stability and decision quality, affirming the efficiency and robustness of the proposed method in complex cloud networks.<br /><br />Summary: <div>
arXiv:2509.04973v1 Announce Type: new 
Abstract: This paper proposes a topology-aware graph reinforcement learning approach to address the routing policy optimization problem in cloud server environments. The method builds a unified framework for state representation and structural evolution by integrating a Structure-Aware State Encoding (SASE) module and a Policy-Adaptive Graph Update (PAGU) mechanism. It aims to tackle the challenges of decision instability and insufficient structural awareness under dynamic topologies. The SASE module models node states through multi-layer graph convolution and structural positional embeddings, capturing high-order dependencies in the communication topology and enhancing the expressiveness of state representations. The PAGU module adjusts the graph structure based on policy behavior shifts and reward feedback, enabling adaptive structural updates in dynamic environments. Experiments are conducted on the real-world GEANT topology dataset, where the model is systematically evaluated against several representative baselines in terms of throughput, latency control, and link balance. Additional experiments, including hyperparameter sensitivity, graph sparsity perturbation, and node feature dimensionality variation, further explore the impact of structure modeling and graph updates on model stability and decision quality. Results show that the proposed method outperforms existing graph reinforcement learning models across multiple performance metrics, achieving efficient and robust routing in dynamic and complex cloud networks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapt in the Wild: Test-Time Entropy Minimization with Sharpness and Feature Regularization</title>
<link>https://arxiv.org/abs/2509.04977</link>
<guid>https://arxiv.org/abs/2509.04977</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-time adaptation, batch norm layer, stability, entropy minimization, representation collapse 

Summary: 
Test-time adaptation (TTA) can be hindered by mixed distribution shifts, small batch sizes, and imbalanced label distribution shifts. The batch norm layer is identified as a critical factor affecting TTA stability. Group or layer norms can enhance TTA stability, but still encounter challenges such as model collapse. To address this, a sharpness-aware and reliable entropy minimization method (SAR) is proposed to stabilize TTA by filtering noisy samples with large gradients and promoting model robustness. Further enhancement is achieved with SAR^2, incorporating regularizers to prevent representation collapse and biased predictions. The methods show improved stability and computational efficiency for TTA in challenging test scenarios. 

Summary: <div>
arXiv:2509.04977v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, 3) online imbalanced label distribution shifts. This is often a key obstacle preventing existing TTA methods from being deployed in the real world. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, i.e., group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases, i.e., the model collapses into trivial solutions by assigning the same class label for all samples. By digging into this, we find that, during the collapse process: 1) the model gradients often undergo an initial explosion followed by rapid degradation, suggesting that certain noisy test samples with large gradients may disrupt adaptation; and 2) the model representations tend to exhibit high correlations and classification bias. To address this, we first propose a sharpness-aware and reliable entropy minimization method, called SAR, for stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Based on SAR, we further introduce SAR^2 to prevent representation collapse with two regularizers: 1) a redundancy regularizer to reduce inter-dimensional correlations among centroid-invariant features; and 2) an inequity regularizer to maximize the prediction entropy of a prototype centroid, thereby penalizing biased representations toward any specific class. Promising results demonstrate that our methods perform more stably over prior methods and are computationally efficient under the above wild test scenarios.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Evolution of Proteins via Bayesian Optimization in Embedding Space</title>
<link>https://arxiv.org/abs/2509.04998</link>
<guid>https://arxiv.org/abs/2509.04998</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, protein language model, machine learning, directed evolution, sequence embeddings <br />
Summary: <br />
Directed evolution involves improving protein function through iterative synthesis and evaluation of variants. Machine learning can enhance this process by selecting promising variants for screening, reducing screening needs. A new method combining Bayesian optimization with a representation based on protein language model embeddings significantly enhances performance compared to existing methods. This representation leads to better results with the same number of screenings. The approach outperforms state-of-the-art machine-learning methods in protein evolution with regression objectives. <div>
arXiv:2509.04998v1 Announce Type: new 
Abstract: Directed evolution is an iterative laboratory process of designing proteins with improved function by iteratively synthesizing new protein variants and evaluating their desired property with expensive and time-consuming biochemical screening. Machine learning methods can help select informative or promising variants for screening to increase their quality and reduce the amount of necessary screening. In this paper, we present a novel method for machine-learning-assisted directed evolution of proteins which combines Bayesian optimization with informative representation of protein variants extracted from a pre-trained protein language model. We demonstrate that the new representation based on the sequence embeddings significantly improves the performance of Bayesian optimization yielding better results with the same number of conducted screening in total. At the same time, our method outperforms the state-of-the-art machine-learning-assisted directed evolution methods with regression objective.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Aware Initialization for Stable and Efficient Neural Network Training</title>
<link>https://arxiv.org/abs/2509.05018</link>
<guid>https://arxiv.org/abs/2509.05018</guid>
<content:encoded><![CDATA[
<div> initialization schemes, unit variance, gradient propagation, network depth, variance increase<br />
<br />
Summary:<br />
This paper presents a comprehensive study on initialization schemes in neural networks, considering the depth information of each layer and the total network. While previous methods focus on maintaining unit variance and gradient propagation, it is found that for deeper networks, simply assuming unit variance does not perform well. A novel approach is proposed to increase the variance of the network in a flexible manner, taking into account the depth of each layer. Experiments demonstrate that the proposed method outperforms existing initialization schemes. <div>
arXiv:2509.05018v1 Announce Type: new 
Abstract: In past few years, various initialization schemes have been proposed. These schemes are glorot initialization, He initialization, initialization using orthogonal matrix, random walk method for initialization. Some of these methods stress on keeping unit variance of activation and gradient propagation through the network layer. Few of these methods are independent of the depth information while some methods has considered the total network depth for better initialization. In this paper, comprehensive study has been done where depth information of each layer as well as total network is incorporated for better initialization scheme. It has also been studied that for deeper networks theoretical assumption of unit variance throughout the network does not perform well. It requires the need to increase the variance of the network from first layer activation to last layer activation. We proposed a novel way to increase the variance of the network in flexible manner, which incorporates the information of each layer depth. Experiments shows that proposed method performs better than the existing initialization scheme.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiSurv: A Multimodal Deep Survival Framework for Prostrate and Bladder Cancer</title>
<link>https://arxiv.org/abs/2509.05037</link>
<guid>https://arxiv.org/abs/2509.05037</guid>
<content:encoded><![CDATA[
<div> Keywords: MultiSurv, deep survival model, multimodal integration, prostate cancer, bladder cancer

Summary: 
- MultiSurv is introduced as a deep survival model incorporating clinical, MRI, RNA-seq, and pathology data for accurate time-to-event prediction in prostate and bladder cancer.
- The model utilizes DeepHit with a projection layer and inter-modality cross-attention to capture complementary prognostic signals across modalities.
- Performance evaluation on the CHIMERA Grand Challenge shows a high concordance index (C-index) of 0.843 for prostate cancer and 0.662 for bladder cancer, indicating robust discriminatory ability.
- The model demonstrates adaptability and potential for clinical translation, highlighting its relevance for personalised risk stratification in prostate and bladder cancer.
- The proposed framework showcases the effectiveness of leveraging multimodal integration with deep survival learning for survival prediction tasks leveraging heterogeneous biomedical data.

<br /><br />Summary: <div>
arXiv:2509.05037v1 Announce Type: new 
Abstract: Accurate prediction of time-to-event outcomes is a central challenge in oncology, with significant implications for treatment planning and patient management. In this work, we present MultiSurv, a multimodal deep survival model utilising DeepHit with a projection layer and inter-modality cross-attention, which integrates heterogeneous patient data, including clinical, MRI, RNA-seq and whole-slide pathology features. The model is designed to capture complementary prognostic signals across modalities and estimate individualised time-to-biochemical recurrence in prostate cancer and time-to-cancer recurrence in bladder cancer. Our approach was evaluated in the context of the CHIMERA Grand Challenge, across two of the three provided tasks. For Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed framework achieved a concordance index (C-index) of 0.843 on 5-folds cross-validation and 0.818 on CHIMERA development set, demonstrating robust discriminatory ability. For Task 3 (bladder cancer recurrence prediction), the model obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on development set, highlighting its adaptability and potential for clinical translation. These results suggest that leveraging multimodal integration with deep survival learning provides a promising pathway toward personalised risk stratification in prostate and bladder cancer. Beyond the challenge setting, our framework is broadly applicable to survival prediction tasks involving heterogeneous biomedical data.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent State Encoders for Efficient Neural Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2509.05084</link>
<guid>https://arxiv.org/abs/2509.05084</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Combinatorial Optimization, Recurrent Encoder, Traveling Salesman Problem, Capacitated Vehicle Routing Problem, Orienteering Problem

Summary:
Neural Combinatorial Optimization (NCO) primarily utilizes construction methods where a neural network constructs solutions by adding one component at a time. The proposed approach introduces a recurrent encoder that computes state embeddings based on both the current state and embeddings from the previous step. This method allows for efficient reuse of computation, resulting in equivalent or better performance with fewer layers compared to non-recurrent encoders. The study evaluates the approach on three different combinatorial optimization problems: the Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem (CVRP), and the Orienteering Problem (OP). The models are integrated into a large neighborhood search algorithm, demonstrating practical relevance and efficiency in optimizing solutions for these complex problems. <div>
arXiv:2509.05084v1 Announce Type: new 
Abstract: The primary paradigm in Neural Combinatorial Optimization (NCO) are construction methods, where a neural network is trained to sequentially add one solution component at a time until a complete solution is constructed. We observe that the typical changes to the state between two steps are small, since usually only the node that gets added to the solution is removed from the state. An efficient model should be able to reuse computation done in prior steps. To that end, we propose to train a recurrent encoder that computes the state embeddings not only based on the state but also the embeddings of the step before. We show that the recurrent encoder can achieve equivalent or better performance than a non-recurrent encoder even if it consists of $3\times$ fewer layers, thus significantly improving on latency. We demonstrate our findings on three different problems: the Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem (CVRP), and the Orienteering Problem (OP) and integrate the models into a large neighborhood search algorithm, to showcase the practical relevance of our findings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of Manufactured Solutions</title>
<link>https://arxiv.org/abs/2509.05117</link>
<guid>https://arxiv.org/abs/2509.05117</guid>
<content:encoded><![CDATA[
<div> HyPINO, multi-physics neural operator, zero-shot generalization, Swin Transformer-based hypernetwork, mixed supervision, labeled data, analytical solutions, Method of Manufactured Solutions, unlabeled samples, physics-informed objectives<br />
<br />
Summary:<br />
HyPINO is a novel multi-physics neural operator that achieves zero-shot generalization across a range of parametric PDEs without task-specific fine-tuning. By combining a Swin Transformer-based hypernetwork with mixed supervision, including labeled data from analytical solutions and unlabeled samples optimized using physics-informed objectives, HyPINO outperforms existing models on seven benchmark problems. An iterative refinement procedure generates a "delta" PINN based on the discrepancy between the generated PINN and the requested PDE, leading to significant error reduction in ensemble solutions. PINNs initialized by HyPINO show faster convergence and lower final error compared to randomly initialized and Reptile-meta-learned PINNs on five benchmarks. This scalable approach demonstrates promising potential for solving complex, nonlinear, and high-dimensional PDE problems with improved accuracy and reduced computational cost. <br /> <div>
arXiv:2509.05117v1 Announce Type: new 
Abstract: We present HyPINO, a multi-physics neural operator designed for zero-shot generalization across a broad class of parametric PDEs without requiring task-specific fine-tuning. Our approach combines a Swin Transformer-based hypernetwork with mixed supervision: (i) labeled data from analytical solutions generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled samples optimized using physics-informed objectives. The model maps PDE parametrizations to target Physics-Informed Neural Networks (PINNs) and can handle linear elliptic, hyperbolic, and parabolic equations in two dimensions with varying source terms, geometries, and mixed Dirichlet/Neumann boundary conditions, including interior boundaries. HyPINO achieves strong zero-shot accuracy on seven benchmark problems from PINN literature, outperforming U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we introduce an iterative refinement procedure that compares the physics of the generated PINN to the requested PDE and uses the discrepancy to generate a "delta" PINN. Summing their contributions and repeating this process forms an ensemble whose combined solution progressively reduces the error on six benchmarks and achieves over 100x gain in average $L_2$ loss in the best case, while retaining forward-only inference. Additionally, we evaluate the fine-tuning behavior of PINNs initialized by HyPINO and show that they converge faster and to lower final error than both randomly initialized and Reptile-meta-learned PINNs on five benchmarks, performing on par on the remaining two. Our results highlight the potential of this scalable approach as a foundation for extending neural operators toward solving increasingly complex, nonlinear, and high-dimensional PDE problems with significantly improved accuracy and reduced computational cost.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Should We Always Train Models on Fine-Grained Classes?</title>
<link>https://arxiv.org/abs/2509.05130</link>
<guid>https://arxiv.org/abs/2509.05130</guid>
<content:encoded><![CDATA[
<div> hierarchical labels, classification, fine-grained training, dataset structure, model capacity 
Summary:
Fine-grained training on hierarchical labels may not universally enhance classification accuracy, as its effectiveness depends on the data geometry and label hierarchy relations. Geometric structure and dataset size play crucial roles in determining the performance benefit of using fine-grained labels. Factors such as model capacity also significantly impact the efficacy of this training strategy. The study investigates the generality of this observation through real and synthetic datasets, highlighting the complex interplay between data characteristics and training approaches in classification tasks. Understanding the nuanced factors influencing the performance of fine-grained training on hierarchical labels can guide researchers and practitioners in optimizing classification models for different dataset contexts. <div>
arXiv:2509.05130v1 Announce Type: new 
Abstract: In classification problems, models must predict a class label based on the input data features. However, class labels are organized hierarchically in many datasets. While a classification task is often defined at a specific level of this hierarchy, training can utilize a finer granularity of labels. Empirical evidence suggests that such fine-grained training can enhance performance. In this work, we investigate the generality of this observation and explore its underlying causes using both real and synthetic datasets. We show that training on fine-grained labels does not universally improve classification accuracy. Instead, the effectiveness of this strategy depends critically on the geometric structure of the data and its relations with the label hierarchy. Additionally, factors such as dataset size and model capacity significantly influence whether fine-grained labels provide a performance benefit.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Learnability of Distribution Classes with Adaptive Adversaries</title>
<link>https://arxiv.org/abs/2509.05137</link>
<guid>https://arxiv.org/abs/2509.05137</guid>
<content:encoded><![CDATA[
<div> Distribution classes, learnability, adaptive adversaries, manipulations, budget <br />
Summary: <br />
The article discusses the learnability of distribution classes in the presence of adaptive adversaries, who can manipulate samples before passing them to the learner. This is contrasted with oblivious adversaries who can only modify the underlying distribution of samples. The concept of learnability with respect to adaptive adversaries is introduced, considering the adversary's budget. It is shown that learnability with respect to additive adaptive adversaries is a stronger condition than learnability with respect to additive oblivious adversaries. This distinction highlights the importance of understanding and accounting for the capabilities of adversaries in the context of learning distribution classes. <div>
arXiv:2509.05137v1 Announce Type: new 
Abstract: We consider the question of learnability of distribution classes in the presence of adaptive adversaries -- that is, adversaries capable of intercepting the samples requested by a learner and applying manipulations with full knowledge of the samples before passing it on to the learner. This stands in contrast to oblivious adversaries, who can only modify the underlying distribution the samples come from but not their i.i.d.\ nature. We formulate a general notion of learnability with respect to adaptive adversaries, taking into account the budget of the adversary. We show that learnability with respect to additive adaptive adversaries is a strictly stronger condition than learnability with respect to additive oblivious adversaries.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights</title>
<link>https://arxiv.org/abs/2509.05142</link>
<guid>https://arxiv.org/abs/2509.05142</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Foundational Models, Taxonomy, Healthcare, Survey<br />
Summary:<br />
The article explores the integration of federated learning and foundational models, providing a comprehensive survey and taxonomy of methods. It identifies, categorizes, and characterizes technical methods for collaborative model training without sharing private data. The focus is on developing practical insights and guidelines for implementing these methods in the healthcare domain. The survey covers topics like self-supervised learning, fine-tuning, distillation, and transfer learning, presenting a comparison of 42 unique methods based on complexity, efficiency, and scalability. With over 4,200 articles initially reviewed, the collection was narrowed down to 250 articles for in-depth analysis, emphasizing the importance of integrating foundational models with federated learning for enhanced data privacy and collaborative model training.<br /> <div>
arXiv:2509.05142v1 Announce Type: new 
Abstract: Federated learning has the potential to unlock siloed data and distributed resources by enabling collaborative model training without sharing private data. As more complex foundational models gain widespread use, the need to expand training resources and integrate privately owned data grows as well. In this article, we explore the intersection of federated learning and foundational models, aiming to identify, categorize, and characterize technical methods that integrate the two paradigms. As a unified survey is currently unavailable, we present a literature survey structured around a novel taxonomy that follows the development life-cycle stages, along with a technical comparison of available methods. Additionally, we provide practical insights and guidelines for implementing and evolving these methods, with a specific focus on the healthcare domain as a case study, where the potential impact of federated learning and foundational models is considered significant. Our survey covers multiple intersecting topics, including but not limited to federated learning, self-supervised learning, fine-tuning, distillation, and transfer learning. Initially, we retrieved and reviewed a set of over 4,200 articles. This collection was narrowed to more than 250 thoroughly reviewed articles through inclusion criteria, featuring 42 unique methods. The methods were used to construct the taxonomy and enabled their comparison based on complexity, efficiency, and scalability. We present these results as a self-contained overview that not only summarizes the state of the field but also provides insights into the practical aspects of adopting, evolving, and integrating foundational models with federated learning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVCompose: Efficient Structured KV Cache Compression with Composite Tokens</title>
<link>https://arxiv.org/abs/2509.05165</link>
<guid>https://arxiv.org/abs/2509.05165</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, KV cache compression, attention-guided, layer-adaptive, long-context inference

Summary:
KV cache size in large language models (LLMs) can become a bottleneck for efficient autoregressive decoding due to its linear growth with context length and model depth. Existing compression methods have limitations such as rigid heuristics or disrupting tensor layouts. This study introduces a novel framework for KV cache compression based on attention-guided, layer-adaptive composite tokens. It aggregates attention scores to estimate token importance, selects head-specific tokens independently, and aligns them into composite tokens that respect the required uniform cache structure. A global allocation mechanism further adjusts retention budgets across layers according to token informativeness. The proposed method achieves significant memory reduction while maintaining high accuracy, surpassing prior compression techniques. Importantly, it is fully compatible with standard inference pipelines, providing a practical and scalable solution for efficient long-context LLM deployment.<br /><br />Summary: Large language models often face challenges in efficient autoregressive decoding due to the linear growth of KV cache size with context length and model depth. This study presents a novel compression framework based on attention-guided, layer-adaptive composite tokens, offering a practical solution for memory reduction in long-context inference tasks. <div>
arXiv:2509.05165v1 Announce Type: new 
Abstract: Large language models (LLMs) rely on key-value (KV) caches for efficient autoregressive decoding; however, cache size grows linearly with context length and model depth, becoming a major bottleneck in long-context inference. Prior KV cache compression methods either enforce rigid heuristics, disrupt tensor layouts with per-attention-head variability, or require specialized compute kernels.
  We propose a simple, yet effective, KV cache compression framework based on attention-guided, layer-adaptive composite tokens. Our method aggregates attention scores to estimate token importance, selects head-specific tokens independently, and aligns them into composite tokens that respect the uniform cache structure required by existing inference engines. A global allocation mechanism further adapts retention budgets across layers, assigning more capacity to layers with informative tokens. This approach achieves significant memory reduction while preserving accuracy, consistently outperforming prior structured and semi-structured methods. Crucially, our approach remains fully compatible with standard inference pipelines, offering a practical and scalable solution for efficient long-context LLM deployment.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection</title>
<link>https://arxiv.org/abs/2509.05190</link>
<guid>https://arxiv.org/abs/2509.05190</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, CNN, Seizure detection, Structured pruning, Efficiency

Summary:
This study introduces a lightweight one-dimensional CNN model with structured pruning to enhance efficiency and reliability in EEG-based seizure detection. The model, trained with mild early stopping to prevent overfitting, achieved an accuracy of 92.78% and a macro-F1 score of 0.8686. Structured pruning involved removing 50% of convolutional kernels based on importance to model predictions. Surprisingly, even after pruning, the network maintained its predictive abilities while increasing precision to 92.87% and improving the macro-F1 score to 0.8707. The findings suggest that structured pruning eliminates redundancy, enhances generalization, and in combination with mild early stopping, offers a promising approach to boosting seizure detection efficiency and reliability, particularly in resource-limited settings. <div>
arXiv:2509.05190v1 Announce Type: new 
Abstract: Deep learning models, especially convolutional neural networks (CNNs), have shown considerable promise for biomedical signals such as EEG-based seizure detection. However, these models come with challenges, primarily due to their size and compute requirements in environments where real-time detection or limited resources are available. In this study, we present a lightweight one-dimensional CNN model with structured pruning to improve efficiency and reliability. The model was trained with mild early stopping to address possible overfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686. Structured pruning of the baseline CNN involved removing 50% of the convolutional kernels based on their importance to model predictions. Surprisingly, after pruning the weights and memory by 50%, the new network was still able to maintain predictive capabilities, while modestly increasing precision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we present a convincing case that structured pruning removes redundancy, improves generalization, and, in combination with mild early stopping, achieves a promising way forward to improve seizure detection efficiency and reliability, which is clear motivation for resource-limited settings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.05193</link>
<guid>https://arxiv.org/abs/2509.05193</guid>
<content:encoded><![CDATA[
<div> challenge, low-rank structure, shifted successor measure, performance guarantees, functional inequalities<br />
<br />
Summary: In this new article, the authors question the common assumption of low-rank structure in reinforcement learning algorithms, highlighting that the successor measure is not low-rank itself. They propose the concept of the shifted successor measure as a naturally emerging low-rank structure capturing system dynamics after initial transitions. The study provides guarantees for estimating a low-rank approximation of the shifted successor measure. Analysis shows that approximation errors are influenced by spectral recoverability, requiring a small shift for effective estimation. The article introduces Type II Poincaré inequalities for Markov chains to quantify this shift and establish a link between shift magnitude and local mixing properties. Experimental validation confirms the performance enhancement in goal-conditioned RL through shifting the successor measure. <div>
arXiv:2509.05193v1 Announce Type: new 
Abstract: Low-rank structure is a common implicit assumption in many modern reinforcement learning (RL) algorithms. For instance, reward-free and goal-conditioned RL methods often presume that the successor measure admits a low-rank representation. In this work, we challenge this assumption by first remarking that the successor measure itself is not low-rank. Instead, we demonstrate that a low-rank structure naturally emerges in the shifted successor measure, which captures the system dynamics after bypassing a few initial transitions. We provide finite-sample performance guarantees for the entry-wise estimation of a low-rank approximation of the shifted successor measure from sampled entries. Our analysis reveals that both the approximation and estimation errors are primarily governed by the so-called spectral recoverability of the corresponding matrix. To bound this parameter, we derive a new class of functional inequalities for Markov chains that we call Type II Poincar\'e inequalities and from which we can quantify the amount of shift needed for effective low-rank approximation and estimation. This analysis shows in particular that the required shift depends on decay of the high-order singular values of the shifted successor measure and is hence typically small in practice. Additionally, we establish a connection between the necessary shift and the local mixing properties of the underlying dynamical system, which provides a natural way of selecting the shift. Finally, we validate our theoretical findings with experiments, and demonstrate that shifting the successor measure indeed leads to improved performance in goal-conditioned RL.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.05207</link>
<guid>https://arxiv.org/abs/2509.05207</guid>
<content:encoded><![CDATA[
<div> sampling-based approach, distributed training, GNN, graph datasets, RapidGNN <br />
<br />
Summary: 
RapidGNN is a distributed framework designed for training Graph Neural Networks (GNNs) on large-scale graphs efficiently. It employs deterministic sampling-based scheduling to aid in cache construction and prefetching of remote features, addressing the challenges of high connectivity in datasets. The framework showcases its effectiveness across various scales and topologies, improving end-to-end training throughput by an average factor of 2.46x to 3.00x compared to baseline methods while significantly reducing remote feature fetches by 9.70x to 15.39x. RapidGNN exhibits near-linear scalability with an increasing number of computing units and achieves a boost in energy efficiency, cutting down energy consumption by 44% for CPUs and 32% for GPUs, outperforming the baseline methods in terms of scalability and energy utilization. <div>
arXiv:2509.05207v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have become popular across a diverse set of tasks in exploring structural relationships between entities. However, due to the highly connected structure of the datasets, distributed training of GNNs on large-scale graphs poses significant challenges. Traditional sampling-based approaches mitigate the computational loads, yet the communication overhead remains a challenge. This paper presents RapidGNN, a distributed GNN training framework with deterministic sampling-based scheduling to enable efficient cache construction and prefetching of remote features. Evaluation on benchmark graph datasets demonstrates RapidGNN's effectiveness across different scales and topologies. RapidGNN improves end-to-end training throughput by 2.46x to 3.00x on average over baseline methods across the benchmark datasets, while cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further demonstrates near-linear scalability with an increasing number of computing units efficiently. Furthermore, it achieves increased energy efficiency over the baseline methods for both CPU and GPU by 44% and 32%, respectively.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Subspace Algorithm for Federated Learning on Heterogeneous Data</title>
<link>https://arxiv.org/abs/2509.05213</link>
<guid>https://arxiv.org/abs/2509.05213</guid>
<content:encoded><![CDATA[
<div> client drift, data heterogeneity, federated learning, deep neural networks, subspace algorithm
Summary: 
This work introduces FedSub, a subspace algorithm designed to tackle the challenges of applying federated learning to large-scale deep neural networks. It addresses the issues of client drift, data heterogeneity, and high communication, computation, and memory costs. FedSub utilizes subspace projection to ensure local updates within low-dimensional subspaces, reducing costs associated with communication and computation. It also incorporates low-dimensional dual variables to counter client drift. The convergence analysis highlights the impact of factors such as step size and subspace projection matrices on the convergence of the algorithm. Experimental results demonstrate the efficiency of FedSub in overcoming these challenges and improving the performance of federated learning on heterogeneous data.<br /><br />Summary: <div>
arXiv:2509.05213v1 Announce Type: new 
Abstract: This work addresses the key challenges of applying federated learning to large-scale deep neural networks, particularly the issue of client drift due to data heterogeneity across clients and the high costs of communication, computation, and memory. We propose FedSub, an efficient subspace algorithm for federated learning on heterogeneous data. Specifically, FedSub utilizes subspace projection to guarantee local updates of each client within low-dimensional subspaces, thereby reducing communication, computation, and memory costs. Additionally, it incorporates low-dimensional dual variables to mitigate client drift. We provide convergence analysis that reveals the impact of key factors such as step size and subspace projection matrices on convergence. Experimental results demonstrate its efficiency.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Enhanced for Amine Emission Monitoring and Performance Analysis in Industrial Carbon Capture Plants</title>
<link>https://arxiv.org/abs/2509.05241</link>
<guid>https://arxiv.org/abs/2509.05241</guid>
<content:encoded><![CDATA[
<div> deep learning, forecasting, amine emissions, carbon capture systems, performance parameters

Summary: 
The article introduces data-driven deep learning models designed to forecast and monitor amine emissions and key performance parameters in amine-based carbon capture systems. Four different deep learning architectures were developed to capture time-dependent process behavior. These models achieved high predictive accuracy exceeding 99% for both emission prediction and system performance tracking. Causal impact analysis was conducted to assess the influence of operational variables on emissions and system performance, revealing specific parameters that can significantly reduce amine emissions and enhance system efficiency. The study demonstrates the potential of machine learning as a predictive and decision support tool for optimizing carbon capture operations under various conditions. The developed framework enables real-time monitoring, scenario testing, and operational optimization, offering a practical pathway for reducing environmental impacts and enhancing the efficiency and sustainability of carbon capture and storage technologies. <div>
arXiv:2509.05241v1 Announce Type: new 
Abstract: We present data driven deep learning models for forecasting and monitoring amine emissions and key performance parameters in amine-based post-combustion carbon capture systems. Using operational data from the CESAR1 solvent campaign at Technology Center Mongstad, four DL architectures such as Basic Long Short-Term Memory (LSTM), Stacked LSTM, Bi-directional LSTM, and Convolutional LSTM were developed to capture time-dependent process behavior. For emission prediction, models were designed for 2-amino-2-methyl-1-propanol (AMP) and Piperazine emissions measured via FTIR and IMR-MS methods. System performance models target four critical parameters: CO$_2$ product flow, absorber outlet temperature, depleted flue gas outlet temperature, and RFCC stripper bottom temperature. These models achieved high predictive accuracy exceeding 99% and effectively tracked both steady trends and abrupt fluctuations. Additionally, we conducted causal impact analysis to evaluate how operational variables influence emissions and system performance. Eight input variables were systematically perturbed within $\pm$20% of nominal values to simulate deviations and assess their impact. This analysis revealed that adjusting specific operational parameters, such as lean solvent temperature and water wash conditions, can significantly reduce amine emissions and enhance system performance. This study highlights ML not only as a predictive tool but also as a decision support system for optimizing carbon capture operations under steady state and dynamic conditions. By enabling real time monitoring, scenario testing, and operational optimization, the developed ML framework offers a practical pathway for mitigating environmental impacts. This work represents a step toward intelligent, data-driven control strategies that enhance the efficiency, stability, and sustainability of carbon capture and storage technologies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in AGC Systems</title>
<link>https://arxiv.org/abs/2509.05259</link>
<guid>https://arxiv.org/abs/2509.05259</guid>
<content:encoded><![CDATA[
<div> Automatic Generation Control (AGC); False Data Injection Attacks (FDIAs); Kolmogorov-Arnold Networks (KAN); interpretability; cybersecurity
Summary: 
Kolmogorov-Arnold Networks (KAN) are proposed for detecting False Data Injection Attacks (FDIAs) in AGC systems with high accuracy and interpretability. The KAN model considers system nonlinearities and extracts symbolic equations for enhanced interpretability. The model is trained offline to learn complex nonlinear relationships in AGC measurements under various scenarios. The KAN model achieves FDIA detection rates of up to 95.97% and 95.9% for the initial model and symbolic formula, respectively, with a low false alarm rate. This approach offers a reliable method for enhancing AGC cybersecurity by providing interpretable and accurate detection of stealthy cyberattacks. 
<br /><br /> <div>
arXiv:2509.05259v1 Announce Type: new 
Abstract: Automatic Generation Control (AGC) is essential for power grid stability but remains vulnerable to stealthy cyberattacks, such as False Data Injection Attacks (FDIAs), which can disturb the system's stability while evading traditional detection methods. Unlike previous works that relied on blackbox approaches, this work proposes Kolmogorov-Arnold Networks (KAN) as an interpretable and accurate method for FDIA detection in AGC systems, considering the system nonlinearities. KAN models include a method for extracting symbolic equations, and are thus able to provide more interpretability than the majority of machine learning models. The proposed KAN is trained offline to learn the complex nonlinear relationships between the AGC measurements under different operating scenarios. After training, symbolic formulas that describe the trained model's behavior can be extracted and leveraged, greatly enhancing interpretability. Our findings confirm that the proposed KAN model achieves FDIA detection rates of up to 95.97% and 95.9% for the initial model and the symbolic formula, respectively, with a low false alarm rate, offering a reliable approach to enhancing AGC cybersecurity.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks</title>
<link>https://arxiv.org/abs/2509.05273</link>
<guid>https://arxiv.org/abs/2509.05273</guid>
<content:encoded><![CDATA[
<div> energy efficiency, deep reinforcement learning, environmental impact, cost efficiency, sustainability

Summary: 
This study explores the energy consumption, greenhouse gas emissions, and monetary costs of seven state-of-the-art deep reinforcement learning algorithms. Using real-time measurements, the authors trained each algorithm on Atari 2600 games and analyzed their energy efficiency. The results show significant variation in energy consumption, CO2 emissions, and monetary costs among the algorithms. Some algorithms achieve comparable performance while consuming up to 24% less energy and incurring almost 68% lower monetary cost than less efficient counterparts. The study also highlights the trade-offs between learning performance, training time, energy use, and financial cost, providing insights into developing energy-aware and cost-efficient DRL practices. This research establishes a foundation for incorporating sustainability considerations into future algorithmic design and evaluation. 

<br /><br />Summary: <div>
arXiv:2509.05273v1 Announce Type: new 
Abstract: The growing computational demands of deep reinforcement learning (DRL) have raised concerns about the environmental and economic costs of training large-scale models. While algorithmic efficiency in terms of learning performance has been extensively studied, the energy requirements, greenhouse gas emissions, and monetary costs of DRL algorithms remain largely unexplored. In this work, we present a systematic benchmarking study of the energy consumption of seven state-of-the-art DRL algorithms, namely DQN, TRPO, A2C, ARS, PPO, RecurrentPPO, and QR-DQN, implemented using Stable Baselines. Each algorithm was trained for one million steps each on ten Atari 2600 games, and power consumption was measured in real-time to estimate total energy usage, CO2-Equivalent emissions, and electricity cost based on the U.S. national average electricity price. Our results reveal substantial variation in energy efficiency and training cost across algorithms, with some achieving comparable performance while consuming up to 24% less energy (ARS vs. DQN), emitting nearly 68% less CO2, and incurring almost 68% lower monetary cost (QR-DQN vs. RecurrentPPO) than less efficient counterparts. We further analyze the trade-offs between learning performance, training time, energy use, and financial cost, highlighting cases where algorithmic choices can mitigate environmental and economic impact without sacrificing learning performance. This study provides actionable insights for developing energy-aware and cost-efficient DRL practices and establishes a foundation for incorporating sustainability considerations into future algorithmic design and evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpikingBrain Technical Report: Spiking Brain-inspired Large Models</title>
<link>https://arxiv.org/abs/2509.05276</link>
<guid>https://arxiv.org/abs/2509.05276</guid>
<content:encoded><![CDATA[
<div> Efficiency, SpikingBrain, large language models, training, inference<br />
Summary:<br />
Efficiency is a key challenge in large language models, with quadratic scaling of training computation and linear growth of inference memory with sequence length. SpikingBrain introduces brain-inspired models for efficient long-context training and inference on non-NVIDIA platforms, leveraging MetaX GPU cluster. The models, SpikingBrain-7B and SpikingBrain-76B, use adaptive spiking neurons and efficient training pipeline. They achieve performance comparable to Transformer baselines using less token for pre-training. SpikingBrain models significantly improve long-sequence training efficiency, maintain stable training on MetaX C550 GPUs, and demonstrate low-power operation with high sparsity. The spiking scheme enables over 100x speedup in Time to First Token for 4M-token sequences, reaching 23.4 percent Model FLOPs Utilization. This work showcases how brain-inspired mechanisms can drive efficient and scalable large model design. <br /><br />Summary: <div>
arXiv:2509.05276v1 Announce Type: new 
Abstract: Mainstream Transformer-based large language models face major efficiency bottlenecks: training computation scales quadratically with sequence length, and inference memory grows linearly, limiting long-context processing. Building large models on non-NVIDIA platforms also poses challenges for stable and efficient training. To address this, we introduce SpikingBrain, a family of brain-inspired models designed for efficient long-context training and inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three aspects: (1) Model Architecture: linear and hybrid-linear attention architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an efficient, conversion-based training pipeline and a dedicated spike coding framework; (3) System Engineering: customized training frameworks, operator libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM, and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the feasibility of large-scale LLM development on non-NVIDIA platforms. SpikingBrain achieves performance comparable to open-source Transformer baselines while using only about 150B tokens for continual pre-training. Our models significantly improve long-sequence training efficiency and deliver inference with (partially) constant memory and event-driven spiking behavior. For example, SpikingBrain-7B attains over 100x speedup in Time to First Token for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4 percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling low-power operation. Overall, this work demonstrates the potential of brain-inspired mechanisms to drive the next generation of efficient and scalable large model design.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Branch Convolutional Framework for Spatial and Frequency-Based Image Forgery Detection</title>
<link>https://arxiv.org/abs/2509.05281</link>
<guid>https://arxiv.org/abs/2509.05281</guid>
<content:encoded><![CDATA[
<div> forgery detection, deepfakes, convolutional neural network, spatial features, frequency-based features<br />
<br />
Summary:<br />
This report introduces a forgery detection framework that utilizes a dual branch convolutional neural network to detect forgeries in images by combining spatial and frequency-based features. The method achieves an accuracy of 77.9% on the CASIA 2.0 dataset, outperforming traditional statistical approaches. Although not as complex as larger forgery detection pipelines, it strikes a balance between computational complexity and detection reliability, making it suitable for practical deployment. The proposed framework provides a strong methodology for digital image forensics and advances the state of the art in visual forensics. It addresses the urgent need for media verification, law enforcement, and ensuring the reliability of digital content. <div>
arXiv:2509.05281v1 Announce Type: new 
Abstract: With a very rapid increase in deepfakes and digital image forgeries, ensuring the authenticity of images is becoming increasingly challenging. This report introduces a forgery detection framework that combines spatial and frequency-based features for detecting forgeries. We propose a dual branch convolution neural network that operates on features extracted from spatial and frequency domains. Features from both branches are fused and compared within a Siamese network, yielding 64 dimensional embeddings for classification. When benchmarked on CASIA 2.0 dataset, our method achieves an accuracy of 77.9%, outperforming traditional statistical methods. Despite its relatively weaker performance compared to larger, more complex forgery detection pipelines, our approach balances computational complexity and detection reliability, making it ready for practical deployment. It provides a strong methodology for forensic scrutiny of digital images. In a broader sense, it advances the state of the art in visual forensics, addressing an urgent requirement in media verification, law enforcement and digital content reliability.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to accelerate distributed ADMM using graph neural networks</title>
<link>https://arxiv.org/abs/2509.05288</link>
<guid>https://arxiv.org/abs/2509.05288</guid>
<content:encoded><![CDATA[
<div> Keywords: distributed optimization, Alternating Direction Method of Multipliers (ADMM), graph neural networks (GNNs), adaptive step sizes, communication weights
<br />
Summary: 
Distributed optimization is essential for large-scale machine learning and control applications. The Alternating Direction Method of Multipliers (ADMM) has been widely used due to its convergence guarantees and decentralized computation capabilities. However, ADMM can be slow and sensitive to hyperparameter choices. This work introduces a novel approach that leverages the message-passing framework of graph neural networks (GNNs) to represent distributed ADMM iterations. By incorporating adaptive step sizes and communication weights learned through a GNN, the proposed method improves convergence speed and solution quality compared to standard ADMM. The network parameters are trained end-to-end by unrolling ADMM for a fixed number of iterations, minimizing the iterates error for a specific problem class while maintaining convergence properties. Experimental results support the effectiveness of the learned variant in enhancing the performance of distributed optimization algorithms. <div>
arXiv:2509.05288v1 Announce Type: new 
Abstract: Distributed optimization is fundamental in large-scale machine learning and control applications. Among existing methods, the Alternating Direction Method of Multipliers (ADMM) has gained popularity due to its strong convergence guarantees and suitability for decentralized computation. However, ADMM often suffers from slow convergence and sensitivity to hyperparameter choices. In this work, we show that distributed ADMM iterations can be naturally represented within the message-passing framework of graph neural networks (GNNs). Building on this connection, we propose to learn adaptive step sizes and communication weights by a graph neural network that predicts the hyperparameters based on the iterates. By unrolling ADMM for a fixed number of iterations, we train the network parameters end-to-end to minimize the final iterates error for a given problem class, while preserving the algorithm's convergence properties. Numerical experiments demonstrate that our learned variant consistently improves convergence speed and solution quality compared to standard ADMM. The code is available at https://github.com/paulhausner/learning-distributed-admm.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest</title>
<link>https://arxiv.org/abs/2509.05292</link>
<guid>https://arxiv.org/abs/2509.05292</guid>
<content:encoded><![CDATA[
arXiv:2509.05292v1 Announce Type: new 
Abstract: The ranking utility function in an ad recommender system, which linearly combines predictions of various business goals, plays a central role in balancing values across the platform, advertisers, and users. Traditional manual tuning, while offering simplicity and interpretability, often yields suboptimal results due to its unprincipled tuning objectives, the vast amount of parameter combinations, and its lack of personalization and adaptability to seasonality. In this work, we propose a general Deep Reinforcement Learning framework for Personalized Utility Tuning (DRL-PUT) to address the challenges of multi-objective optimization within ad recommender systems. Our key contributions include: 1) Formulating the problem as a reinforcement learning task: given the state of an ad request, we predict the optimal hyperparameters to maximize a pre-defined reward. 2) Developing an approach to directly learn an optimal policy model using online serving logs, avoiding the need to estimate a value function, which is inherently challenging due to the high variance and unbalanced distribution of immediate rewards. We evaluated DRL-PUT through an online A/B experiment in Pinterest's ad recommender system. Compared to the baseline manual utility tuning approach, DRL-PUT improved the click-through rate by 9.7% and the long click-through rate by 7.7% on the treated segment. We conducted a detailed ablation study on the impact of different reward definitions and analyzed the personalization aspect of the learned policy model.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving</title>
<link>https://arxiv.org/abs/2509.02718</link>
<guid>https://arxiv.org/abs/2509.02718</guid>
<content:encoded><![CDATA[
arXiv:2509.02718v1 Announce Type: cross 
Abstract: Increasing demand for Large Language Models (LLMs) services imposes substantial deployment and computation costs on providers. LLM routing offers a cost-efficient solution by directing queries to the optimal LLM based on model and query features. However, existing works primarily focus on offline scenarios and struggle to adapt to online settings with high query volume and constrained token budgets. In this work, we introduce the first training-free algorithm for online routing scenarios. Our algorithm leverages approximate nearest neighbor search to efficiently estimate query features and performs a one-time optimization over a small set of initial queries to learn a routing strategy that guides future routing. We provide theoretical guarantees demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$ under natural assumptions, which is further validated by extensive experiments across 3 benchmark datasets and 8 baselines, showing an average improvement of 3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and nearly 4.25$\times$ in throughput.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.04069</link>
<guid>https://arxiv.org/abs/2509.04069</guid>
<content:encoded><![CDATA[
arXiv:2509.04069v1 Announce Type: cross 
Abstract: This paper proposes an exploration-efficient Deep Reinforcement Learning with Reference policy (DRLR) framework for learning robotics tasks that incorporates demonstrations. The DRLR framework is developed based on an algorithm called Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve IBRL by modifying the action selection module. The proposed action selection module provides a calibrated Q-value, which mitigates the bootstrapping error that otherwise leads to inefficient exploration. Furthermore, to prevent the RL policy from converging to a sub-optimal policy, SAC is used as the RL policy instead of TD3. The effectiveness of our method in mitigating bootstrapping error and preventing overfitting is empirically validated by learning two robotics tasks: bucket loading and open drawer, which require extensive interactions with the environment. Simulation results also demonstrate the robustness of the DRLR framework across tasks with both low and high state-action dimensions, and varying demonstration qualities. To evaluate the developed framework on a real-world industrial robotics task, the bucket loading task is deployed on a real wheel loader. The sim2real results validate the successful deployment of the DRLR framework.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Collaborative System of Large and Small Models for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2509.04459</link>
<guid>https://arxiv.org/abs/2509.04459</guid>
<content:encoded><![CDATA[
arXiv:2509.04459v1 Announce Type: cross 
Abstract: The advent of Multimodal Large Language Models (MLLMs) has significantly advanced the state-of-the-art in multimodal machine learning, yet their substantial computational demands present a critical barrier to real-world deployment. Conversely, smaller, specialized models offer high efficiency but often at the cost of performance. To reconcile this performance-efficiency trade-off, we propose a novel Uncertainty-Aware Collaborative System (U-ACS) that synergistically orchestrates a powerful MLLM (e.g., HumanOmni) and a lightweight baseline model for multimodal sentiment analysis. The core of our system is an uncertainty-driven cascade mechanism, where the efficient small model first acts as a rapid filter for all input samples. Only those samples yielding high predictive uncertainty, thereby indicating greater difficulty, are selectively escalated to the MLLM for more sophisticated analysis. Furthermore, our system introduces advanced strategies to handle ambiguous or conflicting predictions, including weighted averaging for predictions of similar polarity and a prompt-based cross-verification to resolve conflicting predictions when both models exhibit high uncertainty. This sample-difficulty-aware approach allows for a dynamic allocation of computational resources, drastically reducing inference costs while retaining the high accuracy of MLLM. Extensive experiments on benchmark datasets demonstrate that our proposed method achieves state-of-the-art performance, while requiring only a fraction of the computational resources compared to using a standalone MLLM.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale Graph Neural Network for Turbulent Flow-Thermal Prediction Around a Complex-Shaped Pin-Fin</title>
<link>https://arxiv.org/abs/2509.04463</link>
<guid>https://arxiv.org/abs/2509.04463</guid>
<content:encoded><![CDATA[
arXiv:2509.04463v1 Announce Type: cross 
Abstract: This study presents the development of a domain-responsive edge-aware multiscale Graph Neural Network for predicting steady, turbulent flow and thermal behavior in a two-dimensional channel containing arbitrarily shaped complex pin-fin geometries. The training dataset was constructed through an automated framework that integrated geometry generation, meshing, and flow-field solutions in ANSYS Fluent. The pin-fin geometry was parameterized using piecewise cubic splines, producing 1,000 diverse configurations through Latin Hypercube Sampling. Each simulation was converted into a graph structure, where nodes carried a feature vector containing spatial coordinates, a normalized streamwise position, one-hot boundary indicators, and a signed distance to the nearest boundary such as wall. This graph structure served as input to the newly developed Graph Neural Network, which was trained to predict temperature, velocity magnitude, and pressure at each node using data from ANSYS. The network predicted fields with outstanding accuracy, capturing boundary layers, recirculation, and the stagnation region upstream of the pin-fins while reducing wall time by 2-3 orders of magnitude. In conclusion, the novel graph neural network offered a fast and reliable surrogate for simulations in complex flow configurations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Representation of Generalized Convex Functions and their Gradients</title>
<link>https://arxiv.org/abs/2509.04477</link>
<guid>https://arxiv.org/abs/2509.04477</guid>
<content:encoded><![CDATA[
arXiv:2509.04477v1 Announce Type: cross 
Abstract: Solutions to a wide range of optimization problems, from optimal transport theory to mathematical economics, often take the form of generalized convex functions (GCFs). This characterization can be used to convert nested bilevel optimization problems into single-level optimization problems. Despite this, the characterization has not been fully exploited in numerical optimization.
  When the solution to an optimization problem is known to belong to a particular class of objects, this information can be leveraged by parameterizing that class of objects and optimizing over this parameterization. The hallmark of a good parameterization is the Universal Approximation Property (UAP): that is, the parameterization approximates any object in the class arbitrarily well. For example, neural networks satisfy the UAP with respect to the class of continuous functions.
  Building on the literature concerned with the parameterization of convex functions, we extend these ideas to GCFs. We present a convex and potentially one-to-one parameterization of GCFs and their gradients that satisfies the UAP. We also compare this class to shallow neural networks and highlight their shared characteristics.
  The ideas pursued here have been implemented in the Python package \href{https://github.com/MoeenNehzati/gconvex}{\texttt{gconvex}}, available online. Using it, we tackle the problem of finding the revenue-maximizing auction for multiple goods and demonstrate how our parameterization can effectively solve this problem.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Prompt Tuning via Recursive Utilization of Black-box Multimodal Large Language Model for Personalized Visual Emotion Recognition</title>
<link>https://arxiv.org/abs/2509.04480</link>
<guid>https://arxiv.org/abs/2509.04480</guid>
<content:encoded><![CDATA[
arXiv:2509.04480v1 Announce Type: cross 
Abstract: Visual Emotion Recognition (VER) is an important research topic due to its wide range of applications, including opinion mining and advertisement design. Extending this capability to recognize emotions at the individual level further broadens its potential applications. Recently, Multimodal Large Language Models (MLLMs) have attracted increasing attention and demonstrated performance comparable to that of conventional VER methods. However, MLLMs are trained on large and diverse datasets containing general opinions, which causes them to favor majority viewpoints and familiar patterns. This tendency limits their performance in a personalized VER, which is crucial for practical and real-world applications, and indicates a key area for improvement. To address this limitation, the proposed method employs discrete prompt tuning inspired by the process of humans' prompt engineering to adapt the VER task to each individual. Our method selects the best natural language representation from the generated prompts and uses it to update the prompt for the realization of accurate personalized VER.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Reinforcement Learning for Model Training, and future directions with GRAPE</title>
<link>https://arxiv.org/abs/2509.04501</link>
<guid>https://arxiv.org/abs/2509.04501</guid>
<content:encoded><![CDATA[
arXiv:2509.04501v1 Announce Type: cross 
Abstract: This paper provides a self-contained, from-scratch, exposition of key algorithms for instruction tuning of models: SFT, Rejection Sampling, REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct Preference Optimization (DPO). Explanations of these algorithms often assume prior knowledge, lack critical details, and/or are overly generalized and complex. Here, each method is discussed and developed step by step using simplified and explicit notation focused on LLMs, aiming to eliminate ambiguity and provide a clear and intuitive understanding of the concepts. By minimizing detours into the broader RL literature and connecting concepts to LLMs, we eliminate superfluous abstractions and reduce cognitive overhead. Following this exposition, we provide a literature review of new techniques and approaches beyond those detailed. Finally, new ideas for research and exploration in the form of GRAPE (Generalized Relative Advantage Policy Evolution) are presented.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling behavior of large language models in emotional safety classification across sizes and tasks</title>
<link>https://arxiv.org/abs/2509.04512</link>
<guid>https://arxiv.org/abs/2509.04512</guid>
<content:encoded><![CDATA[
arXiv:2509.04512v1 Announce Type: cross 
Abstract: Understanding how large language models (LLMs) process emotionally sensitive content is critical for building safe and reliable systems, particularly in mental health contexts. We investigate the scaling behavior of LLMs on two key tasks: trinary classification of emotional safety (safe vs. unsafe vs. borderline) and multi-label classification using a six-category safety risk taxonomy. To support this, we construct a novel dataset by merging several human-authored mental health datasets (> 15K samples) and augmenting them with emotion re-interpretation prompts generated via ChatGPT. We evaluate four LLaMA models (1B, 3B, 8B, 70B) across zero-shot, few-shot, and fine-tuning settings. Our results show that larger LLMs achieve stronger average performance, particularly in nuanced multi-label classification and in zero-shot settings. However, lightweight fine-tuning allowed the 1B model to achieve performance comparable to larger models and BERT in several high-data categories, while requiring <2GB VRAM at inference. These findings suggest that smaller, on-device models can serve as viable, privacy-preserving alternatives for sensitive applications, offering the ability to interpret emotional context and maintain safe conversational boundaries. This work highlights key implications for therapeutic LLM applications and the scalable alignment of safety-critical systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Voluntarily Reported Data Post Mesh Implantation for Detecting Public Emotion and Identifying Concern Reports</title>
<link>https://arxiv.org/abs/2509.04517</link>
<guid>https://arxiv.org/abs/2509.04517</guid>
<content:encoded><![CDATA[
arXiv:2509.04517v1 Announce Type: cross 
Abstract: Mesh implants are widely utilized in hernia repair surgeries, but postoperative complications present a significant concern. This study analyzes patient reports from the Manufacturer and User Facility Device Experience (MAUDE) database spanning 2000 to 2021 to investigate the emotional aspects of patients following mesh implantation using Natural Language Processing (NLP). Employing the National Research Council Canada (NRC) Emotion Lexicon and TextBlob for sentiment analysis, the research categorizes patient narratives into eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and assesses sentiment polarity. The goal is to discern patterns in patient sentiment over time and to identify reports signaling urgent concerns, referred to as "Concern Reports," thereby understanding shifts in patient experiences in relation to changes in medical device regulation and technological advancements in healthcare. The study detected an increase in Concern Reports and higher emotional intensity during the periods of 2011-2012 and 2017-2018. Through temporal analysis of Concern Reports and overall sentiment, this research provides valuable insights for healthcare practitioners, enhancing their understanding of patient experiences post-surgery, which is critical for improving preoperative counselling, postoperative care, and preparing patients for mesh implant surgeries. The study underscores the importance of emotional considerations in medical practices and the potential for sentiment analysis to inform and enhance patient care.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably data-driven projection method for quadratic programming</title>
<link>https://arxiv.org/abs/2509.04524</link>
<guid>https://arxiv.org/abs/2509.04524</guid>
<content:encoded><![CDATA[
arXiv:2509.04524v1 Announce Type: cross 
Abstract: Projection methods aim to reduce the dimensionality of the optimization instance, thereby improving the scalability of high-dimensional problems. Recently, Sakaue and Oki proposed a data-driven approach for linear programs (LPs), where the projection matrix is learned from observed problem instances drawn from an application-specific distribution of problems. We analyze the generalization guarantee for the data-driven projection matrix learning for convex quadratic programs (QPs). Unlike in LPs, the optimal solutions of convex QPs are not confined to the vertices of the feasible polyhedron, and this complicates the analysis of the optimal value function. To overcome this challenge, we demonstrate that the solutions of convex QPs can be localized within a feasible region corresponding to a special active set, utilizing Caratheodory's theorem. Building on such observation, we propose the unrolled active set method, which models the computation of the optimal value as a Goldberg-Jerrum (GJ) algorithm with bounded complexities, thereby establishing learning guarantees. We then further extend our analysis to other settings, including learning to match the optimal solution and input-aware setting, where we learn a mapping from QP problem instances to projection matrices.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Policy Adaptation via Cross-Domain Skill Diffusion</title>
<link>https://arxiv.org/abs/2509.04535</link>
<guid>https://arxiv.org/abs/2509.04535</guid>
<content:encoded><![CDATA[
arXiv:2509.04535v1 Announce Type: cross 
Abstract: In this work, we present an in-context policy adaptation (ICPAD) framework designed for long-horizon multi-task environments, exploring diffusion-based skill learning techniques in cross-domain settings. The framework enables rapid adaptation of skill-based reinforcement learning policies to diverse target domains, especially under stringent constraints on no model updates and only limited target domain data. Specifically, the framework employs a cross-domain skill diffusion scheme, where domain-agnostic prototype skills and a domain-grounded skill adapter are learned jointly and effectively from an offline dataset through cross-domain consistent diffusion processes. The prototype skills act as primitives for common behavior representations of long-horizon policies, serving as a lingua franca to bridge different domains. Furthermore, to enhance the in-context adaptation performance, we develop a dynamic domain prompting scheme that guides the diffusion-based skill adapter toward better alignment with the target domain. Through experiments with robotic manipulation in Metaworld and autonomous driving in CARLA, we show that our $\oursol$ framework achieves superior policy adaptation performance under limited target domain data conditions for various cross-domain configurations including differences in environment dynamics, agent embodiment, and task horizon.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interactive Tool for Analyzing High-Dimensional Clusterings</title>
<link>https://arxiv.org/abs/2509.04603</link>
<guid>https://arxiv.org/abs/2509.04603</guid>
<content:encoded><![CDATA[
arXiv:2509.04603v1 Announce Type: cross 
Abstract: Technological advances have spurred an increase in data complexity and dimensionality. We are now in an era in which data sets containing thousands of features are commonplace. To digest and analyze such high-dimensional data, dimension reduction techniques have been developed and advanced along with computational power. Of these techniques, nonlinear methods are most commonly employed because of their ability to construct visually interpretable embeddings. Unlike linear methods, these methods non-uniformly stretch and shrink space to create a visual impression of the high-dimensional data. Since capturing high-dimensional structures in a significantly lower number of dimensions requires drastic manipulation of space, nonlinear dimension reduction methods are known to occasionally produce false structures, especially in noisy settings. In an effort to deal with this phenomenon, we developed an interactive tool that enables analysts to better understand and diagnose their dimension reduction results. It uses various analytical plots to provide a multi-faceted perspective on results to determine legitimacy. The tool is available via an R package named DRtool.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs</title>
<link>https://arxiv.org/abs/2509.04615</link>
<guid>https://arxiv.org/abs/2509.04615</guid>
<content:encoded><![CDATA[
arXiv:2509.04615v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) has introduced critical security challenges, where adversarial actors can manipulate input prompts to cause significant harm and circumvent safety alignments. These prompt-based attacks exploit vulnerabilities in a model's design, training, and contextual understanding, leading to intellectual property theft, misinformation generation, and erosion of user trust. A systematic understanding of these attack vectors is the foundational step toward developing robust countermeasures. This paper presents a comprehensive literature survey of prompt-based attack methodologies, categorizing them to provide a clear threat model. By detailing the mechanisms and impacts of these exploits, this survey aims to inform the research community's efforts in building the next generation of secure LLMs that are inherently resistant to unauthorized distillation, fine-tuning, and editing.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Environments for Organoid Intelligence with LLM-Automated Design and Plasticity-Based Evaluation</title>
<link>https://arxiv.org/abs/2509.04633</link>
<guid>https://arxiv.org/abs/2509.04633</guid>
<content:encoded><![CDATA[
arXiv:2509.04633v1 Announce Type: cross 
Abstract: As the complexity of artificial agents increases, the design of environments that can effectively shape their behavior and capabilities has become a critical research frontier. We propose a framework that extends this principle to a novel class of agents: biological neural networks in the form of neural organoids. This paper introduces three scalable, closed-loop virtual environments designed to train organoid-based biological agents and probe the underlying mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments with increasing complexity: (1) a conditional avoidance task, (2) a one-dimensional predator-prey scenario, and (3) a replication of the classic Pong game. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation. Furthermore, we propose a novel meta-learning approach where a Large Language Model (LLM) is used to automate the generation and optimization of experimental protocols, scaling the process of environment and curriculum design. Finally, we outline a multi-modal approach for evaluating learning by measuring synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between computational neuroscience and agent-based AI, offering a unique platform for studying embodiment, learning, and intelligence in a controlled biological substrate.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maestro: Joint Graph &amp; Config Optimization for Reliable AI Agents</title>
<link>https://arxiv.org/abs/2509.04642</link>
<guid>https://arxiv.org/abs/2509.04642</guid>
<content:encoded><![CDATA[
arXiv:2509.04642v1 Announce Type: cross 
Abstract: Building reliable LLM agents requires decisions at two levels: the graph (which modules exist and how information flows) and the configuration of each node (models, prompts, tools, control knobs). Most existing optimizers tune configurations while holding the graph fixed, leaving structural failure modes unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for LLM agents that jointly searches over graphs and configurations to maximize agent quality, subject to explicit rollout/token budgets. Beyond numeric metrics, Maestro leverages reflective textual feedback from traces to prioritize edits, improving sample efficiency and targeting specific failure modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%, 4.9%, and 4.86%, respectively; even when restricted to prompt-only optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these results with far fewer rollouts than GEPA. We further show large gains on two applications (interviewer & RAG agents), highlighting that joint graph & configuration search addresses structural failure modes that prompt tuning alone cannot fix.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating NL2SQL via SQL2NL</title>
<link>https://arxiv.org/abs/2509.04657</link>
<guid>https://arxiv.org/abs/2509.04657</guid>
<content:encoded><![CDATA[
arXiv:2509.04657v1 Announce Type: cross 
Abstract: Robust evaluation in the presence of linguistic variation is key to understanding the generalization capabilities of Natural Language to SQL (NL2SQL) models, yet existing benchmarks rarely address this factor in a systematic or controlled manner. We propose a novel schema-aligned paraphrasing framework that leverages SQL-to-NL (SQL2NL) to automatically generate semantically equivalent, lexically diverse queries while maintaining alignment with the original schema and intent. This enables the first targeted evaluation of NL2SQL robustness to linguistic variation in isolation-distinct from prior work that primarily investigates ambiguity or schema perturbations. Our analysis reveals that state-of-the-art models are far more brittle than standard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop in execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries, while LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to 42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We also find that robustness degradation varies significantly with query complexity, dataset, and domain -- highlighting the need for evaluation frameworks that explicitly measure linguistic generalization to ensure reliable performance in real-world settings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DarkStream: real-time speech anonymization with low latency</title>
<link>https://arxiv.org/abs/2509.04667</link>
<guid>https://arxiv.org/abs/2509.04667</guid>
<content:encoded><![CDATA[
arXiv:2509.04667v1 Announce Type: cross 
Abstract: We propose DarkStream, a streaming speech synthesis model for real-time speaker anonymization. To improve content encoding under strict latency constraints, DarkStream combines a causal waveform encoder, a short lookahead buffer, and transformer-based contextual layers. To further reduce inference time, the model generates waveforms directly via a neural vocoder, thus removing intermediate mel-spectrogram conversions. Finally, DarkStream anonymizes speaker identity by injecting a GAN-generated pseudo-speaker embedding into linguistic features from the content encoder. Evaluations show our model achieves strong anonymization, yielding close to 50% speaker verification EER (near-chance performance) on the lazy-informed attack scenario, while maintaining acceptable linguistic intelligibility (WER within 9%). By balancing low-latency, robust privacy, and minimal intelligibility degradation, DarkStream provides a practical solution for privacy-preserving real-time speech communication.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation</title>
<link>https://arxiv.org/abs/2509.04669</link>
<guid>https://arxiv.org/abs/2509.04669</guid>
<content:encoded><![CDATA[
arXiv:2509.04669v1 Announce Type: cross 
Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs) have challenged the dominance of Convolutional Neural Networks (CNNs) in computer vision. ViTs excel at capturing global context, and SSMs like Mamba offer linear complexity for long sequences, yet they do not capture fine-grained local features as effectively as CNNs. Conversely, CNNs possess strong inductive biases for local features but lack the global reasoning capabilities of transformers and Mamba. To bridge this gap, we introduce \textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a hierarchical structure with convolutional blocks in its early stages to extract rich local features. These convolutional blocks are then processed by later stages incorporating multi-directional Mamba blocks designed to efficiently model long-range dependencies and global context. This hybrid design allows for superior feature representation while maintaining linear complexity with respect to image resolution. We demonstrate VCMamba's effectiveness through extensive experiments on ImageNet-1K classification and ADE20K semantic segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K, surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains 47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing 62% fewer parameters. Code is available at https://github.com/Wertyuui345/VCMamba.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring the Graph Structure of Images for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.04677</link>
<guid>https://arxiv.org/abs/2509.04677</guid>
<content:encoded><![CDATA[
arXiv:2509.04677v1 Announce Type: cross 
Abstract: Image datasets such as MNIST are a key benchmark for testing Graph Neural Network (GNN) architectures. The images are traditionally represented as a grid graph with each node representing a pixel and edges connecting neighboring pixels (vertically and horizontally). The graph signal is the values (intensities) of each pixel in the image. The graphs are commonly used as input to graph neural networks (e.g., Graph Convolutional Neural Networks (Graph CNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the images. In this work, we improve the accuracy of downstream graph neural network tasks by finding alternative graphs to the grid graph and superpixel methods to represent the dataset images, following the approach in [5, 6]. We find row correlation, column correlation, and product graphs for each image in MNIST and Fashion-MNIST using correlations between the pixel values building on the method in [5, 6]. Experiments show that using these different graph representations and features as input into downstream GNN models improves the accuracy over using the traditional grid graph and superpixel methods in the literature.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring</title>
<link>https://arxiv.org/abs/2509.04682</link>
<guid>https://arxiv.org/abs/2509.04682</guid>
<content:encoded><![CDATA[
arXiv:2509.04682v1 Announce Type: cross 
Abstract: Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal data for long-term ecological analysis, but intrinsic noise and complex signal dependencies hinder model stability and generalization. Multilayered windowing has improved target sound localization, yet variability from shifting ambient noise, diverse propagation effects, and mixed biological and anthropogenic sources demands robust architectures and rigorous evaluation. We introduce GetNetUPAM, a hierarchical nested cross-validation framework designed to quantify model stability under ecologically realistic variability. Data are partitioned into distinct site-year segments, preserving recording heterogeneity and ensuring each validation fold reflects a unique environmental subset, reducing overfitting to localized noise and sensor artifacts. Site-year blocking enforces evaluation against genuine environmental diversity, while standard cross-validation on random subsets measures generalization across UPAM's full signal distribution, a dimension absent from current benchmarks. Using GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution Pooling and Attention Network (ARPA-N), a neural architecture for irregular spectrogram dimensions. Adaptive pooling with spatial attention extends the receptive field, capturing global context without excessive parameters. Under GetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet baselines and a log2-scale order-of-magnitude drop in variability across all metrics, enabling consistent detection across site-year folds and advancing scalable, accurate bioacoustic monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Representation Learning for Multi-Intent Diversity and Behavioral Uncertainty in Recommender Systems</title>
<link>https://arxiv.org/abs/2509.04694</link>
<guid>https://arxiv.org/abs/2509.04694</guid>
<content:encoded><![CDATA[
arXiv:2509.04694v1 Announce Type: cross 
Abstract: This paper addresses the challenge of jointly modeling user intent diversity and behavioral uncertainty in recommender systems. A unified representation learning framework is proposed. The framework builds a multi-intent representation module and an uncertainty modeling mechanism. It extracts multi-granularity interest structures from user behavior sequences. Behavioral ambiguity and preference fluctuation are captured using Bayesian distribution modeling. In the multi-intent modeling part, the model introduces multiple latent intent vectors. These vectors are weighted and fused using an attention mechanism to generate semantically rich representations of long-term user preferences. In the uncertainty modeling part, the model learns the mean and covariance of behavior representations through Gaussian distributions. This reflects the user's confidence in different behavioral contexts. Next, a learnable fusion strategy is used to combine long-term intent and short-term behavior signals. This produces the final user representation, improving both recommendation accuracy and robustness. The method is evaluated on standard public datasets. Experimental results show that it outperforms existing representative models across multiple metrics. It also demonstrates greater stability and adaptability under cold-start and behavioral disturbance scenarios. The approach alleviates modeling bottlenecks faced by traditional methods when dealing with complex user behavior. These findings confirm the effectiveness and practical value of the unified modeling strategy in real-world recommendation tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.04712</link>
<guid>https://arxiv.org/abs/2509.04712</guid>
<content:encoded><![CDATA[
arXiv:2509.04712v1 Announce Type: cross 
Abstract: Automated vehicle control using reinforcement learning (RL) has attracted significant attention due to its potential to learn driving policies through environment interaction. However, RL agents often face training challenges in sample efficiency and effective exploration, making it difficult to discover an optimal driving strategy. To address these issues, we propose guiding the RL driving agent with a demonstration policy that need not be a highly optimized or expert-level controller. Specifically, we integrate a rule-based lane change controller with the Soft Actor Critic (SAC) algorithm to enhance exploration and learning efficiency. Our approach demonstrates improved driving performance and can be extended to other driving scenarios that can similarly benefit from demonstration-based guidance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)</title>
<link>https://arxiv.org/abs/2509.04721</link>
<guid>https://arxiv.org/abs/2509.04721</guid>
<content:encoded><![CDATA[
arXiv:2509.04721v1 Announce Type: cross 
Abstract: This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic framework for benchmarking the real-time performance of TinyML models on resource-constrained embedded systems. Evaluating key metrics such as inference latency, CPU utilization, memory efficiency, and prediction stability, the framework provides insights into computational trade-offs and platform-specific optimizations. We benchmark three representative TinyML models -- Gesture Classification, Keyword Spotting, and MobileNet V2 -- on two widely adopted platforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets. Results reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent inference latency for AI-specific tasks, while the Raspberry Pi 4 excels in resource efficiency and cost-effectiveness. These findings offer actionable guidance for optimizing TinyML deployments, bridging the gap between theoretical advancements and practical applications in embedded systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning</title>
<link>https://arxiv.org/abs/2509.04731</link>
<guid>https://arxiv.org/abs/2509.04731</guid>
<content:encoded><![CDATA[
arXiv:2509.04731v1 Announce Type: cross 
Abstract: The convergence of Language models, Agent models, and World models represents a critical frontier for artificial intelligence. While recent progress has focused on scaling Language and Agent models, the development of sophisticated, explicit World Models remains a key bottleneck, particularly for complex, long-horizon multi-agent tasks. In domains such as robotic soccer, agents trained via standard reinforcement learning in high-fidelity but structurally-flat simulators often fail due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing capable agents lies in creating environments that possess an explicit, hierarchical World Model. We contend that this is best achieved through hierarchical scaffolding, where complex goals are decomposed into structured, manageable subgoals. Drawing evidence from a systematic review of 2024 research in multi-agent soccer, we identify a clear and decisive trend towards integrating symbolic and hierarchical methods with multi-agent reinforcement learning (MARL). These approaches implicitly or explicitly construct a task-based world model to guide agent learning. We then propose a paradigm shift: leveraging Large Language Models to dynamically generate this hierarchical scaffold, effectively using language to structure the World Model on the fly. This language-driven world model provides an intrinsic curriculum, dense and meaningful learning signals, and a framework for compositional learning, enabling Agent Models to acquire sophisticated, strategic behaviors with far greater sample efficiency. By building environments with explicit, language-configurable task layers, we can bridge the gap between low-level reactive behaviors and high-level strategic team play, creating a powerful and generalizable framework for training the next generation of intelligent agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Foundation Model-Driven User Interest Modeling and Behavior Analysis on Short Video Platforms</title>
<link>https://arxiv.org/abs/2509.04751</link>
<guid>https://arxiv.org/abs/2509.04751</guid>
<content:encoded><![CDATA[
arXiv:2509.04751v1 Announce Type: cross 
Abstract: With the rapid expansion of user bases on short video platforms, personalized recommendation systems are playing an increasingly critical role in enhancing user experience and optimizing content distribution. Traditional interest modeling methods often rely on unimodal data, such as click logs or text labels, which limits their ability to fully capture user preferences in a complex multimodal content environment. To address this challenge, this paper proposes a multimodal foundation model-based framework for user interest modeling and behavior analysis. By integrating video frames, textual descriptions, and background music into a unified semantic space using cross-modal alignment strategies, the framework constructs fine-grained user interest vectors. Additionally, we introduce a behavior-driven feature embedding mechanism that incorporates viewing, liking, and commenting sequences to model dynamic interest evolution, thereby improving both the timeliness and accuracy of recommendations. In the experimental phase, we conduct extensive evaluations using both public and proprietary short video datasets, comparing our approach against multiple mainstream recommendation algorithms and modeling techniques. Results demonstrate significant improvements in behavior prediction accuracy, interest modeling for cold-start users, and recommendation click-through rates. Moreover, we incorporate interpretability mechanisms using attention weights and feature visualization to reveal the model's decision basis under multimodal inputs and trace interest shifts, thereby enhancing the transparency and controllability of the recommendation system.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching</title>
<link>https://arxiv.org/abs/2509.04752</link>
<guid>https://arxiv.org/abs/2509.04752</guid>
<content:encoded><![CDATA[
arXiv:2509.04752v1 Announce Type: cross 
Abstract: This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM health coaching system that integrates personalized machine learning and retrieval-augmented generation to deliver adaptive, evidence-based guidance. SePA combines: (1) Individualized models predicting daily stress, soreness, and injury risk from wearable sensor data (28 users, 1260 data points); and (2) A retrieval module that grounds LLM-generated feedback in expert-vetted web content to ensure contextual relevance and reliability. Our predictive models, evaluated with rolling-origin cross-validation and group k-fold cross-validation show that personalized models outperform generalized baselines. In a pilot expert study (n=4), SePA's retrieval-based advice was preferred over a non-retrieval baseline, yielding meaningful practical effect (Cliff's $\delta$=0.3, p=0.05). We also quantify latency performance trade-offs between response quality and speed, offering a transparent blueprint for next-generation, trustworthy personal health informatics systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Multi-hop Inference Optimization of LLM Based on MQUAKE Framework</title>
<link>https://arxiv.org/abs/2509.04770</link>
<guid>https://arxiv.org/abs/2509.04770</guid>
<content:encoded><![CDATA[
arXiv:2509.04770v1 Announce Type: cross 
Abstract: Accurately answering complex questions has consistently been a significant challenge for Large Language Models (LLMs). To address this, this paper proposes a multi-hop question decomposition method for complex questions, building upon research within the MQUAKE framework. Utilizing the LLAMA3 model, we systematically investigate the impact of multi-hop question decomposition within knowledge graphs on model comprehension and reasoning accuracy, both before and after model training. In our experiments, we systematically partitioned and converted the MQUAKE-T dataset into two distinct formats: a single-hop dataset designed for directly answering complex questions, and a multi-hop dataset constructed using the multi-hop question decomposition method. We then fine-tuned the LLAMA3 model on these datasets and conducted inference tests. Our results demonstrate that, without fine-tuning the LLM, the prediction performance based on the multi-hop question decomposition method significantly outperforms the method of directly answering complex questions. After fine-tuning using the LoRA (Low-Rank Adaptation) method, the performance of both approaches improved compared to the untrained baseline. Crucially, the method utilizing multi-hop decomposition consistently maintained its superiority. These findings validate the effectiveness of the multi-hop decomposition method both before and after training, demonstrating its capability to effectively enhance the LLM's ability to answer complex questions.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models</title>
<link>https://arxiv.org/abs/2509.04781</link>
<guid>https://arxiv.org/abs/2509.04781</guid>
<content:encoded><![CDATA[
arXiv:2509.04781v1 Announce Type: cross 
Abstract: When given the option, will LLMs choose to leave the conversation (bail)? We investigate this question by giving models the option to bail out of interactions using three different bail methods: a bail tool the model can call, a bail string the model can output, and a bail prompt that asks the model if it wants to leave. On continuations of real world data (Wildchat and ShareGPT), all three of these bail methods find models will bail around 0.28-32\% of the time (depending on the model and bail method). However, we find that bail rates can depend heavily on the model used for the transcript, which means we may be overestimating real world bail rates by up to 4x. If we also take into account false positives on bail prompt (22\%), we estimate real world bail rates range from 0.06-7\%, depending on the model and bail method. We use observations from our continuations of real world data to construct a non-exhaustive taxonomy of bail cases, and use this taxonomy to construct BailBench: a representative synthetic dataset of situations where some models bail. We test many models on this dataset, and observe some bail behavior occurring for most of them. Bail rates vary substantially between models, bail methods, and prompt wordings. Finally, we study the relationship between refusals and bails. We find: 1) 0-13\% of continuations of real world conversations resulted in a bail without a corresponding refusal 2) Jailbreaks tend to decrease refusal rates, but increase bail rates 3) Refusal abliteration increases no-refuse bail rates, but only for some bail methods 4) Refusal rate on BailBench does not appear to predict bail rate.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design</title>
<link>https://arxiv.org/abs/2509.04805</link>
<guid>https://arxiv.org/abs/2509.04805</guid>
<content:encoded><![CDATA[
arXiv:2509.04805v1 Announce Type: cross 
Abstract: Modern fronthaul links in wireless systems must transport high-dimensional signals under stringent bandwidth and latency constraints, which makes compression indispensable. Traditional strategies such as compressed sensing, scalar quantization, and fixed-codec pipelines often rely on restrictive priors, degrade sharply at high compression ratios, and are hard to tune across channels and deployments. Recent progress in Artificial Intelligence (AI) has brought end-to-end learned transforms, vector and hierarchical quantization, and learned entropy models that better exploit the structure of Channel State Information(CSI), precoding matrices, I/Q samples, and LLRs. This paper first surveys AI-driven compression techniques and then provides a focused analysis of two representative high-compression routes: CSI feedback with end-to-end learning and Resource Block (RB) granularity precoding optimization combined with compression. Building on these insights, we propose a fronthaul compression strategy tailored to cell-free architectures. The design targets high compression with controlled performance loss, supports RB-level rate adaptation, and enables low-latency inference suitable for centralized cooperative transmission in next-generation networks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation</title>
<link>https://arxiv.org/abs/2509.04810</link>
<guid>https://arxiv.org/abs/2509.04810</guid>
<content:encoded><![CDATA[
arXiv:2509.04810v1 Announce Type: cross 
Abstract: Automating the decision of whether a code change requires manual review is vital for maintaining software quality in modern development workflows. However, the emergence of new programming languages and frameworks creates a critical bottleneck: while large volumes of unlabelled code are readily available, there is an insufficient amount of labelled data to train supervised models for review classification. We address this challenge by leveraging Large Language Models (LLMs) to translate code changes from well-resourced languages into equivalent changes in underrepresented or emerging languages, generating synthetic training data where labelled examples are scarce. We assume that although LLMs have learned the syntax and semantics of new languages from available unlabelled code, they have yet to fully grasp which code changes are considered significant or review-worthy within these emerging ecosystems. To overcome this, we use LLMs to generate synthetic change examples and train supervised classifiers on them. We systematically compare the performance of these classifiers against models trained on real labelled data. Our experiments across multiple GitHub repositories and language pairs demonstrate that LLM-generated synthetic data can effectively bootstrap review recommendation systems, narrowing the performance gap even in low-resource settings. This approach provides a scalable pathway to extend automated code review capabilities to rapidly evolving technology stacks, even in the absence of annotated data.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.04816</link>
<guid>https://arxiv.org/abs/2509.04816</guid>
<content:encoded><![CDATA[
arXiv:2509.04816v1 Announce Type: cross 
Abstract: Estimating accurate and well-calibrated predictive uncertainty is important for enhancing the reliability of computer vision models, especially in safety-critical applications like traffic scene perception. While ensemble methods are commonly used to quantify uncertainty by combining multiple models, a mixture of experts (MoE) offers an efficient alternative by leveraging a gating network to dynamically weight expert predictions based on the input. Building on the promising use of MoEs for semantic segmentation in our previous works, we show that well-calibrated predictive uncertainty estimates can be extracted from MoEs without architectural modifications. We investigate three methods to extract predictive uncertainty estimates: predictive entropy, mutual information, and expert variance. We evaluate these methods for an MoE with two experts trained on a semantical split of the A2D2 dataset. Our results show that MoEs yield more reliable uncertainty estimates than ensembles in terms of conditional correctness metrics under out-of-distribution (OOD) data. Additionally, we evaluate routing uncertainty computed via gate entropy and find that simple gating mechanisms lead to better calibration of routing uncertainty estimates than more complex classwise gates. Finally, our experiments on the Cityscapes dataset suggest that increasing the number of experts can further enhance uncertainty calibration. Our code is available at https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment</title>
<link>https://arxiv.org/abs/2509.04852</link>
<guid>https://arxiv.org/abs/2509.04852</guid>
<content:encoded><![CDATA[
arXiv:2509.04852v1 Announce Type: cross 
Abstract: Estimating density ratios is a fundamental problem in machine learning, but existing methods often trade off accuracy for efficiency. We propose \textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)}, a framework that enables accurate, any-step estimation without numerical integration.
  Instead of modeling infinitesimal tangents as in prior methods, ISA-DRE learns a global secant function, defined as the expectation of all tangents over an interval, with provably lower variance, making it more suitable for neural approximation. This is made possible by the \emph{Secant Alignment Identity}, a self-consistency condition that formally connects the secant with its underlying tangent representations.
  To mitigate instability during early training, we introduce \emph{Contraction Interval Annealing}, a curriculum strategy that gradually expands the alignment interval during training. This process induces a contraction mapping, which improves convergence and training stability.
  Empirically, ISA-DRE achieves competitive accuracy with significantly fewer function evaluations compared to prior methods, resulting in much faster inference and making it well suited for real-time and interactive applications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering with Randomised Observations: Sequential Learning of Relevant Subspace Properties and Accuracy Analysis</title>
<link>https://arxiv.org/abs/2509.04867</link>
<guid>https://arxiv.org/abs/2509.04867</guid>
<content:encoded><![CDATA[
arXiv:2509.04867v1 Announce Type: cross 
Abstract: State estimation that combines observational data with mathematical models is central to many applications and is commonly addressed through filtering methods, such as ensemble Kalman filters. In this article, we examine the signal-tracking performance of a continuous ensemble Kalman filtering under fixed, randomised, and adaptively varying partial observations. Rigorous bounds are established for the expected signal-tracking error relative to the randomness of the observation operator. In addition, we propose a sequential learning scheme that adaptively determines the dimension of a state subspace sufficient to ensure bounded filtering error, by balancing observation complexity with estimation accuracy. Beyond error control, the adaptive scheme provides a systematic approach to identifying the appropriate size of the filter-relevant subspace of the underlying dynamics.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales</title>
<link>https://arxiv.org/abs/2509.04871</link>
<guid>https://arxiv.org/abs/2509.04871</guid>
<content:encoded><![CDATA[
arXiv:2509.04871v1 Announce Type: cross 
Abstract: Recent advances in language and speech modelling have made it possible to build autonomous voice assistants that understand and generate human dialogue in real time. These systems are increasingly being deployed in domains such as customer service and healthcare care, where they can automate repetitive tasks, reduce operational costs, and provide constant support around the clock. In this paper, we present a general methodology for cloning a conversational voice AI agent from a corpus of call recordings. Although the case study described in this paper uses telesales data to illustrate the approach, the underlying process generalizes to any domain where call transcripts are available. Our system listens to customers over the telephone, responds with a synthetic voice, and follows a structured playbook learned from top performing human agents. We describe the domain selection, knowledge extraction, and prompt engineering used to construct the agent, integrating automatic speech recognition, a large language model based dialogue manager, and text to speech synthesis into a streaming inference pipeline. The cloned agent is evaluated against human agents on a rubric of 22 criteria covering introduction, product communication, sales drive, objection handling, and closing. Blind tests show that the AI agent approaches human performance in routine aspects of the call while underperforming in persuasion and objection handling. We analyze these shortcomings and refine the prompt accordingly. The paper concludes with design lessons and avenues for future research, including large scale simulation and automated evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models</title>
<link>https://arxiv.org/abs/2509.04889</link>
<guid>https://arxiv.org/abs/2509.04889</guid>
<content:encoded><![CDATA[
arXiv:2509.04889v1 Announce Type: cross 
Abstract: Advances in computer vision have opened new avenues for clinical applications, particularly in computerized exposure therapy where visual stimuli can be dynamically adjusted based on patient responses. As a critical step toward such adaptive systems, we investigated whether pretrained computer vision models can accurately predict fear levels from spider-related images. We adapted three diverse models using transfer learning to predict human fear ratings (on a 0-100 scale) from a standardized dataset of 313 images. The models were evaluated using cross-validation, achieving an average mean absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis revealed that reducing the dataset size significantly harmed performance, though further increases yielded no substantial gains. Explainability assessments showed the models' predictions were based on spider-related features. A category-wise error analysis further identified visual conditions associated with higher errors (e.g., distant views and artificial/painted spiders). These findings demonstrate the potential of explainable computer vision models in predicting fear ratings, highlighting the importance of both model explainability and a sufficient dataset size for developing effective emotion-aware therapeutic technologies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynGen-Vision: Synthetic Data Generation for training industrial vision models</title>
<link>https://arxiv.org/abs/2509.04894</link>
<guid>https://arxiv.org/abs/2509.04894</guid>
<content:encoded><![CDATA[
arXiv:2509.04894v1 Announce Type: cross 
Abstract: We propose an approach to generate synthetic data to train computer vision (CV) models for industrial wear and tear detection. Wear and tear detection is an important CV problem for predictive maintenance tasks in any industry. However, data curation for training such models is expensive and time-consuming due to the unavailability of datasets for different wear and tear scenarios. Our approach employs a vision language model along with a 3D simulation and rendering engine to generate synthetic data for varying rust conditions. We evaluate our approach by training a CV model for rust detection using the generated dataset and tested the trained model on real images of rusted industrial objects. The model trained with the synthetic data generated by our approach, outperforms the other approaches with a mAP50 score of 0.87. The approach is customizable and can be easily extended to other industrial wear and tear detection scenarios
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multiple Instance Learning Strategies for Automated Sebocyte Droplet Counting</title>
<link>https://arxiv.org/abs/2509.04895</link>
<guid>https://arxiv.org/abs/2509.04895</guid>
<content:encoded><![CDATA[
arXiv:2509.04895v1 Announce Type: cross 
Abstract: Sebocytes are lipid-secreting cells whose differentiation is marked by the accumulation of intracellular lipid droplets, making their quantification a key readout in sebocyte biology. Manual counting is labor-intensive and subjective, motivating automated solutions. Here, we introduce a simple attention-based multiple instance learning (MIL) framework for sebocyte image analysis. Nile Red-stained sebocyte images were annotated into 14 classes according to droplet counts, expanded via data augmentation to about 50,000 cells. Two models were benchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated patch-level counts, and an attention-based MIL model leveraging ResNet-50 features with instance weighting. Experiments using five-fold cross-validation showed that the baseline MLP achieved more stable performance (mean MAE = 5.6) compared with the attention-based MIL, which was less consistent (mean MAE = 10.7) but occasionally superior in specific folds. These findings indicate that simple bag-level aggregation provides a robust baseline for slide-level droplet counting, while attention-based MIL requires task-aligned pooling and regularization to fully realize its potential in sebocyte image analysis.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLaMo 2 Technical Report</title>
<link>https://arxiv.org/abs/2509.04897</link>
<guid>https://arxiv.org/abs/2509.04897</guid>
<content:encoded><![CDATA[
arXiv:2509.04897v1 Announce Type: cross 
Abstract: In this report, we introduce PLaMo 2, a series of Japanese-focused large language models featuring a hybrid Samba-based architecture that transitions to full attention via continual pre-training to support 32K token contexts. Training leverages extensive synthetic corpora to overcome data scarcity, while computational efficiency is achieved through weight reuse and structured pruning. This efficient pruning methodology produces an 8B model that achieves performance comparable to our previous 100B model. Post-training further refines the models using a pipeline of supervised fine-tuning (SFT) and direct preference optimization (DPO), enhanced by synthetic Japanese instruction data and model merging techniques. Optimized for inference using vLLM and quantization with minimal accuracy loss, the PLaMo 2 models achieve state-of-the-art results on Japanese benchmarks, outperforming similarly-sized open models in instruction-following, language fluency, and Japanese-specific knowledge.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning and composing of classical music using restricted Boltzmann machines</title>
<link>https://arxiv.org/abs/2509.04899</link>
<guid>https://arxiv.org/abs/2509.04899</guid>
<content:encoded><![CDATA[
arXiv:2509.04899v1 Announce Type: cross 
Abstract: Recently, software has been developed that uses machine learning to mimic the style of a particular composer, such as J. S. Bach. However, since such software often adopts machine learning models with complex structures, it is difficult to analyze how the software understands the characteristics of the composer's music. In this study, we adopted J. S. Bach's music for training of a restricted Boltzmann machine (RBM). Since the structure of RBMs is simple, it allows us to investigate the internal states after learning. We found that the learned RBM is able to compose music.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RobQFL: Robust Quantum Federated Learning in Adversarial Environment</title>
<link>https://arxiv.org/abs/2509.04914</link>
<guid>https://arxiv.org/abs/2509.04914</guid>
<content:encoded><![CDATA[
arXiv:2509.04914v1 Announce Type: cross 
Abstract: Quantum Federated Learning (QFL) merges privacy-preserving federation with quantum computing gains, yet its resilience to adversarial noise is unknown. We first show that QFL is as fragile as centralized quantum learning. We propose Robust Quantum Federated Learning (RobQFL), embedding adversarial training directly into the federated loop. RobQFL exposes tunable axes: client coverage $\gamma$ (0-100\%), perturbation scheduling (fixed-$\varepsilon$ vs $\varepsilon$-mixes), and optimization (fine-tune vs scratch), and distils the resulting $\gamma \times \varepsilon$ surface into two metrics: Accuracy-Robustness Area and Robustness Volume. On 15-client simulations with MNIST and Fashion-MNIST, IID and Non-IID conditions, training only 20-50\% clients adversarially boosts $\varepsilon \leq 0.1$ accuracy $\sim$15 pp at $< 2$ pp clean-accuracy cost; fine-tuning adds 3-5 pp. With $\geq$75\% coverage, a moderate $\varepsilon$-mix is optimal, while high-$\varepsilon$ schedules help only at 100\% coverage. Label-sorted non-IID splits halve robustness, underscoring data heterogeneity as a dominant risk.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Variance and Covariance Estimation under Differential Privacy in the Add-Remove Model and Beyond</title>
<link>https://arxiv.org/abs/2509.04919</link>
<guid>https://arxiv.org/abs/2509.04919</guid>
<content:encoded><![CDATA[
arXiv:2509.04919v1 Announce Type: cross 
Abstract: In this paper, we study the problem of estimating the variance and covariance of datasets under differential privacy in the add-remove model. While estimation in the swap model has been extensively studied in the literature, the add-remove model remains less explored and more challenging, as the dataset size must also be kept private. To address this issue, we develop efficient mechanisms for variance and covariance estimation based on the \emph{B\'{e}zier mechanism}, a novel moment-release framework that leverages Bernstein bases. We prove that our proposed mechanisms are minimax optimal in the high-privacy regime by establishing new minimax lower bounds. Moreover, beyond worst-case scenarios, we analyze instance-wise utility and show that the B\'{e}zier-based estimator consistently achieves better utility compared to alternative mechanisms. Finally, we demonstrate the effectiveness of the B\'{e}zier mechanism beyond variance and covariance estimation, showcasing its applicability to other statistical tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial intelligence for representing and characterizing quantum systems</title>
<link>https://arxiv.org/abs/2509.04923</link>
<guid>https://arxiv.org/abs/2509.04923</guid>
<content:encoded><![CDATA[
arXiv:2509.04923v1 Announce Type: cross 
Abstract: Efficient characterization of large-scale quantum systems, especially those produced by quantum analog simulators and megaquop quantum computers, poses a central challenge in quantum science due to the exponential scaling of the Hilbert space with respect to system size. Recent advances in artificial intelligence (AI), with its aptitude for high-dimensional pattern recognition and function approximation, have emerged as a powerful tool to address this challenge. A growing body of research has leveraged AI to represent and characterize scalable quantum systems, spanning from theoretical foundations to experimental realizations. Depending on how prior knowledge and learning architectures are incorporated, the integration of AI into quantum system characterization can be categorized into three synergistic paradigms: machine learning, and, in particular, deep learning and language models. This review discusses how each of these AI paradigms contributes to two core tasks in quantum systems characterization: quantum property prediction and the construction of surrogates for quantum states. These tasks underlie diverse applications, from quantum certification and benchmarking to the enhancement of quantum algorithms and the understanding of strongly correlated phases of matter. Key challenges and open questions are also discussed, together with future prospects at the interface of AI and quantum science.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts</title>
<link>https://arxiv.org/abs/2509.04926</link>
<guid>https://arxiv.org/abs/2509.04926</guid>
<content:encoded><![CDATA[
arXiv:2509.04926v1 Announce Type: cross 
Abstract: The controllability of Large Language Models (LLMs) when used as conversational agents is a key challenge, particularly to ensure predictable and user-personalized responses. This work proposes an ontology-based approach to formally define conversational features that are typically qualitative in nature. By leveraging a set of linguistic descriptors, we derive quantitative definitions for qualitatively-defined concepts, enabling their integration into an ontology for reasoning and consistency checking. We apply this framework to the task of proficiency-level control in conversations, using CEFR language proficiency levels as a case study. These definitions are then formalized in description logic and incorporated into an ontology, which guides controlled text generation of an LLM through fine-tuning. Experimental results demonstrate that our approach provides consistent and explainable proficiency-level definitions, improving transparency in conversational AI.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification of kinetic-related injury in hospital triage data using NLP</title>
<link>https://arxiv.org/abs/2509.04969</link>
<guid>https://arxiv.org/abs/2509.04969</guid>
<content:encoded><![CDATA[
arXiv:2509.04969v1 Announce Type: cross 
Abstract: Triage notes, created at the start of a patient's hospital visit, contain a wealth of information that can help medical staff and researchers understand Emergency Department patient epidemiology and the degree of time-dependent illness or injury. Unfortunately, applying modern Natural Language Processing and Machine Learning techniques to analyse triage data faces some challenges: Firstly, hospital data contains highly sensitive information that is subject to privacy regulation thus need to be analysed on site; Secondly, most hospitals and medical facilities lack the necessary hardware to fine-tune a Large Language Model (LLM), much less training one from scratch; Lastly, to identify the records of interest, expert inputs are needed to manually label the datasets, which can be time-consuming and costly. We present in this paper a pipeline that enables the classification of triage data using LLM and limited compute resources. We first fine-tuned a pre-trained LLM with a classifier using a small (2k) open sourced dataset on a GPU; and then further fine-tuned the model with a hospital specific dataset of 1000 samples on a CPU. We demonstrated that by carefully curating the datasets and leveraging existing models and open sourced data, we can successfully classify triage data with limited compute resources.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAIA: An Inpainting-Based Approach for Music Adversarial Attacks</title>
<link>https://arxiv.org/abs/2509.04980</link>
<guid>https://arxiv.org/abs/2509.04980</guid>
<content:encoded><![CDATA[
arXiv:2509.04980v1 Announce Type: cross 
Abstract: Music adversarial attacks have garnered significant interest in the field of Music Information Retrieval (MIR). In this paper, we present Music Adversarial Inpainting Attack (MAIA), a novel adversarial attack framework that supports both white-box and black-box attack scenarios. MAIA begins with an importance analysis to identify critical audio segments, which are then targeted for modification. Utilizing generative inpainting models, these segments are reconstructed with guidance from the output of the attacked model, ensuring subtle and effective adversarial perturbations. We evaluate MAIA on multiple MIR tasks, demonstrating high attack success rates in both white-box and black-box settings while maintaining minimal perceptual distortion. Additionally, subjective listening tests confirm the high audio fidelity of the adversarial samples. Our findings highlight vulnerabilities in current MIR systems and emphasize the need for more robust and secure models.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts</title>
<link>https://arxiv.org/abs/2509.04982</link>
<guid>https://arxiv.org/abs/2509.04982</guid>
<content:encoded><![CDATA[
arXiv:2509.04982v1 Announce Type: cross 
Abstract: Sentiment classification in short text datasets faces significant challenges such as class imbalance, limited training samples, and the inherent subjectivity of sentiment labels -- issues that are further intensified by the limited context in short texts. These factors make it difficult to resolve ambiguity and exacerbate data sparsity, hindering effective learning. In this paper, we evaluate the effectiveness of small Transformer-based models (i.e., BERT and RoBERTa, with fewer than 1 billion parameters) for multi-label sentiment classification, with a particular focus on short-text settings. Specifically, we evaluated three key factors influencing model performance: (1) continued domain-specific pre-training, (2) data augmentation using automatically generated examples, specifically generative data augmentation, and (3) architectural variations of the classification head. Our experiment results show that data augmentation improves classification performance, while continued pre-training on augmented datasets can introduce noise rather than boost accuracy. Furthermore, we confirm that modifications to the classification head yield only marginal benefits. These findings provide practical guidance for optimizing BERT-based models in resource-constrained settings and refining strategies for sentiment classification in short-text datasets.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Resolution Global Land Surface Temperature Retrieval via a Coupled Mechanism-Machine Learning Framework</title>
<link>https://arxiv.org/abs/2509.04991</link>
<guid>https://arxiv.org/abs/2509.04991</guid>
<content:encoded><![CDATA[
arXiv:2509.04991v1 Announce Type: cross 
Abstract: Land surface temperature (LST) is vital for land-atmosphere interactions and climate processes. Accurate LST retrieval remains challenging under heterogeneous land cover and extreme atmospheric conditions. Traditional split window (SW) algorithms show biases in humid environments; purely machine learning (ML) methods lack interpretability and generalize poorly with limited data. We propose a coupled mechanism model-ML (MM-ML) framework integrating physical constraints with data-driven learning for robust LST retrieval. Our approach fuses radiative transfer modeling with data components, uses MODTRAN simulations with global atmospheric profiles, and employs physics-constrained optimization. Validation against 4,450 observations from 29 global sites shows MM-ML achieves MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming conventional methods. Under extreme conditions, MM-ML reduces errors by over 50%. Sensitivity analysis indicates LST estimates are most sensitive to sensor radiance, then water vapor, and less to emissivity, with MM-ML showing superior stability. These results demonstrate the effectiveness of our coupled modeling strategy for retrieving geophysical parameters. The MM-ML framework combines physical interpretability with nonlinear modeling capacity, enabling reliable LST retrieval in complex environments and supporting climate monitoring and ecosystem studies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.04999</link>
<guid>https://arxiv.org/abs/2509.04999</guid>
<content:encoded><![CDATA[
arXiv:2509.04999v1 Announce Type: cross 
Abstract: Advanced Persistent Threats (APTs) present a considerable challenge to cybersecurity due to their stealthy, long-duration nature. Traditional supervised learning methods typically require large amounts of labeled data, which is often scarce in real-world scenarios. This paper introduces a novel approach that combines AutoEncoders for anomaly detection with active learning to iteratively enhance APT detection. By selectively querying an oracle for labels on uncertain or ambiguous samples, our method reduces labeling costs while improving detection accuracy, enabling the model to effectively learn with minimal data and reduce reliance on extensive manual labeling. We present a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based anomaly detection framework and demonstrate how the active learning loop progressively enhances the model's performance. The framework is evaluated on real-world, imbalanced provenance trace data from the DARPA Transparent Computing program, where APT-like attacks account for just 0.004\% of the data. The datasets, which cover multiple operating systems including Android, Linux, BSD, and Windows, are tested in two attack scenarios. The results show substantial improvements in detection rates during active learning, outperforming existing methods.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Need Intent? Revisiting Response Generation Strategies for Service Assistant</title>
<link>https://arxiv.org/abs/2509.05006</link>
<guid>https://arxiv.org/abs/2509.05006</guid>
<content:encoded><![CDATA[
arXiv:2509.05006v1 Announce Type: cross 
Abstract: In the era of conversational AI, generating accurate and contextually appropriate service responses remains a critical challenge. A central question remains: Is explicit intent recognition a prerequisite for generating high-quality service responses, or can models bypass this step and produce effective replies directly? This paper conducts a rigorous comparative study to address this fundamental design dilemma. Leveraging two publicly available service interaction datasets, we benchmark several state-of-the-art language models, including a fine-tuned T5 variant, across both paradigms: Intent-First Response Generation and Direct Response Generation. Evaluation metrics encompass both linguistic quality and task success rates, revealing surprising insights into the necessity or redundancy of explicit intent modelling. Our findings challenge conventional assumptions in conversational AI pipelines, offering actionable guidelines for designing more efficient and effective response generation systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On approximating the $f$-divergence between two Ising models</title>
<link>https://arxiv.org/abs/2509.05016</link>
<guid>https://arxiv.org/abs/2509.05016</guid>
<content:encoded><![CDATA[
arXiv:2509.05016v1 Announce Type: cross 
Abstract: The $f$-divergence is a fundamental notion that measures the difference between two distributions. In this paper, we study the problem of approximating the $f$-divergence between two Ising models, which is a generalization of recent work on approximating the TV-distance. Given two Ising models $\nu$ and $\mu$, which are specified by their interaction matrices and external fields, the problem is to approximate the $f$-divergence $D_f(\nu\,\|\,\mu)$ within an arbitrary relative error $\mathrm{e}^{\pm \varepsilon}$. For $\chi^\alpha$-divergence with a constant integer $\alpha$, we establish both algorithmic and hardness results. The algorithm works in a parameter regime that matches the hardness result. Our algorithm can be extended to other $f$-divergences such as $\alpha$-divergence, Kullback-Leibler divergence, R\'enyi divergence, Jensen-Shannon divergence, and squared Hellinger distance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamical Learning in Deep Asymmetric Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2509.05041</link>
<guid>https://arxiv.org/abs/2509.05041</guid>
<content:encoded><![CDATA[
arXiv:2509.05041v1 Announce Type: cross 
Abstract: We show that asymmetric deep recurrent neural networks, enhanced with additional sparse excitatory couplings, give rise to an exponentially large, dense accessible manifold of internal representations which can be found by different algorithms, including simple iterative dynamics. Building on the geometrical properties of the stable configurations, we propose a distributed learning scheme in which input-output associations emerge naturally from the recurrent dynamics, without any need of gradient evaluation. A critical feature enabling the learning process is the stability of the configurations reached at convergence, even after removal of the supervisory output signal. Extensive simulations demonstrate that this approach performs competitively on standard AI benchmarks. The model can be generalized in multiple directions, both computational and biological, potentially contributing to narrowing the gap between AI and computational neuroscience.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QCA-MolGAN: Quantum Circuit Associative Molecular GAN with Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.05051</link>
<guid>https://arxiv.org/abs/2509.05051</guid>
<content:encoded><![CDATA[
arXiv:2509.05051v1 Announce Type: cross 
Abstract: Navigating the vast chemical space of molecular structures to design novel drug molecules with desired target properties remains a central challenge in drug discovery. Recent advances in generative models offer promising solutions. This work presents a novel quantum circuit Born machine (QCBM)-enabled Generative Adversarial Network (GAN), called QCA-MolGAN, for generating drug-like molecules. The QCBM serves as a learnable prior distribution, which is associatively trained to define a latent space aligning with high-level features captured by the GANs discriminator. Additionally, we integrate a novel multi-agent reinforcement learning network to guide molecular generation with desired targeted properties, optimising key metrics such as quantitative estimate of drug-likeness (QED), octanol-water partition coefficient (LogP) and synthetic accessibility (SA) scores in conjunction with one another. Experimental results demonstrate that our approach enhances the property alignment of generated molecules with the multi-agent reinforcement learning agents effectively balancing chemical properties.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>